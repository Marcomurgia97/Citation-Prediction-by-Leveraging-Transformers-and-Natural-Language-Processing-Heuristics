text
"for the nonuniform acceleration, the best fraf domain peak coordinate is (α i 0, u i 0 ), then the target's acceleration estimationâ l 0 and jerk estimationĝ i 0 are"
"where τ is a time delay, r c (n, τ ) and r sc (n, τ ) are the iacf of auto-term of clutter and cross-terms between clutter and target."
"compared with the frft and fraf algorithm, the rsfrft and rsfraf can quickly determine the doppler frequencies of the suspected target due to the asft, which can reduce the computational burden in case of a large amount of data."
"this section will introduce a fast and refined processing method of radar maneuvering target based on hierarchical detection, comprehensively utilizing the advantages of mtd, and sparse fractional representation, i.e., rsfrft and rsfraf. it adopts two-level threshold processing, that is, first we employ mtd processing under a higher false alarm probability (p fa ) condition threshold (first-level threshold), screening out the rangbins of possible moving targets, and then pass the echoes of these rangbins in parallel through rsfrft and rsfraf operations, and choose the more sparse one to represent the maneuvering target echoes. the constant false alarm detection (second level threshold) is performed in the corresponding range-optimal sparse fractional representation domain (sfrrd). only a few rangbins exceeding the first level threshold are performed in the second detection stage. it would reduce the amount of calculation while ensuring high detection and estimation performance."
"where r s 1 (t m ) and r s 2 (t m ) represent the radial distance of the accelerated motion and high-order motion (jerk motion), respectively, r 0, v 0, a s, and g s are the initial distance, initial velocity, acceleration, and jerk parameters. record the range-pulses two-dimensional data matrix, i.e.,"
"in this section, real radar data are used to verify the performance of the proposed algorithm in clutter background. moreover, the detection performance is compared with hierarchical coherent integration using frft and fraf (frft-fraf-hci). the computational burden is analyzed and simulated as well."
"as the main means of target detection and surveillance, radar is widely used in civilian and national defense security fields, such as air and marine target monitoring and early warning detection [cit] . affected by the clutter environment and the complex motion characteristics of targets, the radar returns of moving target are extremely weak and complex resulting in low observability, which makes it difficult for radar to detect, especially for the maneuvering target [cit] . reliable, fast detection and estimation techniques for maneuvering targets in complex backgrounds have become the key constraints of radar performance. with the development of signal processing, radar has the ability to acquire refined target the associate editor coordinating the review of this manuscript and approving it for publication was min jia ."
"after the iacf, the remaining procedure of rsfraf is the same as the rsfrft, and normally time delay is a constant value [cit] . supposing the spectrum after reconstruction iŝ f(m), the final result of rsfraf is rα(m)."
"according to the property of ft, the subsampling in the frequency domain can be realized by aliasing in the time domain. then the signal after subsampled-fft is"
"features [cit] . by extending the signal dimensions, it provides a new way to further improve the detection and recognition of radar maneuvering targets. however, there are still several difficulties for radar maneuvering target detection at present, which can be summarized as follows."
"the main process of rsfraf is quite similar to rsfrft, and the difference is that rsfraf has a procedure which is called as the instantaneous autocorrelation function (iacf). the detailed procedure of rsfraf is shown in fig. 2 . for a maneuvering target in clutter background modelled as quadratic frequency modulated (qfm) signal, the discrete signal can be expressed as"
"fraf can be seen as the combination of iacf calculation and frft, and the multiplication of iacf is n . so the overall multiplication complexity of fraf is"
the corresponding coordinates of j in the spectrum sequence of the signal r(n) are obtained by hash inverse mapping and are saved in the set u .
"the traditional mtd method can be realized based on fast fourier transform (fft), which has certain advantages in computational efficiency, but it is not suitable for maneuvering target with time-varying characteristics. the doppler spectrum would be widened and the detection performance is degraded. the fractional transform methods employ chirp signal as the composition basis, and the fractional domain representation between the time domain and the frequency domain can reflect the variation of doppler. it is very suitable for time-varying signals without cross-terms interference. the popular transform is fractional ft (frft) [cit] and fractional ambiguity function (fraf) [cit], etc.. however, it is difficult to adapt to large-scale radar echo processing due to the two-dimensional parameter searching, which is time consuming. moreover, the time-frequency resolution is limited by the searching intervals and the transform itself."
"in this paper, we proposed a detection method for maneuvering target via two-stage hierarchical detection. the first stage is for the coarse detection and only these rangebins higher than the first threshold are carried out further processing. and the second stage is for the refined integration and detection using rsfrft and rsfraf according to the signal sparsity. finally, radar experiment with uav target are used for verification of the proposed method, which shows better performance than the traditional coherent integration method, e.g., mtd, frft, fraf, and classical sft based method. the proposed method only processes in a few rangebins which are determined by the first threshold, thereby reducing the amount of calculation while ensuring higher detection performance, and accurate motion parameters estimation of the maneuvering target, such as speed, acceleration and jerk, etc.. then rapid and refined detection of maneuvering targets can be achieved. long-time processing for more complex motion across rangbins will be analyzed in the future. where he lectures radar principle. he has given more than 20 speeches of radar signal processing, especially marine target. he has published more than 70 academic articles, two books, and holds 28 national invention patents. his current research interests include radar signal processing, especially for marine target detection, moving target detection, micro-doppler, and clutter suppression."
"in this paper, in order to find a balance among the detection performance in clutter background, parameters estimation precision, and the computational cost, we proposed a detection method for maneuvering target via two-stage hierarchical detection in frequency and sparse domain. the first stage is the coarse detection processing screening out the range units having possible moving targets. the second stage is called the refined processing, and the robust sfrft (rsfrft) or robust sparse fraf (rsfraf) are proposed and used for moving target with different high-order motions, i.e., accelerated or jerk motion. the hierarchical processing method can achieve a good detection performance and at the same time reduce the computational burden greatly. the radar signal model of maneuvering target is established in section ii. in section iii, we introduce the principle of the two-stage processing. the flowchart of the proposed method and detailed analysis is shown in section iv. finally, simulation results using real radar data and performances analysis are provided in section v. section vi concludes the paper and gives future research direction."
"where η 2 and η 3 are the second detection threshold, which is determined by the p fa . if the detection statistic is lower than the detection threshold, it indicates that the rangebin has no maneuvering target. if the detection statistic is higher than the detection threshold, it indicates that the rangebin has the maneuvering target."
"for a coherent radar, the received radar returns are filtered and amplified and performed sampling for distance and azimuth directions respectively. usually, the range sampling interval is equal to the radar range resolution, and the azimuth sampling frequency is equal to the pulse repetition frequency to ensure that the echo of the moving target can be completely acquired. assuming that the radar and the target are in the same horizontal plane, the radar transmits a linear frequency modulation (lfm) signal."
where ' * ' indicates the complex conjugate operation. the demodulated radar echo data are subjected to pulse compression processing to obtain radar echo data accumulated within the pulse.
"for more accurate results, m loops are needed in the reconstruction process. we define the occurrence threshold γ, and the frequency whose ''occurrence number'' exceeds γ in the loops, is identified as the suspicious target frequency."
perform the second stage integration detection (refined processing). the optimal result selection criteria between the frft-fraf-hci detection and the rsfrft-rsfraf based detection method is different. define the output scr in the optimal transform domain (scr out ) as
"in he is currently a professor with nau. he has authored numerous articles in his areas of expertise and holds 21 national invention patents. he is the author of two books related to radar detection. his research interests include radar target detection and tracking, image processing, and information fusion. dr. guan is a senior member of the cie and a committee member of radio positioning technology branch in cie. he received the prize of the national excellent doctoral dissertation, realistic outstanding youth practical engineering award of cast and was selected for national talents engineering of ministry of personnel of china. he is in the editorial boards of many radar-related journals. he has served in the technical committee of many international conferences on radar."
"1) the signal-to-clutter ratio (scr) of the maneuvering target is usually low with non-stationary characteristics. the echoes have high-order phase and time-varying frequency characteristics, and the traditional filter-based method, i.e., moving target detection (mtd) is more suitable for analyzing uniform moving targets. for maneuvering target detection [cit], the accumulated echo spectrum will span multiple doppler units, i.e., doppler frequency migration (dfm). therefore, the energy is divergent, which is difficult to achieve coherent integration in one doppler bin. the detection performance would degrade accordingly [cit] . 2) radar moving target detection methods based on time-frequency distribution (tfd) extend onedimensional frequency domain processing to twodimensional time-frequency processing, which can reflect the doppler changes with time [cit] . these methods can be regarded as the extension of traditional mtd method, such as short-time fourier transform (stft), wigner-vill distribution (wvd), etc., which have been used in feature extraction, target imaging, and recognition [cit] . however, there are still some problems, such as low time-frequency aggregation level, limited resolution, and partially affected by crossterms, which makes it difficult to meet the actual radar requirements. in addition, most of the methods are signal matching enhancement methods, which means they need to match the target motion characteristics, but in reality, the moving target signal is complex and the accumulated gain would decrease accordingly. 3) by using a phased array radar or multiple-input and multiple-output (mimo) radar [cit], the observation time can be prolonged to obtain more pulses on target increasing the target's energy accordingly. therefore, the refined description ability of the moving target can be improved [cit] . however, the long integration time and the high sampling frequency greatly increase the number of echo pulses [cit] . the algorithm would cost a huge computational burden, consuming a large amount of radar signal processing resources. that is to say high detection performance and computational efficiency are difficult to balance. therefore, it is urgent to develop and study fast and reliable detection methods suitable for maneuvering targets, and to accurately estimate the motion status and parameters. thereby it would lay a foundation for the refined description of the maneuvering target for further processing."
"the threshold is added after the subsampled fft to estimate the signal sparsity and frequency points. in this way, there is no need to preset the sparsity k, and the coordinates corresponding to the frequency points in the z (m) whose amplitude exceeds the threshold are classified into the set j, that is,"
"step 6: take f α i 0 (m) or r α i 0 (m) as the detection statistics and compare them with the threshold under the condition of low false alarm probability (p fa, usually the false alarm probability is not higher than 10 −3 ) to complete the maneuvering target detection."
"this section paves the way for the routing protocols described in section iv, as it examines the propagation characteristics and limitations of optical waves in underwater mediums. this section further presents the architecture of the uowsn and highlights the channel budget link and its associated ber."
"to resolve the above issues of crp, we propose a distributed sector-based routing protocol (ds), which can be seen as a combination of the nearest neighbor problem [cit] and the traveler salesman problem [cit], i.e., a special greedy version of the crp. the distributed nature of ds exists in the fact that each vertex/node is only aware of its neighboring nodes. in other words, each forwarding node can only compute the weight of the edges within its transmission radius."
"where p t x is the average optical transmitter power, η t and η r are the optical efficiencies of the transmitter and the receiver, respectively. it is shown in our detailed technical report that the received power is high for the low values of extinction coefficients [cit] . furthermore, narrowing the laser beam-width results in increasing the received signal power, as the light beam concentrates on the receiver aperture [cit] ."
"to further overcome the limitations of crp and ds, we lastly propose the distributed sweeping-and-sector-based (dss) routing protocol, which is based on tactfully sweeping the forwarding sector bidirectionally (clockwise and anticlockwise) to select a forwarding candidate."
"a wireless sensor network (wsn), in which multiple sensors communicate using uowc technology, is referred to as underwater optical wireless sensor network (uowsn) in the context of this paper. given the transmission range of uowc, the real assessment of uowsn stipulates developing a multihop network where the sensed information is transmitted over a multi-hop path or a series of relaying nodes. this multi-hop implementation has the advantages of increasing the network lifetime and coverage, extending the communication range, and reducing the interference level [cit], mainly when a proper routing protocol governs the way the information travel among the many sensors. it becomes, therefore, essential to develop practical routing protocols for underwater multihop networks, as a means to achieve reliable and efficient communication links, in an attempt to realize ultra-reliable low latency underwater wireless communication systems. a routing protocol which aims to support an ultra-reliable low-latency underwater communication (urlluc) in uowsns must achieve a minimum end-to-end (e2e) latency of 1 ms and maintain a communication link reliability with bit error rate between [10 −6 − 10 −9 ] [cit] . this paper considers a multi-hop uowsn system and addresses the routing problem to achieve an ultra-reliable low-latency underwater communication. the paper main contributions are routing algorithms which can be implemented in a distributed fashion across the multi-hop links, with a reasonable amount of information exchange. the simulation results show how the proposed algorithms outperform the classical protocols, both in terms of reliability and end-to-end latency."
"3) maximum transmission range: towards keeping a reliable connected network, we impose a constraint on the achievable transmission range. we can derive an analytical expression of the latter by defining a ber threshold ber th, substituting the values of p r x los and r p in (3), and solving it for the transmission range yields (6) . note that, in (6), one can obtain a ber th between any two connected nodes if their distance is within the maximum allowable transmission range r max, where w 0 (·) denotes the principal real-valued branch of the lambert function."
"where er f c(·) is the complementary error function, r 0 and r 1 are the numbers of photon arrivals when transmitting a binary bit 0 and a binary bit 1, respectively. the numbers of photon arrivals for r 1 and r 0 are given by"
"from a system design perspective, routing protocols are the policies that determine the optimized paths along which information travel based on system design requirements [cit] . the existing work on routing protocols can be classified into two categories: centralized and distributed. the literature on centralized routing protocols is particularly rich and addresses various network environments, such as terrestrial, underground and underwater networks, e.g., see [cit] and references therein. centralized routing protocols commonly utilize a central node, which is responsible for collecting all the information from the network, to estimate the optimal path between the source and the target, under a given performance metric constraint [cit] . for large-scale sensor networks, however, centralized implementation of a routing algorithm may become infeasible, especially in underwater scenarios, due to the absence of centralized computing processors."
"where k is the number of hops in the route, ζ is the size of the transmitted packet, d s,t is the total traveled distance from source to the target, and ν is the number of transmitted packets. provided that protocols select their transmission path based on the weight of the edges regardless of the number of hops, increasing the density of the network forces the protocols to select a path with a higher number of hops, to have a better end-to-end path cost. fig. 6 highlights the performance of e2e delay based on the selected paths of each protocol for a 75 bytes message with a packet size of 15 bytes. equally important, delay performance of the distributed algorithms in fig. 6 shows a reverse behavior to the reliability metric, leaving us with a trade-off relationship. fig. 6 shows that all three protocols are meeting the latency requirements of urlluc. furthermore, dss provides high latency, as the source to destination traveled path is typically longer, less directed towards the target, and passes through a large number of forwarding nodes due to sweeping. similarly, fig. 6 shows how the proposed ds provides the least delay among the three mainly when operating with narrowed sectors because it can access longer hops and result in a small number of hops. such observation makes ds the strongest nominee among the three investigated routing protocols for realizing low latency uowc system. crp which chooses the shortest e2e distance path shows an unexpected latency performance because sometimes it operates within be r th different than the distributed protocols (i.e., ds and dss), especially in sparse networks, resulting in a longer total distance. also, given that crp only considers the total cost (weight) of the path, it eventuates in a considerable number of hops."
"the distributed sector-based routing protocol is, therefore, expected to provide the merit of low latency communication. it is more probable to travel less distance because most s t fig. 4 . path selection process for a sweeping-sector distributed algorithm where the blue sector covers only the allowable transmission area (light blue is the area covered due to sweeping) and the green dots are the forwarding nodes, where the yellow ones are marking the rest of the possible candidates, and the red dashed line is an imaginary line marking how the divergence angle is bisected with respect to the target. of its candidate nodes are better directed towards the final destination. this routing protocol, however, suffers in term of reliability performance, because of the harsh nature of the fixed distributed sectoring which limits the search area to a low number of candidates (especially for sparse networks and narrowed sectors), so it increments ber th more often than the other two algorithms."
"at each hop, the forwarding node first fixes a sector width bisected by the imaginary line between itself and the target node. then, it disseminates a short control packet request to send (rts) containing the location of itself and the final target node to its neighbors. in this case, only nodes which are within the transmission radius can reply to the issued request. after collecting the responses, the forwarding node selects the node with the largest weight. the rationale behind that is to compromise between the two objectives of urlluc, i.e., reliability and low latency [cit] . we draw an exception to this rule, at the final hop, when the target node is within that sector; the packet data is directly forwarded to it even if it does not have the largest edge weight. the process of nodeforwarding-and-selection is repeated until a successful route is established. fig. 3 illustrates such process. it is essential to mention that all yellow and green dots, in this case, are within the ber th from their forwarding node."
"optical waves are a promising transmission carrier for enabling urlluc, due to their ability to achieve broadband links with high data rate (gbps) and low latency [cit] . they, however, suffer from several impairments caused by their inherent and apparent properties such as absorption, scattering, salinity, temperature, and misalignment of the transceivers due to the sea surface movements [cit] . reference [cit] particularly highlights the impact of inherent and apparent properties of light on modeling the uowc channel, which explains why it is critical to consider the physical layer characteristics and limitations of the uowc systems while designing routing protocols in underwater mediums. in the following, first, we define the link budget for uowc, then evaluate and show the simulations of a single-hop ber in los conditions. finally, we extend the scenario of single hop ber calculation to the multi-hop case."
"1) uowc link budget: in underwater mediums, the optical waves experience a combination of absorption and scattering effects, which are characterized by beer-lambert's law, and which cause extinction coefficients that depend on the water type and the utilized wavelength [cit] . not only do light waves get attenuated by absorption and scattering effects, but they also experience energy loss when they spread larger than the receiver aperture. the overall path loss, therefore, can be expressed as"
"dss is readily amenable to distributive implementation in practice, and the path selection and data forwarding processes follow the same procedure of the ds protocol. dss bridges the optical limitations induced innately, i.e., an optical transceiver can transmit with a wider beam-width for a shorter range, and to a longer distance with a narrower beam [cit] . hence by sweeping with a narrow beam, dss has the opportunity of selecting among an extensive set of vertices with the lower ber links. this is indeed viable since, for the same candidate set as crp and ds, the probability of error of all edges is less as the transmission occurs with a concentrating beam. fig. 3 shows a realization of a selected route using the dss protocol. since this algorithm sweeps its sector, this approach covers a larger searching area resulting in a more extensive set of candidates of which may have better error probability. these candidates might, however, be positioned further from the target node, affecting the traveled distance and the number of hops, especially for large sectors."
"this paper is one step forward towards realizing ultrareliable low-latency underwater optical communication, by means of efficient routing in underwater optical wireless sensor networks. the paper proposes new routing algorithms which can be implemented in a distributed fashion across uowsns."
"in this section, we assess the performance of all the three routing protocols in terms of end-to-end ber and delay, as a function of the number of nodes (i.e., the density of the network). the reason we evaluate the end-to-end ber performance is to ensure the protocols provide a reliable communication link. additionally, the end-to-end delay evaluation metric is used to examine whether the protocols can support real-time applications or not."
"numerical results validate the performance and qualification of the proposed routing protocols towards achieving ultra-reliable low-latency underwater optical communication. the proposed distributed sector-based and sweeping-and-sector-based routing protocols exhibit a trade-off in the superiority of their performance to realize urlluc in the presence of absorption, scattering, and geometrical losses. finally, the paper elucidates the drawbacks of utilizing a centralized routing protocol in uowsns."
"underwater optical wireless communication (uowc) is an alternative approach to acoustic and rf for creating a reliable, real-time broadband link. optical waves distinguishable feature is their high propagation speed, which substantially reduces the transmission delay and supports broadband communication, even in the presence of high absorption and scattering [cit] . the practical deployment of uowc, however, suffers from the short communication range, which is limited to few tens of meters in aquatic mediums, due to the physical properties of underwater propagation and intrinsic properties of light. despite such limitations, the visible spectrum offers a unique behavior of absorption and scattering in the blue/green region [cit] . utilizing a wavelength within such region, therefore, guarantees low latency underwater wireless communication for relatively long ranges."
"third, in clustering, the failure of v 7 to distinguish lattices near 90 can prevent us from creating reasonably homogeneous clusters that can be distinguished with s 6, g 6 or d 7 . of those three, s 6 is the fastest."
"the 24 equivalent positions [cit] in c 3 as permuations and real-imaginary exchanges (x), given in the same order as the equivalent matrices in table 1 the c 3 presentation of the vector (1) from section 2 is ½à100i; à144i; à400i."
"some of the properties of s 6 are simple. the six base axes are orthogonal, unlike those of g 6 [cit] . for example, the matrix projecting onto the s 2 axis is table 1 the reflections in s 6 ."
"6 does not provide a comparable simple suggestion for an asymmetric unit with a single unique representation of each lattice, except by converting to c 3 and back."
"for a bravais tetrahedron (bravais, 1850) with defining vectors a, b, c, d (the edge vectors of the unit cell plus the negative sum of them), a point in s 6 is"
"the three components can be sorted by their magnitude. the second step is to exchange the real and imaginary parts of c 1 so that the real part is less than or equal to the imaginary part (if necessary); that requires also exchanging c 2 or c 3 . finally, c 2 has its real and imaginary parts exchanged if necessary and of course those of c 3 also. note that the ordering of the real and imaginary parts of c 3 is not defined."
"we have presented representations of a space (parameterized as s 6 and c 3 ) based on the selling parameters and using the selling reduction. geometrically, this represents a significant simplification compared with the complex, non-convex asymmetric unit of niggli reduction and g 6 . conceptually, there is simplification due to the orthogonal rather than inclined axes and single type of boundary of the reduced cell fundamental unit. reasoning is simpler in such a cartesian system. for one thing, there are fewer and simpler boundaries to the fundamental unit."
a simple example of the complexity of the task is that we must decide which of the 24 reflections of one of the points is the closest to the other point. using the reduction operations so that other paths are examined is also required. that the reduction operations do not commute means that the order of operations may in some cases be important.
"distance calculations are faster in s 6 than in g 6 . this is due to the simpler structure of the space which leads to simpler algorithms. niggli reduction sorts the cell parameters, eliminating the 24-fold ambiguity that remains in selling reduction. however, that advantage disappears when computing distances because it is still necessary to examine the same edge cases. selling reduction saves time both for the reduction, and, more importantly, for the calculation of distances among lattices in lattice identification, in cell databases, and in cell clustering."
step 3: exchange the real part of c a with the imaginary part of c x . (the alternative choice of exchanging the real part of c a with the imaginary part of c x is also valid.)
"in addition, because selling-reduced cells are defined as having only zero or negative scalars, the space has boundaries at the transitions to positive scalars. therefore, if either of the two different selling-reduced cells is in the vicinity of a boundary, we also need to consider the path changes that may arise from the reduction steps at that boundary. additional, lower-dimension boundaries may be implied when scalars have equal values, but explicit consideration of those in addition to the permutations and sign-transition boundary transformations does not appear to be needed."
step 2: add the original value of the imaginary part of c n to the real and the imaginary parts of c a and c x .
"4.1.1. creating virtual cartesian points. for a point and a chosen operator for the reduction, we separate the point into two vectors: the projection onto the polytope for which the reduction axis is zero and the perp, the projection onto the reduction axis. the reduction operation is applied to the boundary-projected vector, and then the negative of the perp is added to that result. we call that resulting point the vcp (see fig. 1 ). the goal of creating a vcp is that in measuring distances to points in the fundamental unit one can use the euclidean metric of the fundamental unit."
"the c++ code for distance calculations in s 6 is available using https://github.com/; for cs6dist.h, use https://github.com/ yayahjb/ncdist; for pointdistancefollower (follower implementation), s6dist.h and .cpp, use https://github.com/duck10/ latticereplib."
"the fundamental unit in s 6 and c 3 is chosen to be the region where all six scalars are zero or negative. however, there are 24 representations of a general point in that orthant. c 3 provides the possibility of choosing a particular region of the fundamental unit as the asymmetric unit where there is only a single representation of the general point (similar to an asymmetric unit in a space group)."
"more general tunneling of this type is possible using two boundaries bd dwn with projector p bd dwn, and bd up with projector p bd up"
"the reduction operations do not commute, which will add complexity to distance calculations (see section 4 below). the two choices are related by one of the reflection operations. for distance calculations, all of the reflections must be considered, so the choice will not matter in the end."
step 1: subtract the imaginary part of c n from the real part and change the imaginary part of c n to its negative value.
"we require a distance metric that defines the shortest path among all the representations of two points (lattices). common uses of a metric for lattices are searching in databases of unit-cell parameters, finding possible bravais lattice types, locating possible suitable molecular replacement candidates and, recently, clustering of the images from serial crystallography."
"lattice reduction is quite simple in s 6 [cit], but it has a clearer structure in c 3, so it will be treated there (section 3.2). because of the simple nature of s 6, the inverse of each reduction operation is the same as the unreduction operation, so we term them edge transforms. the matrices in s 6 are unitary, so the metric is the same in each region. however, the transformation matrices are not diagonal, with the result that the boundaries are not simple mirrors."
"the 24 equivalent positions [cit] in s 6 as matrices, given in the same order as the reflections in the 24 equivalent positions [cit] in s 6 have corresponding matrices designed to act on s 6 vectors to map them into crystallographically equivalent vectors. for convenience, they are all listed in table 1 . the structure of the set is clearer in c 3 . see table 2, which presents the reflections in the same order."
"the first type of boundary in s 6 is the polytope where one of the six axes is zero. [contrast this with g 6 [cit], which has 15 boundaries of several types.] obviously, the zeros correspond to unit-cell angles of 90 . in s 6, the zeros mark the regions where components change from negative to positive, i.e. the place where cells become nonselling reduced. a second kind of boundary is where certain 'opposite' pairs of scalars are equal; this is more easily visualized in c 3 where those pairs are just the real and imaginary parts of one complex scalar. these are handled as 'reflections' (see sections 2.1 and 3.1)."
"ends of the scan, continuity and only occasional discontinuities in slope (due to boundary crossings). this figure compares results from four metrics: s 6, g 6, d 7 and v 7 . it can be observed that the v 7 metric as seen in fig. 4 is both fast to compute and smooth, and that leads one to ask whether v 7 should not be the favored metric. the issue seems to have not been described well in the literature. for crystallographic purposes, a smooth metric is not sufficient. we also need sensitivity to the differences among lattices, especially for clustering."
"we present the edge transforms as matrices, two for each scalar; the second line for each is the alternate choice of which pair to exchange [cit] ]."
"although s 6 and c 3 simply reorganize the same data, some operations are simpler to visualize in one space than the other. in some cases, we will choose to show only the simpler one."
"in this figure, m represents the application of all 24 reflection matrices. v represents the generation of six virtual cartesian points from an input point."
"the scalars in s 6 are of a single type, unlike cell parameters (lengths and angles) and unlike g 6 (squared lengths and dot products). [cit] state 'the selling parameters are geometrically fully homogeneous'."
"while ncdist has been effective for clustering, the original implementation is very demanding of computational resources. the development of cs6dist, a macro-based s 6 cell distance method, has improved cluster timing, both indirectly for ncdist by first reducing with s 6 before finishing with niggli reduction, and directly by computing s 6 distances in which only six boundaries need to be considered instead of g distance between points using the follower algorithm. to verify the distance algorithms, the 'follower' algorithm has been developed. follower chooses two points and determines the distance between one of them and all of the points on a line between the two original points. here, one unreduced point is chosen and the second point is the reduced point of that point. so the distance between the original point and the final point is zero. distances are shown for the g 6 metric [cit], the v 7 metric [cit], the d 7 metric [cit], and the two implementations in s 6 . timing in ms: g 6 (ncdist) 4542, d 7 676, v 7 7, s6dist 394, cs6dist 14."
"dendrograms of cluster candidates obtained using g 6 . for example, the commonly used ccp4 clustering program blend [cit] has been modified to use s 6 reduction and cs6dist distances and tested on a set of 71 lysozyme 5 wedges from a slightly doped crystal, comparing ncdist and cs6dist timing, on a 12-core, 24-thread amd ryzen threadripper system. the ncdist run took 28 s real time and 72 s user time. the cs6dist run took 25 s real time and 40 s user time. the results were identical. this example and more challenging examples of the application of s 6 in clustering will be discussed in more detail in a subsequent paper."
"4.1.2. using vcps to determine distance. to begin, the six vcps (one for each boundary) are computed for the first of the two input points. then the 24 reflections are computed for those six results plus the initial point itself. the desired distance is the minimum of the distances between the second point and all of the 168 points created in the first step. this is a one-boundary case. monte carlo experiments show that fewer than 1% of the minimal distances can be improved by twoboundary solutions and in most cases the difference is less than 10% (see fig. 2 )."
"the non-diagonal nature of reduction operations in this space means that measuring the distance between points in different regions of space is not as simple as finding the cartesian distance. the edge-transform matrices transform a point in the fundamental unit to another, non-reduced unit, one where one scalar is positive. (continued applications of the matrices will generate one or two more positive scalars.) because of the non-diagonal nature of the matrices, the metric direction will change between each unit. the simple euclidean distance from a point in the fundamental unit to one in another unit is not necessarily the minimal distance. a path broken by reflections and reduction transformations may be shorter. we present two alternative algorithms that do find a valid minimal distance. see sections 4.1 and 4.2."
"it is also important to note that the necessary examination of reflections in calculating a distance may undo any time savings achieved by identification of unique cells in an asymmetric unit, so it is usually better to work in the full fundamental unit, rather than restricting our attention to the asymmetric unit. in the current work, the full fundamental unit is always considered."
two-boundary solutions are created by first generating the same 168 points as in the-one boundary solution. then we generate the six vcps of the second input point and find the minimal distance between the 168 versus the first point and the seven points consisting of the six vcps and the first point.
". unfortunately for us, 90 angles are common in crystals, rendering v 7 an insensitive metric for important regions. three issues can be seen immediately. first, if the derivatives are approaching zero, least-squares in v 7 is likely to not perform well in some cases. second, in the case of database searches, false-positive reports will be common. [cit] explicitly describe the problem:"
"'algorithms are designed to ensure that no known unit cells are missed in the search. the output may sometimes present numerous candidates for a match, but this can be screened readily by the researcher and is not considered problematic since the search is done only once per new crystal studied'."
"the unsorted nature of selling reduction implies that distance calculations will need to consider the reflections. even if a usable sorting of points in the fundamental unit were created, at least some of the reflections would still be required for near-boundary cases."
"the input digitized sequence is generated in matlab. the complete parameter measurement approach using digital iq is implemented. the block diagram of parameter measurements is shown in fig. 10 . the incoming if signal is digitized using analog to digital converter. the digitized if signal x(nt s ) is multiplied by the numerically controlled oscillator (nco) outputs and its 90 0 phase shifted outputs. thus the incoming signal is converted into complex signal. the detailed block diagram of profiles generation is include phase, frequency and amplitude profiles. the amplitude profile is used for detection. the frequency profile is used to find out the frequency. the block diagram of phase, frequency and amplitude profiles generation is shown in fig. 11 ."
lgscore method only'' refers to is presented in fig. 7 . the values in the venn diagram are genes confirmed by the answer set to be among the top 20 inferred genes. the dark area in the venn diagram corresponds to ''genes inferred by the lgscore method only.''
"therefore, in order to achieve multisource operation, improve the motoring performance, and obtain flexible self-charging functions, this paper presents a multiport bidirectional srm drive for solar-assisted heb (sheb) applications. a bidirectional front-end circuit is designed to combine the asymmetrical half-bridge converter for integrated functions. pv panels are employed as an assisted sustainable energy source to extend the driving range and reduce the reliance on charging stations. multiple driving and charging modes are achieved. excitation and demagnetization processes are accelerated in both generator control unit (gcu) driving mode and pure battery driving mode due to the multilevel voltage, which increase the torque capability. the battery can be directly charged by utilizing the traction motor windings and converter circuit, achieving a more compact and integrated converter topology. microcurrent charging is achieved by demagnetization currents in gcu or pv driving conditions. during the braking process, the energy can be flexibly recycled to the battery. in sheb standstill conditions, the battery can be quickly charged by the pv panels and gcu/ac grids, which reduces the reliance on charging stations."
"when the solar irradiance is sufficient, the pv is put into use as an assisted sustainable energy source to contribute to the motor driving, which reduces the reliance on fuels/batteries. in this case, the relays j 1 and j 2 are both turned on to connect the pv and gcu/battery to the main converter."
"in this study, we used co-occurrence-based text-mining to construct a gene network for diseases. after constructing the gene network, we used the abc model to infer various disease-related genes. in our approach, the a-b relationship indicates the disease-gene interaction, and the b-c relationship indicates the gene-gene interactions. the disease-gene interactions are extracted from the text-mining results, and the gene-gene interactions are obtained from google data. these two values are used as weights for the edges to analyze the network."
"the waveforms of battery charging by pv and ac grids in motor standstill conditions are shown in fig. 26, where p z is the encoder signal, and s 3 and s 4 are the driving signals for phase b. the battery can be directly charged by utilizing the traction motor windings and converter drive, without any additional converters and inductors. the pwm signals are applied to the corresponding switches to control the battery charging current. when the battery is charged by the pv, one phase converter is employed due to the low power level. fig. 26(a) and (b) show the battery charging by the pv using phase b leg and winding, where the switching frequency is set to 500 hz and the duty cycle is set to 0.5 and 0.7, respectively. switches s 3 and s 4 are both turned on first to energize the phase b winding, and then turned off simultaneously to make the phase current feed back to the battery bank through diodes d 3 and d 4 . in this condition, battery charging by the pv is achieved, and the charging current can be flexibly controlled by changing the pwm frequency and duty cycle. when the battery is charged by ac grids, three-phase windings are all employed due to the high power level, as shown in fig. 26(c) and (d) . similarly, three-phase windings work in excitation mode by turning on switches s 1 -s 6, and then work in demagnetization mode by turning off all the switches, to achieve three-phase charging. the encoder signal keeps at zero, and thus the charging scheme does not lead to motor movement. fig. 26(c) shows the charging condition without csc scheme, where the three phase currents are unbalanced due to the different phase inductances. therefore, to reduce the impact of the unbalanced inductance, the proposed csc is adopted to balance the phase currents, as shown in fig. 26(d) ."
"lgscore consisted of two values which include the frequency and the google search result. the frequency was obtained from literature data, fig. 2 . outline of the proposed method. in the gene network, all nodes indicate genes inferred by our method. the square nodes indicate disease-related genes from among the inferred genes, and circle nodes indicate genes which are not confirmed as disease-related genes from among the inferred genes. and it is used to infer similarity with a disease. if a gene appears numerous times with other genes in the literature related to a specific disease, then the gene is considered to be a disease-related gene for that disease. the google search result value obtained from google was used to represent the degree of similarity in the relationships between candidate disease genes. if a gene has a high google search result value in relation to other candidate disease genes, the gene is considered to be closely related to the candidate disease genes. in this case, we extract the gene as a disease-related gene. in this process, we assumed that all of genes in the gene network are candidate disease-related genes because they appeared in the disease-related literature. we used the google search results to represent the weight of the gene-gene interactions to extract various disease-related genes which cannot be found by methods which use only the frequency. for instance, certain genes are cited in fewer papers, even when they are meaningfully linked to a disease, as they were recently confirmed to be disease-related genes. in this case, the frequency-based method cannot extract these genes, as the measure considers only the number of papers which contains the gene. hence, our approach uses one more measure, i.e., the google search result, to consider relationships with other candidate disease genes. consequently, our approach considered indirect relationships between candidate disease genes as well as direct relationship with disease. fig. 5 shows a flow chart of the process used to calculate the lgscore. in the literature, we extracted gene pairs using text-mining based on co-occurrences. the frequency value between the genes pairs is calculated by the number of sentences which contain two genes in the literature, and the google search result value is obtained from the google search. the two values are normalized using the z-score measure. after scaling, the frequency and google search result are used to calculate the lgscore."
"fig . 5 shows the simplified system diagram of the proposed srm drive for motoring and charging modes, respectively. the gcu is mechanically coupled with an ice to provide threephase ac power source for rectification. in motoring mode, the battery and gcu can work independently or cooperate by controlling the relay j and switch s f 2 in the bidirectional circuit. the pv is selected for utilization by controlling the relay j 2 according to the solar irradiance. thus, six driving modes are achieved, including the gcu driving mode, battery driving mode, gcu and battery driving mode, pv and gcu driving mode, pv and battery driving mode, and pv driving mode, as shown in fig. 5(a) . in the battery charging mode, by controlling the relays j and j 2, three charging modes are achieved, including the pv charging mode, the gcu charging mode, and the ac grids charging mode, as shown in fig. 5(b) ."
"in order to verify the effectiveness of the proposed multiport srm drive topology, the experiments are carried out on a threephase 12/8 srm prototype for proof-of-concept. the photograph of the experimental system is shown in fig. 20, including table ii . a dspace-ds1006 platform is employed to implement the control algorithm for the proposed system. in the motor test bed, a parker ac servomotor acts as the load, which is controlled by an integrated load controller inside the cabinet. a high-precision torque sensor is installed between the srm prototype and load motor to detect the instantaneous output torque. a three degree of freedom bracket is used to achieve the balanced connection. a 2500-line incremental encoder is installed on the motor frame to detect the rotor position and calculate the motor speed. a programmable dc power supply is utilized to simulate the gcu part, where the voltage is set to 80 v. a 60 v lithium-ion battery pack is employed to be the energy storage equipment. a pv array simulator agilent technology e4360a is employed as the pv source. hall-effect current sensors la-55p are used to detect the phase currents for current regulation control. a multichannel isolated oscilloscope is used to observe the experimental waveforms. a closed-loop controller with current hysteresis modulation is designed to control the phase currents for speed regulation. fig. 21 presents the experimental waveforms of the proposed motor drive operating at 280 r/min and 2 n·m load with only gcu or battery driving, where i a, i b, and i c are the phase currents for phase a, b, and c, respectively, u a is voltage of phase a winding, and i by is the battery current. in the gcu driving mode, the generator supplies the energy to the motor drive, and the battery bank is idle. the multilevel phase voltage is directly achieved by the battery bank in the converter circuit although there is no power supply from the battery, where the excitation voltage and demagnetization voltage are both boosted to accelerate the winding excitation and demagnetization processes in the phase commutation region. meanwhile, trickle current charging is achieved by the demagnetization current due to the inherent characteristics of the proposed drive, as shown in fig. 21(a) . similarly, in the pure battery driving mode, the generator is idle, and the battery acts as the power source to supply the energy to the motor drive. in this condition, the multilevel voltage can also be achieved by the capacitor c 1 in the circuit, where the operation modes are similar to that in fig. 21(a), and the excitation and demagnetization processes are both accelerated due to the increased phase voltage, as shown in fig. 21(b) . fig. 22(a) and (b) present the experimental results when the motor drive operates in the gcu driving mode at 1800 r/min and gcubattery driving mode at 2400 r/min, respectively. in high-speed operation, the phase voltage can also be boosted in the phase commutation region in the gcu driving mode, where the fast excitation and fast demagnetization are also both achieved, as shown in fig. 22(a) . therefore, the torque output is improved accordingly due to the multilevel voltage, no matter in low-or high-speed operations. the experimental waveforms of the proposed motor system driving by the hybrid power source with pv auxiliary at 400 r/min are shown in fig. 23 . in the pv-gcu driving mode, the battery bank is disconnected from the converter circuit, and the pv source is paralleled with the gcu part to supply the power to the motor drive. according to the equivalent power source circuit, the pv voltage is clamped to the gcu voltage in this condition. fig. 23(a) shows the experimental results in the pv-gcu driving mode. the pv and gcu work together to supply the energy to the motor drive, which reduces the reliance on fuels. in the pv-battery driving mode, the generator is disconnected from the converter circuit, and the pv source works with the battery to supply the power to the motor drive. fig. 23(b) shows the experimental results in the pv-battery driving mode. the pv is put into use as an assisted sustainable energy source to contribute to the motor driving, which reduces the reliance on batteries. fig. 24 presents the waveforms under pv independent driving condition. the pv source supplies the energy to the motor drive and the battery bank can be charged by the demagnetization current, which is similar to that in the gcu driving mode."
"after computing the value of standard deviation, the value of threshold can be decided. if the threshold is equal to one sigma, then 68.27% is the probability of detection. if the threshold is equal to two sigma, the probability of detection is 95.45%. if the threshold is equal to three sigma, the probability of detection is 99.73% [cit] . practically many people, choose threshold equal to two sigma. the probability density function with respect to standard deviation is shown in fig. 7 ."
"the abc model refers to a method with which to determine a relationship between ''a'' and ''c'' using the a-b relationship and the b-c relationship. the abc model can reveal novel relationships using two entities which are already known to be related. for instance, if a disease is related to a gene and the gene is linked to a drug, then a candidate relationship between a disease and a drug is inferred by the abc model. in this way, the abc model can infer indirect relationships using direct relationships which are known. the abc model also provides an opportunity to identify new knowledge without special skills. for this reason, the abc model is commonly used in bioinformatics. swanson showed that with the abc model, it is possible to use literature data to infer new relationships. swanson inferred a relationship between raynaud's disease and fish oil using the abc model. a number of text-mining methods using the abc model were subsequently introduced."
"given the low power density of the pv panels, they are insufficient to drive the motor under normal driving conditions. however, in downhill motoring condition, there is only limited energy required to power the motor. the energy generated from the pv source would be more than the srm needed. therefore, the pv source under this motoring condition could be sufficient to drive the motor. in this condition, relays j and j 1 are turned off, j 2 is turned on, and the switch s f 1 is turned on and s f 2 is off. the equivalent circuit is presented in fig. 12 . the winding excitation, winding demagnetization, and freewheeling states are shown in fig. 13 . the pv panels supply the energy independently to the motor in the winding excitation mode, and in the winding demagnetization mode, the battery charging is achieved by the demagnetization current."
"initially based on the pulse width it can be decided whether the signal is pulsed or it is continuous. if the pulse width is higher from some certain limit, it is said to be continuous otherwise it is pulsed and pulse width is measured. the pulse repetition interval of the signal is derived from the time of arrival of the pulse."
"the number of answer set the number of genes in the gene network as shown in fig. 8, our results show that lgscore has a higher percentage of confirmed genes than a random value. furthermore, the results indicate that our method successfully ranked disease genes using lgscore."
lgscore(a) is the summation of two values to consider direct and indirect relationships. the two values are the z-scored results of the frequency value and google search result. the google search result values were much larger than the frequencies before z-scoring. we used the z-score as a scaling factor to make the frequency and google search result values comparable. the frequency indicates the number of sentences in the biological literature that contained both genes. the formulae we used to calculate fre(a) and gsr(a) are shown below.
"this module estimates changes in signal power transmitted through the wireless channel among enbs, henbs, and ue by utilizing path loss, multi path fading, and shadowing models. we adopt indoor and outdoor path loss models defined in wireless world initiative new radio (winner) ii project [cit] to estimate path loss in macro-and femtocell, respectively, as follows:"
"the above figure shows the probability density function of the guassian distribution function. the noise is said to be, to centre the input signals in-phase and quadrature phase components about zero hertz, is performed digitally with the results of the in-phase replicated components, and the low pass filtered in-phase and quadrature-phase components are obtained. using this, the difficulties in signal path phase and amplitude matching are eliminated because only one a/d converter is used. in addition, the dc bias problems associated with analog signal mixing to zero hz are avoided [cit] . the block diagram shown in fig. 8 shows the conversion of input signal into complex signal alongwith the parameter measurement."
"on the other hand, in p-tpd, henb first discovers the closest macrocell enb and measures the reference signal received power (rsrp), r enb, from the enb to determine its transmission power. if we assume that a mue near the henb has similar rsrp to the henb, the received sinr for the mue, s, can be estimated as"
"the measurements carried out based on digital iq method have certain advantages. the components with similar functionality can be realized as it is. like the multiplier required for frequency translation, the exact nature of multiplier, filters can be realized without any error. two identical digital low pass filters also can be realized with similar response. this is not possible in analog world."
"where d is distance from the transmitter to the receiver. in addition, the simulator is able to estimate outdoor-to-indoor (indoor-to-outdoor) path loss for transmission from enb to fues (from henb to mues) utilizing equations (1) and (2). we also consider multi-path fading with an exponential distribution of mean 1, and shadowing with a normal distribution of zero mean and standard deviation 8 [cit] ."
"furthermore, text-mining can reveal novel relationships among biological entities. text-mining can provide opportunities to reduce the time and effort needed to extract relationships between biological entities from a large amount of publications. interest in text-mining is increasing due to the increasing number of electronic publications stored in databases such as pubmed [cit] . furthermore, swanson's abc model [cit] makes text-mining a feasible approach."
"where i k 0 is the initial phase current, i km is the maximum phase current, t is the switching period, and d is the duty cycle of the switching pwm, respectively. then, turning off the switches s 3 and s 4, the current in the phase winding feeds back to the battery bank b through diodes d 3 and d 4, as shown in fig. 14(b) . in this working state, battery charging by the pv panels is achieved, and the charging current can be expressed as follows:"
"the weakest signal that can be detected is known as minimum detectable signal. the incoming signal is compared with the threshold. if the incoming signal is greater than the threshold level, the single is detected. two types of threshold are used. one is fixed threshold, whereas other is noise riding threshold. the fixed threshold is decided in advance. whereas the noise riding threshold is computed based on the noise estimation before the pulse, i.e. during signal absence. the noise is estimated for finite duration in real time."
"packets in the transmit packet buffers in a base station, e.g., enb and henb, are scheduled into available prbs for wireless transmissions by using the packet scheduler module, as shown in figure 5 . well-known packet scheduler algorithms, i.e., round robin (rr), proportional fair (pf) [cit], and maximum carrier-to-interference ratio (max c/i) [cit], are included in the simulator to evaluate performance under different types of packet schedulers. we use pf scheduler for packet scheduling in both enb and henbs in the simulation results in this article."
"modules in the functional part can be classified into resource management modules and modeling modules. resource management modules are used to emulate functionalities related to wireless resources in the lte system. these resource management modules enable the simulator to evaluate capacity in macro-and femtocell networks. conversely, modeling modules help the simulator to consider realistic network environments by modeling wireless channel, traffic patterns, and user mobility."
"the threshold computed using noise estimation is applied for pulse detection. once pulse is detected, the parameters are measured. the pulse parameter constitutes the pdw as shown in fig. 9 . pdw consists of frequency, pw, pa and prf. the minimum, maximum and average value in stable region for frequency and amplitude is also measured."
"1) a simple multiport bidirectional srm drive is designed to combine the gcu, battery bank, pv panels, and srm for flexible driving and charging functions, by using fewer power devices. the charging functions can be directly achieved by the motor windings and converter circuit. 2) six driving modes are achieved. when the solar irradiance is not sufficient, the sheb can work in the gcu driving mode, pure battery driving mode, and gcu-battery driving mode, according to the running conditions. when the solar irradiance is sufficient, the pv can work with the battery/gcu together as an assisted sustainable energy source to contribute to the motor driving, which significantly reduces the reliance on fuels/batteries. in addition, in downhill motoring condition, the pv can provide the energy independently to drive the motor. 3) five charging modes are achieved without external charging converters. the battery bank can be slightly charged by the demagnetization current in the gcu driving mode and pv driving mode. during the braking progress, the energy can be recycled to the battery. furthermore, the battery bank can be quickly charged by the pv panels, gcu, or ac grids at sheb standstill condition, which reduces the reliance on charging stations. corresponding control strategies are presented to achieve the flexible energy control. 4) the dc voltage is boosted and multilevel voltage is achieved by the battery bank in the gcu driving mode, and by the charge capacitor in the pure battery driving mode. the excitation and demagnetization processes are both accelerated due to the boosted dc voltage. the torque capability can be increased by 35% due to the multiport topology without torque ripple increase. although the paper has targeted electrified vehicle applications, the developed technology can be applied to other high-torque and high-speed applications, such as more-electric aircraft, traction drives, and electrical ships."
"feature engineering is the art of extracting useful patterns from data to make it easier for machine learning models to perform their predictions. it can be considered one of the most important parts of the data mining process in order to achieve good results in prediction tasks [cit] . several papers in recent years have included indicators including the simple moving average (sma) for machine learning classification tasks [cit] . an example of an appropriate technical indicator is a sma recording the average price over the previous x days, and is correspondingly included."
"in this paper, we propose a novel approach to infer diseaserelated genes based on google data and literature data. we constructed a disease-related gene network by means of co-occurrence-based text-mining for specific disease-related studies in the literature. we then extracted the google search result value for the every gene pair which has an edge in the gene network from google. the google search result value is then used to re-enforce the gene network. the disease-related gene network has two weights between gene pairs which are linked. one of the weights is a frequency value which is obtained from the literature data, and the other weight is google search result value which is obtained from google data. after constructing a disease-related gene network, we calculated the lgscore using the two weights in the network. using the lgscore, we extracted disease-related genes from the disease-related gene network. our method has three steps. first, we obtain genes and gene-gene relationships from the literature on a certain disease. we then construct a disease-specific gene network based on text-mining results. in the next step, we supplement the gene-gene relationships in the gene network using google data. in the last step, we calculate the lgscores of the genes using the frequency and google search result values to identify disease-related genes based on the lgscore."
"the packet arrival event means the arrival of a new packet for a specific ue. since the higher layers beyond mac layer are considered as an abstraction, the packet arrival is expressed as a form containing attributes of a traffic type, i.e., voip, video streaming, ftp, and web service, and of traffic volume in bits. upon arrival of the event, the event handler commands the traffic generator module to generate an appropriate packet for the service type of the ue. then, the generated packet is inserted in the ue's transmit packet buffer located at the base station in which the ue is currently connected. after packet generation, the event handler generates a new packet arrival event, based on the next packet arrival time, considering traffic characteristics of the service."
"the independent variable for this study is the closing price of bitcoin in usd taken from the coindesk bitcoin price index. rather than focusing on one specific exchange, we take the average price from five major bitcoin exchanges: bitstamp, bitfinex, coinbase, okcoin and itbit. if we were to implement trades based on the signals it would be beneficial to focus on just one exchange. to assess the performance of models, we use the root mean squared error (rmse) of the closing price and further encode the predicted price into categorical variable reflecting: price up, down or no change. this latter step allows for additional performance metrics that would be useful to a trader in the formation of a trading strategy: classification accuracy, specificity, sensitivity and precision. the dependent variables for this paper come from the coindesk website, and blockchain.info. in addition to the closing price, the opening price, daily high and daily low are also included as well as blockchain data, i.e. the mining difficulty and hash rate. the features which have been engineered (considered as technical analysis indicators [cit] ) include two simple moving averages (sma) and a de-noised closing price."
"when the demagnetization current i a is smaller than the excitation current i b, phase a current cannot support the excitation of phase b, and the gcu makes up for the energy deficit, as shown in fig. 8(c) . in this condition, phase a is under the demagnetization voltage of the capacitor c 1, and the voltage and current of phase a in this stage are expressed as follows:"
"2) gcu/ac grids powered battery charging: when the solar irradiance is insufficient, the battery bank can also be charged by the gcu/ac grids, where the working states are similar to the condition of pv charging. the only difference is that threephase windings are all included due to the high power level. in this state, the relay j 1 is on and j 2 is off, and the switches s f 1 and s f 2 are both off. three-phase windings are first energized by the energy from the capacitor c 1 when s 1 -s 6 are all turned on, as shown in fig. 14(c) . then, turning off switches s 1 -s 6, the currents in the three-phase windings goes back to the battery bank b and capacitor c 1 through diodes d 1 − −d 6, as shown in fig. 14(d) . from fig. 14(c) and (d), the battery bank can be flexibly charged by the dc output from the rectification of the gcu/ac grids, through controlling the duty cycle and frequency of the pwm signals."
"in addition to the temporal window, some hyperparameters also need tuning: learning rate is the parameter that guides stochastic gradient descent (sgd), i.e. how the network learns. similarly, momentum updates the learning rate to avoid the model falling into local minima (in terms of error) and attempts to move towards the global minimum of the error function [cit] . we used the rmsprop optimiser to improve on sgd, as it keeps a running average of recent gradients and as a result is more robust against information loss [cit] . according to heaton [cit], one hidden layer is enough to approximate the vast majority of non-linear functions. two hidden layers were also explored and were chosen as they achieved lower validation error. heaton also recommends for the number of hidden nodes to select between the number of input and output nodes. in this case, less than 20 nodes per layer resulted in poor performance. 50 and 100 nodes were tested with good performance. however, too many nodes can increase the chances of overfitting, and significantly increase the time needed to train the network. as 20 nodes performed sufficiently well this was chosen for the final model. an activation function, a nonlinear stepwise equation that passes signals between layers, is also needed. the options explored were tanh, relu, and sigmoid. tanh performed the best but the differences were not significant. the final parameters for selection are batch size and number of training epochs. batch size was found to have little effect on accuracy but considerable effect on training time when using smaller batches in this case. the number of epochs tested ranged from 10 to 10000, however, too many training epochs can result in overfitting. to reduce the risk of overfitting, dropout was implemented as discussed above. optimal dropout between 0.1 and 1 was searched for both layers with .5 dropout the optimal solution for both layers. a keras callback method was also used to stop the training of the model if its performance on validation data did not improve after 5 epochs to prevent overfitting. generally, the rnn converged between 20 and 40 epochs with early stopping."
"in lte system, wireless resources in both time and frequency domains can be commonly or separately allocated to macro-and femtocells. if macro-and femtocells are allocated to different wireless resources, there exists no signal interference between the macro-and femtocells. however, in this case, utilization of the wireless resources may low, since the wireless resources should be segmented into small pieces. therefore, many researchers consider reuse of wireless resources in macro-and femtocells in order to improve utilization of the expansive wireless resources [cit] ."
"when the solar irradiance is sufficient, the battery bank can be charged by the pv source. due to the low power level of pv panels, only one phase converter is employed in the pv powered charging state. in this condition, the relay j 1 is off and j 2 is on, and the switches s f 1 is on and s f 2 is off. the phase windings are employed as inductors, and the driving topology functions as a buck-boost charging converter. the working modes are similar to the pv driving condition. pulse width modulation (pwm) control signals can be applied to the power switches to achieve battery charging. taking phase b for example, the pv source supplies the energy to the phase b winding, when s 3 and s 4 are both turned on simultaneously, as shown in fig. 14(a) . in this working state, the current flowing in the winding can be expressed as follows:"
"another form of rnn is the long short term memory (lstm) network. they differ from elman rnn in that in addition to having a memory, they can choose which data to remember and which data to forget based on the weight and importance of that feature. [cit] implemented a lstm for a time series prediction task finding that the lstm performed as well as the rnn for this task. this type of model is implemented here also. one limitation in training both the rnn and lstm is the significant computation required. for example, a network of 50 days is comparable to training 50 individual mlp models. [cit], the development of applications that take advantage of the extremely parallel capabilities of the gpu has grown greatly including the area of machine learning. [cit] reported over three times faster training and testing of its ann model when implemented on a gpu rather than a cpu. similarly, [cit] reported an increased speed in classification time to the magnitude of eighty times when implementing a svm on a gpu over an alternative svm algorithm run on a cpu. in addition, training time was nine times greater for the cpu implementation. [cit] also received speeds that were forty times faster for training a when training deep neural networks for image recognition on a gpu as opposed to a cpu. due to the apparent benefits of utilising a gpu, our rnn and lstm models are implemented on both the cpu and gpu."
each of the above parameters is having their significance. the instantaneous phase tells us phase variation within the signal presence. if at all it is there it reflects in the frequency profile. the frequency is computed based on the phase differentiation. the instantaneous amplitude tells us the radar is scanning or it is fixed apart from the distance of it.
"from the received signal power obtained through the models, the channel modeling module calculates received signal to interference plus noise ratio (sinr) for each wireless link. in order to consider the amc function in lte system, the simulator includes discrete spectral efficiency lookup table indexed by sinr [cit] . the lookup table defines 35 levels of frequency efficiencies (bps/hz) according to the various modulation schemes, i.e., qpsk, 16 qam, and 64 qam, and the coding rates under the considerations of 1:2 mimo, channel prediction, and harq. based on the frequency efficiency determined from the lookup table for each wireless link, the simulator calculates the number of data bits which can be contained in a prb for the link by multiplying the frequency efficiency with bandwidth (180 khz) and time duration (0.5 ms) of the prb."
"in order to investigate the dynamic performance of the proposed motor drive topology, fig. 25 illustrates the transient progress during the speed change, load change, and braking conditions in a closed-loop system. in fig. 25(a), the motor drive works in pure battery driving mode at 300 and 850 r/min, and in gcu driving mode at 1300 r/min in steady-state operations, while works in battery-gcu driving mode during acceleration process. the battery and generator can work together to improve the acceleration performance of the motor. when the speed reference increases from 300 to 850 r/min and from 850 to 1300 r/min, the actual speed quickly tracks the speed reference. during acceleration, the speed is stabilized within 1 s and follows the given value well, which ensures a fast response to speed changes. as shown in fig. 25(b), when the load changes from 2 to 4 n·m, the speed is quickly stabilized and tracks the given value. therefore, the proposed motor drive presents a rapid dynamic response to speed and load changes, confirming a good robustness. fig. 25(c) and (d) show the experimental results of the braking performance for the proposed drive. the braking response time can be flexibly controlled by adjusting the turn-on and turn-off angles. in fig. 25(c), the braking time is near 2 s when the turn-on angle and turn-off angles are set to 20°and 30°, respectively. in fig. 25(d), the turn-off angle is increased to 40°and the braking time is only 0.7 s, ensuring a flexible braking ability. moreover, during the braking process, the energy can be recycled to the battery bank directly through the proposed drive topology, as shown in fig. 25(c) and (d) ."
"appropriate design of deep learning models in terms of network parameters is imperative to their success. the three main options available when choosing how to select parameters for deep learning models are random search, grid search and heuristic search methods such as genetic algorithms. manual grid search and bayesian optimisation were utilised in this study. grid search, implemented for the elman rnn, is the process of selecting two hyperparamaters with a minimum and maximum for each. one then searches that feature space looking for the best performing parameters. this approach was taken for parameters which were unsuitable for bayesian optimisation. this model was built using keras in the python programming language [cit] . similar to the rnn, bayesian optimisation was chosen for selecting ltsm parameters where possible. this is a heuristic search method which works by assuming the function was sampled from a gaussian process and maintains a posterior distribution for this function as the results of different hyperparameter selections are observed. one can then optimise the expected improvement over the best result to pick hyperparameters for the next experiment [cit] . the performance of both the rnn and lstm network are evaluated on validation data with measures to prevent overfitting. dropout is implemented in both layers, and we automatically stop model training if its validation loss hasn't improved in 5 epochs."
"in terms of the dataset, based on an analysis of the weights of the model the difficulty and hash rate variables could be considered for pruning. deep learning models require a significant amount to data to learn effectively. if the granularity of data was changed to per minute this would provide 512,640 data points in a year. data of this nature is not available for the past but is currently being gathered from coindesk on a daily basis for future use. finally, parallelisation of algorithms is not limited to gpu devices. field programmable gate arrays (fpga) are an interesting alternative to gpu devices in terms of parallelisation and machine learning models have been shown to perform better on fpga than on a gpu [cit] ."
"lgscore and the frequency-based model found the same number of disease-related genes for alzheimer's disease, diabetes, and colon cancer. for lung cancer and prostate cancer, lgscore found more disease-related genes than the frequency-based model. thus, lgscore was able to identify the same or a higher percentage of confirmed genes than the frequency-based model for the five diseases. furthermore, lgscore was able to identify genes not identified by the frequency-based model. for alzheimer's disease and diabetes, both the lgscore method and the frequency-based model returned a high percentage of confirmed genes. for colon cancer and lung cancer, the percentage of confirmed genes was low for both approaches because the size of the answer sets was relatively small. for diabetes, 19 of the top 20 inferred genes were confirmed to be related to disease, and nine of these 19 genes were inferred by lgscore only. for all diseases, the lgscore method inferred a set of genes that the frequencybased model could not identify."
"the torque direction is forward when a current is applied to a phase winding in the phase inductance ascending region. otherwise, a backward torque is produced in the phase inductance descending region."
"the diagram of the proposed srm-based sheb powertrain is schematically shown in fig. 3 . it consists of an energy storage unit, i.e., a battery bank, a power control unit including the multiport converter and controller, a gcu, an ac/dc converter for rectification, pv panels, and an srm using as the traction motor. in this configuration, pv panels are introduced on the top of the bus to provide a sustainable power supply, which not only overcome the drawbacks of short driving range in pure battery working mode due to the limitation of current battery technologies, but also achieve flexible self-charging functions to reduce the reliance on charging stations. fig. 4 presents the proposed multiport bidirectional srm drive topology for sheb applications, which is composed of a bidirectional front-end circuit, a front-end pv-fed circuit, and a conventional asymmetrical half-bridge converter. the front-end circuits include a generator (g), a battery bank (b), pv panels (pv), a rectifier (re), two capacitors (c 1 and c 2 ). moreover, two insulated-gate bipolar transistors s f 1 and s f 2 integrated with fast recovery antiparallel diode, two diodes d f 1 and d f 2, and three relays j, j 1, and j 2 are employed to achieve multiple functions."
"2) winding demagnetization modes: in the gcu driving mode, the battery is idle when the gcu supplies the energy to the motor, where it can be used to boost the dc-link voltage to accelerate the demagnetization and excitation progresses. the relationship between the current and voltage of phase a during the commutation region is shown in fig. 7(a), where a multilevel voltage is achieved."
"the proposed ew receiver is shown in the fig. 2 . the next generation ew receiver is the superhet receiver. the superhet receiver downconverts the rf signal into the fixed if signal of the required bandwidth. the bandwidth is varied depending upon the requirements. the superhet receiver has the advantage of sensitivity due to its limited bandwidth. the proposed method of intrapulse analysis works alongwith the superhet receiver. the if generated from the superhet receiver is digitized. the frequency translation is carried out on these digitized if samples and filtered out the highest component. this method measures the rise time and fall time and it also carries out measurement of parameter variations within the pulse. the method presented in this paper has the advantage of both real time and intrapulse parameter measurements. the proposed method is implemented in matlab. the measurements are based on the in-phase (i) and quadrature-phase (q) processing which gives the intrapulse parameter measurements for ew applications. it measures the instantaneous parameter like phase, frequency and amplitude for intrapulse analysis whereas pulse width and pulse repetition interval for inter pulse analysis. the various radar signal waveforms can be handled by the proposed method."
"for a conventional srm drive, multilevel voltage cannot be achieved due to a constant dc-link voltage, where the output torque is limited in high-speed operations. also, multisource operation and flexible charging functions are not equipped because only one source is connected to the converter circuit. this paper mainly focuses on developing a bidirectional multiport converter topology for sheb applications. compared to the conventional srm drive, the presented topology has several advantages, including multisource operation, improved torque performance, and flexible self-charging functions. pv panels are employed to extend the driving range and reduce the reliance on charging stations. the torque is improved due to the multisource operation. the self-charging functions are directly achieved by using the motor windings and converter circuit. multiple driving and charging modes are achieved by the proposed srm drive, which will be analyzed in detail in the next section."
"in fig. 4, the two genes searched for were estrogen receptor 1 and the epidermal growth factor receptor. the circle in the figure indicates the google search result value."
"the scheduling event indicates execution of the downlink packet scheduler in a specific base station (enb or henb). currently, uplink packet scheduling is not included in the simulator. the event handler makes the downlink packet scheduler to perform scheduling for packets waiting in the transmit packet buffer, whenever scheduling event occurs. the scheduler first calls the channel module to obtain information on the modulation and coding scheme (mcs) and achievable frequency efficiency (bps/hz) for each prb. based on the information, the scheduler schedules appropriate time and frequency resources (prbs) for packet transmissions to its associated ues. the event handler generates a scheduling event for the next scheduling time interval and places the event in the event queue after packet scheduling is completed. at this point, by adjusting execution time of the new scheduling event, the simulator can support various scheduling intervals, e.g., multiple of 1 ms."
"the proposed converter topology has integrated with on-board self-charging functions, which significantly reduces the reliance on charging stations. in sheb standstill condition, the battery bank can be directly charged through the multiport srm drive, without external charging converters. the self-charging can be achieved by the pv or gcu, and the external charging can be achieved by the ac grids."
we obtained google search result values by entering two genes at a time in the google search box. google search results indicate the number of documents that have the search term -in our case the names of the two genes. we used the google search results to enhance the weights of edges in the gene network. we did not use gene symbol names but the full names of the genes in the search box to obtain accurate results. an example of a google search result is presented in fig. 4 .
the conventional methods can't preserve the phase information because these methods use the video signal which is a detector output for processing. hence these methods can't do the intrapulse measurements.
"lgscore is a method that identifies disease-related genes using the literature and google search results to increase the accuracy of extracted relationships. we applied our method to five diseases (alzheimer's disease, diabetes, colon cancer, lung cancer, and prostate cancer) and demonstrated that lgscore extracted a higher percentage of genes known to be related to diseases than three other, comparable methods."
"the various instantaneous parameters [cit] have been measured by the proposed method. the received signal parameters measured includes phase, frequency, amplitude, pulsewidth, signal type and time of arrival or pulse repetition interval of the signal pulses. the low pass sampling is only considered in this case. even though little higher sampling rate is required for iq method. the quadrature sampling also can be carried out [cit] . the quadrature sampling advantage is that it needs two adcs with sampling rate of twice the signal bandwidth. but it introduces analog mixer error which doesn't occur in digital iq method."
"a conventional three-phase 12/8 srm drive is shown in fig. 1, including the srm, power converter, drive circuit, position detection, and current detection. the phase currents are detected by current sensors for current control to generate drive signals for the power converter. an asymmetrical halfbridge converter is usually adopted in srm drives, due to its phase isolation characteristic and excellent fault-tolerant ability. each bridge arm is controlled independently by two switches."
"in scenario 3, we evaluate performance of an mue as varying the number of henbs which are located in the vicinity of the mue. for the purpose, we deploy femtocells and set trajectory of an mue as shown in figure 12 ."
"lgscore uses the frequency value obtained from the literature and the google search result value which is obtained from google. we use the frequency value to denote similarity to the disease and google search result value to indicate similarity to other candidate disease genes to consider special cases. a special case arises when diseaserelated genes cannot be extracted y using frequency weights because they are cited in fewer studies. lgscore indicates the proposed method, and google indicates the lgscore, while the square nodes indicate confirmed genes validated by the pgdb answer set. the hexagon nodes denote confirmed genes validated based on the literature, while the diamond nodes indicate candidate genes which may be related to prostate cancer. none 10 tmprss2 pgdb 11 brca1 pgdb 12 tnf literature 13 tp53 pgdb 14 igfbp3 pgdb 15 apc pgdb 16 cyp1a1 literature 17 brca2 pgdb 18 gstt1 pgdb 19 cyp3a4 pgdb 20 gstm1 pgdb method which uses only the google search result value as the weight of the edges. likewise, frequency denotes the method which uses only the frequency value as the weight of the edges. as shown in fig. 14, lgscore showed better performance than other methods for three diseases. in contrast, google showed poorer performance than the other methods for three diseases. the google search results, however, are useful information with which to supplement the frequency weight in lgscore. for this reason, lgscore has a higher value for the number of genes confirmed by an answer set as compared to the frequency. furthermore, the top 20 inferred genes by lgscore included several genes which differ from the genes inferred by the frequency. these results show that google search results contain useful knowledge for identifying relationships between genes, playing an important role in identifying disease-related genes as indirect information."
"in terms of temporal length, the lstm is considerably better at learning long term dependencies. as a result, picking a long window was less detrimental for the lstm. this process followed a similar process to the rnn in which autocorrelation lag was used as a guideline. the lstm performed poorly on smaller window sizes. its most effective length found was 100 days, and two hidden lstm layers were chosen. for a time series task two layers is enough to find nonlinear relationships among the data. 20 hidden nodes were also chosen for both layers as per the rnn model. the hyperas library 2 was used to implement the bayesian optimisation of the network parameters. the optimiser searched for the optimal model in terms of how much dropout per layer and which optimizer to use. rmsprop again performed the best for this task. in the lstm model, activation functions weren't changed as the lstm has a particular sequence of tanh and sigmoid activation functions for the different gates within the cell."
"a confusion matrix representing the ratio of true/false and positive/negative classifications is used to derive the ratings metrics. accuracy can be defined as the total number of correctly classified predictions (price up, down, and no change). to combat inherent class imbalance (bitcoin price predominately increases) the metrics sensitivity, specificity and precision are also analysed. sensitivity represents how good a model is at detecting positives. specificity represent how good the model is at avoiding false alarms. finally, precision represents how many positively classified predictions were relevant. root mean square error (rmse) is used to evaluate and compare the regression accuracy. to instrument the evaluation of models, a 80/20 holdout validation strategy is used."
"since the human genome was sequenced, a large number of gene-based studies have been performed, and vast amounts of gene data have been generated. these data are stored in databases such as the online mendelian inheritance in man (omim) database [cit] . extracting hidden information from these databases offers new research opportunities and challenges. one of the best known tools with which to extract knowledge is text-mining."
"when wireless resources are commonly utilized by macro-and femtocells, signal interference can occur between macro-and femtocells and between different femtocells. figure 3 shows possible cases of cross-tier interference between macro-and femtocells. in downlink communications, henb interferes with signals transmitted from enb to macrocell ues (mues) ((1) in figure 3a ) while enb interferes with those from henbs to femtocell ues (fues) ((2) in figure 3a) . conversely, in uplink communications, mues and fues, respectively, interfere with signals transmitted from fues to henbs and from mues to enb as (3) and (4) in figure 3a . figure 3b shows possible cases of co-tier interference between different femtocells. in downlink communications, henbs give interference to signals transmitted from henbs to fues in other femtocells as (5) and (5') in figure 3b . on the other hand, in uplink communications, fues interfere with signals transmitted from fues to henbs in different femtocells as (6) and (6') in figure 3b . simulator design and development in the developed system-level lte simulator, we try our best to emulate various functions in the 3gpp lte (-advanced) standard. for this purpose, we include packet scheduling and call admission control (cac) functionalities based on [cit] . in addition, we model wireless channels with multi-path and shadowing effects, four kinds of real time and non-real time ip traffics, and mobility of ues, in order to evaluate performance of femtocells under realistic environments. moreover, the simulator adopts a gui that helps operators of the simulator to set and update simulation parameters easily, to apply arbitrary scenarios using a mouse or trackball, and to observe simulation status and results in real time."
"the performance benefits gained from the parallelisation of machine learning algorithms on a gpu are evident with a 70.7% performance improvement for training the lstm model. looking at the task from purely a classification perspective it may be possible to achieve better results. one limitation of the research is that the model has not been implemented in a practical or real time setting for predicting into the future as opposed to learning what has already happened. in addition, the ability to predict using streaming data should improve the model. sliding window validation is an approach not implemented here but this may be explored as future work. one problem that will arise is that the data is inherently shrouded in noise."
"when the solar irradiance is sufficient, the pv can be put into use as a sustainable energy source for multiple functions, as shown in fig. 15(b) . in both low-and high-speed driving operations, the use of pv reduces the reliance on fuels/batteries for extended driving range, as shown in fig. 16(d) and (e). in downhill condition, the pv panels can also supply the energy independently for motoring operation, as shown in fig. 16(f) . furthermore, the battery self-charging is achieved by the solar energy in sheb standstill condition, as shown in fig. 16(i) ."
"upon execution of a service arrival event in section 'service arrival event', the traffic generator module generates certain type of traffic as a form of packet arrival event introduced in section 'packet arrival event'. since the proposed simulator mainly focuses on the mac layer functionalities, the higher layers beyond the mac layer are considered as an abstraction."
"conventional ew systems are good for the basic radar parameter measurements like frequency, pulsewidth (pw), pulse repetition frequency (prf), pulse amplitude (pa) and direction of arrival (doa) of incoming signal. the measurement of doa technique is not discussed in this paper. in addition to above parameters it also measures the type of emitter based on the pulse width and scan type using pulse amplitude [cit] . the conventional ew systems are not capable to measure the variations of parameters such as phase, frequency and amplitude within the pulse. the block diagram of conventional ew receiver is shown in fig. 1 . the frequency is measured based on the instantaneous frequency measurement (ifm) receiver and other parameters are measured based on the detector output. initially the measurement of radar pulse parameters being carried out using the envelope of the down converted if or rf signal. this envelope is nothing but it is video signal. in the process of envelope detection, the phase information of the signal is lost. therefore these methods are not useful where phase information is to be preserved."
"the bode diagram of three-phase charging is plotted in fig. 19(a), where the amplitude margin and phase margin are negative. from the transfer function and bode diagram, it can be seen that the system is a nonminimum phase system. in order to improve the system stability, the pi compensation is adopted, and the corresponding transfer function is as follows:"
"we removed unnecessary data, such as the author, institute, date, and journal name from the abstract data. we categorized sentences according to parts-of-speech tagging using pos tagger [cit] . fig. 3 shows how a sentence is analyzed using a pos tagger. rectangles in the figure indicate nouns."
"where u g is the capacitor voltage from the rectification of the gcu output, u by is the battery voltage, and i dc is the lower dc-link current. when phase b is turned on and the demagnetization current i a is larger than the excitation current i b, the phase a acts as a current source to power phase b. in this condition, phase a is still under the demagnetization voltage of the battery bank b and capacitor c 1, and phase a current not only feeds back to the battery and capacitor, but also flows to phase b to help phase b set up the excitation current, as shown in fig. 8(b) . the voltage and current of phase a during this stage are given by the following:"
"2) charging control strategy: when the sheb is in standstill condition, the battery bank can be charged by pv panels, ice powered gcu or ac grids. different control strategies are employed, according to the power level of the charging source, as shown in fig. 18 . only one phase converter is employed in pv charging state, due to the low power level of pv panels. the corresponding control strategy is shown in fig. 18(a), where the mppt stands for the maximum power point tracking. when the battery is charged by the gcu or ac grids, the power level is higher than the pv panels. for this scenario, the three-phase converter will work together to achieve high power charging. due to the double saliency structure, the inductance of each phase is different from each other at a rotor position. in order to mitigate the influence of the unbalanced inductance, a current sharing control (csc) method is proposed, and the corresponding control strategy is presented in fig. 18(b) ."
"first, we mined the abstracts of publications in pubmed related to prostate cancer, colon cancer, lung cancer, diabetes, and alzheimer's disease. pubmed provides biological literature data in an abstract format. in the pubmed database, abstract data is generated by search results for an input keyword. to obtain disease-specific abstract data, we used disease names as search keywords in pubmed. we obtained abstract data for each disease from pubmed using five disease names as search keywords. alzheimer's disease and diabetes have been widely studied, as have cancers. in particular, a large amount of abstract data is available for prostate cancer and lung cancer, while less data is available for colon cancer. after preprocessing the abstract data, we connected genes that appeared in the same sentence to construct disease-related gene networks. next, we rebuilt the gene networks using google search results. afterward, we analyzed the gene networks and then extracted disease-related genes. our method processes were applied for each disease."
"predicting the price of bitcoin can be considered analogous to other financial time series prediction tasks such as forex and stock prediction. several bodies of research have implemented the multilayer perceptron (mlp) for stock price prediction [cit] . however, the mlp only analyses one observation at a time [cit] . in contrast, the output from each layer in a recurrent neural network (rnn) is stored in a context layer to be looped back in with the output from the next layer. in this sense, the network gains a memory of sorts as opposed to the mlp. the length of the network is known as the temporal window length. [cit] notes that the temporal relationship of the series is explicitly modelled by the internal states contributing significantly to model effectiveness. [cit] successfully took this approach in predicting stock returns combining a rnn with a genetic algorithm for network optimization."
"in order to effectively mitigate the performance degradation, both cross-tier interference between macrocells and femtocells and co-tier interference between different femtocells should be analyzed. thus, we utilize the simulator developed in this article to analyze performance of macro-and femtocell networks. for the analysis, we define four scenarios for different femtocell deployment cases to analyze effects of the distance between femtocell and mue, the distance between femtocell and macrocell, the number of femtocells, and the distribution of femtocells. since base stations including enb and henb are major interferers due to their high transmission power, we focus on the downlink performance in the following analysis."
"for research and performance evaluation of cellular networks with femtocells, system-level simulation is one of the most useful methodologies [cit] . in this article, we develop a system-level simulator for lte (-advanced) networks with femtocell using event-driven programming. the simulator includes resource management functionalities, such as packet scheduler and call admission control (cac) schemes based on the 3gpp release 10 standard, and models of wireless channel effects, various real-and non-real time traffic patterns, and user mobility. in addition, the simulator is equipped with a graphical user interface (gui) to facilitate configuration, operations, and observations of the simulations."
"the intrapulse measurements are carried out using digital iq method. the intrapulse features includes the type of modulation present in the pulse. since this method preserves the phase, the any variations in the instantaneous phase can be detected. the detected phase variation tells whether the phase is linear or varying within the pulse. the variation in the instantaneous phase profile reflects variation in amplitude profile and frequency profile as abrupt changes."
"here, fre(a) denotes the score calculated based on the frequency values, while gsr(a) denotes the score calculated from the google search result. zscore(x) denotes the z-scoring value of the number x."
"in the pv and gcu driving mode, switches s f 1 and s f 2 are both turned off, where s f 1 acts as a diode that is cutoff due to the lower voltage compared to the c 1 voltage. thus, the battery is disconnected from the circuit. in this condition, the pv is paralleled with the gcu, and the equivalent power source circuit is shown in fig. 9(a) . the pv voltage is clamped to the gcu voltage u g, and the working point of the pv is presented in fig. 9(c) . the pv and gcu work together to supply the energy to the motor drive. each phase encounters three working states including the winding excitation, winding demagnetization, and freewheeling states, as shown in fig. 10 . in the pv and battery driving mode, the gcu is disconnected from the circuit by turning off the relay j, and s f 1 is turned on. the pv is paralleled with the battery bank, and the equivalent power source circuit is shown in fig. 9(b) . the pv voltage is clamped to the battery voltage u by, and the working point of the pv is presented in fig. 9(d) . the pv and the battery work together to supply the energy to the motor drive, and the working modes for one phase are illustrated in fig. 11, where the only difference from the pv-gcu driving mode are that the gcu is changed to the battery bank."
"lstm models converged between 50 and 100 epochs with early stopping. similar to the rnn, batch size was found to have a greater effect on execution time than accuracy. this may be due to the relatively small size of the dataset."
"as shown in fig. 3, identified parts of speech are separated using the '_' character. in our experiment, we selected nouns. noun symbols consisted of nn, nnp, nnps, and nns. we compared extracted words with human gene symbol lists to identify gene names in the sentences. the human gene symbol list was obtained from the hugo gene nomenclature committee (hgnc) [cit] . nodes and edges of the gene network were constructed based on co-occurrences. we linked genes that appeared in the same sentence, and assigned weights to each edge between two genes using the frequency. the frequency of an edge between two genes indicates the number of sentences that refer to both genes."
"we intensively analyze effects of signal interference between macro-and femtocells for downlink communications utilizing the developed simulator. for the purpose, we define four scenarios that should be considered in femtocell deployment and evaluate capacities for both macro-and femtocells under the scenarios based on the simulator."
"call admission control (cac), which is one of the radio resource management functions, plays an important role, since it can prevent network congestion and guarantee a specified level of qos for on-going calls by accepting or rejecting service requests. both enb and henb can perform the cac function considering several conditions, such as channel status, qos requirements for requested services, buffer state, and so on [cit] . in the simulator, we include two cac algorithms, one accepts a new call based on the estimation of wireless resources [cit], and the other decides whether to accept or reject the new call considering the maximum allowed packet delays [cit] ."
"the score of each node is proportional to the number of neighboring nodes and the weight of the edges with the neighboring nodes. this equation is similar to the degree centrality measure in terms of how it uses the edges of a node to calculate the score of the node. however, our scoring function uses the frequency and google search result for the weight of the edges to consider useful knowledge with neighboring nodes."
"in order to effectively facilitate simulations, the simulator adopts the gui developed through the microsoft foundation class (mfc) libraries. by utilizing the gui, simulation operators can set simulation environments, deploy femtocells, control operation of simulations, and observe simulation results in real time."
"in this section, we briefly introduce wireless resources and femtocell networks in 3gpp long term evolution (lte) system, since we refer 3gpp release 9 (lte) and 10 (lte-advanced) specifications [cit] for development of the simulator."
"consideration of user mobility is very important for realistic simulations, as we have mentioned in section 'preliminaries', since movement of ues yields changes in overall capacity of wireless networks by changing link capacities for ues. thus, the simulator includes random walk and random way point mobility models for mue and fue, respectively, [cit] . in the random walk model, an ue first determines coordinates of the destination based on randomly selected direction, velocity, and flight time, and it starts to move to the destination. this procedure is repeated whenever the ue reaches the destination. in the random way point model, ue is initially located at the random coordinates of the randomly selected room. then it remains at the location for randomly distributed time durations. after the time duration, it repeats the same procedure. mobility management module is also responsible for cqi reports from moving ues to their corresponding (h)enbs. 3gpp lte defines two types of cqi reports according to their reporting interval, i.e., periodic and aperiodic. in addition, according to the type of information contained in the report, it can be classified into wideband and sub-band cqi reports. the wideband cqi report contains a measurement referring to the entire system bandwidth, while the sub-band cqi report includes measurements for various sub-bands. for periodic packet scheduling based on pf scheduler which considers frequency efficiency of each sub-band, the simulator basically adopts the periodic sub-band cqi report."
"in this paper, a multiport bidirectional srm drive is proposed for sheb applications, which not only improves the motor system performance in running conditions, but also achieves flexible charging functions. multiple driving and charging modes are achieved. multilevel voltage is obtained to increase the torque capability. pv panels are installed on the bus to extend the driving range and achieve self-charging ability. corresponding working modes and control strategies are investigated in detail. the main contributions of this paper are as follows."
terms used to describe our experimental results are defined in table 3 . ''genes by lgscore method only'' denotes disease-related genes contained in the top 20 genes as inferred by lgscore but not contained in the top 20 genes inferred by other methods (see table 4 ).
"average throughputs of macro-and femtocells according to the number of femtocells per macrocell are shown in figures 14a,b, respectively. since signal interferences from henbs are proportional to the number of henbs, average throughput of macrocell decreases as the number of henbs increases. however, macrocell throughput slowly decreases if henbs use low transmission power. increase in the number of henbs also affects the performance of henbs, since henbs share the same frequency bandwidth and they interfere with one another. however, throughput degradation in a femtocell is much less than that in a macrocell as shown in figure 14b, because relatively large number of walls may exist between femtocells than between femtocell and macrocell. henb with d-tpd reduces its transmission power according to the distance from enb, while henb with p-tpd determines its transmission power by considering path loss from the henb to an mue expected to be existed near the henb. due to this characteristic, henbs with d-tpd usually have lower transmission power than the henbs with p-tpd. thus, average macrocell throughput for d-tpd is larger than that for p-tpd, and vice versa for the average femtocell throughput."
"home evolved node b (henb), which is a femtocell base station, and expanded evolved umts terrestrial radio access network (e-utran) architecture are defined to support femtocells in the lte-advanced system [cit] . henb includes most functionalities of enb and is connected to cellular core networks through the existing internet access and henb gateway."
"the mue moves on a circle around the enb in order to maintain constant received power from the enb. in addition, five groups having different numbers of femtocells (from one to five) are deployed along the path. the minimum distances from each group to mue and to enb are set to 20 and 120 m, respectively. figure 12 femtocell deployments and moving path of mue in scenario 3 figure 13 shows received sinr and spectral efficiency according to the mue's movement. as shown in the figures, both received sinr and spectral efficiency decrease as the number of neighboring femtocells increases. however, we can find that the increase in the number of femtocells affects performance of mue less than the distance between henb and mue does, compared to the results in scenario 1. since the distance between enb is 120 m, henbs set their transmission powers to less than 20 dbm for both d-tpd and p-tpd. thus, for d-tpd and p-tpd, received sinr and spectral efficiency are better than those for fixed transmission power with 20 dbm. we evaluate performance of macro-and femtocells under sparse and dense femtocell distributions in scenario 4. for sparse and dense femtocell distributions, we consider two types of indoor environments which have one and four henbs in a building of one story, respectively, as shown in figure 6c . in addition, four fues share wireless resources for communications in each femtocell. in simulations for both sparse and dense femtocell distributions, femtocell distribution, while it includes four henbs in dense femtocell distribution, as shown in figure 6c . table 4 shows average throughputs of macro-and femtocell for sparse and dense femtocell distributions. throughput of a macrocell is defined as a sum of throughputs of all mues in the considered macrocell. if transmission powers of henbs are fixed, macrocell throughput decreases as transmission powers of henbs increase due to the interference from the henbs to the mues. in addition, average throughput of a macrocell for dense femtocell distribution is less than that for sparse femtocell distribution, since the total number of femtocells in the dense femtocell distribution is four times larger than that in the sparse femtocell distribution. in comparison with d-tpd and p-tpd, henbs in d-tpd generally have lower transmission power than henbs in p-tpd, since henbs in d-tpd linearly reduces their transmission power, regardless of path loss or channel environments. therefore, d-tpd has larger average macrocell throughput than p-tpd, because henbs with d-tpd less interfere with mues. for henbs having fixed transmission powers, femtocell throughput increases as the transmission power increases, since high transmission power improves received sinr and spectral efficiency for fues. in dense femtocell distribution, femtocells in a building may interfere with each other, because they share the same frequency bandwidth. thus, femtocell throughput in the dense femtocell distribution is lower than that in sparse femtocell distribution. in comparison with d-tpd and p-tpd, as we have explained before, henbs with d-tpd generally have lower transmission power than those with p-tpd. thus, p-tpd has larger femtocell throughput than d-tpd as shown in table 4 ."
"the demagnetization voltage is boosted to improve the torque by the battery bank in the gcu driving mode, where a trickle current charging is achieved; and by the capacitor c 1 in the battery driving mode. the gcu and battery bank can cooperate according to different working conditions, to improve the dynamic performance. the pv panels can be put into use as a sustainable energy source when the solar irradiance is sufficient, to reduce the reliance on fuels/batteries for increased driving range. furthermore, flexible battery charging can be achieved when the bus is in standstill condition, where the battery bank can be charged by the pv source when the solar irradiance is sufficient and by the gcu/ac grids when the solar irradiance is insufficient, without external charging converters, which will significantly reduce the reliance on charging stations."
"prediction of mature financial markets such as the stock market has been researched at length [cit] . bitcoin presents an interesting parallel to this as it is a time series prediction problem in a market still in its transient stage. traditional time series prediction methods such as holt-winters exponential smoothing models rely on linear assumptions and require data that can be broken down into trend, seasonal and noise to be effective [cit] . this type of methodology is more suitable for a task such as forecasting sales where seasonal effects are present. due to the lack of seasonality in the bitcoin market and its high volatility, these methods are not very effective for this task. given the complexity of the task, deep learning makes for an interesting technological solution based on its performance in similar areas. the recurrent neural network (rnn) and the long short term memory (lstm) are favoured over the traditional multilayer perceptron (mlp) due to the temporal nature of bitcoin data."
"the rest of the paper is divided into four sections. in section 2, we describe previous studies related to our current work. we described the proposed method in section 3, and present our results and a discussion based on them in sections 4 and 5, respectively. we conclude the paper by discussing the implications of our findings in section 6."
"in this section, we describe our experimental results and present comparisons of our method with comparable methods. we applied our approach to five diseases: alzheimer's disease, diabetes, prostate cancer, colon cancer, and lung cancer. after extracting the 20 genes with the highest lgscores for each disease, we compared the genes with the answer set to verify the feasibility of our method."
"in the battery driving mode, the gcu is idle when the battery supplies energy to the motor, where the capacitor c 1 is used to elevate the dc-link voltage to accelerate the demagnetization and excitation processes. the working modes are similar to the gcu driving mode, and the only difference is that the battery bank acts as a power source instead of the gcu, and the capacitor c 1 is recharged as an additional source for multilevel production during the commutation regions, as shown in fig. 7(b) ."
"as shown in fig. 6, lgscore is classified into four cases: ''high fre(a) + high gsr(a),'' ''high fre(a) + low gsr(a),'' ''low fre(a) + high gsr(a),'' and ''low fre(a) + low gsr(a).'' case 1 indicates that gene a is closely related to the disease and closely related to the candidate disease genes. in this case, gene a has the highest lgscore, and gene a is extracted as a disease-related gene. in case 2, the score of gene a is affected by the direct relationship with the disease as opposed to indirect relationships with the candidate disease genes. if gene a has a high frequency value, gene a is extracted by case 2. in contrast to this, case 3 indicates that the score of gene a is affected by indirect relationships with the candidate disease genes more than it is by direct relationship with the disease. in case 3, lgscore can extract disease-related genes which are cited in fewer papers. case 4 means that gene a is not significantly linked to the disease and candidate disease genes. when using the lgscore method, gsr is used to offset the weakness of a frequency value when used only with the number of cited papers. using these two weights, the lgscore can extract disease-related genes which are cited in fewer papers as well as disease-related genes which have high frequency values."
"lgscore showed better performance for four of the five diseases. for prostate cancer, the percentage of confirmed genes inferred using our approach was twice as high as the percentage inferred using the prince algorithm. for all diseases, the proposed lgscore method inferred a set of genes that the prince algorithm was not able to identify. these findings indicate that lgscore can be used to find disease-related genes not found using conventional methods."
"the service departure event terminates the service that generated this event. for the purpose, the event handler commands the traffic generator module to initiate variables related to the service. subsequently, the event handler deletes the packet arrival event for the service to stop packet generation."
"even though distance between mue and henb is fixed, received sinr of an mue may vary according to the distance between the enb and the henb, since received sinr is ratio of received signal to the sum of interferences and noises. in order to analyze the effects of the distance between enb and henb, in the second scenario, we consider femtocell deployments and the moving path of an mue in figure 10 . in the scenario, distance between enb and mue varies from 50 to 250 m at 50 m intervals, while the shortest distance from henb to mue is fixed to 10 m. table 3 shows average throughput of femtocell as varying the distance from enb to henb. the average throughput of a femtocell is defined as the average of the sums of user throughputs for individual fues. when transmission powers of henbs are fixed, received interference from enb to fue decreases, while the rsrp from henb to fue remains constant as the distance from enb to fue increases. thus, average throughput of femtocells increases as the distances between enb and henbs increase, since the received sinrs of fues increase as the enb recedes from the fues. average throughput of femtocell decreases as the transmission power from henb decreases, for similar reasons. in both d-tpd and p-tpd, when a henb is closely located with the enb, it uses high transmission power to guarantee certain level of received sinr to its fues. on the other hand, if the henb is located far from the enb, it reduces its transmission power to minimize interference to mues. from these reasons, average femtocell throughput decreases when distance from the enb to the henb is larger than 150 and 100 m for d-tpd and p-tpd, respectively."
"that the lstm takes longer to train than the rnn with the same network parameters, may be due to the increased number of activation functions, and thus an increased number of equations to be performed by the lstm. due to the increased computation, this raises the question of the value of using an lstm over an rnn. in financial market prediction small margins can make all the difference. as a result of this the use of an lstm is justified. in other areas the slight improvement in terms of performance isn't justifiable for the increase in computation."
"in their experiments, we confirmed that the abc model is a useful method with which to infer more meaningful relationships. for this reason, we propose a method to identify meaningful diseasegene relationships using the abc model enhanced by google data."
"a network can be used to present complex relationships between biological entities. in particular, a network is widely used to indicate gene-gene interactions such as activation and inhibition relationships. a gene-regulatory network (grn) is a typical gene network. this type of network provides a variety of scoring measures for calculating node scores, such as degree centrality, closeness centrality, and betweenness centrality. using these measures, we can determine more meaningful disease-related genes in a gene network for a specific disease. fig. 1 shows the gene network for prostate cancer constructed by the principle [cit] tool. the principle tool describes gene networks based on the prince algorithm. it uses node colors to represent degrees of gene-disease similarity. in fig. 1, nodes indicate genes, while edges indicate gene-gene interactions. in their gene network, we confirmed that a network can be used to present useful knowledge between biological entities. considering the network characteristics, we constructed a disease-related gene network. we used various node shapes to indicate various gene conditions, such as confirmed genes and candidate genes. in our research, degree centrality was used with google data as a network analysis measure to calculate scores of nodes in a gene network."
"where d mue (meters) is a distance from the henb to the mue, and pl(x) (db) is path loss for the signal interference from the henb to the mue which is x meters apart from the henb. in this case, in order to guarantee more than s target db sinr for mues located farther than d mue target meters, the henb determines its transmission power (p henb target ) from equation (4) as follows:"
different types of radar signal waveforms are described in section ii. the design of digital iq method and its implementation using matlab is discussed in section iii. matlab results are presented in section iv and conclusions are given in section v. fig. 3 . this analog method is difficult to implement without errors. analog mixing of iq sampling introduces some phase difference between the in-phase path and quadrature-phase path. the continuous noise is also added to the signal due to the active low pass filter devices. to overcome these errors quadrature sampling with digital mixing is preferred. the above disadvantages can be avoided with digital mixers and digital filters. the mixing signal frequency f c is chosen one fourth of the sampling rate [cit] . the digital mixing process is shown in fig. 4 .
"when phase a is still in the demagnetization process and phase b current is in a zero-voltage loop [see fig. 8(d) ], there is no overlapped current loop between phase a and phase b. in this condition, phase a is again under the demagnetization voltage of the battery bank b and capacitor c 1 . the voltage and current of phase a during this stage can be expressed as (4) ."
"in indoor environments, channel quality between the cellular base station and mobile node may be low due to many walls and obstacles. thus, wireless communication for indoor environments requires more wireless resources including time, bandwidth, and transmit power, to guarantee required service quality of customers. nevertheless, shortage of wireless resource in cellular networks will be accelerated, since more than 60 % of voice traffic and 90 % of data traffic are expected to be generated indoors [cit] ."
"the simulator is composed of operational and functional parts as shown in figure 4 . the operational part initializes, manages, and finalizes simulations by utilizing initialization, event processing, data management, and finalization modules. the functional part provides various functions and models that are used to emulate lte network environments, including femtocells and ues. for the purpose, the functional part consists of five modules that perform packet scheduling and call admission control functionalities and model various wireless channel effects, real and non-real time traffics, and user mobility. functional modules are tightly integrated with the event process module in the operational part, and they are managed and used in an event-driven manner."
"to evaluate which features to include, boruta (a wrapper built around the random forest classification algorithm) was used. this is an ensemble method in which classification is performed by voting of multiple classifiers. the algorithm works on a similar principle as the random forest classifier. it adds randomness to the model and collects results from the ensemble of randomised samples to evaluate attributes and provides a clear view on which attributes are important [cit] . all features were deemed important to the model based on the random forest, with 5 day and 10 days (via sma) the highest importance among the tested averages. the de-noised closing price was one of the most important variables also."
"the first parameter to consider was the temporal length window. as suggested by supporting literature [cit] these type of networks may struggle to learn long term dependencies using gradient based optimisation. an autocorrelation function (acf) was run for the closing price time series to assess the relationship between the current closing price and previous or future closing prices. while this is not a guarantee of predictive power for this length, it was a better choice than random choice. closing price is correlated with a lag of up to 20 days in many cases, with isolated cases at 34, 45 and 47 days. this led the grid search for the temporal window to test from 2 to 20, 34, 45 and 47 days. to ensure a robust search, larger time periods of up to 100 days were also tested in increments of five. the most effective window temporal length was 24."
"in order to investigate the torque performance of the proposed srm drive, figs. 27 and 28 present the torque and torque ripple comparisons between the traditional system by employing the conventional asymmetric half-bridge converter and the new system by employing the proposed multiport converter. as shown in fig. 27, the torque is improved by fast winding excitation and fast winding demagnetization in both gcu and battery driving modes, especially in high-speed operations, due to the multilevel voltage generated in the phase commutation region. regarding the torque ripple, the torque ripple between the conventional and proposed drives are similar, confirming that the proposed drive will not decrease the motor performance, as shown in fig. 28 ."
"in these equations, a þ n denotes the n-th neighbor node linked by node a, and n(a) is the number of neighbor nodes linked by node a. the frequency(a, b) is the number of appearances of node a and node b in the same sentence, and google search result (a, b) is the google search result value between node a and node b. fre(a) is the score of node a calculated based on the frequency value, and gsr(a) is the score of node a calculated based on the google search result value."
"the charging current for the battery bank in this condition can be expressed as follows: fig. 15(a) shows the hybrid power source cooperation at different speeds when the solar irradiance is insufficient, where the pv is idle. during the sheb starting and acceleration progresses, the gcu and battery can work together to increase the output torque to improve the start-up and acceleration performance, and the power flow is illustrated in fig. 16(c) . at low and high speeds, the gcu and battery can work independently for steady-state operations, where the output torque is improved by the boosted dc voltage, and the power flows are shown in fig. 16(a) and (b). the energy can be recycled to the battery bank during braking conditions, by controlling the phase current flowing in the inductance descending region, as shown in fig. 16(g) . in sheb standstill condition, the battery bank can be charged by the gcu/ac grids without external converters, and the power flows are presented in fig. 16(h) and (j) ."
"wing to the high demand of fuel efficiency and environment protection against air pollution, hybrid electric buses (hebs) have received much attention for urban transportations [cit] . for electric powertrain systems, permanentmagnet synchronous machines (pmsms) are a popular solution because of their high efficiency and high torque density [cit] . however, pmsms need to use permanent magnets, which bring about problems of high cost and poor stability. therefore, many efforts are focusing on developing rare-earth-less or rareearth-free motors for future traction drives [cit] . switched reluctance motor (srm) has received significant interest for electric vehicles [cit], which has no rotor windings and permanent magnets. srms have the advantages of robust structure, low cost, high reliability, wide-speed range, and good fault tolerance ability, which give these motors the ability to work in high-speed, high-temperature, and safety-critical applications."
lgscore baseline icantly increased risk of prostate cancer among individuals who carried the tnf a-308 allele. ding [cit] confirmed a significant association between the cyp1a1 msqi polymorphism and the risk of prostate cancer among asians. brachyury (t) and pyruvate carboxylase (pc) are candidate genes that may be associated with prostate cancer. huang [cit] demonstrated that an in vivo treatment of tumor xenografts with chemotherapy resulted in the selective growth of resistant tumors characterized by high levels of brachyury expression. ihnatko [cit] reported that the levels of pyruvate carboxylase were altered in tumor-bearing mice with anorexia.
"where p henb max is maximum transmission power of henb. in the simulations, we set p henb max, s target, and pl(d mue target ) to 20 dbm, −6 db, and 60 db, respectively. table 2 details the parameters used in the simulations. figure 8 . in the scenario, the distance between enb and mue is fixed to 100 m, while shortest distance between henb and mue varies from 0 to 20 m at intervals of 5 m."
"compared to existing drive topologies, pv panels are employed and multiport is achieved in the proposed motor drive, by using fewer power devices without extra dc-dc converters and inductors. the new topology is derived from an asymmetrical half-bridge converter, and thus has a good fault-tolerance ability. the dc voltage is boosted in both gcu and battery driving modes, where the torque capability is improved accordingly. six driving modes and five charging modes are obtained, achieving a more flexible energy control. the pv can work with the battery/gcu together as an assisted sustainable energy source to contribute to the motor driving, which significantly reduces the reliance on fuels/batteries. furthermore, the battery can be quickly charged by the pv panels, gcu, or ac grids at sheb standstill condition, where the flexible charging and self-charging ability are obtained, and the reliance on charging stations is significantly reduced."
"the handover event is used to manage mobility of ues. the event is generated whenever an ue crosses the boundary of the currently located cell. upon receiving the handover event, the event handler determines if the corresponding ue has ongoing service. if the ue does not receive service, the event handler performs the cell reselection procedure and updates cell id field in the ue information structure to a new cell id. on the other hand, if the ue receives a service, the event handler commands the cac module to determine whether to accept the handover or not. if the new cell cannot accommodate the ue, the service of the ue is terminated, as the same handling procedure for service departure event. however, if the handover of the ue is accepted, as the result of cac, both cell id fields in the ue information structure and the next packet arrival event are updated to the new cell id."
"the simulator provides two stages for setting of simulation environments, as shown in figure 6a . in the first stage, operators can set general system parameters, such as simulation duration, carrier frequency, frequency band, and random seed, and parameters related to traffic generation, such as average load and service duration, distribution of service duration, and portion of each service type. detailed parameters for both macrocell and femtocell base stations and users, including channel effects, number of cells, cell radii, transmission power of enb and henb, and mobility characteristics, can be controlled in the second stage. in addition, type, deployment mode, and transmission power control methods for henbs can also be selected in the second stage. figure 6b shows the main window of the simulator. network topology and positions of ues can be observed in real time on the left hand side, while the control of simulation operations and monitoring statistics are enabled using interfaces on the right side of the main window. by clicking the house or building icon in the left hand side of the main window, we can see the indoor environment window for the femtocell as shown in figure 6c . the floor plan, positions of ues, and snapshots for sinr and modulation and coding scheme (mcs) levels can be observed in the window."
"in this section, we describe the google data and literature data used to test that which components of the two lgscores play a more important role in identifying disease-related genes."
"there are two working modes for the boost converter, including the current discontinuous conduction mode (dcm) and current continuous conduction mode (ccm). in dcm, there is no right half-plane zero so that the converter is stable. in ccm, there is a right half-plane zero that impacts on the stability of the converter."
"deep learning models such as the rnn and lstm are evidently effective for bitcoin prediction with the lstm more capable for recognising longer-term dependencies. however, a high variance task of this nature makes it difficult to transpire this into impressive validation results. as a result it remains a difficult task. there is a fine line between overfitting a model and preventing it from learning sufficiently. dropout is a valuable feature to assist in improving this. however, despite using bayesian optimisation to optimize the selection of dropout it still couldn't guarantee good validation results. despite the metrics of sensitivity, specificity and precision indicating good performance, the actual performance of the arima forecast based on error was significantly worse than the neural network models. the lstm outperformed the rnn marginally, but not significantly. however, the lstm takes considerably longer to train."
"performance degradation caused by signal interferences from femtocells is one of the most important issues in femtocell deployments. as we have mentioned in section 'preliminaries', femtocells and macrocells may interfere with each other and performance of both networks can be decreased, since many telecommunication companies prefer reusing the frequency bandwidths of existing cellular networks for femtocells due to the frequency efficiency and the cost."
"the simulator was developed based on event-driven programming [cit] . event-driven programming is a program paradigm in which the flow of the program is determined by events. since event-driven programming can reduce unnecessary loops and operations, it can enhance efficiency and performance of the program and is widely used in well-known network simulators such as opnet, qualnet, and ns-2 [cit] ."
"to summarize, we showed that 18 genes among the inferred top 20 genes were meaningfully linked to prostate cancer, while the remaining two genes are reportedly involved in other cancers. these results indicate that the lgscore method can extract disease-gene relationships successfully."
"lgscore is therefore an effective method with which to identify disease-related genes. in this paper, we used only nouns among the parts of speech. for further work, we will use other word classes such as verbs, adjectives, and adverbs to improve lgscore. furthermore, we will use other biological information including protein, drugs, and mirna data."
"the remainder of the article is organized as follows. section 'preliminaries' provides background for wireless resources and femtocell networks in 3gpp lte (-advanced) systems. in section 'simulator design and development', we introduce architecture, functionalities, components, and gui for the proposed simulator. then, we analyze interference between macro-and femtocells in the lte system by utilizing the simulator in section 'interference analysis'. finally, section 'conclusion' gives a conclusion."
"a comparison of a results obtained using lgscore and the frequency-based model is presented in table 4 and fig. 9 . the frequency-based model is a method which uses only the frequency as an edge weight in the gene network. all of the processes of the frequency-based model are identical to the lgscore except for the scoring function. the frequency-based model constructs a disease-related gene network based on co-occurrences in the literature. this approach links genes that appear in the same sentence and assigns weights to each edge between two genes using the frequency. the frequency indicates the number of sentences that mentioned both genes. the model infers disease-related genes using the frequency value with regard to other neighboring nodes. the scoring process is identical to the equation for fre(a). for this reason, the nodes and edges in the frequency-based gene network lgscore. the frequency indicates the number of papers which include two genes, and the google search result indicates the value obtained by a google search using the two genes as keywords."
"similar to the demagnetization process, the excitation voltage of phase a is also boosted, due to the impact of the phase c demagnetization, as shown in fig. 7(a) . when the demagnetization current i c is larger than the excitation current i a, the phase c acts as a current source not only to power phase a, but also to charge the battery bank and capacitor. in this condition, phase a voltage is boosted due to the phase c demagnetization under the voltage of the battery bank b and capacitor c 1 . the voltage and current of phase a during this stage are given by the following:"
the i and q components are generated using frequency translation. the output of frequency translation consist one upper and one lower frequency components. the upper frequency components are filtered out by the low pass finite impulse response (fir) filter. at the output of filter the phase is computed using i & q filtered outputs. the proposed digital iq method is discussed in detail in the subsequent section.
"taking the commutation of phase a and b for example, when phase a is turned off and phase b is not turned on, the demagnetization current of phase a flows back to the battery bank b and capacitor c 1 through the antiparallel diode inherent in the switch s f 2, where the battery is charged by the demagnetization current, as shown in fig. 8(a) . the phase a winding is under the demagnetization voltage of the battery bank b and capacitor c 1 . the voltage and current of phase a, i.e., u a and i a, are expressed as follows:"
"network analysis also plays an important role in biological research. gene networks, which describe gene-gene interactions, and protein networks, which describe protein-protein interactions, allow the visual relationships among biological entities in complex biological systems to be presented in a simple, clear manner. network analysis also provides an opportunity to analyze which relationships are meaningful among various candidates. a network analysis provides several analysis measures as well, such as degree centrality, closeness centrality, and betweenness centrality to identify novel relationships among the large numbers of relationships in the network."
"where t e is the total electromagnetic torque, t l is the load torque, j is the combined moment of inertia of the motor and load, and μ is the combined friction coefficient of the motor and load. fig. 2 illustrates the relationship of the phase inductance, phase current, phase voltage, and torque, where region i is the motoring region and region ii is the braking region. as shown in fig. 2(a), the positive or negative torque can be obtained by controlling the phase current flowing in the inductance ascending or descending region, respectively. fig. 2(b) shows the motoring conditions under different excitation and demagnetization voltages. the current rises slowly when the excitation voltage is low, and a negative torque is easily generated due to the demagnetization current existing in the inductance descending region, which degrades the motoring performance. however, this negative torque can be eliminated by increasing the excitation and demagnetization voltage, and the output torque can be enhanced accordingly."
"in this section, we propose a means of identifying diseaserelated genes using google search data and literature data. our method is illustrated in fig. 2 ."
"recently, demands of customers for wireless data communications in cellular networks are rapidly increasing as fascinating mobile devices, e.g., smart phone, tablet device, and ultra-mobile personal computer (umpc), and mobile applications, e.g., mobile internet protocol television (iptv), high-definition video conference, and voice over ip (voip), have been introduced. for example, traffics for wireless data communications have increased by 5000 % over the past three years in at&t networks [cit] . moreover, customers expect to be provided with enhanced quality of services since they spend more communication expenses to telecommunications companies for wireless data communications than they do for voice-only services."
"the zero frequency of the pi regulator should be lower than the polar frequency of the transfer function of the standstill charging system. based on this, the zero frequency of the pi regulator can be determined. after the calculation of the zero frequency, the proportional coefficient can adjust the cut-off frequency and stability margin. after the compensation, the improved bode diagram is shown in fig. 19(b) . from the amplitude margin and phase margin, it can be seen that the system is stable in this condition."
"lgscore. here, fre(a) denotes the degree of similarity with a certain disease, and gsr(a) is the similarity with candidate disease genes. and the nodes and the edges in the lgscore-based gene network are identical. however, the disease-related genes extracted by each method differ because the weights of edges are different in the scoring function. the x-axis indicates the disease and the y-axis indicates the number of inferred genes known to be related to the disease. for prostate cancer, we found that of the top 20 genes inferred based on their lg score, 16 were related to a disease, whereas the frequency-based model only found 14 confirmed genes."
"assume that a henb has recently been installed at an arbitrary location and its power is turned on for the first time. in d-tpd, under the assumption that the henb knows distance from the closest enb, the henb decides its transmission power (p henb dbm) based on the distance from enb, d enb (meters) [cit], as follows:"
"we downloaded abstracts from publications related to the five diseases from pubmed. we obtained 41,257 human gene symbols from hgnc. table 1 summarizes the data used in our experiment. table 1 shows the number of abstracts, nodes, and edges in the gene network for each disease. table 2 shows the answer set databases that we used to validate our results. the ctd [cit], nci [cit], sanger [cit], kegg [cit], pgdb [cit], and ddpc [cit] databases contain information about genedisease relationships. additionally, we described the number of answer sets used in our experiments. ctd data was used to validate our results for alzheimer's disease and diabetes. the results for the cancers were validated using kegg and sanger data. the nci, pgdb, and ddpc databases were also used as answer sets for colon cancer and prostate cancer. we extracted disease-related genes for each disease from the databases listed in table 2 and compared these genes to the top 20 genes inferred using our method and three comparable methods."
"brief information about the ue, i.e., id, velocity, moving direction, and type of ongoing service, can be found by selecting moving ue at both main and indoor environments windows. in addition, by using buttons in the ue information window, received traffic patterns, sinr, and mcs level can be observed via real time graphs. figures 7a,b show examples of the ue information and the graph on traffic patterns."
"current databases contain a vast number of biological publications. making full use of these databases is difficult. one tool that has been used to extract hidden information from publication data is text-mining. [cit] collected microrna-related data using text-mining based on 75 rules. they extracted 878 relationships between 236 instances of microrna data and 79 instances of cancer data in publications. text-mining was thus shown to be a useful tool for extracting data from publications and identifying relationships among biological entities. in their research, we confirmed that text-mining is a useful traditional method that can also be used to extract relationships among biological entities."
"if the output signal exceeds the threshold, the target is declared as present. the threshold level is very critical since it affects the detection probability of the receiver and thus directly affects its performance. if the threshold is too high, then the receiver misses the weak signals and if it's too low the receiver picks up false signal."
"when the solar irradiance is not sufficient, the gcu and/or battery bank are put into use, where the relay j 1 is on and j 2 is off, and the pv is in an idle state. the motor can be driven by the gcu and battery working together or independently, by controlling the power switches in the bidirectional front-end circuit. the winding excitation and demagnetization are both analyzed in gcu and/or battery driving mode in the following part. fig. 6 presents different winding excitation states for the proposed srm drive. when the relay j is on, and switches s f 1 and s f 2 are both off, the motor works in the gcu driving mode and the battery is in an idle state, as shown in fig. 6 (a). when j is turned off, the gcu is idle and the windings are energized by the battery bank, where the motor works in the pure battery driving mode, as shown in fig. 6 (b). when the gcu and battery should be used together to improve the dynamic performance, j and s f 2 are both turned on, where the windings are energized by both the gcu and battery bank, as shown in fig. 6 (c). a freewheeling mode when the current flows in a zero-voltage loop exists during the normal operation, due to the soft-chopping method where s 1 is turned off and s 2 remains on in the turn-on region, as illustrated in fig. 6(d) . the gcu and battery can work together or independently, according to different running conditions."
"the simulation is carried out using matlab. the input signal is generated with the amplitude of ±125 code and the noise level of ±5 code. considering the 8-bit adc with 1 volt reference voltage, the least significant bit (lsb) of the adc is equivalent to 3.9 mv (milli volts). the ±125 adc code is equivalent to ±0.488 v and ±5 code is equivalent to ±19.5 mv. the noise is taken as gaussian and it is spread during the signal presence also. the incoming signal frequency is considered 160 mhz and sampled at a rate of 500 ms/s. the matlab simulation results are presented in fig. 12 . fig. 12(a) shows the signal amplitude plot. the sine wave of 160 mhz frequency is generated. the signal is sampled at the rate of 500 ms/s. fig. 12(b) shows its zoomed version of the signal amplitude plot. fig. 12(c) shows the instantaneous phase plot measured at each 2 ns. fig. 12(d) shows the zoomed version of instantaneous phase plot. fig. 12(e) shows the instantaneous frequency plot and fig. 12(f) shows the instantaneous amplitude plot. both frequency as well as the amplitude is also measured at each 2 ns. comparison study has been carried out between digital iq method and conventional method. the comparison results are tabulated in table i . the convetional method was analog whereas proposed method is digital. digital iq method is having number of advantages over conventional method. the major advantage of this method is duplication. since all components are digital components, so they can be duplicated without any errors [cit] ."
"iii. methodology this paper follows the crisp data mining methodology. 1 the motivation for crisp-dm over the more traditional kdd [cit] revolves around the business setting of the prediction task. the dataset bitcoin dataset used, ranges from the 19th [cit] until the 19th [cit] . a time series plot of this can be seen in figure 1 . [cit] has been excluded as it no longer accurately represents the network. in addition to the open, high, low, close (ohlc) data from coindesk, the difficulty and hash rate are taken from the blockchain. the data was also standardised to give it a mean of 0 and standard deviation of 1. standardisation was chosen over normalisation as it better suits the activation functions used by the deep learning models."
"according to event-driven programming, event-processing module of our simulator, which is the main loop of the program, is divided into two sections, i.e., event selection and event handling. in addition, event queue based on a linked list data structure is defined to arrange generated events. in event selection, event located at the head of the event queue is loaded and the appropriate event handler is executed based on the type of the event. consequently, the event handler processes the event using functional modules, global variables, and information contained in the event. the event selection and handling are repeated until the simulation terminates."
"it is undeniable that extra network infrastructure is required to expand the capacity of cellular networks. the use of femtocells, which are overlaid over the traditional macrocell based cellular networks is a key topic related to the deployment of new infrastructure in a cost-effective manner [cit] . a femtocell is small-sized cellular base station with low-power and low-cost characteristics. it can be deployed wherever wired ip access is possible by either the service providers or the customers. since this inexpensive femtocell is usually deployed indoors and connected to the backhaul through generic ip access, such as digital subscriber line (dsl), cable modem, and fiber-to-the-home (ftth), it can help cellular networks to cost-efficiently secure wireless resources and customers to use wireless data communications with low power and high data rates using existing cellular devices. despite these advantages, several technical issues should be considered for efficient femtocell deployment, e.g., signal interference coordination, resource allocation and management, access strategy, network synchronization, self-organization and optimization, and handover between macro-and femtocells. especially, signal interference between macro-and femtocells should be analyzed and carefully coordinated, since femtocells are expected to share the same licensed bandwidth with traditional cellular networks for high frequency efficiency."
"for performance analysis, we assume that macrocell enb always uses fixed transmission power (43 dbm) for downlink transmissions. conversely, since transmission powers of femtocell base stations (henbs) are closely related to the performance of overall networks [cit], we consider three cases for henbs' transmission power determination, i.e., fixed transmission power, distance based transmission power determination (d-tpd), and path loss based transmission power determination (p-tpd). in the first case, transmission powers of henbs are fixed to a specific value (0, 10, and 20 dbm), while d-tpd and p-tpd determine transmission powers of henbs based on the distances and the path losses among henb, mue, and enb, respectively, [cit] ."
"upon optimizing the cost function in eq. (2), we obtain a set of adapted modulation profiles . to analyze the effect of strf adaptation on the neural ensemble, we consider the average difference between the adapted and initial modulation profiles by computing"
the stimulus reconstruction approach has been successful in a number of neurophysiological studies. the approach was pioneered in studies of the fly visual system [cit] and has been used to study feature encoding in visual [cit] and auditory cortical circuits [cit] . of particular interest are recent studies that have shed light on how cortex represents imagined speech [cit] and the nature of how top-down adaptive feedback influences representation in cortical circuits [cit] .
"more generally, the increasing availability of mobile sound processing applications has resulted in a significant increase in the variety of acoustic environments, communication channels, and noise conditions encountered by existing systems. consequently, this necessitates signal processing strategies that must gracefully accommodate these factors to maintain state-of-the-art performance. we contend that because nature has converged to a robust solution for handling unseen and noisy acoustics, there is much to leverage from auditory neurophysiology when designing automated sound processing systems. generally speaking, cortically inspired feature representations based on spectro-temporal receptive fields underlie a number of successful approaches to noise robust speech activity detection [cit], speech and speaker recognition [cit], and auditory scene classification [cit] . the present study, in concert with other recent work in our lab [cit], represents an extension of this methodology by incorporating the cognitive effects of dynamic, task-driven sensory adaptation as part of the feature extraction pipeline. it is our belief that new and existing systems can only benefit by incorporating the adaptive mechanisms as outlined in this paper."
"digital object identifier 10.1109/taslp.2015.2481179 brain's ability to intelligently alter its response to changing tasks and listening environments. for example, we can easily follow the first flute solo among the cacophony of an orchestra or keep track of a friend's voice at a noisy cocktail party. considerable effort has been focused on studying the nature of this adaptive processing at the perceptual and neurophysiological levels in humans as well as animal models [cit] . moreover, a better understanding of the mechanisms by which the auditory system defines and selectively enhances the acoustic foreground while minimizing the impact of a noisy background has significant implications for signal processing strategies in noisy environments. at the neural level, manifestations of adaptive processing are intricate, operating at multiple processing scales from subcellular all the way through network levels [cit] . for individual neurons, changing behavioral tasks can induce changes in a neuron's spectro-temporal receptive field (strf) [cit] . the strf is a two-dimensional kernel in time and frequency that summarizes the linear processing characteristics of a neuron. it can be thought of as a filter that operates on incoming acoustic inputs in order to extract specific features from sounds. in mammalian primary auditory cortex, strfs exhibit detailed sensitivities to a broad range of acoustic features and are particularly selective of spectro-temporal energy modulations that characterize slow changes in temporal envelopes and spectral profiles of natural sounds [cit] ."
"the solutions to (p1) and (p2) are found numerically [cit] by searching for stationary points of the respective objective functions, i.e., when and . for the regression coefficients, upon convergence of (p1), and assuming the minimum lies within the feasible set formed by the constraints on the, the regression coefficient vector can be written as we interpret the term as a \"prediction error\" and consequently hard-to-predict responses have more influence on choice of the optimal regression coefficients. moreover, because the for are constrained to be positive, those coefficients can be thought of as a population gain vector that applies more weight to task-relevant vs. task-irrelevant neurons."
"beyond the midbrain, the time-varying tonotopic signal is further analyzed by ensembles of neurons in primary auditory cortex (a1) [cit] . cortical neurons essentially act as filters that extract information about the frequency content and spectro-temporal dynamics of an input auditory spectrogram, and each filter's tuning characteristics is described by its spectro-temporal receptive field [cit] . as illustrated in fig. 1, strfs reflect sensitivity to a variety of input energy patterns, with simple shapes preferring highly localized and narrowband fig. 1 . proposed discriminative framework for task-driven strf adaptation. examples of speech and non speech stimuli are passed through a model of the auditory periphery, and the resulting auditory spectrogram is analyzed by a bank of strfs derived from recordings from ferret primary auditory cortex. top-down feedback acts to assign a behaviorally meaningful categorical label to observed population responses, which are subsequently discriminated using logistic regression. feedback from the discriminative model, in the form of the regressor prediction error, is used to iteratively adapt the shapes of the strfs to improve prediction of speech vs. non speech sounds. input to complex shapes preferring spectral, temporal, and joint spectro-temporal variations."
"there is also the possibility to combine both visualization modes, i.e., one mode for displaying global differences and one for individual changes. either combination of relative or absolute visualization of the color or size can be utilized at the same time for the investigation of metabolic changes. this offers a more holistic insight into the data set used and can highlight dynamics that may be overseen otherwise. this enables the investigation of relative and absolute metabolic changes at the same time, which offers a more holistic insight into the used data set."
"based on this knowledge, task-driven adaptation strategies that improve the separation between foreground speech and background nonspeech sounds are particularly attractive for the challenge of speech activity detection (sad). sad refers to the task of assigning a speech or nonspeech label to samples in an observed audio recording and is a fundamental first step in a number of automated sound processing applications. for example, in automatic speech recognition tasks, one should transcribe only speech events in the observed audio. in low-noise environments with a close-talking microphone, sad can usually be solved using traditional measures like signal energy, zero-crossing rate, and pitch (see, e.g., [cit], but performance rapidly degrades in noisy, reverberant, and free-field microphone conditions. however, research in the past decade has begun to focus on the issue of noise-robust sad, with successful approaches leveraging prior knowledge about the differences in acoustic profiles that differentiate speech and nonspeech sounds. early efforts based on statistical models (see, e.g., [cit] have been especially improved in recent studies [cit] . furthermore, a variety of approaches have been designed to specifically exploit the spectral and temporal structure of speech [cit] . most recently, data-driven approaches based on recurrent and deep neural networks have further pushed the state of the art, yielding extraordinary results on difficult corpora [cit] ."
"the zoom handy recorder h4 is an all-in-one recorder which combines a handy size with a highperformance stereo condenser microphone, a secure digital (sd) card recorder, a mixer, an effect section and some more features. it is possible to switch between the stereo mode and a 4-track mode which provides the playback of four simultaneous tracks and the recording of two tracks. there are several sampling rates to choose, the tracks are recordable in wav or mp3 format, and the device is easily connected to the computer via sd. all audio tracks used in the videos are recorded with the zoom handy recorder h4."
". a system that performs well has small and across a broad range of thresholds, hence (1) the corresponding det curve will be close to the origin and (2) eer will be small."
"the visualization mode of changing node colors encodes the quantitative values of the experimental data in a color gradient. either three different colors or three different shadings of one or two colors can be pre-defined to represent the maximum, middle, and minimum measured value of a metabolite. the algorithm calculates the color of all values in-between from the color gradients. compared to the visualization mode mentioned before, this avoids the problem of overlapping metabolites if the network is not dynamically and simultaneously rearranged."
in the extreme state of long-term stored platelets (day 7-10) probably irreversible changes occur which then lead to a faster decay and total cell lysis. considering all phenotypes the decline of platelet metabolism is a nonlinear process consisting of successive metabolic shifts.
"in the third visualization type, metabolites are visualized as round \"aquariums\" showing the corresponding data points as a \"fill level of water.\" compared to the other two visualization modes, this mode combines the representation of absolute and relative values in one graphical presentation. while the fill levels show the absolute measured data points of one metabolite, the color of the fill level representation refers to the relatively measured data points of this metabolite compared to all others."
"we first apply the model to simulate a scenario where a listener adapts processing to focus on speech sounds in additive noise environments. this result is shown in fig. 3(a), and illustrates that the overall effect of task-driven adaptation is to increase population sensitivity to slower modulations, with the effect being stronger for downward vs. upward moving modulations (i.e., the right-vs. the left-half planes, respectively), while suppressing sensitivity to faster modulations away from the origin. this pattern was also found for other random selections of the initial ensemble . the magnitude of the effect depends on choice of model hyperparameters, becoming stronger for decreasing and increasing (data not shown). finally, shown next in panels b and c are the adaptation patterns of two individual neurons, illustrating how the neurons broaden and reorient themselves to better focus on upward and downward modulations as suggested by panel a."
"the experimental data which was used to create the videos of the new time visualization modes were in csv format structured as a table. the first column defines the time, and all others are labeled with the identifier of the corresponding metabolite. the rows identify the different measured time points."
"in this paper, we applied a model of auditory receptive field plasticity that acts in the modulation domain to simulate a listener dynamically adapting cognitive resources to better track speech in noisy acoustic environments. we first described how an ensemble of initial strfs adapt to highlight the differences in spectro-temporal modulation profiles for speech vs. nonspeech sounds. we showed that the model induces strf plasticity that strengthens relatively slow spectro-temporal modulations close to the origin (in the rate-scale domain) while simultaneously suppressing faster modulations away from the origin. we then showed how use of the adapted strfs improves the separation between the representation of speech and nonspeech sounds, resulting in a substantial performance gain in a speech activity detection task across a variety of previously unseen noise types. finally, we explored, via stimulus reconstruction experiments, the extent to which the passive and adapted strf ensembles captured the salient features of the target speech source. these results showed how the use of task-driven adaptation can improve the representation of a speech target in clean and noisy conditions, as confirmed by objective and perceptual measures. this helped shed light as to how the representation improved to help facilitate the detection of speech in noise. the overall results suggest that strfs adapted according to a biologically motivated model of task-driven adaptation can form a noise-robust representation of sound that is applicable to automated speech processing applications."
"as already mentioned for the visualization mode of changing node sizes, either the absolute or the relative value of a metabolite can be represented in this mode. depending on the analysis task both versions can be useful, but the mainly used version will be the representation of the absolute value of a metabolite as the node color."
"we consider two measures to evaluate the quality of a given reconstruction from an inverse mapping obtained by eq. (5). the first is the temporally averaged mean-square error between the original and reconstructed spectrogram, defined as and serves as an objective measure of reconstruction quality. the second is a perceptual comparison between the original time-domain waveform and synthesized version obtained using the gaussian convex projection algorithm [cit] . the comparison between the waveforms is made using the itu standard perceptual evaluation of speech quality (pesq) measure [cit] . pesq ranges between 1 and 5 and correlates well with listener-reported mean opinion scores of perceptual quality, with higher scores indicating higher quality."
"escher is a web-based software for drawing, manipulating, and viewing metabolic networks, implemented in javascript [cit] . in this work, escher's feature for overlaying these networks with data has been extended to cover time series data ( figure s3 ). the source-code of this modified version of escher is available at /christophblessing/escher/. supported features include mapping of time series data to a pair of absolute node sizes and absolute colors as well as the display of animations. the current implementation maps high values to large dark nodes and low values to small bright nodes. escher naively encodes network maps in a tool-specific json format."
"we next select random samplings of strfs from the large physiological ensemble described in section ii-b. the use of a larger ensemble is computationally challenging because the number of parameters involved in the optimization of (p2) becomes prohibitively large. furthermore, we find that random samplings 50 strfs are sufficient to tile the relevant modulation space of speech tokens given the redundant and overcomplete nature of neurophysiological receptive fields [cit] . next, each strf is interpolated along the time and frequency axis to match the temporal and spectral sampling of the input tokens (i.e., 100 hz temporal and 6.04 cyc/oct spectral, respectively). we also assume that each strf has a starting frequency of 90 hz spanning 5.3 octaves. we scale each strf to have unit euclidean norm, and apply the 2d dft, followed by the modulus operation, to obtain the initial set of modulation profiles . this set represents a \"passive\" listening state, one where adaptation is not induced. note that for adaptation we only need to consider the first two quadrants of the dft since for real-valued input . finally, to visualize the adapted strfs in the original time-frequency domain, we use the phase of the original passive filters."
"c urrent sound technologies borrow numerous biomimetic mechanisms widely observed in the brain in order to augment their robust sound processing. however, they remain mostly passive systems constrained to scenarios and conditions for which they were trained. importantly, they fail to take advantage of adaptive capabilities that underlie the manuscript received february 09, 2015; revised june 08, 2015; accepted september 07, 2015. date of publication september 23, 2015; date of current version october 06, 2015. this work was supported in part by the national science foundation under grant iis-0846112, in part by the national institute of health under grant 1r01ag036424, and in part by the office of naval research under grants n000141010278 and n00014-12-1-0740. the associate editor coordinating the review of this manuscript and approving it for publication was prof. deliang wang."
"the previous section showed empirically that the proposed strf adaptation framework improved detection of speech in unseen noisy conditions by increasing the separability between the llr scores of speech and nonspeech ( fig. 4(b) ). however, we sought to better understand how the adapted strf ensemble improved the representational fidelity of a target speech signal. one way to assess the ability of a neural ensemble to encode features of the target source is to use a stimulus reconstruction approach. by reconstructing the observed input to the ensemble, one can assess how the features of the input are encoded by the population. one can then vary the state of the ensemble (i.e., passive vs. adapted) and compare the reconstructions with the original stimulus."
"we also explore the extent to which the passive and adapted strf ensembles encode information about the attended source in additive noise conditions. here we consider a test set of 100 utterances from the timit corpus corrupted by additive fig. 6 . analysis of noisy speech reconstructions. reconstruction quality degrades with increasing noise. however, in all snr cases, the adapted ensembles yield, on average, a higher quality reconstruction with respect to the clean references. results shown for ."
the ascending auditory pathway comprises a hierarchy of stages that transform sound observed at the outer ear to neural responses in central cortical areas. we begin by briefly describing the relevant aspects of this processing pipeline as it pertains to the adaptation framework considered in this paper.
"beyond their inherent tuning to specific acoustic modulations in the signal, cortical neurons can dynamically adapt their filtering properties towards relevant sounds in a task-driven manner. when cognitive resources are directed towards a sound of interest, cognitive feedback is believed to induce strf plasticity, whereby cortical filters adapt from some \"default\" tuning to task-optimal shapes in order to enhance neural responses of task-relevant features while suppressing those of distractors. such plasticity patterns have been observed in a number of neurophysiological studies involving simple tonal stimuli [cit] as well as stimuli characterized by complex spectro-temporal dynamics [cit] . the overall adaptation patterns reflect that of a contrast-matched filter, and it has been hypothesized that such changes serve to improve discriminability between the acoustic foreground and background [cit] . importantly, similar effects have been observed in other sensory modalities [cit], suggesting that such discriminative changes in receptive field shape represent a general strategy used by sensory cortex to highlight task-relevant stimuli. the present work aims to understand the relevance of task-driven adaptation of filter tuning properties in speech processing tasks. human listeners are especially adept at tracking speech sounds in very noisy environments [cit] . this ability engages a number of complex sensory and cognitive processes, most notably adaptive neural plasticity which allows the brain to hone in on conversations of interest. speech signals are well characterized by their spectro-temporal modulation content, and so the fourier domain is a natural space for exploring adaptive feature extraction strategies for speech signals. this is because a number of speech features can be expressed jointly in the spectro-temporal modulation domain, including voicing state, phoneme identity, formant trajectories, and syllable rate [cit] . furthermore, sounds having considerable overlap in time-frequency may in fact be disjoint in the modulation domain, leading to methods for signal denoising and enhancement [cit] . finally, modulation-domain adaptation has connections to a general form of object-based cognitive feedback. specifically, fourier-based analysis facilitates the separation of magnitude and phase components in the signal. adapting the fourier magnitude profile, which characterizes the strength of spectro-temporal modulations present in the signal, separately from its phase profile, which characterizes the relative timing of these modulations, is akin to processing an abstracted representation of the signal, an important component of object-based attention [cit] ."
"as depicted in supplementary figure s1e a fully colored metabolite describes the maximum measured value (full fill level), in contrast, an empty metabolite shows the minimum measured value (fill level is zero). in supplementary figures s1e to s1f also the different colors of the involved metabolites are shown. the darker the color, the higher is the measured value of this metabolite in comparison to all others. for a lighter shading, the average value is lower (supplementary figure s1f) . in contrast to the other types, this visualization mode offers the possibility of getting a reference of the current depicted data point to the maximum and the minimum measured value of this metabolite. for instance, by showing the absolute concentration of the metabolite as a fill level, the estimation of how close the depicted concentration is to its metabolic specific maximum or minimum is more apparent than for the other two visualization modes. [cit], the extraction of the information is also more natural than estimating area sizes or colors."
"to account for the transformation of sound from the outer ear through the auditory midbrain, we use a computational model of mammalian auditory periphery to obtain a time-frequency representation for input stimuli referred to as an auditory spectrogram [cit] . this model accounts for peripheral processing spanning the cochlea through the auditory midbrain. first, an input signal is processed by a bank of 128 constant-q gammatone-like filters. the filters are uniformly spaced along the logarithmic tonotopic axis, starting at 90 hz, and span 5.3 octaves. this is followed by a first-order derivative and half-wave rectification to model lateral inhibition in the cochlear nucleus and acts to sharpen the filter responses in each channel. finally, the responses are smoothed in time using an exponentially decaying filter with a 10 ms time constant to model short-term integration and the loss of phase locking in the midbrain. examples of auditory spectrograms for speech and a nonspeech jet sound are shown in fig. 1 ."
"in this paper, we consider ensembles of neurophysiological strfs estimated from recordings from non-behaving ferret primary auditory cortex, collected in the context of studies not specifically related to the current work [cit] . we use neurophysiological strfs because of their inherent ability to form a rich, redundant, and over-complete neural representation that captures the span of spectro-temporal modulations that characterize natural sounds [cit] . all strfs were derived from neural responses to modulated noise stimuli known as temporally-orthogonal ripple combinations (torcs) [cit] . torcs represent a spectro-temporally rich stimuli for driving cortical neuron responses, and facilitate a mathematically tractable method for estimating the transfer function of a neuron, i.e., its strf [cit] ."
". the constraints on (p1) are justified below whereas the constraints on (p2) are required since modulation profiles are necessarily nonnegative. because is a sum of convex functions, and the constraints on (p1) and (p2) are convex, each subproblem is therefore convex with a unique global minimum. furthermore, since each update to and does not increase the value of, alternating updates to and guarantee convergence to a local minimum of the overall objective function [cit] ."
one strategy for jointly optimizing the shapes of the strf modulation profiles and regression parameters is the use of block coordinate descent where we optimize eq. (2) by alternating between solving two convex subproblems:
"in this section, we first describe a computational framework inspired by auditory processing to induce adaptive driven changes in a set of neurophysiological strfs. we then demonstrate how the adapted strfs yield features that improve sad performance across a variety of unseen noise conditions."
"2) proposed sad system: shown in fig. 4(a) is an example det curve for the noise for the baseline and proposed system using passive and adapted strfs. the det curve is obtained by pooling llr scores across snrs db. because the det curve for the adapted ensemble is closest to the origin with no overlap with the other curves, it represents clear improvement over the baseline and the passive ensemble across all snrs, with absolute reductions in eer of approx. 11% and 3%, respectively."
"in this paper, we take a different approach to sad, and consider to what extent task-driven adaptive retuning of strfs can improve sad performance. we begin by reviewing relevant concepts regarding auditory peripheral and central processing as they pertain to the framework presented here. next, we describe a computational model of task-driven strf plasticity in the modulation domain inspired by auditory neurophysiology and explore its application to the challenge of detecting speech in noisy environments. we show how to induce adaptation in an ensemble of neurophysiological strfs, and demonstrate how the strfs reorient themselves to enhance the spectro-temporal modulations of speech while suppressing those associated with a variety of nonspeech sounds. importantly, we demonstrate how features derived from the adapted strfs improve performance in a sad task in unseen noise conditions with respect to an unadapted ensemble and a recently proposed baseline. lastly, to better understand how strf adaptation affects the representational quality of a target speech sound, we consider a stimulus reconstruction task similar to those recently considered in a variety of neurophysiological studies. we show that stimuli reconstructed from strfs adapted using our proposed framework yield a higher fidelity representation for speech in clean and additive noise conditions using a variety of objective and perceptual measures. overall, the results suggest that a framework for task-driven adaptation formulated in the modulation domain can yield a high-fidelity, noise-robust, and improved representation of the target source that is applicable to automated speech processing tasks."
"in summary, the results of this section suggest that the proposed model of task-driven adaptation induces strf changes that a facilitate higher-fidelity representation of attended speech in clean and noisy environments. this lends further insight as to how such an adaptation strategy is able to improve detection of speech in noisy environments."
"sony vegas pro 12.0 is a professional video and audio editing program provided by sony. it is a standalone commercial setup for windows, here used under windows 10. its design is easy to grasp for novices but also has a wide range of powerful features which are also used by experienced users. sony vegas pro accompanies the whole creation process of a video. the currently latest version is sony vegas pro 14.0. sony vegas pro 12.0 has been exclusively used to create the videos of this study (see supplementary figure s5 )."
"to study how reconstruction performance varies as a function of adaptation state, we use the passive and adapted strf ensembles and to obtain optimal inverse mappings and, respectively. we use 350 clean speech utterances from the timit corpus (approx. 17.5 minutes) to learn the inverse mapping matrices. the neural responses are also standardized to have zero-mean and unit variance prior to obtaining and, and these parameters are applied to subsequent reconstructions. for a given inverse mapping, we also consider a range of inverse filter lengths spanning ms and eigenvalue thresholds . results are reported here for ensembles that achieve minimum average meansquare reconstruction error on a test set of 100 clean speech utterances from the timit corpus. for synthesizing timedomain waveforms, we first apply a nonlinearity to synthesized spectrograms, followed by a maximum of 30 iterations of the gaussian convex projection algorithm. fig. 5(a) are examples of reconstructions from clean utterances obtained using the passive and adapted strf ensembles. we first find that both reconstructions are somewhat noisy, with yielding a distinct temporal distortion whereas introduces spurious patches of spectro-temporal energy. however, both reconstructions are sufficient to capture the broad prosodic characteristics of the reference spectrogram, with good qualitative matches between pitch variations, syllabic rate, and broad formant dynamics over time. furthermore, it is clear that yields a reconstruction with better spectral resolution, since the harmonic peaks during sections of voicing are far more pronounced as compared with . next, panel b shows that across the test set, the adapted ensemble yields an objectively better reconstruction, with yielding a significantly lower reconstruction error as compared to ( -test, ) . finally, the perceptual analysis in panel c shows yields a significantly higher quality waveform synthesis compared ( -test, ) . of course, while pesq values between 2-3 are generally considered somewhat noisy, informal listening tests confirm that the synthesized waveforms are nevertheless intelligible, with conveying a better percept of voicing and pitch."
"in earlier work, we explored a mathematical foundation for task-driven changes in neurophysiological strfs [cit] . here, we build on these concepts to develop a model of task-driven strf adaptation to reliably detect speech in noisy environments. the framework considered in this study is designed to be consistent with neural circuits thought to induce adaptive changes in cortical strfs [cit], and a schematic of the process is shown in fig. 1 . in essence, we model adaptation as an iterative process that alternates between (1) strf perturbations that improve discrimination between speech and nonspeech sounds and (2) updates to the parameters of a linear discriminative model."
"our model predicts that targeting speech versus nonspeech distractors enhances sensitivity of strfs to \"slower\" spectrotemporal modulations. this is illustrated by the average difference modulation profile in fig. 3 . increased slowness in the modulation domain is realized in the time-frequency domain as an overall broadening and reorientation of the strfs, and reflects an enhancement of the modulations known to characterize speech and other natural sounds [cit] . the fact that we obtain improved sad results using \"slower\" filters is consistent with other strategies that concentrate the feature extraction pipeline to the range of speech-specific modulations [cit] . moreover, the strf adaptation patterns observed here are broadly compatible with traditional signal processing schemes that emphasize slow modulations for improving noise robustness in speech tasks [cit] . the distinction here is that our approach adapts the filter shapes \"on the fly\" and, as our sad results suggest, such changes can be compatible with an existing statistical model to improve task performance."
"on various principles and properties of homomorphic encryption is given and then various homomorphic algorithms using asymmetric key systems such as rsa, elgamal, paillier algorithms as well as various homomorphic encryption schemes such as brakerski-gentry-vaikuntanathan (bgv), enhanced homomorphic cryptosystem (ehc), algebra homomorphic encryption scheme based on updated elgamal (ahee), non-interactive exponential homomorphic encryption scheme (nehe) are investigated."
"to better understand how the adapted strfs improve performance, panel b shows an analysis of the distribution of llr scores for the strf ensembles with respect to speech and nonspeech categories. these results show that under the gmms trained on the passive strf features, use of the adapted strf ensemble increases the overall likelihoods of speech and nonspeech. however, despite this added bias to the scores, there is an overall improved separation between the speech and nonspeech distributions as computed using the kullback-leibler divergence ( and assuming gaussiandistributed scores). similar improvements are found across the other noise scenarios."
"for the red blood cell visualization, we used a modified version of the erythrocyte metabolic reconstruction iab-rbc-283 [cit], which had been previously used for building personalized kinetic models [cit] . the model iab-rbc-283 is a full bottom-up created reconstruction of the human red blood cell with 292 intracellular reactions, 77 transporters, 267 unique metabolites and accounts for 283 metabolic genes, which suggest that the metabolic role of erythrocytes is much more diverse and expansive than previously presumed. an existing layout of the (human) red blood cell (rbc) metabolic network was used [cit] ) . the model is freely available on the bigg models database [cit] ."
"finally, panel c summarizes the overall performance of the various sad configurations in terms of average eer across all noise conditions, showing that the adapted strfs improve over the baseline and passive strf results by an absolute 8% and 3.5%, respectively."
"the size of the metabolites can represent either the absolute quantity of one metabolite itself or the relative quantity of this metabolite compared to all others. for more in-depth analysis the absolute quantity seems to be more helpful, however. for instance, if the data points represent metabolic concentration, small changes in concentration would not impart massive changes in node size over time, thereby aiding visualization by allowing significant changes in metabolites' concentration to be more easily identified within the network."
"next, upon convergence of (p2), and assuming the minimum lies within the feasible set formed by the constraints on, the adapted strf modulation profiles can be written as (3) eq. (3) shows how the strf adaptation patterns are consistent with a contrast-matched filter in the modulation domain. first, task-driven strf plasticity directly reflects the spectro-temporal modulation profiles of the speech and nonspeech stimuli, as shown in the averaging term. the impact of each stimulus sample on adaptation is proportional to the difficulty of predicting its corresponding label. importantly, because we have constrained the regression coefficients to be positive, we are guaranteed that speech modulations are enhanced whereas those from nonspeech are suppressed. finally, the first term acts to resist changes from the initial strf modulation profile, with the magnitude of the effect being controlled by and ."
"2) proposed sad system: firing rates are computed as in eq. (1), with 128-channel auditory spectrograms and cube-root compression applied. the strfs are also interpolated to span the full 128 channels. post-processing and dimensionality reduction are performed as described in section iii-a."
"acknowledgment many thanks to shihab shamma, jonathan fritz, and the neural systems laboratory at the university of maryland, college park for providing the ferret strfs used in this work."
"a rule-based method using ps values at 0.179 and 0.357 cycles per millimeter, corresponding to the spatial frequencies of nodular patterns, was used for identification of obviously normal or obviously abnormal rois. figure 2a, b shows the relationship between the normalized ps values of 0.179 and 0.357 cycles per millimeter of the principal and secondary axes. the distribution for the normal rois was shifted to the lower left, whereas that for the abnormal rois was shifted to the upper right. there was a considerable overlap between the two distributions because lung textures in the abnormal rois can consist of complicated texture patterns (nodular, reticular, and reticulonodular patterns), and some normal areas were spread throughout the entire lung. if the values for the abnormal rois were higher than that of the maximum normal roi, these abnormal rois were classified as \"obviously\" abnormal. if the values for the normal rois were lower than that of the minimum abnormal roi, these were classified as \"obviously\" normal."
"the evaluation of the average number of daily flushes per capita could be considered satisfactory to accurately model daily water demand for toilet flushing; nevertheless, these observations may not be universally applicable to all rwh systems. thus, a precise modelling of water demand patterns for toilet flushing is required."
"therefore, the integration of rwh systems into residential buildings is an effective way to minimize the use of treated water for non-potable tasks and supply drinking water in places where water is scarce. however, the performance of rwh systems is highly affected by the spatial and temporal distribution of the rainfall in the area of interest. for this reason, rwh by itself is not able to supply water for all domestic uses and make the householders independent from the conventional water supply system. thus, in order to achieve water self-sufficiency, multiple technologies are needed (willuweit & o'sullivan ) ."
"only in a limited area in the northwestern part of the island in this analysis, a behavioral model was applied in order to investigate the performance of an rwh system in terms of its reliability. water demand for toilet flushing and in this analysis some assumptions have been made in order to estimate water demand for garden irrigation."
"for each value of the tank capacity in the range 1-30 m 3, the average daily value of e t related to the entire analyzed period has been calculated. the box-whisker graphs in figure 2 subsequently, the tank capacities for which three different thresholds of e t could be reached, specifically 75, 85 and 95%, have been identified. maps in figure 3 show the spatial distribution of e t when rainwater is used for toilet flushing."
"where w t is the inflow volume supplied to the tank at time step t (m 3 ), φ is the runoff coefficient depending on water loss (dimensionless), r t is the rainfall at time t (m), a tot is the total catchment surface area (m 2 ), and a is the effective impervious surface area (m 2 ). in the analysed rwh layout, the tank is installed underground, thus the evaporation losses from the tank can be neglected. in this analysis, φ was set equal to 0.9 (wisner & p'ng )."
"in the present study rainfall volumes were calculated using the daily rainfall series recorded from 111 rain gauges over table 1 summarizes the total mean monthly rainfall, the mean daily rainfall and the"
"further developments of this study could include a more accurate estimation of this demand, in terms of temporal pattern and water amount, for example by means of a water balance model."
"in some northern coastal areas this threshold can be reached with a capacity ranging from 2 to 8 m 3, nevertheless in the remaining part of the island, higher capacities are required."
"nevertheless, the collection and use of rainwater through rwh systems can provide a considerable amount of water and, consequently, significant economic savings to householders. due to its characteristics, the collected rainwater can be utilized for different non-potable uses, such as toilet flushing, washing machine use and garden irrigation (or any other use that does not require high-quality water)."
"at a sensitivity of 95%, the specificity was 83.7±7.9% for the rule-based plus ann method which was larger than the value of 79.2±10.3% for the ann method alone. the classification performance in the present study was slightly lower than those in previous studies of cad for the classification with interstitial lung disease [cit] . this is because the overall classification performance was affected by false negatives, which may be classified incorrectly as obvious rois in the rule-based method. therefore, to improve classification performance, further advances in cad systems are required. for example, each ann is trained independently for typical texture patterns and different types of normal lung (small round opacities and soft tissue opacity, etc.) [cit] . a multi-ann may show the highest classification performance. for the highest classification performance on ann, the training took a cpu time of approximately 180 s. in comparison with mtann using chest radiographs [cit], the training times here were short, as the number of input data (32 units) in this study was smaller than that of the input data (81 units) in mtann. however, it was necessary to obtain large training samples (112 rois) in this study. therefore, by obtaining a large number of overlapping subregion rois with smaller matrix size, it may be possible for ann to obtain high classification performance to distinguish between abnormal and normal lung with fewer cases."
we have developed a rule-based plus pattern recognition technique based on the ann for classification of pneumoconiosis on chest radiography. our cad system based on ps would be useful to assist radiologists in the diagnosis of pneumoconiosis.
where t is the total time period under consideration. e t represents the overall water savings that can be achieved by harvesting and using rainwater.
"the modelled rainwater tank is filled exclusively using rainfall volumes collected from a building's rooftop, courtyard and pedestrian areas. under the hypothesis of constant rainfall within each time step t, the rainwater volume can be evaluated using the following expression:"
"the ann was used in many fields as a powerful classification tool. specifically, anns have been applied for distinction between lesions and non-lesions [cit] and for distinction between malignant and benign lesions [cit] in cad schemes. a multi-massive training artificial neural network (multi-mtann) reduced the false positive rate of cad schemes from 4.5 to 1.4 false positives per image at an overall sensitivity of 81.3% [cit] . the training of each mtann in the multi-mtann was performed 500,000 times. training took a central processing unit (cpu) time of 29.8 h on a pc-based workstation [cit] because the training region in input images was divided on a pixel-bypixel basis into a large number of overlapping subregions [cit] . in addition, it was found that the ann could distinguish clustered microcalcifications from normal nonclustered areas based on the ps values. however, there have been no studies regarding the distinction between normal and abnormal patterns in pneumoconiosis using the ann trained with the ps values. therefore, we have developed a cad system based on the rule-based plus ann method for distinction between normal and abnormal regions of interest (rois) selected from chest radiographs with and without pneumoconiosis. to investigate the basic characteristic of the trained ann, we also investigated the effects on overall performance with simulated lesions using various parameters."
"generally, the performance of rwh systems is described in terms of reliability, expressed as the total actual rainwater supply over water demand e t :"
"in particular the quantity and quality of collected rainwater depends on geographic location, local climate characteristics, presence of anthropic activities in the area and storage tank volume. thus, the storage capacity of the collecting tank cannot be standardized, but is highly influenced not only by the above-mentioned factors, but also by the characteristics of the catchment surface and the number of householders. nevertheless an optimal size can be identified on the basis of the system reliability or economic criteria (liaw & tsai ) . fewkes () carried out some analysis on residential rainwater tanks in the uk, producing a series of dimensionless design curves which allows the estimation of the rainwater tank size required to obtain a desired performance given the roof area and water demand patterns. khastagir & jayasuriya () defined a relationship for optimal sizing of rainwater tanks considering the annual rainfall, the water demand, the catchment area and the desired supply reliability valid for in the present study, the reliability of a rwh system for a single-family house in a residential area with four inhabitants has been assessed, considering three different uses of rainwater: toilet flushing, garden irrigation and these two concurrent uses. the system performance has been tested for different catchment surfaces, tank sizes and mean annual precipitation using data from over 100 different sites in sicily. in order to define a temporal pattern for flushing water demand, water consumption data from single-"
"evaluation of water saving efficiency at daily scale considered. simulations were conducted on a daily scale, in order to take into account the effect of extreme rainfall of 24 hours duration and droughts on the system performance."
"the rule-based method was employed for the identification of obviously normal and abnormal rois. the rule-based method was applied to the total number of abnormal and normal rois (112 abnormal rois and 112 normal rois). the ann method was applied for the classification of the remaining rois, which were not classified as obvious cases by the rule-based method [cit] . thus, the rule-based plus ann method was applied to the remaining abnormal and normal rois (54 abnormal rois and 89 normal rois). we randomly divided the data into 54 abnormal rois and 89 normal rois, and obtained the training data (27 abnormal rois and 44 normal rois) and the non-training data (27 abnormal rois and 45 normal rois). finally, the average classification performance for the rule-based plus ann method was determined using five different training data and non-training data sets. the statistical significance of differences between roc curves was determined by the application of a two-tailed paired t test to the az values of the non-training data sets."
"zoysia japonica compadre, a turfgrass more resistant to warm climates than other species. it was also assumed that the garden was only irrigated every 3 days during april, may and september, and on alternate days from june to august. the potential and actual daily evapotranspiration and the irrigation frequency for each month are reported in table 3 ."
"commonly, rwh systems include three principal components: the catchment area, the collection device and the conveyance system. the catchment area usually consists of rooftops, courtyards or other compacted or treated surfaces."
"where ae is the actual evapotranspiration in mm/day. turf- optimum irrigation frequency depends on the geographic location, the plant species, the soil type and the climatic conditions. in this study, the frequency of irrigation was defined based on practical considerations and previous marchione ). the need for further specific information to define the optimal irrigation frequency for turfgrass required some assumptions to be made in this study."
"we obtained 49 rois with a fully simulated abnormal pattern, 73 rois with a partially simulated abnormal pattern, and 142 rois with a non-simulated abnormal pattern in the nontraining data. figure 4a shows roc curves for the distinction between the fully simulated abnormal rois and fully and partially abnormal rois and the non-simulated abnormal rois with the non-training data. the az value (0.991± 0.001) for the fully simulated abnormal rois was larger than this value (0.8398) for the fully and partially simulated abnormal rois. in addition, comparing the az value for the fully simulated abnormal rois with those for the fully and partially simulated abnormal rois, the classification performance on the fully and partially simulated abnormal rois was decreased by 0.158. however, using only the training data with and without the fully simulated abnormal patterns, the ann method distinguished between the fully and partially simulated abnormal rois and the non-simulated abnormal rois. thus, the ann method showed high classification performance regardless of where the partially abnormal patterns (nodular patterns, reticular patterns, etc.) were placed in the rois."
"it has to be remarked that the analysis here proposed is not intended to be exhaustive. indeed, before installing an rwh system, a cost-benefit analysis needs to be carried out, in order to verify if the achievement of a certain threshold of reliability is not too costly and, therefore, economically disadvantageous. this study can be considered as a preliminary analysis to identify in which areas the installation of an rwh system could provide benefits in terms of water saving efficiency and at which level."
"where w dt (m 3 ) is the volume discharged as overflow from the storage tank at time step t, v t (m 3 ) is the volume stored at time step t, y t (m 3 ) is the yield of rainwater from the storage tank at time step t, d t (m 3 ) is the toilet and grass irrigation water demand at time step t, and s (m 3 ) is the tank storage capacity."
"in summary, the study showed that in some areas of sicily rwh systems, integrated with conventional water supply systems, can provide significant water savings. for this reason, incentives and government support should be promoted in order to encourage householders to install rwh water systems in residential urban areas."
"the frequency of irrigation depends on many factors, such as the type of grass to be irrigated, the soil properties and the climatic conditions in the area of interest. in order to define the water demand for garden irrigation, it was assumed that the garden area (200 m 2 ) of the modelled single-family house was planted with turfgrass. therefore, the mean monthly reference evapotranspiration (re) value was evaluated for the area of study using the thornthwaite formula (thornthwaite ). water use for turfgrasses was estimated using a correlation factor, the crop coefficient k c, as follows:"
"once rainwater has been intercepted, it can be filtered or subjected to other simple treatments (such as chlorination), then collected in storage tanks to be used. the advantages of rwh systems are numerous:"
"in this study, the toilet flushing demand pattern was nevertheless, in this case, the simulation of the water balance of the rwh system would have required precipitation data at a sub-daily scale that are not available for all the rain gauges considered in this study. moreover, this study focused on the performance of the rwh system in terms of average annual water saving efficiency."
"despite its efficiency, this method requires that the multiple sources have an identical angular distribution. moreover, it estimates the angular parameters (central doas and angular spreads) together with a 2d search."
"this technique is based on the approximation of the covariance matrix using a few noncentral moments of the source angular power densities. in fact, the covariance matrix r xx can be approximated as follows [cit] :"
"consider an ofdma system where a set of users transmit their signals to the uplink receiver. assuming the timing mismatch between users to be smaller than the length of cyclic prefix (cp), the received signal after cp removal can be expressed by the following matrix form [cit]"
"these figures illustrate the result proved in the first example. the proposed algorithm offers an excellent doa tracking performance and outperforms the fapi-tls-esprit method at low snr values. moreover, when we compare the estimation error of the first central doa made by our proposed method in fig. 3 to that in fig. 8, we note that this error in the first case is lower than the corresponding error in the second case. the same holds for figs. 8 and 9 of the second example, we see clearly that the performances of the fapi-tls-esprit method increase in particular at high snr values. therefore, we can conclude that the improvement made by our method with regard to the fapi-tls-esprit method is more prominent for low snr values."
"while the principle of the proposed cp-ofdm receiver will be introduced in section iii-b, it is worthwhile to highlight that the proposed receiver can be regarded as the generalized version of the conventional cp-ofdm receiver. the cfo and fft component in the conventional cp-ofdm receiver is generalized by using the linear layer, and the demodulator is now replaced by the multi-layer neural network. more remarkably, every component in the cp-ofdm receiver is optimized individually. thanks to the use of neural network (nn) structure, every layer in the proposed receiver will be jointly optimized through the deep learning algorithm introduced in section iii-c. it will be shown that the joint optimization makes a big difference for the frequency synchronization."
"equation 18 represents an approximation of the exact covariance matrix r xx using kr + 1 matrices. note here that (19) is obtained using the r-term taylor series approximation of c(θ) aroundθ k . in practical situations, r should not be much larger than 1. the approximation (18) is used to estimate the different noncentral moments and then derive the central doas from the first noncentral moment. in fact, we consider the following vectors:"
"in this subsection, we propose a simple covariance-fittingbased doa tracking scheme that tracks the central doas of multiple uncorrelated id sources. the implementation of our new approach involves the two following functional structures:"
"consider a uniform linear array of l identical sensors (i.e., with the same gain, phase, and sensitivity pattern). the array receives signals distributed from k id narrowband 1 far-field sources with the same central frequency ω 0 . then, the output of the lth array sensor can be modelled as a complex signal as follows [cit] :"
"in orthogonal frequency-division multiple access (ofdma) uplink communications, every user to the receiver link may suffer carrier frequency offset (cfo), which introduces inter-carrier interference (ici) as well as mutual interference between users. concerning the cfo for every user to be random and independent, it is often hard to conduct frequency synchronization at the receiver side; and this is referred to the well-known multi-frequency synchronization problem [cit] ."
"note here that if the matrix q(θ ) is singular, we can replace its inverse by pseudoinverse. consequently, the central angles can be estimated as follows:"
"it might also be possible to conduct frequency synchronization at the transmitter side through the use of cfo pre-compensation approach, which turns the multi-frequency synchronization problem into a much simpler single-frequency synchronization problem [cit] . this approach lies in the hypothesis of accurate cfo knowledge at the transmitter side, which is however hardly the case in practice due to the feedback delay and cfo dynamics. moreover, the feedback for multiuser cfos introduces extra signalling overhead, which is not favourable for advanced wireless applications such as 5g, iot or tactile internet that are demanding for a short frame length [cit] ."
"given the receiver-side knowledge of cfos, frequency synchronization might be applied onto each individual user's signal, which is obtained through the use of filter-bank [cit] . however, the filter-bank approach cannot capture the signal energy leaked into others' user band; and this is the main reason to cause imperfectness for the frequency synchronization. a possible way to enhance the filter-bank approach is through the combination of iterative multiuser detection and frequency synchronization [cit] . the fundamental bottleneck lies in the demand for accurate cfo and multiuser channel estimation as well as the decoding accuracy."
"are the array response and sensor noise vectors, respectively. we also define the angular auto-correlation kernel for the kth source as the conjugate auto-correlation function between the signals distributed from the kth source and impinging on the array from two different directions θ and θ as follows:"
"once the estimated central doas are obtained, we then propose a kalman-filtering-based tracking algorithm to model the dynamic property of directional changes for the sources. the kf ensures the association between the estimates made at different time points thanks to its predictability characteristic. indeed, at each stage during the tracking process, the central doas predicted by the kalman filter are used to smooth the central doas estimated via the covariance-fitting-based algorithm."
"in order to model the dynamic directional changes for the moving sources, we propose a kalman-filtering-based tracking algorithm. at each stage during the tracking process, instead of using the array output directly as a measurement, we propose to consider the current estimates of the central doas obtained with the covariance-fittingbased algorithm described in the previous subsection as measurements, predicted and updated via the kalman state equation. to do so, we define the state vector for the kth source as follows:"
"therefore, we conclude from (17) that in order to estimate the central doa of the kth source, one should calculate an estimate for the first noncentral moment of its angular power density. to do so, one use a simple covariance fitting optimization technique."
"to justify the use of the kalman filter, we should show that the estimation error caused by the covariance-fittingbased algorithm is gaussian distributed. in fact, we have from (35) that the estimated central doa of the kth source θ k is obtained as follows:"
-central doa estimation via the covariance-fitting-based method : we estimate the central anglesθ (t) via the covariance-fitting-based algorithm as described in the previous subsection. -updating the covariance matrix of the measurement noise vector r(t): we calculate the variances of the measurement noise
"as in the proposed kf tracking scheme, instead of using the array output directly as the measurement process, we use the most current data to form the doa estimates via the covariance-fitting-based algorithm, the measurement noise is then due to the estimation inaccuracy of the covariance-fitting-based method."
"in this paper, we developed a new method for tracking the central doas assuming multiple incoherently distributed (id) sources. this method is based on a simple covariance fitting optimization technique to estimate the central doas in each observed time interval. it also uses the kalman filter to model the mobility of the sources and track the different doas during the tracking period. our method was compared to the fapi-tls-esprit algorithm using the tls-esprit method and the subspace updating via fapi-algorithm in different scenarios. we proved that this new method outperforms the fapi-tls-esprit method. the improvement made by our method with regard to the fapi-tls-esprit method is more prominent for low snr values. we also showed that the proposed method can better track the central doa when the source is gid or lid than when it is uid. therefore, our method depends on the form of the angular distribution function when tracking the central doas. moreover, we proved that the more the sources are spaced, the more the proposed method can exactly track the central doas. finally, when the number of sources increases, the performances of our proposed algorithm decrease. 1 we assume that the delay spread caused by the multipath propagation is small compared to the inverse bandwidth of the transmitted signals. this means that the narrowband assumption is valid in the presence of scattering."
"where · denotes the euclidean norm. this maximumlikelihood approach is optimum, however it does not take advantage of the ofdm structure, and its computational complexity scales exponentially with the block size m . it is also possible to employ the iterative frequency synchronization and multiuser detection algorithm to yield a sub-optimum solution [cit] . however, the computational complexity is still considerably high, and the frequency synchronization performance highly depends on the cfo and channel estimation accuracy."
"it is worth noting that the assumption a1) holds when users within the same mobility range are grouped into the same frequency band using the network slicing, which is one of promising candidates for the 5g mobile technology [cit] . moreover, section iv will show that the bias δ is large enough to support this assumption."
"experiment 2: the objective of this experiment is to evaluate the dnn-based receiver in the single-user ofdm system. throughout the simulation, we employ the receiver structure shown in fig. 3 (b) for the frequency synchronization; the similar performance can be obtained for the receiver structure shown in fig. 3 (a) . the ber performance is obtained by averaging over sufficient monte-carlo trials. the ber performance for the awgn channel is depicted in fig. 5, and that for the mobile fading channel is depicted in fig. 6 . to facilitate our comparison, we also plot the theoretical-ber for bpsk/16qam as well as the cfo-free cp-ofdm performance (considering the conventional cp-ofdm receiver) in both figures. simulation results show: 1) there is a gap between the theoretical-ber for bpsk/16qam and the ber for the cp-ofdm system. this gap is due to the power consumption for cps."
this paper has presented a novel dnn-based cp-ofdm receiver to handle the multi-frequency synchronization problem inherent in ofdma uplink communications. the proposed receiver was well optimized for both single-user and multiuser ofdm systems through the machine deep learning. our computer simulations showed that the proposed receiver is able to offer almost cfo-free performance for both single-user ofdm and ofdma systems operating at a wide range of snrs without the need for accurate cfo estimation.
"note that the user index l is omitted as far as the single-user case is concerned, and we only consider the noiseless case for convenience. moreover, the block s is the frequency-domain symbol block. our interest is to understand: to what extend s can be determined given y (1) . this defines the feasibility condition for the classifier to work appropriately."
"where the channel equalization component is simply the channel inverse λ −1, and the linear layer in (5) is responsible for the cfo compensation as well as the time-to-frequency domain transform. the term linear layer comes from the concept of neural network [r12], where w (1) is the weighting matrix of this layer, and b"
"where m 1k (θ k ) represents the ((k − 1)r + 2)th element of m(θ ). then, we have the following estimation error that also represents the measurement noise of the kth source:"
"where m 1k (θ k ) is the first estimated noncentral moment of the angular density of the kth source obtained from m(θ). moreover,θ k is an arbitrary doa that should be chosen sufficiently close toθ k to reduce the estimation errors."
"otherwise, we verify from (28), (30), (31), and (32) that q −1 (θ) depends only on c ij (θ k ) for fixed values of i and j. then, this term is deterministic. while we have from (29) and (33), p(θ ) depends on the estimated covariance matrix:"
"we assume that the sources are id. this means that for each source, the signal components arriving from different scatterers are uncorrelated. therefore, we have for the kth source [cit] :"
"to obtain a more accurate value of the estimated doa, we replaceθ k by θ k already calculated, and we solve again (27) to obtain the estimates of the central moments m( θ ). finally, we obtain a new value of the estimated central angle from the first estimated central moment m 1k ( θ k ). this operation is repeated a few times. these estimates will be treated as measurements and provided to the celebrated kalman filter to track the doas."
"finally, n l (t) represents the additive zero-mean, gaussian distributed, circular, spatially and temporally white noise (i.e., uncorrelated between the receiving antenna elements and between different snapshots)."
"furthermore, w k (t) is the process noise vector caused by external circumstances like wind and bumps in the road. it is assumed to be gaussian distributed with zero mean and covariance matrix:"
"as we are interested to the crb of the central doasθ which we denote as crb(θ ), we use the inversion of block matrices of lemma [cit] to obtain the following expression of crb(θ ):"
"2 [cit] . mathematically, the first step of life cycle inventory (lci) analysis is the determination of the scaling vector s. the components of this vector scale processes of a system up or down such that the output of unit processes exactly matches the final demand. the final demand vector f includes the reference flows r, which are defined within the goal and scope of a lca study. the scaling vector can be determined by the multiplication of the inverse of technology matrix a with the final demand vector."
"where the components of vector p specify the profit of processes j. [cit], but differs in the way that only those costs are taken into account, which are relevant for the investment decision for the biorefinery options. this choice is concerned with taking the view of potential investors maximizing the profit of a biorefinery. thereby, only the costs are considered that are relevant for the decision on the profitability of the biorefinery (including costs for transportation and additional inputs). besides to this simple lcc approach, however, there are further possibilities to consider cost information by the model. for example, it might also reasonable to explicitly take into account the profit for reference processes competing with biorefineries, or to use discounted cash flow analysis taking into account the time value of money (e.g., net present value). however, additional data must be collected. for simplicity reasons, we chose the standard approach of lcc in this study."
the objective of this paper is to evaluate the performance of the mobile relay architecture where a train is loaded with several users using hypertext transfer protocol (http) and voice over internet protocol (voip) services.
life cycle impact assessment (lcia) transforms the interventions into more understandable impact indicator results. the impact vector h can be calculated by multiplying the matrix of characterization factors q with the inventory vector.
"the program is designed as an lca for a final demand f r on reference flows r, which are characterized by the products of the biorefinery system and the corresponding reference products (biomethane/natural gas, bio-based ethylene/fossil ethylene, hydrolysis lignin/lignite briquettes, ethanol/petrol, organosolv lignin/polyol). the amounts for the multiple components of the functional unit are taken from statistical sources (e.g., [cit] and represent those of a current german demand. other components of the final demand vector f are set to zero. the design of an lca for a demand on the functional unit does not take into account the linkage of downstream processing with the modeled upstream processing so that economy-wide impacts are cut off [cit] . however, the modeling in regard to the functional unit is a general principle of lca [cit] . to ensure the balance between supply and demand of transported beech wood, the constraint 0"
"due to the reduced wood availability in the 50% scenario, regions supplying the feedstock to biorefinery 1 in leuna are distributed throughout germany (fig. 3b ). hence, a large catchment area is required to fulfill the demand of the biorefinery in the scenario with reduced residual beech wood availability."
"the non-dominated solutions of the 50% scenario are illustrated in fig. 5 . the most beneficial solution p1 in terms of impacts on climate change is given by the biorefinery 1 located in leuna. biorefinery 2 in leuna is the most profitable solution p3. contrary to the 100% scenario, the compromise solution p2 is near to p3 suggesting biorefinery 2 in böhlen. an interesting fact is that p3 (in both scenarios) is the solution of the goal programming approach (sect. 3.4.1). however, the identification of all non-dominated solutions, even if the solution is nearly located to another such as p2 in the 50% scenario, clearly is an advantage of a posteriori approaches over a priori methods."
"to determine the value added in terms of a reference flow, the elements of the scaling vector s can be multiplied by the value added of the corresponding processes, where 1 is the summation operator (vector of ones)."
"the aim of this paper is, hence, to demonstrate how lca can be extended by the use of linear programming (i) to determine the optimal choice between new technologies, (ii) to identify the optimal region for supplying the feedstock, and (iii) to deal with multifunctional processes without specifying a certain main product. [cit] and create a rectangular mixed integer linear program and carry out a case study to illustrate its applicability. thereby, two wood-based biorefinery concepts are analyzed in terms of the optimal consequences for the total system under study. furthermore, we modify the original linear program into a weighted goal programming problem and a pareto optimization model those extend the environmental focus on sustainability by adding the economic perspective through a life cycle costing (lcc) approach."
"to our knowledge, there has been as yet no systematic examination of interactive and voip services in mobile relays. the question remains, however, how those services perform under a mobile relay architecture in loaded conditions. an lte/epc mainly includes the following nodes: the evolved node b (enb), the mme, serving gateway (sgw), packet gateway (pgw), and the home subscriber server (hss)."
"the study concerns two exemplary biorefinery concepts. [cit] . the first biorefinery concept produces ethylene, organosolv lignin, and biomethane. the second configuration produces the same products as the first one except for ethylene that is replaced by ethanol. the annually produced amounts of the multifunctional biorefinery processes are illustrated in table 1 . the goal of the exemplary case study is threefold. firstly, we want to determine the biorefinery that is more appropriate to reduce environmental impacts in regard to the current production system. secondly, we want to determine at which site the biorefineries should be located ( fig. 1 ). and finally, to identify districts for supplying woody biomass feedstocks. for simplicity reasons, we focus on four chemical parks of the central european chemical network 1 as potential plant locations, which would in principal allow the building of the two biorefinery concepts: leuna, böhlen, zeitz, and schwarzheide. we assume that all chemical parks can provide the required infrastructure and utilities in the same manner. furthermore, we suppose that either one or none of the biorefinery concepts can be built at a location. data on residual beech wood are estimated at the level of 37 administrative districts in germany according to polley and kroiher (2006 this residual wood is currently not used and remains in forests after chopping, which might be possibly used as feedstock for biorefineries without inducing feedstock rivalry [cit] ). since the 100% use of available beech wood residuals is quite optimistic, we introduce a second scenario in which the availability in each region is reduced by 50% (cf. [cit] ). the transport distances from the districts to the four potential plant locations can be covered by train and/or truck. in this study, we assume that up to 200 km, the wood is transported only by truck. for distances greater than 200 km, the wood is additionally transported by train. thereby, we further assume that a minimal distance (between 10 and 30 km depending on the availability of train network) is covered by truck. varying transport distances for other preproducts of the biorefineries are neglected."
"the so-called multifunctional problem arises if a process generates more than one product and if and only if the delivered functions are not used in the same proportion in the system [cit] . the most common approaches to solve the multifunction problem can be classified into partitioning method and substitution method (heijungs and guinée 2007) . by applying these procedures, the technology matrix a becomes square, since an equal amount of rows and columns exists. the partitioning method divides the multifunctional process into processes that are mono-functional by the use of allocation factors. several ways to gain these factors are possible, e.g., due to physical or monetary characteristics of the products. the substitution method requires the definition of a mono-functional process, which provides the avoided product to the multi-functional process. adding this process to the technology matrix a makes its inversion possible."
"to study the origin of wood supply it is possible to analyze the corresponding scaling factors of the transport processes j t . these factors represent the contribution to the overall wood demand of the biorefineries. figure 3a illustrates the solutions for the 100% scenario. thereby, the biorefinery in zeitz is delivered from regions in the middle west of germany and from regions in the middle south. in contrast, the biorefinery in schwarzheide is delivered by regions in the northeastern part of germany."
"the overall program can be summarized by the formulas 8 to 11 considering that the scaling factors of biorefinery processes s 1, s 2, s 3, s 4, s 5, s 6, s 7, and s 8 are integers. thereby, eq. 8 represents the objective function of minimizing environmental impacts. equation 9 illustrates the system under study including all potential biorefineries, feedstock regions, and transportation alternatives. equation 10 includes the capacity constraints for feedstock availability in the regions. equation 11 considers the fact that only one of the two biorefinery options can be built at a potential location. by using the linear programming model, we are interested in the reduction of the total environmental impacts of the current system. hence, the objective function results from eqs. 2 and 3, and the target is to find a vector s that minimizes the total impacts h of the system, which is expressed by matrix a. a visualization of the modeling principle including the sub-matrices of a is given in fig. 2 . the fundamental principle of the model is to check whether environmental impacts can be reduced by biorefinery 1 or 2 considering 4 potential locations (a brs ) and 37 regions of wood supply (a s ). if not, the model would only choose current processes (a ref ) to provide the components of the final demand."
there can be several general networks that use the same lte/epc track network in order to allow all mobile operators of a country to provide the service on-board. there can be several enbms (one per operator) but radio access network (ran) sharing can also be used to avoid a duplication of embedded hardware in trains. note that figure 1 shows a functional architecture. the operator of the track network can be a mobile operator and several entities can be physically in the same node (e.g. mmet and mmeg are in the same piece of equipment)
"for performing our tests we used network simulator version 3 (ns-3) [cit] . this is a discrete-event network simulator for internet systems that has a well developed lte module for the lte model includes the lte radio protocol stack (rrc, pdcp, rlc, mac, phy) and the epc model includes core network interfaces, protocols and entities. these entities and protocols reside within the sgw, pgw and mme nodes, and partially within the enb nodes. we use the standard ns3 lte model with direct transmission between ues in the train and the network as the reference mode."
"a disadvantage of goal programming might be the requirement of defining of preferences at the beginning of the solution process. alternatively, a posteriori methods are available to prioritize between goals after the generation of all pareto efficient solution. in doing so, epsilon-constraint method has been already used in the field of lca (e.g., [cit] ) . here, in this study, we apply the epsilon-constraint method in gams [cit], b) . thereby, the target"
"the protocol stack when a mobile relay is used is shown in figure 2 on the radio interface and on the s1-interface. due to the use of two lte/epc networks there is an additional level of encapsulation and thus extra-headers compared to a standard architecture. the following headers are added to each packet: 8-12 bytes for gtp [cit], 8 bytes for the user datagram protocol (udp) and 20 bytes for the internet protocol (ip) (with ipv4). the gtp header length is 12 only when packet numbering is activated in gtp. we consider the worst case and thus assume a 12-byte header, which gives a 40-byte total overhead. as the length of the internal packets is variable, an important question is to determine whether this 40-byte overhead is negligible or not compared to the user data average size for popular services such as telephony and http. in this section, we analyse the possible benefits brought by the deployment of mobile relays. we consider two representative services, namely client-server requests and telephony."
"eq. in the 100% scenario and 2.38 mt/a co 2 eq. in the 50% scenario. the contribution analysis for the 100% scenario shows that current substitutable processes still provide the major part of the overall ghg emissions (lignite production 88.48%, natural gas production 8.24%, petrol production 1.95%, ethylene production 1.05%, polyol production 0.25%) whereas the biorefinery system plays a minor part (biorefineries 0.12%, wood production − 0.20%, other upstream processes including transport 0.12%). due to the feedstock availability of german beech wood residuals, only a small ratio of current ghg emissions could be reduced. detailed information on contribution analyses is provided in the electronic supplementary material (milp_lca.xlsx)."
"it is worth mentioning that all these elements are standard lte components, and that the use of this architecture requires no adaptation of protocol stacks or hardware."
"pareto optimal solutions are solutions that cannot be improved in any of the two objectives without degrading the other objective. the pareto optimal solutions of the 100% scenario are illustrated in fig. 4 . in this example, three pareto optimal solutions are identified. the dominated solutions, which are not pareto optimal, are below the blue line. however, more comprehensive problems can result in various non-dominated solutions. solution p1 suggests biorefinery 1 (ethylene) in zeitz and in schwarzheide. in contrast to this environmentally most preferable solution, p3 contains the most preferable solution in terms of profit (table 4) . thereby, biorefinery 2 (ethanol) is located in zeitz and schwarzheide. the solution p2 can be interpreted as a compromise between these extreme solutions of maximal profit and minimal impacts on climate change. furthermore, the solution may become worthwhile due to the fact that both biorefinery concepts (ethylene and ethanol) would be built in zeitz and schwarzheide. this mixture of technologies might be also interesting for decision-making. in contrast to goal programming in which only one solution is determined in accordance with the a priori defined priority order, pareto optimization allows the decision-maker to deal with tradeoffs after investigating the non-dominated solutions."
"in 4g networks, telephony is based on voip. end users are generally sensitive to the packet loss rate and the one-way delay. itu-t g.114 [cit] recommends a maximum of 150 ms one-way latency in order to avoid any problems due to possible echoes and to keep excellent interactivity between the two parties. since this includes the entire voice path, part of which may be on the public internet, the lte/epc network should have a transit latency less than 150 ms. as for the interactive service, using a relay increases the one-way delay."
"the wireless access for passengers is provided by the use of two nested lte/epc networks as shown in figure 1 . the first one is called the track evolved packet core (epct) and acts as a backhaul network. nodes of this network are identified by suffix t (enbt, mmet, sgwt, etc.). we name a terminal of this network a transport operator terminal equipment (toe). there is typically one toe in every train. the second network is the general network. it manages the ues of passengers and could be owned by operators. it has enbm as well as standard enb. note that enbms have access to the general network through the track network: an enbm is connected to a toe and can thus exchange messages with global mobile management entity (mmeg) and global serving gateway (sgwg)."
"in this section we first analyse the qos performance of http client-server file transfer by regarding several parameters such as: file load time, server response time, and throughput. secondly we look at the voip performance based on qos parameters such as: plr and end-to-end latency."
"electronic supplementary material the online version of this article (https://doi.org/10.1007/s11367-019-01650-6) contains supplementary material, which is available to authorized users. determine how environmental flows will change due to the consequences of a decision [cit] . in contrast, attributional lca focuses on describing the life cycle of a product by attributing the environmental flows to the product under study."
"the maximum load time is reduced by the use of a relay from 16 s to 4 s for 60 users and from 39 s to 13 s for 120 figure 10 shows the distribution of the latency. since in the relay architecture packets have more hops, it increases the latency from about 25 ms to 25-37 ms. as the latency is less than 40 ms, this guarantees a good voice calling interaction since the latency threshold is 150 ms. furthermore, a simple proportional-fairness scheduler is used. it does not give any priority to voip packets. by using a dedicated bearer for voip packets on the backhaul, it is possible to give a higher priority to voice packets and to limit the latency increase. the plr for voip flows was measured for each simulation. it is not shown in this paper because only very few packets were lost and using boxplots for displaying results has no statistical significance. for both direct and relay modes, the loss is less than 0.01%. this confirms that the radio conditions of the direct mode are not so bad and our comparison for all services is fair: the configuration parameters we chose represents a reference case (i.e. the direct mode) with a good quality coverage scenario. in such conditions, the gain for interactive services is clearly visible. it can be larger if the coverage inside the train is poor in the direct mode."
the remainder of this paper is organised as follows. in section ii we present the state-of-the-art of mobile relay in lte. in section iii we describe the architecture and the protocol stack when mobile relays are used. in section iv we analyse the advantages and limits of mobile relays. in section v we describe our simulation scenario within the traffic models and we also present our qos parameters. in section vi we present and analyse our findings and finally section vii concludes our document.
"life cycle assessment (lca), and in particular consequential lca, is regarded as an appropriate tool for the assessment of environmental impacts of new bio-based technologies (e.g., [cit] ) . despite the publication of various consequential modeling approaches, consequential lca is still far from a proper systematization [cit] . although the use of consequential and attributional lca for decision-making is still being discussed [cit], all approaches are based on the definition of consequential lca that \"is designed to generate information on the consequences of decisions\" [cit] . consequential lca attempts to"
"this article showed how (mixed integer) linear programming can be used to extend standard lca towards comprehensive decision-making. additional consequential research questions can be addressed such as the determination of the optimal number of new production plants and the optimal region for supplying feedstocks while also taking into consideration transport logistic options. the implicit environmental comparison of new bio-based technologies with fossil reference technologies can be regarded as a feature that has not been provided by other optimization models within the field of lca. the extension of lca by linear programming remains a consistent linear model, which is able to broaden the scope for consequential assessments. the benefit of this extension of lca is to provide a broader and systematic assessment of consequences. the modifications of the program by additional profit requirements (lcc) into a goal program and pareto optimization problem have been identified as promising ways toward a comprehensive multiobjective lcsa."
the lci data for upstream biorefienry processes and for the substitutable reference system is taken from the ecoinvent 3.2 database (cut-off model). the whole problem and the corresponding data can be studied in more detail in the electronic supplementary material (milp_lca.xlsx).
"since the qos in a relay architecture depends of the behavior of both the mobile enb and the enbs covering the track, future works should focus on scheduling techniques for real-time and non-real time services. also the performance of handovers should be studied."
"we consider a generic interactive service as shown in figure 3 with a request-response protocol. the ue sends a request to a server. the server then transfers a file, which can be a web page, a video or a text. the application protocol is http and the transport protocol is the transmission control protocol (tcp). the main qos indicator is the loading time defined as the total time to transfer the whole file from the time the request was sent. to get a better insight into the procedure, we break down the loading time into the server response time and the file reception time (see figure 3) ."
"in our scenario, a train moves in a linear way with a velocity of 100 kmph as shown in figure 5 . base stations (enbt) are regularly deployed along the track and have a 600-m radius. the intersite distance is thus 1200 m. handover is perfect both for the direct and the relay modes (no packet loss and no call drop). the enbt and the enbm transmit on different frequencies. therefore, the inter-cell interference is avoided."
"since we have not considered transportation and other costs so far in this case study, we adopt the model by introducing an additional objective for life cycle costing. in doing so, we introduce for the specific biorefineries an additional input factor matrix f with subcategories k (expressed in monetary units): personal costs, taxes, insurance, and administration. [cit] showed, the consideration of costs for the inputs and outputs of the processes of the technology matrix a can be carried out by using a price vector α. since we focus on the investor's perspective, price information is only required for products that are relevant for the profitability of the biorefineries. the additional objective function, which includes costs for processed and produced commodities of the biorefinery processes as well as the costs for transported beech wood, can be derived as equation 13 consists of two major terms. the first term considers the cash-flows related directly to the biorefinery processes j brs including revenues for biorefinery products"
"where, t fp,u is the time stamp of the first received packet by the ue and t rq,u is the time stamp of the request sent by the ue to the server (see figure 3) . page reception time: this indicator is computed as"
the columns of matrix a brs represent the biorefinery options j brs of each type at each potential location. the rows of this matrix represent the products i r which can also be produced by current reference processes j ref . matrix a ref includes current available processes j ref that produce the same functions i r as the biorefinery processes.
"with, i r biorefinery/reference product i up product of upstream process i t transported wood i s supplied wood in a district j brs biorefinery process j ref substitutable reference process j up upstream process j s region for supplying wood j t transport option"
"the average sever response time is increased by about 20 ms when a relay is used as shown in figure 7 . this is due to the supplementary radio hop. however, the response time can be larger than 80-ms (and even sometimes larger than 120 ms) in the direct mode due to bad radio conditions as opposed to the relay-mode where the maximum value is about 75 ms. this means that using a relay is beneficial even for clientserver applications in which a very small amount of data is downloaded. figure 8 shows the throughput when downloading an http file. when the train is close to the base station, radio conditions are excellent and the rate is high. thus, the maximum throughput in the direct mode is close to the one in the relay 2 mbps vs 1.3 mbps for 100-kb files). however, the minimum throughput is lower in direct mode at about 0.1 mbps in comparison to relay mode at 0.25 mbps. even without specific qos policy, the mobile relay helps to offer a minimum bit rate (of course except in highly saturated conditions with a huge number of passengers). the average throughput is much larger with the relay mode. the throughput gain increases as the file size is larger. due to the slow-start mechanism, the tcp protocol takes greater advantage of the best channel conditions reached by the relay architecture when the file is larger. figure 9 shows the effect of the mobile relay in terms of page load time when the train is loaded by 12, 60 and 120 users downloading http files of 100-kb. for 12 users the median load time is less than 1 s for both the direct and the relay modes. for such low load, the relay does not provide any benefit but does not degrade the qos."
"the amounts of reference flows are specified in regard to the german demand of these products within the current system. thereby, the reference flows need to be specified in the manner that the products of current processes must provide the same functions as of the biorefinery products. in the example, we assume that this is guaranteed, e.g., by referring to mj in the case of ethanol and gasoline."
the next step of the lci analysis is to specify the inventory vector g according to the reference flows. this is done by multiplying the intervention matrix b with the scaling vector.
"our figures represent the statistical distribution of values by using box plots with the upper quartile, median and lower quartile. the whiskers represent the maximum and minimum values excluding outliers. outliers are the points which fall more than 1.5 times the interquartile range above the third quartile or below the first quartile. figure 6 depicts the effect of the mobile relay in terms of page load time when the train is loaded by 60 users downloading http files (within a total of 100 users). for 500-kb files, the median load time is reduced from 7 s to 2.5 s and is thus divided by 3. the maximum load time is reduced by 30 s. users are generally very sensitive to such a reduction of the load time. even for moderate-size file for which the load time is by nature small, it is reduced by the use of a relay. in all cases, the maximum load time is always smaller in the relay mode (1 s vs 2 s for 20-kb files, 4 s vs 15 s for 100-kb files, 22.5 vs 52 s for 500-kb files)."
"in the following sections, we demonstrate how the previously mentioned equations can be modified to create a rectangular choice-of-technology model in regard to the goal and scope of the study (sect. 3.1). therefore, we define a mixed integer linear program (sect. 3.2) and determine the results of an exemplary case study (sect. 3.3). to include lcc aspects, we transform the original problem into a weighted goal programming problem and a pareto optimization model in sects. 3.4 and 3.5, respectively."
"we further define the first eight scaling factors for the biorefinery processes to be integers. this choice is necessary, since the data collection for the biorefineries is based on process simulation for a specific capacity. the intermediate flows and environmental interventions associated with this capacity are not up-or downscalable in a linear manner. the program is completed by introducing lower bounds (set to 0) and a vector of upper bounds c. in regard to the availability of residual beech wood in each district, the data of fig. 1 is considered as upper bounds for s j s . the upper bounds c for the scaling factors of biorefinery processes (s j brs ) were set to 1, since we assume that only one biorefinery can be built at a chemical park."
two train loads are considered : 20 and 100 mobile users. we assume 60% of the passengers make http requests and the remaining 40% have voip communications. the simulation parameters are presented in detail in table i .
"it is clear from the current study that avoiding penetration losses improves the qos for the passengers, even if the mobile relay architecture generates an additional overhead of 40 bytes to each transmitted packet. the gain for interactive services is more important for long downloaded files and for high load conditions."
"the lte module has been modified to implement the mobile relay architecture scenario (see figure 1) . in particular, the encapsulation and decapsulation mechanisms are fully simulated. we tested the performance of the mobile relay architecture and we compared them against a direct transmission scenario."
"we simulate voip communications with an enhanced full rate (efr) codec or equivalently an amr-wb codec in 12.65-kpbs mode (see [cit] ). the source traffic generates a 32-bit data block every 20-ms. there is no voice activity detection. hence, the flow is constant bit rate. the rtp/udp/ip headers are added to each data block. we assume the calls are set up prior to the simulation and did not consider any call arrival and call release. no signalling is simulated."
"to deal with multiple objectives in linear programming, basically, three groups can be distinguished: a priori, interactive, and a posteriori methods [cit] . the first class requires the definition of preferences between goals at the beginning of the solution process. the latter class requires the prioritization at the end after the presentation of all pareto optimal solutions. to illustrate the pros and cons of the broad range of applications, we modify the linear program in the next sections choosing weighted goal programming for an a priori approach and epsilon-constraint method for an a posteriori approach."
"a special look requires the influence of transportation distances on the overall result (profit and environmental impacts). in this study the transportation of feedstock only has a limited influence on the overall profit of the biorefinery. for instance, the difference between biorefinery type 2 in leuna compared to the location in böhlen is 177,280 € per year (table 5 ). in regard to the potential reduction of ghg emissions, the difference is even less significant. here, the biorefinery 2 [cit] kg co 2 eq. per year higher than in böhlen (pareto point 2 and 3 of 50% scenario, table 5 ). comparing this difference with the overall ghg emissions of the system, one might assume that the results are not reliable in terms of data inaccuracies. to address this point, we have a look at the reference flows of the final demand vector. the chosen amounts represent an estimated total demand for germany and, hence, are quite high. however, lower values would not lead to different results and would not reduce uncertainty. in fact, the choice of values for the considered reference flows is arbitrary. the determination of optimal locations for biorefineries is, hence, not affected by this choice. since we here assume identical conditions at the potential chemical parks, the identification of optimal locations is only determined by the transport distances and the corresponding impacts and costs. on the contrary, the choice for the type of biorefinery in terms of environmental impacts is mainly dominated by its emissions and those of the reference processes. in terms of profit, the optimal solution is mainly dominated by the corresponding costs for biorefinery (pre)-products. in conclusion, the results for the optimal choices of locations for biorefineries may be considered being robust in the face of data uncertainties in other parts of the model. a problem that might result is numerical issues when solving a problem with large discrepancies in order of"
this study has focused attention on the analysis of the qos performance of mobile relays in a standard lte architecture for a railway scenario. we evaluated the qos of http and voip services when a train is loaded up to two hundred users simultaneously using the lte network.
"to address the choice for a certain district, which deliver the residual beech wood, the model is extended by the matrices a t, −a * t, a s and −a * s . matrix a s is an identity matrix to consider the different alternatives of beech wood supply from the 37 districts. matrices −a * s and a t are needed to include the alternatives of beech wood transport from each district to each one of the 8 biorefinery alternatives. the linkage of the these alternatives to the biorefinery options is represented by matrix −a * t . upstream processes in matrix a up, which are needed to provide the pre-products of the biorefineries, the cultivation, and transportation of wood and the reference products, complete the rectangular a matrix."
"to solve the mixed integer linear program, the intlinprog solver in matlab was used. an additional scenario is analyzed in which the available amount of beech wood is reduced by 50%. the results for 100% scenario (1.14 mt dm beech wood in germany) indicate that two biorefineries with a capacity of 0.4 mt dm are built at the considered chemical parks. since the scaling factors for biorefineries are defined as integers, not the total available amount of beech wood is exhausted. the optimal locations to build the biorefineries are zeitz and schwarzheide. it is visible that for both scenarios, the ethylene-producing biorefinery 1 is more preferable to reduce greenhouse gas emissions than the ethanolproducing biorefinery 2 (table 2) . when reducing the available residual wood for all districts by 50%, another location becomes more preferable than those of the 100% scenario. this is an interesting result, since one might assume that one of the locations identified in the 50% would be chosen. the total reduction of the impacts on climate change compared to the impacts of the current system (h ref"
"société du grand paris is in charge of designing and constructing the new 200km of fully automated metro lines around paris, connecting the 3 paris airports as well as the main suburban and innovation areas. these \"grand paris express\" lines, interconnected with the existing paris subway, regional trains and bus lines, will carry around 2 million passengers per day. société du grand paris is seeking to provide continuous high quality telecommunication services to passengers in grand paris express stations and inside trains. thus, société du grand paris is interested in developing new technologies such as mobile relays in order to serve this purpose."
"we assume that the new biorefinery products compete with existing products of the current system. hence, we define substitutable reference products for each biorefinery product: biomethane vs. natural gas, bio-based ethylene vs. fossil ethylene, hydrolysis lignin vs. lignite briquettes, ethanol vs. petrol, and organosolv lignin vs. polyol. the definition of these avoided products is a long-discussed issue in lca. since the general societal aim is to move towards a bio-based economy, we use fossil-based references that provide the same functions."
"the beauty of cloud model is that the overall property of a concept can be represented only by the three numerical characters, which are ex, en, and he to represent the concept as a whole. ex is the mathematical expectation of the cloud drop distributed in the universal set. en is the uncertainty measurement of the qualitative concept, which is determined by both the randomness and the fuzziness of the concept. he is the uncertainty measurement of the entropy, which reflects the agglomerating extent of the uncertainty of drops in numerical domain."
"where k is a constant which can be adjusted by its own stability and experience. if the quantitative variables only have single limit, like v [c min, +∞] and v [-∞, c max ], we must determine its expected value or default value, and then calculate the numerical characteristics by (1) ."
"it requires a certain membership function, once the membership functions are defined, the fuzzy comprehensive evaluation is not any longer fuzzy to the concepts of factors. the latter utilizes the reasoning by qualitative knowledge rather than precise mathematical models, and evaluates the attribute factor with uncertainties based on cloud theory, where the uncertainty in the information is kept as much as possible. by contrast, in the cloud-based method the outputs are uncertain each time but, in the same overall variation trend as fuzzy comprehensive evaluation. that just reflect the way of human being's thinking, they show the different opinions in different mood of the same time, but the overall arguments are consistent."
"although a wide range of literature has appeared on teaching effectiveness of simulations and experiential exercises, there is hardly any objective evidence to conclude that simulations and experiential exercises indeed result in learning [cit] . it is unclear whether the perceived learning of players also means actual learning (see for further reading gosen and washbush [cit] ). on the other hand, there are many authors claiming that games do help to increase the understanding of complex situations. wenzler and chartier [cit] state that the properties of the parts can only be understood in the dynamics of the whole. games and simulations are a mechanism to show the big picture (gestalt understanding) in a condensed period of time."
"in this paper we describe our experiences with the use of a simulation game to exemplify a multi-agent system in the domain of supply chain management, namely the barge handling problem in the port of rotterdam. the problem is considered as most urgent in hinterland container barge navigation by the port of rotterdam. the problem is characterized by many different players with conflicting interests in a highly dynamic environment (we introduce the problem in the next section). through the simulation game we aimed to realize the four goals described in the introduction of this paper."
in the time bars we can depict waiting profiles as shown in fig. 2 . this is done only in scenario 4. waiting profiles are depicted on a logarithmic scale and show the maximum number of time slots a barge has to wait before it can be processed.
"consequently, the final comprehensive evaluation based on the cloud models is named as bc (ex, en, he), which is depicted by blue cloud drops shown in the fig 6. and fig. 7 ."
cloud is a model described by linguistic values for representation of the uncertain relationship between a specific qualitative concept and its quantitative expression. the cloud's global shape reflects the importance of the qualitative concept.
"after the rules are modeled by cloud theory, the uncertainty reasoning structure is constructed. when there is an precise input (x 1, x 2,…, x n ), the certainty degrees of every single-rule generator are calculated to all concepts in the preconditions of each of the cloud rules. if there is more than one positive certainty degree, we choose the maximum y j, which is called activated strength. through y i chooser and the forward cloud generator, the cloud drops (x i, y i ) are generated."
"central coordination as solution for the alignment problem seems obvious, but the terminal and barge operators are not willing to accept such a solution. the main two reasons are: i) they want to keep control over their own operations and ii) they are reluctant to share information that could undermine their competitive position [cit] . we developed a distributed planning solution by means of a multi-agent system [cit] . this solution meets the two requirements of the actors, namely that they can stay in control of their own operations and that they only need to share limited information."
"in this paper, the cloud model is introduced to develop a cloud evaluation algorithm for seafarers' competency assessment. the effect of human factors is highlighted and the cloud theory is used to model attributes of human element. the rule base is derived from fuzzy comprehensive evaluation. then the corresponding rules are fired by optimal cloud models. consequently, the final evaluation result based on the cloud graph is obtained. finally, simulation results demonstrate the effectiveness and superiority of the proposed paradigm."
"the output will be constructed by means of virtual cloud or backward cloud generator if there are large enough amount of cloud drops. consume that there are more cloud drops (x 1, u 1 ), (x 2, u 2 ),…, (x n, u n ), the backward cloud generator cg -1 can produce the expectation, entropy, as well as the hyper-entropy of the virtual cloud of final result."
during the game we observed the following. all players could easily work with the user interface. players quickly adopted their role as barge operator planner and all players enjoyed playing the game.
we were surprised how people changed their attitude towards the system after playing the game. for most of the participants it seemed that experiencing the solution had a much greater impact than through only vocal or written presentations of the solution.
"if we combine one pre-condition cloud generator and one post-condition generator, as shown in fig. 1, we can obtain a single-condition-single-rule generator. in this way, uncertainty is transited in the reasoning process by rule generator. as a result, we can easily construct the qualitative rules by cloud theory based on the relationships between linguistic atoms and cloud models. the cloud-based qualitative rule generator includes single-condition-single-rule, single-condition-multi-rule, multi-condition-single-rule and multi-condition-multi-rule."
"however, whether future system users are willing to accept new technology is in particular depending on their perception and expectations of the system [cit] . according to the technology assessment model [cit], the two main factors for users to decide on the acceptance of new technology are i) perceived usefulness and ii) perceived ease of use. perceived usefulness is defined as 'the degree to which a person believes that using a particular system would enhance his or her job performance' and perceived ease of use as 'the degree to which a person believes that using a particular system would be free from effort' [cit] ."
"the course of the game is as follows. all barges start at the entrance of the port. before the game starts, all players receive a similar assignment, namely a set of terminals that have to be visited. the assignments of players differ but are equal in difficulty. players can begin to plan a terminal rotation as soon as the game leader has started the game. from then on, the game proceeds with a constant clock speed and performs a simulation based on the decisions all players make. players can make and reconsider their decisions until the game ends."
"we consider the risk of failing of the game as a threat. we experienced through all the workshops that failing of the game may be caused by a variety of factors, varying from instable network connections, through unexpected user actions, and (small) bugs. the risk of failing is much higher than when playing a board game. we therefore decided eventually to play the game only at our own network, which was stable and could be tested well. the reason was that players usually have high expectations of playing a game and are not very patient when it does not work. this also determines the image they have of the illustrated system. developing a bug-free and fool-proof game takes time and requires extensive testing with non-experienced users in order to experience as many combinations of events as possible."
"comparing the two operators between the yi chooser and the forward cloud generator, in fig. 6, the final result is a little closer to 'low' cloud in the original comment set's synthetic cloud model, and the 'common' cloud is covered by most of the drops of the final blue synthetic assessment cloud."
"over the last three years we have played the game at more than seven occasions with different groups of people, varying from students to practitioners. among the practitioners were terminal operators, barge operators, consultants, and people from the port authority. in this section we describe our experience with the game in two workshops. in workshop 1 we played the game with managing directors of barge operators and in workshop 2 we played the game with barge operator planners. we pick these two workshops, since we played the game in both workshops with part of our target group, namely practitioners that deal with the barge handling problem daily. this gave us the opportunity to evaluate whether the game meets the aims we wanted to achieve (see section 1). each of the workshops lasted for about three hours. unfortunately, in these two workshops no terminal operator participated. in that stage of the project, playing the game with this part of the target group was already a great step."
"clearly, to let the system become effective, it has to be accepted by at least a critical mass of terminal and barge operators. users need to get a sound understanding of the functioning of the system. in particular, they need to get a clear picture of the extent of autonomy they have in the system to decide on their own operations and the amount of information that is shared. these two aspects relate to the key elements in a multi-agent system, namely the agent intelligence and the interaction protocol. the game is designed to allow players to play the role of the barge operator agent and to let them experience different interaction protocols."
"in the barge handling problem we deal with specific business constraints. terminals compete with each other and so do barge operators. in the port of rotterdam we deal with about 30 container terminals and -on a daily basis -a number of more than 60 barges visiting these terminals. terminal and barge operators realize that they all can benefit from adopting it to enhance their cooperation, but they are reluctant to do so as they fear this may undermine their competitive position. however, the problem is urgent, since it results in significant efficiency losses for barge and terminal operators. also, the port authority considers the problem as urgent since it affects the quality of the hinterland connections."
"we experienced that people were able to think about applying the system in practice and how it would impact their daily work. both groups of people also realized that this system would result in a different job content of the planners, less attention to routine tasks that can easily be automated, and an attention shift to the relationship with the terminals and non-routine tasks that require specific knowledge and experience of planners."
"as stated in section 2, simulation games that exemplify multiagent systems are rarely found in the literature. they confine themselves to fields as policy making, water management and irrigation systems. the contribution of this paper is that it describes a realtime multi-player simulation game which illustrates a multi-agent system for a complex logistical system with many different actors with conflicting interests. by letting representatives of these actors play the game, we create mutual understanding on multiple levels. playing the game with different scenarios, the players learn to appreciate the value of an adequate protocol for information exchange. one of the developments in supply chain management that gains increasing attention (see, e.g., the dutch national innovation programs in logistics and supply chains) is the alignment (or coordination) of activities and interests of the various members in the supply chain. part of the coordination in the chain may be done centrally, but it is likely that a large part will be organized through distributed planning. simulation games as discussed in this paper provide an effective way to communicate distributed planning solutions with supply chain players and to see the merits of such solutions, which may accelerate their acceptance."
"digital images play a very significant role in the modern era. almost every story published in newspapers and magazines uses digital images. many times digital images are used to build some perception about celebrities and political leaders. in case of legal issues also, they are used as corroboratory evidences. so, before believing what we see, it is very much necessary to establish the truth of the image. due to the growing influence of the digital images, the instances of manipulated images are also going up. hence, the research area of image forgery detection has been growing very fast in recent times [cit] . broadly, the forgery detection techniques can be classified in to active and passive categories [cit] . in case of active techniques, some prior information is embedded in the image at the time of capturing and later it can be used to verify the authenticity of the image. one such technique is watermarking [cit] . in case of passive techniques, no prior information about the image in question is required. copy move forgery or cloning, splicing and retouching are the main techniques of creating image forgery [cit] . retouching is performed by applying some filters to change the appearance of a person like the age or mood as shown in figure 1 . in splicing, some part of intended image is replaced by the content from some other image as shown in figure 2 . the statistical parameters of that region are quite different from the rest of the image. hence, statistical techniques are applied to detect such forgeries. however, in copy move forgery, content from the same image is used to hide some region of the image [cit] as shown in figure 3 . copy move forgery is more prevalent and challenging due to its ease of performing and extent of manipulation. also, to make it difficult for detection, the manipulator performs some additional operations like adding some noise, compressing the manipulated image, rotation and scaling of the duplicated region etc. the very fact about unedited captured images that no significant regions can be exactly same, is exploited in detecting copy move forgery. so, any duplication of a significant region is treated as a case of copy move forgery. block based methods are used to divide the image into overlapping regions and then blocks are compared to find duplication. the main drawback of such methods is the large number of overlapping blocks in case of high resolution images. also, these methods are not robust to large affine transformations applied to the duplicated region like rotation and scaling. a keypoint based hybrid method is proposed, which detects the keypoints, distinct in terms of intensity pattern around them and invariant to affine transforms like rotation and scaling. speeded up robust features (surf) keypoints [cit] are detected in the image and then binary robust invariant scalable keypoints (brisk) feature descriptors [cit] are calculated at these keypoints and matched to find duplicated region. the time taken for detecting copy move forgery by the proposed method is compared with the existing keypoint based method using surf and found significantly faster."
"to avoid too many factors in the same level, we decompose the index system into multilevel since the weight of each factor would be too small if all of them are in the same layer of the original system. the new model evaluates the factors from the bottom to up. and the upper level will use the synthetic evaluation vector from the lower level. we call the new model as multilevel fuzzy synthetic judgment."
the binary features resulted from brisk are matched for similarity using knn search. hamming distance is used to find nearest neighbor. hamming distance is the difference of the number of bits in two compared bit strings. all the nearest neighbors do not represent the legitimate duplicate keypoints. to discard the outliers distance ratio of the two immediate neighbors of a point is compared with a threshold value (ranges from 0.3-0.
"another challenge is to find a balance between complexity and the extent of realism of the simulation. we chose, for instance, to use an abstract picture of a port area to manage the expectations of the users. users should be aware that the game illustrates the multiagent system at a high level and does not, e.g., consider tides, ship stowage, restricted opening times of terminals, and so on. we experienced that this worked out well."
"last but not least, although the game was not developed for educational purposes, we experienced that the game was also useful in that respect. it provides students a view on how barge handling activities are organized in the port of rotterdam and it helps them to quickly get an understanding of the idea behind and the potential of a distributed planning system."
in section 1 we described the goals we aimed to reach by means of the simulation game. in this section we reflect on the extent to which we were able to reach these four goals successively. in our reflection we combine the experiences of all the workshops we organized.
"the brief literature overview shows that games are considered to have potential to increase the understanding of complex systems and support the discussion between stakeholders based on common experiences. moreover, they can help users to develop their expectations of the system. although simulation games are a promising tool to exemplify multi-agent systems, we only found a few contributions that really did so, namely [cit] . these contributions give support for the statement that simulation games are an appropriate means to improve the understanding of the functioning of a multi-agent system, to create a common problem understanding, and support a focused discussion on model choices. they describe experiences in the field of policy making, water management, and irrigation systems. considering the limited number of multi-agent systems that are deployed in practice and the complex nature of implementing distributed planning solutions due to the many different actors involved, we are convinced that more research is needed on how to bridge the gap to practice and get multi-agent systems implemented."
"on arrival at a terminal the barge queues and waits for handling. when the barge has been processed by the terminal, the player gets a notification and the corresponding time slot turns blue (c.f. fig. 1 ). after the handling has been completed, the barge immediately heads to the next planned terminal. on arrival at this terminal the barge again enters the queue and waits for handling."
"the experiment is conducted on image dataset with different size images. figures 8-27 show detection results for different types of post processing operations including rotation and scaling. the time taken to detect forgery is not directly dependent upon the image size, rather it depends upon the duplicated region size and texture of the image. the number of keypoints generated and the corresponding descriptors processed are proportional to the duplicated region and brightness variations. as evident in case of 'maskcopy.bmp' and 'kore.bmp'. the size of the first one is greater than the later, but time taken as well as keypoints generated are more for the smaller image. table ii shows high stability of the method in terms of correct detection ratio. figure 6 and figure 7 show the performance of the method against jpeg compression and noise addition respectively. in case of small duplication regions and low contrast images the threshold ratio to identify good matches has to be quite high. the variation of threshold value (ρ) is tradeoff between number of valid keypoints and outliers. low value of 'ρ' will cut the outliers but also restrict the number of valid keypoints."
"in this section we give a description of the game. we describe successively the game setting and typology, the role and task of the player, the course of the game, different scenarios, the user interface, the game architecture, and choices to reduce the complexity of the game."
"the method is tested using correct detection ratio which is ratio of valid keypoints to total matched keypoints without using morphological operations. it is compared with the existing method [cit] . the proposed algorithm may be divided into three sections: (1) extracting surf keypoints; (2) define descriptors at these keypoints; (3) matching descriptors to locate the forged regions. the time taken by the first and third section is same for the proposed and the existing method are same as both extract the surf keypoints and uses knn search. however, the time taken by section 2 is different and comparison is provided in table1 for descriptor extraction. the method is evaluated for stability by correct detection ratio, which is ratio of the keypoints in the forged region to the total keypoints detected. another parameter termed relative detection efficiency is used to observe the behavior of the algorithm against the post processing operations of jpeg compression and noise addition. relative detection efficiency is the ratio of correct keypoints detected in presence of post processing operation to the keypoints detected without using such operations."
"the workshop started with an introduction of the barge handling problem and an explanation of how we perceived the problem. we then introduced the concept of a multi-agent system and explained the reasons for this approach compared to central coordination. after that we introduced the game by explaining the role of the players, the user interface, and the first round. we played three rounds with scenarios 1, 3, and 4, respectively. players were informed about the number of rounds we played, but we introduced the rounds one after the other. this means that players were not aware of which rounds they were going to play."
"1. to communicate our multi-agent solution to practitioners in an accessible way to help them to get a clear understanding of the system. we expect that this i) contributes to the acceptance of the solution, ii) supports a focused discussion on design choices and assumptions, and iii) supports a discussion about the conditions for implementation. 2. to use the game as a practical validation of our multi-agent system."
with respect to our second goal we found that the game was especially helpful to see how people make decisions. we were surprised to see how people change their way of decision making depending on the information they have (we reflect on this when discussing our fourth goal). the game was not useful to validate the simulation results or to prove that one interaction protocol resulted in lower average turn-around times. one reason is that people have to process all the information themselves and do not always make the best decisions possible. another reason is that we did not play the game often enough with the same players to get statistically significant results.
"the proposed method is significantly faster than the existing method using surf. it is robust to affine transformations like rotation and scaling as well as other post processing operations like jpeg compression and gaussian noise addition. in the matching process threshold is set manually depending upon the size of the copied area and the texture of the input image. so, there is further scope to devise the threshold automatically. for large values of rotation and scaling number of outliers also increases. morphing techniques may be used to reduce the false keypoints and better localization of the duplicated region."
"when designing a distributed planning solution for the barge handling problem, we focused on both acceptance and optimization. we designed the system such that the conditions for participation are acceptable for future system users. moreover, the system allows the actors to optimize their operations and obtain individual benefits. this is important, because if we are able to design a highly efficient system, but the system is not acceptable for the actors in practice, then it will never become effective."
"nowadays, barge and terminal operators try to align their operations by making appointments. these appointments are made by telephone, fax, e-mail, and a system called portbase. unfortunately, it frequently happens that appointments are not met by either the barge or the terminal operator. appointments turn out to be unreliable. there are several reasons for this, like the fact that appointments are sometimes not even feasible at the time they are made or the fact that a disruption at one terminal quickly propagates to other terminals and barges. these events often require re-planning of appointments."
"however, the numerical characteristics of cloud models are still determined by practical experience again and again. and the accuracy of the result is decided by the establishment of the factor evaluation standard model, which is expressed by natural linguistic words."
"the red vertical line in the lower part of the screen denotes the progress of time and the black vertical line the time before which the player has to complete his/her activities. as soon as the processing of a barge has been completed, the corresponding time slot turns blue. time slots denied by the terminal turn red and approved time slots turn green. proposed (and not communicated) time slots are depicted as grey."
"a cloud evaluation method is proposed, to resolve the uncertainty in assessing a complex system. the randomness and fuzziness of object's attributes are both considered, where various uncertainties in the complex system are well kept and investigated by cloud theory. and the different attribute factors are depicted with different cloud models of varied linguistic words. and the final result also can be expressed by cloud models with different numbers or kinds of comment linguistic language. finally, the evaluation results are a cloud graph which is more intuitive and understandable."
"according to the knowledge and experience of experts, k qualitative inference rules can be established based on the fuzzy synthetic judgment. they are presented by the priorities and frequencies. there is a rule database consisting of n qualitative rules."
"when designing a solution for the barge handling problem (through a multi-agent system) we paid special attention to design choices that could influence the acceptance of the users and the extent to which users can optimize their operations. however, when presenting the solution to practitioners we felt that it was hard for them to get a picture of how the system works in practice and whether it provides a solution they are willing to adopt and support. we therefore decided to develop a simulation game to communicate our ideas and to help future system users to get a clear picture of what the solution is about."
"scenario 1 first-come first-served, no waiting information on arrival at a terminal this is the most basic scenario in which the players make no appointments with terminals and also get no information about the occupation of a terminal. barges are processed first-come first-served at a terminal. the player in fact only decides on the sequence in which terminals are visited. scenario 2 first-come first-served, with waiting information on arrival at a terminal in this scenario barges are again processed first-come first-served, but on arrival in the queue of a terminal they get information on the length of the queue, i.e., the number of time slots a barge has to wait before it will be processed. as soon as the player leaves a terminal (s)he has no insight anymore in the queue length of that terminal. players can always decide to leave the queue and go to another terminal. scenario 3 appointments, limited information of occupation of the terminal players are processed based on appointments with the terminals. to get an appointment with a particular terminal operator, the player has to send a request for a particular time slot to the terminal operator. a terminal operator replies to a barge with 'yes' or 'no' whether the request has been accepted. if a request is rejected, the player has to send a new proposal until a time slot is accepted. in this way players get fragmentary insight in the occupation of a terminal, which is comparable to practice. scenario 4 appointments, full information of occupation of the terminal"
"planning a rotation means that players indicate in which sequence they want to visit their terminals and, depending on the scenario played, at what time they want to have their appointments. in section 4.4 we discuss the different scenarios in detail. as soon as a player has decided on the first terminal to visit, his/her barge starts to sail to this terminal. a player can always make up his/her mind and decide that the barge should head to another terminal, as long as the barge has not yet arrived or is not being handled at a terminal."
our experiences reveal that a simulation game is an effective means to contribute to the 'perceived usefulness' and 'perceived ease of use' of our system by future system users.
the process of evaluation can not stop repeating and optimizing itself in endless cycles until it satisfies the experts in the field. the expected value of the cloud can be considered as quantitative output value of the final result of the comprehensive evaluation if it is necessary.
"models different attribute factors in the set of standards are very often qualitative variables, which are described in natural language by experts in the field. nevertheless there are still quantitative variables, for instance, the formalization like"
"cloud theory has succeeded in the fields of data mining [cit], intelligent control and prediction [cit], land price evaluation [cit] and large-scale system evaluation [cit], etc."
"requests to the terminal are answered with a random (configurable) delay. we do so to prevent that players quickly evaluate many time slots. moreover, each request constitutes a burden for the terminal operator agent and also in practice the terminal operator might need some time to send a reply. in the game we can play each of the four scenarios separately or in combination, by letting one part of the group play scenario 1 and the other part scenario 2, or one part plays scenario 3 and another part scenario 4."
"the barge handling problem is the problem how to align the operations of barge and terminal operators in a port, such as the port of rotterdam. in the port of rotterdam, (container) barges are an important means to transport containers between the port and the hinterland. when barges visit the port, they typically visit about eight container terminals to load and unload containers. a sequence of terminal visits is called a rotation. the sequence in which a barge visits these terminals depends on several factors, such as the availability of terminals. terminals, on the other hand, also have their preferences when to handle a specific barge. an alignment of activities is thus required."
"over the last decades, several studies have been performed to the potential of multi-agent systems for solving various problems in supply chains. however, until now only a few of these systems are deployed in practice [cit] . clearly, gaining acceptance for a solution is essential to get the system implemented, especially when a lot of independent (and even competing) actors are involved. in this paper we describe our experiences with the use of a simulation game that we developed to gain acceptance of a multi-agent system for the barge handling problem."
"developing a real-time multi-player simulation game is technically challenging, due to the many events that take place and that have to be synchronized for all players in the game. playing the game requires stable network connections, to prevent that a client loses the connection with the server and stops functioning. we experienced that, despite extensive tests, small bugs still popped-up and disturbed the playing of the game. players experience a (small) bug as very negative, especially when it gives them a disadvantage while playing the game or even forces them to quit the game round."
"reducing the learning period of people from real time to simulated time, allows for steeper learning curves. [cit] give an example of how games can contribute in understanding dynamic business networks and the effect of different strategies. [cit] state that games are good at explaining the content of multiagent systems. especially for the purpose of supporting negotiations, it might be necessary to give the actors insight in the functioning of the system and discuss whether the model's assumptions match their own representation of the system dynamics and whether agents have a correct range of possible actions."
"research has confirmed that human element is involved deeply in maritime accidents, which is drawn the attention from all relevant fields of international communities, especially from the international maritime organization (imo). nowadays, human element has been the hot topic in research field and seafarers' competency has developed accordingly. since the abstractness and uncertainty in human element, how to depict the qualitative concepts in natural language and reflect the randomness and fuzziness and so on is the most important problem in scientific evaluation process. it is well known that the delphi method, analytic hierarchy process (ahp) [cit], probability and statistics method [cit], grey system theory [cit] and so on. to assess marine traffic safety, traditionally many researchers have studied in statistical method [cit] . this is necessary to have a database of very large number of accidents information, whereas it is difficult to obtain sound data, and the standards for this field are not consistent with different countries. grey relational analysis (gra) [cit] and fuzzy theory [cit] have been comprehensively used in maritime safety assessment recently. especially fuzzy comprehensive evaluation method is not appropriate to these cases, which different attribute factors are described by varied linguistic words and final result is expressed by different linguistic atoms. hence, these methods mentioned above have been modeled by precise mathematical tools more or less. however, marine traffic safety assessment, known as a \"navigator-ship-environment\" system, is a large-scale complex system with much uncertainty information inherently. fortunately artificial intelligence with uncertainty would be effective to circumvent the difficulty of marine safety evaluation. cloud model as a quality to quantity interchangeable model is presented. it combines the fuzzy mathematics, probability and statistics, which has made a great progress in assessing qualitative problem [cit] ."
"a drawback of real-time multi-player simulation games is that there are many factors that may cause the system to fail, varying from instable network connections, through illegal user actions, to (small) bugs. players usually have high expectations when playing the game. when the game does not perform well, then they assess this as very negative which impacts the image of the game organizer negatively. they may easily conclude that the game was not well prepared or, even worse, that the exemplified system is not worth implementing. since the game is clearly a communication tool, we therefore expect that extensive testing certainly pays off."
"the reason why we block time slots randomly on initialization of the game is to realize a realistic utilization degree of the terminals. how the time horizon is divided in time slots is configurable, but suppose that we have played the game with about 96 time slots (four time slots per hour in a 24 h time period). if we do not block time slots and we play the game with about ten players, then, per terminal, at most ten time slots during a planning horizon are required for handling. in that case the utilization of the terminals is unrealistically low (in our case about 10%) as well as the game complexity."
"from the fig. 7, the blue cloud model of the final result is a little far from the 'common' cloud model, but a large proportion is in the 'common' cloud. that is the difference between the different operators. the reason for the difference between them is that the activated strength is minimized by 'logic-and' of the four precondition of the rule corresponding the four concepts ac 1, ac 2, ac 3, and ac 4 . the degrees of 'logic-and' between the four antecedents are not stated clearly in logic. whereas the 'soft-and' which is introduced in is to implement the activated strength from the four antecedents. hence, we come to the conclusion that the outcome of the two different operators is consistent with each other, i.e. the general quality of seafarers on board is common with the precise input to the system. however, in the latter one, the four antecedents of the rule are all considered by the 'soft-and', which makes the final result is more flexible and exact."
"with the game we tried to achieve four goals, namely i) to communicate our solution to practitioners, ii) to practically validate the multi-agent system, iii) to prototype the user interface, and iv) to evaluate the way players perceive different interaction protocols. in the paper we described the design of the game and the choices we made. we concluded with a description of our experiences in two workshops with practitioners and a discussion on the extent to which our goals are realized."
"during the game, players can see a map of the port (including all the terminals) on their computer screen (see fig. 1 and section 4.5). on this map players can also see the ships of other players sailing. we divide the time horizon in time slots of equal length. handling of a barge at a terminal takes one time slot. the duration of the handling may not be realistic, for instance when a time slot corresponds with 15 min handling time. however, for the purpose of this game our choice suffices. moreover, it is fair that the handling at each terminal takes an equal amount of time for all players. whether the players get information about free time slots at the terminal depends on the scenario that is played (see section 4.4)."
"the present paradigm's main idea is that by cloud modeling different attribute factors including qualitative and quantitative variables, the optimal cloud models of each factor are found, and through x-condition cloud, y-condition cloud and backward cloud generators, the post-condition clouds of the corresponding rule are fired by soft-and operator. consequently, the final evaluation result is obtained, which is described by the distribution of cloud drops. the flow chart is shown in fig. 3 ."
"there is a strong relation between games and simulation. both provide an environment to support learning or the acquisition of skills [cit] . simulations make it possible to mimic the behavior of a 'real' situation under the influence of a set of variables that are considered to be important, whereas games stimulate competition between players within a predefined set of rules [cit] . angelides and paul [cit] state that gaming-simulation is a sequential decision making exercise through which players can experience the consequences of their decisions rapidly in an artificial environment containing some characteristics of a real situation. the aim is to enhance a comprehensive understanding of a complex system and to develop learning skills."
"ryan [cit] claims that changing management practices requires learning skills that can be met through games. he states that problems and issues become increasingly interrelated and that, as more people become involved in decision making, priorities become less clear and implementation gets more difficult. his experience is that simulation games are extremely useful in developing an appreciation of systems thinking, i.e., viewing the phenomenon under consideration as a consequence of the interaction of the system constituents. ryan [cit] states that simulation games create common experience among participants, which they can refer to when discussing the system concepts (see also [cit] ). le bars and le grusse [cit] describe their experience with a simulation game and a decision support system for (collective) decision making among several actors. they show that this combination improves discussions between stakeholders and facilitates the emergence of acceptable solutions. they mention that acceptability of the solution in this case is more important than the optimal solution. a game-like setting has also been applied in an earlier study of our problem [cit], where it is reported that through a game the participants start to realize the seriousness of the problem and the potential benefits of an agent based system."
"the most pervasive cloud model is normal cloud model, which plays a significant role in academic research and practical applications. the algorithm is applicable to the one-dimensional universal space situation, and also applicable to the two or higher dimensional situation. the algorithm is as followed:"
"human errors are the most important cause in the marine traffic safety system assessment, in which much uncertainty and abstract information are involved. it is not suitable to be modeled by precise mathematical tools. hence, the synthetic assessment algorithm based on cloud modes is much more suitable to attribute factors are expressed by natural language values."
"-layout of the network -total number of terminals in the port -length of the planning horizon -number of time slots in the planning horizon -number of players -number of terminals the players have to visit -speed of the simulation -sailing speed of the barges -delay in the response of the terminal after a request of the barge -fraction of time slots that are blocked randomly to decrease terminal availability and to increase the game complexity -which scenario is played (scenario 1, 2, 3, or 4, or certain combinations)."
"for the multi-level index system, we should divide the index system into q levels. then we evaluate the factors from the bottom to up. and the upper level should use the synthetic assessment vector from the lower level."
"during discussions with managing directors and the barge operator planners we made the following observations. through the game we got detailed and focused discussions about the multi-agent system, assumptions we made, practical issues that have to be addressed, et cetera. feedback at this level of detail we did not get during sessions in which we only explained the system by means of a presentation. people were able, as we experienced, to quickly understand the concepts behind the multi-agent system and the interaction protocol. through the interaction of the decisions of all the players they could see the dynamics of the system as a whole."
"given a set of keypoints, the brisk descriptor is composed as a binary string by concatenating the results of simple brightness comparison tests. in brisk, the characteristic direction of each keypoint is identified to allow for orientation normalized descriptors to achieve rotation invariance which is important to general robustness. the key concept of the brisk descriptor makes use of a pattern used for sampling the neighborhood of the keypoint. to negate aliasing effects the intensity values at these keypoints are smoothened using gaussian. for a sampling point pair (x, y) and smoothed intensity values at these points i'(x) and i'(y) respectively, the local gradient g(x, y) is estimated as in (3)."
"in the game the player gets the role of barge operator planner. the terminal operator role is taken care of by the computer. the task of the player is to plan a rotation (sequence of terminal visits) along the terminals in the port that (s)he has to visit. aim of each player is to minimize the turn-around time of his/her barge in the port, i.e., to plan the terminal visits such that the barge leaves the port as soon as possible. how visits along terminals have to be planned differs per game scenario. we describe the game scenarios in section 4.4. since multiple players make decisions simultaneously, they influence each other's possibilities to be handled at a terminal."
"we described our experiences with the use of a simulation game to exemplify a multi-agent system. we designed a game for a specific case, namely the barge handling problem which is the problem to align barge and terminal operations in a port. in the problem we deal with multiple independent players that have to communicate to align their interests. in an earlier study we developed a multiagent system solution for the problem. however, when presenting the solution to practitioners we felt that it was hard for them to get a picture of how the system would work in practice and whether it provides a solution they are willing to adopt and give support for. we therefore decided to develop a simulation game to communicate our ideas."
"at the start of the game we hand out a flag to each player. when logging-on the game, a player has to select the user-id that corresponds with his/her flag. the flags are a nice feature of the game, since they help players to recognize each other's barge. moreover, the flags enhance the sense of reality. last but not least, they cheer up the game room."
"planners indicated that through the game they had a better understanding of how the system works, the information that is exchanged, and the way they can make their decisions."
"after the evaluation object is determined, the three numerical characteristics of cloud models should be established. if the numerical characteristics of cloud model are given by n experts, the qualitative variables can be represented by a synthetic cloud, and the numerical characteristics of the synthetic cloud are calculated as follows: take the variables of the factors in the same coordinate axis. look at fig.3 . that is seafarers' average sailing age cloud models cluster in the same coordinate axis. when there is an input, the corresponding interval would be fired, the optimal cloud model should be chosen."
"the user interface consists of three parts (see fig. 1 ). the upper left part is a graphical representation of the game state, depicting the location of all the barges and terminals. the upper right part of the screen presents the performance of the barge. the lower part is reserved to plan a rotation. in this part of the screen a time bar is given for every terminal the player has to visit. this time bar is divided in time slots. the player can select a time slot by clicking on the specific time slot. the selected time slots can be communicated with the terminals by clicking the button on the left bottom corner of the screen. as long as the player has not pushed this button, the plan is just a proposal made by the player and not communicated to the game server (and the terminals)."
"during the discussion afterwards we got some striking remarks. one managing director mentioned that, when coming to the workshop, he did not expect to see a solution to the barge handling problem that day. during the introduction of the problem his expectations went down. his expectations decreased even further after playing scenario 1 and 3. but when playing scenario 4 he thought: this is what we need. after the game he was enthusiastic about the solution we developed. this experience was shared by other managing directors. all participants were aware of the fact that the game simplified the current situation, but they considered it as a promising basis for a solution."
"after the game we had some discussions. planners said they had become enthusiastic about the solution we presented. they had seen many solutions and had low expectations, but after playing the game they spoke about 'sparks of hope'. although the system would replace parts of their tasks, they expected that their job could become more interesting as they would have better tools and information to make their planning decisions. moreover, automating the routine tasks leaves time for more interesting, non-routine tasks that require expert insight of the planners."
"with respect to our first goal we experienced that the game is a useful tool to communicate the functioning of a multi-agent system to practitioners. within a few minutes people get a clear understanding of how the system works and they are able to reflect on the solution and the choices and assumptions made. also, playing the game gives food for a focused discussion afterwards. we got more detailed feedback than when we explained our solution using a vocal presentation only. moreover, people responded with greater enthusiasm after playing the game than after a vocal presentation only. through the game, players were able to understand the properties of the parts in the dynamics of the whole (see also [cit] )."
"our third goal is related to the user interface. we experienced that the players usually have no difficulty to work with the interface. both barge operator directors and planners value the fact that an overview of the port was given with all ships sailing, as well as the waiting profile information. this information is important for them to make better planning decisions. in a feasibility study we continued developing the user interface of the barge operator agent based on the game interface [cit] . we found that the game is a nice way to prototype the user interface and to create an understanding by practitioners about the way they can use this interface."
"then the distribution of x on u is defined as a membership cloud, and is denoted by n 3 (ex, en, he), every x is defined as a cloud drop [cit] ."
"we conclude that a simulation game has many advantages for communicating a multi-agent system to practitioners. it helps people to quickly get an understanding of the system and to decide whether they are willing to give support for implementation. the game is also useful as prototyping technique for the user interface and to evaluate the way players perceive the different interaction protocols. to practically validate the performance of the multi-agent system, the game is less useful, unless players are trained in playing the game and play the game in multiple rounds to get statistically significant results. this was beyond the scope of our research."
"although the participants had difficulty to understand the role of the agent, they obtained (in a very short period of time) an understanding of the way the system functions, the information that is exchanged and the benefits of the multi-agent system, being that everyone can make decisions autonomously and without sharing much information."
"-when making an appointment a barge is added to the schedule of the terminal. barges waiting in the queue, are processed according to this schedule. -if a player has made an appointment and arrives late at the terminal, then the appointment is cancelled and a new appointment has to be made. a barge is not processed without an appointment. -if a terminal is idle then it looks in the queue whether one of the next planned barges is already waiting. if so, then the first planned barge (waiting in the queue) is processed during the idle time slot. if no barge is waiting (or no barge with an appointment) then the terminal leaves the time slot idle."
"in this paper, a cloud model based assessment paradigm has been proposed to evaluate human element. cloud models are utilized to extract uncertainty information and fuzzy comprehensive evaluation methods are introduced to build relationship rules. as a consequence, optimal cloud models are selected to activate the post-condition cloud of corresponding rules. finally, the comprehensive evaluation result can be derived from the proposed assessment system. experimental results show that the approach has higher reliability and better understanding for marine traffic safety assessment."
we are convinced that without the game we would not have been able to get the support of practitioners to make next steps towards implementation. the game has paved the road to acceptance of the system.
"the algorithm of generative cloud drops or hardware is called cloud generator, which includes forward cloud generator, backward cloud generator, condition x cloud generator and condition y cloud generator. when cloud's three digital characteristics (ex, en, he) and the special value x 0 are given, the cloud generator is called condition x cloud generator, when cloud's three digital characteristics(ex, en, he) and valueμ 0 are given, the cloud generator is called condition y cloud generator. condition x cloud generator and condition y cloud generator are the basis of doing uncertainty reasoning by cloud models [cit] ."
"in the past, ideas have been proposed to establish a central trusted party to coordinate the activities of both terminals and barges. however, this has proven to be unacceptable for the stakeholders for two main reasons. first, every actor wants to stay autonomous, i.e., in control of its own operations. second, every actor is reluctant to share information as it fears that others will use this information to improve their competitive position."
"for instance, how do people interpret and use the information that is provided and what does it mean for the decisions they make? 3. to use the game as a prototype solution for the user interface. for instance, is the presented information useful and is the interface intuitive and clear? 4. to evaluate the way players perceive different interaction protocols and how this affects their performance."
the response of the planners was important for the managing directors to decide on their support for implementation of the system. let us describe the setup of the event and our experiences with the game.
"the last scenario resembles our solution to the barge handling problem. players again have to make appointments with terminals. however, this time they get insight in the occupation of the terminal during the day. on screen the player can see which time slots are available. in this way the player has more insight in the waiting time at terminals and has more information to plan a rotation. note that a time slot that appears to be free at the moment the decision is contemplated, may be occupied at the actual time of request due to decisions of other players. moreover, the availability of a terminal changes over time through the actions of other players."
"when we evaluate, we should divide the quantitative indexes into different sections, divide the qualitative indexes into different levels, and give uniform degree by membership function."
the outline of the paper is as follows. section 2 presents a literature review. in section 3 we briefly describe the barge handling problem followed in section 4 by a description of the game. section 5 presents our experiences with the simulation game. in section 6 we reflect on the extent to which we were able to reach the above four goals. section 7 gives our overall conclusions.
"in the game we can play four scenarios, which are constructed based on i) the amount of information that is exchanged about the occupation of the terminals and ii) whether players can make appointments with terminals. let us describe the four scenarios."
"after all terminals have been visited and the handling at all terminals has been completed, the barge automatically heads back to the entrance of the port which is also the end point of the game. on arrival at the end point, the barge's arrival time is logged and the player has completed his task. the game server sends the player a message on his ranking. the game ends when the game leader stops the game (for instance after there is a winner) or when the time for a game round is over."
"after the discussion we explained our assumptions and choices, but this did not have as much impact as the game experience. at this meeting the directors declared that they were interested in further research to the implementation of our solution in practice. a few months later we started a feasibility study with these barge operators to investigate whether and how implementation of the system is feasible."
"our future work will study this problem and the qualitative and quantitative methods are mixed used in the process of assessment. and the study of 'soft-and' operator in uncertainty reasoning is also our main work in the future. he now is a professor at dalian maritime univerty, and his interest is marine safety, human element and navigation education."
"the setup of the workshop was similar to the workshop with the managing directors. we started again with an introduction of the barge handling problem. this took quite some time, since the planners wanted to react, share their experiences, and give their view on the problem. however, they all agreed on the way we described the problem. after that we briefly introduced the concept of multiagent system and explained the reasons for this approach compared to central coordination. the introduction of the game was similar to the workshop with the managing directors."
"the game setting (or scope of the game) is a port with a variable number of terminals. barges have to visit (a subset of) these terminals to load and unload containers. all barges enter and leave the port via the same waterway. this waterway also determines the start and end point of the game. players have to take a role in the game. specifically, we focus on a particular stakeholder within the system, namely the barge operator. the game is industry specific, namely for the barge handling problem in a port, and is a functional game as we focus on the planning discipline within (terminal and) barge operator companies (see also [cit] )."
"the game was also useful to evaluate how players perceived different interaction protocols. when we provide no information about the occupation of terminals, we see that players become passive in the sense that they plan a rotation and hardly ever change it. the reason is that they do not have any information to decide upon. when players can ask a terminal whether a time slot is available (scenario 3), we see that players start to plan and re-plan a rotation. after some time it becomes harder to get a time slot (due to the actions of other players) and players are then more quickly satisfied with just a feasible rotation. when we provide waiting profiles (scenario 4) we find that players have to process a lot of information. they have more opportunities to optimize their rotation (since they have more information), but it is quite demanding to process this information (although, in the game settings offered, players only have to plan five terminals). this is even more so when playing the game under a high time pressure. the fact that actions of others frequently force a player to re-plan, makes this information processing task even more cumbersome. however, in a practical implementation, agents can help to pre-process information and to support (or take over) the decisions of the planners."
"over the past few years we have studied the barge handling problem, which is a clear example of an inter-organizational supply chain coordination problem where distributed planning offers a promising solution compared to central coordination. the barge handling problem is the problem how to optimize the alignment of barge and terminal operations in a port. barge and terminal operators are independent companies and need to align their operations for loading and unloading containers. barge operators have to decide on the sequence in which a barge visits the terminals and the terminals in turn have to decide on the way they schedule the visiting barges. each of the actors has its own objectives and behaves opportunistically due to the competitive relations."
"acceptance of and the intention to use information technology have been studied widely in the scientific literature. in the previous section we already mentioned the technology acceptance model (tam), a widely used model stating that the intention of a user to use information technology is determined by two beliefs: perceived usefulness and perceived ease of use [cit] . much evidence has been provided that supports the statements made in tam [cit] . with respect to our study this means that perception and expectation of usefulness and ease of use are important determinants for system acceptance, which are to be enhanced by our simulation game."
"in this step, the different attribute factors are modeled by cloud theory. the three numeric characteristics of the cloud models' of the factor 'language level of personnel on board' are shown in table i, which are investigated and modified by researchers in the field. and the cloud numerical characteristics of synthetic assessment of the seafarers' general quality are shown in table ii. according to (2), we get the synthetic cloud models of factor 'language level' as follows: and the cloud cluster of 'language level of seafarers' is shown in fig.4 . when a quantitative value comes, the corresponding cloud model is fired and the corresponding optimal cloud models are found."
"in this example, the uncertainty reasoning process is a four-condition-one-rule reasoning, which is shown as in so for as mention the operator between the yi chooser and the forward cloud generator, which can be used by the different operator 'logic-and' or 'soft-and'. the 'soft-and' is realized by four-dimension cloud, and the logic-and is much easier than soft-and. we can see the difference between fig. 6 and fig. 7 ."
"according to the knowledge and experience of experts, the rules are cleaned up and standardized. totally, 93 qualitative inference rules can be established based on the fuzzy synthetic judgment."
comparing the fuzzy comprehensive evaluation and the synthetic assessment method base on cloud models are used for personal factors' analysis. we notice that there are fundamental differences between them.
"what they valued especially was the information on the availability of terminals (scenario 4) and the position of ships in the port. currently, they lack this information which makes them dependent on information of the barge shippers. moreover, the lack of information hinders them to optimize the barge rotations. the remaining part of the discussion was about preconditions that have to be met as well as practical aspects that have to be addressed. this discussion was valuable, since participants could give detailed and pointed feedback."
the planners indicated at the start of the workshop that they feared that the system would take over their jobs. they were interested but not enthusiastic to hear about our solution. during the game we made the following observations. several barge operator planners were playing the game as if they had to plan the ships in reality. they made their decisions and started to look what other players did. the simplifications we made were not a barrier for them to understand the basic notions of the multi-agent system. a few planners needed some help to understand how the interface worked. the planners enjoyed playing the game.
"the single-track model is widely used in the study of vehicle lateral dynamics. the single-track model is also called bicycle model, as shown in figure 1, which considers lateral and yaw motions under the assumption of negligible lateral load transfer, roll, and compliance steer while traveling on smooth road with constant speed. one of the main effects taken into account is the tire lateral slip phenomenon. however, the other main effects regarding vehicle behavior (such as axle deformation, axle kinematics, and suspension elastokinematics effect) are not modeled. the lateral tire force can be approximated as linear relationship with tire slip angle when vehicle lateral acceleration is less than 0.3 g. so it is fairly accurate for the linear bicycle model, and the lateral tire force generation falls into the linear range. as tire slip angle increases due to a large steering maneuver which results in large lateral acceleration, the model will lose the fidelity compared to those responses of actual vehicle [cit] . in this paper, a single-track vehicle model considering the tire forces saturation is used for state estimation and stability control."
"time offset is another key element to ensure proper interpretation of the class separability. the beginning of the records is less powerful in this regard, whereas the maximum separability is achieved at later stages. it is well known that initial calibration and sensor stabilization may cause border effects in these recordings, but more recordings, including timestamps for events, would be necessary to find out exactly what are the factors influencing this trend. as a general recommendation, the longer the records, the better, and if possible, discard the initial samples for analysis."
"the traction and brake forces of each individual wheel can be independently controlled by means of controlling the corresponding electric drive motor [cit] . active safety systems can significantly prevent vehicle accident or relieve drivers' workload. and with the in-wheel motor powertrain system, the advanced driving assistance system (abs, braking assistance, electronic stability program (esp), etc.) could be realized by software without additional mechanical or hydraulic components with greater reliability and increased safety [cit] . in-wheel motored cars are capable of offering more passenger space than traditional engine-powered cars [cit] . it is necessary to optimize the tire forces distribution and make the forces exerted by all tires efficiently, so as to keep them inside the tire friction ellipse. additionally, with stringent vehicle emission regulation and fast development of motor and battery technologies, the in-wheel motor powertrain has become one of the most popular configurations in the area of alternative energy vehicles (aevs) and are currently being developed by various car manufacturers [cit] . aevs have better mobility and fuel economy performances compared 2 shock and vibration to the conventional internal combustion engine vehicles [cit] ."
"as for the statistics employed, sampen outperforms fuzzyen in all cases. despite being an evolution and improvement, fuzzyen does not achieve the auc obtained with sampen, in contrast with previous works [cit], where fuzzyen was clearly better. this means that derivatives are not necessarily always more effective than the original metrics, and more characterization studies would be necessary to define the optimal application domains in each case. however, despite the limitations of cgm data, classical regularity estimators can successfully be applied, as with other biomedical records."
"in figure 11, it can be clearly observed that compared with no control, the vehicle with the robust control strategy could follow the driver's intention perfectly, which means the maneuverability of controlled vehicle is improved significantly. figure 12 compared the sideslip angle of mass center; we can find that the designed estimation strategy can constrain the sideslip angle effectively. this means that the robust controller could improve the vehicle stability especially when vehicle is in high speed or sharp curve turning condition. figure 13 shows the lateral acceleration of vehicle mass center comparison. it is well known that large lateral acceleration brings more load transfer and could lead to inadequate using of the tire forces. with the proposed control algorithm, the lateral acceleration is smaller compared with no control. so the proposed controller has potentials in the improvement of vehicle stability."
"the influence of record length is a very well-known weakness of most entropy metrics, not only those studied here. in contexts where the acquisition stage ensures a sufficiently long time series, this weakness can be easily overcome. however, this is not the case with blood glucose time series, where the invasiveness of the sensors and the limitations of the process (low sampling rate, battery life, calibration) do not enable measurements longer than a few days at most."
"the saturation of readings is also a usual disturbance in cgm data. a single epoch of six saturated consecutive values in the entire 1728 sample record has a great impact on the auc, greater than that of missing samples. moreover, this artifact is very difficult to remove since it would entail the reconstruction of the missing values. as a consequence, it would be advisable to implement some kind of alarm to detect this situation and implement corrective measures as soon as possible, during the acquisition stage."
"from a practical point of view, sideslip angle is considered as a significant signal in assessment of the transient response of vehicle stability. in this section, the estimation method of sideslip angle of vehicle mass center is presented based on the virtual tire forces sensor, as shown in figure 3 . the vehicle state information is estimated by using the ukf, which was proposed by julier and uhlman [cit] . detailed explanations of the ukf theory can be found in ref [cit] . the flowchart of the sideslip angle estimator is presented in figure 4 . based on (1)- (3), the vehicle dynamic system can be rewritten in the discrete-time nonlinear state transition equation:"
"where is the armature voltage, is the armature resistance, is the armature current, is the motor rotational speed, and is the armature inductance, cemf is the counterelectromotive force coefficient. and the control logic of the permanent magnet synchronous motor (pmsm) is shown in figure 6 ."
"glucose time series are not stationary [cit] . from a practical perspective in this study, this means the values obtained for the entropy metrics using a certain time window will surely differ from those obtained at another time window. in addition, many doctors have reported intuitively that the beginning of the records could probably be less reliable or stable. clinically, there are also temporal changes due to initial calibrations of the measuring devices, the learning curve related to their proper use or wearing and changes in treatment adherence before or after a clinic appointment [cit] . this is known as the white-coat effect, and it is a well-known disturbance that causes significant temporal changes in physiological markers during clinical visits [cit] . these changes can arguably play a role in the correct analysis of the resulting time series, since they are not related to the dynamics of the glucose control systems, but to other irrelevant external factors."
"in recent years, active safety systems have been developed and commercialized aiming at improving vehicle high speed safety and making drivers more aware of the situation around them. the objects of these systems are to keep the vehicle stable under extreme driving conditions and follow the driver's intention. all body motion control and disturbance forces as well as aerodynamic forces applied to the vehicle are generated in the contact patches between the tire and road surfaces. the maximum tire friction forces are determined by the road surface conditions and tire vertical forces. the summations of longitudinal and lateral tire forces cannot exceed the boundary of the tire friction ellipse. in-wheel motor evs have many advantages as a platform for vehicle motion control in the viewpoint of vehicle stability control [cit] . motors are mounted inside each wheel and driven independently, and this brings more potential in the improvement of vehicle handling and stability performance because of its fast response with precise control and high efficiency."
"in order to improve the robust performance of vehicle lateral stability especially for high speed condition, a robust controller is designed in this paper, which is insensitive to external and internal disturbances and parameters variations, such as tire stiffness variation when tire forces are in saturation. combining the sideslip angle observer, the robust controller is designed to generate the yaw moment to improve the longitudinal performances and lateral stability of the inwheel motored vehicle."
"the main idea of diabetes control is assessing a time series (blood glucose) [cit] . traditionally, this was performed by means of punctual fasting blood measurements. this was obviously inappropriate."
"the influence of the record length was characterized by increasing the number of samples used in the entropy calculations in steps of 288 samples, that is one day. as depicted in figure 5, the longer the record, the higher the auc, and therefore, the more separable the two classes are. the auc remains more or less constant up to four days, and then, it increases significantly. this could be due to the achievement of a length that enables a more robust entropy estimation (greater than 1000 samples), as recommended in some works [cit] and visually justified in figure 8 . however, it does not mean sampen or fuzzyen are not usable at short lengths, because the important feature is the dissimilarity between entropy values from each class, not their absolute values. in any case, sampen yields better results than fuzzyen for all the lengths except for extremely short records (only 18 or 36 samples). it is important also to note that records were cut from data at the center of the entire available records to avoid possible border effects and ensure more data stability."
"therefore, the entropy-likely fluctuations associated with temporal changes had to be studied with regard to the signal classification capability of these methods. for this purpose, auc was computed for three-day time windows shifted one day from zero up to three days, with two days overlapping, using records of six days long. the results of these experiments are shown in table 7, including a statistical significance analysis. table 7 . auc performance of sampen and fuzzyen on a day-by-day basis. analysis using 3-day time windows shifted 0 days up to 3 days to visualize the effect of the location of the data. performance varies depending on the specific epoch being processed, with a clear trend towards better classification using the later epochs and a significant decay in performance at the beginning of the records."
"the proposed systems help improve stability and response at high speed and make the vehicle have a tendency to understeer. an understeering vehicle is self-stabilizing through its tendency to return to straight ahead motion after any external disturbances. figures 14 and 15 plotted the front wheel active steering angle and the total added yaw moment from the controller. figure 16 presents tire force allocation results from the total added yaw moment. it can be found that, combined with the active front steering and motor in-wheel actuators, the vehicle stability is improved significantly. and the controller integrates the longitudinal and lateral dynamic to make good using of tire forces so as to avoid the saturation of tire forces."
"the presence of one-sample gaps in the time series did have a significant impact on the separability of the two classes under analysis. both metrics worsened their performance at each step, although fuzzyen appeared to be a little bit more robust. arguably, it can be hypothesized that these missing samples may hinder the classification of blood glucose records, and they should be avoided, if possible, or filtered out with some kind of interpolation. for real interference levels of 10%, the separability becomes very poor, even for a baseline auc higher than 0.90. it is also important to note that this analysis was carried out in terms of classification performance, not in terms of changes in absolute entropy values, which surely took place [cit] ."
"optimal control algorithms are widely used in implementation of the industry control, but it is not always tolerant to changes in the environment disturbances or the control system. a control system is robust if it is insensitive to differences between the actual system and the nominal model of the system which is used in the controller design [cit] . these differences are considered as model uncertainty or perturbations. application of the robust control theory is important to build the reliable and precise systems [cit] . a robust control algorithm is designed to improve the lateral stability for in-wheel ev combined with online sideslip angle observer, as shown in figure 2 ."
the yaw stability mainly depends on the steering angle input and yaw moment control. and the active front steering control is directly input as yaw moment generator. the tire longitudinal tire forces can be calculated by
"because the yaw rate is mainly affected by the steering angle input and the contribution of δ is relatively small, (6) can be further simplified as"
this study analyses the impact of the typical artifacts found in blood glucose records on the class segmentation capabilities of sampen and its derivative fuzzyen. the influence of the parameters is practically removed by a grid search of the optimal configuration for the purpose of each experiment.
"since most records were at least 288 samples long, but only 23 out of 47 achieved the required six-day duration, only those 23 were used in the experiments for all lengths (figures 6 and 7) . this way, the final dataset remained constant during the computation of all the tests. for a single day, namely the mid-288 samples, lengths of 18, 36, 72 and 144 samples were also studied. the maximum performance was achieved for the longest records, as expected. at day 6, auc using sampen was 0.90 and 0.82 for fuzzyen. for durations shorter than four days, both metrics yielded approximately the same results (auc close to 0.75)."
"there are no standardized metrics for cgms evaluation and perhaps different goals on different patients may require different metrics. specifically concerning complexity, it is crucial to choose the right complexity metric, optimize its parameters and analyze the influence of sample length, missing data, sensor saturation and time offset. this is precisely the goal of the present paper. we assessed the metric's discriminating power comparing two time series of a sample of patients before and 10 months after undergoing a therapeutic maneuver (djlb) known to modify glucose metabolism, and we evaluated if and how these metrics were able to detect those changes."
"the performance was assessed using the area under the roc curve (auc) [cit] values obtained for the classification of two input classes (months 1 and 10 for the database described in section 2.2). auc is a widely-used measure in a diversity of classification schemes, including those based on entropy metrics in the context of biomedical applications [32, [cit] ."
where is the closed-loop response time which is a control characteristic of motor torque controller; is the motor driving torque command from control algorithm; motor is the motor torque coefficient.
"this distance is thresholded using a predefined parameter r, the number of distances found, ∀j, and for a specific i, within such a threshold, stored in a counter b i (r). this process is repeated ∀i and the final value averaged:"
"as expected, record length is pivotal for a reliable entropy assessment of the records. although classification potential was always higher than 0.75, even for 288 samples, measured in terms of auc, more robust results were obtained for longer records. in any case, we would recommend not to use records shorter than 24 h, since it is important to cancel out the chronobiological effects on the glucose dynamics, for example sleep and the fasting periods and meals during the day. for shorter series, it would be necessary first to characterize these chronobiological effects."
"although more than one missing sample can be found consecutively (missing epochs), that case is easily detected and usually addressed splitting the record at that point (the longer the gap, the less reliable the calculations become [cit] ), and therefore, only the most frequent case of a single missing sample was analyzed."
"assuming that the torque split is 40 to 60 percent between front and rear axle, the four wheels driving torque can be obtained according to (34)∼(36) with consideration of the tire force friction circle constrain."
"given the claimed better performance of fuzzyen reported in the literature [cit], in comparison with its predecessors apen and sampen, we wanted to study the applicability of this new metric to glycemia data, taking into account the possible ill effects caused by the record features stated above, and their characterization. based on the dataset obtained from a study of the endocrine consequences of duodenal-jejunal exclusion [cit], this paper comparatively assesses the capability of sampen and its derivative fuzzyen to distinguish between classes, under different conditions in terms of record length, artifacts and border effects. the clinical implications of such a classifier can be varied and diverse. changes in glucose dynamics could be correlated with other anthropometric, biochemical or hormonal characteristics [cit] in order to try to anticipate the rate and intensity of metabolic improvements after the exclusion and better understand the possible mechanisms of its effects. it could also be used as a screening tool for patient/treatment selection."
"the experimental dataset was recorded at the third department of medicine, department of endocrinology and metabolism, charles university in prague, czech republic. this database contains 91 records of 30 diabetic patients that underwent a duodenal-jejunal bypass liner implantation. records contain measurements at baseline (before implantation), 1 month and 10 months later and 3 months after removal. durations span from a few hours (796 samples being the shortest) up to more than 6 days for a few records (2022 samples being the longest), with a sampling period of 5 min. sensors had to be recalibrated twice per day, mainly at the beginning of the recordings, and that is why possible border effects were likely to be present during the first hours or even days of the recordings due to the learning curve."
"the metric for the classification was sampen or fuzzyen, and the input time series underwent different transformations to account for the effects targeted in this characterization study: record length (section 3.2), missing samples (section 3.3), sensor saturation (section 3.4) and time offset (section 3.5). the block diagram of the analysis proposed is shown in figure 1 . figure 1 . block diagram. the duodenal-jejunal bypass liner (djbl) was implanted into a number of obese patients with type 2 diabetes mellitus. glucose time series for each one were recorded one month and 10 months after implantation. this study analyses the possible differences in the glucose control at those stages and the influence that artifacts found in these records may exert on non-linear metrics performance, specifically sampen and fuzzyen. the outcome of the process is the estimated classes a1 * and b1 * ."
"in order to improve the yaw stability of vehicle, the active front steering system and motor differential driving system work coordinately. the rule of the control coordination can be described as follows: when vehicle lateral acceleration is relative large, it means that tire tends to work close to the saturation zone. the values of yaw control torque will increase by differential longitudinal tire forces, so as to make a good use of tire forces. likewise, in order to improve the longitudinal stability and traction ability of vehicle, when lateral acceleration is small, the active front steering will work."
"missing samples is a common issue in blood glucose time series (figure 9 ), but there is no study so far that has characterized the impact of the incomplete data on the signal classification performance. researchers usually discard records or epochs with too many missing samples (20% is a heuristic threshold we had used in the past), and a common approach for short series of missing data is to reconstruct those values from the neighboring observations available. the experimental database included many records with missing samples, but they were interpolated before the experiments (pre-filtering). the assessment of the impact of missing samples was conducted using random synthetic ones (spikes down to zero), with customized percentages ranging from 2.5% up to 10%, in 2.5 steps. the results are shown in table 5 . in order to gain more insight into the influence of missing samples, the 10% case in table 5 was re-analyzed after applying an interpolation scheme to remove all the gaps. using the fuzzy metric, from an auc of 0.74, sensitivity of 0.72 ± 0.075 and specificity of 0.686 ± 0.084, the performance after interpolation was back to that of the initial case shown in table 1 . this was also the case for sampen, which suggests that linear interpolation is a suitable tool to account for missing samples."
where δ is the corrected yaw moment generated from differential driving from motors; δ is total required driving torque from driver; is the th longitudinal tire force; is the th tire roll radius.
"cgm data are an extremely useful source of metabolic information with a myriad of current and future applications. however, records are usually very short and noisy, mainly in terms of missing samples and sensor saturation, and these artifacts may arguably interfere with the correct interpretation of the results using the otherwise successful entropy features. this study was aimed at characterizing the changes induced by such artifacts, enabling the arrangement of countermeasures in advance."
"missing samples seem to interfere significantly with the estimation of the underlying dynamics in glucose time series. even with a very low ratio of 2.5%, there was a significant reduction of the auc obtained, and this reduction was consistent along all the ratios tested. as for relatively usual higher ratios of 10% missing samples, it could become impossible to distinguish between the two classes. fortunately, this artifact can be easily removed by just interpolating the missing samples, and this should be a routine procedure in the preprocessing stages of this kind of biomedical record."
"the possible effect of the specific time window on the analysis is quantified in table 7 . time windows of three days were taken from the beginning of the records of at least six days long of the experimental database, and the calculations of sampen and fuzzyen were repeated for all the possible windows, shifted one day in each case, with two days overlapping. this experiment was devised to find out if the global differences found were due only to non-stationary changes, or if the differences were regularly distributed along the entire records. although class differences are fairly significant in most of the epochs analyzed, there is a clear trend towards higher differences at later stages. this may be due to a more stable glucose monitoring, a better device calibration or just a correlation with the learning curve linked to the whole process of cgm. it is important to note, however, that the beginning of the records seems to be the most unreliable part in terms of class segmentation, namely there seems to be a border effect on cgm records that should be avoided during analysis."
"the sine-with-dwell maneuver is widely used to evaluate the performance of electronic stability control system [cit] . in this section, in order to evaluate the performance of the stability control algorithm, the numerical simulation was implemented in the matlab/simulink environment and cosimulated with the carsim software. carsim is commercial vehicle dynamics software which is widely used in automotive industry. the performance of robust control is validated under sine-with-dwell maneuver. figure 7 shows the steering wheel input angle of the sine-with-dwell maneuver. from this simulation scenario, all the dynamic characteristics of the vehicle can be adequately described, and the yaw stability of motor in-wheel ev can be fully verified. in the simulation, a vehicle runs at a constant speed of 50 km/h. the vehicle parameters are listed in under the sine-with-dwell simulation scenario, the robust control for vehicle stability combined with the wheel torque distribution strategy should not only meet the demand of vehicle steering maneuverability improvement but also improve the vehicle body stability under high speed condition. yaw rate response is the maneuverability index, while the sideslip angle of mass center is an important index for vehicle stability [cit] ."
"this paper proposed the ∞ robust control strategy for motor in-wheel four wheels driving electric vehicles. the proposed control strategy integrates the longitudinal and lateral dynamic control through the active front steering and yaw moment control systems considering the saturation characteristics of tire forces. an upper level control system is designed by using ∞ robust control strategy combined with the online sideslip angle observer to specify a desired yaw moment and correction front steering angle to work on the evs. the four-wheel torque is obtained optimally by the wheel torque distribution control algorithm. according to the simulation results, it can be asserted that the robust control algorithm could improve vehicle maneuverability and high speed stability. this is very important for vehicle safety. the future works will be focusing on how to distribute the torque smoothly among the axles according to different road friction coefficients or driving situations."
"it is important to note that most entropy measures are very sensitive to input record length, and this fact may contribute to the differences in performance with the six day-long records and their shorter counterparts. namely, the differences found are not only due to physiological or monitoring reasons, but also due to analytical needs. this sensitivity is graphically illustrated in figure 8 . sampen becomes stable at approximately 1000 samples, nearly four days, whereas fuzzyen stability is reached at 200-300 samples, one day. this may also explain why the performance of fuzzyen does not vary with length as much as that of sampen, mainly from day 4, as depicted in figure 5 ."
"the main metric to quantify the performance of the methods was auc, including a statistical significance assessment for some cases, an loo cross-validation, and a global classification accuracy score for the optimal configuration. auc is a very popular metric to assess the performance of a classifier due to its simplicity, robustness (insensitive to class asymmetry) and straightforward interpretability: if a classifier a has a greater auc than a classifier b, a has a better average performance than b [cit] . auc quantifies the classifier's ability to avoid false classification [cit], with a performance threshold for random guessing of 0.5. in other words, the closer auc is to 1.0, the better is the expected performance of the classifier."
"the paper is organized as follows: vehicle dynamic model and control reference model are presented in section 2, while in section 3, the sideslip angle estimation is designed for the controller, and system controller is designed and analyzed based on robust control theory. tire forces distribution and in-wheel motor control are illustrated in sections 4 and 5, respectively; the efficiency of the proposed control is demonstrated by simulation results in section 6. at the end of this paper, the conclusion and future work are given."
"sensor saturation refers to the impossibility for the device to provide readings above (end of scale) or below (minimum sensitivity) some certain thresholds, usually due to technical constraints or sensor attachment problems. this is very usual in real continuous blood glucose monitoring. before the user or the physician notices there is something wrong with the placement of the sensor or any other device malfunction, such as the loss of or incorrect calibration parameters, some samples have already been acquired. figure 10a depicts a record with low saturation values at 2.2 mmol/l, whereas in figure 10b, the record is saturated at 22.2 mmol/l at some points (limitation of the recorder used). these records did not belong to the experimental dataset. the assessment of the impact of saturation was conducted using random saturation pulses, of 22.2 mmol/l of amplitude, and with customized lengths of six up to 60 samples, in six-sample steps. they were randomly located in the records in a similar way as the spikes, but only one pulse per record. the quantitative results are shown in table 6 . table 6 . auc results obtained for the analysis of the influence of reading saturation on the classification capability."
"the performance of both metrics depends on the value of the parameters m and r, and specifically for fuzzyen, n. these values are very application specific, and for optimal performance, an exploratory analysis of a range of values must be carried out in advance (grid search). during the experiments, and in all cases, r values tested ranged from 0.15-0.30, in 0.01 steps, and m values from 1 up to 3. this way, the influence of the parameters on the results was minimized, and only optimal configurations in the specified subset, in terms of maximum auc, and in accordance with the recommended values for m and r, were considered."
"sensor saturation is another record disturbance that also significantly damages classification performance. even for very short saturated epochs (60 samples, five hours at one sample per five minutes, 3.5% of the six-day records), the two classes become almost undistinguishable. this is also another quite frequent issue in most blood glucose records, and it is almost impossible to remove using signal processing techniques, since the real signal cannot be reconstructed. therefore, this disturbance should be detected and corrected as soon as possible at the acquisition stage."
"in order to study the static behavior of the bistable module, a commercial force sensing probe is used (see fig. 5 ). the blockage force can be evaluated using this force sensor. the experimental set-up is shown in fig. 5 . an optical microscope is installed on the scene to observe the measurement. we move the force sensor to push the blocked bistable"
"based on the aforementioned motivation, this study considers electromagnetic field optimization (efo) algorithm to solve the scpip. the efo is a relatively new and effective algorithm on global optimization problems, and it has been shown that the efo outperforms other optimization algorithms and effectively balance the exploration and exploitation performance [cit] . conversely, it is also reported that the traditional efo tends to suffer poor exploitation performance on specific optimization problems [cit] . therefore, this study introduces an adaptive version of electromagnetic field optimization to solve the scpip efficiently, which is called the adaptive efo (aefo). the proposed aefo adaptively controls the algorithm parameters and explores the search space effectively, especially in the early stages of the search process, whereas exploitation is emphasized in the latter phases. in addition to the adaptive control of parameters, boundary control and randomization procedures are modified in the algorithm. in computational studies, performance of the proposed algorithm is tested into two parts. first, the aefo is performed on a recently introduced global optimization benchmark problem set and compared to the efo solutions to identify the efficiency of the adaptive control mechanism of the proposed algorithm. in the second part, the aefo is tested on the well-known pv models and compared to the original version of the efo, artificial bee colony algorithm (abc), particle swarm optimization (pso), and differential evolution algorithm (de) in identical test conditions. the aefo is further tested against recent metaheuristic algorithms, which are presented to solve the scpip. computational results and statistical tests show that the aefo significantly achieves superior performance to competitor algorithms."
"in the classification phase of the efo, the population is divided into three groups with different polarities. in this point, two different control parameters as p field and n field are employed in order to determine the emps for each group. the p field and n field represent the percentage of the allocated solution for positive and negative part, respectively. the remaining solutions form the neutral part."
"the webtool gui is the standard interface allowing non-technical users (e.g., decisionmakers, msp stakeholders, planners) to perform cea and muc analyses starting from the pre-set case studies."
a ramp voltage is applied to the thermal actuators (a1) to actuate the bistable mechanism from stable position 2 to stable position 1. fig. 11 shows the results.
"compared to other existing decision support tools in msp, the tools4msp ''approach'' is more flexible as it (1) incorporates a multi-objective toolset (cea and muc) which can be extended for other analysis purposes (e.g., scenario analysis, comparative msp or ecosystem services assessment); (2) it enables management and treatment of different datasets and formats and (3) it provides different levels of usability ranging from experienced modellers to more user-friendly modelling through guis."
previously reported solar cell data were used to support this study and are available at doi.org/10.1080/01425918608909835. this prior study (and datasets) is cited at relevant places within the text as reference [cit] . 14 international journal of photoenergy
"where i sd1 and i sd2 are, respectively, the diffusion and saturation currents, and n 1 and n 1 are, respectively, the diffusion and recombination diode ideal factors. the other parameters are the same as defined for the sd model."
"additionally, the estimated solar cell parameter values and corresponding rmse values, which are achieved from the best run, are listed in tables 9 and 10, it is obvious that the aefo has a better convergence performance than the efo, pso, abc, and de, which also shows the outperforming convergence ability of the aefo. second, to further show the superiority of the aefo, the performance of the aefo is compared with the ten state-ofthe-art scpip optimizers, such as the abc [cit], artificial bee swarm optimization (abco) [cit], biogeography-based optimization algorithm with mutation strategies (bbo-m) [cit], cat swarm optimization (cso) [cit], generalized oppositional teaching-learning-based optimization (gotlbo) [cit], harmony search-based algorithm (hsa) [cit], improved jaya algorithm (ijaya) [cit], mutative-scale parallel chaos optimization algorithm (mpcoa) [cit], and teaching-learningbased abc (tlabc) [cit] . tables 11 and 12 present the best found rmse values, corresponding the parameters, mean, and standard deviations of rmse over the runs obtained by the aefo and other ten algorithms on sd and dd. it should be noted that the results of competitor algorithms are directly taken from the corresponding studies. among the results listed in tables 11 and 12, the best rmse, mean, and deviations on each problem among all algorithms are shown in bold. as per the results in table 11, the aefo reaches better solutions than all algorithms expect the cso, where they show similar performance. on the other hand, the aefo achieves smaller standard deviations than the cso, which indicated the robustness of the aefo. inspecting the results in table 12, it is evident that the aefo outperforms all algorithms in terms of mean and standard deviation. as summary, results in tables 11 and 12 reveal that the aefo has the potential to be very effective in solving the scpip."
"as the last step, the fitness of the v is evaluated, and if it is better than the worst solution of the current population, it is inserted to the population, and the worst solution is eliminated. the processes of classification, candidate emp generation, random search, and selection are repeated until some termination criterion is satisfied."
"in this section we present a cumulative effects assessment based on the stand-alone tools4msp package applied for the adriatic sea. the case study set up has been downloaded from the tools4msp geoplatform. it consists of a directory named ''adriatic_sea'' containing the input geospatial layers related to human uses, environmental components and pressures and environmental component sensitivities. an example of the case study set up is released within the tools4msp software package (https://github.com/cnr-ismar/tools4msp/tree/master/data/demo_case_study) and is available for test and demo purposes."
"proposed aefo is tested on a recently introduced global optimization benchmark problem set and compared to the standard efo. results of these experiments show that the aefo exhibits better performance and outperforms the efo by finding better results for most of the functions. in the second part of the computational studies, the aefo is tested on a well-known scpip benchmark problem set, which is generated from 57 mm diameter commercial silicon solar cell. first, the performance of the aefo and aefo variants is pointed out by comparing it with the efo. after that, the results of the aefo are compared with the results of the efo, abc, pso, and de in detail. since the i − v characteristics of the solar cells are determined with very small values, the rmse values are directly affected by the rounding procedures while computing the errors. in addition to the comparisons based on the rmse values, t-tests are carried out to reveal whether there exists a significant difference between the aefo and the other metaheuristic approaches. finally, the performance of the aefo is compared with ten stateof-the-art algorithms. as a result of the computational studies, it should be specified that the proposed aefo is capable of finding effective results for the scpip concerning other metaheuristic algorithms considered in this study."
"in the sense of optimization problem for accurate estimation, the error function for the scpip can be transformed into equations (5) and (6) for the sd and dd, respectively. the x in these equations represents the decision variables to be optimized for the scpip, where table 1 shows the descriptions of the decision variables and their lower and upper bounds for the sd and dd models [cit] ."
"as a gap exists between the actuator and the bistable mechanism, they will be in contact after a delay. mechanism is switched when the voltage reaches 17v. in fig. 10, the first phase from stable position 1 to unstable position (red point) is controlled by the actuators. the duration is about 117 ms for a displacement of 25 µm. the second phase is very fast compared to the first phase and it can be neglected. this transition does not present overshoot and vibrations. since the final position is blocked by a stop block, the accuracy and repeatability are ensured. this result has been obtained on all the measurement tests."
"problem. in this section, two sets of tests are carried on different pv models (i.e., sd and dd models) to exhibit the performance of the proposed aefo on the scpip. first, the effect of different positive and negative field ratios on the performance of the aefo is analyzed. second, the aefo is compared to the original efo and other well-known metaheuristic algorithm (i.e., abc, pso, and de) in detail. furthermore, in this subsection, results of the aefo are compared with a number of state-of-the-art algorithm results. for these experiments, the population sizes are set to 40, and 5-second time limit is employed as the termination criteria for all algorithms."
"after activation mechanism, when the bistable mechanism moves the blockage force is obtained. modules with three different distances between the stable positions have been designed the measured blocked forces are shown in table1."
"lower and upper bounds for the index j, respectively. last, rand is a random real number within the range 0, 1 . then, the fitness of the emps in the initial population is determined, and the population is sorted based on the fitness values in nonincreasing order."
"in fig. 4, two stable positions are shown after the activation. the stable position 1 is now in the blocked state. it can be switched by thermal actuators between these two stable positions. fig. 2 . sem picture of a bistable module before activation. bistables modules with different distances between the stable positions were designed. according to the designed digital microrobot, the minimum distance defines the resolution of the microrobot. the resolution depends on the fabrication process."
"to solve the scpip, there exist several solution approaches in the literature, which are mainly divided into two groups: deterministic and heuristic solution approaches. regarding the deterministic approaches, a number of methods are employed by the researchers, such as nonlinear least squares based on the newton model [cit], iterative curve fitting [cit], lambert w-function [cit], and j-v model [cit] . however, these deterministic solution approaches are not efficient to solve the scpip since they need continuity, convexity, and differentiability conditions for being applicable and involve heavy computations [cit] . to cope with the complexity of the scpip, heuristic methods are used as an alternative to deterministic solution approaches."
"the tools cea and muc models implemented can support the development of maritime spatial plans within the implementation process of the msp directive (2014/89/ce) in various case study areas and marine waters in the mediterranean sea and beyond. the package is regularly upgraded within the tools4msp geoplatform (data.tools4msp.eu) including ongoing implementations of pressure specific analysis of cea, cea backsourcing (cea-b; [cit] ) and the integration of marine ecosystem services oriented analysis of anthropogenic threats (mes-threat) in support of environmental management and resource restoration (depellegrin & blažauskas, 2013; [cit] b) . the tools4msp software package was used in adriplan (adriatic and ionian maritime spatial planning) and ritmare (ricerca italiana per il mare) projects and is currently implemented by various research communities within ongoing european projects around the mediterranean, such as supreme (supporting maritime spatial planning in the eastern mediterranean) or portodimare (geoportal of tools & data for sustainable management of coastal and marine environment)."
"algorithm 3 summarizes the main steps of the aefo, which also presents the main change of the proposed algorithm as adaptive control mechanism of ps rate and r rate (line: 35) and the modified boundary check and randomization procedures (line: 26 and line: 28) are shown in algorithm 3."
"to reach the desired blocked force and an accurate distance between the two stable positions, two stop blocks are designed. stop block 2 (see fig. 1 -e) can be easily designed by putting a frame in front of the stable position. however, it is difficult to obtain a second blocked position because of the monolithic fabrication. a dedicated stop block (stop block 1) was designed. it allows obtaining a blocked force in the stable position (see fig. 1-d) . activation is required to put it in position. this bistable mechanism is fabricated using silicon on insulator (soi) wafers with 100µm thick device layer (young's modulus: 69gpa), 300µm thick handle and 1µm sacrificial oxide layer."
"dynamic characteristics show duration, speed, accuracy and repeatability of state changes of a micro-positioning or microrobot system. we will study these characteristics of the bistable mechanism from a stable position to the other stable position in this section. for this measurement, the bistable modules are prepared for an in-plane measurement. we use a high resolution (10 nm) interferometer from sios technology to test the dynamic response of bistable mechanism by measuring the displacement of the shuttle of a bistable module. all the test devices are mounted on an anti-vibration table (see fig. 7 )."
"the webtool gui implements a four-steps workflow: (step 0) webtool selection; (step 1) case study area selection; (step 2) study area selection & dataset configuration; (step 3) geospatial and statistical outputs. an example of the outputs (step 3: results) [cit] '' is shown in fig. 4 . through the gui, the geospatial"
"as stand-alone library, tools4msp requires advanced programming skills for its usage, but provides more flexible integration with other libraries and python packages also outside tools4msp modelling framework. it is particularly suitable for planning authorities seeking advanced modelling procedure for msp/iczm and management purposes. the scientific community, consultancies and programmers can use and further develop the library for different research objectives and integration into decision-support-systems."
"2.1. single-diode model. the sd model consists of a current source in parallel with a diode, a shunt resistor to represent the leakage current, and a series resistor to denote the losses of load current. this model has commonly used to describe the static characteristics of solar cells because of its simplicity and accuracy [cit] . figure 1 represents the equivalent circuit for the sd model, where v t is the terminal voltage, r s is the series resistance, r sh is the shunt resistance, i t is the terminal current, i ph is the photo-generated current, i d is the diode current, and i sh is the shunt resistor current."
"second, the boundary control and randomization procedures of the traditional efo are modified. in the aefo, electromagnets that become higher or lower than the limits are set back to corresponding limits, instead of the random generation within the search limits. additionally, in the random search step of the proposed aefo, a randomly selected electromagnet is regenerated within limits instead of a sequence-based approach."
"in order to validate the performance of the proposed aefo, computational experiments are performed into two main parts. first, the aefo is tested on a recently introduced global optimization benchmark functions and compared to the efo. in the second part of the computational studies, a well-known scpip benchmark problem set is used. this part initially presents the effects of the parameter values on the algorithm performance. then, the aefo is compared to the efo and also three well-known metaheuristic algorithms: abc, pso, and de. finally, the results of the proposed aefo are compared to ten state-of-the-art scpip optimizers. for the experimental studies, the aefo and other competitor algorithms (efo, abc, pso, and de) are implemented using matlab 8.1 and executed on the same computer with intel xeon cpu (2.67 ghz) and 16 gb of memory."
"an extended analysis performed by the ''ebm tools network'' (https://ebmtoolsdatabase. org/) in support of ecosystem based management has recollected and classified various tools, with respect to types, costs, skills, data and technological requirements. two categories of tools have emerged in the recent years as analytical support to decision-makers: the sea use conflict analysis and the cumulative impacts assessment."
"regarding the popular metaheuristic algorithms, simulated annealing algorithm [cit], genetic algorithm [cit], particle swarm optimization algorithm [cit], differential evolution algorithm [cit], pattern search [cit], artificial bee colony algorithm [cit] are widely used for the scpip. in addition to these well-known heuristic algorithms, there exist several papers in the literature which consider more recent approaches, such as bacterial foraging algorithm [cit], teaching-learning-based optimization algorithm [cit], biogeography-based optimization algorithm [cit], chaos optimization algorithm [cit], artificial fish swarm algorithm [cit], bird mating optimizer approach [cit], artificial immune system [cit], evolutionary algorithm [cit], cat swarm optimization algorithm [cit], moth-flame optimization algorithm [cit], jaya optimization algorithm [cit], chaotic whale optimization algorithm [cit], imperialist competitive algorithm [cit], bee pollinator flower pollination algorithm [cit], shuffled complex evolution algorithm [cit], memetic algorithm [cit], interior search algorithm [cit], collaborative swarm intelligence approach [cit], and cuckoo search algorithm [cit] . on the other hand, it has been proven by no-free-lunch theorem [cit] that none of these algorithms is able to solve all type of optimization problems. as a result of no-free-lunch theorem, it should be denoted that a new algorithm is always likely to exhibit better performance on the scpip compared to the existing solution methodologies."
"the msp process involves several user categories, from data producers (e.g., domain experts like ecologists and modellers) to stakeholders and planners. it requires a solid command of geographical information to create a more comprehensive understanding of coastal and marine areas and to support management and planning policies. the development and implementation of spatial data infrastructures (sdi) at multiple levels (i.e., local, regional, national and global) matches the need to make geographical data more accessible and interoperable [cit] . however, various authors [cit] have highlighted the importance of the integration of geoportals in the context of sdis and the role of a user-driven and community-based development as fundamental aspects for an effective and efficient use of the resources [cit] ."
"in the literature, a lot of works concerning the design and the fabrication of smart microgrippers as microrobot end-effectors to perform micromanipulation tasks are presented. the designed microgrippers usually consist of an amplification mechanism which permits to amplify the motion induced by microactuators. this approach allows reaching a high resolution [cit] . other designs of microrobots include fabrication of mobile microrobots with micrometer dimensions. b.r. donald designed a mobile microrobot with 200µm in length [cit] . us national institute standards and technology (nist) proposed a mobile microrobotics challenges aiming to design microrobots no bigger than 600 micrometers. [cit] have been successfully used to actuate various types of microrobots. however, despite their intrinsic high resolution, these materials present some drawbacks, making both the modeling and efficient control big challenges. they present complex, nonlinear and sometimes non stationary behaviors [cit] . although the closed-loop control can avoid these problems, the integration of small sensors into the microrobot is very difficult [cit] ."
"as previously, since there is a gap between the actuator and the bistable mechanism, the contact is obtained after a delay. the bistable mechanism is switched when the applied voltage reaches 17v. in fig. 12, the first phase (from the stable position 2 to unstable position) is similar to the transition from the stable position 1 to unstable position. however, during the second phase from unstable position to the stable position 1, overshoot appears. an overshoot of 4.5µm is measured (see fig. 12 ). actuation time between the two stable positions almost depends on thermal actuators because the free motion of bistable mechanism from the unstable position to stable position is very fast and it can be ignored (see fig. 12 )."
"as a future work, this study may be extended by considering other mathematical models for the solar cells. furthermore, the proposed aefo can be performed on new benchmark problems generated by different commercial solar cells to further validate the performance of the algorithm. finally, the proposed algorithm may be adapted for other continuous optimization problems existing in the literature."
"the candidate solution generation step of the efo is the most important part of the algorithm [cit] . in this step, one emp from each group is selected randomly. then, a random number between 0 and 1 is generated. if this number is lower than the predetermined control parameter, ps rate, then the corresponding electromagnet of the candidate emp is set as the electromagnet of the emp from the positive field. otherwise, the electromagnet of the candidate solution is determined as follows:"
"regarding the sd and dd models described above, the scpip can be defined as identifying the parameters of equation (1) for the sd model and equation (2) for the dd model within their lower and upper bounds. the aim of the problem is to estimate the best parameter values for the solar cell models that produce an accurate approximation between the i − v measurements from the physical experiments and the values from the mathematical model. hence, equations (1) and (2) can be rewritten as an error function as shown in equations (3) and (4) for sd and dd models, respectively."
"the remainder of the paper is organized as follows. in section 2, the scpip is described and the mathematical formulation of the problem is given. section 3 presents the details of the efo. section 4 introduces the proposed aefo for the scpip. computational results are given in section 5. finally, a conclusion part with future research perspectives is provided in section 6."
"1 although the system is currently tethered, the next prototype will include a wireless controller so that the system can be controlled from a mobile application. conclusions: this advanced simulator allows for unlimited possibilities with regard to creating clinical scenarios. our ambition is to become a reference ecmo training center in poland so that our highfidelity ecmo simulator can be used to its full potential and for the benefit of more clinicians and their patients around poland."
"the plugin eases data transformation operations reducing the need of manual data preparation and standardization procedures. furthermore, archiving the pre-processing expressions in combination with the case study makes the transformation of the input data more explicit and the entire process more transparent and replicable."
"a broad user community composed by decision-makers, planners, academics, research institutions, msp stakeholders and the general public can use the case study configuration to run the build-in version of the tools4msp library implemented in the geoplatform (fig. 2, webtool gui) . more in detail, the tools4msp webtool gui implements a four step workflow: (step 0) webtool selection (muc or cea); (step 1) case study selection, choosing from a pre-set of case studies; (step 2) case study configuration, optionally outlining a custom subregion of analysis or a custom combination of human uses, pressures, and environmental components; (step 3) generation of geospatial and statistical outputs. the outputs are published in geonode and accessible through standard interoperable services. the produced reports, graphics and statistical outputs are archived in a dedicated data store catalogue and can be further visualized and re-used also by non technical stakeholders. in the results section an application of the tools4msp geonode plugin for case study setup and cea analysis in the adriatic sea is provided."
"icrorobots are widely used for microassembly and micromanipulation of artificial and biological objects with dimensions ranging from 1 micrometer to 1 millimeter. resolution and accuracy in the submillimetric range are needed in order to interact with micrometer sized objects. methods and strategies used to build conventional robots are often not applicable in the microworld due to scale-down effects [cit] . new mechatronic approaches, new actuators and new robot kinematics are required."
"in this paper, an adaptive version of the efo is introduced to solve the scpip for single-and double-diode mathematical models. the proposed aefo efficiently searches the solution space with regard to adaptive changes on the importance of negative and positive electromagnetic particles. to test the performance of the proposed algorithm, computational studies are carried out into two parts. in the first part, the"
"(ii) an adaptive version of the efo is introduced by enriching the algorithm employing an adaptive search strategy. additionally, modified boundary check and randomization procedures are used for the candidate solution generation. by these novel modifications, the performance of the traditional efo is improved (iii) detailed comparisons between the efo and the aefo variants and also between the aefo and the other recent algorithms are presented. the outperforming performance of the aefo is proved by statistical significance tests"
"since the scpip is a global optimization problem, the proposed aefo is easily adapted to solve the sd and dd models by integrating equation (10) to the algorithm as the cost function. the decision variables of both the sd and dd models are defined as the electromagnets of the emps. on the other hand, the lower and upper limits of the solar cell parameters are set as the boundary values of the electromagnets. with regard to this solution representation, the rmse value of any solution vector represents the fitness value of figure 3 represents the flowchart of the parameter estimation process for the sd and dd models by the proposed aefo."
"sea use conflict analysis has been extensively applied in different geographical contexts [cit] to investigate and spatially identify conflicts between coastal and marine activities for current conditions and for the comparison of possible future scenarios. in particular, the coexist project developed the grid tool (georeferenced interactions database; [cit] ) to analyse the level of coexistence among uses, depicting areas where different sectors more likely overlap in space and time."
"for the experiments, the i − v characteristics of a 57 mm diameter commercial (r.t.c. france) silicon solar cell (under 1000 w/m 2 at 33°c) are used as the benchmark data [cit] . this benchmark set has been widely used to evaluate the performance of different optimization algorithms [27, 32, 34, [cit] . the lower and upper values of the solar cell parameters of both models are given in table 1 ."
"the maritime use conflict (muc) tool implements the coexist methodology to estimate the spatial distribution of the conflicts between sea uses. the inputs of the tool are: (i) the area of analysis; (ii) the grid cell resolution; (iii) layers of presence/absence for each human use present in the area (e.g., location of aquacultures, location of oil and gas platforms); (iv) an expert based characterization for each human use through four attributes (vertical scale, spatial domain, temporal domain and mobility). according to the attributes of each use three pre-defined rules, included in the coexist methodology are dynamically applied to estimate the potential conflict score for each pair of uses. the potential score varies from 0 (no conflict) to 6 (very high conflict). afterwards, the area of analysis is subdivided into regular grid cells according to the specified resolution and, on each cell, information about spatial overlapping human uses are extracted. finally, on each cell the total muc score is calculated summarizing the potential conflict score for each pair of overlapping uses. the main output is a geospatial distribution of muc score over the entire area of analysis. [cit] ."
"the computation results for the sd model are shown in table 5, which reveal that the standard deviation of the aefo is less and the mean is better than the peer algorithms. as can be seen from table 5, the mean rmse values achieved by the aefo_0.05p_0.45n is 9 860219e − 04 with 1 747734e − 11 standard deviation after 30 runs. the aefo_0.05p_0.45n is apparently superior to all competitor algorithms except aefo_standard, where there is no statistically significant difference between the two algorithms. for the dd model, which is presented in table 6, the aefo_0.05p_0.45n reaches the mean rmse value of 9 872174e − 04 with 1 966037e − 06 standard deviation after 30 runs. it can be clearly concluded from table 6 that the aefo produces promising results and outperforms all algorithms. as a result of the experiments, the p field and n field values are set as 0 05 and 0 45, respectively. further, the aefo_0.05p_0.45n is depicted as the aefo for the upcoming comparison studies."
"in fig. 2 the detailed implementation architecture of the tools4msp geoplatform is presented. the geospatial cms is the core of the system and is based on the geonode [cit] software, a django-based web platform for developing community-based sdi. geonode facilitates the upload and management of geospatial datasets, making them discoverable and available via standard open geospatial consortium (ogc) protocols and web mapping applications. geonode also allows users to automatically upload, describe and share the outputs produced by the tools4msp package. the tools4msp package is python-based open source software available on github [cit] ) . the tools4msp core library implements the algorithms for cea and muc geospatial modelling and the base functionalities to read the input geospatial datasets and configurations, to apply data transformation operations (such as normalization, different users can have different modes of interaction with the geoplatform: administrators perform the case study setup, by specifying the connectors to the geospatial repositories and defining the pre-processing chain for environmental and human use layers harmonization and utilization in the case study (fig. 2, case study setup gui)."
"in the initialization step, the efo generates a randomly distributed population of n, which is made of electromagnets within the search space, where n represents the number of the emps, i.e., population size. then, the fitness of the initial population is evaluated, and the population is sorted regarding their fitness in descending order. then, the population is classified into three groups: (i) fittest emps as positive part, (ii) emps having the lowest fitness as negative part, and (iii) the remaining emps as neutral part. afterwards, one emp is selected from each group, and a new candidate emp is formed using the search mechanism of the efo. in order to keep the population diversity and to improve the exploration ability of the algorithm, one electromagnet of the candidate solution has a chance to be updated in the random search phase. last, the fitness of the candidate emp is evaluated, and if it is better than the worst particle in the current population, then the candidate will be inserted into the population, whereas the worst emp will be eliminated."
"(ii) the search mechanism in equation (12) shows strong exploitation characteristics. first, better solutions attract and the worse solution will repel the selected neutral particle. second, the golden ratio is employed, where more weight is given to the attraction force. moreover, by utilizing the ps rate parameter, candidate electromagnet has a chance to be copied directly from the positive emp (iii) efo generates one candidate emp in each cycle, and the fitness of the candidate is only compared with the worst solution in the current population. if the candidate solution is not better than the worst, there will be no chance to be accepted to the population (iv) the random search part is the only process in the efo, which is responsible for exploration"
"according to the results of the last section, the actuation of the bistable mechanism from stable position 2 to stable position 1 shows overshoot which is not suitable for microrobotics or micro-positioning. indeed, the final position should not be exceeded. in order to control the switching and avoid the overshoot without feedback, we propose an open-loop control strategy to obtain a damped transition. the strategy is based on the use of two pairs of thermal actuators during switching operation. the principle is based on the use of one pair of thermal actuators (a2) to catch the bistable mechanism during the movement (see fig. 13 ). we use the set-up described in fig. 8 to make the experimentation."
"to move the bistable mechanism from a stable position to the other one, we designed two pairs of thermal actuators. the chosen dimensions of the thermal actuators enable to switch the bistable mechanism ( fig. 1-c) ."
"in the random search step of the efo, the random real number is generated between 0 and 1. if the generated random number is lower than the control parameter, i.e., r rate, then one selected electromagnet of the candidate emp (v) is reinitialized within the search space randomly. the electromagnet selection in the random search follows the order of the electromagnets, which is controlled by a counter in the algorithm, and the subsequent electromagnet is updated in each random search step. if the end of the emp is reached, then the random search starts from the beginning in the next time."
"2.2. double-diode model. the dd model consists of two diodes in parallel with the current source and a shunt resistance to consider the effect of recombination current loss in the depletion region [cit] . the dd model provides more precise solution with regard to the consideration of this loss, especially at low voltage [cit] . in figure 2, the equivalent circuit of the dd model is represented. similar to the sd model, the output current is described as follows:"
"the objective function (7) aims to minimize rmse (root mean square error) of the experiments, which is determined by using equation (10) . the constraints (8) describe the boundaries of the decision variables. finally, the decision variables are described in constraint (9) . efo classifies the emp population into three groups as positive, neutral, and negative polarities according to the predetermined ratios, and the attraction-repulsion mechanisms among those emps will guide the population to the global optimum. the main idea behind the search mechanism of the efo is that the negative emps will repel, whereas emps having positive polarities will attract the neutral emps. furthermore, efo employs the golden ratio to balance the attraction and repulsion forces to favor the exploitation behavior of the population [cit] ."
"renewable energy has experienced a tremendous increase in recent decades because of the depletion of conventional sources oil, coal, or natural gas. among various kinds of renewable energy sources such as wind, wave, nuclear, and biomass, solar or photovoltaic (pv) energy is the most important source due to its properties such as effectiveness, wide-scale availability, unlimited capacity, and safe-use [cit] . furthermore, pv, which is able to provide power for specific purposes, is an emission-free system with direct conversion from solar energy to electricity [cit] . since solar cell installation has received great attention, numerous researchers have focused to maximize the efficiency of pv systems. in order to control and optimize pv systems, it is required to accurately simulate the characteristics of the pv system before installation. the accuracy of the pv systems mainly depends on the parameters of solar cells, which are generally not provided by the cell manufacturers [cit] . therefore, it is vital to identify the parameters of solar cells or modules based on nonlinear mathematical models. among a variety of existing models in the literature, the main ones are the single-diode model (sd), the double-diode model (dd), and the pv module model [cit] . the problem of extracting the parameters of solar cells from the experimental data is called the solar cell parameter identification problem (scpip) in literature."
"in fig. 1 the interactions between the tools4msp geoplatform, external application and potential end-users are presented. as a whole, they represent an integrated system of software, users and web services capable to effectively support msp activities. overall, the system can be described by four components:"
"the geoplatform is a community-based integrated web application. data are managed in a sdi over the entire workflow, from the collaborative upload in a web portal, to the creation of metadata, the choice of appropriate visual encodings, the composition of maps, the set up of use cases and the elaboration through specific modules producing final maps and descriptive reports. internally, the tools4msp geoplatform is divided into two sub-components: the geospatial content management system (cms; 1.a) and the tools4msp package (1.b). a more detailed description of the geoplatform is given in the next section."
"at the current stage, the tools4msp modelling framework has been applied in various areas of interest in the adriatic sea, such as entire adriatic sea [cit], italian adriatic sea [cit] ) and regional scale analysis for the northern adriatic sea [cit] b) and emilia-romagna region [cit] b) . in the following section results for tools4msp geonode plugin and stand-alone library for the adriatic sea will be presented."
"to switch from stable position 1 to stable position 2, the voltage is applied on the thermal actuators (a2). the interferometer detects the motion of the bistable mechanism. the result is shown in fig. 9 ."
"where v is the candidate solution and j is the corresponding electromagnet (index). p, n, and k are the indexes of the selected emps from positive, negative, and neutral parts, respectively. φ is the golden ratio constant, which is used to guide the candidate solutions towards the positive part as can be seen from equation (12); the candidate emp is generated based on the randomly selected neutral particle, where the positive emp will attract and the negative emp will repel the candidate solution. the algorithm of the candidate solution generation step of the efo is given in algorithm 2."
"to get over these difficulties, we proposed a new kind of microrobots named digital microrobots [cit] . these new microrobots are based on bistable modules. each module offers two stable and blocked positions. desired positions can be obtained by switching the bistable modules. sensors are not needed because bistable modules offer two known positions. this approach associated with microfabrication technology allows building monolithic microrobots that can be controlled in open-loop. prototypes of bistable modules have already been fabricated. in this paper, we focus on the characterization and the control of a bistable module. static and dynamic performances including switching from one position to the other are tested. a control strategy is proposed to obtain perfomances compatible with the requirements of microrobotics. this paper is organized as follows: in section two, the bistable module is presented. sections three and four are dedicated to the static and dynamic performances. in section five a control strategy is proposed and the last section concludes this paper."
"the inputs of the tools4msp cea tool are: (i) the area of analysis; (ii) the grid cell resolution; (iii) layers representing intensity or presence/absence of human uses (e.g., intensity of fishery and maritime transport, presence of aquacultures and oil & gas platforms); (iv) layers representing intensity or presence/absence of environmental components (e.g., seabed habitats, probability of presence of nursery habitats, probability of presence of marine mammals); (v) use-specific relative pressure weights and distances of pressure propagation; (vi) environmental component sensitivities related to specific pressures or more general ecological models that describe the response of the environmental components to a specific pressure. similarly to the muc tool, the area of analysis is subdivided into regular grid cells. then, on each cell, information about the presence of human uses and environmental components are extracted. afterwards, the geospatial layers on human uses are propagated and combined to estimate the geospatial distribution of different pressures (e.g., marine litter, underwater noise, abrasion) for the entire analysis area. finally, the geospatial distribution of single and cumulative effects and impacts are estimated combining together the pressure layers and the environmental components layers through a sensitivity score. the msp-oriented integrated system"
"the considered module has been activated in the blocked position 1 (see fig. 8 ). two switching transitions are studied. for the transition to stop block 2, we power the actuators (a2) and for the transition to stop block 1, the actuators (a1) are powered."
"component 4: third party data repositories msp-related workflows need relevant and updated data to be analysed and processed, the interaction between this component and the tools4msp geoplatform highlights the ability of the system to integrate data from other data portals and sdis, such as shape adriatic atlas (http://atlas.shape-ipaproject.eu/), emodnet portals (http: //www.emodnet.eu/), eea services (https://www.eea.europa.eu/data-and-maps), coconet web gis (http://coconetgis.ismar.cnr.it/) or the european atlas of the seas (https: //ec.europa.eu/maritimeaffairs/atlas/maritime_atlas). all these portals use interoperable ogc-compliant web services to exchange spatial information; based on geonode features, the tools4msp geoplatform allows users to display external layers (e.g., served from remote web map services; [cit] ) and enriches its catalogue with relevant data through the harvesting of standard web services (geonode remote services). the creation and maintenance of this network of collaborations allow to harmonize existing multiple efforts and improve the availability of spatial datasets for users interested in msp-related information."
the handle layer is used to support the entire structure and the bistable structure is fabricated in the device layer (see fig. 2 and fig. 3) .
"background: poland is setting up its first regional ecmo program and relies heavily on the use of simulation in testing processes and training clinicians. 1 as ecmo is a complex and expensive procedure, we developed an advanced ecmo simulator for highfidelity medical simulation training. 2 -6 it can be used to modify any type of full-body patient simulator and allows for the creation of an unlimited number of scenarios. methods: the system is equipped with an electronic core control unit (ccu) (figure 1 ), a set of synthetic valves, pressure sensors, and hydraulic pumps. the major functions of the ccu are to stabilize the hydraulic system (flow of simulated blood, differential pressures in the arterial and venous lines), providing instant information about the system to the user via a display. electric valves and sensors provide 'on-thefly' information to the ccu about the actual system's status and it can be made to respond to specific instructions imitating the physiological circulatory system and simulating several scenarios (i.e. bleeding, low pressure, occlusion, reaction to proper and incorrect pharmacological treatment). it can be connected to an ecmo machine to act like the human body during ecmo run. silicone tubes (modified polyethylene) that can be realistically cannulated using ultrasound imaging represent the artificial vessels. the ccu is made of electronic components that can be integrated to customize any mannequin as shown in figure 1 . the hardware includes both digital and analogue components that are controlled by a software run on a computer connected to the ccu via a serial port (rs232) (figure 2 ). the software allows for the visualization of measurements obtained from the sensors and the control of the pumps and valves via electronic controllers. the controllers affect the ecmo circuit simulated blood flow, and hence the readings from the ecmo machine sensors, to recreate various clinical scenarios. results: every component used can be easily replaced. the total cost of the simulator modification, excluding the cost of the computer or future mobile device, is approximately 200 usd, and the consumable parts cost about 20 usd. it has been used to help simulate successfully a range of scenarios."
"to describe the i − v characteristics of the solar cells, there exist several models in the literature. in this study, the sd and dd models, which are the most commonly used models since their practical usage for the solar cells, are taken into account [cit] . in this section, the sd and dd models are introduced, and the scpip based on these models is defined."
"after introducing the designed bistable module as the basic module for building digital microrobots, we presented the static and dynamic characterization of the module. the dynamic behavior of the mechanism for one transition revealed overshoot and vibrations. an open loop control strategy has been developed in order to improve the dynamic behavior. this strategy has significantly improved the dynamic behavior and the response is well damped with no overshoot. the general performances of the module are compatible with the requirements of micropositioning. the design and open loop control of a bistable module is a milestone for the development of digital microrobotics."
"this paper presents architecture, implementation and practical application of an msporiented software package named tools4msp. the tool is presented as geonode plugin and as a stand-alone library determining different levels of usability by different user groups. as a plugin, tools4msp supports a wide user community that facilitates the implementation of collaborative analyses improving the reusability and sharing of the result outputs. the integration within a geospatial cms allows to manage the entire processing data workflow, from the collaborative upload in a web portal, to the creation of metadata, the choice of appropriate visual encodings, the composition of maps, the set up of use cases and the elaboration through specific modules producing final maps and descriptive reports. the usage of the plugin is particularly suitable as it provides a user-friendly interface appropriate to decision-makers, regional authorities, academics and msp stakeholders (e.g., fishers, engos, industry)."
"it is well known that the compromise between exploration and exploitation throughout a run is critical to the success of a metaheuristic algorithm. exploration means the ability of an algorithm to search for unvisited points in the search region, whereas exploitation is the process of refining those points within the neighborhood of previously visited locations to improve the solution quality. in a word, it can be concluded from the above observations that the efo algorithm is good at exploitation but may have poor exploration behavior. therefore, an adaptive version of the efo is presented in this study."
"the phase from the first stable position to the unstable position is controlled by actuator (a1) (see middle curve of fig. 14) . when the unstable position is reached (the time is at 1.69s), the first phase is finished. fig. 13 . schematic of control system of a bistable module. the shuttle goes through the unstable position, and enters the second phase (from the unstable position to the other stable position). the actuators (a2) are used to control this phase (see bottom curve of fig. 14) . the bistable mechanism is maintained by the actuator (a2) and moved to the second stable position (see top curve of fig. 14) . fig. 15 shows the comparison between the switching with and without control. we can observe that the use of this control can reduce significantly the overshoot when the contact is established between the shuttle and the actuators. it does not present the overshoot in the final position. the whole switching time is about 205ms."
where the grid current references i * d (k) and i * q (k) are determined from the active and reactive powers in accordance with equation (11) .
"internal validity focuses on the analysis of the extracted data [cit] . since the data analysis in this systematic mapping study only uses descriptive statistics, the threats to internal validity are minimal."
"architecture recovery (ar): uncovering architecture design and related design decisions based on existing implementation and documentation of the system [cit] . this activity transforms existing architectures from implicit to explicit. architecture description (adp): documenting an architecture using a collection of elements, such as architecture views and models. this activity codifies architecture in a consistent way. the main goals of adp are facilitating the expression and evolution of software systems, providing a blueprint for system development, and supporting the communication among stakeholders [cit] . architecture understanding (au): comprehending the elements of an architecture design and their relationships, as well as the corresponding design decisions (why the architecture is designed the way it is). this activity helps architects and concerned stakeholders to acquire thorough knowledge about an architecture. the difference between ar and au, is that the information made explicit during ar is used as input to au and gets transformed into knowledge. architecture impact analysis (aia): identifying the architectural elements affected by a change scenario, including directlyaffected and indirectly-influenced parts of an architecture [cit] . the outcome of this activity helps architects understand the dependencies between the changed parts and the affected parts of an architecture. architecture reuse (aru): using existing architectural assets, such as architectural design elements, decisions, patterns, and other assets, in the solutions addressing various architecting problems [cit] . by performing this activity, architectures of better quality can be achieved at lower cost. note that, as described in section 2.2.1.2, we conducted a trial study search before the formal study search of this mapping study. these general architecting activities were identified according to the results of the trial study search. we started with this initial list, with the option of adding other general architecting activities identified during the search process. eventually, we did not identify other general architecting activities during the formal study search."
"a knowledge-based approach in this study refers to any approach from km that can directly contribute to the architecting process. we identified five such approaches, i.e., knowledge capture and representation, reuse, sharing, recovery, and reasoning, from two previous survey papers on ak [cit] . these approaches are described below."
"we read the abstracts of the papers that were left after the first round selection and checked them against the inclusion criteria i1 and i2, and the exclusion criteria e1 and e2. selection results were verified by two reviewers and any disagreements among the reviewers were discussed and resolved. if a resolution of the disagreement is impossible to achieve at this stage, the paper was included. if it is difficult to judge whether a paper should be included or not, the paper was included for the next round selection."
"this paper presents a development power control strategy based on model predictive control for grid-tie 3l-t-type inverter while ensuring dc-bus capacitor voltage balancing and reduction of switching frequency. moreover, the inverter voltage reference is applied to compute the equivalent cost function for determining the best control input, leading to reduce the computational effort compared with the conventional fcs-mpc method. a comparison study indicated that the proposed method achieves high-performance control of power and low thd of the current compared with the classical dpc-svm. simulations and experiments verified and highlighted the effectiveness of the proposed method."
"to explain the use of the terms and boolean operators in actual searches, we take the advanced search in ieeexplore database as an example. we fill in the first field (i.e., the first textbox) in the ieeexplore search interface with the string ''architecture or architectural or architecting'' and the second field with the string ''knowledge or semantic or rationale or reasoning or decision or information''. we divide the terms in outcomes into small groups with two terms each and fill in the third field with one group at a time. the terms in a group are joined by the boolean operator or. the three fields are joined by and. the reason we partition the outcome terms into small groups is that there are many terms in outcomes, and the returned results will be too many to judge in a reasonable length of time if we fill in the third field with the long string covering all the terms in outcomes. in other words, a search in ieeexplore database is composed of a number of small searches. note that, generally, a search field in the search interface of a specific database has a limitation on the string length or the number of search terms. thus, a search is often divided into a number of small searches."
"the study search and selection results, and the study results distribution are presented in this section. fig. 2 presents the study search and selection results in the three rounds of selection. in the search phase, we retrieved 16,144 papers, including 3036 papers from the automatic searches in electronic databases and 13,108 papers from the manual searches in target journals, conferences, and workshops (i.e., [cit] ). 409 studies were left after the first round study selection. 296 studies were excluded in the second round study selection and 113 studies were left for the final round study selection. finally, we got 55 studies selected."
"conclusion validity (a.k.a. reliability) is concerned with whether the mapping study can be repeated by other researchers and get the same results [cit] . to make the results of this mapping study reproducible, the search terms used in automatic search and the search sources for both automatic and manual searches are presented. moreover, inclusion and exclusion criteria for study selection are also defined. however, different researchers tend to have different understandings on these criteria, thus the results of study selection performed by various researchers are likely to be varied."
"it is worth mentioning that there are different terms of the cost function between the traditional mpc and the proposed method as shown in equations (14) and (19) . with the traditional fcs-mpc [cit], all predicted control variables are considered in the optimization loop. in this case, 27 predictive currents i p d (k + 2) and i p q (k + 2) are needed to enumerate in the cost function (g cnv (u k+1 )) to obtain the best switching state, which is applied to the inverter at time k + 1. on the other hand, only one inverter reference voltage u * inv (k + 1), which is achieved through the estimated grid currentî g (k + 1) and grid current reference i * g (k + 2), is taken into account for the loop optimization. then, by only one evaluating cost function (g md f (u k+1 )) with simple code optimization, the optimal control input is achieved. therefore, the proposed technique reduces the computational time of the performance criterion optimization compare with the traditional fcs-mpc. the comparison of computational cost between two controllers is summarized in table 2, which highlights the benefit of the proposed method. to validate the efficiency of the proposed algorithm, the function tic-toc in matlab is used to measure the computation time. significantly, the minimum, average and maximum values of evaluation time of the conventional fcs-mpc are 17, 29, and 41 µs, respectively, and 10, 22, and 34 µs for the proposed algorithm, respectively. figure 4 shows the impact of the computation time in two controllers. this can lead to a 24% reduction of computational complexity, resulting in an increase of sampling frequency for improving the control performance. consequently, this method has many practical applications for power converters in term of a time-consuming optimization algorithm, thereby supporting the feasibility of real-time applications with low-cost processors. the overall strategy of the proposed control algorithm is illustrated in figure 5 . figure 5 . block diagram of the proposed control scheme. finally, the control objectives are obtained by enumerated the proposed cost function, as demonstrated in algorithm 1. algorithm 1 algorithm of modified model predictive power control with reduced computational complexity."
"the inaccuracy of the data extraction results may negatively affect the classification results of the selected studies. the quality assessment on the selected studies helps to increase the accuracy of the data extraction results. the quality assessment results of the selected studies are shown in table 13 according to the assessment questions listed in table 7 . the mean score of all the selected studies is 3.47. with that score, we can conclude that the overall quality of the selected studies is acceptable. the mean score of q1 is 0.44, which results from 5 studies with score 0.0 (no evidence) and 22 studies with score 0.2 (evidence obtained from demonstration or working out toy examples). in other words, almost half of the selected studies have very low level of evidence. all the studies got full scores on question q2. we paid special attention to questions q3 and q4 since their scores can reflect the accuracy of the data extraction results. the higher score of the study, the more accurate the data extraction results will be. thus, the score provides a clue about whether the data extraction results of a study need to be checked more carefully. each reviewer checked extracted data of all selected studies with special attention to the ones with low scores in q3 and q4, and reached a consensus when disagreement exists about the extracted data. interestingly, the average score of q5 is very low since 69.1% (i.e., 38 out of 55) of selected studies do not mention the limitations or drawbacks of their work."
"grid-connected power converters have proven to be important in many industrial applications such as active power filters, distributed systems, and renewable power generation systems [cit] . the conventional control strategy is the voltage-oriented control [cit] which can ensure the steady-state and performance via the traditional pi current controllers with pulse width modulation (pwm) or space vector modulation (svm). however, the completeness of the current decoupling of the internal current controller and accurate tuning parameters are the drawbacks of this method. this technique has a low dynamic response and is not suitable for a nonlinear system. recently, to improve the performance, direct power control (dpc) [cit] has been presented by employing a look-up table to decide the proper switching state of the inverter. nonetheless, a big ripple of the active and reactive powers is the problem of this approach. moreover, it is necessary to require a small sampling time to obtain a reasonable steady-state and good transient performances. to overcome these issues, various approaches have been developed such as using dpc established on extended-state observation with pwm [cit], port-controlled hamiltonian system [cit], sliding mode control [cit], and predictive control [cit] ."
"2.2.1.1. search scope. this subsection describes the search scope of this mapping study, including time period and search sources. we include a number of electronic databases (listed in table 3 ), journals (listed in table 4 ), conferences (listed in table 5 ), and workshops (listed in table 6 ) that are relevant to the study topic."
"over the last few years, multilevel converters have been recognized as an alternative approach for high-power electronics owing to the improvement of capacity and performance compared with the two-level converter [cit] . in particular, the t-type inverter topology is preferred due to the benefits in term of low conduction losses and high sinusoidal waveforms of the output voltage [cit] . the unbalance of capacitor voltages is the big problem for this configuration. however, this issue has been addressed by several methods [cit] ."
"to address this issue, we conducted a systematic mapping study (or shortly mapping study) on the application of knowledge-based approaches in sa. a systematic mapping study is a form of secondary study aimed at getting a comprehensive overview on a certain research topic, identifying research gaps, and collecting evidence in order to direct future research [cit] . it allows the studies in a domain to be plotted at a high level of granularity thereby answering broader research questions regarding the current state of the research on a topic [cit] . another form of secondary study is a systematic literature review (slr), which targets to identify, evaluate, and interpret all available studies to answer particular research questions, which require more in-depth analysis [cit] . we selected to perform a mapping study instead of a slr since the involved domains, i.e., software architecture and knowledge-based approaches, are quite broad areas. our focus is not on some particular aspects of the involved domains, but the combination of two domains."
"sw c in equation (12) is the additional constraint imposed on the cost function to reduce the switching frequency of the inverter. thus, its expression can be given by:"
"where hd (q, t) is the distance between query image q and images in the database t. h q and h t are the color histograms of query and the database images respectively and m is the number of bins of histogram. in this particular experiment, the comparison of query image with images in the database is done on 251 bins (bin no. 50 to bin no.300) of histogram."
rationale: various architecting activities may have specific needs on different knowledge-based approaches. we want to know in which architecting activities the knowledgebased approaches have been applied and how often the knowledge-based approaches have been applied in each architecting activity. this information can help us to identify the gaps of the application of knowledge-based approaches over various architecting activities and to identify which architecting activities require more exploration on the table 1 primary support from general to specific architecting activities.
"this mapping study covered five main tasks across four phases as illustrated in fig. 1 . the task of study selection includes three sub-tasks, i.e., selection by title (the first round selection), selection by abstract (the second round selection), and selection by full text (the final round selection). the next round study selection refined the results of the previous round study selection. in phase 1, the study search was performed and, meanwhile, the study selection by title was conducted. in phase 2, we did the study selection by abstract based on the results of the study selection by title. in phase 3, we read the full text of all the papers filtered in the study selection by abstract. to make the review more efficient, we simultaneously conducted three (sub-) tasks: the study selection by full text, study quality assessment, and study data extraction. this means, when reading a paper, we made the final decision on whether a paper should be selected or not. if this paper was finally selected, we assessed its quality and extracted the data from it. in this way, we only needed to read the paper once for performing the three (sub-) tasks. we synthesized the extracted data in phase 4 to answer the research questions. the following subsections elaborate on the main tasks of this mapping study."
"please note that we have combined architectural maintenance and evolution in the same activity. we adopt the following definitions: architectural maintenance is about correcting faults and adapting to a changed or changing operational environment [cit], while architectural evolution focuses on implementing new requirements [cit] ."
"we employed two search methods: the automatic search and the manual search. with the automatic search method, we searched studies in an electronic database (e.g., ieeexplore) with search terms by the search engine provided by this database. the search engine checks the search terms against the metadata, including the title, abstract, and keywords, of each paper in the database. with the manual search method, we browsed all the papers within the time period specified in section 2.2.1.1 in target venues (i.e., the journals, conferences, and workshops listed in tables 4-6, respectively) without using search terms but by using the names of the target venues. the use of an automatic search ensures a high coverage (i.e., retrieving studies in as many venues as possible) of potentially-relevant studies by searching electronic databases using search terms. on the other hand, it may provide many irrelevant studies. manual search aims at increasing the completeness (i.e., deriving as many studies as possible in target venues) of potentially-relevant studies in target journals, conferences, and workshops, which are of high relevance to the study topic. although most of the target venues are indexed by the included databases, the manual search is complementary instead of repetitive to the automatic search: the relevant studies published in the target venues may be filtered out in the automatic search, but are likely to be found in the manual search. moreover, some target venues (e.g., seke and ijseke) are not indexed in the included databases, and the manual search therefore may retrieve some relevant studies that the automatic search cannot find. the results of manual search are independent of the results of the automatic search, thus these two search methods can be performed in an arbitrary sequence in this mapping study. in the rest of this section, we first scope the time period and publication sources of the study search, and then present the search strategy used in this mapping study."
"there are only three selected studies using knowledge recovery to support architecting activities. to be specific, s1 uses semantic annotation, which is a method of knowledge recovery, to facilitate refactoring architecture artifacts for maintaining the system quality during ame; s15 proposes architectural design decision recovery approach (addra) to recover architectural design decisions for the purpose of improving architecture documentation (note that addra is also a method for architecture recovery); s39 presents an approach to recover implicit assumptions in architecture design to support ame. fig. 5 shows the distribution of the selected studies using kcr over architecting activities. kcr is widely employed in all architecting activities within the scope of this mapping study. we are not surprised by this result because the output of kcr provides the basis for using other knowledge-based approaches in architecting activities. for example, one of the prerequisites of using knowledge sharing, reasoning, and reuse is the existence of captured knowledge that is represented in certain forms. however, there are significant differences in the popularity of the kcr application in various architecting activities. specifically, kcr is more popular in ae, as, adp, and au, but is seldom employed in aia and aa. this is partly because kcr is frequently used to capture and represent architectural knowledge for various purposes in ae, as, adp, and au. another reason is that adp and au require the architecture to be represented in a certain form for better description and further easier understanding. finally, aia and aa have received little attention in using knowledge-based approaches, thus kcr is seldom employed in these two architecting activities."
"the disadvantage of global color histogram (gch) and histogram of corners (hoc) is that large number of bins of histogram (251) is used for comparison. in order to reduce the complexity in searching large image database, the original image is compressed by using a log operator and then histogram of the compressed image is computed. the resulting technique is very efficient in that it uses histogram of only 10 bins of each component for comparison."
"knowledge management (km) technologies have been employed across the spectrum of se activities for more than two decades [cit], for example in requirements elicitation [cit], architecture evaluation [cit], software testing [cit], and software documentation [cit] . in recent years, the sa community has paid increasing attention to the application of km in the architecting process [cit], establishing the field of architectural knowledge (ak). this has resulted in acknowledging that the most important types of ak, architectural design decisions (e.g., choosing a particular architectural pattern for a design issue) and design rationale should be treated as first-class entities in sa [cit] ."
"to reduce the bias on study selection results, a pilot selection was performed to ensure that the reviewers reached a consensus on understanding the criteria. also, the study protocol was discussed among researchers to ensure a common understanding on study selection. furthermore, in the second and final round study selections, two reviewers conducted the selection process in parallel and independently, and then harmonized their selection results to mitigate the personal bias in study selection caused by individual reviewers."
the five architectural activities mentioned above cover the entire architecture lifecycle. we call them specific architecting activities to distinguish them from general activities described later. general architecting activities can be performed across the architecting process to support architects in achieving the goals of specific architecting activities. we identified the following general architecting activities:
"whereas hd (q, t) (eq. no.1) is the distance between query image q and images in the database t. hq and ht are the histograms of log-r component of query and the database images respectively and m is the number of bins of histogram (10 bins for each component). similarly compute histogram distance (hd) for log-g component and log-b component. figure 5 shows the histogram of the log-r component of the image shown in figure 1 ."
"this subsection presents the application domains where knowledge-based approaches have been applied to, with a special focus on the studies with industrial evidence, i.e., the evidence obtained from industrial practice or industrial cases. as shown in table 11, 18 studies did not specify explicitly the application domains and the rest of the studies are classified into 13 application domains. note that, if a study does not provide related information about its application domain, we classify this study into the domain ''not specified''. the application domain ''embedded software'' has received the most attention in applying knowledge-based approaches in sa. for instance, s27 proposed a knowledge-based quality-driven architecture design and evaluation approach, which was validated in the architecture design and evaluation of a secure middleware for embedded peer-to-peer systems."
"to make the study selection results objective, we defined selection criteria that were employed in the study selection process. in addition, in the second and final round study selections, two reviewers conducted the selection in parallel and harmonized their selection results to mitigate the personal bias in selection results caused by individual reviewers."
"where n sw 1x, n sw 2x, n sw 3x and n sw 4x denote the number of commutations of each leg in the time interval (t sim )."
"the overall goal of this mapping study is to get an overview of existing research on the application of knowledge-based approaches introduced in section 2.1.2 in architecting activities. to obtain a detailed and comprehensive view on this topic, this high-level goal is decomposed into four research questions (rqs):"
"architectural implementation (ai) also receives little attention on using knowledge-based approaches. there are only three studies in this area. two (s24 and s41) of them focus on object-oriented design and the third one (s48) concentrates on service-oriented architecture. to improve object-oriented design (i.e., architecture implementation in object-oriented design) using the accumulated knowledge systematically and effectively, s24 proposed a method based on a defined rules catalog (that unifies various knowledge such as heuristics, principles, and bad smells) and the relationships between rules and patterns. s41 represents the successful design experience in cases (a method of kcr) and employs case-based reasoning approach to implement architectures in object-oriented design by reusing the cases. s48 introduces an ak base on wireless services, as a means of sharing and reusing existing ak, to improve the quality of service-oriented architecture (soa) implementation and speed up the soa development with decreased cost. in the area of ai, the design and development communities have gained much experience on how to implement various architectures. knowledge-based approaches can help designers and developers to capture, represent, share, and reuse their experiences on ai. for example, with knowledge-based approaches, designers are able to extract architecture patterns and pattern using context in the form of knowledge from past designs to facilitate the understanding and implementation of composing solutions (e.g., combination of patterns) that achieving specific quality attributes."
"analyzing the map (fig. 3 ) from the perspective of architecting activity dimension, we get the distribution of selected studies over architecting activities. as shown in fig. 6, all the architecting activities (including general and specific architecting activities) employ knowledge-based approaches. however, significant differences in the study distribution exist over the ten architecting activities. knowledge-based approaches are mostly used in ae with eighteen selected studies. in contrast, only two studies address the problems in aia."
"rationale: the use of knowledge-based approaches in architecting activities may take place in various application domains, such as embedded systems and financial software. by answering this question, we want to know what application domains the knowledge-based approaches have been applied to and how often the knowledge-based approaches have been applied to each domain. this information can help us to identify which application domains have gained more interest in applying knowledge-based approaches."
"by analyzing the data extracted from the 55 selected studies, we get an overview of the study results distribution. as shown in fig. 3, a map is used to present the results distribution on the study topic ''application of knowledge-based approaches in software architecture (akasa)'' over the range of architecting activities, knowledge-based approaches, and time period. in the left part of fig. 3, a bubble represents one study or several studies on an architecting activity published in a certain year. in the right part of fig. 3, a bubble denotes one study or several studies applying a certain knowledge-based approach in an architecting activity. the numbers in a bubble are the identification numbers of the corresponding studies in the 55 selected studies as listed in appendix a. for instance, the left-top bubble in the right part of fig. 3 denotes that the 6th, 40th, and 48th studies are about the application of knowledge reuse in the architecture reuse activity. in the rest of the paper, study sn denotes the nth study in the 55 selected studies. the detailed analysis of this map (fig. 3) is presented in the rest of this section."
"2.2.1.2. search strategy. search strategy is important for a mapping study, since it affects the quality and completeness of retrieved studies and the effort we need to spend. the search strategy and steps in this mapping study are described below:"
"in a real-time control system, it is necessary to consider a time delay produced by computation time. to overcome this problem, in the traditional approach [cit], the predicted variables at instant k + 1 are estimated by using the dynamic model, measurement feedback and previous switching status at instant k. next, the associated predicted variables at time k + 2 are obtained from the estimated variables at time k + 1 and all switching states of the inverter. thus, the optimal control input is achieved from cost function optimization at time k + 2 and implemented to the inverter at k + 1, as shown in figure 3 . as mentioned in section 2.2, a control action"
"software architecture (sa) [cit] s as a distinct discipline within software engineering (se) and entered its golden age a decade later [cit] . sa is defined in the iso/iec ieee 42010 standard as the ''fundamental concepts or properties of a system in its environment embodied in its elements, relationships, and in the principles of its design and evolution'' [cit] . we adopt this definition for the current study; we further discuss more details about the process of architecting in section 2.1.1."
"knowledge capture and representation (kcr) extracts knowledge from diverse sources as well as its acquisition directly from the stakeholders, and expresses knowledge in certain forms so that the knowledge can be used for automatic or human reasoning. on one hand, knowledge representation accompanies knowledge capture since the knowledge should be represented in certain form when captured; on the other hand, knowledge capture may happen during knowledge representation. for instance, when one uses a conceptual model to transform (e.g., annotate) implicit knowledge to explicit knowledge, knowledge capture takes place at the same time. we thus consider knowledge capture and knowledge representation as a combined knowledge approach. knowledge reuse (kr) applies existing knowledge (e.g., architectural patterns) in a particular context for various purposes [cit] . knowledge sharing (ks) exchanges knowledge (e.g., skills or expertise) among individuals in a community or an organization. knowledge recovery (krv) recovers explicit knowledge from tacit knowledge, e.g., decision rationale that is not documented. in km theory, knowledge is classified into two types: tacit knowledge which resides in people's head, and explicit knowledge which is codified in a certain form [cit] . knowledge reasoning (krs) draws conclusions and derives new knowledge from existing knowledge through inference. an example is reasoning based on the rationale knowledge of the existing architectural design decisions to make new decisions addressing new or modified design issues."
"a set of journals (table 4), conferences (table 5), and workshops ( table 6 ) that are of high relevance with the study topic were included. two criteria for selecting the publication venues are that (1) they should be highly relevant to sa or both sa and km and (2) they are leading journals, conferences, and workshops in the study area. conferences such as c3, c4, and c5 and workshop w1 were reasonably chosen as the sources in this mapping study since they are top conferences and unique workshop on sa and knowledge-based sa. as sa is a significant sub-area of software engineering (se), papers on sa are usually published in journals and conferences in se. therefore, the publication venues in se or both se and km should be included. such publication venues include journals j1, j2, j3, j4, j5, j6, j7, j8, j10, j11, and conferences c1 and c2. workshop w2 was included because it is a workshop about requirements knowledge, which has a close relationship to architectural analysis, especially in terms of architecturally significant requirements. conferences c6 and c7 were selected because studies on architectural maintenance and evolution can be published in these two conferences. in addition, journal j9 focuses on software quality where research on knowledge-based approaches to improve architecture quality can be published."
where the inverter voltages u inv_d (k) and u inv_q (k) are achieved from equation (9) by using the previous state s opt (k) . substituting the grid current references i * d (k + 2) and i * q (k + 2) by their predicted currents i
estimate the grid current: the grid current componentsî d (k + 1) andî q (k + 1) at time k + 1 are evaluated by utilizing equation (15) with the previous optimal switching state (u k ) at time k.
"to verify the control performance of the proposed control scheme for the grid tie 3l-t-type inverter, examinations were conducted in matlab/simulink simulation environment with simpowersystems toolbox. the system parameters are as indicated in table 3 . the control algorithm was implemented by utilizing the matlab function block. to evaluate the steady-state control performance, the mean absolute percentage error (mape) was used to estimate the power ripple and neutral-point voltage deviation as:"
"before the study selection, we conducted a pilot selection to ensure that all the reviewers had a consensus on the understanding of these selection criteria. the inclusion criterion i1 is applied in the first and second rounds of study selection but not in the final round since the abstract of a paper has provided enough information to judge whether the paper is really about software architecture or not. the exclusion criterion e2 is not applicable in the first round of study selection as we cannot know the publication type of a paper, i.e., published as a full paper, abstract, tutorial, or talk, by reading its title. the inclusion criterion i2 and exclusion criterion e1 are applicable and used in all the three rounds of study selection. fig. 1, study selection is divided into three sub-tasks and each sub-task was performed in different phases. as mentioned in the second paragraph of section 2.2, we performed the first round study selection along with study search. we checked the title of each paper against the inclusion criteria i1 and i2, and the exclusion criterion e1. if there was any doubt whether a paper was relevant or not, it was included for the next round selection."
"(1) knowledge capture and representation (kcr), knowledge reuse (kr), knowledge sharing (ks), knowledge reasoning (krs), and knowledge recovery (krv) are all used in the architecting activities, but there are significant differences on the popularity of these knowledge-based approaches. kcr is the most popular approach that is widely used in all the architecting activities. krv is almost ignored as it is seldom employed in the architecting activities. more investigation needs to be performed to improve architecting activities using krv. (2) knowledge-based approaches are used in all ten architecting activities. however, the study distribution is highly uneven over the ten activities. comparing with other architecting activities, aia receives the least attention on using knowledge-based approaches. this research area needs to be further explored. also, ai using knowledge-based approaches is a promising research area and requires more research effort. (3) 30 publication sources from journals, conferences, and workshops are identified, which means the study topic, application of knowledge-based approaches in sa, has received wide attention from the sa community. in addition, there are one leading workshop (shark), conference (wicsa), and journal (jss) respectively as the publication venues for this study topic. the trend on the number of published studies in each year indicates that researchers have made increasingly effort in this topic during the last decade. (4) knowledge-based approaches are applied to the software architecture of a wide range of application domains, and the domain ''embedded software'' has received the most attention. industrial application and cases were conducted in eight application domains, but each domain has only few (no more than three) studies."
"to investigate industrial relevance, we further looked into which of the studies were industrial. we identified that 15 selected studies reach the evidence level 5 (i.e., evidence obtained from industrial studies) or above (i.e., the studies with score 0.8 and 1.0 in q1 of table 13 ). the application domain distribution of these studies is presented in table 12 . studies with industrial evidence were conducted in eight application domains. the domain ''financial software'' has received the most attention in applying knowledge-based approaches in industry. however, each domain has only few studies (i.e., no more than three)."
the harris detector is widely used in object detection and image retrieval system. the number of the corners detected in an image depends on the size and structure of the image [cit] .
"we got 16,144 papers returned from the literature searches including the papers from both manual and automatic search, of which 55 were finally selected for data extraction. based on the extracted data, we get an overview on what knowledge-based approaches are currently employed in which architecting activities and on the popularity of various knowledge-based approaches used in general and specific architecting activities. we draw conclusions around four main points:"
"in the field of ak, a number of km approaches and tools have been developed for efficiently and effectively improving the practice of software architecting [cit] . software architecting is essentially a knowledge-intensive process that is composed of several activities, e.g., architectural analysis, synthesis, and evaluation [cit] . architecting activities are mainly conducted by an architect in collaboration with a set of stakeholders involved towards the construction of the architecture of a software-intensive system. knowledge-based approaches, such as knowledge sharing and reuse, can be used to facilitate architecting activities. for instance, knowledge about the use of specific architectural patterns in a system development can be shared and reused during architectural synthesis of a similar system. in order to make systematic use of knowledge-based approaches to facilitate architecting activities and develop appropriate methods for using knowledge-based approaches in sa, it is necessary to have a clear understanding on the state of the art of the application of knowledge-based approaches in software architecture. to the best of our knowledge, there is currently no study on what knowledge-based approaches have been employed in what architecting activities, and on the gaps in the existing application of these approaches to various architecting activities. note that the ''application'' of knowledgebased approaches in this study does not limit to the practical application of knowledge-based approaches in architecting, but the use of knowledge-based approaches in architecting in both industry and research, for the purpose of a wide coverage of this study. in addition, the comprehensive understanding by this study can also identify the needs for applying specific knowledge-based approaches in architecting and which architecting activities require more exploration on the application of knowledge-based approaches, thereby proposing promising research directions."
"for evaluating the performance of the algorithms, we used coil-100 image database [cit] . our image database contains 792 images with 11 different classes (a to k). some of the sample images which are used as query images are shown in figure 6 . table 1, table 2 and table 3 gives % precision/recall for all 11 classes using global color histogram (gch), histogram of corners (hoc) and log-histogram (lh). from each category randomly five images are chosen as query image and for every query image precision/recall values are computed. table 1 : average precision/recall for 11 classes (a to k) and 5 queries each using gch table 2 : average precision/recall for 11 classes (a to k) and 5 queries each using hoc"
where u * inv_d (k + 1) and u * inv_q (k + 1) represent the desired inverter voltages at instant k + 1; and u inv_dn and u inv_qn stand for all possible inverter voltage components in dq synchronous reference frame.
"rationale: the topic of this mapping study is broad and there should be a number of venues to publish the related studies. in addition, the research on the study topic has been carried out for decades. we want to get an understanding on whether there are specific publication venues for these studies and when the effort regarding the study topic was made. this information can help us to identify the leading publication venues where the authors can better disseminate their research results and the trend of the number of published studies in this topic."
"the increasing number of selected studies over the last decade shows that the application of knowledge-based approaches is receiving increasing attention from the sa research community. the selected studies are published in 30 different venues, indicating that extensive attention on this study topic is being paid from researchers with a broad range of different research interests in sa. these two facts indicate that this study topic is likely to remain attractive. however we would urge the research community to strive for high-level evidence in future studies."
texture analysis is one of the most difficult problems in the area of computer vision. features based on textures can be useful in distinguishing between objects. the two main types used in computer vision to describe the texture of a region are structural and statistical. structural methods include morphological operator and adjacency graph. statistical methods include wavelet transform [cit] .
"with the implications discussed in section 5, we encourage the researchers and practitioners in sa community to carry out more empirical studies with high-level evidence on applying various knowledge-based approaches in the architecting process, in order to build a solid foundation for improving architecting activities with the support of knowledge-based approaches."
"final round study selection. we read the full text of the papers left by the second round selection and employed the inclusion criterion i2 and the exclusion criteria e1 and e2 to decide whether the papers will be finally included. in this round of study selection, we also checked whether multiple selected papers published the same study. if two papers publish the same study in different venues (e.g., in a conference and a journal, respectively), the mature one gets selected, and the less mature one (i.e., the one published in a conference) is excluded. again, we discussed any disagreements on selection results and achieved consensus on the final selection results."
"shape next to color and texture is considered as important visual feature in describing the objects in images. the two main types used in computer vision to describe the shape of a region are boundary-based descriptor and region-based descriptor. boundary based shape descriptor describes the external characteristics of the region. boundary based shape descriptors include area, perimeter, eccentricity and orientation. region based shape descriptor describes the internal characteristics of the region. region based shape descriptor include statistical moments [cit] ."
"step 2: trial searches using various combinations of search terms were performed. the search terms were improved according to the results of the trial searches. the trial results inspired us to come up with some missing terms and synonyms of existing terms. both types of terms were added when performing formal searches. we got the terms included in the three parts listed below: population: architecture, architectural, architecting. intervention: knowledge, semantic, rationale, reasoning, decision, information. outcomes: architecture design, architectural design, architecture analysis, architectural analysis, architecture synthesis, architectural synthesis, architecture evaluation, architectural evaluation, architecture maintenance, architectural maintenance, architecture implementation, architectural implementation, architecture recovery, architectural recovery, architecture documentation, architectural documentation, architecting process, architectural preservation, architecture preservation, architectural variability, architecture variability, architecture description, architectural description, architecture understanding, architectural understanding, impact analysis, architecture evolution, architectural evolution."
"(1) the results of this mapping study call for more investigation on improving aia with knowledge-based approaches. aia is an architecting activity that is frequently performed to determine which architecture elements are affected when a change scenario happens. the associated architectural knowledge on this change scenario, such as related design decisions and dependencies to these decisions, should be considered and employed to improve aia. (2) this mapping study also shows that the application of knowledge recovery approach in various forms needs to be explored seriously. in many architecting cases, architects need to recover the knowledge about architecture design, especially when maintaining or evolving the architecture that is not well described and documented. but little work has been done on the application of knowledge recovery in architecting activities. (3) as discussed in section 3.3, ai can benefit from knowledgebased approaches, e.g., knowledge sharing. however, little effort has been made in this area. ai is a knowledgeintensive activity, in which one architecture design can be implemented in a variety of detailed designs by different designers. knowledge-based approaches can be used to capture the knowledge in various designs, share them with other designers, and reuse them according to different contexts. this research area has a clear need for more investigation. (4) little work has been done to provide automatic and semiautomatic knowledge reasoning to support architecting activities. knowledge reasoning plays an important role in architecting activities, for example, it can be used to justify if a design decision is an appropriate one to address an asr [cit] . however, knowledge reasoning usually requires considerable upfront effort to formally represent the knowledge, which is a prerequisite for automatic and semi-automatic knowledge reasoning. when knowledge reasoning can be (semi)-automated in architecting activities, the efficiency of architecture design, maintenance, and evolution will be significantly improved. (5) more high level evidence-based research using knowledgebased approaches in architecting activities is needed. the overall evidence level of all the selected studies is relatively low, i.e., only 15 out of the total 55 selected studies reach the evidence level 5 (i.e., evidence obtained from industrial studies) or above. although the research community is increasingly aware of the benefits of using knowledge-based approaches in software architecture, low-evidence-level studies cannot provide architects sufficient confidence to have a try. therefore, there is an urgent need of high-evidence-level studies showing the real benefits that architect practitioners can get from knowledge-based approaches. (6) although knowledge-based approaches have been applied to the architectures of a wide range of application domains, there are still some application domains in which knowledge-based approaches are not or seldom employed so far (at least according to the results of this mapping study), e.g., telecommunications software, healthcare software, and cloud software. the software systems of such domains are often large-scale and complex. for instance, in the telecommunications domain, the software system of a typical wireless base station has millions of lines of source code and dozens of subsystems. knowledge-based approaches, such as knowledge sharing and reuse, can facilitate conveying architectural knowledge to stakeholders and the reuse of the existing architectural knowledge of a specific domain."
step 1: the preliminary search terms were identified according to the study topic and the research questions. the search terms used to retrieve relevant studies in the automatic [cit] .
where the grid current components in the dq coordinate frame are achieved by their values in the αβ stationary reference frame by using park transformation:
"as mentioned before, the general architecting activities can be used to facilitate specific architecting activities during the architecting process. based on our own understanding and experience on the architecting activities, we outline what the primary support that each general architecting activity can provide to a specific architecting activity in table 1 . ar can support evolving a legacy system or a system without a well-documented architecture during ame; adp can be used to record the architecture design and related decisions with the rationale behind them during as; au can help architects to get a fair understanding for analyzing, evaluating, implementing, maintaining, and further evolving a system (i.e., support for aa, ae, ai and ame); aia can assist architects to identify affected architectural elements when a new design decision is added or an existing design decision is changed during as and ame, and when analyzing if a new requirement is an asr (i.e., support for as, ame, and aa); and aru is performed to use the existing architectural assets to address similar problems during as. note that other types of secondary support may exist from general to specific activities but are not discussed here. our goal here is not to exhaust all possible types of support from general to specific architecting activities but to present and explain the potential support from general to specific architecting activities."
"e1. the paper, in which none of knowledge-based approach is applied or software architecting is not the focus, is excluded. e2. the paper that is published in the form of abstract, tutorial, or talk is excluded."
"construct validity is concerned with whether the concepts being studied are interpreted correctly and whether the relevant studies can be collected completely [cit] . in this mapping study, the architecting activities and knowledge-based approaches are the key concepts under consideration. to ensure the correct interpretation of these key concepts, we checked the definitions of the concepts with related literature and discussed these definitions among the authors to reach a consensus on the understanding of them. the correct interpretation of the concepts helps to extract the data precisely from the selected studies. in order to ensure the completeness of study search, we design a search strategy that employs a combination method of automatic search and manual search. furthermore, to make sure the high coverage of potentially relevant studies in automatic search, we improved the search terms according to the results of the trial search before the formal search is performed. however, the refined list of search terms might not be comprehensive, some relevant papers, therefore, might be missing. to address this issue, manual search is used as complementary search to automatic search. we checked all the papers published in relevant journals, conferences, and workshops so that the potentially relevant studies published in these venues are included."
"we used boolean operator or to join alternate words and synonyms in each part (i.e., population, intervention, outcomes), and used boolean operator and to join the terms from the three parts respectively. the full search string is like this:"
compute the histogram of query image as well as images in the database. figure 2 shows the histogram of the image shown in figure 1 . then we use the histogram distance (hd) to compute the similarity measure between query image and images from the database.
"architectural implementation (ai): refining architecture design into detailed design, and then implementing the design in code [cit] . this activity implements the software system according to the architecture design. architectural maintenance and evolution (ame): correcting faults, adapting to a changed or changing operational environment, and implementing new requirements in an architecture [cit] . this activity ensures the consistency and integrity of the architecture during the next product releases."
"as revealed in fig. 6, aia is not a well-explored research area in architecting activities in terms of the application of knowledgebased approaches. s4 proposes a meta-model of architectural design decisions to make the design decision knowledge explicit (a method of kcr), based on which an algorithm for aia is designed to automatically identify all the architectural elements affected by a changed architectural design decision; s17 employs kcr and kr to make explicit the architectural design decisions and their relationships, based on which the results of aia are more reliable. these two studies both consider architectural design decisions as first-class architecture elements and perform aia from the perspective of changing architectural design decisions. we argue that ks and kr should receive more attention in aia, as the knowledge about aia of existing architectures can facilitate the aia of similar architectures (e.g., the architecture of software product lines). consider a product line and concrete products a and b. performing aia fig. 4 . distribution of selected studies over knowledge-based approaches. fig. 5 . distribution of selected studies using kcr over architecting activities."
"the results of this systematic mapping study may be influenced by the coverage of study search, bias on study selection, and inaccuracy in study data extraction. therefore, four types of threats to the validity of the study results are discussed in the following subsections."
"to clarify the scope of this mapping study, several key concepts need to be explained before formulating the research questions. architecting activities that comprise the architecting process and knowledge-based approaches in knowledge management are the core concepts of this mapping study. we first describe the architecting activities and knowledge-based approaches, and then define the research questions of this mapping study."
"this mapping study indicates that the application of knowledge-based approaches in sa is a quite immature area in both research and practice. first, more than two thirds of the selected studies (37 studies out of 55) are published in conferences and workshops, and only 30.9% (17 out of 55) of the selected studies have reached the maturity of a journal publication. furthermore, only 27.3% (i.e., 15 out of 55) of the selected studies reach the evidence level 5 (i.e., evidence obtained from industrial studies) or above. in particular, only one study (s52) provides evidence obtained from industrial practice (i.e., the evidence level 6). finally, almost half (49.1%, 27 out of 55) of the selected studies fall into the evidence level 2 (i.e., evidence obtained from demonstration or working out toy examples) or below. especially, five selected studies do not provide any evidence."
"rationale: architecting is a knowledge-intensive process, and different knowledge-based approaches have been used in this process. we want to know which knowledge-based approaches are used and how often each knowledge-based approach is used in the architecting activities. this information can help us to identify the gaps of the application of various knowledge-based approaches and the needs for applying specific knowledge-based approaches."
"considering a sampling interval t s, the discrete-time of the grid current is obtained by employing the first-order forward euler approximation for equation (7):"
"histogram of images provides a global description of the appearance of an image. the information obtained from histograms is enormous. just by looking at the histogram of the image, a great deal of information can be obtained. the main advantage of the histogram is that it can be determined very quickly and it is invariant to rotation [cit] ."
"knowledge capture and representation (kcr) is most frequently used in architecting activities among the five knowledgebased approaches. kcr is used in 42 studies out of total 55, which means 76.4% of the total studies employ this knowledge-based approach in various architecting activities. knowledge recovery (krv) and knowledge sharing (ks) are seldom used. more specifically, krv is employed in only three studies out of total 55 (i.e., 5.5%), and ks is used in five studies out of total 55 (i.e., 9.1%). knowledge reuse (kr) and knowledge reasoning (krs) rank second and third respectively among the knowledge-based approaches in terms of the popularity. comparing with kcr, these two approaches are merely employed in a small number of studies, but these two approaches are used more than twice as much as ks and krv."
"content based image retrieval (cbir) retrieves the similar images from the large image database based on visual features such as color, texture and shape. the content based image retrieval (cbir) system includes two steps. the first step of cbir is feature extraction. set of features extracted from image is called as image signature. size of image signature is less as compared to original image. second step of cbir is similarity measurement. similarity measurement is used to compare the features of query image with features of images in the database so that top relevant images can be retrieved from the image database [cit] ."
"whereû g denotes the grid voltage magnitude that is achieved by using the phase-locked loop (pll). as a result, the dynamics model in continuous time is rewritten from equations (3), (5) and (6) as:"
2.2.2.1. selection criteria. the inclusion and exclusion criteria are used in the three rounds of study selections to decide whether a study should be included or not.
i1. the paper is about architectures of software-intensive systems rather than hardware or building architecture. (some papers on hardware or building architecture may be retrieved in the results of automatic searches.) i2. the paper employs at least one knowledge-based approach to address the problems in architecting activities.
"looking at the map (fig. 3 ) from the perspective of knowledgebased approach dimension, we get the distribution of selected studies over knowledge-based approaches in fig. 4 . this figure fig. 3 . studies distribution of akasa over the range of architecting activities, knowledge-based approaches, and time period."
"calculate the inverter voltage reference: according to equation (16), the reference components of inverter voltage u * inv_d (k + 1) and u * inv_q (k + 1) at time k + 1 are calculated in regard to the grid current references i * d (k + 2), i * q (k + 2) at time k + 2 and estimated grid currentsî d (k + 1) andî q (k + 1) at time k + 1. then, the neutral-point voltage at time k + 1 is computed from grid current i g (k) and all admissible switching states switching states u k+1 . (3) evaluate and select the best control input: the optimal control input that has the lowest of the cost function g md f is implemented at time k + 1 to the 3l-t-type inverter:"
"the rest of this paper is organized as follows: section 2 describes the context, research questions, and execution steps of this mapping study. section 3 presents the synthesis results of the extracted data from the selected studies and answers the research questions. section 4 discusses the threats to validity of this mapping study. section 5 comprises a discussion of the results and their implications, with conclusions in section 6."
"the context of this study, including the involved domains (software architecture and knowledge management) and the research questions, is discussed in section 2.1. the detailed process of this mapping study is presented in section 2.2."
"external validity is concerned with the representativeness of the selected studies regarding the overall goal of the mapping study [cit] . the results of this mapping study were considered regarding the knowledge-based approaches in the sa domain. therefore, the presented classification and systematic map of the selected studies and the conclusions drawn are only valid in the study topic. the predefined protocol is helpful to obtain a representative collection of studies in the given study topic. the representative venues for manual search also contribute to improving the representativeness of the selected studies. [cit], which does not affect the representativeness of the selected studies. [cit], and none of them focused on applying knowledge-based approaches in software architecting [cit] ."
"step 3: formal searches, including automatic and manual searches, were conducted. meanwhile, the first round study selection was performed accompanying the study searches. the study selection criteria are specified in section 2.2.2.1. the results of the first round selection of the two types of searches were recorded, and then were merged after removal of duplicated selection results as the input for the second round study selection."
"we performed the mapping study according to the steps described in section 2.2. we first present an overview of the study results, and then analyze the results of this mapping study to answer the rqs defined in section 2.1.3."
"color is one of the most important image descriptor used in cbir. human beings can identify thousands of color shades and intensities as opposed to a few shades of grey. rgb model is a widely used color model. it is composed of three color components red, green and blue. apart from the rgb color model, there are various other color models that one comes across in literature are ntsc, ycbcr, cmy and hsv. in image retrieval, a color histogram is the most commonly used color feature representation [cit] ."
"application of knowledge-based approaches. the difference between rq1 and rq2 is that the former is centered around knowledge-based approaches, while the latter is centered around architecting activities."
"shows that knowledge-based approaches kcr, kr, ks, krs, and krv are all used in architecting activities, but there is a significant difference in terms of the popularity of these knowledge-based approaches."
"to answer the rqs defined in section 2.1.3, we extracted specific data from the selected studies. table 8 describes the data items to be extracted in this mapping study. the first five data items (d1-d5) and the data item d7 directly contribute to the answers of the rqs. the data item d6 supports the explanation of the results of this mapping study in sections 3.2 and 3.3. to make sure the data extraction results less biased, two reviewers performed the data extraction independently, and then each reviewer checked the data extraction results by the other reviewer, and finally two reviewers achieved a consensus on the data extraction results."
"the main purpose of standard ieee 802.11n amendment is to provide mac service data unit (msdu) transmission throughput of at least 100 mbps in destination logical link control / service network access point (llc/snap). the mac layer of ieee 802.11n provides aggregate msdu (a-msdu), aggregate mac protocol data unit (a-mpdu) and block acknowledgement (ack) schemes to increase the throughput of msdu transmission. different from msdu transmission using distributed coordination fuction (dcf) channel access scheme, the amendment of standard ieee 802.11n explains that msdu transmission, which uses a-msdu, a-mpdu and block ack schemes, is transmitted using enhanced distributed channel access (edca) scheme. to calculate and evaluate the throughput of msdu transmission using a-mpdu and block ack schemes, an analytical model that can be used to predict the throughput of mac layer in ieee 802.11n accurately is needed."
"the parameters used to simulate the a-mpdu and block ack schemes are shown in table iii msdus with the size of 2200 bytes. the a-mpdu frame is assumed to be transmitted using the basic access mode. in every transmission of a-mpdu frame, then the transmitter would transmit 26 bytes of the bar frame to discover whether the a-mpdu sub-frame is received by the receiver without errors."
"if it is assumed that sinr is constant throughout mpdu frame transmission and ack frame duration, each of the mpdu frame error and ack fep is:"
"the average of channel in busy condition due to transmitted a-mpdu experiencing collision, by considering the effect of anomalous slot, can be calculated as follows:"
"we use mathematical simulation to investigate how fep performs on dcf scheme that is affected by the bit error in ht-phy and inter-frame collision. we calculate fep based on two variables, msdu size and number of sta. in this paper, fep in bianchi and tinnirello models are used to compare the proposed dcf model."
"after bit stream is attained from demodulation process, bit stream will enter de-interleave and decoding process. de-interleave process does not affect the bit error probability because it merely changes the position of bit stream. meanwhile the decoding process affects the bit error probability because it is measured based on the probability of some bits out of bits which experience error."
"the effect of bit error in ht-phy and frame collision to fep based on the sta number is shown in figure 5 and figure 6 . table ii shows the parameter that is used in simulation. simulation result shows that in msdu transmission using 1 and 4 spatial streams with the number of sta competitor increasing, fep will also increase. figure 5 shows fep of dcf scheme on msdu transmission using 1 spatial stream. the size of msdu transmitted is 2200 bytes. simulation result shows that if the number of competing sta and ber increases; fep also increases. when the number of competing sta equal to 5, fep equals to 0.4023, 0.2973 and 0.2858 for each ber equals to 10 -5, 10 -6 and 10 -7 . fep will increase even higher when 1 spatial streams in ht-phy layer is used to transmit msdu with a higher number of sta. in msdu transmission with the number of stas equal to 50, fep will be 0.6727, 0.6152 and 0.6089 for each ber equals to 10 -5, 10 -6 and 10 -7 . bianchi and tinnirello models each has fep of 0.2799 and 0.2845 when the number of stas equal to 5, and the fep will increase to be 0.5787 and 0.6082 when the number of stas equal to 50."
"bianchi and tinnirello models produce a different result. fep by both models is constant if transmitted msdu size increases. fep of both models is also constant when transmission is with a different spatial stream number and ber condition. thus, both bianchi and tinnirello models can only predict fep based on inter-frame collision."
"to model the msdu transmission slot using a-mpdu and block ack scheme, we use the assumption of damage in a-mpdu frame can be caused by: a-mpdu frame collision, a-mpdu sub-frame error, bar frame error and ba frame error. a-mpdu frame collision is the frame that carries multiple mpdu, which are damaged, caused by more than one sta transmitting a-mpdu in the same slot. a-mpdu sub-frame error is one or a number of a-mpdu sub-frames that carry an msdu or a-msdu that is damaged by bit error in ht-phy layer. each of slot type has different event probability. the probability of idle slot, transmission slot and collision slot can be calculated using bianchi and tinnirello analytical model [cit] . in our model, we also calculate the probability of a-mpdu error slot, the probability of bar error slot and the probability of ba error slot. the probability of errors slot are calculated based on the bit error rate (ber) in ht-phy layer."
"the further parts of this paper are organized as follows. in section 2, the relevant research review is delivered. in section 3, the a-mpdu and block ack slot modeling, such as the transmission slot modeling, the collision slot, and the error slot, is delivered. in the section 4, the mathematical simulation and the simulation result discussion are presented. in the last section, which is section 5, the conclusion and future work are presented."
"multiple input multiple output-orthogonal frequency division multiplex (mimo-ofdm) channel in ht-phy ieee 802.11n can be modeled as low fading channel toward frame duration; thus, it can be assumed that the channel condition is constant throughout the duration of a frame. every ofdm subcarrier can be treated as additive white gaussian noise (awgn) subchannel with a constant sinr throughout frame duration. signal vector subcarrier k received by receiver mimo-ofdm can be expressed as follows [cit] :"
"the ieee 802.11n standard has introduced the msdu transmission using a-mpdu and block ack scheme. in this paper we have derived the analitical model that can be used to estimates the throughput of a-mpdu and block ack scheme with ht-phy layer. our analitcal model consider the effect of anomalous slot and frame error probability. the simulation result shows the a-mpdu scheme can increase the msdu delivery throughput, particularly the big-sized one. in the condition of ber equal to 10 -7, the msdu having the size of 2200 bytes if transmitted using the a-msdu scheme only produces the throughput of 129.5 mbps. however, if transmitted using the a-mpdu scheme, the throughput can increase to 279.7 mbps. the small-sized msdu transmission using the a-mpdu scheme does not significantly increases the throughput. the throughput of msdu delivery with a smaller size than 400 bytes produces a lower throughput than the transmission using the a-msdu scheme with the threshold of 7935 bytes. the a-mpdu scheme throughput decrease in the smallsized msdu delivery is caused by the increase of mac layer overhead, and the probability of the a-mpdu subframe experiencing errors also increases. the mac layer overhead increase and the error frame probability in the a-mpdu subframe result in the decrease msdu delivery throughput."
"if bit error is assumed to be distributed in uniform and independently on each data segment, bit error probability on data segment with l byte length is:"
"channel in busy condition caused by bar frame error can be calculated as follow: (23) the average duration of channel in busy condition caused by bar frame error, by taking into account the effect of anomalous slot can be calculated as follow:"
the probability of a-mpdu sub-frame error can be calculated as follows: (36) in equation (34) is the error probability of bar frame. the error probability of bar frame can be calculated as follows:
"wlan ieee 802.11 has developed rapidly, and it has attracted many researchers and industries. the results of the researches have created new enhancement and its have been adopted to standard ieee 802.11 [cit] . among others developments that have been adopted are the increase of data rate in physical (phy) layer and provision of quality of service (qos) mechanism in medium access control (mac) layer. the data rate of phy layer up to 54 mbps in 2.4 ghz and 5 ghz frequencies was based on the amendment of standard 802.11g [cit] and 802.11a [cit] . the last amendment to increase the data rate of phy layer upto 600 mbps using high throughput (ht) phy was introduced in standard ieee 802.11n [cit] . meanwhile, the provision of qos mechanism in mac layer was adopted based on the ieee 802.11e standard [cit] . in the future, the data rate of phy layer is at least 1 gbps using ieee 802.11ac and ieee 80211ad standard [cit] ."
is the length of an mpdu in an a-mdpu. the maximum length of an mpdu in an a-mpdu is 4095 bytes and the maximum number of mpdus in an a-mpdu is 64.
"the receiver reception can be measured as sinr per bit (notated as γ ) and average sinr per symbol (notated as γ ). when the receiver in ht-phy receive signal that is modulated using bpsk, qpsk or m-qam, the receiver then demodulates the signals into bit stream. bit stream attained from the result of bpsk and qpsk signal demodulation has bit error probability as follows [cit] :"
"is the error probability of a-mpdu frame. based on the calculation for bit error probability after demodulation and decoding process in ht-phy, the error probability of an a-mpdu sub-frame can be calculated as follows:"
"the average of msdu total bytes in a-mpdu frame received correctly by the receiver in the same transmission slot, and by also considering the effect of anomalous slot can be calculated as follows:"
"in equation (14) is the duration of a successful a-mpdu transmission. in basic access transmission mode, can be calculated as follow: (15) is the overhead of ht-phy layer., and are the duration of a-mpdu, bar and ba frames transmission times. they are dependent to modulation and coding schemes (mcs) index and encoding type used in the ht-phy layer. if bcc encoding is used, it can be calculated using the following equation: (16) (17) (18) is the length of a-mpdu frame can be calculated as follows:"
"the estimation of the duration of a-mpdu transmission using edca scheme, by also calculating the effect of a-mpdu sub-frame error, can be calculated as follows: (34) is the probability of a slot in edca scheme when idle, is the probability of a-mpdu to be transmitted, is the probability of a-mpdu to be damaged due to collision in the channel, is the probability of error of a-mpdu sub-frame, is the probability of error of bar frame and is the probability of error of ba frame."
"anomalous slot can also happen when the transmitted frame collides or experiences an error. if the transmitted frame by sta on a slot collides or experiences an error, the first slot after extended inter-frame space (eifs) duration cannot be used to transmit the frame by the last sta transmitting the frame or by any other sta which detects collision on channels."
"in the ht-phy layer, the a-mpdu frame is added by the preamble of plcp ht-greenfield. the field of a-mpdu is coded using bcc with the coding rate 5/6. the a-mpdu bits coded and interleaved are then modulated using 64-qam. the ofdm signal is transmitted using the bandwidth of 40 mhz. among the ofdm symbols is the 400 ns guard interval inserted. ht-phy is assumed using 4 spatial streams. if the 30 a-mpdu sub-frames managed to be received without errors in the receiver, the receiver further sent back the ba frame with the 32 bytes length. figure 7 shows the msdu throughput performance at destination llc/snap. the throughput is calculated based on the bit error probability after the demodulation and decoding process in the ht-phy layer with the bit error probability variation of 10 -5, 10 -6, and 10 -7 . it is assumed that there are 5 stas, all of which used the same parameter of ht-phy and mac as shown in table iii ."
"tinnirello model used slot types same as bianchi model [cit] as shown in fig. 1 . bianchi and tinnirello modeled dcf scheme based on three different slot types. the slot types are idle slot, transmission slot, and collision slot. but unlike bianchi model, transmission slot and collision slot in tinnirello model has considered the anomalous slot phenomenon."
"as explained in ieee 802.11n standard document [cit], the coding process on ht-phy can used binary convolutional code (bcc) or low density parity check (ldpc). if it is assumed that the decoder used is bcc, the probability of a bit to experience error after demodulation and decoding process is:"
the average duration of busy channel due to a-mpdu frame that is successfully transmitted and by taking into account the effect of anomalous slot can be calculated as follows:
"probability of mpdu frame collision (p ) can be calculated using equation (7) in tinnirello model [xx] . finally, fep in dcf scheme which is caused by mpdu collision, mpdu error and ack error can be calculated using equation (13) ."
"in the document of ieee 802.11 standard, it is explained that when a wireless medium is busy in a slot time, the counter backoff values decrement is stopped. in other words, counter backoff decrement is only conducted when the slot is idle. the rule of backoff counter decrement at the end of time slot causes there is a slot after successful transmission only accessed by the last station (sta) who is transmitted the frame. this slot is called the anomalous slot."
"is the number of msdu bytes in a-mpdu frame successfully received by destination llc/snap. if the ber condition is good, then all a-mpdu sub-frames with an msdu or a-msdu inside can be received well. however, if the ber condition is bad, then one or more a-mpdu subframes can be experiencing error. thus, can be calculated using the number of msdu bytes in a-mpdu sub-frame that is successfully received. the length of all msdus received by the receiver both in error and non-error a-msdu sub-frames can be calculated as follow: (27) if the bit error in ht-phy layer is assumed as distributed uniformly in a-mpdu frame, the estimation of length that only has good a-mpdu sub-frame can be calculated as follows:"
"the figure 6 shows fep of dcf scheme in msdu transmission using 4 spatial streams. when the number of competing stas equal to 5, fep equals to 0.6515, 0.3342 and 0.2896 for each ber equals to 10 in this section, we also use the mathematical simulation to analyze the analytical models of the a-mpdu scheme and the block ack scheme. we calculate the msdu throughput at destination llc/snap transmitted using the a-mpdu scheme and the block ack scheme. the throughput is calculated based on the msdu size transmitted and the number of sta. to shorten the simulation process and the result discussion, we only show the a-mpdu throughput transmitted by ac [cit] having the highest transmission priority in the edca scheme. then, we compare the throughput produced by the a-mpdu and block ack schemes with the a-msdu scheme throughput so that the throughput increase using the a-mpdu and block ack schemes can be discovered."
"in modulation m-qam, k log m can be defined where k is the number of bit represented in one ofdm symbol which uses difference in phase and amplitude as m 2 . error symbol probability modulated using m-qam is:"
"the small-sized msdu transmission using the a-mpdu scheme does not significantly increases the throughput. the throughput of msdu transmission with a smaller size than 400 bytes produces a lower throughput than the transmission using the a-msdu scheme with the threshold of 7935 bytes. the a-mpdu scheme throughput decrease in the small-sized msdu transmission is caused by the increase of mac layer overhead, and the probability of the a-mpdu sub-frame experiencing errors also increases. the mac layer overhead increase and the error frame probability in the a-mpdu sub-frame result in the decrease msdu transmission throughput. figure 8 shows the msdu throughput performance at destination llc/snap transmitted using the a-mpdu and block ack schemes based on sta number increase."
"the probability of a frame experiences corrupt or error depend on the probability of bit error in ht-phy. a bit experiences error depend on the received signal level in ht-phy receiver side. for the calculation of the probability of bit error to be more accurate, received signal level measured in the form of signal to interference and noise ratio (sinr)."
where is the number of a-mpdu sub-frames that are damaged in an a-mpdu frame and is the total number of a-mpdu sub-frames in an a-mpdu frame.
"where n is the number of data segment, l is the segment length of a packet, mod is the modulation on segment i and γ, is the sinr per bit on segment i. it is worth noted that in one data segment, the average sinr value and the kind of modulation used is the same. however, for a different segment sinr and/or modulation used can be different."
"during phase rsa-3, services previously identified are described, modeled, and composed, resulting in the functional software architecture of the sors. therefore, interfaces, contracts, quality characteristics, and relationships of all robotics services should be created. the following activities are carried out in this phase."
"in phase rsa-1, the application is described in terms of goals, activities, and characteristics about the robotic system and its operating environment. additionally, applicable policies, rules, and constraints related to the robotic system are identified. as a result, the document of requirements of the robotic system is produced. the activities performed during this phase are detailed as follows."
"in order to evaluate the archsors process, we have performed an experiment with 30 students of a preparatory course for the french national robotics competition 5 . these students were divided into two groups: (i) one to design the software architecture of a sors using archsors and (ii) other to design it in an ad hoc manner. the software architectures were evaluated using metrics of coupling, cohesion, and modularity, since these metrics directly impact on quality attributes such as modifiability, reusability, and buildability. results pointed out that students using archsors designed software architectures that score better in these three metrics and, therefore, tend to present higher quality."
"rsa-a 2.3 -identify available capabilities: robotics experts identify capabilities that are already available and can be reused. these capabilities are identified from different sources, such as: (i) robotic systems developed in previous projects; (ii) repositories of services for sors; (iii) development environments, such as ros and mrsd, which provide a set of native services for sors; (iv) companies that provide device drivers and other capabilities related to their products; and (v) general purpose repositories, such as service brokers."
"rsa-a 1.1 -initiate project activities: the main goals and characteristics of the robotic application are defined, described, and documented. robotics specialists should perform brainstorm meetings to identify: (i) goals related to the robotic application; (ii) activities that the robotic system should perform to achieve these goals; (iii) the type of robotic system that will be developed, i.e., if the application involves a single robot, a team of robots, or a swarm; (iv) the type of robot (or types of robots) that will be used, the characteristics related to its mobility (if it will be mobile or non-mobile), the way it will move through the environment, its size, and so forth; and (v) the environment where the robot will be used (indoor, outdoor or both). at this point, no assumption is made on which hardware devices will be used in the robotic system. rsa-a 1.2 -identify policies and rules: robotic applications must be conform with applicable policies and rules to be commercialized and used. a policy, for instance, can be defined by a law that regulates the operation of a given type of robotic system. rules are restrictions on the robotic system design and operation that must be respected to comply with a given policy."
"rsa-a 1.3 -identify constraints: based on the decisions made in activity rsa-a 1.1, constraints related to the robotic application are identified. these constraints are associate to both hardware requirements and real-time operation. infrastructure requirements, such as battery consumption, processing power, network availability, and robot autonomy, are considered in the identification of hardware constraints. use scenarios are carefully identified and described in the definition of real-time constraints. afterwards, constraints associated to these scenarios are detected and prioritized. as robotic systems are often used in safe-critical domains, real-time constraints are very important and they must guide the rationale behind service identification and composition."
"rsa-a 1.5 -define functional requirements: based on the outcomes of the previous activities, information related to the robotic system are obtained, resulting in a set of functional requirements. these requirements represent the functionalities that the sors should provide to perform the robotic application."
"using soa, complex robotic systems can be developed by assembling functionalities provided by independent, distributed software modules called services. designing robotic systems using soa allows integration of heterogeneous hardware devices and reuse of complex algorithms, since services are provided through auto-descriptive standard interfaces. due to its relevance, several works reporting the use of soa in robotics are available in literature, such as those that we identified in our previous work [cit] . besides that, development environments specially focused on the design of service-oriented robotic systems (sors) can be also found [cit] . nevertheless, few attention has been paid to the development of sors software architectures. currently, most of software architectures are designed in an ad hoc manner, without a systematic approach of development, hampering the construction, maintenance, and reuse of robotic systems. the consideration of quality attributes since the software architecture design is a critical concern, as these systems are often used in safety-critical contexts."
"rsa-a 4.4 -detail sors concrete architecture: finally, the overall structure of the functional architecture is described in a document containing all information related to its design. textual descriptions of the diagrams and design decisions are documented. additional views of the architecture, such as deployment view, can also be created."
"archsors process is divided into five phases that can be applied in an iterative, incremental manner. the phases are divided into a set of activities, which are detailed into a comprehensive set of tasks. however, for sake of space detailed information and diagrams are only available in the spem (software & systems process engineering metamodel specification) version of the process 1 . since archsors is an incremental process, sors software architectures can be successively refined from reference architectures into concrete architectures. in short, to establish a software architecture using archsors, it is first necessary to characterize the robotic application and to produce the document of requirements (step rsa-1). following, in step rsa-2, requirements are used to model the application flow and to identify capabilities that the robotic system should provide. in step rsa-3, the functional architecture is described and represented in terms of the services used to provide the identified capabilities. in step rsa-4, step rsa-5, the sors software architecture is evaluated using architectural analysis methods. after that, if necessary, the evaluated architecture is refined through new iterations on the design process. software architects (functional and technical) and robotics experts are involved and conduct the phases of the process. these phases are detailed as follows."
"rsa-a 1.6 -define quality requirements: in this activity, quality requirements of the sors are identified considering: (i) application goals; (ii) policies and rules; (iii) constraints; and (iv) standards associated to policies and rules. afterwards, brainstorm meetings are carried out to prioritize the most important quality requirements. in a previous study [cit], we have already identified a set of quality requirements considered as the most important to embedded systems. these requirements can be used as a starting point for this activity."
"the main objective of this paper is to present archsors (architectural design of service-oriented robotic system), a process that aims at filling the gap between the systematic development of service-oriented systems and the current ad hoc approaches used to develop sors. the archsors process provides prescriptive guidance from the system specification to architecture evaluation. results from our experiment indicate that archsors has positive impact in modularity, cohesion, and coupling of sors software architectures, thereby improving important quality attributes such as reusability and maintainability."
"rsa-a 4.2 -design of refactored components: services that provide capabilities from existing robotics assets are designed. to perform the refactoring, design documentation of the robotics assets is analyzed and new diagrams representing the robotics components are created."
"in this phase, the sors technical software architecture is evaluated and the compliance with requirements and systems constraints is assessed. different evaluation methods can be used to perform this evaluation, such as inspection check lists and scenario-based methods. moreover, the architectural description itself should evaluated to identify and eliminate defects related to omission, ambiguity, inconsistency, as well as strange and incorrect information. as a result, a more reliable software architecture version of the robotic architecture is achieved."
"rsa-a 3.3 -define service constraints: to ensure the compliance with the overall robotic system constraints, each service must guarantee its individual set of constraints. the clear description of constraints at architectural level is crucial to the determination of which participant (i.e., concrete service) will be able to provide a given service. thus, the capabilities document is updated with information about the constraints of each robotics service of the architecture."
"rsa-a 2.2 -decompose the robotic application: based on the defined model, the robotic application is decomposed into capabilities, which provide a set of functionalities of the sors. to support this activity, we established a taxonomy that lists a comprehensive set of service candidates for sors [cit] ."
"the remainder of this paper is organized as follows. section 2 presents arch-sors and describes its phases. section 3 discusses on archsors evaluation. in section 4, we present our conclusions and perspectives of future work."
"rsa-a 1.4 -identify standards: robotic systems may need to be certificated to ensure the compliance with policies imposed to its operation. to obtain certification, development standards are applied both to robotic system and its development process. thus, at this point, all standards related to the sors are identified. different standards can be applied to robotic systems and it depends on its own characteristics and on the environment where it will be used."
"soa has been increasingly adopted for the development of sors, getting advantages of soa and resulting in more flexible robotic systems. the main contribution of this paper is to put forward archsors, a process that intends to systematize the development of sors software architectures and, as a consequence, to improve the quality of such systems. experiment results point out that archsors can positively impact on the quality of sors. as future work we plan to perform a case study on the development of sors using archsors."
rsa-a 3.1 -specify robotics services: the document containing information about the robotic capabilities is updated and a detailed description of the roles played by each service is created. this document links the requirements of the robotic system to the requirements provided by each service.
"archsors is a process that promotes the systematic development of sors software architectures. it explicitly considers the identification and assessment of constraints and quality attributes that are essential to robotic systems. the process also encompasses the main phases proposed by the consolidated soma (service-oriented modeling and architecture) method [cit] . archsors was established based on sors software architectures available in the literature [cit], a set of reference architectures that encompass knowledge of how to structure robotic systems [cit], and our expertise on critical embedded systems. fig. 1 outlines the overall structure of the archsors process."
"rsa-a 4.1 -design of new components: services that are not available for reuse and need to be developed are further detailed and represented. different diagrams can be designed, illustrating both design and runtime aspects of the services. the representation of the internal structure of services may be done by using ordinary object-oriented (oo) modeling and different design patterns."
"rsa-a 3.6 -specify robotics components: robotics services are often abstractions of functionalities provided by the coordination of one or more components, i.e., capabilities that were not directly exposed as services. thus, relationships among services and components of the sors should be described and modeled using different representations, such as uml component diagrams."
"rsa-a 3.5 -define services composition: the composition of robotics services is defined and the relationship among service partners are detailed. these partnerships are designed considering obligations of consumers and providers defined in the service contracts. in addition, complementary information about the interactions are described, such as service partners that should be hosted in the same infrastructure. these constraints are used to support decisions made during the design of the functional architecture described in the next phase."
"rsa-a 4.3 -rationalize technical decisions: technical architects and robotics experts decide about hardware infrastructure and implementation strategies that will be used during the robotics services concretization. in addition, decisions are made on how the services of the robotic system will be deployed. as a result, a document reporting the rationale on service concretization is created."
"over the last years, robots have increasingly supported different areas of society. robots are no longer used only inside factories, but inside houses [cit] and on the streets [cit] . due to this high demand, robotic systems used to control robots are becoming larger and more complex, creating a great challenge to the development of this special type of software system. researchers have been investigating different architectural styles focused on providing more quality for robotic systems. robotic systems development has evolved from procedural paradigm to object-orientation, and thence to component-based architecture [cit] . more recently, service-oriented architecture (soa) [cit] become focus of attention as a promising architectural style to develop more reusable, flexible robotic systems."
"in phase rsa-2, the robotic application is described in terms of functionalities necessary to achieve robotic system goals, the flow between these functionalities, and capabilities that are responsible for providing them. thus, the application flow is modeled and then decomposed into different robotic capabilities. a capability is a service candidate that may either be already available or need to be developed. descriptions of the activities of this phase are presented as follows."
"based on the quality requirements of the robotic system and the services constraints, quality requirements related to each robotics service (i.e., qos requirements) are identified and the capabilities document is again update. qos requirements represent information about how functionalities of robotics services should be provided."
"rsa-a 1.7 -document sors requirements: based on the results of the two previous activities, the document of the sors requirements is created. this document will guide the description of the robotic application flow and support the identification of robotic capabilities. the document should be reviewed by all stakeholders to ensure that it is correct, complete, and in accordance with the robotic application goals and characteristics."
"previous projects of non-service-oriented robotic systems are investigated to identify assets that can be provided as capabilities. these assets are packages, software modules, legacy applications, and algorithms (such as for localization and mapping) that can be wrapped as capabilities and then provided as services for the robotic system."
"rsa-a 2.1 -model the robotic application flow: during rsa-a 2.1, the flow of activities of the robotic application is described using description languages such as unified modeling language (uml) 2 activity diagrams and business process model and notation (bpmn) 3 . the robotic application is described in terms of: (i) functionalities performed in parallel; (ii) functionalities performed in sequence, i.e., that depend on the result of the execution of previous functionalities; and (iii) functionalities that provide results based on the combination of results from other functionalities. thereafter, the model is reviewed to check whether it fulfills all functional and quality requirements."
"rsa-a 3.7 -document sors functional architecture: the outcome of phase rsa-3 is a document describing the sors functional architecture. this document is produced by updating the capabilities document with all developed models, the design rationale applied in the modeling, and all useful information regarding the functional aspects of the robotic system."
"rsa-a 3.2 -model robotics services: based on the updated capabilities document, services of the robotic system are modeled. as mentioned before, different types of adl can be used to describe interfaces, contracts, and operations of the services in the architecture. in sors, contracts, associated to the interfaces, usually enforce three types of interaction: (i) synchronous remote procedure call (rpc); (ii) asynchronous rpc; and (iii) service subscription, which is a long-term interaction in which the service client implements a handler method to receive notifications from a service provider."
"rsa-a 2.5 -identify assets that can be refactored: assets that are useful for the robotic system but can not be provided directly as robotic capabilities should also be identified. these assets have to be refactored in order to be reused as capabilities of the robotic system. rsa-a 2.6 -rationalize capabilities: services of the sors are obtained based on the analysis of capabilities. discussions are made to decide which capabilities will be exposed as services and which capabilities will be provided as components that support these services. as a result, a document is created to report: (i) capabilities related to the robotic application; (ii) functionalities provided by each capability; (iii) architectural elements used to provide each capability; and (iv) the design rationale related to these decisions."
"in this phase rsa-4, the functional architecture is detailed in terms of modules of software and hardware devices used to develop services of the robotic system, resulting in the technical architecture of the sors. descriptions of the activities conducted in this phase are presented as follows."
"half of the 22 terms we used to clean the nanocellulose search strategy did not affect the coverage of core-nanocellulose publications in the nuclei research areas, as depicted in fig. 3 . to the other half, none term could reduce the coverage in more than 5 %. the terms that influenced research area 13.6.4 the most were ''*cell wall*'' and ''hemicelluloses'' while ''*cyto*'', ''gene'' and ''*cell wall*'' were the ones that decreased the most corenanocellulose coverage in cluster 13.6.11."
authors' contributions oa organized the real-time simulation models and optimization problems and drafted the manuscript. pf carried out the overall concepts of csp and demand response programs designing. zv raised and developed the overall idea of the work. all authors read and approved the final manuscript.
"snomed-ct (systemized nomenclature of medical clinical terms) covers more than 360,000 medical concepts that are taxonomically modelled in 18 partially overlapping hierarchies; individual concepts are associated to lists of equivalent terms (synonyms) [cit] ."
"reference [cit] ) utilized a realistic methodology by considering csp as a load aggregator in order to evaluate the impact of dr programs in the day-ahead colombian electricity market. they considered that csp can bid in the wholesale market by using the demand-bidding program. they also presented a tool for the market operator in order to quantify the impact of dr programs on the system. realistic values from colombian market have been used in order to perform numerical tests. the numerical results shown that the penetration of demand-bidding program changes the dispatch for different generation units. although they utilized real input data for the model, the final results are totally numerical without any experimental test and validation."
"in this step, the system takes as input the medical document to be sanitised and the list of sensitive terms s. the goal is twofold: first, all the terms of the document that appear in s are detected and marked as sensitive; after that, the system uses the kb to sanitise the formerly detected terms by replacing them with suitable generalisations."
"if the use of dr programs is merged with distributed renewable energy resources (drers), the grid operator would be able to fully benefit from these concepts and participate in market negotiations [cit] . however, both dr and drers should have enough capacity of reduction and generation in order to participate in the market negotiations. based on several surveyed references [cit], the minimum reduction capacity of customers for participating in dr programs in several electricity markets is 100 kw. this means that in such markets it is not possible for small and medium consumers, such as typical residential customers, to contribute to market negotiation individually [cit] . in order to overcome this barrier, a third party entity can be considered as a solution in order to aggregate small and medium scale resources and represent them as a unique resource in the energy market negotiations with adequate capacity [cit] . curtailment service provider (csp) is a concept that can be considered as a third party in the electrical network operation [cit] . however, before the implementation of business models, it is required to test and validate the model concepts in reliable and physical simulation platforms, which are capable to provide actual measurement and control in order to identify future problems [cit] . for this purpose, the use of fully computational resources to simulate an electrical distribution network can be very difficult and unaffordable, and perhaps the produced results will be far from the reality [cit] ). therefore, a real-time simulation strategy would be a satisfying solution for integrating both reality and simulation results [cit] ."
"as it was mentioned before, in order to implement a dr program in a network, there is a minimum capacity for load reduction, which should be reached by the customers in order to be able to participate in the dr event. in this context, if a consumer has enough capacity of reduction, he can directly establish a contract with the system operator or dr managing entity. however, for small and medium consumers, who do not have adequate capacity of reduction, a third party entity (csp in this paper) should aggregate these consumers, and allow them to participate in dr event as one. this means that the small and medium consumers can establish a dr contract with a csp in order to be able to utilize dr programs, and participate in the wholesale market negotiations. the overall concept of the proposed csp model is illustrated in fig. 1 ."
"in this section, at first, the results obtained from optimization problem are provided, and then, the real-time data acquired by op5600 from simulink is presented. the optimization results belong to all the csp network, however, the main focus is given to the prosumer in bus #15, and to the two producers in bus #10 and #5. figure 7 illustrates the optimal resource scheduling results of the csp for network players in bus #15, #10, and #5."
"to simulate and evaluate the protection that our system would be able to achieve for textual medical documents (e.g. electronic healthcare records), we used the wikipedia descriptions of the set of medical entities that are considered as sensitive by u.s. state and federal laws [cit] . as stated in the introduction, these legislations mandate hospitals and healthcare organisations to redact some medical-related concepts that are considered of confidential nature before releasing patient records to, for example, insurance companies, in response to worker's compensation or motor vehicle accident claims, or a judge, in case of malpractice litigation [cit] . usually, all references to potentially discriminating conditions like alcohol and substance abuse, sexually transmitted diseases (std), mental diseases or hiv/aids status should be redacted/sanitised. all these terms were feed as the input to our method in order to obtain the list of sensitive terms s from snomed-ct (step-0a) and to compute the sanitisation threshold (step-0b). a total of 6 wikipedia articles each one describing the main entities from a medical perspective were taken: std, hiv, aids, mental disorder, alcohol abuse and substance abuse. as done in other works [cit], wikipedia descriptions were chosen as representatives of the kind of textual information that could be found in healthcare records due to being freely accessible and authoritative sources of information, and also because of their high informativeness and tight discourses, which configure a challenging scenario from the perspective of document redacting/sanitisation. snomed-ct, wordnet and odp were used to retrieve term generalisations and bing 4 was employed as the web search engine to obtain term hit counts for probability calculus."
"there are, however, noticeable differences between the specific protection method and the probability calculus. first, the strategy based on term generalisation preserves around 100% more utility (34,5% vs. 21,3% for non-contextualised probabilities and 74,1% vs. 31,5% for contextualised probabilities, in average) than the one performing term removal. in fact, figures obtained by pure redaction (i.e. removal) suggest that the protected outputs would be hardly usable for human readers and also for data analysis. moreover, the contextualisation of the probability calculus adds a comparable degree of utility improvement over the baseline protection strategy (31,5% vs. 21,7% for pure redaction and 74,1% vs. 34,5% for term sanitisation, in average). in this latter case, the observed improvement is motivated by:"
"in section 3.2 we argued about the suitability of using the web as general corpora for the probability calculus needed for ic/pmi assessments, and of using web search engines (wses) as proxies to obtain these probabilities. however, probabilities of sensitive entities computed from the number of explicit occurrences of terms referring to them, that is, the hit count provided by the wse when querying the terms, may be negatively influenced by language ambiguity. specifically, when the words used to refer an entity are polysemic (e.g. crabs can refer to a sexually transmitted disease or to a crustacean), the probability computed from the web hit count may overestimate the probability of the underling concept (e.g. crabs as a std), because the web hit count includes all the appearances of the word, regardless of their meanings; thus, the entity will be considered as less informative than what it truly is. likewise, if an entity can be referred by means of different synonyms (e.g. the terms"
"an independency test was conducted to evaluate the effectiveness of the procedure proposed. the test involved retrieving the number of publication from the top five authors before and after cleaning the initial set of nanocellulose publications, obtaining the percentage of decrease and the position on a ranking of authors. we performed the analysis to the top authors from each nuclei research area."
"given the burden of manual document redaction/sanitisation, the development of automatic schemes capable of dealing with textual healthcare data and avoiding the disclosure caused by semantic inference while retaining the utility of the output document, is a clear need for medical organisations."
"the csp always tends to supply the demand using the local resources. renewable resources are the first ones that csp utilizes for scheduling since these have a lower price from the csp standpoint. after that, dr resources, especially rtp program, are the next options for csp scheduling, and external suppliers would be the last choices for the csp since it is considered as the most expensive resources from the csp standpoint. therefore, an optimization problem is required for the csp in order to provide an optimal resource scheduling. however, as previously stated, the optimization problem is not a core contribution of the paper since the main focus of the paper relies on demonstrating that the hybrid simulation platform is also capable to implement optimal scheduling results and dr programs."
"the presented case study tested and validated the system capabilities in terms of controlling and monitoring the real resources from the simulation environment by using a real-time simulator. the results of the case study are the ones actually measured from the real and laboratory loads and generators. these results demonstrate that the real implementation of management scenarios, namely demand response programs or resources scheduling, electrical grid conditions play a key role since voltage variations affect the consumption and generation profiles. moreover, it is shown that the results obtained from the optimization have a little difference compared to the real-time simulation results. this is because of the reaction of the real resources outside of simulation environment; real resources take some time in order to fulfill the system goal and reach the desired consumption or generation level. in fact, this issue validates the need for a real-time simulation and hardware-in-the-loop methodology using real hardware resources, before massive implementation of business models. availability of data and materials please contact author for data requests."
"as a final result, fig. 12 shows the total aggregated generation from bus #15, #10, and #5 in the csp network in real-time, which is the sum of generation profiles shown in figs. 9, 10 and 11."
"this section concerns the presented csp model applying dr programs to its consumers, and the optimization formulations utilized by the csp in order to optimally schedule the consumption and generation resources."
"an analysis of the content of publications in the peripheral research areas was conducted. we wanted to check whether these articles have the nanomaterial as an object of research. if not, they were considered noise. because an evaluation of all research area retrieved would be too labor intensive, we made a selection. the checking task was performed only on those clusters that matched one of the following criteria:"
"the network considered for the csp model (right side of fig. 1 ) is an internal low voltage distribution network of a university campus in porto, portugal [cit] . the csp grid includes 21 bus with underground cables, where a mv/lv transformer in bus #21 connects the csp grid to the main network. this csp network is considered as a part of the main network containing 220 consumers, and 68 dg units (left side of fig. 1 ), which has been developed by the authors in the scope of previous works [cit] ."
price-based: in which the end-users modify their consumption based on the electricity price variations. real-time pricing (rtp) is an example of this group; incentive-based: in which the grid operator pays a remuneration to the end-users in exchange for modifying their consumption pattern. direct load control (dlc) is an example of this group.
"-as argued in section 3.5, term ambiguity (i.e. polysemy and ellipsis) is minimised when using contextualised probabilities. given that this ambiguity tends to underestimate baseline ic values and, thus, to force the method to replace terms by more abstract generalisations to fulfil the privacy criterion (i.e. those with lower ic), unambiguous probabilities contribute to retrieve more accurate and, thus, more utility-preserving generalisations."
"hiv and human immunodeficiency virus refer to the same disease) or entities referred in a discourse are not explicitly mentioned in text due to ellipsis, the resulting probabilities will be lower than expected and, thus, the entity informativeness will be overestimated."
"it is important to state that this first search retrieves publications that are not necessary about nanocellulose-focused research, i.e. one of the nanocellulose term from table 1 might have been cited on the title/abstract/key-words from an article, but this article is not referred to the nanomaterial. in fact, nanocellulose is an interdisciplinary field of research [cit], b), which might explain the emerging of many search terms ( table 1 ) that are frequently used by one or two research area. however, we are interested in publications that have nanocellulose as the main object of research and it is quite challenging to separate these publications from the ''noisy'' set applying regular boolean searches on databases. thus, the recall was favored at this first stage on the proposed delineation, but the next stages focused on increasing the precision."
"for the prosumer in bus #15, a 4 kva and a 30 kw load emulate the consumption, and a 1.2 kw wind turbine emulator is responsible for wind generation. moreover, for the two pv producers, in bus #10, a 1.8 kw dc power supply connected to a dc/ac inverter emulates a pv producer, and in bus #5, a real installation of pv system with a maximum capacity of 10 kw stands for another pv producer. all this equipment is controlled and monitored via op5600 in matlab/simulink."
"in this case study, dg units are considered as the cheapest resource for csp so these will be the first resource to be utilized by csp to supply the demand. in the meantime, the price of buying energy from external suppliers is considered as the most expensive resource of the csp. this means that, in high consumption periods, it is affordable for csp to perform dr programs to reduce the consumption in order to avoid purchasing energy from the external suppliers. in this context, it is considered that the prosumer in bus #15 has established a dr contract with the csp for 2.5 kw reduction between periods #8 to #19."
"step-0a) will be run the first time the system is deployed. after that, recalculations should be only needed when the privacy requirements (i.e. the list of sensitive elements defined by the legislation) or the medical knowledge base (snomed-ct) change."
"new technologies have played a crucial role on improving healthcare delivery. data digitalisation, in particular, has paved the way for the extensive adoption of electronic health records (ehr) systems that have enabled clinicians and researchers to access, manage and exploit large amounts of valuable patient data easily [cit] . nevertheless, as data becomes more accessible and easily copied and transferred, the confidentiality of the patients is more likely to be jeopardised."
"there are several constraints considered for this optimization problem. the first constraint stands for load balance, as shown in eq. (2). the second constraint, in eq. (3), concerns dg units and prosumers, where indicates if the customer is a prosumer, its dg generation supplies the local demand first, and then, if there is any generation surplus, it will be injected to the csp network."
"in this sense, classification systems are an indispensable tool to study the structure and dynamics of scientific fields because they can simplify literature search and information retrieving procedures. [cit] highlighted that the assignment of individual publications or journals to research areas still remains an open question in scientometrics. most used classification systems are based on journal assignment, such as the web of science and scopus systems. the drawback of these journal-based classification systems is the fact they do not deal properly with multidisciplinary journals or interdisciplinary research [cit] . some studies also question the appropriateness of the database journal-based classification system, such as for the purpose of normalizing citation impact indicators [cit], assignment of individual publications [cit] or evaluating complex and emerging domains [cit] provides an interesting overview of current field delineation procedures, challenges of information retrieval and the advantage of hybrid approaches."
"to delineate representative nanocellulose research areas, the cwts publication-level classification system was used. [cit], the system has been updated yearly since it first version. [cit] and indexed in the science citation index expanded and the social science citation index. the main characteristics of the classification are as follows. clusters of publications are created on the basis of citations from one publication to another. around 15 million of publications are processed in the current version. the clusters contain publications from multiple years (traditionally a time span of 15 years) and are created at different levels in a hierarchical way. each publication is assigned to one cluster only at each level. in this research we considered the lowest level wherein to be part of a cluster, a publication must be related, either directly or indirectly, to at least 49 other publications in terms of citation. finally, a cluster is considered and in many cases validated as representative for disciplines, research areas, fields or topics. figure 1 presents a schematic representation of the distribution of the clustered web of science publications according to cwts publication-level classification system [cit] . predefined nanocellulose publications are indicated as black circles and the first step is retrieving all research area that contains at least one of them. figure 2 depicts the proposed procedure as an iterative process which can be described in four main steps:"
"an analysis from the topics shared by the nuclei areas was performed with support of vosviewer. these topics were extracted using nlp applied on titles and abstracts and the resulting noun-phrases are presented on fig. 8 . the more the noun-phrase is located in the left part of the map the more it appeared on titles and abstracts from papers addressed to cluster 13.6.3 (red). otherwise, the noun-phrase more to the right belongs to cluster 13.6.11 (blue). noun-phrases between these areas represent topics that occurred constantly in both clusters. noun-phrases from nuclei cluster 13.6.3 (cellulose nanocrystals/microfibrillated cellulose) refers to nanocelluloses produced by top-down approaches, its variables of processes, and its uses mainly in nanocomposites. on the other hand, nuclei cluster 13.6.11 (bacterial cellulose) contain noun-phrases related to nanocellulose produced using bottom-up approaches, such as biosynthesis, and its aspects. it was also verified that the subjects shared by these nuclei clusters refers mainly to properties and morphology of nanocellulose (for example, electrical conductivity, high tensile strength, high mechanical strength, cytotoxicity, crystallinity index, crystallite size, pore size, etc.); and characterization techniques (for instance, x-ray diffraction, nuclear magnetic resonance spectroscopy, scanning electron microscope image, fourier transformation infrared spectroscopy)."
"the rest of paper is organized as follows: section 2 presents the related works and clarifies the main contribution of this paper. section 3 explains the presented csp model including all dr programs and mathematical formulation regarding the optimization problem. section 4 presents the real-time simulation model considered for the csp, where all hil infrastructures and matlab/simulink models implemented in the op5600 are denoted. section 5 discusses a case study implemented using the presented model in order to test and validate the system capabilities, and its results are presented in section 6. finally, the main conclusions of the work are presented in section 7."
"if a customer establishes a contract with the csp for dlc, he will give permission to the csp to directly control the devices whenever it witnesses with critical periods, for instance, while he faces a technical or economic reason. for this purpose, the customer will be notified about the event, and receive remuneration based on the actual kwh reduction. if a customer makes a contract with csp for red. program, while the csp decides to apply dr event, the customer will be notified for consumption reduction, and if it is accepted, he receives remuneration based on kwh reduction. finally, if a customer has a rtp contract, it specifies a value of electricity price somehow that if the electricity price raises and is greater than that specific value, he will decrease the consumption as much as it is specified in the contract. this means that the customer will be notified about the real-time electricity price, and if he agrees to participate, he will reduce the consumption. in the rtp program, the customer will not earn remuneration for the consumption reduction, which means rtp program has no cost from csp standpoint."
"legislation [cit], specifying the kind of entities that should be protected because they are potentially discriminatory (e.g. aids/hiv, mental diseases, sexually transmitted diseases and drug or alcohol abuse). in any case, other privacy requirements may also be incorporated. then, it uses a knowledge base (kb) to compile a list of terms that will be considered as sensitive and, thus, should not appear in any publicly disclosed medical document. we use snomed-ct as knowledge base."
"as it can be seen in fig. 1, the csp is an intermediate entity between the demand side and grid side. in the demand side there are several small and medium scale consumers, producers, and prosumers (a consumer who can also produce); the csp will be responsible for aggregation, scheduling and remuneration of dr events. in the grid side, the csp will be in charge of market negotiations, energy trades, and bids with grid players, such as market or system operator. also, the csp should be able to accommodate the uncertainty that is related to the actual consumption and generation and to the actual response of consumers to dr events. in some cases, the csp can make direct load control (dlc) in the consumers loads but in other cases, without control hardware, the forecasting tools are very relevant in order to take full potential of the dr event. such aspects should be considered in the scheduling and remuneration phases of the method since it doesn't imply with the hil simulation model, only with the parameters of each simulation."
"in the first experiment, as done in many methods [cit], we did not considered semantically related terms, that is, we solely applied the first step of the proposed method. evaluation results are shown in table 1 . since the detection process of"
"the results shown in fig. 12 are the actual measurements from the generation resources, which op5600 acquired in real-time from different resources with different time intervals and merged into a unique profile. since this is a real aggregated generation profile, it can be used by a csp for several simulation purposes, such as remuneration processes or electricity market negotiations."
"the second part of the evaluation quantifies the amount of utility that the protected output preserves with regard to the input document, and compares our method with different redaction/sanitisation strategies."
"as future work, we plan to improve and extend the linguistic analysis of texts by incorporating other terms that may cause disclosure (e.g. verbs in sentences like \"he drinks too much\"). ontologies modelling verbs such as wordnet could be used to assist the sanitisation process. a deeper linguistic analysis may also contribute to improve the accuracy by detecting negated assertions (e.g. \"aids negative\") and thus avoiding unnecessary sanitisations. further tests will be also performed with sources written in different languages in order to illustrate the applicability of our method given the availability of linguistic parsing tools for such languages. finally, experiments with real medical data, which is the focus of our system, are also planned."
"reference [cit] ) introduced a co-simulation platform, called virtual integration laboratory (virgil) with hil devices in order to evaluate dr programs in a residential building located in denmark. this platform is able to control the ventilation system of the building, as well as to integrate power system simulation, communication, and control. in the case study, the authors surveyed the impacts of a dr program defined by the csp on the controlling of the ventilation system in the building using virgil. the results demonstrated the capabilities of the developed model, in which the ventilation systems can track the changes with 1-min time interval in order to perform the decisions at the certain time. however, they focused on a particular consumer on the csp network, and they do not describe how the csp perform the optimization and the day-ahead scheduling of the resources in order to define dr programs."
"unfortunately, as it will be discussed in section 2, methods available in the literature are limited and they only partially cover some of those features."
"official regulations have been developed at this respect within the medical context. for example, the health insurance portability and accountability act (hipaa) [cit], states safe harbour rules about the kind of personally identifiable information which should be removed in medical documents prior allowing their publication. more specifically, the hipaa requires 18 data elements (called phi:"
"we introduce here the pareto principle (or 80/20 rule) as the distribution of publications track cumulative distribution. the 80/20 rule of thumb states that ''roughly 80 % of the effects come from 20 % of the causes'' and it is a widespread in social science phenomena [cit] . [cit], de solla price proposed a cumulative advantage distribution model appropriated to probabilistic explain success-breeds-success distributions that are widespread in bibliometrics, such as the pareto and zipf distributions. in the delineation, we hypothesize that 80 % of the core set will be assigned to 20 % of the areas. to reach these relevant research areas, the steps below were carried out:"
"the consumers are 12 residential buildings, 6 commercial buildings, and 2 industrial units. also, the producers are all renewable resources including 22 pv systems and 4 wind generators. moreover, external suppliers are considered."
"since our interest was to analyze the whole field of nanocelulose research, these two cluster were considered as nuclei of research in nanocellulose. other clusters, in which the representation of the initial set was much lower, were considered peripheral research areas and their relevance to nanocellulose research was evaluated in the following section."
"to contextualise the hit count resulting from term queries performed to the wse, the generalisation will be attached to the term using a logic operator supported by the wse, such as and or +. we have applied this procedure to the queries evaluating the disclosure risk of t in t during"
"in the second test, the whole method was applied, which detects both sensitive terms and those that are semantically related. given that the detection criteria of the second step depends on how probabilities are computed, we performed two runs for each article: one using the basic probability calculus and another one with the contextualised version proposed in section 3.5 to minimise language ambiguity. evaluation results are depicted in table 2 . recall figures in this second experiment are noticeably higher than those of the first due to the evaluation of semantically related terms (93-96% vs. 13,4%, in average). this is especially relevant given that recall figures directly measure the degree of privacy achieved in the output. it is interesting that, in around half of the cases, the recall reached a 100%. this suggests that the outputs are perfectly valid in a real setting."
"at the bottom right of the map, electrospinning process, conductive polymers, and electro-active cellulose-based papers are positioned together. there is, however, no relevant citation connection between them. electrospinning is a technique used to produce micro-and nano-sized polymer-based fibers, and nanocellulose has been studied to improve the mechanical property of the final fiber [cit] . nanocellulose electrical and magnetic properties have also been explored to be used with conductive polymers and optically transparent electro-active papers [cit] . the other three research areas (cellulose dissolution, cellulose surface, and tempo mediated oxidation) are the smallest ones and their development must be monitored as they are associated to some issues concerning nanocellulose. for instance, tempo mediated oxidation is a current technique to perform pre-treatment of nanocellulose while there are high interests on cellulose surface research to develop new surface modifier to prevent aggregation from nanoscale cellulose particles [cit] ."
"in the next section we discuss the effect of cleaning up the core set of publications by using 'cleaning terms', i.e., terms to increase the accuracy of our initial set. moreover, we present a basic structure of the field on the basis of the delineation we developed and further analysis."
"precision, on the other hand, is lower than in the previous experiment (54-75% vs. 100%, in average). this is caused by the larger number of false positives that result from the imperfections of the automatic assessment and, especially, of the probability calculus. in fact, we observe noticeable differences between contextualised on non-contextualised versions of the probability calculus. in the former case, precision is noticeably higher (75,3% vs. 54,5%, in average), which suggests that:"
"in other words, the outcomes of the optimization process, which is the economic resource scheduling, is the rate of power that has been requested from each consumer and generator in the csp network. in order to implement these optimal scheduling results on the real resources, the conditions of the electrical network are important. the op5600 real-time simulator used in this paper enables us to address this fact; we can validate the actual amount of reduction in the consumers and actual generation of energy resources and obtain the actual measurement results in order to be employed in the remuneration phase."
"checking the research topics associated with nanocellulose can provide insights into current technical challenges concerning this nanomaterial, such as increasing the scale of production minimizing costs, characterization of sources and mechanical properties. surface modifications to reduce moisture adsorption and improve the adhesion between the nanomaterial and the polymeric matrix, thermal degradation, and biocompatibility with living tissues has also been target of research [cit] ."
"redaction is a privacy-preserving method that aims to avoid (or at least mitigate) the disclosure of raw confidential data, such as textual documents (in contrast with specific privacy protection methods focusing only on relational databases [cit] ). redaction is based on blacking-out, obscuring or eliminating sensitive terms in the documents prior to their release. selecting which elements of the document have to be redacted is crucial, because a weak redaction process may disclose sensitive data. on the other hand, a too restrictive redaction may destroy the utility of the document, a situation that goes against the purpose of data releasing."
"this paper presents an automatic method to sanitise textual medical data. several aspects differentiate it from related works. first, several knowledge bases are exploited to retain, as much as possible, the semantics and, thus, the analytical utility and readability of the protected output. second, on the contrary to methods focusing on specific types of sensitive entities (like e-mail addresses or social security numbers) [2, 3, 13, 15, 17-19, 25, 35, 36, 40, 41], our approach does not make any assumptions on the structure of the terms to protect. as a result, it in can be applied to textual contents regardless the fact that terms (e.g. diseases, symptoms, treatment, etc.) present or not a regular structure. finally, it carefully considers the disclosure risk caused by the presence of semantically related terms, which is especially critical in the medical context in which most terms appearing in a document are likely to be semantically related up to some degree."
"the real-time simulation model implemented for the csp is shown in fig. 2 . in this model, it is considered that there are 20 consumers and prosumers, and 26 producers."
"utility preservation is significantly lower when both sensitive and semantically related terms are protected (step-1 and 2). this suggests that a large percentage of terms appearing in the input document were indeed semantically related to the entities to be protected, which is coherent with the tight discourses that characterise wikipedia articles. these results are also coherent with the nearperfect recall reported in table 2, which suggests that most of the risky terms where properly identified and protected. thus, the observed differences in utility preservation quantify the cost derived from the protection of risky related terms, that is, the cost of the more robust privacy guarantees."
"in any case, document redaction/sanitisation is significantly hindered by the need to deal with textual data and because of the existence of semantic relationships between the textual terms in the document. the fact that many sensitive terms lack a regular structure (e.g. disease names, in contrast to e-mail addresses or social security numbers) limits the effectiveness of automatic methods based on pattern matching or trained classifiers [cit] . semantic relationships, on the other hand, may enable the re-identification of redacted/sanitised elements from the presence of related terms left in clear form."
"-the non-contextualised term probabilities resulted in a too strict sanitisation, that is, too many terms were unnecessarily sanitised, which is reflected in the lower precision. this was either because the baseline informativeness used as threshold was underestimated, or because the amount of information disclosure caused by related terms regarding the sensitive ones were overestimated. polysemy and ellipsis) had a positive contribution in enabling a more precise detection, even when causing a slightly decrease in recall (93,45% vs. 96,31%)."
"when these optimization results are obtained, they are provided to the op5600 real-time simulator as inputs. consequently, the real-time simulator starts to control and manage the hil equipment in order to implement the optimization results in real-time. the consumption profiles shown in fig. 7b will be emulated by the 4 kva and the 30 kw loads; the wind production curve shown in fig. 7a will be provided to the 1.2 kw wind turbine emulator; the pv profile in bus #10 will be modeled by the 1.8 kw pv emulator; and the pv curve in bus #5 will be the real profile of gecad pv production. the output of the optimal energy resource scheduling model is a requested amount of power for each resource to manage its demand or generation in a certain period. however, the actual implementation of the demand reduction and the dg production requested for each real resource will depend on the electrical grid conditions. in fact, this is one of the advantages of using real-time simulation (op5600) and laboratory equipment, as hil, for modeling consumption and generation profiles. with this method, we validate the actual demand reduction, and the actual dg production in order to be used in the simulation results, namely for remuneration and aggregation goals."
"regarding the 1.8 kw pv emulator located in bus #10, an arduino® has been utilized in the dc power supply in order to manage the output power between 0 and 100% of capacity. in fact, the dc power source simulates the pv arrays, which provides dc voltage, and the dc/ac inverter is a usual model that is utilized in real pv installations. the simulink model for controlling and monitoring the equipment in bus #10 is shown in fig. 5 . in this model, two groups of tcp/ip blocks have been employed, one for the dc power source (arduino®), and the other for the dc/ac inverter. by this way, the op5600 transmits the desired value of pv generation to an arduino via a modbus tcp/ip request and receives the real-time ac power generation from the dc/ac inverter."
"notice that, by definition, τ corresponds to the ic of the most general term in s, which is the one that imposes the stronger privacy constraint; this is because it defines the maximum amount of sensitive information that is allowed to be revealed in the sanitised output without incurring in disclosure. the more general the terms in s are (i.e. the lower their ic and, thus, τ), the stricter the sanitisation becomes because a lower degree of information disclosure is allowed."
"in the same manner as in the previous step, a threshold for dr values should be defined, both to detect which semantically related terms produce too much disclosure and, if that is the case, up to which level those should be protected (i.e. sanitised). given that the threshold τ (see eq. (3)) quantifies the baseline amount of information that should not be revealed about the entities to protect, any q that, in addition to g(t), reveals equal or more information than τ about any t will be considered as risky."
"this means that the presence of q in a document does not provide any particular evidence of t. on the other hand, positive pmi values reflect the amount of information of t disclosed by the presence of q."
"the general workflow of the proposed method is depicted in figure 1 . it consists of two preliminary steps (step-0a and step-0b) that take the input privacy requirements (e.g. current legislations) and automatically set the method parameters; these steps are expected to be executed only once, just previously to the first sanitisation process. then, two main steps (step-1 and step-2), which are executed for each document to sanitise, will be in charge of detecting and protecting sensitive terms and those that are semantically related, according to the input privacy requirements. these steps are explained in detail in the next subsections."
"in fig. 8, the set point values are the ones that op5600 transmitted to the 4 kva and to the 30 kw loads in order to simulate the consumption of the prosumer in bus #15. the green and blue lines are the responsibility of the loads in real-time. the dr event has been applied between the instant of 240 s (period #8) to 570 (period #19), which leads to reducing the consumption. as it can be seen in fig. 8, whenever the rate of consumption changed, the loads require some time to reach the desired consumption level. this is one of the main differences between the experimental works and simulation works; in simulation environment the consumption rate changes immediately, however, consumers modeled by hil devices employed in this platform require some time to meet the desired rate of consumption since it accommodates analog control in ac loads."
"in order to compute term probabilities from the web in an efficient manner, several authors [cit] have used the hit count returned by a web search engine (e.g. bing, google) when querying the term t. in our approach, term probabilities are computed in this way. where n is the number of web resources indexed by the web search engine."
"additionally, fig. 11 demonstrates the pv generation results of bus #5 in the csp network. the results shown in this figure are the real production data of gecad pv system with 10 kw capacity of generation. the considered generation profile is for an entire day with 1 min time interval (1440 periods of 1 min, for 24 h) available in gecad database, and the profile shown in fig. 11 is the same profile; however, op5600 acquires the data with 0.5 s time interval (1440 periods of 0.5 s, in total 720 s)."
"the goal of the proposed method is to automatically sanitise a textual medical document according to certain privacy requirements, which would be usually specified by current legislations on medical data privacy; even though, if needed, other sources of privacy requirements may be incorporated. this will be done in a way that (i) sensitive elements (defined by the privacy requirements), and also those apparently innocuous terms than can effectively re-identify the former by means of semantic inference, will be sanitised, and (ii) the sanitisation process will try to preserve the utility of the resulting document as much as possible. in the following, we call terms to (noun) phrases designating a concept (e.g. aids and acquired immune deficiency syndrome are terms referring to the same medical concept)."
"in the same way as in the previous section, pmi probabilities can be computed from the web hit count provided by a web search engine:"
"the large and detailed taxonomic structure of snomed-ct is especially suited for this purpose, because generalisation steps are fine grained, and this allows retrieving generalisations that accurately fit the sanitisation criterion (i.e. fulfilling the threshold but retaining maximum information)."
"-the number of false positives that are unnecessarily redacted/sanitised in step-2 is significantly lower when using contextualised probabilities, as suggested by the precision figures reported in table 2 . thus, the utility of the protected output will be noticeably higher thanks to the more accurate detection process."
"protected health information) to be removed from a redacted document. the goal of such regulation is to maintain individual's anonymity while preserving healthcare outcomes, which are useful for medical research, intact."
"given that terms in s define the privacy requirements of the sanitisation process, we assume that the amount of information (ic) they provide states the baseline amount of sensitive information that should not be revealed in the protected output in order to avoid disclosure [cit] . as it will be detailed in the following sections, this baseline will act as sanitisation threshold to decide up to which level sensitive terms should be generalised in the sanitised output to meet with the privacy requirements and, also, which semantically related terms are risky because they disclose too much information about a sensitive entity."
"obviously, the generalisation of sensitive terms preserves a larger amount of information than the straightforward term removal (97% vs. 91,5%, in average). this illustrates the advantages of the exploitation of a knowledge base to improve the utility of the protected output."
"left ax is right axis fig. 4 percentage of noise of core-nanocellulose publications and proportion between core-nanocellulose publications and total number of publications over research area. source: cwts web of science publication-level database between author and author e. the cleaning step did not affect the publications from author e. in the case of cluster 13.6.11, the ranking of authors stays constant after the cleaning step despite the decrease in the number of publications from the authors (except author b), notably for authors a and c. the cleaning words that most affect these authors were ''*cell wall*'', ''hemicellulose'', ''lignocellulos*'', ''cyto*'', ''lignin distribution'', ''gene'', fig. 6 overview of the number of nanocellulose-focused publication and number of clusters (research areas) in four moments of the delineation procedure proposed. source: compiled by the authors ''phenotype'', ''xylanase'' and ''xyloglucan''. a quick look on the excluded paper's titles from these authors indicated that most of excluded were non-noise publications. however, it is important to state these publications were only omitted from the process of retrieving the final set of clusters, despite the fact that they are present at further analysis because they are part of the cluster. besides the effects verified, the cleaning step did not affect the overall ranking of authors in both nuclei research areas. this fact supports the effectiveness of the delineation procedure used to retrieved nanocellulose research areas."
"the proposed method relies on the information theory and, in particular, on the notion of information content (ic) of textual terms to automatically guide the sanitisation process. the idea is to use the notion of ic to quantify the amount of sensitive information provided by any of the terms to protect."
"with the previous step, sensitive terms are analysed and detected independently, which is similar to what is done in many sanitisers [cit] . for example, when sanitising sexually transmitted diseases (std) from a document, the term chlamydia will be marked as sensitive because it is a specialisation of std (according to snomed-ct) whereas other terms that are not specialisations like sexual contact or genitals will remain in clear form. however, these last terms are highly related with std and they may enable its re-identification by means of semantic inference [cit] . in fact, most terms appearing in a discourse are semantically related up to a degree [cit] and, thus, they may negate sanitisation efforts in which terms are managed independently."
"nanocellulose is a generic term referring to cellulose nanofibrils and cellulose nanocrystals [cit] . the main difference between these two types of cellulose in nanoscale dimensions relies on the degree of crystallinity, which impacts on mechanical and functional properties of final products [cit] . cellulose nanocrystals are basically shorter and rod-like crystalline cellulose, whereas cellulose nanofibrils are long chains of alternate amorphous and crystalline cellulose. both types of nanocellulose can be obtained from renewable sources, including natural fibers, plants, pulp and forest and agricultural residues. in this case, mechanical process and chemical/enzymatic approaches are used to obtain cellulose nanofibrils and cellulose nanocrystals, respectively. moreover, cellulose nanocrystals can be biosynthesized by bacteria, resulting in the also called bacterial cellulose [cit] ."
"in case of overlap (i.e. a term which is contained in several knowledge bases), the strict order is i) snomed-ct, ii) wordnet and iii) odp, because the former provide a more detailed structuring of medical concepts. only in such cases in which q is not found in any knowledge base, it will be removed from the output."
"a second test verified the effect of the cleaning process on the coverage of key-authors (top 5) from each nuclei research area (13.6.3 and 13.6.11) in nanocellulose, as can be seen from tables 3 and 4. the top authors from cluster 13.6.3 are not the same top authors from the cluster 13.6.11 due to the fact that they simply did not overlap."
"it is worth to mention that, given that semantically related terms q may or may not correspond to medical concepts, general knowledge structures covering domains other than the medical one will be needed to retrieve generalisations. in our case, we use the taxonomies provided by wordnet [cit], an structured thesauri covering more than 100,000 concepts, and odp (open directory service), whose taxonomy structures web resources in more than 1,000,000 categories, in addition to snomed-ct."
"at the end, utility figures obtained by our complete method (step-1 and 2, with contextualised probabilities and term generalisations) are just a 17-23% lower than baseline approaches that do not consider semantic relationships (74% vs. 91,5-97%, in average), while providing much more robust privacy guarantees (93,5% vs. 13,4% of average term recall, as reported in tables 1 and 2 )."
"figures 8, 9, 10, 11 and 12 show the final results of the real-time simulation for 12 min. all the results illustrated in these figures are adapted from op5600 and matlab/simulink. in this model, the time step for real-time simulation is configured as 0.5 s. this means that op5600 transmits the optimization results to hil devices with 30 s time interval (one value for each period), and then, it acquires the real-time data with a 0.5 s time interval. the results shown in figs. 8, 9, 10, 11 and 12 are the behaviors and reactions of customers during the scheduled events; they accepted their availability for these events in the day before."
"as it can be seen in fig. 7a, all dg units are responsible to provide the maximum generation to the csp network since it is the cheapest resource of the grid. additionally, fig. 7b shows the load reduction in the scope of red. dr program, which csp utilized for prosumer in bus #15, for reducing the consumption in order to avoid purchasing energy from external suppliers."
"as fig. 2 shows, the csp network has been modeled in matlab/simulink in op5600. in this network, several real and laboratory equipment has been employed in order to emulate the consumption and generation profiles of the csp players via the hil methodology. bus #15 is considered as a prosumer, bus #10 is dedicated to a 1.8 kw pv emulator, and bus #5 includes a real 10 kw pv producer."
"encouraged to utilize dr programs for reducing their electricity bills, and the grid side will benefit from that by reducing congestion of the grid and lowering the consumption in the peak periods [cit] . dr programs are categorized into two main groups [cit] :"
"numerically, the ic of a term t is computed as the inverse of its probability of appearance, p(t), in a corpus. in this manner, general terms (e.g. disease) will be assumed to provide less information than specialised terms (e.g. chlamydia), because the former are more likely to be referred in a discourse."
"step-2. in this manner, the detection and sanitisation of related terms q will be less affected by the ambiguity associated to the evaluated t. thus, whenever a sensitive entity is queried to the wse to compute ic and pmi, its generalisation will be attached. since this generalisation is retrieved from snomed-ct, we can be sure that it will correspond to the appropriate meaning of the entity, thus enabling the desired disambiguation (e.g. the generalisation of crabs retrieved from snomed-ct will refer to a type of std). as stated above, this contextualisation is considered when querying t in the expression of dr (eq. (7)):"
"there are few research works concerning the real-time simulation of a csp model; this was a motivation for the present work. this paper provides a real-time simulation model for a csp by using several real and laboratory hardware equipment considered as hardware-in-the-loop (hil). the csp aggregated resources include consumers, producers, and prosumers who do not have adequate capacity of generation and dr reduction in order to participate in the wholesale market negotiation, therefore, they establish a contract with csp in order to be aggregated and managed by this third party. moreover, an optimization problem is developed in this paper in order to be used by csp for optimal resources scheduling, which aims at minimizing its operation costs. the hil equipment employed for csp resources contain two small and medium scale laboratory loads, laboratory wind turbine and photovoltaic (pv) emulators, and a real pv producer, which are controlled and managed by a real-time simulation machine (op5600) through matlab/simulink environment. in this way, the main purpose of the paper is to demonstrate that the hybrid simulation platform is also capable to implement optimal scheduling results and dr programs."
"in order to understand the difference between the two nuclei research areas, a map of noun-phrases based on text corpus (titles and abstracts) was created. the map compares the topics that characterize each nuclei cluster and provide themes that they share. [cit], 2015) ."
"the proposed delineation procedure enabled us to retrieve relevant publications from research areas involving nanocellulose. seventeen research topics were identified, mapped and associated with current research challenges on nanocellulose. two of them were highlighted as nuclei since they contain most part of the initial set of publications. the effect of the cleaning step on nuclei and peripheral clusters provided valuable feedback and demonstrated its importance to establishing relevant clusters afterwards. the independency test showed that the cleaning procedure did affect the number of publication from most of the top author. however, the author's ranking did not change significantly. in future research, we intend to develop new research aiming at excluding the manual checking on the peripheral clusters and include a procedure more automated."
"nanocelluloses are cellulose-based nanomaterials, which are sustainable and has a great potential for innovation [cit] . nanocelluloses have been a research area for many countries, including the major producers of cellulose worldwide, such as the usa, canada, finland, sweden and brazil [cit] . different disciplines are involved with nanocellulose research since its properties and behavior have allowed applications as reinforcement agent in composite materials, packing material, optically transparent paper for electronic devices, texturizing agent in cosmetics and food, bio-artificial implants and bandages [cit] ."
"to achieve those goals, the proposed method builds on an information theoretic characterisation of term sensitiveness and disclosure risk [cit] and an accurate calculus of term probabilities from the web. the fact that the privacy criterion can be defined by simply listing the set of entities that should be considered as sensitive makes our approach intuitive (from the perspective of the privacy guarantees that one may expect from the protected output), and especially suitable to be applied in coherency with legislations on medical data privacy, which are specified in the same manner."
"the main side effect of document redaction is that it significantly reduces the utility of the protected content [cit] . another important drawback is that the existence of obscured or blacked-out parts can raise the awareness of the document's sensitivity in front of potential attackers [cit] . this is especially problematic in documents linked to a specific knowledge area such as healthcare, because the number of different textual elements that usually appear on the documents is relatively limited. therefore, leaving blacked-out parts increases the probabilities for an attacker to determine the redacted terms by means of some characteristics such as their context, their length, etc. [cit] . a more suitable alternative to document redaction consists on generalising sensitive content instead of removing it, a measure that preserves more content utility [cit] . this is usually referred to as document sanitisation [cit] . as a result of content generalisation a less detailed but still useful document is obtained, while no explicit clues about the document's sensitivity are given."
"precision quantifies the proportion of sensitive terms identified by our method that have also been identified by the human experts. the lower the number of unnecessary detected terms is, the higher the precision is and, thus, the better the utility of the protected output will be because a lower amount of terms will be unnecessarily redacted or sanitised."
"recall quantifies the proportion of sensitive terms identified by the human experts that our method has been also able to identify. thus, the higher the recall is, the higher the privacy of the output will be. in document redaction/sanitisation, recall usually plays a more important role than precision because a low recall implies disclosing data that may negate the whole sanitisation [cit] ."
"to measure the degree of utility preservation we first quantify the utility of a document as the sum of the amount of information provided by all the terms w appearing in a document d (i.e., noun phrases detected by means of the syntactic parsing, regardless of being latterly identified as sensitive or not)."
"in total, the csp can perform resource scheduling and aggregation processes considering dg units, external suppliers, and dr resources. in this context, the csp should utilize an optimization problem in order to optimally manage the resources, which will be demonstrated in the next sub-section."
1. the research areas were listed in descending order of the total number of publications from the core-nanocellulose; 2. research topics with one or two publications from the core-nanocellulose were excluded. this yields 86 research areas; 3. the first 17 clusters were selected corresponding to 20 % from 86 clusters.
"in this section, the real-time simulation model considered for the csp is presented. several real and laboratory hardware equipment is employed in order to simulate the model in real-time considering the hil methodology."
"the proposed method has been implemented in java and run over an intel core2 quad 2.66ghz with 4gb ram, windows 7 and a 100mb internet connection. the average runtime for the sanitisation of the six wikipedia sources was 16.7 minutes. note that most of the runtime is devoted to perform queries to web search engines and to wait for the results, but this waiting periods are highly parallelisable."
"this study aims at proposing a delineation procedure to retrieve relevant research areas addressed to a specific topic. nanocellulose was selected as a case, but it may be used for other subjects, of course. [cit] . this paper is structured as follows. in the next section, we describe the overall delineating procedure and its general issues. next, we discuss details concerning specific parts and tasks. we present and discuss results in third section and finally fourth section draw our conclusions."
"-as a result, the global accuracy (i.e. f-measure) of the detection process with contextualised probabilities significantly surpasses that of its non-contextualised version (83% vs. 69,3%, in average). this suggests that contextualised probabilities better capture the informativeness and information disclosure of sensitive entities and related terms, respectively."
"the objective function presented for csp to minimize his operation cost (oc) is shown by eq. (1). in this model, it is considered that technical verification of network is"
"delineating scientific fields is a complex task as boundaries are not frequently well established since scientific studies have become more complex and interdisciplinary. more and more exchange of knowledge between scientists from different disciplines is involved. our approach retrieves and delineates the real nuclei and the peripheral research areas concerning nanocellulose studies. this clear separation provides suggestions for further research, putting the nuclei research in context. we already check in this paper the share of topics between the nuclei research areas, but we also intend to understand the knowledge flow from peripheral research topics to the nuclei areas. another idea involves the mapping of how the peripheral areas provide the necessary knowledge to face nanocellulose current challenges, and how countries and scientific institutions are contributing to this evolution. the investigation will evaluate the added value of peripheral clusters to the nuclei and what type of analyses they are necessary and in which cases they can be ignored."
"in sum, the simulink models implemented in the real-time simulator (op5600) have been demonstrated in this section. all simulink models have been designed by relying on the hil methodology in order to control real hardware resources and utilize real data in simulink."
"noun-phrases were obtained with support of vosviewer corpus map analysis applied to titles and abstracts from publications belonging to these clusters, which were processed separately. the cleaning-terms extraction took into account only high frequency terms from clusters contents. however, we highlight that not every highly frequent words found were used. for instance, ''ethanol'' (alone) is one of the most frequently word from cluster 16.3.2, but it was not incorporated on the set of cleaning terms from table 2 due to the fact that nanocellulose can be a sub-product of ethanol produced from cellulose enzymatic fermentation-also called, second generation ethanol [cit] . thus, excluding publications containing ''ethanol'' would have an undesired effect on the final set of nanocellulose publications. this not happened with the word ''ethanol yield'', though. in order to avoid such undesired effects, we studied the effect of each potential cleaning-term on the nuclei clusters before selecting the final ones. table 2 presents the final terms used to clean the nanocellulose-focused publications retrieved using the search expression from table 1 . they were applied on the title, abstract, author's keyword and keyword plus search field. the effect of this cleaning task on the nuclei clusters and the peripheral clusters we used will be discussed in the results."
"in other scenarios, such as when medical records are released to insurance companies or legal counsel [cit] to be used as a support for legal claims (e.g. workers' compensation claims, motor vehicle accident claims, etc.), privacy protection regulations are focused towards ensuring the confidentiality of individual's data. in those cases, documents have to be still linked to a patient but any information that may impair her dignity and/or be the cause of discrimination must be removed. regarding this last point, many us state and federal laws stipulate that elements such as hiv status, drug or alcohol abuse and mental health conditions must be redacted before releasing medical records to third parties [cit] ."
"as a result of this process, the final sanitised document is obtained in which both sensitive terms and those semantically related ones that were found to be risky are sanitised."
"the evaluation showed that: i) the analysis of semantically related terms, ii) the contextualisation of probability queries and iii) the replacement (instead of removal) of sensitive terms by appropriate generalisations improved the detection recall of sensitive information (i.e. the practical privacy of the output) while contributing to preserve the output's utility."
"a map containing the seventeen selected research topics was created with support of vosviewer software [cit] . the map positions the research topics on the basis of their citation relations, i. e the closer two topics, the more frequent the citation traffic between them. the node labels also match the main content of the clusters."
"overall, research topic 13.6.11 had its core-nanocellulose publication reduced in 17.5 % while the decrease to cluster 13.6.3 was 10.2 %. nonetheless, both clusters still 0,0 1,0 2,0 3,0 4,0 5,0 6,0 \"*cell wall*\" \"*cyto*\" \"*lignin distribution*\" \"*plant growth*\" \"delignification\" \"ethanol yield\" \"gene\" \"glucosidase\" \"hem icellulose\""
"13.6.3 13.6.11 concentrated publication from the core-nanocellulose after the cleaning tasks (the proportion was 74.0 % to research area 13.6.3 and 72.1 % to 13.6.11). therefore, they still had the status of nuclei research areas. to the 20 peripheral research topics whose nanocellulose set of publication were evaluated, no direct correlation was observed between the proportional relevance of each clusters and the percentage of noise, according to fig. 4 . four research topics had a high percentage ([70 %) of 'noisy' publications mainly focusing on biological issues of plants, ethanol production, and enzymes aspects, not having the nanomaterial as a final object of research. since these four were used to select the cleaning terms, the cleaning affected them highly. two of them were even eliminated (5.1.21 and 13.19.17). furthermore, other peripheral clusters had their nanocellulose publication coverage diminished, as shown on fig. 5 . figure 6 provides an overview of the number of nanocellulose-focused publication and number of clusters (research areas) in four moments of the delineation procedure proposed. considering the only first and the last moment, the number of nanocellulose-focused publication diminished 52.6 % while the number of clusters decreased 96.8 %. nanocellulose-focused publications were more affected between moments 1 and 2, when the value reduced 30.1 %. in contrast, the number of clusters diminished 79.9 and 80.2 % respectively among the others moments."
", which corresponds to the whole grey area in figure 3 . moreover, in the same manner as given the above discussion, the final dr expression that considers the fact that sensitive terms t are replaced by generalisations g(t) and also the presence of semantically related terms q is the following:"
"the csp is able to perform the resources scheduling considering external suppliers, dg units (especially renewable producers and surplus of prosumer generation), and also dr programs. for this purpose, the customers that intend to participate in dr events can establish a contract with the csp in three different programs: direct load control (dlc), load reduction (red.), and real-time pricing (rtp). the characteristics of these dr programs are shown in table 1 . the load shifting program is an fig. 1 the overall architecture of csp model important tool for the csp since it can manage the consumption and shift it based on the available rate of generation. a load shifting model has been developed by the authors in the scope of their previous work, and since the purpose is to show that the developed model can also implement dr programs, three different programs have been presented in this paper and load shifting is not considered."
"the final set of nanocellulose publication comprised 2600 nanocellulose publications (named now as core-nanocellulose) and they were assigned to 428 research areas, which still would be a highly number of cluster to be evaluated. in fact, 81.0 % of these clusters included only one or two publications from the core-nanocellulose publication, which raised questions about their actual relevance to the advances on nanocelulose studies: the maximum proportions are less than 3.3 % and, in the cases were the absolute numbers are high, the proportion is less than 0.2 %. therefore, a selecting step was introduced."
"in sum, the mathematical formulation of the csp model for resource scheduling with the objective of minimizing the operation costs, are shown in this section. in the next section, this formulation will be implemented in the real-time simulation model considered for the csp."
"the use of demand response programs and distributed energy resources, especially renewable generation, are a key role of nowadays distribution network. moreover, in order to have an efficient solution for these resources management, third-party entities, namely curtailment service providers, are very relevant in this scope. the model presented in this paper concerns the real-time simulation of a curtailment service provider by utilizing several real and laboratory hardware resources. all the equipment presented in this paper has been employed in the real-time simulator as hardware-in-the-loop in order to take advantage of realistic results inside of the simulation environment. moreover, an optimization problem is developed in this paper, which enables the curtailment service provider to have an optimal solution for the resource scheduling with the aim of minimizing the operation costs."
"the main focus of the present paper is to implement a real-time simulation platform for an optimization-based csp model considering several real and laboratory hil infrastructures, which are controlled based on the optimal resource scheduling of a csp and dr programs implementation. the hil equipment enables the system to validate the developed methodology by using real data and enables actual measurements and control of hardware devices that are outside the simulation environment. the scientific contribution of this paper is to address an optimization based csp model to the small and medium scale consumers and producers by employing a hybrid simulation platform, including real systems, laboratory emulators, and mathematical models."
"in sum, the results regarding the real-time simulation model of the csp are illustrated in this section. these results prove the platform skills in order to validate a business model for optimal resource scheduling using several real and laboratory hardware resources and the real-time simulator."
"step-2 depends on the probability calculus used to detect and generalise terms, we include the results with the basic probability assessment and with the contextualised version proposed in section 3.5. table 3 . utility preservation according to the different redaction/sanitisation strategies."
"bibliometrics has been used to quantitatively assess scientific fields within the context of science policy and research management as well as a useful instrument to study and gain insight, in complex fields or research areas. meanwhile, scientific studies have become more complex and interdisciplinary, involving the exchange of knowledge between scientists from different disciplines in specific subject of research. furthermore, bibliometrics has been dealing with an increasing huge volume of scientific data available on databases, which might affect negatively the precision on information retrieving. nanotechnologyfocused research is a good example. many studies aimed at collecting publications and patents using, for example, key-word strategies [cit], journal-level strategies [cit], patent classification-level strategies [cit] and a mixes of approaches [cit] . [cit] conducted an intensive comparison of the methodologies used to retrieve nanotechnology literature and found that most of the lexical queries produced similar rankings for the top nanotechnology subject areas, the top journals, and the most prolific countries and institutions. the reason of the similarity would be attributed to the core set of keywords shared by these search strategies. nonetheless, the problem often is: how to delineate a field or research area; how to retrieve the relevant data; which publications to include and which not; what insights can be obtained from the set of publication retrieved [cit] ."
"as it can be seen in fig. 3, two constant blocks in simulink indicate the desired consumption to be consumed by the 30 kw and the 4 kva loads. in the case of 30 kw load, the output of constant block will be divided into the four binary outputs through a comparator algorithm in order to be provided to the digital output board. in the case of 4 kva load, the output of constant block will be converted to modbus tcp/ ip format with ieee 754 standard, which is four hexadecimal numbers. furthermore, there is an energy meter for these two loads, which measures the consumed active power and transmit it to op5600 in real-time via modus tcp/ip. by this way, op5600 is able to transmit the favorable amount of power to the loads and simultaneously receives the real-time consumption of them. the simulink model regarding the wind turbine emulator is shown in fig. 4 . in this model, since the wind turbine is controlled by analog output of op5600, the wind speed data should be converted to a 0 to + 10 v signal. the output of a constant block, which is favorable wind speed value, is converted to 0 to + 10 v voltage range through two developed algorithms implemented in two matlab function blocks, and therefore, op5600 controls the emulator based on the provided voltage. simultaneously, an energy meter measures the generation of the emulator and transmit it to the op5600 in real-time via modbus tcp/ip. the tcp/ip configurations blocks regarding the energy meter are not shown in fig. 4 since these are similar to the ones in fig. 3 ."
"as explained in the introduction, depending on the privacy requirements, protection of healthcare documents may pursue two different goals: (i) preserve the individual's anonymity; or (ii) ensure the confidentiality of her data. for the first case, the hipaa rules [cit] specify which identifying elements should not appear in any protected medical document in order to avoid re-identification. regarding the second case, governmental legislations stipulate [cit] which sensitive elements should be masked from any medical record (to avoid potential discrimination) before releasing it to a third party."
"step-1 just looks for sensitive terms and for their taxonomic specialisations (retrieved from snomed-ct), a large number of risky terms that are semantically related (other than taxonomically) with the entities to protect are left in clear. indeed, since wikipedia articles usually present a tight focus on the described entity, most of the terms they contain are semantically related to the entity to protect [cit] . these results illustrate the importance of considering semantic relationships in document redaction/sanitisation and how an independent evaluation of terms is usually not enough to achieve enough protection in front of semantic inferences [cit] . on the other hand, given that terms detected as sensitive are extracted from snomed-ct unambiguously, precision is perfect in all cases."
"the simulink model considered for real 10 kw pv producer in bus #5 is the same as the model presented in fig. 5, however, the difference is that there is no dc power source, and therefore, op5600 has no control over the pv generation and is only able to monitor the real-time ac power generation of the unit. this has been implemented via a group of tcp/ip blocks that request the real-time ac power generation from the dc/ac inverter."
"after performing the delineation, an independency test was conducted to verify how the steps from process affected the number of publication from the main authors in nanocellulose. we also had a look on the arrangement of the selected research topics on a map and provided an initial discussion of how an analysis of these cluster can support nanocellulose research management."
"we do not claim that our selecting procedure is perfect, but a quick analysis of the chosen research topics showed themes currently found in nanocellulose literature."
"once the checking process was completed, specific terms were identified and considered to clean the initial set of nanocelulose publications. only research topics with high percentage of ''noise publication'' were used. this task aimed to increase the precision of information retrieving as explained before."
"in this paper, we describe an automatic scheme designed to sanitise textual medical documents (e.g. electronic healthcare records) without assuming any kind of structural regularity in the entities to protect. moreover, it puts special emphasis in the detection and sanitisation of terms that are semantically related to sensitive entities, in order to avoid disclosure via semantic inference. the present work offers the following contributions:"
"the characterisation of disclosure risk presented in eq. (5) assumes that all the information that an attacker may gather about t is given by q, that is, t is assumed to be removed in the output document."
"first, we evaluated the detection accuracy of sensitive and related terms. to do so, two human experts were requested to manually sanitise each wikipedia article with the aim of detecting terms that, under their opinion, may help to disclose any of the entities stated as sensitive (i.e. std, hiv, aids, mental disorder, alcohol abuse and substance abuse). the initial inter-agreement was 0.88; however, they were requested to agree on the differences to obtain consensual results to which evaluate our method."
"to the ranking of authors from cluster 13.6.3, the number of publications decreases more significantly to authors b, c and d and a change on the ranking position occurred 5.1.4 13.19.17 13.6.2 13.19.3 13.6.1 9.13.4 13.6.33 13.6.43 13.6.7 18.5.4 13.6.17 13.6.22 13.6.12 13.6.9 13.6.31 13.6.29 1.33.9 13.18.1 13.11.41 percentage proportion"
"in this section, a case study tests and validates the developed simulink models in op5600. for this purpose, 24 periods of 30 s (12 min in total) are considered for running the model in real-time and obtain the results. the 12 min period, which is a rather short one, has been selected in order to provide deeper focus on the results analysis. longer periods can be easily implemented using this hybrid platform. the consumption and generation profiles considered for the day-ahead scheduling of the csp are shown in fig. 6 ."
"to tackle this problem, we incorporate the final sanitisation step (step-2), in which terms that were not found to be sensitive in the previous step, but that may disclose any of the already detected ones, are also protected. in order to achieve this, we rely on an information theoretic estimation of the risk of disclosing sensitive terms [cit] due to the presence, in the same document, of semantically related ones, which can be strictly medical or not. hereinafter we refer to each of these related terms as q."
"as it can be seen in fig. 9, the set points are the scheduled values that have been requested from the wind turbine to be emulated. consequently, the emulator produces power and transmits the actual measurements of active power generation (green line in fig. 9 ) to the op5600. by this way, the system is able to emulate a scheduled wind generation profile based on the electrical grid conditions, such as voltage variations. figure 10 shows the real-time results of the pv emulator considered in bus #10. as it is clear in fig. 10, the set points are the scheduled amount of power to be generated by pv emulator, and the simulated pv profile (blue line) is the real generated profile by the pv emulator, which has been transmitted to op5600 in real-time. also, as fig. 10 illustrates, there are a lot of variations in generation curve, which is due to the voltage variations in the ac side of the inverter. in this condition, the controller section of the dc power supply attempted to keep the generation level on the desired generation level."
"a total of 20 (peripheral) clusters were evaluated. the analysis regarded only articles from the initial dataset of nanocellulose publications assigned to these clusters, i.e. we checked whether the publications that contained one term from table 1 are in fact presenting outcomes of nanocellulose researches. the task involved reading their title and deciding if the article was a study focused upon nanocellulose or not. when the information was not clear based on the title, the abstract was also consulted."
"regarding the equipment connected to bus #15 (prosumer), in a 30 kw load, there are four relays that increase or decrease the rated consumption, and in a 4 kva load, there is an arduino® (www.arduino.cc), which manages the amount of consumption. the relays in 30 kw load are connected to digital output board of op5600, and arduino® has been connected to op5600 via ethernet interface, with modbus tcp/ip protocol. also, in wind turbine emulator, there is an induction motor coupled with the generator, in which the motor emulates the wind turbine. the motor has a speed controller unit, which manages the speed of the wind, and therefore, the output power generation of the machine. the speed controller unit of this machine is connected to the analog output of op5600. more information about these resources is available in fig. 2 real and laboratory hil equipment in matlab/simulink csp model [cit] . figures 3 and 4 present the simulink models, implemented in op5600 for controlling these resources via hil."
"as fig. 6a illustrates, in some periods the csp is able to manage the consumption relying on local dg and dr resources, however, in other periods csp is forced to purchase energy from external suppliers. the aggregated profiles shown in fig. 6b, are related to the prosumer in bus #15, and to two pv producers in bus #10 and #5, which are real data of gecad research center in porto, portugal, adapted from gecad database. the profiles shown in fig. 6 are day-ahead profiles, which enable the csp to manage and schedule the consumption and the generation of the network for the next day. by this way, the customers will be notified of the scheduled events for next day, and therefore, they have adequate time in order to negotiate and response their availability for the next day events."
"the list of sensitive terms will include the different synonyms and lexicalisations provided in snomed-ct for each entity to protect (e.g. for stds: sexually transmitted disease, venereal disease, vd, etc.) and also all of its taxonomic specialisations, which inherit the semantics of their -sensitiveancestors (e.g. for stds: gonorrhoea, syphilis, chlamydia, etc.). hereinafter, we refer to the resulting list of sensitive terms as s. notice that we do not explicitly consider the post-coordinated expressions that can be created from snomed-ct concepts to refer to other more complex concepts. in such cases, we would only refer to the parts of the expression that are considered as sensitive by the privacy criterion."
"on the other hand, the theoretical decoding, sw-qam, requires several complementary methods that overcome a specific type of problem. table 5 summarizes four important requirements of the theoretical and neural decoding schemes: data modeling, led localization, psf modeling, and unknown parameters. absolutely, the theoretical decoding scheme requires a small model for a psf image, whereas the neural decoding scheme requires a large image dataset for training. as expected, the neural decoding scheme has advantages over the theoretical decoding scheme in led localization, psf modeling, and unknown parameters. in other words, led points, psf images, and unknown parameters are automatically localized, modeled, and estimated, respectively, by our proposed cnn architectures."
"another time-driven method is antclust [cit] . in this method the ch nodes are selected at the beginning of each round, as in previous approaches. to improve the network lifetime antclust selects the new chs according to battery level and distances to neighboring sensor nodes. a specific feature of this method is the application of the clustering algorithm based on a model of the chemical recognition system of ants."
"on the other hand, the neural decoding method can accommodate all of the unknown parameters included the non-linearity through its learning process as generally shown in the result of neural decoding methods. therefore, we just need to optimize the objective function and then the unknown parameters will be estimated automatically."
"neural network (nn) is an efficient machine that is able to be applied to solve a variety of problems such as prediction, recognition, and fitting tasks. moreover, it is also powerful in estimating unknown parameters. due to nn's capability in a variety of tasks, we can take the advantage of nn on light interference elimination, noise removal, and correct decoding in vlc with a camera receiver, especially when working with an sw-qam modulation scheme [cit] . the problem of using a camera as a receiver in vlc is no different from that in computer vision. not surprisingly, some problems in vlc with a camera can be solved by a computer vision method."
"another advantage of neural decoding is its use of an stn layer for the automatic transformation of input images. the stn layer can help overcome a moving camera problem that usually occurred in a real world situation. on the other hand, if we used the theoretical decoding, we would be required to use an essential technique for detecting the led position in the image area. in our previous sw-qam scheme, manual detection was used because the led position was fixed. furthermore, if we use a wiener filter to improve the decoding function, the psf might be changed due to the camera moving in 3-dimensional space. this is the reason why stn, one of the neural network layer types, is applied to overcome the problem of a moving object in the image, for the unfixed led position. the stn is also trainable through a backpropagation algorithm. the input to the stn is a single frame, then 6 outputs from 6 input frames are concatenated to it and then fed into the next layer as illustrated in fig. 2(b) . in other words, it is the same as the architecture in fig. 2(a) with an additional preceded stn layer to its input layer."
the available ch rotation algorithms can be divided into two categories. the first category includes time-driven algorithms. according to these algorithms the sensor nodes take the role of ch for a predetermined time period. the second group consists of energy-driven algorithms that take into account residual energy of sensor nodes and change ch node when a predetermined portion of the energy is consumed.
"where emh denotes energy consumed by the sensor node (cluster member) during a cycle when an event is detected and data transmission is performed, eml is energy consumption of sensor node during a cycle when no event is detected and transmission is suppressed. the event occurrence probability p i is evaluated based on historical data. the fractions of time when particular sensor nodes take the ch role (t i ) must be carefully selected to maximize the network lifetime. according to the proposed method, the time fractions t i are determined by solving the following system of linear equations to ensure the same expected lifetime for all sensor nodes:"
in the literature several methods have been proposed to extend lifetime of wsns by properly managing ch nodes. this section briefly reviews the state-of-the-art methods for ch selection in wsns and discusses the main contribution of the paper.
"the objective of the experiments was to measure the energy consumption of sensor nodes and to estimate the lifetime of wsn. during experiments the lifetime of wsn with smart nodes was compared for the proposed ch rotation method and the state-of-the-art approaches, including time-driven and energy-driven ch rotation algorithms."
"applications of deep learning to communication research have been proposed in various works, such as channel modeling, equalization, decoding, demodulation or prediction [cit] . for instances, o'shea and hoydis [cit] introduced a channel modeling that provides an end-to-end learning framework for encoding and decoding processes in a physical layer. the whole communication system (from the transmitter to receiver) is modeled based on an autoencoder neural network architecture such that all the modulation and demodulation techniques are automatically optimized by training algorithms. moreover, they also developed another architecture called radio transformer network (rtn) derived from stn to improve the performance of their autoencoder. the rtn defines a parametric transformation of a symbol at the receiver as a correction for the autoencoder's error so that the performance of the entire system can be improved. o'shea's work demonstrates that deep learning is applicable to communication systems and that deep learning is robust to various unknown parameters such as channel imperfection and non-linearity [cit] ."
"the comparison presented in figure 18 additionally includes the basic versions of rrch, edcr, and fdcr algorithms without suppression of unnecessary transmissions. it can be observed that the suppression approach used by smart sensor nodes greatly contributes to the lifetime extension. at the same time, the proposed method allows us to take full advantage of the smart sensor node capabilities to suppress unnecessary transmissions for prolonging the wsn lifetime."
"during initialization, the wait time, i.e., time when sensor node waits for the sync frame, is different for each node. the wait time (sync wait time) depends on address of the node (see line 2 in algorithm 4). this prevents the situation when many neighboring sensor nodes take the ch role at the same time. an example of wsn cluster initialization is presented in figure 2 . in this example sensor nodes 1 and 2 are initialized at the same time and want to join the wsn cluster. initially, the ch node is not present, thus node 1 does not receive sync frame during its wait time, and becomes the ch. as node 1 takes the ch role, it starts broadcasting sync frames. one of such frames is received by node 2, which then join the cluster as cluster member. similarly, the 3rd sensor node, which starts its operation later than the other nodes, acts as cluster member after receiving the sync frame."
"here, we propose a novel decoding method based on our sw-qam scheme but using a neural network instead. this decoding method is called neural decoding. (hereinafter, sw-qam will be called a theoretical decoding.) by using neural decoding, we are not only able to reduce light interference and estimate other unknown parameters efficiently, but also able to decode the encoded symbols in a single stage. furthermore, in a real world situation, the camera may be moving while capturing a frame that may cause the positions of led points not to be fixed in the image area. with the modularity of the neural network model, the moving camera problem can be solved by adding a spatial transformer network (stn) [cit] layer into the neural decoder."
"in this paper, the great potential of cnn is also applied to visible light communication (vlc) research especially for the square wave quadrature amplitude modulation (sw-qam) scheme [cit] . in image-sensor-based vlc, an image sensor or a camera plays the role of a receiver that captures a sequence of led images and then decodes the captured images back to a sequence of symbols as described in sw-qam. the role of the image sensor can be viewed as a visual recognition task. therefore, cnn can be a good solution for a fully automatic sw-qam decoder. the sw-qam decoder requires three important techniques to make it works properly: (i) detecting the center of the leds, (ii) removing the light interference among leds for multiple led setting, and (iii) decoding the encoded symbols from the captured images. these difficulties and complexities can be eliminated by replacing the sw-qam decoder with the neural decoder. in this paper, we focus on the use of multiple leds due to its high complexity. absolutely, this scheme still works for a single led."
"to make sw-qam optimal, we need to optimize all of the complementary methods. in terms of rmse, m-densenet121 and m-resnet18 were the first and second best methods with spread values of 0.0239 and 0.0288, respectively. here, minidensenet was the third one. fig. 6 shows the decoded symbol spread which relates to the rmse value for all test methods. evidently, m-densenet121 provided a good precision, although its accuracy was less than those of minidensenet and m-resnet18."
"from the performance evaluation in four different situation datasets, i.e., fixed led dark-, fixed led bright-, unfixed led dark-, unfixed led bright-lighting datasets, the minidensenet was the most promising model with high accuracy and the lowest complexity. in this case, we had four separately different models for each situation. here, we trained a single model that can accommodate all situations using minidensenet architecture. we used a minidensenet with an stn layer because the datasets included the unfixed led images."
"however, the deeper model may lead to more complexity; the model requires a larger number of parameters and computations. also, it may lead to the overfitting problem; it fits well on training data, but not on testing data. therefore, there is a trade-off between the lowest mse model and the model complexity. however, in our case, minidensenet had lower complexity and more accurate in terms of a very low error rate, although its rmse was slightly higher than m-densenet121."
"for the dark lighting condition, sw-qam and minidense-net were the best performed methods without any misclassified symbol, error rate of 0%. m-densenet121 was the second best with a very small error of 0.02%. the other three neural decoding methods were comparable; m-vgg16bn and m-resnet18 result error rates of 1.04% and 1.49%, respectively, while vgg-like was the worst one with a little high error rate of 2.79%. in terms of rmse, m-densenet121 was the best performed method with rmse of 0.0038 that is around 10 times smaller than other methods. here, the value of rmse tells us how the average spread distance of decoded symbols is far from the reference point. a lower rmse means the less spread of decoded symbols away from the reference point. clearly, m-densenet121 had a very small deviation as shown in fig. 5(d) . generally, a lower rmse leads to a lower error rate, but it is not always true. the accuracy of the model does not always depend on the rmse value, but the precision does. for instance, m-densenet121 yielded more precision, a very small value of rmse, than sw-qam and minidensenet, but it provided a bit error, a misclassified symbol. this happened when we look at fig. 5(d), there was a misclassified symbol, denoted by an orange cross, at the grid of (0.7, 0.7). in this case, m-densenet121 was sacrificing one symbol while keeping the rmse sufficiently low. in summary, fig. 5 shows how the decoded symbols spread around the reference points. if we look at the spread of symbols, m-resnet18 was more precise than m-vgg16bn, but its accuracy was a little lower."
"additionally, figure 5 shows changes of energy consumption as a function of the probability of event occurrence. when analyzing these results, it should be noted that for cluster member the energy consumption increases with the probability of event occurrence. the reason behind this dependency is that the smart sensor nodes suppress transmissions to ch if no important event is detected. thus, if the event probability is higher, the cluster member must perform the transmission more frequently. in contrast, ch member reports the aggregated results to the sink at each step of the wsn operation and its energy consumption does not change significantly with the event probability. the network lifetime can be determined with the different assumptions described in the previous section. due to the nature of the detected data, which is implemented in a relatively small area, the network lifetime is defined in this study as the time from the start of all nodes to the discharge of one of them. this is due to the exponential drop in detection quality after any of the nodes are switched off. battery discharge is indicated by the blinking led that was attached to the microcontroller module."
"the paper is organized as follows. related works are surveyed in sections 2 and 3 includes presentation of the proposed algorithm, which enables extending lifetime of the sensor network. experiments and their results are described in section 4. finally, conclusions are given in section 5."
"in the algorithm configuration, we used pytorch [cit] and numpy [cit] libraries for implementing cnns based on our sw-qam scheme. all cnns were trained with our datasets and their minibatch and epoch were set to 32 and 200, respectively. we also used adam algorithm [cit] with a small initial learning rate of 10 −4, due to its fast convergence time and good performance in our pre-experimental trials. we saved the best model that has the lowest error rate based on the evaluation of the validation dataset among 200 epochs and then test it on the testing dataset. note that the validation and testing datasets were excluded from the training scheme of updating model parameters."
"we encode each symbol in a 64-qam constellation scheme, with the maximum value of 0.7 in the x and y axes, such that the first symbol is located at (−0.7, 0.7) and the preamble symbol is located at (−1, 0) as shown in fig. 3 . the outputs of the neural decoding are two regression values for the x-y coordinates of a decoded symbol in the 64-qam constellation. moreover, when we use n leds as a transmitter, the number of neural network outputs are n pairs of x-y coordinates. in our proposed scheme, the outputs of the neural network are four nodes, i.e., two pairs of x-y coordinates since we use two leds as the transmitter. this type of output is selected for our work so that it can be easy to visually compare them with those from the theoretical decoding method. however, in our preliminary experiment, we found that this regression output type yields better performance than a 64-symbol or 6-bit classification output type. therefore, the regression loss function or mean squared error (mse) is chosen for all of the neural decoding training phases to estimate the x-y coordinates in the 64-qam constellation."
"in this study, the term lifetime refers to the time interval of the wsn operation before death of the first node. it should be noted that the uninterrupted operation of all sensor nodes in wsn is crucial for many applications where the feedback from the network must be reliable [cit] . this paper introduces a method for extending lifetime of the above-mentioned wireless sensor networks with smart nodes. the proposed method combines a new algorithm for rotating the ch role among sensor nodes with suppression of unnecessary data transmissions."
"in the best practice of neural network training, increasing training sets will make better a generalization and avoid an overfitting problem [cit] . in our case, the model learns a rich variant of data, i.e., we trained the model on a larger dataset so that the model can learn a better understanding of the data variant. with this strategy, the model can have a better generalization than those trained on each specific dataset."
"the algorithm of sensor node initialization (algorithm 4) is executed every time a sensor node starts to operate as well as when ch node is no longer present, e.g., due to malfunction or battery depletion. the initialized sensor node waits for sync frame. if the sync frame is received, the sensor node joins the wsn cluster as cluster member. in opposite situation, when sync frame is not received, the node takes the ch role. it should be noted that the sync frame includes information about current step of cluster operation. the ch node broadcasts the sync frame periodically, as shown in algorithm 2 (line 4). thus, a sensor node can join the cluster at any time, e.g., after its recovery."
"we refer to the estimated number of flops as the model complexity. the last column of tables 2 and 3 shows the number of flops required for each model. the complexity can reflect the inference time. the less the model complexity, the less the inference time."
"responding to automatic image transformation, an stn layer is added in front of the main decoder as shown in fig. 2(b) to appropriately transform the captured images. in our implementation, we use an stn layer that provides 6 output parameters. these 6 parameters play an essential role in performing linear transformation such as rotation, translation, dilation, cropping, and skewing of images. to demonstrate the capability of our neural decoder in solving the moving camera/object problem, we trained neural decoding with an stn layer such that the stn and the main decoder are initialized to the identity transformation and the pretrained cnn from the fixed led position dataset, respectively, as schematically shown in fig. 2(a) . in this experiment, five modified cnn architectures in conjunction with an stn layer: m-vgg16bn, m-resnet18, m-densenet121, vgglike, and minidensenet were tested with the unfixed led position dataset. we implemented the main neural decoder with transfer learning by the use of the pretrained model from the fixed position dataset models, and then we trained the whole networks. the test results are shown in table 3 ."
"the wireless communication module installed in sensor nodes is based on zigbee technology (xbee s2c), which provides wireless end-point connectivity to devices. these modules use digimesh firmware developed by digi. digimesh simplifies network configuration and increases reliability in conditions where devices required for network operation may fail. moreover, this technology is easy to use. the xbee modules support multiple network topologies such as point to point, point to multi-point, mesh, and cluster tree. the system includes an implemented data retransmission mechanism. the electrical diagram of the test environment for single sensor node was shown in figure 3 . as shown in figure 3 . an additional node (microcontroller without wireless communication module) is included in the experimental testbed to measure the energy consumption for all sensor nodes. the measurements collected by the additional nodes are not available for the sensor nodes and are used only for evaluation of the wsn lifetime. thus, the sensor nodes cannot use these measurements for making decisions about ch rotation."
"in this experiment, the theoretical decoding, sw-qam, was excluded from the performance comparison. the main reason was that the position of the led center and the psf of these datasets were different from the fixed led position dataset because the camera was not stable. hence, sw-qam needs complementary methods such as localization, object detection, and psf modeling. these computer vision methods are beyond the scope of our discussion."
"the associate editor coordinating the review of this manuscript and approving it for publication was mithun mukherjee . cnn architecture has several spatial convolutional layers that are able to learn automatically from data through the back-propagation algorithm. since then, there have been various computer vision tasks that take advantage of cnn developments such as image classification [cit], object detection [cit], and tracking tasks [cit] ."
"limitations of the direct transmission have motivated the development of more sophisticated data collection methods that divide the wsn into clusters. these methods reduce the number of data transmissions and save energy of sensor nodes. the sensor nodes in a given cluster send information to ch node, which aggregates the data and transmits them further to the sink or to another ch. however, as already mentioned in section 1 the ch role has to be rotated among sensor nodes to balance their energy consumption and avoid fast energy depletion of the nodes that takes ch role for a longer time."
the objective of the proposed method is to prolong the lifetime of wsn by appropriately rotating the ch role among smart sensor nodes. it should be kept in mind that the considered wsn lifetime corresponds to time interval before death of the first sensor node.
"typical wsns are composed of nodes that sense their environment by using built-in sensors and transmit the sensor readings to a sink node [cit] . in contrast, smart sensor nodes have an additional ability to process the collected data, make decisions, and recognize relevant events based on the sensed information before sharing it with other nodes [cit] . instead of transmitting raw data readings, the smart sensor nodes report the detected events. moreover, to recognize the important events in larger regions, many smart nodes must cooperate. it means that the sensor nodes from a given region of interest are grouped into cluster and exchange sensed information to make a common decision regarding occurrence of an event [cit] . in this case, the individual sensor nodes assess partial detection results and report them to one selected node, which acts as a cluster head (ch). one ch node is selected for each cluster. the ch node combines the collected partial results to recognize events in the region of interest. finally, the information about events is reported by ch node to the sink."
the average energy consumption (e i ) of i-th sensor node in time period t depends on the fraction of time t in which sensor node i takes the ch role. let us denote this fraction of time as t i then the average energy consumption can be expressed as follows:
"the use of a unified system containing only a single neural network model is one of the supporting reasons why the neural decoding outperforms the theoretical decoding. in addition, the unified system helps us easy to optimize a single model instead of multiple components of the sw-qam decoder."
"the method presented in this paper prolongs the lifetime of wsn with smart sensor nodes. the considered smart sensor nodes have extended computing capabilities and enable comprehensive data processing to transmit information about detected events instead of raw sensor readings. to detect events in larger areas, the smart nodes communicate and cooperate in clusters. one node in cluster must be selected as the ch. the introduced method allows the sensor nodes to decide how long they should take the role of ch to maximize lifetime of wsn. to this end a lightweight energy consumption model is used, which takes into account probabilities of event occurrences for particular nodes. the results obtained during long-term experiments conducted on the hardware testbed shows that the proposed solution gives better results than state-of-the-art approaches when the event probability is constant as well as when the probability changes over time. thus, it can be concluded that the proposed algorithm enables effective adaptation of the ch rotation process to the current situation of events detection. as for future works, an interesting topic is to examine the ch rotation methods for wsns with more hierarchy levels and for hybrid sensor nodes that are equipped with different sets of sensors. other further research directions include evaluation of the method for more complex statistical models with different distributions of event probability and experiments with larger wsns."
"from experimental results, m-vgg16bn and vgg-like architectures were poor performance, either in terms of error rate or rmse, in most cases. it can be seen that, in tables 2 and 3 decoding architectures. on the other hand, the architectures with skip connections commonly perform better, because the gradient effectively flows through the skip connections."
"the above-discussed state-of-the-art ch rotation methods have important drawbacks that limit their effectiveness in applications to wsns with smart nodes. first of all, the existing methods are based on an assumption that all sensor nodes in a cluster transmit their sensed data with the same, constant frequency. the possibility of suppressing data transmissions by the smart sensor nodes is not taken into account in those methods. it should be kept in mind that the considered smart nodes perform transmission when recognizing an important event. each node can detect different number of events and the frequency of events can change in time. it means that the energy consumption of smart node changes dynamically and may differ significantly between nodes. thus, in case when the smart nodes in a cluster suppress unnecessary transmissions, the existing methods can fail in achieving the balance of energy consumption for all nodes, which leads to reduced network lifetime."
"in this section, we compare the performance of our previous theoretical decoding, an sw-qam decoder with a wiener filter, and five modified architectures of neural decoding under the same settings of experimental setup as those described in sw-qam [cit] ."
"in centralized version of the leach protocol (leach-c) [cit] each node sends information about its current location and residual energy level to the sink, which uses a centralized clustering algorithm to select ch nodes. the sink computes average node energy, and the sensor nodes with energy below this average cannot be selected as ch for the current round. this method aims at evenly distributing the cluster-head nodes throughout the network. however, the centralized approach requires additional data transmissions form sensor nodes and involves a complex clustering procedure, which must be performed by the sink."
"in the proposed method, it is necessary to determine parameters of an energy consumption model. these parameters are related to hardware solutions used to build sensor nodes, and can be determined in advance for a given node architecture. therefore, the experimental calibration of the parameters after installation of the sensor network is not performed."
"when designing smart sensor nodes, two families of microcontrollers were taken into account: arm and avr. both can switch off unused functional blocks, but differ in the approach to energy saving. the microcontrollers of avr family (attiny, atmega) by default have all modules such as adc, i 2 c, spi enabled. different approach is used in arm systems. here, initially all modules are switched off. it is also possible to turn off the power supply to individual input/output ports, which in the case of the aforementioned avr family of microcontrollers is not possible. in this research the arm microcontroller was used (stm32f103), which is clocked at 72 mhz frequency and offers greater accuracy of timers, enabling better synchronization of sensor nodes, while maintaining compact dimensions of the device."
"with nn's capabilities for accommodating several complementary methods, we can have a unified system for decoding by means of neural decoding. then, its algorithmic procedure can be described as follows."
"data transmission is the most energy expensive operation for energy constrained wireless sensor nodes. according to the suppression approach, consumption of battery power in sensor nodes is reduced by transmitting the data only if it is necessary [cit] . in the considered scenario, a smart sensor node, which did not detect any relevant event, can skip transmission to ch node to save the energy. in general, the transmissions of partial results to ch can be suppressed if the event detection results that would be obtained by the ch with and without these partial data are the same."
"in this subsection, we evaluated the performance of the proposed scheme on two datasets: (i) a fixed led position dataset that the camera for image acquisition is stable and (ii) an unfixed led position dataset that the camera is unstable or moving around while keeping the led points inside the frame. for the fixed led position dataset, both the theoretical and neural decoding methods were evaluated. on the other hand, for the unfixed led position dataset, only the neural decoding methods were evaluated. the performance metrics used for evaluation are error rate for decoded accuracy, rmse for precision, and flops for model complexity. further, we use a constellation diagram to visualize the classification of decoded symbols. here, we plotted the x-y coordinate of two leds in the 64-qam constellation for all scenarios to visualize the performance of each method. a grid center of constellation represents a reference point. a distance between the correct decoded symbol and the reference point represents precision measured in terms of rmse. most importantly, the robustness of the neural decoders is evaluated by different environments such as dark-and bright-lighting conditions and by different conditions such as moving and non-moving cameras. the achievement of robustness can be reflected through our used evaluation metrics, i.e., error rate and rmse."
"to prolong the lifetime of wireless sensor network, a balance must be achieved between energy consumption of all sensor nodes. ch node consumes more battery energy than the remaining sensor nodes. the reason behind unequal energy consumption is that ch node must remain active most of the time, while other nodes are active only when processing and transmitting data. during inactive time, the sensor nodes are switched to sleep mode for saving energy. thus, the ch node depletes its energy resources faster than the other nodes, which leads to reduced network lifetime. the balance of energy consumption can be restored by periodically reassigning (rotating) the ch role to different sensor nodes [cit] ."
"the main contribution of this paper is a new ch rotation algorithm, which takes into account individual energy savings achieved by the smart sensor nodes that suppress the unnecessary data transmissions. the proposed ch rotation algorithm was implemented in a prototype of wireless sensor network. during experimental evaluation of the new algorithm, detailed measurements of real energy consumption and network lifetime were conducted. results of these experiments have revealed that lifetime of the prototype sensor network is extended when using the proposed method in comparison with state-of-the-art ch rotation algorithms."
"stable election protocol (sep) [cit] was intended for heterogeneous sensor networks, where sensor nodes have different initial levels of energy. in sep new ch nodes are randomly selected at the beginning of each round, just like in leach. an advantage of this protocol is that sensor nodes with higher energy become chs more often than the nodes with lower energy resources. this approach prolongs the time interval before the death of the first sensor node."
"when analyzing the examples from figure 1b,d) it can be observed that the wsn lifetime is maximized if both nodes die at the same time. this observation is also confirmed by conclusions of the related works, e.g., [cit], where more complex scenarios were considered than those discussed above. therefore, the general rule can be formulated that energy consumption of sensor nodes has to be balanced to prolong the wsn lifetime."
"as discussed so far in this section, it can be concluded that the neural decoder is robust to the light interference, since it can accommodate different types of psfs in different situations of the unstable position of leds in the image."
"the plot in figure 18 show results of the extended experiments. the columns in this plot correspond to average values of the wsn lifetime for all considered scenarios, while the error bars show maximum and minimum lifetime observed during tests. the average lifetime values in figure 18 were determined by taking into account the experimental results of the 10 scenarios that were mentioned earlier in this section. similarly, the maximum and minimum lifetimes were taken from the set of the results obtained for 10 scenarios."
"the contribution of this work also includes experimental evaluation of the ch rotation methods with use of real wsn. comparison of the proposed method with time-driven and energy-driven approaches was conducted for physical prototypes of smart sensor nodes. results of this study include measurements of wsn lifetime in real-world conditions. in contrast, the results presented in related works were obtained using simulation software."
"the second example (figure 1b) shows that the lifetime of wsn can be extended by changing the sensor node, which takes the ch role. in this example, node 1 takes the ch role for cycles 0-249, and then the ch role is performed by node 2. both sensor nodes deplete their energy at the same time step, after 500 cycles. it means that lifetime of the wsn is prolonged to 500 cycles. the lifetime of 500 cycles is the maximum time for the considered wsn example. it should be noted that the maximum lifetime is achieved when both sensor nodes die at the same time step. lifetime of the wsn for different initial energy levels of sensor nodes is analyzed in figure 1c,d ). in these examples the initial energy equals 100 units for sensor node 1 and 60 units for sensor node 2. figure 1d) shows that without ch role rotation (node 1 is ch) both nodes die simultaneously after 400 cycles of the wsn operation. thus, for the above-mentioned levels of initial energy the maximum wsn lifetime is equal to 400 cycles. in figure 1c ) the change of ch node after 250 cycles results in decreased wsn lifetime, as node 2 depletes its energy approximately at cycle 330."
"the help of skip connection makes a model easy to scale. in general, the deeper the model, the better the performance [cit] . as shown in tables 2 and 3, m-densenet121 had more layers (deeper) than others, so it yielded the lowest rmse. on the other hand, in the case of training data, that statement is always true for the model that has skip connections. however, if the model is very big (deeper), it may lead to the overfitting problem. it is similar to a fitting problem with a higher polynomial order. the higher the order, the better the performance. it is always true on the training data but is not on testing data. one may need to add more training data when going deeper to avoid the overfitting problem. note that the optimization was subjected to the mse loss function. thus, the best model is the one that has the lowest mse according to the objective function."
"nevertheless, the neural decoding needs a large dataset for training when compared to the theoretical decoding. from a comparison of both theoretical and neural decoding methods on a fixed led position dataset, the results are comparable. specifically, the neural decoding is slightly better than the theoretical decoding in terms of accuracy. on an unfixed led position dataset, the performance of the neural decoding with an stn layer tends performed sufficiently well. however, note that the neural decoding still depends on the theoretical encoding, sw-qam encoding scheme, because if a symbol is invalidly encoded and modulated, no decoding scheme will successfully decode that symbol."
"we have proposed a neural decoding method based on an sw-qam scheme for vlc. in real situations, the sw-qam decoder becomes inefficient for decoding the encoded signal, especially in the case of a moving camera as shown with an unfixed led position dataset. however, it is easily solved by the neural decoding embedded with an stn layer. based on the neural decoding concept, we can also develop a robust and unified system for our decoding purposes, regardless of led localization, light interference elimination, noise removal, and other unknown parameters. all can be solved by a single system, i.e., neural decoding with a training algorithm. in this paper, we have shown the use of a single minidensenet model that provides excellent generalization performance in all experimental scenarios."
"during tests of the elaborated wsn it was observed that increased energy consumption is caused by retransmissions that occurs in case of incorrectly synchronized sensor nodes. it should be noted that the cluster members send data to ch node according to tdma schedule and their timers need synchronization to avoid transmission errors due to collisions. moreover, if cluster member is not correctly synchronized their messages can be not received by the ch node. in such situations, when frame transmission is not finished successfully, the xbee modules perform retransmissions. the impact of synchronization period on energy consumption by sensor node is illustrated in figure 5 . the results presented in figure 5 were obtained for two synchronization periods: 5 min and 10 min. in the first case the ch node consumed 265 mwh of energy on average, while cluster member used 189 mwh. after extending the synchronization period to 10 min, the average energy consumption increased to 268 mwh for ch node, and to 195 mwh for cluster member. the sensor nodes consume less energy when the synchronization is performed every 5 min since in this case fewer retransmissions are necessary. the number of retransmissions is reduced as the sensor nodes are better synchronized and send their data in the tdma slots allocated to them, exactly when the ch is waiting for the data. in case when the synchronization is performed every 10 min, the number of transmissions executed out of the specified time slots increases at the last phase of the synchronization period. such transmissions cannot be completed correctly and the data are retransmitted at the next opportunity. based on these results, a short synchronization period was used in further experiments."
"the coulomb counter is useful in measuring the depletion of battery energy in wireless devices. it enables continuous current measurement in situation where long-term observation is required. the measuring process is based on differential measurement over a shunt, i.e., a resistor that has a small amount of resistance and is connected in series. the coulomb counter detects potential difference (voltage drop) across the resistor and determines current flowing through the circuit using ohm's law. due to parameters of the internal resistor and the maximum energy consumption of the sensor node, the smallest amount of energy to be measured is 26 µah. the ltc4150 module generate impulse on an output pin each time 0.1707 mah is absorbed by the sensor node. these impulses are counted by an additional microcontroller to evaluate the energy consumption over a long period of time. it was assumed that all sensor nodes have the same battery capacity of 700 mah."
step 1. concatenate three frames of a preamble symbol and three frames of a received symbol as the input of neural decoder. step 2. predict the x-y coordinate in the qam constellation of the symbol using the neural decoder. step 3. classify/decode the symbol based on the grid location in the qam constellation.
"fixed clusters of sensor nodes are considered in leach-f [cit] . in this approach the clusters are formed once, using the centralized clustering algorithm developed for leach-c. the sensor nodes within a cluster are numbered and the ch position rotates among these nodes according to the round-robin method. it means that the first sensor node becomes ch for the first round; the second node is ch in the second round, and so forth. this protocol eliminates the communication overhead of the cluster formation procedure at the beginning of each round. however, it is not suitable for dynamic networks."
"a distributed algorithm for organizing sensor nodes into clusters was introduced in the hierarchical routing protocol called leach (low-energy adaptive clustering hierarchy) [cit] . according to this protocol, the ch role is randomly rotated among the sensor nodes to not drain the battery of a single sensor node. new ch node is selected after a certain time (round), which is determined a priori. the ch node creates tdma (time division multiple access) schedule. thus, the sensor nodes send their data in time slots allocated by the ch node. the ch node collects data and sends them to the base station. operation of the leach protocol is divided into rounds and each round consists of two phases: set-up phase and steady phase. in set-up phase the new ch is selected and sends an advertisement, which includes information about its role. in the steady phase data transmission begins and sensor nodes send their data to the ch node according to allocated tdma slots. leach allows the ch nodes to perform local aggregation of data in each cluster to reduce the amount of data transmitted to the base station."
"this paper introduces a new ch rotation method, which takes into account probabilities of detecting events and transmitting data by smart sensor nodes. according to this method, the ch node estimates the probabilities of data transmission for all cluster members. based on the estimated transmission probabilities, using a lightweight energy consumption model, the ch node decides when the change of ch must take place. the determined time of ch change ensures equal energy consumption for all nodes in the cluster. this leads to maximization of the network lifetime, which is defined as the time to death of the first sensor node."
"the wsn built for research purposes consists of four smart sensor nodes. three of the sensor nodes can act as cluster head or cluster member. the fourth node is used as sink. each node of the wsn contains a microcontroller, a communication module and an analogue light sensor als-pt19. the measurement of energy consumption is performed by using ltc4150 coulomb counter [cit], which is an external module of the microcontroller."
"in this paper, we focus on the capability of cnn for the role of decoding function in different situations. however, we still have some rooms for neural decoding improvement. for instance, one can improve the performance of neural decoding by carefully redesigning cnn architectures and applying optimization techniques in the training stage."
"recent developments in smart sensor technology have opened new perspectives for advanced applications of wireless sensor networks (wsns) in many domains including healthcare, automation, infrastructure, and environment. the smart sensor nodes in wsn enable complex processing and analysis of sensed data with increased computational power [cit] . wsns have become an important part of 5g mobile technology that open new perspectives for advanced applications of smart sensors in future internet of things (iot) applications [cit] ."
"in this paper, we propose a novel neural decoding method based on sw-qam scheme. the proposed method is fully decodable and independent from any additional techniques."
"for models of neural decoding, their complexities were much higher than that of theoretical decoding. these complexities were part of profiling library for a pytorch model, i.e., torchscope. however, for the neural decoding, we have a unified system instead of a modular system. thus, we only need to optimize a single model. here, the lowest complexity among the neural decoding model is minidensenet whose complexity is around 100 times of sw-qam. for the unfixed led position, the complexity is slightly different from the fixed one, because the stn architecture is a small model as explained in section iii-b."
"for detailed evaluation of the proposed ch rotation algorithm it was necessary to conduct experiments with various probabilities of event detection by particular sensor nodes. it would be difficult to control the light intensity to achieve a desired probability of event detection. therefore, a function was programmed, which allows the sensor nodes to randomly generate the events with a given probability. las a result, the smart sensor node performs transmission when an event is randomly generated. if no event is generated then the data transmission is suppressed. the simple suppression procedure does not require any additional parameters. this approach has significantly facilitated the experimental evaluation."
it should be also noted here that the energy balance is significantly disrupted by the mechanism of transmission suppression. the smart sensor nodes that detect a higher number of important events must perform the data transmissions more frequently and consume a larger amount of energy.
"in heed (hybrid energy-efficient distributed clustering) protocol [cit] the new ch nodes are also selected after a predetermined time, corresponding to duration of one round, which is similar to the methods discussed above. however, in case of heed the chs are selected by taking into account the amount of residual energy. for each sensor node the probability of becoming ch is proportional to its battery level. the distributed ch selection procedure requires only local information about neighboring sensor nodes, but involves considerable energy expenditure to periodically rebuild clusters."
"initial research was conducted to verify the possibility of evaluating both the energy consumption and the wsn lifetime with use of sensor nodes powered from batteries. however, when repeating the measurement with the same settings, significantly different results were obtained. this problem occurs due to the fact that the batteries are never charged exactly in the same level. to solve this problem a power supply unit, mean well apv-12-5 [cit], was used during experiments. the lifetime of wsn was determined for each analyzed scenario by the additional node, which performs measurements of energy consumption. death of sensor node was detected each time the sensor node has consumed a predetermined amount of energy (3500 mwh). this approach has ensured repeatability of the conducted experiments."
"in this study, a vgg-like architecture is used with filter sizes of 5 and 7, while most of the modified cnns that we work with have a filter size of 3. this increases the neural network capacity while still uses a small number of features and layers. however, the number of filters is still much smaller than those used by other architectures. in this way, the neural network size is much smaller than the original vgg (lower complexity). please note that the convolutional and fc layers are followed by leaky-relu [cit] as an activation function."
"the smart sensor nodes in wsn execute the operations presented by the pseudo codes in algorithms 1-4. the operations are repeated in regular time intervals (steps). during normal operation the non-ch node (cluster member) reads data from its sensors, detects events based on the collected sensor readings and, if an important event is detected, transmits the detection results to ch node (see algorithm 1). for energy saving, the sensor node wakes its communication module up only when the data transmission is necessary. in case when the initial energy of sensor nodes is not known in advance, the cluster member can include information about its residual energy in the first data frame, which is send after selection of new ch. as shown in algorithm 2, the normal operation of ch node consists of collecting sensor readings, detecting events, aggregating results received from cluster members, and sending the aggregated results of event detection to sink. once the last step of the normal operation period is finished, all sensor nodes in wsn switch to executing the ch selection algorithm (algorithm 3)."
"another drawback of the state-of-the-art methods is that they require parameters calibration for each application and each deployment of sensor network. for instance, the most important parameters that have significant impact on effectiveness of those methods, and need to be carefully calibrated, are the time duration of one round (for time-driven approaches) and the energy threshold (for energy-driven algorithms)."
"moreover, in an image-sensor based vlc for localization research, [cit] applied a neural network (nn) localization method and used a stereo vision technique for vehicle positioning. they introduced two positioning methods: a cooperative-vehicle positioning (cvp) system using optical camera communication (occ) method in complementary with a computer vision technique and an nn based technique. they used the light from leds at the rear of a car to communicate with another car installed with a stereo camera so that the latter car can determine the position of the former car. this work shows that the nn method outperformed the computer vision method with occ based technique. moreover, nn does not need complex mathematical modeling to build a model. nn only needs to learn from a set of training data then it can build a model automatically through a learning algorithm."
"for the bright lighting condition, minidensenet and m-resnet18 were the first and second best performed methods with error rates of 1.06% and 1.33%, respectively. on the other hand, m-vgg16bn and vgg-like were the two worst performed methods. for the theoretical decoding, sw-qam was the third best. its accuracy for the bright lighting condition was not as good as that for the dark lighting condition. this happened because of the sub-optimal complementary methods, i.e., wiener deconvolution and led localization, in estimating the frame intensity. for the wiener deconvolution, psf modeling is needed; therefore, when the non-optimal psf is used, it usually affects the overall performance of decoding. for the led localization, the led position can be manually localized, because it was fixed at a certain point. however, if the camera is moving (it can say that the led is not fixed), we need a computer vision technique for localization. for this reason, the performance of sw-qam depends on those of the complementary methods."
"extensive experiments were conducted to compare the above-discussed methods for 10 different scenarios. the experiments involved dynamic scenarios, where the probability of event occurrence changes in time. each dynamic scenario assumes different changes of the event occurrence probabilities during 15-min cycles, similarly to the second scenario, which is analyzed above in this section. static scenarios were also included in the experiment. in this case, the event probability for a given node was constant, but different probabilities were assigned to particular nodes."
"in case of energy-distance aware clustering scheme for wireless sensor networks (e-dacs) [cit] the ch is selected for each cluster depending on its residual energy, distance from other sensor nodes in cluster, and distance to sink. new ch selection is executed periodically, once energy level of current ch becomes less than the energy of any other sensor node in its cluster."
"the considered wsn was designed for indoor detection of moving objects, e.g., people passing through a corridor in a building. therefore, each sensor node was equipped with a light sensor [cit], which converts the lighting level into a proportional value of the voltage at its output. this voltage is then measured through one of the adc (analogue-to-digital converter) channels of the microcontroller. an example of lighting level measurement is shown in figure 4 . it should be noted that the designed sensor nodes recognize important events as significant changes in the voltage level at the light sensor output. to be detected as event, the registered changes must be above a threshold, which is determined dynamically by using a moving average filter [cit] . determining the threshold in this way guarantees correct detection of objects in various lighting conditions. the moving average filter is used with time window of 100 samples. the smart sensor nodes were programmed to perform the algorithms presented in section 3.2. in this implementation, one step of the wsn operation corresponds to 0.5 s. it means that the sensor data are collected, event occurrence is verified, and result is reported to the sink two times per second. this enables detection of fast-moving objects."
"for large clusters with many sensor nodes, the computations of tch i that require solving the system of linear equations (4), can lead to significant energy expenditure and computation time, when performed by the ch node with limited resources. thus, an optional procedure was proposed where the system of equations is solved by the sink (base station), which is powered from the mains and has greater computational power than ch node. according to this procedure the ch node sends a request to the sink with estimated probabilities of event occurrence (p i ) and receives the tch i time. it should be also noted that the equation system can be effectively solved by using artificial neural networks [cit] or approximate algorithms [cit] . moreover, an approximate solution is sufficient for the proposed ch rotation method, as the tch i time must be determined with precision that corresponds to the time interval between two successive ch selections. at the time of ch selection procedure, the cluster members listen to sync frame, which is broadcasted by the ch node. the sync frame includes address of the new selected ch. if a cluster member receives the sync frame with ch address equal to its own address then it switches to act as ch node. another basic function of the sync frame is time synchronization of all nodes in the cluster. the sensor nodes are synchronized by resetting their timers on reception of the sync frame."
"the key issue in applications of wireless sensor networks is the limited lifetime of battery-powered sensor nodes. thus, network management methods are necessary to enable effective use of the energy resources of sensor nodes [cit] ."
"in the case of dark lighting condition, m-resnet18 was the best and followed by minidensenet with error rates of 0.52% and 0.90%, respectively. from fig. 7, it can be seen that m-resnet18 and minidensenet showed satisfactory constellation plots, although m-densenet121 was more excellent because it had the lowest rmse value of 0.0134. for bright lighting condition, minidensenet was the best performed method with an error of 2.73% and followed by m-densenet121 with an error of 3.16%. again, m-densenet121 showed the less spread points with rmse value of 0.0340, as depicted in fig. 8. however, m-vgg16bn and vgg-like performed defectively in most cases either for error rate or rmse as shown in table 3 and figs. 7 and 8. in addition, it was the same as in the fixed led position dataset; the architectures with skip connections, m-densenet121, m-resnet18, and minidensenet, performed better in terms of precision (spread) as can be seen in the decoded symbol constellation results in figs. 7 and 8 . m-densenet121 was still the most precise model in all datasets as depicted in figs. 5, 6, 7, and 8, but it had a trade-off between precision and model complexity (flops)."
"the basic method for organizing data transfers in wsn is direct transmission [cit] . this method assumes that all sensor nodes have the same role and their task is to send all sensors readings to a base station (sink). in this case, the ch nodes are not present. it should be noted that the sensor nodes consume energy mainly when they are accessing the communication channel and transmitting data. thus, the transmission of all sensed data to base station results in high energy consumption and reduced lifetime of sensor nodes. moreover, scalability of this method is low as the base station must collect and process large amounts of data from all sensor nodes. if workload of the base station is too high, the sensor data may be lost. the complexity of direct transmission protocol is negligible, and its implementation is simple. however, the low energy efficiency of this protocol makes it not suitable for most applications."
"energy-distance aware clustering method (edac) [cit] elects chs based on two parameters: the residual energy of sensor nodes and the energy expended for transmitting data from sensor nodes to potential ch. these two parameters are combined in a metric, which quantifies how good a sensor node would be as a ch. using the metric evaluated for all sensor nodes in a cluster, each node decides whether it will be the new ch. selection of new ch is made when the residual energy of current ch drops below a predetermined threshold."
"the performance of all test methods was evaluated with the fixed led position dataset on two lighting conditions, i.e., dark and bright. the comparative results in terms of error rate, rmse, and the estimated number of floating operations (flops) are shown in table 2 ."
"in principle, numerous model simulations and calculation procedures have been reported for the uncertainty evaluation in measurements. these include the monte carlo method [cit], fuzzy sets theory, grey system theory, and bayesian theory [cit] . these evaluation models are primarily developed with the aim of successfully finding a single best set of parameter values. the focus is on the steps of deriving a corresponding measurement model, the assignment of the probability distribution functions (pdfs) to the input parameters in the model, and the propagation of the distributions through the model [cit] . therefore, in numerous studies, the procedure for calculating the arithmetic mean of the initial quantities followed by conversion into an estimate of the measured quantity yields a shifted estimate because of their nonlinear relationship. from the comparison of these models, the methods of type a and b evaluation following the standards of gum and jjf seem to be best suited for conducting the data evaluation and consistency checking of a data acquisition system with sensor networks. a sample study illustrated how the method can be implemented."
"during the process of data evaluation and uncertainty analysis, monte carlo methods have shown to be a reasonable tool for uncertainty analysis and delivering realistic results when the model is highly complex [cit] . from the comparisons, it is obvious that the methods of type a and b evaluation following the standards of gum are appropriate for the evaluation using the data acquisition system. the corresponding probability distribution functions (pdfs) could not be retrieved between the data acquisition system and various sensors. such a model is invalid for uncertainty analysis and sensor network evaluation because the relationships between the data acquisition system and sensors are unknown [cit] . thus, the algorithm following gum for the uncertainty analysis proves to be efficient for evaluating the performances of the sensor networks and data acquisition system. from the case study in this research, it is concluded that calibration of the sensors and reduplicated measurements are essential and indispensable to eliminate the systematic errors and standard deviation during the data acquisition process. every effort must be taken to ensure that this system is appropriate for environment monitoring and poultry management by decision makers while simultaneously considering the multiple sources of uncertainty [cit] ."
"expanded uncertainty refers to the 95% coverage interval of the measurement results, which is a key component in the uncertainty evaluation of the effective calibration. this function can be written as"
"the usefulness of this data acquisition system for poultry production depends on the performances of the sensors. therefore, the calibration of the sensors and uncertainty analysis of the sensor data must be conducted completely to maximize the reliability of this system. the traditional procedures of the statistical approaches are strongly dependent on specific assumptions. a complete uncertainty budget has to be calculated for each sensor and any other effects of the instability sources fulfilling the standards of gum. the measured values in this system depend on the calibration task of the instruments, signal sampling process of the module, data processing with the data server, and any other influences for each sensor. the relevant uncertainty sources are shown in the cause and effect diagram in fig. 4. engenharia agrícola, jaboticabal, v.38, n.6, p.857-863, nov./dec. 2018"
"the hardware of the data acquisition system (short for system 1) mainly consisted of various sensors, signal conversion modules, and a datalogger embedded, and its software was a data acquisition program, as presented in fig. 1 . the data acquisition module was designed to collect the environmental parameters including air temperature, relative humidity, and co2 concentration of a compartment housing for laying hens. the power supply and digital output of the sensors were 24 v dc and 4-20 ma. other sensors with the same signal output can also be feasible for this system. the features of the sensors used in this system are listed in table 1 ."
"typically, the arithmetic mean, standard deviation, and measurement error are used to combine the results of the test. the bessel formula to estimate the sample is"
"1-perch system, 2-water line, 3-feed trough, 4-egg collection, 5-nest box, 6-perch (a) system design (b) system figure figure 3 . photos of the perch system for laying hens."
"the key component of this system is the signal conversion module (model dam-e3058f, art technology development ltd, beijing, china) with the following specifications: 8 channels of analog output, 10 hz sampling rate, and 16-bit resolution. the module interacts with the data server through web request based on the modbus communication protocol. it allows the system to read out the values from the sensors and transfer the signal values to measured values using a linear interpolation method."
"the key component of this system is the signal conversion module (model dam-e3058f, art technology development ltd, beijing, china) with the following specifications: 8 channels of analog output, 10 hz sampling rate, and 16-bit resolution. the module interacts with the data server through web request based on the modbus communication protocol. it allows the system to read out the values from the sensors and transfer the signal values to measured values using a linear interpolation method. the ethernet router (ht-3gw, hlwt-tech ltd, beijing, china) is used to establish communications with remote sites based on third-generation wireless networks (3g), as shown in fig. 1 . the terminal users connect to the system for monitoring the environmental condition and determine the poultry performance remotely."
"in recent years, the improvement of informative networks and sensor technologies has enabled the rapid growth of wireless and online management systems. these systems with sensor networks can remotely monitor and maintain communication with many unfavorable physical environments such as remote geographic regions, inaccessible dangerous locations, and commercial poultry farms [cit] . the management systems based on networks have many wide applications, ranging from earth observations [cit] to agricultural production [cit] . numerous sensor networks are used to measure the environment and directly guide production procedures. thus, it is important to evaluate the measurement uncertainty and sensor network errors with a web-based data acquisition and remote management system so that the uncertainty of the system can be estimated correctly [cit] . moreover, analyzing the sensor data will increase the confidence of the authorities in the results to make decisions."
"as listed in table 2, the measurement uncertainties of this data acquisition system mainly originate from the sensors of relative humidity and co2 concentration having a higher relative uncertainty (ur). relative humidity sensors employing a moisture sensitive device (msd) are commonly used for monitoring a wide range of application sectors. however, the long-term stability of this sensing device is still a major challenge owing to environmental pollution and static electricity [cit] . in addition, based on the multi-point air sampling and analyzing system [cit] and wide applications of electrochemical-based sensors for co2 sensing methods [cit], the co2 concentration measurement can be easily disturbed by airflow fluctuation and human operation. hence, very precise measurements and error evaluation were needed and adopted to elicit the disturbances and provide excellent stability."
"from the cause and effect with uncertainty sources of this system (figure 4), it is concluded that the sensitivity and uncertainty of environmental sensors could be used to examine the system performance and stability. adding the relative uncertainties of the three environmental parameters, the total uncertainty of this system in accordance with the standards of gum is given in [eq. (9)"
"this article aimed to demonstrate a web-based data acquisition system and the entire process of the uncertainty analysis for sensor networks. the specific objectives of this work were: (1) to introduce a web-based data acquisition system using the labview software program to measure environmental parameters for poultry management, (2) to deliver an algorithm for the uncertainty analysis of various sensor data based on the methods of type a and b evaluation, and (3) to examine the system performances based on the uncertainty analysis of the sensor network."
a solution is to digitalize the uncertainty sources using mathematical modeling. the general model for the evaluation of the system performances depending on input n quantities of each sensor can be given in the following form:
"the data acquisition program was written in the labview graphical programming language (national instruments, austin, tx, usa). the program was installed on a data server (poweredge r710, dell, round rock, tx, usa) with two xeon quad-core processor e5520 (8 m cache, 2.26 ghz) and 8 gb ram (random access memory). the main user interface of the program includes four sections: program management panel, data acquisition setting panel, data preview panel, parameter configuration panel, as shown in fig. 2 . in the program management panel, the users can set the acquisition interval and manually stop the program. in the data acquisition setting panel, the users can add more channels for data acquisition, select data collection points, and modify the internet protocol (ip) address of the modules. the data preview panel is used to display real-time environmental parameters, acquisition information of date-time, house-identification, and collection-position. in addition to the above panels, most importantly, the program can use the parameter configuration panel to input the measurement range and output type (4-20 ma, 0-5 v, or any other types) of the various sensors. measured values i x were calculated by:"
"where, in this uncertainty analysis, the method of type b evaluation is related to the accuracy of the sensors, precision of the measuring instruments, data processing approaches, and any other effects, as shown in fig. 4 . the value (ub) of the evaluation method can be expressed as"
"where, i x are the measured values obtained from each sensor used in the web-based data acquisition system over the measurement period, and s x are the true values derived from the measurement system with standard instruments after calibration."
"two datasets were collected. one was with the sensor data measured by the web-based data acquisition system (system 1) as the calculated values for the uncertainty analysis. the other one was with the reference data recorded by the measurement system (system 2) with standard instruments after calibration as the true values for the data evaluation. both data were collected simultaneously in the same pen with the same birds under the same configuration. the sources of data collection were the main environmental parameters of the laying hen houses including the air temperature, relative humidity, and carbon dioxide concentration. for the evaluation of the measurement errors, the same data were recorded continuously for 7 days with an acquisition interval of 5 min when the laying hens were between 32 and 33-weeks old."
"in this research, a web-based data acquisition system using the labview software program for poultry management was developed and an uncertainty analysis of various sensor data was proposed for error correction. the results indicated that most of the instabilities sources were related to the measurement activities of the relative humidity and co2 concentration. the total uncertainty of this system was estimated as 15.9%, fulfilling the standards of gum for current performances. to improve the system accuracy, the standards of gum were used to provide the calculated mobility with the measured data. the precision control of this system was achieved by obtaining accurate data from the sensors. to improve animal management, high-level algorithms of uncertainty analysis are being tested at the supervisory level. through repeated calibration and practical application in commercial farms, this system could be an excellent management tool for poultry production, which will increase the profitability and quality."
"more recent models of wm microstructure represent axons as straight, impermeable, cylinders. the ball-and-stick model 139 by adopting the stick model of axons, to explain the dki metrics. in reference 10, the model is extended with a free water compartment 144 to account for in vivo csf contamination."
"i. the process is iterative rather than linear: earlier steps predominantly inform later steps, but, very often, later steps will reveal new information that require rethinking and repetition of earlier steps."
"double diffusion encoding (dde) ( figure 6d ) consists of two successive sde blocks, separated by a so-called mixing time. 80, 86 dde has also been referred to as the double pulsed-field gradient spin-echo sequence, 87 or the double wave-vector experiment. 88 five distinct usages of dde target various microstructural features in different ways. 67 the first varies the relative gradient directions of the two sde blocks to quantify microscopic anisotropy (see, e.g., references 89-92). the second utilizes parallel gradients but a variable mixing time to measure exchange rates (see, e.g., references 23, . in this experiment, the first encoding block perturbs the signal fractions of different components, which then gradually restore to equilibrium. exchange is measured by gradually increasing the mixing times while using the second encoding block to monitor this equilibration process. the third usage of dde employs parallel and antiparallel gradients and a short mixing time to vary the degree of flow compensation and thereby improve estimation of blood volumes (see, e.g., references 85, 96 and 97). the fourth uses parallel and antiparallel gradients and short mixing time to estimate compartment sizes (see, e.g., references 88, and 98). finally, the fifth usage targets pore size and shape distributions in heterogeneous media by noting the retention of diffusion diffraction patterns. [cit] oscillating diffusion encoding (ode) can be achieved with the oscillating gradient spin echo (ogse) sequence, which replaces the constant gradient pulses in sde with pulses that have oscillating gradient amplitude. 103 oscillating waveforms may follow smooth sine or cosine functions ( figure 6e ), 103, 104 from square waves ( figure 6f ) 103, 105, 106 or even irregular square waves. 107 with sde, estimation of the diffusivity in small compartments requires short diffusion times, which limits the achievable b-value and thus the sensitivity to microscopic features. ode can maintain b-value at short diffusion times by repeating multiple pulses. this can enhance the sensitivity to the diffusion coefficient in small pores and thus facilitate estimation of small sizes. 108, 109 however, recent work 110, 111 suggests that the primary benefit of ode for the estimation of axon diameters (or cylindrical pores in general) arises in the presence of orientation dispersion or uncertainty, because ode retains sensitivity to size while avoiding high b-values, which lead to low signal from free diffusion along cylinders that are not perfectly perpendicular to the encoding gradients."
"despite nosocomial modeling's natural fit with the abm approach, it is a fairly recent area of exploration for healthcare abms [cit] . one of the earlier simulation efforts modeled antibiotic resistance in hospitals, contrasting and an individual based model with that of a differential equation based model, including consideration of where they can be used in conjunction with one another [cit] . another study investigated the spread of a nosocomial pathogen in a dialysis unit using a monte carlo individual based model [cit] . the dialysis unit is a very good example of where agent based models may be particularly useful as ''the frequency of patient visits and intimate, prolonged physical contact with the inanimate environment during dialysis treatments make these facilities potentially efficient venues for nosocomial pathogen transmission'' (pp. 1176). in related paper [cit], the same authors developed a fairly abstracted nosocomial abm within an intensive care unit, advocating for a ''conceptually simple discrete element (agent-based or cellular automata) models [that] can explicitly address 'geographic' considerations and probabilistic transmission dynamics germane to the spatially intricate environments and small population sizes characteristic of icus'' (pp. 174). in another nosocomial abm of an intensive care unit, operational and epidemiological features are considered in an attempt to estimate the effect of understaffing and overcrowding on infection spread [cit] . the abm simulated contact-mediated pathogen transmission, which should allow one to establish quantitative relations between patient flow, staffing conditions and pathogen colonization in patients. another individual based approach investigated the role of cohorting, with the aim of minimizing the possible interactions between individuals within a ward [cit] . in a relatively recent nosocomial abm, a combination of differential equation models and probabilistic models are used for each agent in order to simulate changes, over time, in the bacteria sub-populations within the agent's body [cit] . as with many abm efforts, work is ongoing in terms of validation and verification. in order to construct biologically plausible transmission risk models that can guide cross infection control, researchers have developed an rfid tracking system in an ed by which to extract agent contact data on the understanding of the critical role that contact patterns play in cross-infection control [cit] . this type of high-fidelity individual data, topography, as well as contact patterns is ideally suited for an abm as well."
"other scenarios that have been investigated using abms within healthcare settings include optimization of computer terminals [cit], serving as another illustration of incorporating inanimate objects as agents within the abm. the use of electronic devices including stationary workstations, mobile workstations, tablets, and smartphones in healthcare delivery is evolving very rapidly, and will likely develop momentum that will outpace the insights of abms in this area. other abms are oriented to interactions between hospitals where patient diversion on response to load was modeled [cit] ."
"the foundational premise and the conceptual depth of abm is that simple rules of individual behaviour will aggregate to illuminate complex and/or emergent group-level phenomena that are not specifically encoded by the modeler and that cannot be predicted or explained by the agent-level rules. in essence, abm has the potential to reveal a whole that is greater than the sum of its parts [cit] ."
"we can learn how model parameters respond under departures from the model assumptions by generating data using a model or procedure that is more complex than the one used for fitting. numerical approaches are useful in this context, and have been used to illuminate how dti parameters respond to e.g. crossing fibres or partial volume effects, 14, 233, 234 and to study degeneracy in parameter estimation. 156 monte carlo (mc) simulations are especially useful to simulate dmri of complex microstructure. 84, 228, 235, 236 in mc simulations, random walkers are released in a numerically defined microstructure substrate. for each walker, the phase accrued from a simulated gradient waveform is recorded and used to predict the signal. substrates can be constructed to match model assumptions of, for example, parallel nonabutting cylinders, as in references 84, 28, 235 and 239. alternatively, the model can be challenged by constructing substrates with higher complexity. such simulations have been conducted to learn how the model parameters respond to changes in fibre shape, permeability, undulation or dispersion. 34, 63, 84, 114, 228 segmented histology slides can also yield a substrate with a high level of complexity in both the intra-axonal and extracellular spaces. 109, 237 physical phantoms, which represent but generally simplify the tissue of interest, provide the opportunity to test models with measured data but from idealized or simplified samples. ground-truth values of model parameters can sometimes be controlled in the phantom construction, and importantly obtained by independent methods such as microscopy. different materials are used for phantom construction. inert materials such as glass or plastics offer long shelf life, high reproducibility and good control over microstructural parameters. conversely, biological phantoms, e.g. vegetables or cell cultures, have shorter shelf life, but are often cheap and easy to prepare, although the features of the microstructure are harder to control and measure. axon-like phantoms have been constructed from hollow glass capillaries and used to verify diffusion models, 238, 239 validate the relation between pore sizes and diffraction patterns from dde, 101 test size estimation with ode, 240 study microscopic anisotropy 241 and test dmri with free gradient waveforms. 214 fibre-like phantoms can also be constructed with coelectrospinning, 229, 242 or formed by liquid crystals. 243 asparagus stems have capillaries that can approximate large axons, 230, 237, 244 while asparagus puree can be used to study microscopic anisotropy. 218 phantoms that approximate round cells can be constructed from oil-water emulsions. 81 such phantoms were used to test compartment models 81, 105, 245 and later to test multimodal microstructure estimation. 246 yeast cultures also form phantoms with isotropic diffusion in distinct intra-and extracellular compartments. 94, 167, 247, 248 yeast cells feature temperature-dependent membrane permeability, detectable by dde, 70 and cell sizes in the range of 4-8 μm, which makes yeast cell suspensions an ideal testbed for methods aimed at cell size estimation. 98, 107 measurements in a microstructural environment close to in vivo can be performed by using fixed tissue, which offers many of the same benefits as phantoms. a drawback is that the ground-truth microstructure cannot be controlled and may be less well characterized than in phantoms. many challenges must be addressed to obtain high-quality data of fixed tissue. 249 it is of key importance to minimize the interval between death and fixation, 250 although once fixated successfully the microstructure and the diffusion parameters can be stable for years. 249 however, the degree to which the fixation alters the microstructure is unknown. 251 alternatively, measurements on viable tissue samples can be obtained to avoid possible fixative-related biases. [cit] results from comparisons between dmri-derived parameters and microscopy of fixed tissue have revealed timedependent diffusion congruent with diffusion restricted within axon-sized compartments. 20, 68 ogse-frequency dependence in both the intra-axonal and extracellular spaces, 109 and good agreement of myelinated neurite fractions from dmri and histology. 4 image analysis of fixed tissue has also been used to quantify levels of axonal orientation dispersion. 7, 254 direct validation where the same tissue is used for both mri and histology is particularly useful to guide the interpretation of model parameters, for example, by comparing axon diameters estimated from dmri with those estimated from histology. 10 naturally, such validation is impossible in healthy human brain tissue, and can be challenging even in a preclinical setting. indirect validation by comparison to literature values is an measurements on phantoms provide an opportunity to validate the pulse sequence implementation, as well as sanity check parameter estimates in simple geometries where ground truth is relatively well defined and to some extent controllable. d, fixed tissue offers means to obtain high-quality data without motion artefacts, while still having access to ground truth via, for example, electron microscopy (bottom, courtesy of mark burke). e, measurements on viable (rather than fixed) tissue in a maintenance chamber can be performed to alleviate potential alterations of the tissue microstructure from fixation; the image pair at the bottom shows electron microscopy from tissue that spent 10 h in a tissue maintenance chamber (left) compared with tissue deprived of glucose and oxygen for 2 h (right); courtesy of simon richardson. f, in vivo imaging combined with ex situ microscopy can be performed in preclinical conditions or in patients undergoing surgery to assess the agreement of mri-based and microscopybased analysis alternative, for example, regarding axon diameter estimation in humans. 11 resections in patients with e.g. brain tumours may, however, offer a rare opportunity for direct validation. 255, 256"
"although sde, dde and ode have been most commonly used to date, there are no theoretical reasons for limiting the gradient waveform to such designs. benefits may arise from using irregular waveforms 112, 113 ( figure 6g) . specific examples include the combination of ode with dde into a double oscillating diffusion encoding (dode) sequence, which may improve size and shape estimation. 114 other approaches utilize multidimensional waveforms to disentangle microscopic anisotropy from variation in isotropic diffusion, which is not possible with sde alone. 115 examples include triple diffusion encoding (tde), 116 circularly polarized gradients, 117 magic angle spinning of the q-vector 118, 119 and q-trajectory encoding (qte). 120,121"
"fitting figure 2 illustration of the microstructure imaging paradigm, which fits a model relating microscopic tissue features to mr signals in each voxel to produce microstructure maps. for example, various techniques to map indices of axon diameter [cit] 20, 21 use a simple geometric model of white matter microstructure, consisting of parallel non-abutting impermeable cylinders that represent axons. the methods acquire a set of images with varying diffusion weighting and fit the model in each voxel to recover estimates of cylinder size and packing density, which provide maps of indices of axon diameter and axon density. mri maps from reference 11. electron microscopy courtesy of mark burke"
"they do not help with over-parametrization, but such models can help identify the locus of possible parameter combinations that can explain measurements thus supporting more reasoned downstream inference that accounts for parameter uncertainty."
"abms may also be useful in hospital facility design where additional importance may concern the role of the hvac system within various departments. this would imply a hybrid of simulation techniques, likely encompassing an abm and computational fluid dynamics model. in another instance, a hybrid abm-des model for emergency medical services in a city is conjectured, although an actual model or simulation has not been reported [cit] . in a more pedestrian optimization, resource planning for placement of rfid readers in an rfid system may be integrated into the abm as a means of estimating errors associated with the tracking system [cit] ."
"in grey matter, the dendrites range from 0.2 to 3 μm in diameter for dendrites both proximal and distal to the soma of the neuron. 25 the dendrites branch from the soma membrane in a formation like a tree crown; see figure 4 ."
"using biophysical models of diffusion to estimate tissue microstructure follows a longstanding tradition in the field of physical chemistry which applies models of this kind to determine microstructure of inanimate samples. 122 for example, packer and rees 81 quantified the size distribution of oil droplets using a model of spheres with log-normal distributed radii. pioneering works such as this have inspired the adoption of this approach in biomedical imaging. in the early days of diffusion tensor imaging (dti) 123 it was hoped that simple indices such as the eigenvalues of the diffusion tensor, or combinations of them such as the mean diffusivity or fractional anisotropy, would reflect wm tissue properties such as myelination or fibre density. in regions of approximately parallel fibres, such as the cc, contrast in these parameters may arise from such tissue properties and has been used in this way in the literature, e.g. reference 124. however, in general, the effects of orientation dispersion dominate such contrast and more sophisticated models are necessary to separate the effects. 5 this section reviews such models in the context of assessing brain tissue microstructure."
"experiment design refers to the finite set of data points we sample from the, generally infinite, space of possible measurements. the choice of experiment design is critical in obtaining good parameter estimates in any model-based estimation task. in quantitative mri, the experiment design is the choice of pulse-sequence parameters in each image that provides the data to which we fit the model. in diffusion mri, the pulse-sequence parameters might be, depending on the complexity of the model, just the b-value or a more complete set of defining parameters, e.g. δ, δ, g, t e, for the sde sequence. 145 a good design has to balance competing effects, such as increasing sensitivity with b-value but lower signal-to-noise ratio as t e increases to accommodate higher b-value. to design a protocol for front-line application, i.e. use in a biomedical study or clinical application rather than development, we seek the combination of measurements that, for a particular budget of acquisition time, maximizes sensitivity to the parameters of a model we have chosen to use."
"yet, abms are considered to be a very promising and complementary technique by which to simulate hospital dynamics, with arguments for their more widespread use within healthcare will depend on more widely adopted and more effective conceptualization and implementation tools [cit] . some researchers claim that the ''signature'' success of abms in public health is in the study of epidemics and infectious disease dynamics [cit], where the successes of abms have demonstrated the importance of the role of social networks, human movement patterns, transportation systems, and the disease dynamic itself. this overwhelming amount of research in applying abms to the study of large scale infectious disease spread (e.g. influenza, stis) is not addressed here. abms applied to institution-scale environments (rather than regional scales) are nonetheless emerging as an excellent vehicle for modeling hospitals due to their inherent ability to leverage social network analysis in a similar manner to social interactions of a large scale infectious disease."
"statistical model selection 199, 200 seeks the model that best explains observations. the problem is important in microstructure imaging, because our understanding of the mechanisms of mr signal generation from complex tissue architectures remains crude so we typically have to make empirical choices. moreover, the degree of complexity that our data can support is unclear. broadly two strategies are available. the two strategies often broadly agree, and provably so, 201 because a model that predicts unseen data best is generally one that fits visible data well but without overfitting, i.e. using the smallest possible number of parameters. however, inconsistencies certainly arise (see, e.g., reference 147) . the strongest conclusions come from using multiple model-selection strategies to identify concurrence of model ranking."
"the amico framework, 221 lemonade, 222 wmti, 142 and reference 223 are all examples. however, confidence estimates are less straightforward to obtain from such techniques than from classical parameter estimation."
"brain tissue contains neurons and glial cells and is separated into two types: grey matter (gm) and the white matter (wm). the gm contains the cell bodies, i.e. somas of neurons and glial cells, as well as neuronal dendrites, short-range intra-cortical axons and the stems of long-range axons extending into the white matter. the wm is dominated by densely packed and often myelinated axons that emanate from the soma of neurons in gm, and project to distal gm areas or other parts of the body. glial cells are also found in wm. figure 3 illustrates the neuron and its environment, which in combination provide the basic mechanisms for brain function via communication between brain regions. 24"
"vi. robust and usable software together with exemplar applications are essential for translation to widespread uptake. both require substantial investment of time, but are essential steps in identifying problems of usage and interpretation, as well as engineering a technique for frontline application."
"when designing an abm for hospital applications, there are choices in system attributes that become design decisions unique to the context and objectives of the model. an abm is inherently agent-centric, and the model arises from the consideration and definition of the agent's environment, the agent's characteristics, and the agent's interactions with other agents."
"the first multi-modality microstructure imaging techniques are just starting to appear. for example, recent work on g-ratio estimation and mapping combines estimates of myelination from quantitative magnetization transfer 124, 261 or myelin water imaging 262 with estimates of axonal density from diffusion mri, albeit without using an integrated model of both contributing modalities. early work combining estimates of pore density from diffusion mr and optical imaging provides an example of an integrated model. 246 exciting possibilities arise in joint modelling of relaxometry (see, e.g., reference 263) and susceptibility imaging (see, e.g., reference 264), with diffusion mri, since the former are confounded by microstructural orientation effects, which the latter is able to estimate with relatively high accuracy. however, serious challenges arise in multi-modal microstructure imaging. first, the idea demands unified models relating mr to tissue features across multiple contrasts, none of which currently have broadly agreed models even independently. second, practical issues arise in processing and alignment of images from different contrasts, which often use quite different acquisition and reconstruction algorithms, to ensure sets of measurements from corresponding volumes of tissue."
"and t 2 are the longitudinal and transversal relaxation times (disregarding imperfections of the rf pulses). in general in pgste, care must be taken to account for both the t 1 weighting and diffusion weighting from crusher and imaging gradients, which can confound both experiment design and subsequent analysis. 76 another unconventional implementation of sde has gradient pulses of different lengths-the 'long-short' sequence, which provides particular sensitivity to pore shape. 77 common targets for sde acquisitions in the brain are, in addition to cell size and shape mentioned above, properties such as anisotropy, fibre density, exchange and ivim. quantification of anisotropic diffusion for fibre direction estimation and tractography is covered elsewhere in this special issue. the degree of anisotropy of distinct compartments, such as axons, and the density of these compartments, can be estimated from experiments with multiple diffusion weightings; see, e.g., references 78 and 79. compartment sizes, e.g. axon diameters, can be probed by sde with variable diffusion times. 80, 81 restriction manifests as a reduced signal attenuation compared with free diffusion as the diffusion time increases, and the smaller the compartment the more marked the reduction of attenuation. exchange between compartments 82,83 manifests as increased signal attenuation, compared with full restriction, as diffusion time increases. the similar effects of increasing restriction length and exchange on the signal amplitude make them challenging to disentangle in practice with sde. 84 finally, for low b-values the sde signal also captures effects of pseudorandom flow (the ivim effect), 65 which can inform on capillary blood volumes, although quantification can be sensitive to noise. 85"
"the interstitial, or extracellular, space (ecs) is the space that surrounds anatomical structures such as cells, axons and dendrites. invasive microscopy techniques suggest that the fraction of ecs in adult brain of various non-human species is 15-35%. 60 however, neither electron nor light microscopy can provide reliable measurements of ecs fraction, because chemicals used in the processing of the tissue for the display introduce dehydration. the resulting shrinkage effects have been reported to be as low as less than 1% and as high as 65%. 32, 33, 41, 61"
"the brain contains three vessel systems for blood perfusion: arteries, veins and capillaries. the capillaries contain the smallest vessels and ensure perfusion in brain tissue. they range in diameter between about 5 and 10 μm, and capillary density in cortical layers is high compared with wm. 64 macroscopically, the capillaries generally appear randomly organized, 64 and when perfused produce a water-dispersion effect, called intra voxel incoherent motion (ivim), 65 similar to water diffusion through brownian motion, although dispersion is more rapid and has a different time dependence."
"microglia are macrophages that provide the first reaction for many cns injuries. 58 their soma is 10 μm in diameter, and total coverage (with processes) is about 15-30 μm. like astrocytes, microglia are territorial cells. 59"
"we can expect methods of parameter estimation to become more reliable, enhancing the precision of parameter maps. the dictionary-based approaches discussed in section 4.3 offer great promise and are essential for processing very large databases of images. techniques such as multi-model inference 199 are largely unexplored in microstructure imaging, but should help to avoid consistent bias in parameter estimates while avoiding over-fitting. such techniques also enable quantification of uncertainty in parameter estimates arising not just from noise, but also from uncertainty in the choice of model, thus quantifying more precisely our belief that parameter estimates reflect true underlying tissue microstructure."
"the intra-axonal space shown in figure 3b is the space encapsulated by the cell membrane of axons and contains macromolecules and proteins, as well as solid filaments and mitochondria. in axons, the cytoskeleton consists of filaments that maintain the axon's shape and internal organization, and acts as mechanical support for the intra-axonal transportation system i.e. the microtubules. the microtubules have a diameter of about 25 nm and are the intra-axonal railway transporting substances to and from the cell body, both retrograde and anterograde; they are easily seen with em."
"sde sensitizes the mr signal to diffusion using a pair of gradient pulses ( figure 6a ), which encode and decode the positions of spins. the sequence maintains a magnetic field gradient, defined by the gradient vector g, during each pulse of length δ. the onsets of the two pulses have separation δ, which determines the diffusion time. diffusion during and between the pulses leads to an attenuation of the mr signal, and this attenuation increases (i.e. signal decreases) monotonically with the variation in the distance traversed by the spins, i.e. their dispersion, between the two pulses."
"current models have several common limitations. first, at the heart of compartment models is the division of the measured signals into separate compartments. this is necessary to disentangle the signal into the contributions from the various underlying cellular components, but the validity of the division is difficult to assess directly. while experimental evidence suggests that the wm signal can be divided into intra-and extra-axonal ori- 63 and dendrites to branch 165 (see figures 4 and 5) . the impermeability assumption may be reasonable in healthy wm over the typical timescales of diffusion mr experiments, but is probably violated in pathology. 70, 94, 166 fourth, extra-axonal space is often assumed to exhibit time-independent hindered diffusion and is modelled by a diffusion tensor, sometimes with a tortuosity model. however, in vitro experiments 167, 168 suggest that the extracellular space can exhibit non-gaussian diffusion in a densely packed environment. further studies 169, 170 suggest that the time dependence of extra-axonal diffusion might not be negligible for experiments involving diffusion times of about 10 ms to about 100 ms. various models assume fixed or otherwise constrained diffusivity parameters, but recent evidence 6, 156, 171, 172 suggests violations of this assumption in the brain. models for brain tissue often neglect the ivim effect (see section 2.1), which again may be reasonable for healthy tissue, but can be significantly altered in pathology."
"another enhancement to abms and to simulation in general arises when data analysis augments the simulation. for example, researchers have analyzed data to identify best scenarios extracted from discrete event simulation of an ed [cit] . although scenarios were extracted from a des as compared to an abm, the same type of enhanced data analysis are beginning to emerge in abm, borrowing heavily from nonparametric methods in operations research. although abms are a useful paradigm for aiding to the understanding of a complex system on their own, this significant existing contribution will be augmented with the integration of data analytics."
"finally, we note the importance of out-of-sample measurements in model selection (and verification). often different or new types of measurement can reveal weaknesses or differences among models. for example, recent results from spherical tensor encoding 6 or from very rich data sets with high b-values 173 show that the noddi model does not explain the full range of signals, particularly from grey matter, despite it fitting data from standard clinical acquisition protocols fairly well."
"more recently, others have modeled improvements to patient flow using an abm running on a high performance compute resource [cit] . the abm was built with netlogo and representative of the role for which an abm is well suited. the objective of the study was to provide impact values of alternative policies for patient diversion. not unexpectantly, the results indicated that patients that do not require urgent attention and are fast tracked or diverted improves the capacity of the ed and reduces the length of stay of patients that remain in the ed. more extensive consideration of abms for patient flow in eds are developed by the same researchers [cit], including the utilization of an abm within a decision support system for eds [cit] . a contrasting technique to model patient flows would be des [cit] . in similar work, the role of re-triage in improving ed patient flow is examined [cit] . the results are not unexpected and lend additional credibility to the use of abms in healthcare modeling by facilitating and modeling ''what if'' scenarios. in other work, a pseudo-agent based approach is introduced into a des in an attempt to capture the representative strengths of each modeling approach for simulating an emergency department [cit] . in that work, the importance of interaction at the agent level is illustrated, not typically captured with des. fig. 1 illustrates where modeling nosocomial infections, or hospital-acquired infections (hais) would be characterized within the range of healthcare models."
"agent based modeling has seen a tremendous growth in many areas over the past 15 years and more recently one of these areas being hospital and healthcare settings. the primary application of abms to hospital environments examine patient flow (e.g. emergency departments) [cit] and other hospital operational issues, and using abm to examine the dynamics of infection spread within a hospital (e.g. the hospital's role in an influenza epidemic [cit] and the dynamics of nosocomial infection spread [cit] ). abms in healthcare have also examined economic models of healthcare, removed from scale of the patient itself; these models are not surveyed here."
"1. for any non-linear model, f depends on the choice of model parameters. this creates circularity: we can only optimize the design for predefined parameter values, yet we do not know the parameter values, which is why we estimate them. moreover, in mri, the parameter values vary spatially, but we can only choose one protocol common to all voxels. in practice, we break this circularity by: (i) assuming that the opti- shells each with common pulse timing. this reduces the dimensionality to 3m. even with this relatively small number of dimensions, the search requires multiple runs of a lengthy stochastic optimization to find good solutions, as the objective function has many local minima. an alternative strategy is to optimize proxy quantities, such as the maximal b-value in the minimum echo time. 120 these caveats reveal that experiment design is an inexact science at least in application to microstructure imaging. however, even the suboptimal solutions we obtain can make substantial practical differences; again see figure 7 . in fact, even in very high-dimensional experiment design problems, optimization can find useful solutions and provide important insight into the choice of protocol; see, for example, we emphasize that the experiment design strategy above is only appropriate once a suitable model has been identified. preceding steps in the development of microstructure imaging techniques can require quite different design strategies. for example, model comparison experiments (section 4.1) tend to cover the useful measurement space as widely as possible to reveal all significant and potentially useful effects. this contrasts strongly with f-based designs, which cluster measurements around a few points at which the signal changes rapidly with parameter values. other developmental steps use f-based optimization, but in different ways. for example, to establish (or rule out) sensitivity of a particular family of measurement to a particular parameter (can we measure any realistic axon diameters in white matter with sde and 40 mt/m gradients?) requires the best possible combination of measurements within the family; see, for example, reference 145. similarly, to make statements about which pulsesequence family is most sensitive to a particular parameter (is ogse more sensitive to axon diameter than sde?) requires the optimal combination of measurements within each family; see, for example, references 110 and 214. the literature on experiment design (e.g. reference 208) contains a variety of other strategies that are relatively unexplored in microstructure imaging and quantitative mri."
this section provides some background context for the rest of the article. first it provides information on the anatomy of brain tissue at the cellular scale-the primary targets for diffusion mri and microstructure imaging. second it reviews the range of diffusion mr measurements available to probe this anatomy.
"diffusion is encoded into the mr signal by time-varying magnetic field gradients. here, we focus on the most common type of pulse sequence for dmri, 66 which yields so-called single diffusion encoding (sde). 67 we will also briefly cover other encodings that can overcome some of the limitations inherent to sde; figure 6 illustrates the various pulse sequences we consider."
this section summarizes the current state of the art in models relating the diffusion mr signal to features of brain tissue through a historical review.
"although simulation and modeling in healthcare facilities is not new, agent based modeling within these settings is a relative newcomer. this survey paper focusses on hospital abms, which is an agent centric approach as opposed to more established areas of simulation which tend to the process oriented. the key differences between modeling techniques such as discrete event, system dynamics, network analysis, and abm are well-documented and to date, the majority of research in healthcare simulation has utilized monte carlo, discrete event simulation (des), and system dynamics rather than abms [cit] ."
"the myelin sheath consists of 80% lipids and 20% proteins and wraps around the axon in layers about 10 nm thick, as illustrated in figure 3 . the myelin sheath divides into segments along the axon with regularly spaced gaps called 'nodes of ranvier' or just 'nodes' . the internodal distance is approximately proportional to the outer axon diameter (i.e. myelin and axon) with a coefficient of proportionality of about 100. thus the segments are 0.2-2 mm long, 46 whereas the nodes of ranvier themselves are 1-2 μm long. 47 the myelin insulates the axon, which boosts the conduction speed along axons by a factor of about 5.5. 35 the outer diameter of a myelinated axon has an optimal ratio to the inner axon diameter (i.e. without the myelin). the ratio (inner diameter divided by outer diameter) is known as the gratio, and in normal cns simulations suggest the g-ratio that optimizes conduction speed is about 0.7. 48 figure 5 in the primate cc, the fraction of unmyelinated axons as observed with em is small compared with myelinated axons. across the mid-sagittal cc, the largest fraction of unmyelinated axons can be found in the genu (16-20%), which includes the prefrontal corticocortical projections. 32, 33 the function of unmyelinated axons is still not clearly understood, but reference 38 provides some thoughts."
"once we have a model and some data, a variety of options are available for fitting the model to the data to obtain parameter estimates. an accompanying article in this special issue 22 focuses on some aspects of this challenge, but we summarize key techniques briefly here for completeness."
"numerical simulations support investigations of the robustness of parameter estimates under ideal conditions. the basic premise is to predict the signal for a given measurement protocol, add noise and fit the model in multiple repetitions. synthesizing from and fitting back the same model can establish effects of protocol design and noise level on parameter estimates (see, e.g., reference 231), the range in which parameters can be estimated with high accuracy 84, 232 and the interplay between the hardware constraints and parameter estimates, e.g. the available gradient strength and the estimated axon diameter index, as in reference 21. such evaluations establish an upper bound for the parameter accuracy, which can be compared across sampling protocols."
"several of the models oriented to nosocomial infections are known as 'individual based models', in which agents are limited by definition to individuals (persons). one such example is a mathematical individual based model for studying infection spread in a nursing home [cit] . by contrast, the notion of an agent based model expands the definition of agent beyond an individual person, to include inanimate objects that can act as vectors of transmission for nosocomial infections. this concept is supported by a significant body of evidence that non-person agents play a significant role as infection transmission vectors [cit], including the cdc's overview of sars related information [cit], which states ''the virus also can spread when a person touches a surface or object contaminated with infectious droplets and then touches his or her mouth, nose, or eye(s). in addition, it is possible that the sars virus might spread more broadly through the air (airborne spread) or by other ways that are not now known'' (pp. 1)."
"in order to assess the degree to which model parameters capture the underlying features of the tissue, evaluations can be performed by simulations or by combined dmri and microscopy measurements in phantoms or tissues. figure 8 summarizes the spectrum of approaches. each provides a different balance between realism of microstructure and control of ground-truth values. numerical approaches typically offer high control at the expense of realism, whereas the reverse holds true for measurements in tissue."
"to summarize, models potentially incorporate many biophysical influences on the signal. however, practical acquisition protocols support estimation of relatively few model parameters. model constraints, such as ignoring certain effects, fixing certain parameters, enforcing relationships on parameter combinations or imposing prior distributions on parameter values, are unavoidable. the art of model design involves first selecting constraints that are not overly restrictive or erroneous, and second understanding the behaviour of the model when its constraints or assumptions are violated. at the time of writing, the community is far from convergence on either issue, and much debate continues over what constraints and assumptions are reasonable and how best to interpret parameters. 6, 9, 156, 173"
"tology offers the potential to study the live brain in situ in healthy volunteers or patients. the relative ease of data acquisition allows population studies that provide insight into anatomical variability. furthermore, its non-destructive nature allows repeat measurements to monitor changes during normal development or pathological processes. clinically, virtual histology avoids biopsy and the potential side effects of the invasive procedure, and provides a window on tissue changes when the risk of side effects prohibits biopsy. moreover, the wide field of view that virtual histology provides potentially reduces false negatives that may arise from, say, poor targeting of a biopsy. figure 1 compares typical images from classical histology and microstructure imaging. the clear advantage of classical histology is its level of anatomical detail; its submicrometre image resolution provides vivid insight into the cellular architecture of tissue, whereas microstructure imaging figure 1 comparison of classical histology and microstructure imaging showing a range of microstructure imaging techniques in the current literature organized by target tissue feature. a-d, imaging indices of neurite (axon or dendrite) density with classical histology from reference 4 (a) and by model-based dmri (b-d). maps show the cylinder fraction from reference 4 (b), orientation dispersion (od), neurite density index (v ic ) and isotropic fraction (v iso ) from noddi (c), 5 and isotropic fraction, 'stick density', and tissue mean diffusion from codivide (d). 6 e-g, imaging fibre orientation distribution. e, estimation of fibre directions from histology and corresponding estimates from dmri. 7 f, in vivo fibre orientation mapping using constrained spherical convolution. 8 g, combined mapping of microstructure and orientation by the spherical mean technique. 9 h-l, imaging indices of axon diameter. h, histology provides high-resolution maps enabling measurements of individual axon diameters; images from reference 10. i, estimated axon diameter distributions from diffusion mri using axcaliber in reference 10 of the in vivo rat-brain cluster into groups reflecting corresponding diameter histograms from histology. j-l, axon diameter indices from the monkey brain using activeax (j), 11 ex vivo spinal cord (k) 12 and in vivo spinal cord using 300 mt/m gradients (l). 13 m-p, imaging cell shape indices. m, classical histology reveals elongated cells in a meningioma to the left and rounder cells in a glioma to the right; from reference 14. n, fractional anisotropy from dti is low in both meningioma and glioma tumours, but the microscopic anisotropy (μfa) from divide is more specific to cell shape and shows high value in the meningioma only. 14 o,p, a similar measure of the microscopic anisotropy from double diffusion encoding in a rat brain (o) 15 and a healthy human brain (p). 16 q-t, imaging myelin density. classical histology by luxol fast blue shows reduced myelin density in the brain of a multiple sclerosis patient (q) and mri-derived maps using quantitative relaxometry show similar features (r). 17 s, mri used to track the myelination in infants. 18 t, an early example of the myelin water fraction from relaxation-weighted mri 19 provides only statistical descriptions of the tissue over the extent of millimetre-sized image voxels. in some applications, rich and specific content of classical histological images is important: for example, in enabling a cancer histopathologist to identify the presence of minute fractions of mitotic cells. however, many tasks that histologists perform seek broader statistical changes over a relatively wide extent of tissue. for example, the density and diameter distribution of axons in a white matter pathway determine its information-bearing capacity; different densities, shapes and configurations of cells discriminate different types of brain tumour; widespread protein deposits are hallmarks of alzheimer's disease. in such applications, the precise detail of cellular architecture is less important and the benefits of microstructure imaging can significantly outweigh those of traditional histology."
"(rather than one 90°and one 180°pulse as in pgse) that excite, store and recall the magnetization 73, 74 (compare figure 6, panels b and c). in pgste, only t 1 relaxation, which is slower than the t 2 relaxation pertinent in pgse, takes place between the second and third 90°pulses; pgste thus retains more signal at longer diffusion times. however, in pgste half of the signal is lost in the storage and recall process compared with pgse. 75 thus, pgste has an snr advantage over pgse only when the time between the gradient pulses exceeds ln"
"abms provide a natural description of a system that can be calibrated and validated by representative expert agents, and is flexible enough to be tuned to high degrees of sensitivity in agent behaviours and interactions. as such, they play a vital role as an information translation vehicle. the lexicon used to develop an abm is the lexicon of area experts and of the institution under consideration (e.g. a hospital), reflecting the world in a real and specific a manner as possible. in essence, one builds a laboratory where the behaviours of individuals are similar to those in the real-world emergency department and the one observes what happens when the rules of behaviour and interactions are changed. the underlying abm engine may be quite complex and utilize the most advanced processing and hardware techniques available, but this level of detail is not required in developing the model or in the analysis of its output."
"microstructure imaging relies on a model that relates microscopic features of tissue architecture to mr signals. in general, the approach acquires a set of images with different sensitivities and fits a model in each voxel to the set of signals obtained from the corresponding voxel in each image. the process yields a set of model parameters in each image voxel, which constitute parameter maps of microscopic tissue features. diffusion mri is a key modality for microstructure imaging, because of its unique sensitivity to cellular architecture. the technique sensitizes the mr signal to the random dispersion of signal-bearing particles, typically water molecules, over diffusion times from the millisecond range up to around one second. the mean free path over this time at room or body temperature is in the micrometre range, i.e. the cellular scale, so the cellular architecture of the tissue strongly influences the dispersion pattern of the molecules. thus diffusion mr measurements support inferences on tissue microstructure."
"in general, the modeling of hai or nosocomial infections is perhaps the best suited area for abms within healthcare institutions. this is largely a consequence of being able to address all of the model components included in fig. 1 figure 1 . an agent based nosocomial model within healthcare models."
"the fisher information matrix 207 applications of these ideas in quantitative mri include dixon imaging, 209 quantitative magnetization transfer 210 and arterial spin labelling. 211 in diffusion mri microstructure imaging, reference 145 outlines a framework for optimizing the design in this way. the acquisition protocols for activeax in references 11 and 21, noddi in reference 5 and filter exchange imaging (fexi) in reference 212 all use this approach. these optimized designs can produce substantial improvements in parameter maps over ad hoc designs; see figure 7 for an example."
"recently, various techniques move away from gradient descent to use linearized fitting routines, convex optimization or dictionary-based methods. these can avoid local minimum problems and dramatically reduce computation time, at the cost of some precision of the final estimates."
"some early simulation models within healthcare settings were not specifically denoted as abm but carry all the characteristics of abm. [cit], researchers discussed a simulation model of an ed, recognizing the strengths of abm as a means of communication across disciplines, indicating that part of their validation process was consultation with area experts (doctors and nurses). the model provided a means evaluating ''what if scenarios'', specifically, alternative triage methods. their work was also one of the first to recognize the importance of visualization as a strong element of abm and the requirement of using as real data as possible of available [cit], others argued for the use and requirements of abm for hospital management, although they do not appear to have built a working model [cit] . another early paper investigated the role of an abm within an ed, deriving it from a more traditional intelligent software agent perspective or multi agent system (mas) perspective [cit] . that work introduces processes, treatments, individual agents and protocols for their interaction."
"in several gm areas, including hippocampus and parahippocampal gyrus, which explains the reduction in mean diffusivity (md) observed from the corresponding dti data. in addition, they show that the percentage change in dendritic density index induced by the learning task is consistently larger than the percentage change in md. this example highlights the increased sensitivity for detecting subtle microscopic changes with specific markers of microstructure. however, since charmed is a wm model and does not explicitly model orientation dispersion, it may lead to biased estimates of dendritic density in gm and axonal density for wm with non-negligible orientation dispersion."
"model-based diffusion mri aims to provide biologically more specific interpretation than standard techniques such as dti. for example, the range of model-based approaches reviewed in the previous section aim to map, and support subsequent analysis of, indices of specific microstructural features that dti conflates, such as neurite density, axonal diameter and neurite orientation distribution. this has great appeal for studying normal brain development, maturation and aging, as well as understanding a broad range of brain disorders. this subsection reviews current examples of such applications to biologically motivated questions."
"noddi has since been applied to study normal brain development, 179 chart the trajectory of brain maturation [cit] and investigate neurodevelopmental disorders, 183,184 neurodegeneration 185,186 and other neurological disorders. 187, 188 the simplified two-compartment charmed model in reference 142 has also been increasingly adopted to enhance our understanding of brain development, 189, 190 neurodegeneration 191, 192 and neuroinflammation. [cit]"
iii. theoretical development of the model must go hand in hand with the design and choice of sequence and measurement protocol to ensure that the data acquired provide sensitivity to the parameters we intend to estimate.
"an evolving literature exists with respect to applying abms, alone or in complement to other techniques, to the operations of eds. in general, this literature addresses system-level performance dynamics, quantified in terms of patient safety [cit], economic indicators [cit], staff workload and scheduling [cit], and patient flows. while much of the literature addresses system-level operational concerns during periods of typical operation or stasis, there is also a literature on modeling of healthcare operations during critical incidents like disease outbreaks and terrorist attacks [cit] ."
"the standard procedure is to use maximum likelihood estimation, typically via non-linear fitting such as gradient descent, independently in each image voxel. standard implementations of noddi, axr imaging and axcaliber all use this approach and typically report a single best-guess parameter estimate in each image voxel, although gradient descent techniques do often provide an additional measure of confidence in each parameter estimate, which can be useful. sampling techniques, such as markov chain monte carlo (mcmc), sample the posterior distribution on the parameter values, which can provide a more complete picture of confidence in each parameter estimate as well as avoiding local minima problems associated with gradient descent; activeax uses a multi-stage fitting process involving gradient descent followed by mcmc sampling. the use of averaging across direction prior to fitting, 9, 90, 171, [cit] as in the spherical mean technique 9, 171 or methods that compute higher-order moments, 173, 219, 220 can provide invariants to the fibre orientation distribution sensitive only to fibre composition. this can avoid fitting to very large numbers of measurements by non-linear optimization, which increases speed and enhances stability."
"the b-value summarizes the overall diffusion weighting of a sequence and, for the sde sequence some unconventional implementations of sde can offer distinct practical benefits. for example, the level of eddy currents can be reduced by implementing sde in a double spin-echo sequence or with asymmetric gradients. 71, 72 while sde is often implemented as a pulsed gradient spinecho (pgse) sequence ( figure 6b ), the pulsed-gradient stimulated-echo sequence (pgste) can provide longer diffusion times than standard pgse ( figure 6c ). in pgse, the diffusion time is limited by t 2 relaxation, since the snr decays with t e as exp(−t e /t 2 ). pgste comprises three 90°pulses"
"a major new avenue for microstructure imaging in the future is to combine information from diffusion mri with that from other mr contrasts or even other modalities. various other quantitative mr techniques offer complementary information on tissue microstructure to diffusion mri, and the construction of models informed by multiple contrasts has the potential to resolve ambiguities that are intrinsic to single-modality approaches."
"abms tend to be labour intensive and are often deployed for specific experiments or studies. although time consuming, they generate vast quantities of data for each run. typically, the many runs are used to extract statistics that can be used to demonstrate the impact of the policy or intervention being simulated. this massive data generator also offers the potential to be mined and used in machine learning or pattern classification algorithms. for example, instead of having emergency physicians travel through the ed to see patients in individual treatment rooms, the patients would travel through the ed to visit the (stationary) physician, with this policy generated via a genetic program combined with an ed abm [cit] ."
the various steps above are all important in the development of successful microstructure imaging techniques. figure 9 illustrates broadly how the different steps fit together. the key messages are the following.
"for free diffusion, the diffusion coefficient alone determines the range of distances for a particular δ and δ. the degree of attenuation thus provides a direct estimate of the diffusion coefficient. however, in restricted diffusion, the maximal distance any spin can travel between encoding and decoding is limited, which in turn limits the signal attenuation. the attenuation depends on the restriction distance and, in multiple dimensions, the shape of a restricting pore. thus, multiple sde (or other) measurements obtained by varying the different parameters of the sequence, i.e. δ, δ and g, inform estimates of size and shape of restricting pores; see references 68-70 for examples."
"iv. very good arguments must be identified for increasing model complexity over the simplest available models that reasonably explain the trends in the data (as described in section 4.3 figure 9 microstructure imaging development pipeline. we identify three phases. a theoretical phase first gains an understanding of the microstructure of the tissue of interest, it then brainstorms measurements (i.e. choice of sequence and protocol) likely to provide sensitivity to the imaging targets within that tissue, and similarly brainstorms candidate models linking these measurements to tissue features. an experimental phase then acquires data to test, compare, validate and select working models and acquisition protocols. finally, an application phase develops user-friendly software enabling widespread uptake of the technique. this must involve work with end-users in real-world applications to turn prototype code into a working system, understand user behaviour and provide the necessary education to use the technique appropriately. the phases, as well as the steps within them, are mutually informative, so the pipeline is iterative rather than linear measurements may incur unexpected and easily missed artefacts. benefits and drawbacks should thus be analysed and tested carefully."
"this paper reviews the current status of and advocates for the increased use of abms within healthcare settings, particularly within hospitals. in this context, abms of nosocomial infection spread are among the most advanced and numerous at this time, with an emerging body of work associated with abms investigating patient flow and other operational processes in hospitals. however, abms are not without their disadvantages as well. some of these disadvantages are related to developing robust validation and verification techniques which the abm research community agrees upon; this is a difficulty faced by many simulation modalities. other difficulties arise from the challenge of generating accurate models of agent behaviours and interactions, as well as data extracted from the systems being modeled. the emergence of abms will likely be within a more integrated simulation and analysis suite, often combined with other established techniques as demonstrated within the more recent literature. the role of abms as useful simulation vehicles within healthcare facilities is still in its infancy, but offers tremendous potential for the better understanding and optimization of these complex systems."
"ii. theoretical work to understand tissue structure and signal generation are necessary to find good models, but not sufficient. empirical steps are essential to refine candidate models that theoretical brainstorming identifies. this includes both statistical model selection to reject models that do not explain the data, and validation against independent measurements to find models that best estimate histological parameters."
"relative to topography and agents. hai abms may be useful in assessing the effectiveness of different infection control protocols or policies, intervention costs, as well as shedding light on potential confinement failures which would accompany widespread infection dynamics [cit] ."
"the image resolution of mri typically provides voxels of a few mm 3 in volume, from which we aim to draw microanatomical statistics such as anisotropy, cell sizes and axon diameters-features in the micrometre length scale. axons often extend across many voxels, and each voxel can contain hundreds of thousands of axons, which can adopt a wide variety of configurations, e.g. bending, fanning, crossing etc., 7, 62, 63 as illustrated in figure 5a . moreover, axons are not straight even within a single voxel, as figure 5b illustrates, which further complicates the task of modelling the geometry of axons at the millimetre scale."
"illustration of gradient waveforms and sequences used for diffusion encoding. a, the single diffusion encoding (sde) sequence consists of a pair of pulsed gradients defined by three parameters δ, δ and g that together define the b-value. the encoding gradients must be implemented in a pulse sequence. b, the spin echo (se) sequence is composed of an excitation pulse (90°) and a refocusing pulse (180°). c, the stimulated echo (ste) sequence replaces pgse's 180°pulse with two 90°pulses that store and recall the magnetization. the effective sign of the gradient is reversed by the 180°pulse in pgse or by the last two 180°pulses in the pgste sequence, explaining the difference between the effective waveform in a and the actual waveform in b and c. panels a,d-g show the effective waveform. d, the double diffusion encoding (dde) sequence has two pairs of pulsed gradients, separated by a mixing time t m . the gradient directions of the two blocks may differ. oscillating gradients, with either smooth (e) or square (f) waveforms, can provide short encoding times when performed with high frequency f. g, irregular gradient waveforms can also be used, for example, to obtain isotropic diffusion encoding (a) (b) the mechanism that attenuates the signal is phase dispersion: the phase of each spin corresponds to the distance it moves in the direction of the gradient; a wider range of distances (i.e. greater dispersion) leads to a wider range of magnetization phases contributing to the signal; with greater phase dispersion, the magnetizations have a lower sum, so we observe greater attenuation of the net signal."
"additionally, unlike the charmed model, the geometry of individual cylinders is not explicitly modelled. instead, an apparent transverse diffusion coefficient is used to reflect the combined effect of cylinder radius, bending, undulation, non-vanishing permeability etc. similar to the charmed model, the extra-neurite diffusion is modelled with a general diffusion tensor."
"as we mentioned in section 3.2, the range of applications of microstructure imaging techniques in the brain is expanding rapidly. the idea further extends naturally to non-brain applications, although different models are required to explain the signal. recent developments in non-brain cancer imaging [cit] extend the paradigm from the brain with extremely promising results. many other opportunities are available in, for example, imaging the heart, muscle tissue, liver, kidney, lung, placenta and many other solid organs. the appetite for such techniques from clinicians and biomedical researchers can be very high, because they rely solely on painstaking histological analysis for specific information on tissue structure. however, it can take time to educate such users in the precise capabilities of such techniques. as non-invasive approaches become more available and popular, we must be careful to understand their limitations and educate users of such limitations to avoid misinterpretation. nevertheless, exposing techniques to users is essential to identify both potential and limitations-we hope an understanding of the pipeline outlined in figure 9 can help to balance these competing pressures and expedite the development of powerful future microstructure imaging tools."
"there are however intermediary code bases that are typically open and community supported. these are usually verified to some degree but usually not to the degree of a commercial offering. all forms of abm development have associated learning curves. the largest and most popular commercial abm offering is that within anylogic (anylogic.com). opensource abm frameworks include repast (http://repast.sourceforge.net/), netlogo (http://ccl.northwestern.edu/netlogo/), and swarm (swarm.org)."
"this article reviews the current state of the art in microstructure imaging of the brain using diffusion mri. we thus focus on diffusion mri techniques that aim to estimate and map tissue properties via biophysical models and mention only in passing diffusion mri techniques based on signal models, which other parts of this special issue cover in more detail. 22 this kind of technique has reached an important turning point in recent years with its transformation from largely a technical research topic to widespread application in biomedical studies. with this in mind, the review aims to emphasize practicalities of developing microstructure-imaging techniques designed for front-line application while giving a critical review of the state of the art. thus section 2 provides some background information on brain anatomy at the scale we are sensitive to with diffusion mri together with the nature of the measurements we make. section 3 then reviews the state of the art in models underpinning current microstructure imaging techniques and the range of current applications. section 4 focuses on practical issues in the development of microstructure imaging techniques, specifically model selection, experiment design, parameter estimation and validation. that section concludes with an outline of the microstructure-imaging development pipeline. finally, section 5 discusses the future of diffusion mri microstructure imaging of the brain highlighting opportunities for future research, development and application, and considers the wider perspective of applications outside the brain and exploiting contrasts other than diffusion mri."
"simulation offers a potential to identify improvements and new understandings in how a facility operates. simulation has the potential to models real-world variability, lessens testing and implementation costs of planned changes, and helps to minimizes the risk of errors in implementing changes. patient tracking through their stay in the hospital by using technologies such as radio frequency identification (rfid) and improved electronic reporting and dashboards are one example of initiatives that can be integrated with simulation studies to generate valuable information on social dynamics within the institution."
"(ii) optimizing acquisition strategies and imaging protocols to yield acceptable scan times; (iii) identifying specific applications in which they offer a clear benefit; and (iv) constructing sufficiently stable and parsimonious models that relate their signals to useful tissue features. immediate applications do arise in verifying, or highlighting limitations of, models designed using only standard sde sequences, as in reference 6."
"this paper surveys the application of agent based modeling (abm) and simulation of complex social dynamics within the institutional scale of a hospital. hospitals are a promising area of continued abm research with the concomitant potential for substantive outcomes. healthcare around the world deals with a perennial pressure to find cost efficiencies, and target areas include the optimization of healthcare processes and flow, reducing emergency department (ed) wait times and length of stay, and reduce admission times. within these areas, hospitals rely on the experience of practitioners for improvements in triage procedures, diverting low-acuity patients, reconfiguring the healthcare staffing model, and reorganizing operational units both physically and procedurally."
"nosocomial agent based modeling initiatives focus upon explicit modeling of structure and behaviour extending the agent based model to include individuals, inanimate objects, and locations, in order to investigate an organization's policies and practices in the event of a serious nosocomial infection outbreak. much of the current efforts in nosocomial abms set the framework for potential future efforts in modeling and evaluation of organization's documented infection control plans (policies and practices). for example, best practices [cit] are available for healthcare practitioners and policy makers dealing with health care-associated infections in patient and resident populations. this may be a useful reference to model, as a means of identifying and evaluating their effectiveness. at this time, best practice documents typically reflect ''consensus positions on what the committee deems prudent practice and are made available as a resource to the public health and healthcare provider'' (p. ii). clearly this is also an opportunity for abm models to contribute to a collaborative, multi-stakeholder effort."
"in general agent based modeling is 'bottom-up' systems modeling from the perspective of constituent parts. systems studied are modelled as a collection of agents (in social systems, most often people) imbued with properties: characteristics, behaviours (actions), and interactions that attempt to capture actual properties of individuals with a high degree of diversity and fidelity. in the most general context, agents are both adaptive and autonomous entities who are able to assess their situation, make decisions, compete or cooperate with one another on the basis of a set of rules, and adapt future behaviours on the basis of past interactions. agent properties are determined by the modeler and are ideally derived from actual data that reasonably describe agents' behaviours -i.e. their movements and their interactions with other agents. the emergence of a data culture, also called 'big data' and associated 'big data analytics', offers new opportunities to use real world data, even in near real time, as inputs into abms. the modeler's task is to determine which data sources best govern agent profiles in a given abm institutional simulation."
"the structure of the dendritic tree and its branches, their extent and architectural outline depends on the type of neuron. 25 in general, relatively few (typically one to five) dendrites extend from the soma body itself, but the total number of branches varies from four to more than 400. the cerebellar purkinje cells have the most branches. the spatial extent of the dendrite tree ranges from 15 to 1800 μm radial distance from the soma to the tip of the most distal dendrite. dendrites from each neuron strongly intermingle with those from other neurons in their neighbourhood to form a dense and complex dendritic network. 25 the golgi-cox stain, as shown in figure 4, visualizes just a fraction of neurons so does not reveal the full complexity of the dendritic network, but does highlight the variety of shapes of the dendritic tree. figure 4f also shows small protrusions from the main shaft of the dendrites, which are called dendritic spines; see reference 30 for a review of their structure and function. for neurons in the cerebral cortex, the dendritic trees are mostly isotropic, extending and branching evenly in all directions, whereas elsewhere, e.g. in the layers of the hippocampus, the trees can be highly anisotropic to support inter-layer connectivity. 25, 26"
the remainder of this paper is organized as follows. section ii surveys the application of abms to hospital and similar institutional settings. section iii discusses data sources that may be useful in extending the models more fully. section iv provides reference examples that encompasses many of the phenotypes of a typical hospital centric abm. section v provides a summary.
"despite the great promise, current microstructure-imaging techniques remain simplistic with clearly identifiable limitations. a variety of paths are available for amelioration of current limitations and for greater advances towards the next generation of quantitative microstructural imaging techniques."
"the distance between the two nodes in the network can be calculated using the vertices (x i, y i ) and (x j, y j ) of the nodes. the formulated distance is shown in equation (2). the distance between the intermediate nodes can be calculated by equation (3)."
"in sa algorithm, we should ensure the processing solutions are the right strategies of edge server placement, which means the solutions must represent an i-tier dominating set of the related graph g."
"there is only a small amount of research on the edge server placement problem in wman and most of them ignore the budget limitation of mec service vendors. in this paper, we focus on saving the cost of deploying edge servers in a wman, and we address it as the problem of minimizing the number of edge servers while ensuring some constraints related to network qos. then, we present an ilp formulation for the edge server placement problem and divide it into two categories of problems according to the capacity constraint. due to the poor scalability of the ilp, we extend the definition of dominating set in graph theory and transform the problem into the minimum i-tier dominating set problem. furthermore, we propose two heuristic algorithms and a greedy based algorithm to solve these two kinds of problems, respectively. the simulation results show that the proposed algorithms are promising. for our future work, it is interesting to investigate how to use other existing networks, such as 4g network, to deploy edge servers efficiently."
"we define a temperature t f in to form a stopping condition. when the temperature is lower than t f in, our algorithm will terminate."
"manets are designed with several nodes connected by wireless links without any specific infrastructure. the mobile devices are freely moved in all direction. due to the absence of centralised architecture controller, there is no central monitoring agent in the network. in ad-hoc network all the nodes are ready to forward the data to other nodes but which node should forward the data was decided dynamically based on the network connectivity. routing in mobile ad-hoc network is difficult due to occurrence of frequent link failures, therefore routing protocols were designed effectively to manage the route or link failure by applying some re-routing techniques. multipath routing provides number of alternative paths in case any route failure occurs between the source and destination nodes in the network. if channel capacity of the single path routing overloaded due to heavy network traffic congestion then multipath routing can be done for balancing the network congestion occurred."
"the revise phase can be manual or automatic depending on the output values. the automatic review is given for nonsuspicious cases during the estimation obtained for the reuse phase. for cases detected as suspicious, with output values experimentally determined in the interval [0.35, 0.6], a review by a human expert is visually performed. as cbr learns, the interval values are automatically adjusted to the smallest of the false negatives. the greater limit is constantly maintained throughout the iterations. the review consists of recovering queries that are the most similar to the current one together with their previous classifications. this combines a clustering technique for the selection of similar requests with a neuronal model for the reduction of dimensionality, which permits visualisation in 2d or 3d as shown in figs. 3 and 4 ."
"the selection of similar cases is carried out through the use of a neural gcs network. the different cases are distributed in meshes and the mesh containing the new case is selected. to visualize the similar cases (those in the selected mesh), the dimensionality of data is reduced by means of the cmlhl neural model [cit] which performs exploratory projection pursuit by unsupervised learning. considering an n-dimensional input vector (x), and an mdimensional output vector (y), with ij w being the weight (linking input j to output i), then cmlhl can be expressed as:"
"there exist a few studies on the edge server placement problems [cit], and most researchers overlook the problem of how to minimize the number of edge servers while ensuring delay constraint, which is vital to user experience and service providers. since the budgets of mec service vendors are always limited, it is impossible to deploy edge servers everywhere and it is encouraged to reduce cost as much as possible instead. the cost of deploying edge servers is mainly related to two factors [cit], which are site rentals and computation demands. the former factor means that the more locations are selected to deploy edge servers, the higher is the cost. the latter factor tells us that the greater is the computation demand, the greater is the number of edge servers, which will result in higher cost. in practice, at the beginning of a project, the vendors often do not have enough funds to support the best service, but just to meet the basic needs of users for reducing cost as much as possible. for example, a mec service provider is planning to deploy the mec systems to serve all the population in a city. regrettably, the capital of this mec service provider is restricted, which means that it is impossible to deploy edge servers everywhere. the urgent matter is to economize while ensuring the basic requirements of users, such as access delay. redesigning a wholly new scheme is always expensive and impractical. in addition, edge server deployment scheme is directly connected with the business models, and different deployment schemes will result in different business benefits. fortunately, we can utilize the legacy of existing technologies to deploy mec service effectively and economically. wireless metropolitan area networks (wman), consisting of a large number of wireless access points (aps), has attracted attentions of academic areas naturally due to its available infrastructure [cit] . it is certain that deploying edge servers on existing aps can save more funds than deploying them on new places."
"the retrieval phase is broken down into two phases; case retrieval and model retrieval. case retrieval is performed by using the query_category attribute which retrieves queries from the case memory (c r ) which were used for a similar query in accordance with attributes of the new case c n . subsequently, the models for the mlp (mlp r ) and svm (svm r ) associated with the recovered cases are retrieved. the recovery of these memory models allows the improvement of the system's performance so that the time necessary for the creation of models will be considerably reduced, mainly in the case of the ann training."
"as previously mentioned, the aiida-sql agent is an specialization of a cbr agent which is the key component of a multi-agent architecture and is geared towards classifying sql queries for the detection of sql injection attacks. below, a new classification mechanism incorporated in the internal structure of the aiida-sql agent is explained in detail."
"we analyzed the impact of degree constraint d q on the number of edge servers by fixing the network size at 300 and delay constraint at three hops. the cluster size constraint was relaxed to eliminate the interference. as shown in figure 12, both gmedswc and gmeds are significantly affected by the degree constraint where the number of edge servers decreases as the degree constraint increases. this is because every ap can connect more aps when the degree constraint increases. in addition, we can also see that both curves have a gentle trend, which indicates that, when the degree constraint reaches a specific value (here the value is 6), its impact will be weakened."
"the constraint in equation (1) ensures that each edge server has enough capacity to process all the computation tasks offloaded to it, and the inequality in equation (2) ensures that the distance between each ap and the corresponding edge server is no more than the distance upper bound as to satisfy the access delay requirement. the inequality in equation (4) is the degree limitation and the inequality in equation (5) represents the cluster size constraint. the inequality in equations (2), (4) and (5) are considered for the requirements of network qos. equation (3) denotes that each ap can only be assigned to only one edge server. the inequality in equation (1) represents the computation demand constraint, and the inequality in equation (2) is the delay constraint. if different values are assigned to c(j), it is the definition of esppoc problem. otherwise, it is the esppwc problem. in equation (10), neighbor(i) is defined as the set of node i's neighbors in graph g, and deg(i) is the number of neighbors that are in the same cluster with node i. for the poor scalability of ilp, in later sections, our goal is to solve esppoc and esppwc problem, respectively. we transform them into the minimum dominating set problem in graph theory and appropriate algorithms are devised according to their features. finally, an example of our solution is given."
"in recent years, portable devices such as mobile phones, tablets and laptops have dominated most people's daily lives due to their convenience and powerful functions. unfortunately, the performance of these portable devices appears poor, as the applications such as face recognition, natural language process and augmented reality are becoming more and more compute-intensive. moreover, mobile devices themselves are restricted by their own battery lives, computation capacities and sizes. mobile cloud computing (mcc) [cit] has been widely used to cross the barrier mentioned above, whose main idea is that offloading the compute-intensive tasks of mobile devices to remote clouds with substantial computation resources rather than executing them locally. although mcc is fruitful, it incurs new problems. on the one hand, the distance between mobile devices and remote clouds might be too far to tolerate. on the other hand, a tremendous number of mobile devices connect to remote clouds, and compete for computational resources with each other, which can lead to network congestion. both sides will make the user experience terrible, since most applications are not only compute-intensive but also latency-sensitive. a promising approach to overcome the shortages of mcc is to bring computation and storage resources to the edge of network, which means enabling the edge of network to be the optional destinations for task offloading, and mobile edge computing (mec) [cit], cloudlet computing [cit], and fog computing [cit] are such technologies proposed to utilize the edge of the network to mitigate the burdens of remote clouds and reduce delay [cit] . in this paper, for convenience and consistency, we refer to those devices with rich resources (e.g., a mec server, a fog node or a cloudlet) as edge servers while ignoring specific differences among them [cit] . with the help of edge servers, mobile devices can offload all or part of their assignments to them without caring about what will happen, and they just need to receive the execution results [cit], which augments the performance of mobile devices and relieves the burdens of remote clouds greatly."
selecting minimum hop counts alone will not be able reduce the transmission delay and packet loss. in order to deliver the packets efficiently with less loss rate and interference; crin are selected based on the node movement which moves towards the destination and longer time link connectivity of the nodes. selecting the crin node based on reliability should be closest to the destination as shown in figure 1 .
"the remaining parts of this paper are organized as follows. related works are reviewed in section 2. the network model, integer linear programming (ilp) formulation and the definitions of aforementioned two categories of problems are presented in section 3. two heuristic algorithms are proposed for the first kind of problem in section 4. then, a greedy heuristic approach is introduced for the second kind of problem in section 5. an example of the solutions of these two problems is given in section 6. simulation results are provided to evaluate the performance of the proposed algorithms in section 7. section 8 concludes the paper and future works."
"the rest of this paper is structured as follows: section 2 describes the problem that has prompted most of this research work. section 3 explains the internal structure of the aiida-sql agent used as a classifier and visualizer agent. finally, the conclusions and experimental results of this work are presented in section 4."
"this paper introduces the hybrid intelligent agent named aiida-sql (adaptive intelligent intrusion detector agent), designed according to the strategy of an intrusion detection system (ids) and aimed at detecting sql injection attacks. aiida-sql is characterized by the integration of a casebased reasoning (cbr) engine which provides it with a great level of adaptation and learning capability, since cbr systems make use of past experiences to solve new problems [cit] . this is very effective for blocking sql injection attacks as the id mechanism uses a strategy based on anomaly detection [cit] ."
"a simulation scenario with 50 nodes is deployed and configured as the subterranean wireless network in order to validate the method proposed and their specifications is mentioned in the table 1 . programming in c++ and object-oriented tool command language (otcl) is done to determine the locations of various targeted nodes. the performance of the existing system lrd is compared with the proposed crin system, which is presented in this paper using simulation results. in order to analyse the performances, the data delivery rate, data loss rate, delay rate, normalised routing overhead is compared through simulations."
"in addition to the cbr motor in the aiida-sql agent's internal structure, an integrated mixture of an artificial neural network (ann) and a support vector machine (svm) is used as a classification mechanism. by using this mixture, it is possible to exploit the advantages of both strategies in order to classify the sql queries in a more reliable way. finally, to assist the expert in the making of decisions regarding those queries classified as suspicious, a visualization mechanism is proposed which combines clustering techniques and unsupervised neural models to reduce the dimensionality of the data."
"once the average speed of the nodes is estimated, then the availability of the link between them needs to be determined for efficient routing of data. the availability of link is found between all the nodes. the probability of identifying the link status between the nodes is done by identifying the total link availability time t l by considering the average speed between the nodes. the continuous availability of the link can be determined if both nodes of the link keep their movements unchanged i.e., speed and direction. the link availability and their quality are well determined by the available distance between the nodes. the nodes within the higher transmission range have higher quality. the probabilistic partial prediction of link availability can be obtained by considering the distance between the relay nodes from the source node. the source node selects the intermediate node x at distance (x, y) from the destination within the transmission range (t r ),"
"in manets the nodes move freely from one place to another. there is no any centralised infrastructure support hence the nodes are self organised and self healing. due to fast movement of nodes, frequent link breakage occurs between them in the network. if src and dest nodes are within transmission range, data sent directly to the destination. if source and destination are out of transmission range then intermediate nodes are required to deal the packet transmission. in order to find an efficient route between source and destination the weight of the links should be calculated based on the speed of the node movement. once the weight of the link quality estimation is done, then better quality links are selected from source to destination. based on the node speed and movement i.e., mobility preference, the core relay intermediate nodes (crin) are selected to pass the data to the destination."
"to empirically test the proposed approach with a real-life dataset, legal queries were sent from a designed user interfaces of a made-to-measure web application. sqlmap 0.5 [cit] was used to automatically dispatch malicious queries. this tool is able to fingerprint an extensive dbms back-end and retrieve remote dbms databases."
"as discussed in the above section, gmeds and sameds are proposed to solve the esppoc problem where edge sever vendors can customize the capacities of edge servers according to the historical statistic information. unfortunately, these two algorithms are improper when there are constraints on the capacities of edge servers. to this end, we design a new greedy-based minimum extended dominating set algorithm with capacity constraint (gmedswc) to solve the esppwc problem. similar to gmeds, we apply i-tier dominating set to divide wman into disjoint clusters in gmedswc. the main difference between esppoc and esppwc is that there are constraints on the capacities of edge servers in esppwc problem."
"to minimize the number of edge servers while ensuring latency constraint is an analog of the minimum dominating set problem in graph theory is a classical np-hard problem. here, we first introduce the basic concepts, then extend the definition of the dominating set, and use it to describe the edge server placement problem. the traditional definition of dominating set is given as follows."
"obviously, v is always an instance of i-tier dominating set for any given i, and calcost(v, i) is equal to the number of nodes in v all the time. therefore, v is selected to be the initial solution of sameds."
"an sql injection attack takes place when a hacker changes the semantic or syntactic logic of an sql text string by inserting sql keywords or special symbols within the original sql command which is executed at the database layer of an application [cit] . different attack techniques exist which include the use of sql tautologies, logic errors / illegal queries, union query and piggy-backed queries. other more advanced techniques use injection based on interference and alternative codification [cit] . the cause of the sql injection attacks is relatively simple: an inadequate input validation on the user interface. as a result of this attack, a hacker can be responsible for unauthorized data handling, retrieval of confidential information, and in the worst possible case, taking over control of the application server [cit] ."
"in fig. 1, the different stages applied in the reasoning cycle can be seen. in the retrieval stage, there is a selection of queries sorted by their type and the memory classification models. in the reuse phase, as seen in fig. 1, a multilayer perceptron (mlp) and an svm are simultaneously applied to carry out the prediction of the new query. subsequently, a new inspection is performed which can be done automatically or by a human expert. in the case of the query resulting as suspicious, further visual inspection will be carried out by a human expert. at this stage, the most similar cases are selected by means of a growing cell structures (gcs) network [cit], and visualized by the unsupervised neuronal model named cooperative maximum likelihood hebbian learning (cmlhl). as a result, the human expert will graphically see the relationship between the suspicious query and the recovered most similar queries. during learning, memory information regarding the cases and models will be updated. below, the different stages of the cbr reasoning cycle associated with the system are described in more detail."
"to compare the success rates, a test on the classification of queries was conducted by applying the following classifiers: bayesian network, naive bayes, adaboost m1, bagging, decisionstump, j48, jrip, lmt, logistic, logitboost, multiboosting adaboost, oner, smo, and stacking. the different classifiers were applied to 705 previously classified queries (437 legal queries and 268 attacks). the consecutive process to carry out the output test was the following: selecting one of the cases, extracting it from the set, conducting the model starting from the remaining cases and classifying the extracted case. this process is repeated for each one of the cases and techniques in order to analyze each query without it being used to build the model. the performance of the different classifiers can be seen in fig. 2 . as can be seen in fig. 2, the highest-performance system is aiida-sql, whith a success rate of 698/705. the query is a clear case of sql injection so it should be classified as an attack. the system classifies this query as illegal since an output value of 0.84 is given by the classifiers mixture. fig. 3 shows the provided visualization of queries if the manual revise phase would have been carried out. the most similar queries are recovered and depicted in different colours: legal queries are shown in green, attacks in red and the analysed query in blue. nonrecovered queries are shown in grey. the above detailed illegal query is closer to the group of queries in red, so the human expert would clearly consider this query as an illegal one. this query is not a clear attack because it is usual to have a literal in the right side of the or operator, and it can be legal or suspicious."
"algorithm 3 gives the description of gmedswc in detail. in each iteration, for the the largest coverage of aps within i hops, we first choose an ap s with the largest value of extdeg(v, i) as the head of a cluster from the uncovered aps, as can be found in lines 5-12. then, we examine whether the cluster head s can satisfy computation demands of all the aps in the cluster. if the condition is satisfied, a cluster s composed v and the aps covered by it will be built, and the aps in s will be labeled as \"covered\" and deleted from the graph g. otherwise, we iteratively remove a ap from the aps covered by s until all computation demands of s and aps covered by s is not greater than the capacity of the edge server co-located with s. in this solution, there are three strategies to rebuild the clusters: (1) big first; (2) random; and (3) small first. as the names suggest, big first strategy removes the ap with the highest computation demand, random strategy removes the ap randomly and small first strategy removes the ap with the lowest computation demand. the impact of different strategies on algorithm performance is shown in experimental results. after the cluster satisfying the computation demand constraint is built, it is then adjusted to meet the degree constraint and cluster size constraint as gmeds does. similarly, aps in the rebuilt cluster will be labeled as \"covered\" and deleted from g. our algorithm will terminate when all aps are covered."
"to solve the esppoc problem in a tolerable time, we propose a greedy based minimum extended dominating set algorithm (gmeds) to get the solution for the placement of edge servers when the delay constraint is given. then, a simulated annealing based minimum extended dominating set algorithm (sameds) is devised to prevent the local optimization of gmeds. the details of these two algorithms are discussed in this section."
"we finally investigated the impact of cluster size constraint s q on the performance of our algorithms. similar to the last subsection, the degree constraint was neglected and we evaluated the performance of these algorithms by fixing network size at 300 and delay constraint at three hops. figure 13 shows that the number of edge servers obtained by all three algorithms decreases as s q increases because each edge server will have more chances to cover as many aps as possible. likewise, the curves have same gentle tails, from which we can know that, when s q increases to a specific value, its influence on the performance of our algorithms will fade."
the reuse phase inputs are the information of the retrieved cases c r and the recovered models mlp r and svm r . the combination of mlps and svms is fundamental in the reduction of the false negative rate. [cit] .
"cbr is a paradigm based on the idea that similar problems have similar solutions. thus, a new problem is resolved by consulting the case memory to find a similar case which has been resolved in the past. when working with this type of systems, the key concept is that of \"case\". a case is defined as a previous experience and is composed of three elements: a description of the problem that depicts the initial problem; a solution that describes the sequence of actions performed in order to solve the problem; and the final state, which describes the state that was achieved once the solution was applied."
"motivated by the annealing technology in metallurgy, sa was proposed as a metaheuristic to approximate global optimization in a very large search space. it is well known that sa has the ability of jumping out from local optimization to achieve the global optimization, which can be the important supplement to the greedy algorithm mentioned above. there are six primary ingredients in sa: (1) cost function; (2) initial state; (3) neighbourhood generation; (4) cooling schedule; (5) acceptance criteria; and (6) stopping condition. given an optimization problem, we should first define a proper cost function to effectively evaluate the generated solutions. then, a initial solution is required to begin the algorithm, and neighbourhood generation methods are needed to help the solution move from one state to another. to jump out from local optimization, the cooling schedule and acceptance criteria should work cooperatively to determine the probability of accepting a new generated solution. in the beginning of sa, a worse solution has a high probability of acceptance. however, as the temperature cools down, sa merely tends to adopt a better solution. finally, during the sa procedures, the cost function value eventually converges, and the search is terminated if the stopping condition is satisfied. in this paper, we focus on devising an efficient sa method for the esppoc problem."
"given a solution s and a new solution new_s generated from s, we define the difference of cost between new_s and s as ∆, as shown in equation (12) ."
the average delay is plotted in figure 5 which shows that the delay value is lower for the proposed scheme crin than the lrd. the minimum value of delay means that higher value of the throughput of the network.
"although gmeds is able to obtain feasible results, it might get stuck at local optimum solution. therefore, we adopt a widely used heuristic, namely simulated annealing (sa), to convergence a global optimal solution. in this section, we present a brief overview of sa, followed by a detailed discussion on the operations of sameds."
"finally, the information is represented and the associated queries are recovered with the retrieved mesh, as can be seen in figs. 3 and 4 ."
"in this paper, we investigate how to make the greatest use of the legacy of existing wman to deploy edge servers efficiently and economically. it is the first time to address the problem of minimizing the number of edge servers while ensuring access delay requirement in a wman context. there are two main challenges in this problem: one is how to choose the fewer aps co-located with an edge server to provide all users with good service, and the other is how to properly assign the tasks to edge servers. in response to these challenges, we divide the wman into clusters, and the cluster heads are co-located with edge servers. in a cluster, all members offload the tasks to the cluster head. we extend the definition of dominating set, and transform the addressed problem into the minimum dominating set problem in graph theory. according to different conditions, we propose the greedy and simulated annealing based algorithms to find the optimal solutions."
"in this section the aiida-sql agent is presented, with special attention paid to its internal structure and the classification mechanism of sql attacks. this mechanism combines the advantages of cbr systems, such as learning and adaptation, with the predictive capabilities of a combination of anns and svms. the use of this mixture of techniques is based on the possibility of using two classifiers together to detect suspicious queries in the most reliable way."
"the mobility models of mobile users are designed to describe movement pattern including their locations, speed and acceleration over time. in the proposed scheme, the link quality between the nodes can be found by estimating link quality, link traffic and channel interference since the dynamic network protocol is lack in the cost metrics of link estimation and channel quality."
"by using equation (4), the value of link connectivity threshold (lct) can be fixed by estimating partial probability of link availability between the current source node and the next relay node towards the destination. the lct value is fixed so that the longer time and shorter time link connectivity nodes are determined based on the outcome of the link status. the link connectivity time between the nodes is calculated using time period as follows."
"for the largest search space, the initial temperature t 0 in sameds was set to 10,000, and the stopping condition t f in was 0.01. in this way, the temperature was always suitable to all kinds of search spaces and the work to adjust the temperature for different network sizes could be neglected. moreover, the cooling factor α was given a value of 0.99, which is in the suggested range (0.8-0.99). for comparison, we adopted the random-esppoc algorithm as the benchmark algorithm, which randomly selects an ap to deploy an edge server until the whole network is covered."
"where t 0 is initial time, t 1, t are time duration for link connectivity from initial time and t w is outage probability."
"in this section, we first describe the details of the network model. then, the precise definition of the edge server placement problem is given by formulating it as an ilp."
"the outage probability is the available signal strength between the nodes. the link failure occurs between the nodes when the nodes are out of communication range. outage probability of nodes is determined for the occurrence of link failure when they are out of range. the distance and signal strength are the two factors used to determine the link failure. longer time link connectivity of the nodes is determined as better quality nodes. if two or more paths with the value greater than the lct, then the shortest path with minimum hop counts is determined among the path to reduce the transmission delay. then the data is routed in the determined shortest path crin. the channel interference can be reduced by selecting these higher link connectivity nodes. the flow model of the proposed work is shown in the figure 2 . therefore the efficient routing is done by estimating link quality between the nodes is shown in the algorithm 1 of link quality estimation in routing."
the data delivery rate (ddr) can be defined as the ratio between the total packets delivered by the senders and the corresponding receivers in the network. it is given by equation (5); where n represents the number of nodes in the network. figure 3 the packet delivery rate of the proposed crin scheme is higher than the packet delivery rate of the existing lrd method. this crin scheme provides guarantees a high packet delivery rate because of the link broken is minimised in manet.
"it can be seen that a higher temperature has a higher probability to accept a worse solution as a new solution. however, in the end of the cooling process, only better solution will be reserved as a new solution, and it finally converges on the best solution."
"we finally investigated the impact of two important parameters on the performance of gmedswc. as described in section 5, if the sum of computation demands in a cluster exceed the given capacity of an edge server, there are three strategies to rebuild the cluster, and different cluster rebuilding strategies have different results. therefore, it is nontrivial to investigate the influence of different rebuilding strategies on the performance of esppwc. in addition, it is interesting to study the effects of different capacities of edge servers on the performance of esppwc. as shown in figures 8 and 9, big first strategy has the best performance and small first always achieves the worst performance. therefore, it can be seen that choosing a proper cluster rebuilding strategy is also an important issue. figures 10 and 11 show that the value of capacities has great effects on the performance of gmedswc, namely the higher the capacity is, the fewer edge servers are needed. in figure 10, we can see that, when the capacity of edge server is set to 300,000 mhz, the number of edge servers is 66.0% less than the situation with the capacity set to 100,000 mhz. moreover, in figure 11, an additional 108.6% edge servers are needed when the capacity of edge server is decreased from 300,000 mhz to 100,000 mhz."
"the nodes with longer time link connectivity and less channel interference are selected as crin nodes to extend the network lifetime and to improve the network performance. the speed variations are fixed preliminarily and the mobility model used here is random way point model. in random-based mobility simulation models, the mobile nodes move randomly and freely without restrictions in any directions. the velocity of the nodes is measured to find the moving direction of the node. if there is a change in velocity then simultaneously there will be a change in node's direction. so the intermediate relay nodes are selected based on their speed and direction towards the destination in a timely fashion. to be more specific, the destination, speed and direction are all chosen randomly and independently of other nodes."
"the architecture of mec is shown in figure 1, consisting of remote cloud, internet, aps and edge servers. to utilize the legacy of wman, we divide the wman into disjoint clusters (the circles made up of dashed lines in figure 1 ), which comprise different numbers of aps, and the edge servers are placed at the cluster heads. generally speaking, the sizes of the clusters are limited and different. for any ap with computation tasks needed to be offloaded, the edge server within the cluster is the first choice, and the edge servers are connected by the internet to prevent failures, thus the tasks of an ap can be offloaded to heads of other clusters if its own cluster head is in trouble or overloaded. furthermore, the remote cloud is another choice for the offloaded tasks."
the selected kernel function in this problem was polynomial. the values used for the estimation are dominated by decision values and are related to the distance from the points to the hyperplane.
"once the output values for the ann and the svm are obtained, the mixture is performed by means of a weighted average of the error rate of each one of the techniques. a weighted average is applied to take into account the error rate of each technique. if different classifications are obtained from each technique, the query would then be classified as suspicious, and subsequent revision would be launched. before carrying out the average, the values are normalized to the interval [cit], as svm provides positive and negative values and those of greater magnitude."
"the learning phase updates the information of the new classified case and reconstructs the classifiers offline to leave the system available for new classifications. the ann classifier is reconstructed only when an erroneous classification is produced. in the case of a reference to inspection of suspicious queries, information and classifiers are updated when the expert modifies the information."
average delay refers to the time difference between packets sent and packets received. it is given by equation (7); where n represent the number of nodes in the network.
"normalised routing load (or normalised routing overhead) is defined as the total number of routing packet transmitted per data packet. it is calculated by dividing the total number of routing packets sent (includes forwarded routing packets as well) by the total number of data packets received. due to longer link connectivity among the nodes, the routing overheads are seems to be minimised in the proposed scheme as the transmission rate is higher for the crin as shown in figure 6 ."
"during designing localisation algorithm for mobile sensor networks [cit] ) mobility should be taken into account. at the beginning, nodes have no knowledge about their positions. the anchor (node equipped with gps) broadcasts location packets periodically while travelling through the deployment area. the localisation process is repeated periodically. the nodes follow a rectilinear movement and these nodes have a constant velocity and direction during certain time periods and the analysis of localisation algorithm proved that by decreasing the anchor broadcasting interval, the localisation error decreased. a cross-layer distributed algorithm called interference-based topology control algorithm proposed for delay and interference control mechanism [cit] . the transmission delay, queuing delay and contention delay are taken into account in the algorithm for reducing average delay and by increasing the transmission power interference can be reduced and hop counts gradually decreases. aodv with n th backup route (aodv n th br) technique [cit] ) that provides backup routes in aodv environment to avoid overhearing of rrep packets due to neighbouring nodes and more power is required to process multiple copies of data. the distances between all the nodes are calculated using distance vector calculation and node's remaining energy is calculated and selected as backup route based on threshold value in case any route failure occurs in the routing path. link based routing protocol [cit] was proposed to establish a stable and sustainable path based on the calculation of the mobility degree of a node relative to its neighbour. optimised link state routing protocol (olsr) is used in this mechanism which consists of multi-point relay (mpr) selection and topology discovery. probability based mechanism was used to enable accurate estimation of the link's stability by considering the signal strength variations."
"the existing studies have explored how to utilize the legacy of wman to deploy edge servers or apply sdn technologies to help the deployment of edge servers, but most only pay attention to minimizing the average delay between edge servers and mobile users, and there are limited studies that focus on minimizing the number of edge servers while ensuring the access delay requirement. different from existing studies, in this paper, we investigate the edge server placement problem in an economic perspective. our objective is to reduce cost as much as possible and meet the requirements of mobile users at the same time."
"agents are characterized by their autonomy, which gives them the ability to work independently and in real-time environments [cit] . the aiida-sql agent presented in this study interacts with other agents within an architecture still under development. these agents carry out tasks related to message capture and syntactic analysis, administration, and user interaction. as opposed to the tasks performed by these agents, the aiida-sql agent executes classification sql queries that we will subsequently define in greater detail."
"a significant role of routing protocols in manet is to find and establish routes between node pairs. if every wireless links in the network has same characteristics then the shortest path between the nodes can be easily determined. in ad-hoc routing protocols finding the shortest path between source and destination is based on minimum hop count metric. the optimal routing with higher throughput can be obtained by choosing the nodes with high link quality and channel bandwidth. however due to dynamic topology, frequent link failures occurs in the network and predicting the link quality is still challenging."
"the value obtained by the ann for this query was 0.24. however, svm deemed that the output value was 0.48. the mixture gave an output value of 0.36, which is in the range of suspicious queries. if the ann would have been applied alone it would have considered this query as a valid one. however, the svm would have considered it as an attack. the mixture deemed suspicious and a review was carried out manually."
"we conducted extensive experiments to evaluate the proposed algorithms by synthetic network topologies where all algorithms ran on a machine with the windows 10 and 64 bit operating system, 4 ghz intel i7 cpu and 16 gb ram. we should note that all proposed algorithms were implemented in matlab."
"selection of next hop based on the speed of movement of nodes and transmission range reduces overall energy consumption, link failures and delay constraints. an efficient route is established between source and destination to minimise the hop counts and to reduce the packet loss in the network. the success of the data transmission via crin is analysed using ns-2 tool. network performance is enhanced through shortest path so that messages may be delivered in a timely manner without any significance delay. the packet transmission rate is improved by 0.4% for the proposed crin method. simulation analysis shows that the crin scheme has better performance and the overall efficiency is improved by 40% compared to the existing method in the communication network. the work can be enhanced further by including security measures for message authentication in order to prevent vulnerable attacks."
"where: η is the learning rate, τ is the \"strength\" of the lateral connections, b the bias parameter, p a parameter related to the energy function [cit] and a is a symmetric matrix used to modify the response to the data [cit] . the effect of this matrix is based on the relation between the distances separating the output neurons."
"in recent years, internet attacks have increased due to the large number of information systems connected to the internet. one of the most serious security threats around web applications and databases has been the sql injection attack [cit] . this attack can be used to gain confidential information, to bypass authentication mechanisms, to modify the database, and to execute arbitrary code [cit] . a large number of solutions has been proposed so far [cit], but they can not defend against sophisticated attack techniques such as those that use alternate encodings and database commands to dynamically construct sql strings. these approaches lack the learning and adaptation capabilities for dealing with attacks and their possible variations in the future. in addition, a limitation of the majority of these solutions is that they are based on centralized mechanisms with little capacity to work in distributed and dynamic environments."
"there are three ways to move from current state to another state in our algorithm: (1) adding a random node from the set of uncovered nodes to current solution; (2) deleting a random node from current solution; and (3) selecting a node from current solution randomly and replacing it with a random node from uncovered nodes. all of these ways are able to change the calcost(s, i) value of current solution to achieve the state movement in sameds."
"at the same time as the estimation through neuronal networks is performed, it is also carried out by the svm model, a supervised learning technique applied to the classification and regression of elements. the algorithm represents an extension of nonlinear models [cit] . additionally, svm allows the separation of element classes which are not linearly separable. to do so, the space of initial coordinates is mapped in a high dimensionality space through the use of functions. due to the fact that the dimensionality of the new space can be very high, it is not feasible to calculate hyperplanes that allow the production of linear separability."
"proof of theorem 1. for each set s, s ⊂ v, let u be the set of nodes uncovered by s within i hops. each node v, v ⊂ s, can connect at least one member of s within i hops and for each node v, v ⊂ u, there exists a link between v and at least one node in u. thus, for every node not in s ∪ u, there is a link between the node and at least one member of s ∪ u within i hops, and, according to definition 2, s ∪ u is an i-tier dominating set of g."
"the frequent changes in the link are occurred due to the node's movement. the speed of the node may vary according to the environment as well as their characteristics. the active link between the nodes is determined using probabilistic prediction based link availability method. selecting more reliable neighbour nodes with high link connectivity is one of the methods to achieve better network performance rate. the nodes with lower travelling speed have longer time link connectivity and the nodes with higher travelling speed have shorter time link connectivity. link estimation includes cost metrics such as distance between nodes, travelling speed of the node."
"two examples of the similar queries recovered for the manual review are shown below. the first one is a clear attack, while the second is not. as a conclusion, it can be said that the combination of different artificial intelligence paradigms allows the development of a hybrid intelligent system with characteristics such as the capacity for learning and reasoning, flexibility and robustness which make the detection of sql injection attacks possible. the proposed aiida-sql agent is capable of detecting these abnormal situations with low error rates compared with other existing techniques, as shown in fig. 2 . there are not strong differences in the performance of the best techniques, but in the case of computer security only one false negative is very important as it could extremely damage the protected system. some other techniques could be applied in the mixture to improve the performance. the presented approach also provides a decision mechanism which eases the review of suspicious queries through the selection of similar queries and their visualization by means of an unsupervised neural model. for future work, we will thoroughly study the query clustering process in order to take into account the characteristics of queries with greatest influence in sql injections."
"data loss rate is defined as the ratio of the packets lost while senders send their packets to their corresponding receivers. it is given by equation (6); where n represents the number of nodes in the network. figure 4 shows that the number of packets lost, the crin mechanism have low packet drops compared to the scheme of lrd. the crin has reduced packets lost due to highest security routing. the link strength is better among core relay node thus the proposed scheme diminished the least packet loss when compared to the existing scheme. lrd scheme get to fall the highest packet due to the weaker node link."
"the speed of the node can be determined using node's velocity and its travelling direction. the displacement of the node with respect to time from its initial position is estimated as node's velocity. the distance travelled by the node in the corresponding direction with respect to time is calculated as speed of the node. the average speed of nodes is given in equation (3),"
"despite the wonderful advantages of edge servers, to a large extent, where and how to deploy them has been neglected. most studies assumed that the edge servers they need have been properly deployed and merely concentrate on the offloading and resource allocation strategies [cit] . however, without optimal edge server placement strategies, these jobs may be unrealistic because all the services offered by mec are based on the reality that edge servers has been deployed and can be harnessed easily. in addition, edge server placement strategies are related with not only user experience but also the cost of mec service vendors. therefore, it is of significant importance to investigate the edge server placement problems as to construct a solid foundation for the other works in mec."
"during the visual review, similar queries were recovered and the data dimensionality was reduced to perform 3d visualization. the obtained result to be shown to the human expert is depicted in fig. 4 . at first glance, it can be determined that the query is a legal one viewing the position with respect to the similar queries."
"we investigated the algorithm for esppwc. similar to the results presented in the last subsection, we first chose random-esppwc as the benchmark algorithm to compare with gmedswc. the difference between random-esppoc and random-esppwc is that random-esppwc should satisfy the given constraints. in addition, to show the influence of the capacity on the placement solutions, we compared gmedswc with gmeds in the same topologies. it is worth noting that the mentioned big first strategy was adopted to achieve the best performance of gmedswc, and the capacity of each edge server was given a value of 200,000 mhz. we first evaluated these three algorithms by varying the network size and fixing delay constraint, as above. figure 6 shows that, with the increase of network size, the number of edge servers grows, as is consistent with the realistic condition. it can be seen that gmedswc and gmeds outperform random-esppoc, having 10.5% and 24.6% fewer edge servers, respectively. figure 6 also shows that the capacity constraint results in the increase of the need of edge servers. as shown in figure 7, the number of edge servers decreases as the delay constraint increases, and about 49.0% more edge servers are required when the capacity constraint is attached."
"the basic unit of the hidden layer is the lstm neuron [cit] which consists of one memory cell, one input gate, one output gate, one forget gate, and three adaptive and multiplicative blocks. the structure of an lstm neuron is shown in figure 3 ."
"emotion is any conscious experience characterized by intense mental activity and a certain degree of pleasure or displeasure. it primarily reflects all aspects of our daily lives, playing a vital role in our decision-making and relationships. in recent years, there have been a growing interest in the development of technologies to recognize emotional states of individuals. due to the escalating the image is copied from https://bit.ly/2w3ks5u, article by stefan pettersson use of social media, emotion-rich content is being generated at an increasing rate, encouraging research on automatic emoji classification techniques. social media posts are mainly composed of images and captions. each of the modalities has very distinct statistical properties and fusing these modalities helps us learn useful representations of the data [cit] . emotion recognition is a process that uses low-level signal cues to predict high-level emotion labels. with the rapid increase in usage of emojis, researchers started using them as labels to train classification models [cit] . a survey conducted by secondary school students suggested that the use of emoticons can help reinforce the meaning of the message 1 . researchers found that emoticons when used in conjunction with a written message, can help to increase the \"intensity\" of its intended meaning [cit] ."
"after dynamic traffic polling, the whole traffic that included the 3 links were estimated. the three timemismatching detected traffic series were aligned by linear interpolation."
"polling period adjusting strategy. the next polling period is adjusted dynamically according to the variation of traffic. the period is extended if the variation is low, while the period is shortened if the variation is high."
"under the distributed traffic detection architecture, the tasks of agent satellites' monitoring, traffic collection, and management are assigned to different igso satellites, so that the whole network operation states can be detected in near real time."
"we then looked at the importance of emojis in addition to text for the emotion classification task. we used three different model architectures., the first was the bag of words model for the whole caption (emojis embedded in between text). the second being attention mechanism with whole caption as input (emojis embedded in between text), where we consider an embedding layer consisting of both textual and emoji embeddings to train an attention model. in the third architecture, emojis and text are considered as two different features and fed through different input layers. the accuracies have been found to be better if emojis embedded with text is considered as sequential input to the attention model. this can be related to a study by kevin cohn which says that emojis on social media are being used as a language 11 and hence a bi-lstm model would give better accuracies for sequential data. for example consider the caption \"my for life!!\", here is used in context of \"love\". if both text and emoji are considered as different inputs, this would confuse the model since can be used in different contexts table 10 : accuracies of emotion classification task using attention mechanism combining text and emoji as input for bi-lstm (as seen in table 2 ), this decreases the classification accuracy. this is the reason resulting in high accuracy when both text + emoji is considered as sequential input to attention model."
"the traffic prediction performance of lstm was compared with arma (2,1) [cit] which is a conventional traffic prediction method used in satellite networks. considering the limited on-board computational power and storage, the cpu usage and memory usage of the dynamic traffic polling schemes using different prediction methods were also compared. the experiments run on a personal computer with intel core i54570 (3.20 ghz), 16 gb, 64-bit operating system windows 10. the prediction performances of arma and lstm are shown in table 1 . lstm has less rrmse than arma in traffic prediction. lstm improves the accuracy of traffic prediction. the cpu usage and memory usage of dynamic traffic polling using arma and lstm are, respectively, shown in table 2 . the dynamic traffic polling scheme with lstm has a little less memory consumption and a little more cpu usage than that with arma. we found that lstm has been used in the satellite system for on-board task planning [cit], which means the on-board computing resources can support for running lstm. so, after a systematic consideration, lstm instead of arma was chosen in traffic prediction of satellite links."
"it has been proved that textual information is more appropriate than visual features and increases the accuracy of emoji prediction in case of multimodal information [cit] . hence we tried to check the importance of textual features for emotion classification task. we use the attention mechanism to learn the importance of words towards emotion classification task. instagram allows users to post a caption of maximum 2200 characters which translates to approximately 300 words 9 . it would be challenging to train a bi-lstm with such long input sentence length, we experimented on different input sentence lengths to check the variation of accuracy of bi-lstm on sentence length. figure 5 gives the clear picture of variation of accuracy measures namely precision, recall and f score with input sentence length to the bi-lstm using word embeddings trained using fasttext on the set of captions. we have observed that maximum accuracy for classification is achieved when the input sentence length is nearly 80 words per caption and thereafter there is no significant rise in the accuracy of the model. we also reported our accuracies of the emotion classification task using the bag of words model (which is also a extensively used approach for long input sentences) for text using different word embeddings, and we have observed that attention mechanism has achieved better accuracies than the bag of words model. this can be due to the reason that bag of words model gives equal importance to all the words which is not the case with attention mechanism. table 5 and table 6 report the accuracies for emotion classification task using different word embeddings and using different model architectures attention mechanism and bag of words model respectively."
"we have used the 152-layered residual network which is the current state of the art model architecture for image classification. also the state of the art model architectures for emoji prediction task for images have used resnet architecture [cit] . here we trained a 152-layered residual network with learning rate as 0.0001 and stopped the training when there is no further increase in accuracy on the validation set. however, we also checked the classification accuracy using other resnet model architectures."
"in this paper, we explored the usage of different emojis in different emotional contexts and the importance of different modalities towards emotion classification task. we have presented a multimodal emotion classification approach which makes use of all modalities -emoji, textual and visual features. we have further shown that combining all modalities can outperform state of the art unimodal approaches (based on only on textual or visual or emoji contents). we also observed better accuracy for the emotion classification task when the caption (emoji and text) is considered as sequential input compared to accuracy when used as different input features. as a future work and with [cit] as reference, we plan to work on building transfer learning approaches using pre-trained classifiers to learn the emotional features from visual and textual contents towards emotion classification task. we also plan to evaluate our approaches using human annotated test set."
"rrmse between the estimated total traffic curve and the real total traffic curve is computed to evaluate the estimation performance. we also tried another two interpolation methods, nearest and cubic spline, in the procedure of total traffic computation. the performance of total traffic estimation is listed in table 3 . it can be seen from table 3 that there is little difference among the linear, nearest, and cubic spline methods. this reason may be that the estimation results mainly depend on the sampling series and they are not sensitive to the alignment method. therefore, linear interpolation is chosen in order to save the calculation cost."
"in this section, we report the accuracies of different experiments which we performed. we first check the classification accuracy when each modality is used independently, then we combine these modalities and check the classification accuracy of multimodal features towards the emotion classification task. all models are trained using the keras library and run on theano background on a cuda gpu, using adam's gradient descent optimizer. for all experiments, models are trained using 60% of the dataset, validated on 20% of the dataset and tested on the remaining 20% of the dataset."
"we collected set of all user captions from each post and followed pre-processing steps and trained a fasttext word embedding model. we chose fasttext over other conventional word embedding models due to its capability of capturing sub-word information which plays a significant role in social nlp tasks. also fasttext word embedding model has proved to give greater accuracies compared to other word embedding models over various nlp tasks such as sentiment analysis, emotion detection, sarcasm identification [cit], and emoji prediction [cit] ."
"the rest of this paper is organized as follows. in section 2, the methodology for network traffic detection, collection, and representation of bds-3 is introduced. the methodology includes a dynamic traffic polling scheme, a distributed traffic detection architecture and a time-varying graph model. the simulation scenarios and their results are shown in section 3. the conclusion of our work is in section 4."
"these networks have been shown to be efficient for a wide range of nlp tasks and have improved the accuracy over the lstm networks because the current hidden state vector is computed using the past and future information. the importance of a word is highly context-dependent, i.e., the same word can have a different degree of importance in different context. to incorporate this perspective, we add an attention layer [cit] on top of this encoded bidirectional lstm which helps the model decide the importance of each word for the emotion classification task. for instance, words like \"lovely\" or \"extraordinary\" are likely to add more weight to the emotion carried in the text. the attention layer helps the model learn the importance of each word using attention scores."
"assuming that the sampling number is n, the predicted traffic value of every polling round is expressed as fŷðt 1 þ,ŷ ðt 2 þ,⋯,ŷðt n þg, the corresponding real traffic value is fyðt 1 þ, yðt 2 þ,⋯,yðt n þg. the relative root mean square error (rrmse) defined in equation (10) is utilized to measure the prediction performance of lstm. because the number of neurons in the hidden layer of the lstm model can influence the prediction performance, experiments were conducted to determine the number of neurons of the dynamic traffic polling. as illustrated in figure 10, the descent speed of rrmse is fast before 30 neurons and is slow even fluctuant after 30 neurons. the more neurons, the more is the complexity and computation cost. so, we chose a 30-neuron hidden layer for the tradeoff between performance and complexity."
"2.3. the time-varying graph for a satellite network. we applied tvgs to describe a dynamic satellite network and its traffic. a graph model is designed to depict the topology, node and link attributes, and traffic that change over time."
"emoji_embeddings _senselabels: emoji sense labels are the list of different senses what emoji mean in different contexts. the emoji embedding from the set of sense labels is calculated following the bag-of-words model shown in figure 4 . for example \"amusing\", \"swagger\" and so forth are the sense labels listed by emojinet for the emoji ( )."
"in this section, we present and motivate the models that we used to learn features from an instagram post composed of a picture and the associated comment for emotion classification task."
where c represents the velocity of transmission. the latency spent on transmitting request messages or traffic response messages in the process of traffic detection can be calculated according to the result of equation (9).
"the network traffic detection, representation, and collection framework are proposed based on the satellites, their orbits, and functions in bds-3. geo and igso satellites function not only as navigation systems but also as relays which forward space-based measurement and control information [cit] . because the inclined orbit angle of igso is more than 0°, it is visible at high-latitude area. igso is more favorable than geo for the meo information relaying [cit] in the designed distributed traffic detection architecture."
"we then looked at the importance of textual features for emotion classification. recent research has identified the importance of textual features over visual features for task involving social media data [cit] . the same has been observed in the case of emotion classification. a caption may contain emoji or text, hence we looked at the importance of these modalities for emotions classification individually. unlike twitter where users are restricted to a caption of character length 300, instagram allows users to use a caption of length 2200 which can give room to about 350 words. [cit], maximum word count of a caption is 423 words, and the average number of words in the caption is about 44 words. we then looked at the variation of accuracies with input sentence length. we observed that the maximum classification accuracy at a input sentence length of 80 words/caption. further increasing the sentence length did not affect the classification accuracy."
"3.1. simulation scenario establishment. we established simulation scenarios by using stk (satellite tool kit) tools. we simulated the dual-layer satellite network that was composed of 24 meos in standard walker constellation 24/3/1, 3 igsos with phase difference of 120°, and 3 ground stations in china [cit] . the simulated constellation structure of the satellite network is shown in figure 7 ."
2.1. the dynamic traffic polling scheme. the dynamic polling scheme consists of two parts: (1) the data prediction algorithm and (2) the polling period adjusting strategy [cit] . the block diagram of our dynamic polling scheme is depicted as figure 1 .
6.4.2 attention mechanism for whole caption. according to the study by kevin cohn which says emojis can be used as language on social media 10 it is noted that the sense of emoji depends on the context of use [cit] . hence bi-lstm which have been the sota for sequential data would capture the sense of the emoji in the textual context with the help of recurrent units. hence we train an attention model to check the accuracy where complete caption (emoji+text) is fed to the input layer of a bi-lstm. table 7 reports the accuracies of emotion classification task using attention mechanism on whole caption.
"the changing topology and the varying traffic of bds-3 can be modeled by a time-varying graph (tvg) [cit] . tvg was developed from a static graph that was largely used to model network entities and their relations in the form of vertices and edges. tvg maps states of the graph at different times. tvgs have three mapping approaches, namely, the snapshot, the log file, and the whole-graph method, respectively. in the snapshot method, each graph snapshot is a static representation of a network at a time point. the log file method is a modified snapshot method whereby each historical snapshot with a timestamp is recorded and stored in a log file. the whole-graph method records each kind of network elements (i.e., vertex, edge, or attribute) with a valid time point or a valid time interval. after analyzing the three kinds of tvgs and the characteristics of bds-3, we choose the whole-graph method to model the dynamic bds-3. with the bds-3 tvg model, the detected traffic of links and the corresponding network topology can be stored. the whole network traffic and its evolution can be derived. the tvg information is essential for bds-3 management and traffic load balancing. this paper focuses on the network traffic monitoring, collection, and representation with the aim of detecting and storing the network traffic of bds-3 in time. it has three contributions. the first one is that a dynamic traffic polling scheme is proposed for detecting the traffic of a link. the second is that a tvg in terms of the whole-graph method is used to model the dynamic satellites, the topology, and the traffic of bds-3. the third is that a distributed network traffic detection architecture in terms of snmp was introduced for bds-3."
"the visibility between all the satellites and the ground station in sanya is shown in figure 8 . as shown in figure 8, the igso satellites keep links with the ground station, while the meo satellites intermittently connect with the ground station. in our distributed traffic detection framework, the 3 igso satellites were designed as the detectors. the 24 meo satellites were dynamically assigned to one of the 3 detectors according to dijkstra's shortest path principle. most of its orbit cycle time, a meo satellite could find an igso satellite to relay the data to the ground station. if the under the distributed traffic detection architecture that we presented, the total connectivity time between the meo 03 and sanya station through igso satellite relay is 82 h. our distributed traffic detection framework reduces the interval to obtain the traffic of whole links because the time spent on the traffic detection of each link is largely shortened."
"when the detected traffic and its related information, such as node and link attributes, and timestamps are transmitted to the ground stations, they are organized in terms of the introduced tvg model. we chose a graph database, neo4j, to store the information of the tvg model of the simulated satellite network. we can query the information of a satellite or a link from the neo4j database. we can also snapshot the traffic of the whole satellite network at a given time. one snapshot of a part of the satellite network is shown in figure 12 . the attributes and traffic information of the link highlighted in yellow are shown at the bottom of figure 12 . the information of the tvg model for bds-3 can be well stored in the neo4j database. the database supports quick query of the nodes, edges, and traffic in the bds-3 system. neo4j will be an efficient tool for bds-3 management."
"the polling period adjustment strategy determines how to adjust the next polling period if the predicted variation exceeds the thresholds [cit] . the dynamic polling strategy makes the time at which the traffic is detected different from one link to another. a time alignment algorithm should be applied to synchronize the traffic of different links in order to compute the whole network traffic at specified intervals. the widely used time alignment methods are lagrange interpolation, piecewise linear interpolation, and cubic spline interpolation."
"recurrent neural networks (rnns) are a class of neural networks that take sequential data or time series data as input and compute hidden state vector at each time step, and these networks make use of the entire history of inputs to compute hidden state vector. a lstm network is a special class of rnns which has a memory cell and three gating units."
"we estimated the total traffic per 15 s according to the time resolution of 15 s. the results of the total 3-link traffic are shown in figure 11 . in figure 11, the estimated total traffic curve is similar to the real summed traffic curve."
"we then looked at the importance of the third modality-emoji features, we used the different emoji knowledge concepts extracted from emojinet and trained a neural network. we used the vector average of the emojis embeddings used in each post as the feature vector and input of the neural network. considering the use of in different contexts, the emoji is the most frequently used emoji to express love or joy. hence using only emoji features for emotion classification is not a good idea and the same is observed in the results. the classification accuracy using emoji features is about 32.83% which is very low compared to textual features."
beidou is one of the most rapidly developing global navigation satellite systems (gnss). china's beidou navigation sys-of traffic values in terms of the polling period adjustment strategy. the dynamic polling strategy can minimize the interference to the communication of links and reduce the cost of wireless transmission and storage.
"the prediction algorithms commonly used in the polling dynamic strategies of terrestrial networks are statistical analysis methods, such as the linear regression model [cit] and markov chains [cit], and machine learning methods such as svr (support vector regression) [cit] . the proposed traffic prediction algorithms are mainly arma (autoregressive moving average) [cit] and neural networks that include bpnn (back propagation neural network), rfbnn (radial basis function neural network) [cit], esn (echo state network) [cit], rnn (recurrent neural network) [cit], and lstm [cit] . because the traffic detection algorithm should run on satellite systems, we choose the prediction algorithm in consideration of both the ability of computation and storage of satellite systems and the accuracy of prediction."
"most nlp tasks are limited to the scarcity of labeled data. earlier many researchers have used manual annotation technique to evaluate their models, but this requires much understanding of the emotional content of all the expressions, is time-consuming, requires much effort and may differ according to one's perspective. this creates a misinterpretation of emotion and effects the accuracies of respective tasks. hence in most social nlp related tasks namely sentiment analysis and other emotion classification tasks hashtags were used as features for automatically labeling data to corresponding categories [cit] . however, the rapid increase in the usage of emojis on social media helped researchers use emoticons as features for data labeling. [cit] has introduced a transfer learning approach for emotion classification, sentiment analysis, sarcasm identification through emoji prediction on the text."
"resnet architecture [cit], composed of convolution layers is currently the state of the art for image classification tasks achieving high accuracies [cit] . before the introduction of resnet's, deep cnns were used for most image processing tasks. resnet is a feed forward cnn that utilizes two or more convolution layers [cit] . it has been observed that the accuracy of image classification majorly depends on the depth of the network, i.e., the number of layers which can be related to the number of layers within the network [cit] . hence we used different types of resnet model architectures, namely resnet-101 and resnet-152 to check the classification accuracy in different cases. resnet model architecture was first tested on the imagenet dataset where the number of classes for classification was 1000. since our task of emotion classification requires us to learn features from both text and image, we use the resnet model to learn a feature vector from an image. we use the implementation of resnet by kotikalapudi which is open-sourced and available on github 8 . table 3 reports the precision, recall, macro f1 score observed using different resnet model architectures."
"detection. assume the satellite-ground link is a laser link with a 150 mbps rate [cit] /03/30, 00 : 00 and 09 : 00 as the traffic of the link between an igso satellite and a ground station. the average rate of this dataset is 92 mbps, which is less than 150 mbps. so, the wide dataset was considered a reasonable virtual traffic of the detector-ground links. we cut the wide dataset into several slices to represent the traffic of three detectorground links. in the dynamic polling period adjusting strategy, the minimum polling period was set as 15 s in order to save network resources. so, the time resolution of the detected traffic time series is 15 s. figure 9 shows the traffic sampling points and the traffic detection values of three simulated detector-ground links."
"here we present two different model architectures: one which considers complete caption (emojis embedded in between text) as sequential input and fed to a single input layer, while the other considers text and emoji as different input features and are fed to two different input layers as illustrated in figure 6 . 6.4.1 bag of words approach for whole caption. here we consider complete caption as the input to the classification model, and learn the feature vector by calculating vector average of the embeddings of entities, i.e., word embeddings of words and emoji embeddings of emojis. since the emoji embeddings are learned using the word embeddings, this supports the addition of word embeddings and emoji embeddings to calculate vector average. table 8 reports the accuracies of emotion classification task using bag of words model with different emoji knowledge concepts and different word embedding models."
"emoji_embeddings _definitions: emoji definitions are the textual descriptions that relate to the context of use of particular emoji. the emoji embedding from the set of descriptions is calculated using the bag-of-words model shown in figure 4 . for example, consider the emoji, emojinet lists \"one of the temperate seasons, summer is the warmest of the four temperate seasons, falling between spring and autumn. \", \"the period or season of summer. \" and so forth as emoji definitions for the emoji ( )."
"we make use of different knowledge concepts, namely sense forms, sense definitions, emoji names from emojinet and use the bag-of-words model (figure 4), fasttext trained word embeddings to learn the emoji representations. since we use the word embeddings to learn emoji representation, we could say that both emojis and words are embedded on a similar vector space. we define two types of knowledge embeddings termed as emoji_embeddings _defini-tions, emoji_embeddings senselabels learned using emoji sense definitions and emoji sense forms extracted from emojinet respectively. then we evaluate our model using these embeddings as external knowledge concepts."
"in this section, we explain the different pre-processing steps to ensure the quality of our instagram dataset. in total, we have collected about 1.1 million posts of different languages from instagram using instalooter api posted between 1st [cit] to 20th [cit], a duration of 20 days. the preprocessing of this corpus consisted of a set of filtering, followed by annotation, as discussed next."
"bds-3 is being established as a gnss. it is not only responsible for positioning and navigation but also for message communication and further data transmission. traffic detection and representation are basic techniques for automatic management and control of bds-3 with the aim of ensuring safety operation. under the snmp protocol, a dynamic traffic detection method including traffic prediction by lstm and a polling adjusting strategy was proposed for detecting the traffic of each link in bds-3. a distributed traffic detection architecture was also established to collect the traffic and its related information in near real time. a tvg model was introduced to represent the dynamic topology and the traffic of each link at specific times. we used stk to simulate"
"finally, we combined the three modalities -textual, visual, and emoji. table 10 reports the classification accuracies observed using different emoji knowledge concepts wherethe caption (text + emoji) is sent through attention mechanism and image is sent through resnet-152 model. we then merge the outputs of these layers and train a softmax on top of this for classification. this combination of all the modalities results in better classification accuracy."
"psychological studies conducted in the early '80s provide us strong evidence that human emotion is closely related to the visual content. images can both express and affect peopleś emotions. hence it is intriguing and important to understand how emotions are conveyed and how they are implied by the visual content of images. with this as a reference, many computer scientists have been working to relate and learn different visual features from images to classify emotional intent. convolutional neural networks (cnns) have served as the baselines for major image processing tasks. these deep cnns combine the high and low-level features and classify images in an end-to-end multi layer fashion."
"output layer journal of sensors figure 5 shows the distributed detection architecture and the message transmission direction. the agent satellites are allocated to detector satellites in terms of the shortest route principle. the detection mechanism under the distributed detection architecture is described as follows. to begin with, the detector satellites, respectively, send request messages to the agent satellites in their coverage. then, the agent satellite responds messages containing traffic information to the detector satellites. next, the detector satellites forward the collected information to the ground station. the detector satellites can exchange their collected information if needed."
"we used historical traffic to train a lstm model, and then utilized the trained model to predict traffic. lstm is acknowledged as the state-of-art prediction model for time series prediction [cit] . our lstm model consists of one recurrent hidden layer and one output layer. the block diagram of our lstm model is shown in figure 2 ."
"considering that we want to get the total network traffic at time t, we align the traffic of each link by means of linear interpolation. assume that the detected traffic of link j is"
"this paper investigates the vibration reduction and trajectory tracking control problems for the helicopter suspension cable system with input saturation and external disturbances by backstepping method. firstly, an auxiliary system is proposed to tackle with the problem of input saturation. then, based on the proposed auxiliary system and the lyapunov theory, an adaptive boundary control is introduced to reduce the cable's vibration and follow a pre-given trajectory. under the developed control scheme, the closed-loop system for the suspension cable system of a helicopter is proved to be uniformly ultimately bounded."
"the third situation that causes this error is deleting the variable before using the same variable in the copyout data clause, as in figure 2c . finally, there is a syntax error that can also lead to this type of error and is not detected by the compiler when the programmers write the enter data region directive without the ''acc'', as shown in figure 3 . in this case, the same array ''a'' is in both the enter and exit data regions, but the compiler will ignore the enter data region directive, which affects the application in the same way as in figure 2b ."
"where ς 1 and ς 2 are positive constants. theorem 1: for the helicopter suspension cable system (1) under boundary conditions (4) and (5) with external disturbances and input saturation, under the designed adaptive control law (16), the problem of uniform ultimate boundedness for the closed-loop system is investigated. moreover, the following targets will be realized with the proposed control scheme:"
"2 (t)+αtx 2ṙ (t)−αktx 2 (t)ξ 1 (t)− αtx 2 (t) (t) + αtx 1 (t)ṙ(t) + αkt ξ 1 (t)ṙ(t) + αt (t)ṙ(t) − αkt ξ 1 (t)x 1 (t) − αtx 1 (t) (t) − αkt ξ 1 (t) (t), assumption 1, assumption 4, and lemma 2, (26) can be rewritten aṡ"
"for detecting code parallelism issues, our tool will test the generation of threads, which includes gang and vectors. our static phase is based on the analysis of the user source code and will generate gang and vectors for each compute regions to compare with the actual gang and vectors from the execution of the user code. our static phase will insert statements in the user code to extract the actual gang and vectors from the user code to compare them with our generation of gang and vectors. this will help us to detect any related errors to include user code work sequentially rather than work in parallel because of not using openacc directives properly. this could help the user to enhance their code's performance because the user assumes that some compute regions work in parallel, but they actually work sequentially. the following algorithm in listing 4 shows the process of detecting race condition in loops:"
"there is a similarity between livelock and deadlock, except that livelock occurs when two or more threads change their status continuously in response to other thread changes without performing any useful work. in openacc applications, there is a relationship between deadlock and livelock, as discussed in the previous section and shown in figure 15 . the gpu livelock is causing a deadlock in the cpu, while the cpu livelock will keep the process busy forever, and none of the processes will make any progress and will not be completed. also in livelock, the threads might not be blocked forever, and it is hard to distinguish between livelock and volume 7, 2019 other long-running processes. finally, livelock can lead to performance and power consumption problems because of its useless busy-wait cycles."
"turbulence and wind may render that the suspension cable can not keep tight all the time. hence, representing a suspension cable system of a helicopter by a set of ordinary differential equations is not suitable under the circumstance. in general, a system with vibration is often represented by a distributed parameter system from a mathematical perspective. therefore, in this paper, the suspension cable system of a helicopter will be modeled as a distributed parameter system, whose state variables are related with both time and space. for the distributed parameter system, boundary control is an effective control strategy to realize vibration reduction according to engineering experience. the boundary control scheme will be proposed to decrease the vibration of flexible cable for the suspension cable system of a helicopter."
"building massively parallel applications is challenging and has several difficulties that can affect the system's efficiency and accuracy. one of these difficulties is that parallel applications if badly programmed they can have non-determined behavior, which makes it hard to detect parallel errors or test parallel applications. also, run-time errors vary from one programming model to another, and it is not easy to determine whether errors have been corrected or are hidden when modifying the source code."
"moreover, in fig. 7 and fig. 9, the longitudinal vibration amplitudes p(z, t) of the suspension cable become big at about 40s resulted from the motion of helicopter. however, in fig. 4, the longitudinal vibration amplitudes p(z, t) of the suspension cable is not affected by the motion of helicopter by using the control approach in this paper."
"our tool analyzes the user source code to explore and get different information to be used in the detection of openacc run-time errors. from the user source code, our tool obtains the following: 1) counting the number of compute regions, structured and unstructured data regions. determining the start and end for each of them. 2) detecting if there is an independent clause and in which compute region they are located. 3) creating a variable list for all variables located in each compute region and storing them in the data structure. 4) detecting loops in each compute region for further investigation regarding the data race test, to include the following information:"
"after analyzing the recent openacc version 2.7 documents [cit], consulting openacc-related books, and conducting several experiments, we classify and identify several run-time errors based on our experiments and build several programs to simulate these errors to discover their behavior and effects on openacc-related applications, as shown in figure 1 . programmers can cause these errors when they try to parallelize their applications by using openacc. also, there are some directions and instructions that openacc documents indicated to avoid some errors, and when programmers do not follow these instructions, that can cause errors. in the following, we classify openacc run-time errors that cannot be detected by the compilers and explain each class and discuss their causes."
"despite efforts made to test parallel applications, there is still more work to be done with respect to high-level programming models used in heterogeneous systems. we have noticed that openacc has several advantages and benefits and has been used widely in the past few years, but its errors have not been investigated or identified as clearly as the other programming models or targeted by any testing tool. finally, to the best of our knowledge, there is no parallel testing tool to test applications programmed by openacc or designated to test openacc applications and detect their run-time errors."
"under the pd controller (42), fig. 7 and fig. 8 represent the vibration amplitude p(z, t) of the helicopter suspension cable system and the trajectory tracking curve of helicopter."
"regarding the unstructured data region, the algorithm in listing 3 detects run-time errors that occur in data clauses, including create and copyin while entering the data region, as well as copyout and delete at the exit data region. in addition, the data clause variable that appears in the entering data region must appear in any data clause at the exit data region. otherwise, it will cause undetermined behavior or run-time errors in the worst case. the unstructured data region is different from the structured data region in different aspects, including the fact that the unstructured data region can have multiple starting and ending points, can branch across multiple functions, and memory exists until explicitly deallocated. furthermore, the enter data region directive handles device memory allocation, and the developer can only use the create or copyin data clauses. the exit data region directive handles device deallocations, and developers can only use either the copyout or the delete data clauses."
"sometimes when programmers are misusing openacc data clauses, the compiler will not detect the presence of errors, but after compilation and during runtime, an error message will be issued indicating the invalid value without giving extra information about the error type or the cause of this error. several scenarios explain this type of error and its causes. this error can occur in both structured and unstructured openacc data regions. in the unstructured openacc data region, several scenarios can cause this type of error. one scenario arises when there is a variable in the copyout clause in the exit data region without having the same variable in any data clause in the enter data region, which will cause an error at the runtime without being detected by the compiler. as shown in figure 2a, the array ''a'' at the copyout clause will cause this error because you simply ask the gpu to copy the array ''a'' to the cpu, but this variable is not present in the gpu, which will then result in an error message during runtime of ''invalid value''. also, another case is when programmers write the data clause in the exit data region without writing the enter data region, as shown in figure 2b ."
"although some data dependency can be avoided by using a reduction clause, misusing the reduction clause in some cases will lead to a race condition. also, the race condition can be caused by the absence of the reduction clause. the reduction clause combines the results for each copy of the reduction variable from different threads with the original variable at the end of the openacc compute region. therefore, the variable should be initialized to some value based on the used operator before using the reduction clause. otherwise, undefined behavior will result and lead to a wrong final result that cannot be detected by compilers. the pgi compiler can detect the summation reduction clause in some cases and add it as implicit reduction but cannot detect the other reduction operations, and the other compilers cannot detect any reduction operations."
"we conducted many experiments and built several applications to test and simulate run-time errors that can occur in openacc, and we studied their behavior to better understand them and discover their causes and effects on the applications. our contribution is to identify openacc errors that cannot be detected by compilers and that can happen without the programmers' awareness. also, we classify these errors into five main types based on error similarity and their causes, as well as how they behave and their effects on the system. we explain these errors with examples and determine their causes with respect to application output when they occur."
"due to openacc's hiding some details, openacc compute regions have an implicit barrier at the end of each compute region, and we cannot be sure about the thread executions as well as the thread arrival at the end of each compute region at a certain time. therefore, we can consider the end of each compute region as a potential deadlock point that might or might not occur. as a result, our static tool will mark the end of each compute region for further checking during runtime."
"finally, in data clause detection, the present data clause refers to the listed variables already present on the device, so no further action need be taken. this is most frequently used when a data region exists in a higher-level routine. because there is no data movement, we do not check the variable behavior; we check only the syntax with the previously mentioned algorithms. we cannot detect run-time errors resulting from the present clause using our static approach, and s a result, we will mark this data clause for further testing because this clause needs to be tested using dynamic testing techniques."
"the following table 2 shows selective comparative study for our testing approach that from all 50 benchmarks that we experimented, we choose ten benchmarks, two from each benchmarks suites we discussed before, as a sample to be displayed on this table. this table shows some statistics about the chosen benchmarks, including some openacc related information we used in our static testing approach. finally, figure 17 shows the testing time for each of the tested benchmarks, where also will be used in our future overhead measurements when we apply our insertion statements for our future dynamic testing."
"when parallelizing for loop what happened in reality, the threads are using different values of iteration variable in this loop variable and may be running in parallel at the same time. openacc does not make any guarantee regarding the execution order of the threads. it is even possible that the last iteration of the loop may be completed before the zero loop iteration. this will lead to a potential race condition. therefore, our static approach will instrument the source code for data dependency analysis as well as execution order analysis. in our situation, we will focus on the openacc parallel region because when it has been used, the developer is responsible for ensuring that the code has enough parallelism, while the kernel directives depend on the compiler to detect whether the code has enough parallelism or should be executed sequentially. our static approach will also mark a parallel clause for further dynamic investigation during runtime."
"to realize the control targets of tracking trajectory and decreasing vibration for the suspension cable system of a helicopter, the following assumptions and lemmas are introduced."
"our static analyzer will partially detect openacc deadlock and mark some places of the code for further dynamic testing. the main purpose of our deadlock detection is to ensure there is no livelock in each compute region because it will cause a deadlock in the host. as a result, we analyze each loop in the compute region and examine their conditions to avoid any possibility of ''always true'' situations. however, our static analysis is limited to loops and detecting their conditions to avoid deadlock. more testing will be needed during run-time for any undetected situations, which will be marked by our static analyzer, and a warning will then be sent to the programmer."
"the input saturation is considered in this paper, then the relationship between the desired control input ν(t) and the actual actuator output u(t) can be written as:"
"a boundary control scheme has been adopted to investigate the problems of vibration reduction and trajectory tracking for the suspension cable system of a helicopter with input saturation and external disturbances. to tackle with the problem of actuator saturation, an auxiliary system has been given. based on the proposed auxiliary system, an adaptive boundary control law has been designed by employing backstepping method. with the introduced control approach, the vibration amplitude and the trajectory tracking error have converged ultimately to a bounded compact set by choosing suitable design parameters. the effectiveness of the proposed adaptive boundary control scheme has been verified by a numerical simulation. the problem of vibration reduction will be investigated for the planar model of a helicopter suspension cable system in the future study. moreover, the problem of varying cable length which is a meaningful topic will be considered in the future research."
"different causes can result in a race condition, including executing processes by multiple threads and where the execution sequence makes a difference in the result, the execution timing, and order. also, this happens when there are two memory accesses in the program, where they both are performed concurrently by two threads or are targeting the same location. in addition, by using openacc, developers cannot guarantee the thread's execution order. because of the developers' responsibility to ensure there is no data dependency, openacc is more likely to have race conditions. therefore, our static approach focuses on covering different causes of openacc race condition to ensure an openacc race-free code."
"the algorithm in listing 2 begins by receiving the tested data clause variable as well as the data clause type, and then determining the variable places in the source code to be examined in three locations: in the source code, the data region, and before and after this region. the data clause variable will be tested differently in these three locations to ensure correctness and report any errors that cannot be detected by any compiler. in addition, different data clause types will be listing. 2. check structured data clause algorithm."
"since e 2 (t) is a cross-term, the relationship between e 2 (t) and e 1 (t) should be given firstly. the rigorous analysis will be shown in the following section."
"we assume that any openacc data clause is potentially error-prone because developers seem to use them inefficiently, and the compilers cannot detect any errors that related to openacc data clause directives. our approach uses static testing that can statically detect openacc data clause related to run-time errors, including the copy, copyin, copyout as well as the other openacc data clauses. the data clauses related variables will be monitored and tested to ensure their correctness and to detect any unsafe behavior because these errors cannot be detected by any compiler, and they will lead to wrong results. also, these monitored data clauses variables can be used to be instrumented for further dynamic testing during run-time. in more details, when openacc directives are founded in the targeted source code, our algorithm will determine the areas that have openacc data clauses and their type as well as the region that they belong to whether compute, structure or unstructured data regions. this mechanism will help our algorithm to call the related function to test this part of the code and check for a run-time error statically. in the case of testing the unstructured data region, there is an additional test which is counting the number of variables that appear in the enter data region, which appears in copyin or create clauses. also, counting the number of variables that appear in copyout or delete clauses at the exit region. then, comparing these two numbers, and they should be equal if not an error message with the related information will be sent to the user as we will explain in listing 1."
"in addition, we briefly explained our testing tool architecture design in section 5, and we discussed our static approach to detect openacc errors in section 6. we used several techniques and algorithms to detect different types of openacc errors based on their causes and how they behave. since we are dealing with big-sized codes as well as errorprone applications, we choose to use the static approach to resolve as many errors as possible before compilation and without executing the code. this will give us the ability to analyze the code in detail and obtain full coverage of the source code. the static analysis gives us the ability to detect actual run-time errors as well as potential errors from the source code, which will be beneficial in enhancing the testing execution time by minimizing the errors that will need further testing during run-time. finally, our static approach will mark the code that has potential errors or needs further testing during run-time, as well as determining the code parts that need inserted statements for further testing."
"openacc has several directives and clauses used to accelerate source code without many changes. openacc directives have been divided into the data region, which is responsible for the data movements between host and device, and the compute region, which is used for executing the code on the device [cit] . openacc's data region is defined by data directives that determine data lifetime on the device and is divided into structured and unstructured data regions, both used for data movements but in different aspects and behaviors. openacc can have multiple data and compute regions within the same source code."
"our tool will investigate each compute region, loop, and equation in detail, analyzing the variables in these compute regions and their behavior and values, reporting any possible race condition to the programmer. dependency analysis also will be conducted for any equation in any given loop to detect any actual or potential race condition. also, the errors in loops that prevent execution will be detected in the case of writing wrong conditions or any other mistakes in the loop statement."
"testing parallel applications built by using programming models is a difficult task because if badly programmed they can have a non-determined behavior, which makes it challenging to detect their errors when they occur and determine the causes of these errors, whether from the user source code or the programming model directives. also, it is difficult to see if the errors have been corrected or are still present but hidden, even when these errors have been detected and the source code modified."
"over the past few years, openacc has become increasingly used in many high-performance computers, including the top supercomputer at the world summit. also, openacc attracts more non-computer science specialists for accelerating their systems in several scientific fields, including weather forecasting and simulations. [cit] . this programming model is used for supporting parallelism in sequential programming languages by adding openacc directives without low-level details and"
"openacc has several advantages and features that give it the ability to parallelize the code, including portability. unlike other programming models like cuda, which only work on nvidia, openacc is portable across platforms and different types of gpu [cit] . also, openacc can work with various compilers and requires less programming effort, which gives it the ability to add parallelism to existing code with less code, decreasing programmers' workloads and improving their productivity. finally, openacc supports three levels of parallelism by using three openacc clauses, including gang, worker, and vector, which are coarse, medium, and fine-grained parallelism, respectively."
"when programmers want to parallelize their loops, they can use openacc directives to enable the loop body to be executed in parallel using concurrent hardware execution threads. the loop iterations might run in parallel at the same time without considering the iteration order. moreover, the last iteration can be executed and completed before the first iteration, which will lead to a potential race condition if the execution order is important to the application. also, the data dependency between iterations causes a race condition. as a result, based on the application analysis and requirements, the programmers should parallelize their loops carefully to ensure there is no dependency in their loop body because if so, that will lead to a race condition, causing the application to fail to meet the user requirements. figure 9a shows an example of a race condition resulting from data dependency. in this example, some x[i] may be read or written by two different threads simultaneously, which causes a race condition because of the data dependency in the second loop. also, in figure 9b, when using the keyword ''kernels'' instead of ''parallel'' and the loop body has data dependency, this will generate an error message on runtime, indicating illegal address during kernel execution."
"our static tool will detect any openacc independent clause and determine their place in the source code and in which compute region they are located. then, the source code will be analyzed to detect any dependency in each equation in the independent compute region to ensure there is no dependency; if there is a dependency, our tool will detect the race condition that could result from this dependency and report them to the user. this examination will be done because of the nature of the independent clause of the openacc, because it tells the compiler that the programmer is responsible for making sure this area is independent; as a result, the compiler cannot detect that and resulted in race condition. the algorithm in listing 4 shows our static analysis of detecting data dependency and independent clause analysis."
"the following table 1 shows the openacc errors and our static tool's ability to detect these errors. our evaluation takes each type of openacc error and our tool ability to detect them fully or partially, where full detection means that our static tool can detect this error, while partial detection means that our tool can detect some cases, while other cases need to be tested during run-time or investigated more than static testing. in some cases, our tool cannot detect these errors by using static testing, and the errors need dynamic testing due to the nature of the openacc error and the causes behind them."
"in this approach, we will check and examine different openacc directives and clauses to identify actual and potential run-time errors. since there is a wide range of errors and directives to be covered, we also classify our testing approach into several classes that include openacc data clause checking, reduction checking, and asynchronous checking, as well as instrumenting data race and deadlock for further checking in the dynamic phase of our approach. the targeted openacc source code will be classified into potential error data region code, free data region code; potential error compute region code, free compute region code, and serial code. in detail, potential error regions refer to the regions with actual and potential errors, while the free regions refers to regions without errors."
"considering the pd control strategy is a linear method, it will be more persuasive to use a nonlinear control method as a comparison. thus, under the auxiliary system (7) and adaptive law (15), another nonlinear control approach which has been used in article [cit] will be adopted to check the effectiveness of the proposed control method in this paper."
"with π 1, π 2, π 3, π 4, π 5, π 6, and π 7 being positive constants."
"the second area tested by our algorithm is before the structure data region or the compute region. the variables' initialization and assignment will be tested in this area, detecting any potential error situations varying from one data clause type to another. one error satiation can occur in this area of code caused by the copyin data clause variable. as a result of the copyin data clause behavior that takes the variable from the cpu into the gpu, we need to determine whether these variables are initialized or have value before going to the gpu. as a result, if the copyin variables are not initialized or part of an equation, an error message will be displayed to the developer that includes the error type and the consequences of this error."
"openacc has many advantages and features that lead to increased use to achieve high parallel systems working in a heterogeneous architecture. openacc is designed for performance and portability that maintains existing sequential code and parallelizes it by using high-level directives without considering many details. this will attract more noncomputer science specialists to use openacc to accelerate their systems when building powerful simulations with minimum investment of effort or time in learning how to program gpus. therefore, misunderstanding openacc directives and clauses can lead programmers to misuse them or to fail to follow the openacc instructions correctly. in this case, several run-time errors can occur due to openacc's nature as well as behavior. programmers could cause these errors when trying to parallelize applications without following directions to avoid some of these errors."
"there are many studies that have investigated parallel applications errors and identified them and their causes. also, many programming models-related errors have been identified, including mpi, openmp, cuda and opencl. however, openacc has not been investigated or identified as thoroughly as the other programming models."
"data management is one of the features supported by openacc, which uses data clauses to conduct data movement between cpu and gpu and vice versa. the programmers must be aware of the usage of each data clause and how to use them in both structured and unstructured data regions. in this classification, we identify run-time errors resulting from the mishandling of openacc data clauses and directives, which leads in turn to non-deterministic behaviors or wrong results. openacc data regions are divided into structured and unstructured data regions, as we explained in section 3. the following identifies and explains openacc data clause-related errors that cannot be detected by the compiler."
"the rest of this paper is structured as follows. sections 2 and 3 will discuss related work and briefly give an overview of openacc. our classification of openacc run-time errors will be discussed in section 4. in section 5, we will explain our testing tool architecture design, and in section 6 our static approach in detecting openacc errors will be explained in detail. in section 7, we will discuss some aspects of our study, and our tool will be evaluated in section 8. finally, conclusions and future work will be discussed in section 9."
"when the developers use the independent clause that tells the compiler that this loop is data-independent, this can cause problems if there is a dependency. the developers' responsibility is to ensure not using this clause if there is a data dependency because the independent clause allows developers to indicate that the iterations of the loop are dataindependent of each other. as a result, our static approach will investigate whether the developer decision is correct by conducting data-dependency analysis in this case."
"compare fig. 4 with fig. 7, it is obvious that the control performance by using the proposed adaptive boundary control is better than the pd control scheme (42) . from fig. 5 figure 7 . the longitudinal vibration amplitude p(z, t ) of the suspension cable with proposed pd control scheme. and fig. 8, it can be seen that the good control performances can be achieved by utilizing two control strategies."
"in this paper, consider a suspension cable system of a helicopter which is provided by fig. 1 . the longitudinal deflection of the helicopter suspension cable system is only considered in the whole paper."
"moreover, in spite of considering input saturation, there exists a control scheme render that the control targets of tracking trajectory and decreasing vibration for the helicopter suspension cable system can be achieved."
"tested differently in our algorithm based on their purpose and behavior. we studied and examined these data clauses to ensure that our approach can detect any potential run-time errors related to openacc data clause directives, which to the best of our knowledge have not been detected before in any compiler or testing tool. the developers will receive an error message if any incorrect situation arises."
"in addition, some syntax errors affect loop parallelism and prevent openacc from being executed correctly without compiler detection or programmer awareness. the results of these errors range from wrong results to performance issues. for instance, when programmers write the openacc loop directives without using the ''parallel'' or ''kernels'' directives, as shown in figure 10a, it simply executes the loop, ignoring the openacc directive and without compiler detection, which leads to wrong results. similarly, if the programmers write the openacc directives using ''parallel'' instead of using ''parallel loop'' when they parallelize their loop, this will also not be detected by the compiler, but also will not yield wrong results. however, the loop will be executed sequentially without the programmers' awareness, which will affect the overall application performance, as shown in figure 10b. 3) shared data read and write race reading, writing, and updating to or from a shared array by multiple threads concurrently can be critical and error-prone, and all need to be handled carefully by programmers. for instance, writing data by thread ''a'' in a specific location and the same location is read by another thread ''b''; in this case, there is a potential race condition. this shared data race can be classified into read-after-write, write-after-read, and write-after-write, but in read-only cases, there is no data race. the c and c++ programmers should use the ''restrict'' keyword [cit] whenever the pointers are not aliased and the compiler needs to be able to parallelize the code; otherwise, it will not work in parallel. finally, we have multiple loops, each of which has a temporary array used during its own calculation, as in figure 11 . if the array ''tmp'' is not declared to be private in each loop, then the ''tmp'' shared array might be accessed by different threads executing different loop iterations in an unpredictable way, which will cause a race condition and lead to the wrong results."
"in unstructured openacc data regions, if the programmers want to copy data from cpu to gpu but mistakenly use the data clause create instead of copy or copyin, this will lead to wrong results without the programmers' or compilers' awareness. figure 4a shows an example of this error, in which the programmers' use creates the array ''a'', which will create a space in the gpu without considering the previous values of the array ''a'' in the cpu, which will in turn eventually lead to wrong results. another case of this error arising is when the programmers mistakenly use the delete clause at the exit data region when the variable needs to be used in the cpu, as shown in figure 4b, which causes the variable to not be copied from the gpu to the cpu, therefore leading to wrong results. similarly, when the exit data region directive has not been written or the ''acc'' keyword is forgotten, this also will lead to wrong results as shown in the codes in figures 4c and 4d, respectively."
"proof: under the adaptive control law (16), the stability of the closed-loop system will be analyzed in the following section. a lyapunov function is introduced as follows:"
"step 2: the adaptive control method which is a strong robustness and simple control method will be used to compensate for the effect of unknown external disturbance in the following section. moreover, an adaptive control law can be designed as follows:"
"our static approach considers those problems and the following algorithm in listing 6 to check the potential errors related to a reduction clause, including the reduction variables' initialization based on their operator types. concerning reduction variables involved in multiple nested loops where two or more of these loops have associated loop directives, the reduction clause containing these variables must appear in each of those loop directives; otherwise, a race condition will occur. finally, our algorithm will check if any of the variables included in the openacc compute region have one of the reduction operations without using reduction directives; this will be reported as an error to the developers, as an absence of reduction clause can lead to the race condition. in addition to inserting some instrumentation statements for the parts that cannot be detected during our static phase for further dynamic checking during run-time, and this instrumentation will be used in case of the absence of the reduction clause when our static approach determines that the compute region has reduction operation and the developers forget to use the reduction clause. our static approach will instrument this region by using our reduction-inserted statement."
"in the following section, first, by using backstepping method, a boundary control law will be designed for the helicopter suspension cable system subject to input saturation. an auxiliary system is introduced to compensate for the effect which is resulted from input saturation. then, based on the proposed control scheme, the uniform ultimate boundedness of closed-loop will be investigated, moreover, the trajectory tracking error and the vibration amplitude will be guaranteed to converge ultimately to a small neighborhood of zero."
"to demonstrate the process of control design clearly, the schematic diagram of control design is given by fig. 2 . since input saturation is considered, an auxiliary system is introduced to compensate for the error u(t) between control input and actuator output. the format of the proposed auxiliary system can be described as follows:"
"deadlock in openacc can be divided into a host (cpu) and a device (gpu) deadlock. the device deadlock can occur when two threads get stuck waiting for each other to release the lock on a shared resource. in addition, openacc is considered lock-free programming, which is more challenging than simply using locks in protecting critical regions, as in openmp [cit] . in openacc, the host deadlock can be a result of having device livelock because of the nature of the openacc hidden implicit barrier at the end of each compute region, and the execution of the cpu will not proceed until all threads on the gpu have reached the end of the parallel compute region. in other words, the cpu will be waiting for the gpu to finish its work while the gpu is continuously busy because of the livelock. we call this cpu deadlock implicit barrier deadlock, as shown in figure 15a, in which the second compute region calculating the array ''b'' is in an infinite loop, and the cpu is stuck waiting for all threads to reach the end of the compute region. the second case of host deadlock is similar to the previous case, but it behaves differently than when programmers use the openacc asynchronous and wait directives, as shown in figure 15b . in this case, the deadlock will not occur at the end of the compute region because it has been assigned to an asynchronous queue, and the cpu will proceed with the execution until it arrives at the wait directive, where the deadlock will occur. if there is no wait directive, the deadlock will occur at the end of the code. finally, the interaction between the asynchronous and wait directives will determine how the deadlock will behave."
"in some situations, the synchronization between the cpu (host) and the gpu (device) is important to maintain data coherence. therefore, data updating operations between host and device and vice versa should be done carefully, and programmers must determine when and how to update their data; otherwise, this can cause a race condition. the code in figure 8 is an example of a race condition that occurs because of cpu/gpu synchronization. the elements in the ''hist'' array might be updated by multiple threads concurrently, which causes a data race. the openacc update directive updates the values of the ''hist'' array between gpu and cpu in the same data region without ensuring data independence between each element because of the openacc parallel directive, which depends on the programmers to ensure data independence for each element in the array. finally, when programmers use ''kernels'' to parallelize their code, they will avoid the data race, but they will lose loop parallelism because the code will execute sequentially, as the usage of ''kernels'' relies on the compiler to determine dependency in this case."
"the openacc reduction clause can generate private variable copies for each loop iteration in the openacc compute regions, collecting and reducing all these copies into one final result based on the specified operations, which will be returned from the compute region to the cpu. the operator on the scalar variable can be specified by the reduction clause, which supports some common operations such as summation, multiplication, maximum, minimum, and various bitwise operations. some compilers can detect reduction of the summation variable and implicitly insert the reduction clause, but for other operations and other compilers, the programmers should always indicate the reductions in their codes. even though the reduction clause can avoid some data dependencies by combining the results of each copy of the reduction variable with the original variable at the end of the compute region, the absence or misuse of this clause will lead to a race condition in some cases. also, the variables involved in the reduction clause must be initialized properly based on the reduction clause operations before using them in the clause, or undefined behavior will result. figure 13 shows an example of a race condition that occurs as a result of reduction clause absence. the variables ''sum'' and ''multi'' will cause a race condition resulting in wrong results, and they should be included in a reduction clause. some compilers can detect that the variable ''sum'' needs to be in the reduction clause and will implicitly generate a reduction clause, but other compilers cannot detect it or the variable ''multi''."
"3) shared data read and write race detetion for detecting read/write race conditions, our tool builds a table for each equation that has more than one array variable because it could have data dependency, using the related loop information to find the index values. then, compare their values to find if there are two or more threads with writing and reading in the same variable. we store this information in the data structure, as shown in figure 16: listing. 4. data dependency race detection algorithm. our tool computes the values for each equation in a given compute region and compares to test if there is a thread read and another write in the same place; this indicates a race condition called read/write race condition. also, if there is one thread writing to place and another thread writing to the same place, this will cause a write/read race condition. also, in the case of two threads writing in the same variable, this causes a write/write race condition. however, in the case of two threads reading from the same variable, this will not cause a race condition. the data structure in figure 16 will be used to detect read and write from multiple threads' race condition in our static testing tool. this will be shown in the following algorithm in listing 5, which can detect read after write, write after read, and write after write race conditions."
"one potential cause of a race condition in openacc is the misuse of the reduction clause. therefore, our static approach conducts extra testing for this situation to ensure that there is no misuse of this clause, as the compiler will not detect errors if there are any. in openacc reduction clause and for each loop iteration, variable copies are generated, and all of these private copies from different threads are reduced into one final result that returns to the cpu to be available at the end of the compute region. reduction clause operators can be specified on the scalar variables, which include several operations, but we focus on the main following four operations: summation, multiplication, maximum and minimum operators, and others in our future works. some compilers will detect reduction of the reduction variable and implicitly insert the reduction clause, but in other cases, it is the programmer's responsibility to indicate reductions in openacc codes."
"in the structured openacc data region, there are different causes of this type of error, including using the wrong data clause or forgetting to write the openacc data directives correctly, which are considered syntax errors, but the compilers do not detect them. for example, in figure 5a, the programmers use the copyin data clause instead of the copy data clause, while the array ''a'' needs to be copied from the cpu to the gpu and then copied back to the cpu. in this situation, the final result will be incorrect because by using copyin, the array will be copied to the gpu and stay there without being copied back to the cpu. also, if the keyword ''data'' has not been written in the structured data region directive, as in figure 5b, any data clause will not be activated because the compiler simply ignores the data region directive and completes the compilation, which results in incorrect values. however, when the keyword ''acc'' is missing, the pgi compiler 18.4 will generate implicit copy for the array ''a'' that only occurred in the structured data region, but in the unstructured data region the compiler will do nothing, as we see in the previous examples."
"as previously discussed, the data dependency can cause a race condition in openacc and run-time errors, as well as preventing the code from being parallelized. the compiler data dependency analysis does not always have enough information to make a decision as to whether the code can be parallelized or work sequentially, as in the case of using openacc kernel directives. therefore, programmers sometimes need to provide the compiler with this information, which can be done by using the openacc independent clause that tells the compiler that a specific loop is data-independent, meaning that there is no dependency or relationship between any two loop iterations, thus overriding the compiler's loop dependency analysis. however, the use of the openacc independent clause can be a solution, but also can cause a race condition when there is data dependency and the programmers use this clause, which allows the compiler to generate code to compute these loop iterations using independent asynchronous threads. the code shown in figure 14 is an example of using the independent clause in a loop that includes data dependency, resulting in a race condition. in this example, the programmers tell the compilers that these loop iterations are data-independent with respect to each other, but they still have a dependency that causes a race condition, resulting in wrong and inconsistent results. finally, programmers must be cautious when using this clause, because if any array element is written by iteration, and if there is another iteration that also writes or reads, this will cause a race condition, except for variables in the reduction clause."
"the following algorithm in listing 1 shows our main data clause checking algorithm, which started by exploring the targeted source code to find openacc directives that including data clauses. then, saving some openacc data clause related information for further using in our algorithm. when a compute region, structure or unstructured data region are founded we first check their openacc syntax by using (chk_acc_syntax) to find some related errors, because not all syntax errors are detected by the compiler and we will cover these errors that cannot be detected."
"after completing our openacc syntax checking, our main checking algorithm will mapping between each directive type with its appropriate checking algorithm, which includes (chk_struct) and (chk_unstruct) to test structured, unstructured data regions and compute region that will be discussed in algorithms in listings 2 and 3. in the case of checking unstructured data regions, our approach will count the number of variables that appear in enters and exit data regions. finally, a comparison of these two counters, if they are not equal, an error message will be issued to the developers because in unstructured data region the variables at the enter data region (in create or/and copyin data clauses) must appear at the exit data region clause (in copyout or/and delete data clauses)."
"finally, the last part of the code to be covered is after the region. in this part, we focus on the data clause variables' appearance as to whether it appears or is used after the region, which will cause a run-time error in some situations, and the developer will receive an error message indicating that. these error situations include the appearance of copyin and create data clauses variables because these data clauses will not move out of the gpu, so their results will not be considered by the cpu, and the final result will be wrong."
"openacc has support for both asynchronous and wait directives. when using asynchronous computation or data movement, developers are responsible for ensuring that the program has enough synchronization to resolve any data races between the host and the gpu. in openacc, there is an implicit default barrier at the end of the parallel accelerator region, and the execution of the local thread will not proceed until all threads have reached the end of the parallel region. by default, all openacc directives are synchronous; that means that the cpu thread sends the required data and instructions to the gpu, and after that, the cpu thread will wait for the gpu to complete its work before continuing execution. using asynchronous and wait directives allows the cpu to continue working while the gpu works at the same time, which allows the pipelining execution of the system and enhances performance. however, when developers use these listing. 5. read write race detection algorithm."
"also, one of the reasons for openacc deadlock in the cpu is having livelock in the gpu. this happens because of the nature of the implicit barrier at the end of each compute region. that means the gpu will be busy with the livelock while the cpu is waiting for the gpu to finish its operation. in the usage of the asynchronous directive, the gpu livelock also causes cpu deadlock, but by different behavior than in the usage of the wait directive, which will cause the cpu to have a deadlock in that statement. in this case, the deadlock behaviors are based on the asynchronous and wait directives interactions. to detect these situations, our static analysis will investigate the source code to ensure that there are no infinite loops in each compute region. any situation that cannot be detected during our static phase will be marked for further checking."
"similar to the previous algorithm, this checking process firstly receives data clause variable and type from the main algorithm and then scans the source code to allocate the variable locations in the data region, before and after. an addition check will be conducted to test the variables' appearance upon entering the region and an exit region when any variable appears at one only; an error message will be displayed to the developer. the second step is checking the data clause variable for the three locations based on their data clause type to detect any potential error and report to the developers. one example of the checking process is checking the copyin clause in the unstructured data region, which is used to allocate memory in the gpu and copy data from the host (cpu) to the device (gpu) when entering the data region. in this case, the copyin variables on entering data should be deleted or copied out when exiting the data region. however, the compiler does not detect the related run-time errors that can occur. when the algorithm in listing 3 deals with an unstructured data region copyin clause, there are some similarities and differences from the previous structured situation. before the unstructured data region, our algorithm will use the same mechanism to detect any related errors that can be affected by the copyin variables. however, in the after data region, the variable will be tested if it appears after the data region, and if the same variable is not part of the copyout clause at the exit data region, there is an error, and the developer will receive an error message to address this problem. finally, when the variable is in the unstructured data region, there are some cases where algorithm 3 detects them as errors and sends error messages to the developers. the variables in the copyin clause must appear in either the copyout or delete clauses, but not both because this will be detected and reported as an error."
"in this classification, this type of error will result in wrong results in some cases, similar to the previous classification, but this error will also affect application performance and gpu memory. the main concept behind this classification is that we identify cases of some unnecessary programming operations such as keeping unused variables and matrices in the gpu. also, they create temporary arrays in the gpu for some operations and keep them unnecessarily after finishing operations, which can affect system performance by allocating unnecessary data to gpu memory and consuming space and energy. in figure 6, the arrays ''a'' and ''b'' have been copied to the gpu for calculating the array ''c'', but after this operation has been completed, these two arrays still consume gpu memory without any further usage. in this case, the programmers should delete these two arrays per the exit data region directive. also, further errors can result from using the same gpu memory with another part of the code, which conflicts with gpu memory or slows down the execution of the other operation. this type of error only happens in the unstructured data region because the programmers' responsibility is to use the enter and exit data regions directives correctly and to determine allocation and deallocation operations based on the data clause directives. in the structured data region, the programmers determine only the data clause that they need, and the compiler deals with internal operations. when exiting the data region, the exit data directive handles gpu memory deallocation by using either the delete or copyout data clauses. finally, one of the most important issues that some programmers might not be aware of is if they have as many exit data directives for a given array as enter data directives."
"by using the openacc asynchronous and wait directives, the cpu can continue working while the gpu works at the same time, allowing the system to be executed in a pipeline manner, thus enhancing performance. as a result, the programmers can ensure the synchronization of their applications when using asynchronous and wait directives to avoid causing a race condition between host and device. the race condition can be caused by misusing these directives without considering the dependency between different parts of the code, including openacc compute regions or data movements. an example of a race condition caused by misuse of openacc asynchronous and wait directives is shown in figure 12 . the array ''a'' will be assigned to asynchronous queue number 1, while array ''b'' is assigned to queue number 2, and the cpu continues working without waiting for these two queues to be completed before computing array ''c''. the race condition occurs as a result of dependency between different parts of the code where arrays ''a'' and ''b'' are needed before calculating array ''c'', which leads to wrong results."
"also, another main contribution is to design and build a new static testing tool capable of detecting openacc errors by using static testing techniques. we built a testing tool that can detect as many openacc errors as possible by using static testing techniques. our tool has successfully detected openacc errors, including data transmission errors and memory errors, as well as some race condition and deadlock cases. also, our tool has detected openacc potential errors and marked them for further testing during run-time, which will be part of our future work."
"we noticed that some syntax or logical errors caused some of the openacc run-time errors, but compilers do not detect these syntax errors and just ignore the directive if it is not written correctly. some of the run-time errors might have similar names, but behave differently in openacc and have different causes. in our study, we included only the run-time errors in openacc applications that cannot be detected by the compilers. finally, the programmers' responsibility is to understand their code and the openacc different data regions as well as compute regions to maximize their code being parallelized and benefiting from openacc's capabilities and features."
"in the structured openacc data region, when programmers use openacc data clauses to manage the data movement between gpu and cpu or vice versa, and the array is not present in the gpu or partially present for any reason, this will be not detected by the compiler, and the error message will be issued during runtime. one reason for this situation is multiple routines controlling the data movements in the same application; therefore, the programmers should be aware of tracing the variables' movements both from and to the gpu. this error can be avoided by using the openacc api function to test for the presence of the variable in the gpu before further execution and preventing this error from occurring."
"in the case of data region testing, different situations can cause run-time errors and can be detected during the static testing phase of our algorithm and display an error message indicating the error place and type, with some additional information for the developers to consider when correcting these errors. these situations including, for example, when a copyin data clause variable is part of an equation, this volume 7, 2019 will be indicated as an error because the copyin data clause will not return the values to the cpu. therefore, any results stored in this variable will end in the gpu, and the cpu will complete execution without considering the newest result of this variable, which leads the developer to get the wrong result without the compiler knowing or detecting."
"as we noticed in table 1, the majority of openacc data clause-related errors can be detected and resolved by using our static approach, while race condition and deadlock can be detected partially and need further testing during run-time. however, our tool will minimize the number of errors that need to be detected during run-time, as well as the parts that need further testing as marked by our static tool. finally, our static testing tool has successfully detected openacc errors including data clause-related errors, data transmission errors, and memory errors, as well as some race condition and deadlock cases."
"the programmer should be careful when dealing with updating data between host and device and should know when and how to do this updating. sometimes this can cause a race condition. in this case, our static approach will investigate any update between host and device to ensure data coherence and warning the developers in case of any potential error. the data-dependency analysis can be used in some cases if it involves the updating operation. also, our static approach will insert an instrumentation statement to check the values between host and device to ensure correctness. these instrumentation statements will capture the updated variables on the one side (gpu or cpu) before the updating process and compare the results with the developer's updated variable on the other side (gpu or cpu) and compare the two values. volume 7, 2019 if they have any differences, this will be reported as an error to the developers. these instrumentation statements will be executed during our dynamic phase."
"the algorithm in listing 2 is responsible for checking data clauses founded in the structured data region or compute region because they have the same behavior and roles. the targeted openacc data clauses in this algorithm are copy, copyin, copyout, create, and present. however, the present data clause will be instrumented for further dynamic testing because it cannot be detected during our static approach. structured data region is defined as that part of the code that has explicit start and end points where data lifetime both begins and ends, and the memory only exists within the data region. compute region is defined as the region in which the computation processes are executed in gpus, whether it is a parallel region, kernel region, or serial region. our static approach targets data clause variables, checking for their initialization, assignments, and usages in detecting any error related to any data clause. some of these tests for the variables are similar, and some of them are different depending on the data clause related to the tested variable."
"the race condition is a common error in parallel applications, as well as in different programming models. however, the openacc race condition behaves differently and has different causes as a result of the nature of openacc and its execution in a heterogeneous system architecture. the race condition can arise from executing processes concurrently in multiple threads without considering the sequence of the execution, while the thread execution sequence is critical to the final result. also, a race condition can occur when there is competition between several threads to access the same memory location. additionally, when programmers use openacc for parallelizing their applications, there is no guarantee of the thread execution order [cit] . as a result, the programmers are responsible for examining their code to make sure there is no data dependency, and they should not make any assumptions about the thread order execution. therefore, openacc is more likely to have a race condition resulting from different causes and situations. we classify the openacc race condition causes into the following six categories:"
"our static tool will understand the tested source code that includes c++ and openacc, analyzing the source code syntax and semantics to be checked to ensure its correctness. different information will be extracted from the source code and displayed in a log file for the programmer for further debugging. this information will include the total number of compute region and structured and unstructured data regions, as well as their starts and ends in the source code. also, the variables in each compute region will be stored with their related information, including their related compute region and which part of the equation, as well as in which loop if it is within a loop. in addition to any information related to loops, equations, and parallel threads that will be displayed in the log file with much details of the tested source code. in the following we will explain our static approach in detecting several type of openacc errors based on our previous classifications."
"finally, it is difficult to test parallel applications due to the different factors and complicated scenarios that can cause run-time errors, as well as the nature of parallel applications and their behavior. these reasons lead to more effort to build the testing tool in terms of covering every possible scenario of the test cases and the data."
"finally, to the best of our knowledge, there is no published work that identifies or classifies openacc-related errors, as well as a testing tool that designated for detecting errors in openacc applications. in our future work, we will build a dynamic testing tool for detecting openacc run-time errors that we cannot detect or partially detect with our static testing tool."
"in section 4, we identified and classified run-time errors that can occur in openacc applications and explained them with examples. these errors can happen because of programmers' lack of understanding of openacc and misuse of some openacc directives and clauses. also, some of these errors occur when the programmers try to parallelize their code without fully understanding it and the regions that can be parallelized, considering different aspects as well as the data movements. in addition, some errors are related to the nature of openacc and how it behaves, as well as the high-level programming used in openacc where the programmers only use the directives without considering the operations behind these directives, which is one of the openacc features of which programmers should be aware."
"another cause of this error is programmers' use of openacc data clause variables designated by functions; this designation should be determined by reference, not by value. in figure 7, the variable ''val'' in the function ''func'' is not the same as the variable ''val'' in the copyin data clause in the main program. when the programmers call passes ''val'' by value, this means it will make a copy of ''val'' and send it to the function, which will then add it to the present data clause. therefore, an error message will be issued to indicate that the variable ''val'' is not present in the gpu. the solution to this error is to pass the variable ''val'' by reference, which lets the compiler know that the variable ''val'' is on the gpu, and the function ''func'' can then use it directly."
"as one of the most important skills of helicopter, helicopter suspension cable system has drawn much attention of many experts and scholars in the past decades and many nice research results have been achieved [cit] . in the processing of helicopter lifting, if the vibration range exceeds the allowable limits, it may result in the damage of the suspension load and even threaten the life security of the pilot. thus, the research for vibration attenuation is a meaningful and challenging topic. some common and effective control strategies have been adopted to reduce vibration, such as input shaping [cit], delayed feedback control [cit], dynamic programming approach [cit] . however, input shaping and dynamic programming approach are open-loop control strategies which are sensitive to external disturbance. in addition, these methods are only suitable for the linearized system model which presents the stability of equilibrium points. considering the process of helicopter lifting, the aforementioned approaches are not suitable to eliminate the vibration for the helicopter lifting system. moreover, the distributed disturbance resulted from atmospheric"
"all openacc directives are synchronous by default, which means that when there are operations or instructions sent from the cpu to the gpu to be processed or calculated, the cpu will wait for the gpu to complete the assigned work before continuing further cpu execution. by using synchronous operations, openacc ensures the operation execution order when running on the gpu as run in the original program, ensuring that the program will work correctly with or without an accelerator [cit] . however, while waiting for gpu computation to be completed, the system resources will be unused for a while, which is not an efficient way to run the code. therefore, openacc supports asynchronous operations by assigning the work to the gpu while the cpu can complete other operations, which allows the applications to be worked asynchronously and can thus enhance efficiency."
"our architecture is responsible for analyzing the input source code to detect static errors before compilation. as we discussed openacc errors previously, we noticed that we could detect some openacc run-time errors from the source code, and these errors should therefore be resolved because they will definitely occur at run time. after compilation and during run time, potential run-time errors might or might not occur based on the execution behavior. we can detect the causes of these potential errors from the source code before compilation by using static testing. however, these potential errors will become run-time errors if they have not been detected, and the programmers should be warned and consider them."
"in this case we focus on testing two main aspects: first, to test if there is no dependency between the asynchronous directives, and if they can work pipelining without errors. in other words, the asynchronous operation region should not have variables needed by another region; otherwise, a race condition will occur because the data will arrive before or after the time that it is needed. second, our approach will test the completion of the asynchronous directives at the end of the region; otherwise, a deadlock situation can arise. the first situation can be tested during our static phase, and some instrumentation statements will be needed for further runtime investigation. the second case will be instrumented by our static approach and will need further dynamic testing to detect any errors that may occur. finally, the asynchronous operation can cause deadlock and race condition and needs to be marked by our static analyzer for further testing with a dynamic approach."
"artificial neural networks and statistical methods are widely reported for different chemical engineering applications [cit] . kappa number prediction models are useful in cases where an on-line analyzer is not available, or as an inferential sensor to be used as an additional kappa number indicator, giving a warning to the operators when large discrepancies are observed (between measured and estimated values), as a reference model for device calibration monitoring or for a better understanding of the process behavior as well. with a kappa number inference, more information can be achieved and then used to determine actions to control the process in advance."
"in this research, the causal map discovered by the conditional gc test is simplified by finding its maximum spanning tree, where a maximum spanning tree of a weighted graph is a spanning tree with a maximum total weight among all the spanning trees. in a maximum spanning tree, each node has one incoming edge except the root (i.e. the starting point) and at least one outgoing edge except the endpoints. for a causal map, maximizing the total weight in the spanning tree is equivalent to retaining the most significant causal relationships during the simplification procedure. therefore, it is expected that all uncritical links can be removed from the causal map, while the note corresponding to the root cause variable can be highlighted because it appears as the root of the tree which has only an outgoing edge and no incoming edge."
"regardless of whether new or existing processes are to be modelled, the objectives of the data analyses may be used in monitoring the state of the process. understanding the relationship between factors and responses, process diagnosis and optimization allows operators to follow the process behavior when it shifts from one condition to another. in this context, anticipating demand changes is critical in the process industry with high capacity utilization [cit] ."
"the rehabilitation equipment proposed in this paper is based on the utilization of a proportional directional control valve, allowing the development of a rapid, accurate, and cost-effective control system, adjustable to the various external loads it is subjected to. the positioning system used in this application ensures flexibility, speed, and a low cost and sufficient precision for concrete operational requirements. such systems are recommended for all linear pneumatic motors required to stop in more than two positions, and have stays of different durations."
"in this paper, we introduce multi-perspective convolutional cube (mc 2 ), a novel model for conversational machine reading comprehension. the cube is viewed from different perspectives to fully understand the history of conversation. by integrating cnn with rnn, fusing 1d and 2d convolutions, extending causal convolution to 2d, our model achieves the best results among published models on the coqa dataset without fine-tuning bert. we will study further the capability of our approaches on other datasets and tasks in the future work."
"from perspective iii-1. we use self-attention to enhance the current passage representation as follows: at last, we view the cube from perspective i again to synthesize the global informationĥ"
"this research compares routing protocols, however, more specifically, it investigates the cause of performance loss or gain in multi hop ad hoc networks. we have confirmed the findings of the only other peer reviewed experimental study [cit] that tested babel. the conclusion is that in small wireless networks, babel offers higher throughputs. the results confirm that the overhead of olsr is higher than batman [cit], but contradict other studies that claim large throughput differences between olsr and batman [cit] . the results of this study suggest that the performance of olsr and batman is similar."
"a variable selection process helps to decrease the risk of overfitting the model by reducing the number of independent variables in the model. this task is also important when identifying neural models since redundant variables may worsen its general performance. besides, one consideration in the choice of predictor variables is the extent to which a chosen variable contributes to reducing the remaining variation in the response after allowance is made for the contributions of other predictor variables that have tentatively been included in the model. other considerations include the importance of the variable as a causal agent in the process under analysis; the degree to which observations on the variable can be obtained more accurately, or quickly, or economically than those on competing variables; and the degree to which the variable can be controlled [cit] . therefore, the stepwise method was carried out in order to eliminate variables that do not affect the kappa number significantly, with significance levels α of 0.1 for both variable inclusion and removal [cit] . as a result, 11 independent variables were selected as variable inputs to the mlr (a linear model) and ann (nonlinear model), as used in different approaches [cit] . the variable subset is listed in table 1, with the respective time delays in relation to the dependent variable kappa number (output)."
"( 1) when applied recursively to each successive observation in the series, each new smoothed value (forecasted ŷ t ) is computed as the weighted average (given by α) of the current observation (y t-1 ) and the previous smoothed observation (y t-1 ). the previous smoothed observation was computed in turn from"
"this model was used to perform forecasting from the second dataset. the histogram of residuals is presented in figure 5, with a visual evidence of normal distribution, with no residuals points beyond ± 4. this provides a better result of such model in comparison to the previous ses approach."
"the application of this concept to link state routing is known as fish-eye state routing (fsr) [cit] . these techniques have been shown to significantly reduce overheads [cit] . the reason that imprecise or slightly inaccurate information can be tolerated is because routing decisions are made on a hopby-hop basis. this means that if a node is many hops away, a route in the general direction will often suffice."
"that is 24.85%. this value deviates from a contraction of only 20% indicated in the technical specifications provided by the manufacturer. the actual capacity of the muscle to shorten beyond the values provided by its technical specifications enables the rehabilitation equipment to carry out greater strokes than those initially imposed, thus amplifying the possible variation range of lower limb joint rotation angles."
a. olsr olsr [cit] was an was an initial attempt at standardizing a proactive link-state routing protocol. olsr was first implementation by tonnesen [cit] and has been continued by numerous contributors. it is currently the most used ad hoc routing protocol.
"a prototype of the lower limb rehabilitation equipment was developed based on the kinematic diagrams presented above. figure 5 shows two views of this, in different stages of the motion [cit] . the patient uses the equipment while lying on a bed. the system can be used for either left or right leg, due to its axially symmetrical construction. an important requirement satisfied by the rehabilitation equipment is that of low weight, as well as easy handling and transport to and by patients. this is due to the utilization of light materials, in particular aluminium, from which most components are made."
"to estimate the coefficients in the regression model, usually an ordinary least squares (ols) method is used due to both its mathematical convenience and the ability to provide explicit expressions for the model [cit] (2) in this instance, multiple linear regression was used to determine the statistical relationship between the response (kappa number) and the explanatory variables (digester process variables)."
"partially interconnected. the multi-layer perceptrons (mlp) is the most popular neural network architecture in use today, where information travels exclusively from input to output nodes. this is discussed at length in most ann books [cit] . in general, one hidden layer using sigmoidal-type activation functions is enough for approximating any continuous non-linear function [cit] . the number of input and output units is directly defined by the problem. the definition of the number of hidden units to be used is part of a search procedure being defined experimentally. once the number of layers and number of units in each layer have been selected, the network's weights must be set by minimizing a prediction error function. this is the role of the training algorithms."
"using the modeling data subset, the neural network model was constructed following the steps of specification, selection and final model estimation. matlab (matrix laboratory) v.7.9 .0 was used to estimate the ann parameters."
"however, the conventional gc [cit] only considers the causal relationship between a pair of time series, while industrial processes are multivariate in nature. for constructing the entire causal map, repeated pairwise analyses can be conducted. nevertheless, pairwise gc is defined based on two principles [cit] : the cause happens prior to its effect, and the cause contains unique information about the future values of its effect over the period of analysis. in industrial processes, the second principle is often violated due to the correlation among variables. as a result, pairwise gc often gives over-complex and misleading results."
"continuous passive motion (cpm) is an optimum instrument within the therapeutic arsenal of rehabilitation professionals. correct application of continuous passive motion for rehabilitation purposes requires certain background information: the patient's degree of suffering, the accurate diagnosis of the patient's condition based on muscle and joint analysis, as well as the morphopathological state of the structures to be mobilized. the main control parameters of passive mobilization are the applied force, conducted stroke, velocity of displacement, acceleration, duration and frequency of motion, all of which need to be adapted to the clinical state of the patient and the set target. this entails equipment for passive mobilization that allows adjustment within a certain range of all mentioned parameters."
"before we describe ad hoc routing protocols, we will first discuss the challenges in multi hop ad hoc network protocols. this will explain the inadequacy of traditional routing protocols, such as rip, ospf and eigrp."
"the scalability of batman counts on packet loss and thus, like other algorithms, ogms are broadcast as unreliable udp packets. as nodes continuously broadcast ogms; without packet loss, these messages would overwhelm the network. the scalability of batman depends on packet loss and thus it is unable to operate in reliable wired networks."
"models may be divided into theoretical and empirical ones. theoretical models explain the nature of the reactions, phenomena and different process conditions. empirical models are based on experimental data. kraft pulping has been modeled to various levels of complexity. the development of chemical reaction rate expressions that take place during kraft pulping is arduous because of the heterogeneous nature of the system, multivariable and interactive chemical and physical processes and long residence times. nonetheless, modeling and simulation of pulping processes have become valuable tools to the pulp and paper industries [cit] ."
"the purpose of this layer is to extract useful information for upper layers. we embed questions and passages into a sequence of vectors with the latest contextualized model, bert [cit], separately. instead of fine-tuning bert with extra scoring layers, we fix the weights of bert like sdnet and aggregate l hidden layers generated by bert as contextualized embedding for all bpe [cit] tokens."
"routing is one of the most central and important areas in the wireless multi hop ad hoc network architecture. despite it's importance, and the hundreds of different routing protocols proposed over the past decade, few real world experimental studies have investigated routing. this research study provides an experimental comparison between optimised link state routing (olsr), better approach to mobile ad hoc networking (batman) and babel. these protocols represent different approaches to routing in multi hop ad hoc networks. olsr is a link state routing protocol and babel is an advanced distance vector routing protocol. the batman routing protocol does not fit neatly into pre-existing routing taxonomies. it can be loosely described as a biologically inspired routing protocol."
anecdotal criticisms of olsr state that a significant amount of mpr redundancy is needed to prevent link state databases from becoming desynchronized and forming routing loops. the additional mpr redundancy increases overheads; reducing performance. these criticisms led others to explore a fundamentally different approach to routing.
this study also captured routing protocol overheads. the nodes were not powerful enough to capture the traffic traversing their interfaces when routing thousands of packets persecond. this made the determination of the exact routing overhead difficult. to measure routing protocol overheads all wireless nodes were placed within range of an external capturing device. wireshark was used to capture packets over 60 second intervals. an aspect of this test which requires consideration is that the overheads of routing protocols may have been different in topologies where the nodes were not all within transmission range of one another.
"the results of the bandwidth tests, shown in fig 3, reveal that the babel routing protocol provided better throughputs than olsr, batman l3 or batman l2. fig 3 also shows that batman l2 outperformed batman l3 and olsr in three of the four topologies, however, the performance differences are too small for definitive conclusions. one peer reviewed study [cit] concurs that babel offers greater throughput than both batman and olsr. this study also found that olsr performed poorly, however, i believe that this result may have been caused by a bug in the version of olsr used."
"a popular approach to reduce routing overheads, in both proactive and reactive protocols, is to limit the dissemination of routing information. the origins of limited dissemination techniques were founded in distance routing effect algorithm for mobility (dream) [cit] . dream reduces network over-heads by updating distant nodes less frequently than nearby nodes."
"unreliability in wireless networks create numerous other problems. in link-state routing, the dijkstra algorithm provides 100% loop freedom as long as the link state databases are synchronized. reliable routing information is critical because desynchronization, which can be caused by lost updates, leads to routing loops. the difficulty in ad hoc networks is that conditions are constantly changing. the shared medium means that, for efficiency reasons, routing information must be unreliably broadcasted. overheads will therefore be higher in ad hoc routing protocols [cit] . ad hoc routing protocols require mechanisms to reduce these overheads."
"the learning curve is shown in figure 5 . it reflects the performance of models under different training epochs on the development set. we can observe that our model completely surpasses sdnet at every epoch. and it outperforms all baseline models only after 5 epochs and achieves the best performance after 18 epochs. especially, our model achieves 72.472% on f1 score only after the first epoch, which is about 10% to 20% higher than sdnet. thus with fewer training epochs, our model still can perform well."
"the batman routing protocol is being developed as both a user-space routing protocol, that operates at the network layer, as well as a kernel-space implementation running at the data link layer. this study experiments with both routing protocols, referring to them as batman l3 and batman l2. only a couple of real world experimental evaluations of these protocols exist [cit] . further experimental tests are required to ascertain the validity of the conclusions made in these studies."
"this layer plays an important role in our model, which aims to incorporate question information into passage representation further and reason from different perspectives by our proposed convolutional cube. the cube represents the hidden states of passages in a conversation. we will describe these perspectives in figure 3 in the order of x to } in figure 2 . to consider global context of each turn besides local information across different dimensions, perspective i equipped with rnn is inserted before other cnn perspectives."
we also performed bandwidth tests. one gateway node was connected to a dedicated server running the lighthttpd web server. wireless nodes were simultaneously issued instructions to download a large 158mb file from the lighthttpd server. the downloads were timed. the elapsed time between when the download command was issued and the final node completed the file transfer was recorded. these tests were performed multiple times for each routing protocol in each topology and the results were averaged.
"pneumatic muscles are designed for generating either a rotation motion (rotary elastic chambers) or a translation motion (linear pneumatic muscles). figure 1 presents two examples of rehabilitation equipment of the knee and ankle, respectively, actuated by rotary elastic chambers [cit] . such equipment developed by the friedrich wilhelm bessel institute (fwbi) of bremen, germany, is compact, economic and easy to manipulate, some of their main characteristics including:"
"the new network type for multi hop wireless networks involves electing or selecting relay nodes that are responsible for flooding link state messages. the technique is better explained diagrammatically. given the routers shown in fig 1, an efficient dissemination technique will try to find a set of nodes that can relay topology information to surrounding nodes. in fig 2, the grey routers have been elected as mpr nodes to broadcast topology information to the rest of the network. finding the minimum set that can be chosen as relay nodes is more efficient, however, it is also an np hard problem [cit] ."
"ann are data intensive, needing a considerable amount of data to get reliable results, and great care should be taken in designing and testing networks, using separated datasets. briefly, the ability of direct input-output nonlinear mapping, robustness, and the possibility of working with multiple inputs and outputs, make ann an efficient tool for modelling complex processes."
"as known, a causal map is a directed graph that represents the cause-effect relations. for root cause diagnosis, each node in the causal map represents a candidate process variable, while the edge with an arrow between a pair of nodes indicates a causal relationship between two variables. in this research, a weight is assigned to each arrow to indicate the strength of causality, which is equal to the f-statistic used in the conditional gc test. in doing this, the causal map has a form of the weighted directed graph."
"complex processes with significant time delays are difficult to optimize and control. an example of such a process in the kraft pulp mill is continuous cooking, which is the dominant pulping method in modern mills [cit] . the role of the pulp digester is to remove lignin from wood chips. kappa number is the most used index for measuring residual lignin present in the pulp [cit] . it is measured either using online concentration analyzers, or in the laboratory by lignin oxidation with potassium permanganate under acidic conditions. the digester primary control objective is to produce uniform pulp with minimum variability, contributing to keep quality and stability in the following fiber line steps. a low kappa number affects negatively pulp strengths because of the carbohydrate dissolution, resulting in a substantial loss in pulp yield. on the other hand, the main production failure in a continuous digester occurs when a high kappa number pulp is achieved, which raises the bleaching chemicals costs, organic charge to the effluent treatment station and plugging risks at the screen plant as well (which forces reduction of the production). in recent years, regarding pulp yield aspects, the trend in bleachable-grade chemical pulping has been to push the kappa number as high as possible, just below the fiber liberation point [cit] . thereby, considering the natural *corresponding author. e-mail address: flaviomcorreia@uol.com.br wood quality variations, long residence time and the tendency of the mills to end the cooking at a higher kappa number, a better accuracy in kappa number control is a keynote to digester operation."
"the construction of the mechanism includes a festo pneumatic muscle of 20mm diameter and an initial length of 750mm. the maximum relative contraction of the muscle, as specified by the manufacturer, is 20% of its relaxed length (in this case a contraction of 150mm). consequently a multiplying mechanism with a mobile pulley had to be included between muscle and slide, which amplifies the stroke of the slide to the required value ( figure 6 ). while the evident benefit of the pulley system is the doubling of the slide stroke, its disadvantage is reducing the force exerted by the slide to half of the force generated by the pneumatic muscle. as known, the force generated by a pneumatic muscle depends on its relative contraction and diminishes as the stroke increases. a force transducer was used for measurements, in order to determine the capacity of the selected muscle to develop a sufficiently great force at the slide. thus for a 150mm stroke of the muscle the force generated and subsequently reduced via the pulley system was of 260 n, which is more than enough to mobilize a disabled lower limb. it needs be pointed out that this measured value of the force exceeds the one indicated in the manufacturer's technical specifications. upon entering into the dedicated software supplied by the manufacturer of the pneumatic muscle, musclesim, the concrete input data (20mm pneumatic muscle diameter, 750mm pneumatic muscle length, 150mm desired stroke and 6 bar feeding pressure), the programme displays the theoretical force provided by the muscle as being 338.2 n. the studied equipment includes a system of pulleys meant to double the length of the stroke; implicitly the force generated by the muscle will be halved. thus, theoretically, the value of the force generated by the system should be 169.1 n. however, in our experiments, a significantly greater force of 260 n was measured. it can be asserted therefore that the muscle is capable of higher performance than specified by the manufacturer."
"time series analysis and forecasting has become a valuable tool in different applications. the ability to forecast optimally, understanding the dynamic relationships between variables, is of great practical importance [cit] . if physical interpretation is less important and a complex system needs to be described by a simple input-output model, a data driven approach may be applied. this observed behavior is mapped by a mathematical representation that does not have a physical basis. much statistical methodology is concerned with models in which the observations are assumed to vary independently. in many applications the dependence between the observations is regarded as a challenge, and in planned experiments, randomization of the experimental design is introduced to validate analysis conducted as if the observations were independent. however, many data in engineering and industries occur in the form of time series (a set of observation generated sequentially in time), where observations are dependent and where the nature of this dependence itself is of interest (chase jr., 2013) . the body of techniques available for the analysis of such series of dependent observations is called time series analysis, which may be classified as linear or nonlinear. in this paper two univariate (ses and arima) and two multivariate methods (mlr and ann) are evaluated and they are briefly described as follows."
"a flat addressing structure provides adaptable self-forming and self-healing properties, however, many of the advantages implicit in hierarchical addressing are lost. mechanisms that prevent broadcasts are problematic. address summarization is another feature that is unable to be used by multi hop ad hoc routing protocols."
"in recent years, causality analysis has received increasing attention in the research field of fault diagnosis, because a causal map can provide an intuitive way of representing the fault propagation pathways and revealing the root causes [cit] . usually, the construction of a causal map requires the knowledge from a plant engineer [cit] . in some applications, such information may be insufficient or too complicated to construct a causal map that is practical in root cause diagnosis. therefore, data-driven causality analysis methods, such as bayesian networks [cit], transfer entropy [cit], granger causality (gc) analysis [cit], and dynamic time warping (dtw) [cit], have received increasing attention. it is noted that none of the methods outperforms the others in all the situations. just as the famous saying goes: \"all models are wrong but some are useful\" [cit] in specific applications. this work mainly focuses on gc analysis which investigates the flow of information between time series using a statistical hypothesis test."
"selected variables shown in table 1 were used for identification of the mlr model (equation 2). using the software eviews (econometric views v.5), ols was applied in order to obtain a relationship between the dependent variable (kappa number) and the eleven regression variables. as a result, table 2 indicates the estimated parameters for kappa number estimation (variables are described in table 1) ."
"there are well documented problems with etx [cit] . perhaps the biggest problem is that etx does not incorporate bandwidth. this may cause etx to favour fewer slow long distance links over a larger number of high speed links. despite these problems, etx is used by numerous routing protocols such as olsr [cit] and babel [cit] ."
"although the utilization of the conditional gc technique is helpful in root cause diagnosis of process faults, the type i error, i.e. the incorrect rejection of a true null hypothesis, may still occur and make the results difficult to read. in addition, it is noted that the causal links identified by conditional gc do not necessarily correspond to the path of fault propagation. when the candidate set of the process variables is improperly selected for diagnosis, the resulting causal map may be misleading. specifically, when redundant variables are included in the candidate set, the causal map identified by conditional gc often contains causal links irrelevant to the fault propagation and becomes unnecessarily complex, which is unfavorable for root cause diagnosis. furthermore, the loops in the causal map make it more difficult to identify the root cause. in such situations, it is desired to further simplify the causal map and highlight the root cause variable. here, the maximum spanning tree is introduced to solve the above-mentioned problems."
"in this section, the tennessee eastman (te) process [cit] ) is utilized to illustrate the feasibility of the proposed method. [cit] ifac adchem shenyang, liaoning, china, july 25-27, 2018 has been widely used for testing various monitoring and control algorithms. it consists of five main units, including a reactor, a condenser, a separation tower, a stripper and a compressor. there are eight components in the streams of the plant, namely four reactants a, c, d and e, two products g and h, a byproduct f and an inert component b. totally 52 process variables, including 11 manipulated variables and 41 measured variables, are recorded in both normal operation and abnormal situations triggered by 20 different types of faults. the sampling interval is 3 min. in each scenario, the fault occurs at the 161st sampling time point. the flowchart of the te process and the variable list can be found in the literature [cit] . in the following of this paper, the process variables are denoted as v j, where j is the variable index."
"recently, many cmrc datasets, such as coqa [cit] and quac [cit], are proposed to enable models to understand passages and answer questions in dialogue. here is an example from the coqa dataset in figure 1 . we can observe that the second and third questions omit key information. it is impossible for both hubilly went to the farm to buy some beef for his brother's birthday. when he arrived there, he saw that all six of the cows were sad and had brown spots. the cows were all eating their breakfast in a big grassy meadow … …"
"3. for each node except the root, keep the incoming edge with the largest weight and discard all other incoming edges. totally (j -1) edges are retained."
"2) if u is a node in the cycle and v is a node outside the cycle, then a weight w(v c,v) is assigned to the outgoing edge from the pseudo-node v c to u."
"applying the coefficients indicated in table 4 at equation3, the fitted model was applied to the validation data set to perform predictions. a histogram of the residuals is presented in figure 8, which presents no values beyond ± 3, with a significant frequency between -1 and 1, indicating a better description of the kappa number in comparison to the first two approaches. predicting kappa number in a kraft pulp continuous digester: a comparison of forecasting methods"
"in the proposed experiment, the wireless nodes were alix 500mhz x86 embedded pcs with 256 mb of ram and atheros cm9 wireless cards. the platform and routing protocol versions can be found in table i . all routing protocols were tested with their default configuration."
"7. for each pseudo-node, select the incoming edge with the largest weight. without loss of generality, suppose this edge connects a node (u m ) outside the cycle and a node (v m ) in the cycle. then, replace the original incoming edge of v m (selected in step 3) by the edge from u m to v m . in doing this, the cycle is eliminated."
"multiple regression analysis is one of the most popular statistical estimation procedures. it is an extremely powerful tool that enables the researcher to learn more about the relationships between the data being studied [cit] . the optimal input variable set will contain the minimum input variables required to properly describe the behavior of the output variable, with a minimum degree of redundancy and with no uninformative (noise) variables. the compromise between these extremes is what is usually called \"selecting the best regression equation\" [cit] ."
"a time series is a set of observations generated sequentially in time, in a continuous or discrete way, which may be classified as linear or nonlinear [cit] . examples of linear methodologies are the auto regressive integrated moving average models, generally indicated as the arima (p,d,q) model where the parameters p, d, and q are nonnegative integers that refer to the order of the autoregressive, integrated, and moving average parts of the model, respectively. arima models are a class of models that have capabilities to represent stationary (the process remains in equilibrium around a constant level or mean, variance, and autocorrelation through time) as well as non-stationary time series to produce accurate forecasts based on a description of historical data of a single variable. the time series data is examined to check for the most appropriate class of arima processes through selecting the order of the consecutive and seasonal differencing required to make the series stationary, as well as specifying the order of the regular and seasonal auto regressive and moving average polynomials necessary to adequately represent the time series model. the autocorrelation function (ac) and the partial autocorrelation function (pac) are elements of time series analysis and forecasting. ac measures the amount of linear dependence between observations in a time series that are separated by a lag k. a pac plot helps to determine how many auto regressive terms are necessary to reveal one or more of the following characteristics: time lags where high correlations appear, seasonality of the series, trend either in the mean level or in the variance of the series [cit] . time series and the arima method have been useful in the chemical industry [cit] and in different fields of the applied sciences [cit] ."
"the split horizon rule states that a route should never be re-advertised through the same interface that the route was received. this rule is used to avoid count-to-infinty routing loops in wired networks. in multi hop ad hoc networks, nodes must be able to rebroadcast routing information over the same interface which means that split horizon may not be used. rip and eigrp are therefore inapplicable."
"traditional link state algorithms such as ospf are equally inappropriate for routing in ad hoc networks. the ospf network types that have been designed for existing wired networks, namely; point-to-point, broadcast, non-broadcast multi access, point-to-multi-point and virtual, do not meet the requirements of multi hop ad hoc networks. although multi hop ad hoc networks are a broadcast based technology, the broadcast network type provided in ospf is inappropriate. in ospf broadcast networks, designated routers (drs) are elected for multi-access ethernet segments. ospf routers within this network will maintain adjacencies with the drs. the aim is to reduce ospf's overhead by reducing the number of adjacencies. the ospf broadcast network type requires all nodes to be in direct contact, normally known as a full mesh. this cannot translate to multi hop wireless networks because, the underpinning idea in multi hop ad hoc networks is that nodes are beyond direct communication range of other nodes. the other ospf network types; point-to-point and point-tomultipoint, lead to prohibitive amounts of overhead with as few as 20 nodes [cit] ."
"the kraft pulp continuous digester is a tubular reactor where wood chips react with an aqueous solution of sodium hydroxide and sodium sulfide (referred to as white liquor) to remove lignin from cellulose fibers. most continuous digesters consist of three basic zones: impregnation, cooking and washing, where the flow of white liquor is either co-current or counter-current with respect to the chips flow [cit] ."
"various factors affect the overall kraft pulping reaction rate, including thermal and fluid dynamic factors, liquor and chip diffusion characteristics and the delignification reactions [cit] . continuous kraft pulping is a complex process by its nature. some of the reasons are raw material variability, long time delays involved, non-linear behavior, complexity of chip column dynamics, operational disturbances, scarce availability of process measurements and strong interdependencies between process stages and variables [cit] . continuous cooking is one of the major unit operations in the pulp mill and its proper control determines the quality characteristics of the brown stock pulp and subsequent stages."
"from a qualitative perspective, all routing protocols were equally reliable and rarely suffered from tcp dropouts. packet delivery ratios for the three routing protocols were also consistent. results varied for the different topologies however all packet delivery ratios were between 99.6% and 99.98%. these results differ from other experimental studies which found significantly lower packet delivery ratios [cit] ."
"the equipment proposed in this paper lends itself, in addition to recovery of lower limb joints (hip, knee, ankle) via continuous passive motion also to utilization by athletes, for training leg musculature or analysing muscle performance."
"these four methods can be adapted to any continuous reactor, turning this manuscript of interest for the pulp and paper industry audience and for different chemical industries as well. table 5 summarizes some forecasting indexes from the four methods studied, as expressed in equations 5, 6 and 7. also included is the percentage of absolute deviation value lower than 1 kappa unit (which is considered an acceptable value for mill applications). arima presented the lowest mad, followed by mlr, ann and ses. arima obtained more than 90% of mad points lower than 1 kappa unit, which is very appreciable, giving reliability to digester operation. concerning the rmse and mape, arima presented the best result as well, followed by mlr, ann and ses. similar results may be observed for the mlr and the ann models, and also in the residues histogram ( figures 5 and 11) . in a general way, the kappa number is driven by its past value, as indicated in figure 6 . then, for this case, the complexity of a neural network modeling would not compensate for its use. a sensitivity analysis study, which is beyond the"
"we conduct our experiments on the coqa [cit], a large-scale cmrc dataset annotated by human. it consists of 127k questions with answers collected from 8k conversations over text passages. as shown in table 1, it covers seven diverse domains (five of them are in-domain and two are out-of-domain). the out-of-domain passages only appear in the test set. aligned with the official evaluation, f1 score is used as the metric, which measures the overlap between the prediction and the ground truth at word level."
"the main requirement for modern medical rehabilitation equipment is compliant behaviour, adjustable to patient suffering, even if at the cost of lower positioning accuracy or speed. compliant behaviour can be active or passive. while active compliance is specific to rigid drives and is obtained by means of software, passive compliance entails the elastic elements included by the structure of the actuator. passive compliance is characterized, among other features, by unlimited resilience to impacts and a more \"natural\" behaviour, thus rendering it attractive for medical recovery [cit] ."
"comparative tests were performed over four different topologies. the first topology was performed with all nodes in direct communication range of the gateway. this topology was used as a control whereby no routing was occurring. the remaining three topologies featured random placements of nodes throughout a building. no specific attempt was made to dictate a particular topology, however, the nodes were placed far enough apart to ensure a multi hopping topology. in the experimental setup, the transmission power was reduced and all wireless nodes were placed in different rooms. this study measured; packet delivery ratios, bandwidth and routing protocol overheads."
"for illustration, fault 1 is considered, which is caused by a step change in the a/c feed ratio in stream 4. in detail, the c feed is increased, while the a feed is decreased. the change of the a feed leads to a decrease of the amount of a in the recycle stream, i.e. stream 5. consequently, the composition of a in stream 6 (v 23 ) is also decreased. in order to compensate the influence of such disturbance, the feedback control system increasing the a feed in stream 1 (v 1 ), which eventually results in an increase of the a feed in stream 6 (v 44 ). according to the process understanding, the root cause should be identified as v 23, although a large number of variables are affected by this fault."
"in contrast, some researchers have used software computing methods developing empirical predictive models for kappa number using different data-driven approaches. [cit] . wood chip moisture content and densities and alkali and sulfidity in the white liquor were modeled in a pilot plant. [cit] developed a kappa number prediction model and fault diagnostics of continuous digesters using clustering techniques.the results showed the usability of the combined hybrid system in the monitoring of the process and the kappa number prediction. [cit] developed a novel method using multivariate regression to capture the dependencies among the system parameters and quality measures for large industries, presenting results of regression adjustments as an interactive case study simulation of a double vessel softwood pulp continuous digester. [cit] adapted the purdue model to the physical characteristics of a kamyr digester. this model was able to represent satisfactorily both dynamic and steady states of the digester operation, improving information from previous models. predicted data obtained from this model were compared to measured ones from mills, such as blow-line kappa number, yield, free liquor temperature profile, and pulp production rate. [cit] selected 29 cooking variables from his experience with a continuous digester, and used a mlr and ann for predictive models, concluding that the ann presented better results. galicia et. [cit] applied soft sensors using secondary measurements based on multivariate regression techniques. they developed a software sensor in order to reduce the number of regressor variables and also to provide superior prediction performance of kappa number applied in both simulated and industrial continuous kamyr digester case studies. kraft pulping has been a widely studied subject, especially concerning softwood pulp. nevertheless, there are only a few references to kappa number prediction techniques concerning statistical and artificial neural network models from industrial hardwood pulping data. in this sense, this work brings an important contribution to the studies involving hardwood processing."
") and w is trainable. then two different bilinear attention functions are used to estimate the probability of the start and end according toĥ p t,i andĥ q t . we choose the position of the maximum product of these two probabilities as the best span. for other answer types, such as yes, no and unknown, we condense the passage representationĥ p t,i toĥ p t like questions and classify the answer according to [ĥ p t ;ĥ q t ]. to train the cube, we minimize the sum of the negative log probabilities of the ground truth start position, end position and answer type by the predicted distributions."
"ospf, batman and babel have been implemented and tested as network layer routing protocols. recently, attempts have been made to create data link layer protocols such as 802.11s and l2 batman. there is considerable debate as to whether routing is better performed at the network layer or data link layer with the ietf and ieee both working on independent routing protocols. either scheme will violate the layering principle. a flat ip addressing scheme will break the usual hierarchical addressing that occurs at the network layer. equally, routing at the data link layer is fundamentally wrong because the data link layer will be performing routing; which is a network layer function. regardless of the layer chosen, traditional layering principles will be distorted."
"1) if u is a node outside the cycle and v is a node in the cycle, then a weight w (u,v c ) is assigned to the incoming edge from u to the pseudo-node denoted as v c ."
"an important characteristic of this equipment is its adaptability to various dimensions of patients' lower limbs (children, adults). in this respect the joints of the mechanism can be brought closer together or distanced by simply adjusting the length of the bars."
"we compare our mc 2 with other baseline models 2 in table 1 : pgnet [cit], drqa [cit], drqa+pgnet [cit], augmented drqa [cit], bidaf++ [cit] ), flowqa [cit] and sdnet . our model achieves significant improvement over these published models. comparing with the previous state-of-the-art model, sdnet, our model outperforms it by 3.2% on f1 score. and sdnet also takes pre-trained bert as embedding without fine-tuning. especially, our single model surpasses the ensemble model of both flowqa and sdnet. figure 4 shows the gap between in-domain and out-of-domain on the test set. although all mod-1 sdnet comes from experiments of the original author. sdnet * refers to the proportion of fig. 2 in the original paper. 2 we only consider published models on the coqa. although some models perform better on the leaderboard recently, they usually focus on fine-tuning bert model. els perform worse on out-of-domain datasets compared to in-domain datasets, our model only drops 3.8% on f1 score. it is the smallest drop between in-domain and out-of-domain among all models, which proves that our model has very good generalization ability. besides, our model achieves the best performance on both in-domain and out-ofdomain datasets."
studies conducted on the motion limits of lower limbs revealed the amplitudes of the rotation angles to be achieved by the bearing joints ( figure 3 ):
"as indicated in figure 6, the correlogram of the first dataset shows a slow continuous decay from autocorrelation, and significant bars from partial correlation until second-third order. the correlogram also indicates that the kappa number exerts a strong influence on the next value. this way, arma (1,2), arma (2,1), arma (2,2) and arma (1,1) parameter subsets were tested, presenting good results (in this order) as shown in table 3 . considering accuracy and parsimony properties, arma (1,2) was chosen as the best forecasting model and its estimated parameters are displayed in table 4, where c is the constant term, ar(1) the autoregressive, ma(1) and ma(2) the moving average terms. ac and pac functions from residuals are presented in figure 7, indicating a white noise and homoscedasticity of residuals."
"further on, an innovative constructive solution is presented for rehabilitation equipment for lower limb bearing joints (hip, knee, ankle), endowed with a pneumatic muscle-based actuation system. the paper is structured as follows: section two presents the kinematic diagram and the construction of the rehabilitation equipment, section three describes the structure of the actuation system and section four presents certain functional characteristics of the equipment. the paper concludes with section five including the main conclusions and outlining future directions of research."
"after defining the architecture model, the second dataset was used for validation. to maintain the same criteria for comparison with the 3 methods (ses, mlr and arima), a specific algorithm was developed in a matlab code to use the parameters obtained in modeling data from the validation dataset, considering that the ann default of the software uses the same dataset for training, selection, and validation of model parameters. thus, a conjunct of residuals (predictedobserved) was obtained, for which the histogram is indicated in figure 11 ."
"as a general conclusion to the research reported in this paper it can be asserted that the results obtained prove the capability and high performance of the proposed rehabilitation system, as well as the eligibility of pneumatic muscles as adequate actuators of medical recovery systems."
"control of a pneumatic muscle is difficult because of the non-linearity of its physical parameters. over time various control strategies were developed for pneumatic muscle actuated equipment, and a number of research reports on its positioning systems can be found in literature. for example a kohonen-type neural network was used for positioning the end-effector of a robot with a precision of up to 1 cm [cit] . papers [11…15] argue in favour of controlling pneumatic muscle-actuated manipulators by pid, of fuzzy pd + i learning control, robust control, feedback linearization control and variable structure control algorithms."
"conditional gc [cit] ) is a multivariate version of gc, which includes simultaneously all measured variables into the ar models. in conditional gc analysis, the full model has a form of"
"this paper discusses kappa number estimation using different modeling approaches in a continuous cooking process. data from an industrial continuous digester were used to compare the performance of these kappa number predicting methods. four different methods were compared considering accuracy of the results. ses and arima methodology were developed in dynamic models using the observed and predicted kappa number values. mlra and ann models were done with 11 process input cooking variables. considering that none of the data points included in the validation subset were used in the training phase, it is possible to conclude that the ann, mlr and arima models are quite acceptable considering practical application in predicting kappa number, providing digester operators with an accurate on-line estimation to be used as an inferential sensor. these models presented a desirable normal distribution with zero mean in residuals. considering the results obtained in this study, the arima model showed better accuracy when compared to the others, according to all statistical forecasting indexes evaluated, followed by mlr, ann and ses. with these measurements it is possible to estimate the blow-line kappa number before the end of the cooking process, allowing the operating personnel to make faster corrections concerning kappa number deviations. arima methodology may be a useful tool for pulp mills, since it can be applied to optimize and control the cooking process and may be easily included in any electronic spreadsheet, updated from time to time as more data become available."
"to buy some beef mans and machines to understand such questions without dialogue history. most of existing methods consider conversation history by prepending previous questions and answers to the current question, such as bidaf++ [cit], drqa+pgnet [cit], sdnet and so on. however, the latent semantic information of dialogue history is neglected. and the model may confuse some unrelated questions and answers in a sentence. although flowqa [cit] utilizes intermediate representations of previous conversation, the flow mechanism can not synthesize the information of different words in different turns of conversation simultaneously. moreover, previous models only use recurrent neural network (rnn) as their main skeleton, which is not parallel due to recurrent nature. and rnn can only grasp information from two directions, either forward or backward. but for conversation, humans usually consider history from different perspectives and answer questions comprehensively."
"considering the author's experience working with the process control of this pulp mill, seventeen process variables that influence the delignification reactions were selected, which are: chip bulk density, chip consistency, chip bin retention time, chip bin temperature, chip meter speed, liquor/wood relation, effective alkaline charge, sulfidity, top digester temperature, top digester pressure, upper cooking screen alkali concentration, upper cooking screen temperature, lower cooking screen alkali concentration, lower cooking screen temperature, lower extraction percentual flow, washing liquor flow/ chip speed relation and previous kappa number."
"conversation is one of the most important approaches for humans to acquire information. different from traditional machine reading comprehension (mrc), conversational machine reading comprehension (cmrc) requires machines to answer multiple follow-up questions according to a passage and dialogue history. however, these questions usually have complicated linguistic phenomena, such as co-reference, ellipsis and so on. only considering conversation context profoundly can we answer the current question correctly."
"multi hop ad hoc networks are designed to have selfforming and self-healing properties to deal with topology changes. given the failure of links or nodes, these networks must automatically reform. in traditional wired networks, directly connected interfaces are configured with ip addresses in the same subnet. hierarchical addressing schemes will not work in multi hop ad hoc networks because; following the failure of one link, a new ip address and network mask would be required to reform a link with a different router. thus the addressing structure should be flat."
"the characteristic feature of olsr, which differentiated it from competing link state routing protocols, were mprs. mprs reduce the number of redundant link state transmissions by electing specific nodes as relays. selection is performed in a manner such that every olsr node is a direct neighbour of a mpr. the olsr protocol also uses fsr techniques which will frequently update nearby nodes and infrequently update distant nodes. fsr reduces the overhead of link state messages in larger networks."
"in a general way, setting aside ses (the simplest), all the methods present good accuracy. moreover, arima showed better values in all forecast indexes. likewise, more than 90% of the prediction data is lower than 01 kappa unit of deviation in the arima model, confirming it to be the best option among the analyzed models."
"making a forecast is to predict a future observation. forecasting is an important issue for manufacturing companies. several decision making processes need accurate forecasts in order to choose proper actions relevant to different production aspects. for this reason, over the years practitioners and academics have devoted particular attention to how forecasting can be improved to increase forecast accuracy [cit] ."
"hop count simply favours the path with the least number of hops. the hop count metric is used for simplicity, not performance and the limitations are well known. the traditional problems worsen in multi hop ad hoc networks because paths with fewer hops are likely to be routes between distant, lower data rate links. in many cases this will lead to the utilization of longer distance, lower speed paths. these unintentional cross layer interactions led to performance degradations [cit] . hop count performs poorly in multi hop ad hoc networks [cit] ."
"in multi hop ad hoc networks, the self forming, self healing characteristics mean that variables such as bandwidth and delay cannot be manually entered as they are in ospf or eigrp. as a result, designing routing metrics for multi hop ad hoc networks is a difficult endeavour."
"to measure packet delivery ratios, a simple ruby program was created to send icmp messages from the gateway to all nodes. the program was written such that only one icmp was present in the network at any one time. this ensured that the losses measured did not include congestion based losses. these tests were based on the success of 10,000 icmp messages and were performed many times for each routing protocol in each of the four topologies."
"as regards the service life of the deployed pneumatic muscle, this has not been the subject of the conducted tests. however, considering the manufacturer's specifications, the expected service life of the muscle ranges from 100,000 to 10 million switching cycles for typical applications."
"due to the rapid development of technology, the scale and complexity of many industrial plants is continuously increased. in order to ensure process safety and productivity, a large number of sensors are installed for data collection and process monitoring. in recent decades, data-driven multivariate statistical process monitoring (mspm) techniques have been widely applied to both continuous and batch processes [cit] ."
"besides indexes mad, mape, and rmse described in equations 5-7, models were selected using others statistical decision criteria, like the akaike information criterion (aic), schwarzs bayesian criterion (sbc), durbin-watson (dw), and theil inequality coefficient (tic). except for dw, a lower mean is considered better in the evaluation of all these criteria. the dw metric is an error pattern indicator (if the pattern is random, the dw will be around 2). these and related scalar measures to choose between alternative models in a class are discussed in some texts on statistics [cit] . the eviewsv.5 software was used to estimate the parameters for the arima models and subsequent statistical analysis. based on both the autocorrelation function (ac) and the partial correlation function (pac), arma models were identified (from"
"the expected transmission time (ett) metric [cit] improves etx by adding the ability to measure bandwidth. ett implementations are limited because they require a standardised way to obtain the data rate from the wireless driver. until such mechanisms are widespread, ett implementations will be problematic and suffer from interoperability problems. ett is a significant improvement over etx, but, it is difficult to practically implement."
"a re-estimation of the weights matrix for the ann model, using both the training and the validation datasets, was carried out. figure 10 depicts the final neural network model with eleven inputs and three hidden neurons. the error can be determined by running all the training cases through the network, comparing the actual output generated with the desired or target outputs. the algorithm therefore progresses iteratively, through a number of epochs. in each epoch, each training case is submitted in turn to the network, and the target (collected in the mill) and actual (model estimates) outputs are compared to the error calculated. this error is used to adjust the weights, and then the process repeats. the initial network configuration is at random, and training usually stops when a given number of epochs elapse or when the error stops increasing."
"upon inspection of the batman l2 and batman l3 results, the only major performance difference between data link layer routing and network layer routing is the packet size. data link layer routing protocols will not require a network layer ipv4/ipv6 header and may therefore be smaller. this argument is of minimal consequence because the routing protocol is a far bigger determinant of overheads. to illustrate this point, babel, which uses a large ipv6 header, increasing the packet size of every routing message by 40 bytes, has lower overheads than batman l2 which operates without a ip header. we conclude that the layer has few performance benefits or drawbacks and that the decision to use one or the other should be architectural."
"the artificial muscle models roughly the functioning of human muscle fibres and has a number of beneficial characteristics such as shock absorbing capacity and shock resistance, low weight, reduced dimensions and low mass per power unity, stick-slip free operation, elasticity (springlike behaviour) due to air compressibility on one hand and variation of force with displacement on the other. compliance is directly related to air compressibility, and consequently pneumatic muscle behaviour can be influenced by adjusting the control pressure. the magnitude of the compliance can be calculated starting from the expression of the force developed by a pneumatic muscle when subjected to a pressure p. in this case muscle volume increases by dv, corresponding to the modification in length dl, and the force is [cit] :"
"in addition to the functional characteristics described in section four of the paper, the economic aspect entailed by such rehabilitation equipment is also significant. the cost of the described constructive solution actuated by a linear pneumatic muscle is estimated at 60-70% of the equipment currently available on the marketplace, actuated by electric motors."
"where rss 0 is the residual sum of squares (rss) of the reduced model, rss 1 is the rss of the full model, and n is the total number of observations used to estimate the models. the null hypothesis is rejected if the f-statistic is greater than the confidence limit corresponding to a desired false-rejection probability."
"in our study, which used the default routing parameters, babel consistently outperformed other protocols. we questioned whether this was due to a better selection of routes, or lower overheads. recall that topology 1, was designed as a control and all nodes were within direct range of the gateway. a curious artefact in the results is seen in topology 1. in this topology, no actual routing decisions were being made. as no routing decisions were being made in topology 1, throughput differences were likely a result of protocol overheads."
"by fitting the models in (5) and (6) and conducting the hypothesis test with the f-statistic defined in (4), [cit] ifac adchem shenyang, liaoning, china, july 25-27, 2018 conditional on other variables does not lead to the rejection of the null hypothesis. as a result, the causal map obtained by the conditional gc test is largely simplified and more meaningful comparing the results of the conventional pairwise gc, based on which it is easier to find the root cause of the process fault in diagnosis."
"rmse: root mean square error (7) reliability was measured by the mad and the mape. accuracy was measured by the rmse. a benefit of the rmse is that it is measured in the same units as the original data, while its drawback is that large errors can dominate the value [cit] . these forecasting indexes from the four methods are summarized in table 5 . in addition, the residuals (predicted(observed values) were also considered by means of the residuals histogram. the results for each approach are depicted in the following."
"this paper presents pneumatic muscle-actuated rehabilitation equipment for lower limb bearing joints. the mechanical structure of the equipment is described, as well as the structures of the actuation and positioning systems together with their operational characteristics. special focus is placed on describing the utilized pneumatic muscle, highlighting its special characteristics that render it optimum for medical equipment developed for patient rehabilitation by continuous passive motion."
the linear motions of the equipment are generated by a pneumatic muscle that displaces a slide (rotational and translational element d) along a distance up to 300mm. this stroke is required for simultaneously satisfying the initial conditions imposed for the maximum amplitudes of the bearing joint rotation angles.
"in this context, the objective of this study is to compare performance of different dynamic inferential models for kappa number prediction. four methods -single exponential smoothing (ses), box-jenkins (arima), multiple linear regression analysis (mlr) and artificial neural networks (ann) were used to formulate and compare the kappa number inferential capability of a eucalyptus kraft pulp continuous digester. advantages and limitations of these four methods were discussed."
flat addressing requires a significantly larger routing table because a separate routing entry will be required for every node. larger routing tables and changing link conditions may also impose frequent updates and heavier cpu loads.
"single exponential smoothing is a method used to smooth and forecast a time series without fitting parameters of a model. it is based on a recursive computing scheme, where the forecasts are updated for each new incoming observation. exponential smoothing is considered a simple prediction technique, yet it is used in practice where it shows good performance [cit] . it is used for short-range forecasting, usually just one step into the future. the model requires a large number of observations, assumes that the data fluctuate around a reasonably stable mean, i.e., it is not appropriate for data that has a seasonal component, trend or consistent pattern of growth [cit] . the formula for simple exponential smoothing is expressed as:"
traditional routing protocols such as rip and ospf update too infrequently to deal with the constant changes that occur in multi hop ad hoc networks [cit] . a frequent stream of hellos and topology exchanges is required to track the constantly changing link conditions. ad hoc routing protocols require significantly lower hello and topology exchange intervals.
"a particularity of the schematic of figure 6 is that, upon cutting compressed air pressure, the slide is unable to return to zero position on its own. once no more compressed air is fed to the pneumatic muscle, its free end returns to its initial, resting position. this, however, does not entail the returning of the slide to return to its initial (withdrawn) position, as there is no force to cause this. for this reason the returning of the slide is ensured by including a repositioning spring in the construction of the equipment."
"artificial neural networks (ann) have been successfully applied not only for chemical engineering purposes [cit], but also in many other different fields. indeed, in any situation that offers difficulties for predicting the behavior, classification or control of a system or a process, neural networks have been used successfully. power and ease of use (although using sophisticated modeling techniques) are the ann key success factors. using representative process data and training algorithms, the network may learn the data structure. they are applicable to situations in which a relationship between input and output variables exists, but this relationship is too complex to be described in an explicit or phenomenological way [cit] . an ann is a parametric model composed of process units called nodes (or neurons), ordered in layers and fully or"
"to introduce other linguistic features token by words and facilitate answer selection, we choose the first token of a word in bpe to represent the word. generally, the first token is often the root of the word and can represent main meaning of the whole word. and it also contains information of rest tokens in the word with the bidirectional structure of bert. besides, we split the long sentence by shorter windows and combine them again when the sentence exceeds the maximum length of pre-trained bert."
"lower limb joint rehabilitation equipment currently available on the marketplace has inconveniently rigid structures, displaying a deficiency in self-adaptability to the patient's state of mobility. for this reason the current electromechanical actuation system needs to be replaced by another capable of ensuring adaptability, conformity and safety. an adequate actuator for a rehabilitation device needs to provide physically adjustable compliance and damping, must facilitate energy storage and release at certain given times, and has to ensure soft contact with the patient, similar to the behaviour of human muscles."
"in the cases of root cause diagnosis, x 1 and x 2 represent the time series of two different process variables. because the industrial processes are inherently multivariate, repeated pairwise analyses are often required to construct the entire causal map. however, the correlation among the process variables breaks the second assumption of the pairwise gc analysis, affecting the interpretation of the results. the multivariate gc analysis technique is better suited to such situations."
"for both inflation and deflation the generated contractions were measured for each integer value of the pressure. as illustrated in the graph, the greatest difference between muscle contractions is of 20.86mm and occurs for feeding compressed air at a pressure of 2 bars."
"our experiment compares olsr, batman and babel. attempts were made to use ad hoc on demand distance vector (aodv) routing, however similar to recent studies [cit], implementation problems made this infeasible. attempts were also made to use the open 802.11s [cit] hybrid wireless mesh protocol (hwmp) routing protocol. unfortunately, this routing protocol only works with a newer wireless driver known as ath5k. due to performance problems with this driver, it was necessary to revert to the older madwifi driver which ruled out open 802.11s as a consideration."
"the overhead of the routing traffic, in bytes, is shown in table ii. this shows that under default settings, olsr transfers the greatest number of bytes of routing protocol overhead. comparatively, babel produces a minuscule overhead, however, the number of bytes transferred is not a exact measure. due to the fixed overheads of ifs (inter frame spacing), dcf (distributed coordination function), preambles and trailers; updates are more efficiently transferred in fewer large packets rather than multiple small packets. table ii shows the number of routing packets transmitted per minute. it is evident that batman transmits the largest number of routing packets/frames. recall that batman detects routes by frequently broadcasting small ogms. this describes why batman transmits the largest number of routing messages. it also explains why the average packet size is so small because batmans ogms do not carry data describing routes. babel and olsr routing messages are comparatively larger because they carry routing information."
"the passive compliance of an actuator can be constant (its modification being achievable only by replacing the elastic element), or can be variable, which ensures a permanent adaptability to a given situation. at present several actuators with passive variable compliance are known, characterized by energy efficiency and robustness, safe operation, and biomimetics, consequently including characteristics copied from biological systems. pneumatic muscles and actuators with variable passive compliance have such biomimetic behaviour, characterized by the property that their apparent output stiffness, and thus the stiffness of the actuated joint, can be changed independently of the actuator output position."
"the safety of the system is controlled by both hardware (limit switches) and software if the motion velocity is increased over the limits. the most important aspect, however, is related to patient safety and concerns avoiding the occurrence of pain during rehabilitation exercises. this is the main reason for that pneumatic muscles are used, as air compressibility renders their behaviour inherently compliant."
"the data used in this work were collected from a eucalyptus kraft pulp continuous digester, as indicated in figure 1 . the equipment under study is a kamyr single vessel vapor phase digester using the extended modified continuous cooking emcc process [cit], from a market pulp mill of 500,000 air dried metric tons (admt)/year capacity, located in minas gerais state in brazil."
"disabilities caused by posttraumatic afflictions of lower limb bearing joints are known for their high incidence, namely approximately 55% of all patients admitted to medical recovery clinics [cit] . this calls for the deployment of specific recovery equipment that takes over part of the work of kinetic therapists. this paper presents the construction and functional characteristics of a piece of equipment developed for the rapid recovery of patients by passive motion, the novelty consisting in its actuation by means of pneumatic muscles."
"the entire bar system is set into motion by translation element d, driven by a pneumatic muscle. deployment of the pneumatic muscle as actuator solves certain main requirements of rehabilitation equipment. thus the price of a muscle is significantly lower than that of an actuation system made up of an electric motor and a screw-nut mechanism, hence a lower-cost final product. also utilization of a pneumatic muscle eliminates end-ofstroke shocks, due to one of the most attractive aspects of pneumatic actuation -compliance -meaning favourable answer to commands."
"a separate conclusion is that, in small multi hop ad hoc networks, the overhead of the routing protocol has the largest impact on throughput. in the future, similar tests should be run in larger experimental set-ups. statistics on cpu load and convergence time could also be interesting. this study concludes that babel provides higher throughputs in small networks but it is untested in larger networks. these findings should provide the impetus for further experimentation."
"starting from the kinematic diagram of the equipment, the correspondence between the rotation angles of the knee and the hip joints and the distance travelled by the slide could be established, considering that this is the only motion parameter that can be entered into programmes written in winpisa. figure 9 shows the graph of slide displacement versus time. for this cycle of motion winpisa also plots the evolution of slide speed versus time and of speed versus displacement. figures 10 and 11 show these dependencies. analysis of the graphs yields the following conclusions:"
"white liquor penetrates and diffuses into the wood chips as it flows down through the impregnation zone. the mix is heated to a target cooking temperature where bulk delignification starts, and the majority of lignin is removed. the cooking process is stopped at the beginning of the washing zone by reducing the temperature and then cooked pulp is washed in a counter-current washing zone, using wash liquor injected at the bottom of the digester."
"due to confidentiality reasons, the kappa number dataset was standardized, i.e., auto scaled to unit variance and mean centered, according to equation 4 [cit] : figure 2 presents the time series evolution from the first dataset (used for modeling), and figure 3 presents the time series evolution from the second dataset (used to test generalization capacity). in figure 2 there are some long peaks around observations 350 and 850, while in figure 3 they are present around observations 380 and 820. these peaks occurred due to wood density variations."
"3) if neither u or v is in the cycle, the weight of the edge between them is equal to w (u,v), which is same as the corresponding weight in the original graph."
"multi hop ad hoc nodes will often be low power, low cost embedded machines that must deal with a variety of environmental conditions. therefore, the cpu power of these devices will be constrained. these devices will also operate using wifi chips which offer less bandwidth than equivalent wired links. bandwidth limitations will be exacerbated in dense networks because of media contention."
"a further role of this spring is to compensate the hysteresis of the pneumatic muscle. occurrence of hysteresis is explained by the friction between the exterior wall of the muscle elastic tube and its enveloping tissue. hysteresis represents a major disadvantage in the deployment of pneumatic muscles, particularly in applications that require high accuracy positioning."
"for root cause diagnosis, a candidate variable set should be determined. here, the principal component analysis (pca) contribution plots [cit] are utilized for screening. fig. 1 shows the average t 2 contribution of each process variable between the 161st and the 240th sampling time points. the variables with the first 20 largest contributions are selected for further analysis. because of the smearing effect [cit], these variables are all outside the corresponding control limits. conditional gc test is conducted to discover the causal relationships among these 20 process variables. the result is shown in fig. 2, which can be further transformed to a causal map (not shown here). in this figure, the x-axis denotes the indices of the cause variables, while the y-axis represents the indices of the effect variables. a solid block indicates that there is a granger causality between the corresponding variable pair, which means the f-statistic is larger than the confidence limit corresponding to a false-rejection probability of 0.05. therefore, a direct way to identify the root cause variable(s) is to look for the rows with no solid block. in fig. 2, the variables correspond to the null rows are v 30, v 36, v 43 and v 44 . obviously, such results are not correct, because the real root cause is v 23 . among the identified 4 variables, only v 44 is directly affected by the root, i.e. v 23 . the reason is multi-factorial. the inherent errors in statistical inference, the redundant candidate variables, the nonstationary and nonlinear characteristics contained in variable trajectories, etc. all may lead to misleading diagnosis results. therefore, it is necessary to implement the maximum spanning tree. the result is shown in fig. 3 . it is clear that v 23 is the root cause of fault 1 because it has no incoming edge and appears as the root of the tree. also, this variable affects v 1 and hence v 44 . such result is confirmed with the process understanding. nevertheless, the maximum spanning tree does no retain the entire information of fault propagation, although it successfully highlights the root cause variable. the reason is that, in a tree, any two nodes are connected by exactly one path. however, in an industrial plant, recycle streams and feedback control loops widely exist. the real propagation path of a process fault is seldom describable with a tree. therefore, we do not recommend tracing the fault propagation with the maximum spanning tree. this method is developed mainly for the easier identification of the root cause of a fault."
"in batman, routing information is not communicated directly, instead, each node broadcasts packets called originator messages (ogms) every second. when received by neighbouring nodes, ogms get re-broadcasted. route selection for a given destination is based on the node from which the most ogms have been received for a particular destination. the number of ogms that can be accepted is limited to a constantly moving window. this window limits the history of ogms that are allowed to describe a given route."
"v. addressing and osi layers discussion routing was traditionally envisaged to occur at the osi network layer and will therefore be used with any link layer technology. however, an advantage of routing at the data link layer is that any network layer protocol may operate over the top. when using a data link layer routing protocol, ipv4, ipv6 and dhcp will be able to operate above the routing protocol and provide convenient addressing mechanisms."
"initially, data samples containing missing data, dubious values, and evident outliers were removed, as well those below 50% of the normal running production. all the process data are relative to 30 minutes frequency, and were obtained from the dcs (digital control system). the kappa number data were obtained from an on-line kappa number analyzer (kappa q -supplied by metso automation), which uses an automatic sample collecting system and makes analysis by optical properties using a previous calibration curve."
"motion therapy is generally accepted as a rehabilitation method, if a consequence of an accident or surgical intervention whereby the patient's locomotor system is affected. therapy based on devices generating continuous passive motion is used in order to restore mobility and improve the residual motion abilities of the patient."
"analysis of the operational behaviour of the bearing joint rehabilitation equipment was achieved by means of a programme written in winpisa that commands a sequence of different displacements of the stroke. the winpisa programme causes a stepwise motion of the slide by a 50mm increment, the speed being set to its maximum possible value."
"next, these variables were properly adjusted according to the retention time delay as presented in figure 1 . to exemplify this adjustment, a chip sample collected at 00:00 (hh:min) at the chip bin conveyor entrance is compared to a kappa number measured in a sample collected from the blow digester at 03:30 (hh:min). a temperature at the top of the digester (and others located in the top digester) is compared to the kappa number measured in a sample collected from the blow-line digester at 03:00h later and so forth."
"the working dataset was divided into two independent groups (months of january and february). the first one (with 1471 observations) was used as reference for model identification, that is, to estimate the model parameters, whereas the second (with 1343 observations) was used to verify the generalized forecasting capacity of the previously identified models."
"the same procedure is also used to diagnose fault 7 which is related to the c header pressure loss. the feed flow of stream 4 sharply drops when the event occurs to the process. consequently, the level of the reactor becomes lower than usual and many process variables are affected because of fault propagation. in order to maintain the reactor level, the feedback controller adjusts the flow rate of stream 4 (v 4 ) [cit] ifac adchem shenyang, liaoning, china, july 25-27, 2018 manipulating the corresponding valve opening (v 45 ). therefore, v 45 and v 4 are the most likely root cause variables. 20 process variables are selected based on the contribution plot shown in fig. 4, based on which conditional gc test is adopted for causality analysis. the result is plotted in fig. 5, according to which v 27 and v 30 are mistakenly chosen to be the root cause. the implementation of the maximum spanning tree largely improves the result. as shown in fig. 6, v 45 is the root of the tree, while v 4 is the variable most close to the root. such a result is reasonable. again, it is emphasized that the path between the nodes on the tree is not necessarily in accordance with the path of fault propagation, especially for the nodes far from the root. 6. conclusions in recent years, the causality analysis technique, gc, is utilized in root cause diagnosis of process faults. however, its performance may be affected by the correlation among process variables. in this research, multivariate conditional gc is adopted to handle this issue. in addition, the diagnosis performance is further improved by using the maximum spanning tree. the feasibility of the proposed method is illustrated by the case studies on the te process. in the end of this paper, we would like to give some future perspectives on the related research topics. first, in a strict sense, gc test, as well as many other causality analysis tools, such as transfer entropy, is only suited to analyze the causality between stationary time series. in industrial processes, the trajectories of process variables often become non-stationary when abnormal events occur, violating the presupposition of gc test. this is an important issue that affects the performance of root cause diagnosis. the utilization of dtw may solve the problem in a certain sense [cit] . however, dtw is only suited to the cases where the cause and effect time series have a similar shape. it is necessary to devote more research efforts on this topic. second, the causality analysis results are sensitive to the selection of the candidate time series. in the context of process monitoring, fault isolation, i.e. the identification of faulty variables, is an important preparation step before conducting root cause diagnosis. it is recommended to use the isolation methods that can avoid the smearing effect [cit] ifac adchem shenyang, liaoning, china, july 25-27, 2018 often affected by the selection of the time window in which the causality analysis is conducted. by using a method like gc, the task of causality analysis is similar to a model identification problem. during different time periods, the system may have different excitation signals which lead to different identification results, i.e. causal maps. fourth, a related question is whether normal operation data should be used in causality analysis. as mentioned, the identified causal relationships do not necessary indicate the path of fault propagation. therefore, it should be very careful in result interpretation. usually, using fault data only may highlight the causality related to fault propagation, while incorporating normal operation data may provide a more complete view of the cause-effect relationships among process variables. fifth, the parameter determination in the gc models is another issue to consider. for example, the conventional gc assigns a same value to the time lags of different variables. it is doubt whether it is a best way. in summary, it is of great need to bridge the gaps between the statistical theories of causality analysis and their industrial applications on root cause diagnosis."
"to reiterate the challenges, distance vector routing protocols must solve the count to infinity problem without split horizon. link state routing protocols must develop a new and much more efficient network/interface type for multi hop ad hoc networks. in addition to these problems, new ad hoc routing protocols must be able to accept a greater number of changes over less reliable links. the routers must operate with less cpu power and comparatively low bandwidth links. a flat addressing structure must also be used. this means that the advantages of heirarchial routing are lost. with flat addressing, every node will require an individual route and thus, routing tables may be large and subject to frequent change. ospf's areas, is-is's levels, administrator configured metrics and address summarization are inappropriate due to the self-forming self-healing requirements of multi hop ad hoc networks [cit] . this makes the goal of routing in multi hop ad hoc networks highly challenging."
"for model b, which assumes that the mixture ratio is independent across loci, the probability of observing the evidence is calculated by taking the product of the probability of observing the evidence at all the loci, which in turn is computed by integrating over the sample space of the mixture ratios:"
"a prevalent issue, present in all publications above, is the speed in which the hand locations are determined -a factor that greatly influences the perceived quality of interaction. delays that surpass real-time are considered unacceptable [cit] . regarding opencapsense's sophisticated processing capabilities, gesture recognition applications can be implemented on the demonstration board itself. combined with a high sampling rate this enables real-time hand gesture figure 9 . the captap controlling a multimedia application with radial menus. it is based on an array of opencapsense boards and an accelerometer to detect knocking and tapping. recognition while preserving a good precision. a recent prototype we are developing is the captap -a combined hand tracking and knock detection system that uses multiple opencapsense boards configured as a sensing array and a single accelerometer interfaced through i 2 c. the system, shown in figure 9, allows controlling typical multimedia applications with selection indicated by hand position and actions triggered by different knocking events that are registered by the accelerometer."
"considering the broad spectrum of applications for capacitive sensors, we also investigated different electrode materials. copper, which combines excellent surface resistance and low cost is the material most commonly used for electrode design. however, in many applications, such as gesture recognition, it is desirable to realize capacitive proximity sensing on transparent surfaces like windows or displays."
"this integral is approximated using a fixed set of mixture ratios in δ n−1 for each n. this set of mixture ratios was determined by employing k-means clustering [cit] to uniformly distribute the set of ratios over the simplex and are specified in s3 table. let l be the set of all loci in the evidence sample, e l be the evidence at locus l and s l be the genotype of the suspect at locus l. the str loci used for forensic dna analysis are assumed to be in linkage equilibrium and independent of each other, conditioned on the mixture ratio [cit] . hence, we obtain:"
"we have realized a smart couch (shown in figure 8 ) that can sense the postures of up to two users [cit] . therefore, we applied eight loading mode sensors that were hidden under the upholstery of the smart couch. two electrodes were placed under both arm rests, two at the backrest and four electrodes under the seating cushion. the electrodes under the armrests and in the backrests measure the distance of a person's body through a thick wooden plate and the upholstery. the electrode sizes were chosen to be as big as possible, resulting in electrodes with the size of 30 x 10 cm underneath the armrests and electrodes with a size of 30 x 20 cm under the seating cushions and backrests. the opencapsense board performs a series of measurements and then passes it to our software framework sensekit for feature extraction and classification. using weka's rbf network classifier, we can obtain a classification accuracy of more than 97.5 % for 10 different postures. this test was performed using the recordings of 18 different subjects, splitting the training and test data sets into 9 subjects each."
"bread, sleeping and eating. integrating the knowledge of capacitive proximity sensors and applying a support vector machine with a very easy feature set could enhance the classification accuracy by 6.3 % to 73.5 % [cit] ."
"differences also exist in the way the models treat the underlying mixture ratio of the evidence sample, which specifies the proportions in which the contributors gave rise to the mixture and is unknown in case-work samples. some authors assume that the mixture ratio is the same at all loci [cit] whereas others allow the mixture ratio to be different at distinct loci [cit] ."
"verbal expressions of the lr are prone to misunderstanding and cannot be coherently combined with other evidence [cit] . moreover, changing the lr verbal scale can cause a change in the way the numerical lr is communicated to the trier-of-fact. while we do not advocate their usage, they are employed in practice and thus the present paper employs verbal scales to demonstrate how lr variation between models potentially impacts the testimony of different experts."
"pervasive interfaces are often driven by a variety of different sensors, such as accelerometers or temperature sensors. these sensors can be interfaced using an expansion header driven by an inter-integrated circuit (i 2 c) bus. the board also includes a controller area network (can) interface for realizing larger sensing arrays and simplify prototyping in automotive applications, where can is ubiquitously used."
"in future work, we plan to support hybrid measurements that combine loading mode and shunt mode, which will improve the performance for various applications. additionally, we want to simplify the integration of wireless systems by providing expansion cards based on either zigbee or bluetooth. in terms of applications a major interest is augmenting and improving our gesture recognition applications by adding more sensors and studying the associated effects. finally, we will optimize the presented sensors, for example by designing more sophisticated transmitter and receiver frontends for shunt-mode sensors."
"we presented opencapsense: a highly flexible opensource toolkit that enables researchers to rapidly prototype pervasive applications based on capacitive sensors. compared to previous toolkits opencapsense supports different measurement modes, provides a basic set of reliable signal processing algorithms, and can be easily interfaced via usb, can, or i 2 c. we provide libraries for java and c that allow easy integration of opencapsense into existing projects and a strong integration of the weka framework to support machine-learning based post-processing of the sensor data."
"in the majority of cases-i.e., 91.34% (738 out of 808) of the cases-the lrs from all five runs fell in the same category or bin, resulting in the same verbal expression, or interpretation, based on the five lrs (table 4, fig 1) . in all four models, the lrs for all the 1-person samples, except one sample for which model c and model d led to more than one verbal expression, fell in the same bin, indicating there is little ambiguity in demonstrating the level of support for one hypothesis over the other in single source samples. we observed that in certain 2-and 3-person mixtures, the lrs from different runs fell in different bins, leading to more than one verbal expression. these lrs were typically associated with individuals who were minor contributors or had low template masses. of these, most cases involved lrs falling in adjacent bins leading to verbal expressions of 'very strong' and 'strong' or 'strong' and 'moderately strong'. in 12 instances, with all four models, (last column of table 4 ) the lrs fell in three verbal bins or fell in two bins that were not adjacent to each other. moreover, we observed that in one 1-person profile, two 2-person profiles and in four 3-person profiles the lrs for a contributor with a low template mass fell both above and below 1, emphasizing the uncertainty associated with evidence from contributors with low template masses. for example, in a 1:4:4 0.28ng 3-person mixture, model d had higher lrs for contributor 1 (starting template mass: 0.03ng) out of the 202 sets of lrs, the majority resulted in the same interpretation between runs. intra-model variability increased with an increase in the number of contributors and with a decrease in the contributor's template mass. the range in which the models exhibited intra-model variability differed between models."
"smith [cit] distinguishes three capacitive measurement modes that are outlined in figure 2 . loading mode relies on measuring the displacement current caused by the presence of a grounded object in proximity of a single transmitting electrode. the main advantage of this mode is that electrodes can be placed arbitrarily and can be easily shielded against the influence of existing electric potentials in the neighborhood. exemplary applications that rely on this sensor mode are capacitive tables, shelves and couches [cit] . the second method, the shunt mode, is based on the principle that a grounded body part affects the electric field between a transmitter and a receiver electrode. in this 2 e.g. http://arduino.cc/playground/main/capacitivesensor mode, each sensor electrode can be configured as either transmitter or receiver. measurements can be conducted between all receiver-transmitter combinations, resulting in"
"the loading mode sensor is based on a timer configuration called astable multivibration [cit] . the timer controls the charging and discharging cycles of the capacitor that is created by the sensing electrode and the surrounding environment, e.g. a person's limb or body. it toggles succeeding charging and discharging cycles at the time when an upper or lower threshold voltage at the virtual capacitor is reached. in order to measure the capacitance in a certain direction and prevent disturbances from objects nearby, a shield electrode can be placed directly underneath the measuring electrode. the shield is driven with the same potential as the sensing electrode, such that the capacitance between the two electrodes is negligible."
"the four models tested are variants of the probabilistic models used by ceesit [cit] . these models were chosen to reflect common modeling assumptions in the published literature, as discussed in the introduction."
https://doi.org/10.1371/journal.pone.0207599.t004 . contributors 2 and 3 in this sample both had 'extremely strong' interpretations from all models and their p-values had an upper bound of 10 −9 (fig 2) .
"we have created two exemplary sensors that can be applied to realize all measurement types. loading mode sensors are especially suitable for implementing larger sensing systems for activity recognition or whole-body interaction [cit] . shunt mode sensors can be applied to realize gesture recognition interfaces with a high spatial resolution [cit] . using transmit-mode measurements, we can distinguish between different users. this measurement mode can be realized applying our shunt mode sensors."
"we have developed a generic software framework called sensekit that has an interface to opencapsense [cit] . it analyzes the inputs of the virtual serial port created by the opencapsense board. sensekit has a generic processing pipeline for classification and evaluation, it supports windowing, feature extraction and different classifiers. the strong integration of the weka framework 3 eases the development of applications relying on machine-learning, such as activity recognition. therefore, classes can be recorded as training data and then evaluated in real-time classifications. moreover, platform-independent opencapsense apis for c and java can be used for developing own applications."
"within the forensic sciences, the accepted method by which to report the weight of dna evidence in the courtroom is by presenting likelihood ratio (lr), which compares the probability of observing the evidence under two alternative hypotheses [cit], and is expressed as:"
"for purposes of this work, we use the true number of contributors, n, for the analysis of each sample. we employ the following alternative hypotheses for h p and h d in the lr calculation."
"the evaluation results for a shunt mode sensor with a measurement window of 20 ms are illustrated in figure 6 . the results show that smaller electrode sizes provide a better performance at low distances, while bigger electrode sizes are more suitable for detecting objects at high distances. it is notable that the measurement object turns into transmit mode at object distances below 5 cm, letting the sensor values increase again. this distance depends on the electrode size: when using small electrodes with a size of 50 x 50 mm, transmit mode comes into effect at approximately 2 cm, while the distance increases to about 5 cm on larger electrodes sized 100 x 100 mm."
"in recent years, pervasive interaction technologies like body parameter sensing and gesture recognition replaced input modalities such as keyboard and mouse in many applications [cit] . the realization of such new user interfaces requires one to consider a variety of interaction modalities. besides different multi-touch and camera-based interaction systems, the research area of capacitive sensing for pervasive interaction gains increasing interest."
"the mixture interpretation process can be thought of as a binary hypothesis test in which the hypotheses are as follows: the lr is a statistic that expresses how many times more likely the data are under one hypothesis than the other. however, a large lr does not necessarily mean that the person of interest is a contributor, nor does a small lr preclude the person of interest from being a contributor, since the lr is sensitive to the quality of the data as well as to assumptions on the dropout probability, number of contributors, etc. [cit] . the data presented herein demonstrate that for certain samples, the lr varied to a degree that affected a verbal classification based on the model used. the p-value of the lr is a summary statistic of the lr distribution conditioned on the defense hypothesis: it is the probability that a randomly chosen individual has an lr at least as large as the person of interest's lr. one informative aspect of the p-value is that if the classification of individuals as contributors or non-contributors based on the pvalue is that it allows control of the type i error rate, or false positive rate (fpr). the fpr is the probability of incorrectly rejecting the null hypothesis when it is true and misclassifying the person of interest as a contributor."
"currently, it is challenging for researchers to experiment with capacitive proximity sensing as there are few publically available hardware solutions. moreover, data on measurement resolution is rarely available, distances and measurement techniques. opencapsense, shown in figure 1, is a novel open-source toolkit for capacitive sensing and is presented in this paper. it allows capturing and analyzing data from a multitude of capacitive sensors at update rates of up to 1 khz. this enables prototyping novel pervasive real-time applications based on capacitive sensing, such as multi-object tracking and fall detection."
"current capacitive proximity sensing systems for localization and fall detection face the problem of having a very low update rate of 10hz or less [cit] . thus, a fall is usually detected by evaluating the time a person lies on the floor. however, a fall detection could benefit greatly from a very high update rate that enables a system to reconstruct the course of the fall precisely. we placed eight sensors and their corresponding electrodes (30 x 20 cm) under an ordinary carpet. the smart carpet can detect a fall with a very high update rate of approximately 104 hz for each sensor using a round-robin sensor scheduling. therefore, we have reduced the measurement interval to 1.2 ms. this very small measurement interval is associated with increased noise but can reliably detect a fall situation as shown in figure 11 ."
"regarding the loading mode sensor, we evaluated the influence of different electrode materials and sizes. the final results for a measurement window of 10 ms are shown in figure 6 . spatial sensor resolution of a shunt mode sensor in relation to the surrogate arm distance. figure 5 . as expected, larger electrode sizes provide a better resolution than smaller ones. at distances of 20 cm, the resolution of a 100 x 100mm copper electrode is almost four times higher than the resolution of an electrode with a size of 20 x 20 mm. regarding different materials, transparent ito electrodes perform almost as well as copper electrodes. although ito has a higher surface resistance than copper, the influence of that property turns out to be of low significance due to the very small displacement current flowing from the electrode to grounded parts in the environment. pedot:pss showed similarly good resolution for near distances. at distances above 30 cm the resolution decreases strongly, which can be attributed to the non-uniform application of the polymeric conductor on the pet caused by inkjet printing. we can conclude that larger electrodes perform significantly better in loading mode when detecting objects at great distances. the transparent properties, easy inkjet printing and the good performance of pedot:pss qualifies the conductor for rapidly building prototypical user interfaces. in later development stages, ito can provide a very good performance comparable to copper."
"the controller board is built around a ti tms320f28069 32-bit microcontroller with native floating point unit and provides eight sensor channels for measurements. sensors are connected to the board by a standard usb cable. a single sensor channel represents a generic interface that supports different sensor types. apart from two power supply lines, there are two software-configurable lines for communicating with a sensor. the first line leads to a microcontroller's general-purpose-input-output port (gpio) while the second line can be switched to either an analog-to-digital converter (adc) or a time-capturing unit. this architecture enables us to dynamically configure the toolkit's sensor channels for different sensor types. for example, digital output signals, like frequency-modulated rectangular pulses, benefit very much from time-capturing units that can measure time intervals between the rising and falling edges of a signal."
"given the extensive usage and reporting of dna evidence to the courts, calculation and interpretation of the match statistic has substantive implications to criminal justice policy and practice. while the lr has gained precedence over the rmne approach, the proliferation of continuous systems that compute the lr using different underlying model assumptions warrants an investigation into the final outcomes acquired from various models."
"distinct measurements for n electrodes [cit] . this is particularly suited for applications that require a high number of measurements to gain precision, e.g. gesture recognition of one or more hands [cit] . the third method is called transmit mode and can determine proximity of a human body by coupling a person to a changing electric potential and receiving the signal using a grounded sensor electrode as the receiver. transmit mode requires a user to be connected to an electrode which is only applicable in certain scenarios, like user identification at multi-touch tables [cit] ."
"h p : the evidence is a mixture of data from the suspect (with genotype s) and n−1 other unknown, not necessarily related contributors, whom we term the interference contributors."
"many pervasive environments require affordable and accurate methods for tracking humans. capacitive proximity sensors are especially suited for this use case as they can be integrated unobtrusively into the environment. in real usecases, this monitoring requires to strongly consider privacy aspects. researchers have therefore realized floor tiles and carpets employing capacitive proximity sensing that can be deployed in assistive environments and home automation scenarios [cit] . smart floors can support the elderly and disabled persons in their activities of daily living, for example by monitoring emergency situations such as falls."
"various materials can be applied in those scenarios. for example, a foil of polyethylene terephthalate (pet) coated with indium tin oxide (ito) provides both high conductivity and high transmittance of light at visible wavelengths. another interesting material is pedot:pss, an electrically conductive material made from polymers. it is used in different research disciplines like printed field-effect transistors and photovoltaic cells. since it can be applied to different materials using ink-jet printing, it promises very good properties for rapidly prototyped sensing electrodes [cit] . both variants can be used to create printable transparent conduction films that offer manifold alternatives for sophisticated sensor designs and rapid-prototyping."
"in order to analyze the capabilities of the opencapsense toolkit, we have conducted different experiments investigating the behavior of our loading and shunt mode sensors. there are two essential factors that can characterize this behavior. the first factor is the maximum spatial distance between a sensor electrode and a detectable object at which a presence might be registered. the second factor is the signal-to-noise ratio that can be affected by electromagnetic interference, thermal noise or ambient temperature variation. we combine both factors to derive the spatial resolution of our system."
"as the spatial resolution is calculated on interpolated curves of the sensor characteristics and their corresponding standard deviation, its raw curve is not uniform. therefore the r s curves were smoothed, which allows for comparable inferences on the resolution."
the time-capturing units analyze signals with a temporal resolution of 12.5 ns. analog receivers are usually connected to an adc that samples the signal at high data rates up to 100 khz and 12 bit resolution. this method allows for obtaining a set of values representing the signal's amplitude in a certain timespan.
"regarding the different properties of human arms and hands, e.g. shape and conductivity, it is obvious to use a standardized measuring body. such an object should have properties similar to a human arm and be easily applicable to different experiments. [cit] introduced a grounded aluminum tube with a length of approximately 48 cm and a diameter of 8 cm that acts as a surrogate arm. they showed that this tube can be an appropriate replacement for a human arm, delivering slightly deviant measurement results. based on this description, we developed a test setup that is shown in figure 3 ."
"the toolkit was evaluated for spatial resolution, the influence of different electrode materials on this metric, and comparatively analyzed with the captoolkit -a platform that is widely used in research projects. finally, we showed the broad applicability of the opencapsense toolkit and capacitive sensors in general by presenting different studies, ranging from light wearable devices to fall detection systems in fixed installations. still, these applications present only a small subset of potential scenarios that can be realized. we kindly invite interested researchers to use our system and further expand the horizon for capacitive sensing devices. the documentation, schematics, source code and evaluation results can be found at http://www.opencapsense.org."
"using r s as the main characteristic value of a capacitive sensor system, it is reasonable to investigate if different electrode surface materials result in diverging spatial resolutions. additionally we investigate the dependency between the size of a sensing electrode surface and the resulting spatial resolution, which provides essential information that is required when prototyping capacitive sensor applications. the sensing electrodes test set contains rectangular shaped copper electrodes of various sizes. furthermore, we compare copper electrodes to electrodes made of ito and pe-dot:pss. using the introduced test setup and measurement method, the normalized sensor characteristics and the spatial resolution were determined."
"iþ the unit n−1 simplex; and let f θ denote the probability density function of θ. for models a, c and d, this density is assumed to be uniform over δ n−1 and that θ is the same over all loci. for model b, it is assumed that the contributor mixture proportions at each locus are independent and identically distributed as uniform distributions over δ n−1 . for all models apart from b, to calculate the numerator of the lr, we first integrate over the sample space:"
"employing this technology, it is possible to implement interfaces that are able to determine gestures, body movements and environmental changes at typical distances up to 50 cm [cit] . in contrast to camera-based methods, capacitive sensing has the advantage of being robust against changing lighting conditions and visual occlusion. additionally, capacitive sensors have significantly lower impact on a user's perceived and actual privacy, compared to optical tracking methods. energy-efficient sensors can be deployed unobtrusively underneath furniture, carpets or within walls. sensed data can be processed with computationally cheap algorithms. however, the drawbacks are a limited resolution and error-proneness in environments with many conductive objects or electrical devices that affect electric fields. using capacitive proximity sensors, reasearchers have realized location tracking systems [cit], wearable activity recognition systems [cit], smart furniture [cit] and gesture recognition systems [cit] ."
"in order to demonstrate the capabilities of the opencapsense toolkit and to observe its properties in real application scenarios, we built a variety of prototypical systems that we will present in the following."
"based on our experiment setup, we evaluated the loading mode sensor for both, captoolkit and opencapsense, using the same methodology. figure 7 shows a comparison of different sampling windows. as expected, the resolution of both, opencapsense and captoolkit, increases for longer sampling windows. comparing both 10 ms measurement series, opencapsense shows a distinct improvement to captoolkit. for distances of 200 mm, opencapsense improves the resolution by the factor three compared to captoolkit. regarding a reduced sample window of 2.5 ms the resolution is robust and higher than captoolkit. as captoolkit supports minimum measurement windows of 10 ms, a smaller window could not be selected for comparison with opencapsense. it can be concluded that opencapsense's temporal and spatial resolution is a significant improvement to captoolkit and allows investigating new pervasive application scenarios with very fast update rates."
"we have developed our own capacitive proximity sensing toolkit -opencapsense -that enables researchers to rapidly prototype capacitive proximity sensing applications, such as pervasive user interfaces for whole-body interaction or activity recognition. similar requirements have driven the development of the captoolkit, resulting in analogous design decisions [cit] :"
"2) shunt mode sensor: in shunt mode, a receiver electrode is used to measure the displacement current from a transmitter electrode [cit] . when a human body part enters the electric field, the field between a transmitter and receiver is interrupted. this results in a decrease of displacement current and thus a decreasing capacitance between the transmitter and receiver. due to separate transmitter and receiver electrodes, shunt mode offers the possibility to perform parallel measurements using multiplexing approaches [cit] . the shunt mode sensor measures the displacement current floating from a transmitter electrode to the receiver electrode."
"the four models used in this study were tested on all the true contributors to the samples in the testing set. thus, a 1-person sample resulted in one lr, a 2-person sample resulted in two lrs and a 3-person sample resulted in three lrs. since a sampling algorithm was used to calculate the numerator (sampling of the genotypes of unknown contributors in mixtures) and the denominator (sampling of the genotypes of random contributors), the lr value varies from run to run. to analyze the run-to-run variation of the four models in this study, each model was run five times on all the samples in the testing set."
"a total of 101, 1-, 2-and 3-person samples were used to test the four models in this study (see s2 table for details). these 1-person test samples were created using the same protocol described for the single source samples in the calibration set. multi-person samples were created by mixing appropriate volumes of the single source dna extracts to attain the various ratios specified in s3 table. once mixed, these samples were re-quantified and then amplified using the target masses from s2 table. the 1-person samples contained dna from 30 different individuals, the 2-person samples contained dna from 6 different individuals (3 combinations) and the 3-person samples contained dna from 6 different individuals (2 combinations). none of the contributors to the calibration set were present in the testing set and none of the contributors to the testing set were present in the calibration set."
þ is the probability of the interference genotypes under the peak height distribution. the number of genotype samples j is not a constant in the ceesit framework. genotype samples are generated in batches until the probability converges such that the difference in
"model b. this model is similar to model a in all but one aspect-the underlying mixture ratio of the sample is modified. the mixture ratio specifies the proportion of a sample contributed by each individual (e.g., the major and minor contributors in a mixture, if any) and is unknown for an evidence sample. the mixture ratio can be treated in the probability calculation in at least two ways: a) assuming that the mixture ratio is constant across all the loci and integrating over the sample space of values that the mixture ratio can take or b) allowing the individual locus mixture ratios to be independent of each other. to study the impact of changing this assumption, we developed model b, which does not assume that the mixture ratio is the same at all the markers but instead assumes that the mixture ratio varies independently from one locus to another."
"the findings of this study have implications for the usage of, and communications associated with, probabilistic genotyping systems. as forensic laboratories implement probabilistic genotyping systems, characterizing the sensitivity of the lr to model assumptions of a continuous mixture interpretation method is necessary. model differences and modifications are expected as these systems mature. the results of this paper suggest that any updated version of existing mixture interpretation software be tested on a large number of known samples to establish the range in which the system is deemed to be reliable and to verify that its results conform to expectations. moreover, if the software is intended to be applied to low template samples, performing validation studies on such samples would inform the analyst as to the lrs typically obtained for such samples."
gesture recognition is one of the mostly applied application scenarios that has been evaluated for capacitive proximity sensors [cit] . the unobtrusive applicability allows detecting gestures over a distance through nonconductive materials. the position of one or more hands can be determined using different analytical or probabilistic models.
"our goal here was to evaluate the system's predictions for each of the three areas we have trained and labeled \"1\" (left side hallway), \"2\" (central hallway), and \"3\" (right side hallway). as detailed at the beginning of section 6 for each area we carried out measurements on two consecutive days in 4 different locations and for each we took 10 fingerprints. of these 4 locations we choose 2 (only for this test's purposes, as we are only taking into account \"border\" locations). this makes a total of 20 fingerprints (10 fingerprints * 2 locations) for each area. thus we have a set of training data of 60 fingerprints (20 fingerprints/area * 3 area) per day. to validate the system, we selected as test data set both fingerprints (the ones corresponding to the day when we trained the system and the ones corresponding to the previous/following day) for all locations (indoors and borders). here we only show the results corresponding to when we train the system with the fingerprints that are taken on the second day and validated with the previous one. as we discussed at the beginning of section 6.3, we intend to study how changes in the stability of the waps from one day to another affect to the predictions of the areas. also, we aim to study how to adjust our algorithm to delete unstable waps with the configuration that we saw in table 1 such predictions evolve as we take 16, 10, and 5 waps."
"a filesystem consistency level (fscl) is obtained by the combination of a consistency level governing file operations and the use or not of the close-to-open semantics. this leads to six different fscls. in figure 4, we list in the last column one or more matching implementations for each level."
"the three operations available at the storage layer implement accesses to the filesystem. as a consequence, the consistency of the distributed storage governs the consistency of the fuse interface. the fuse interface is linearizable (resp. sequentially, eventually consistent) when operations at the storage level are linearizable (resp. sequentially, eventually consistent)."
"real-life systems could be complicated because of hierarchical structures and complex data operations; real-time behaviors are sometimes essential in such systems due to the interaction with the real world; in addition, unreliable environments could result in stochastic behaviors so that probability is necessary. these characteristics present the difficulty in properly designing and developing such systems. applying model checking techniques in this domain is therefore very challenging, due to the requirements of an expressive enough modeling language as well as efficient model checking algorithms."
. this section captures the results of both experiments carried out. section 6.3.1 presents the results of the simpler scenario based on the measurements on a single aisle section (divided into 3 areas) and section 6.3.2 captures the results of the second experiment based on measurements from 3 different wall and door separated areas.
"when assessing the effect of an intervention within a community, we intuitively expect that the size of the intervention compared with the size of the existing epidemic is an important factor: the larger the epidemic, the more \"diluted\" will be the role of the intervention in controlling further spread of hiv. at the time of the design we need to identify which of the following scenarios applies to our sample:"
our first goal is to determine whether each one of these incident individuals is phylogenetically linked with the intervention group or with a nonintervention (or control) group. we consider that a phylogenetic link with the intervention group is established when the relationship of the t mrca for anyone in the intervention group and a given incident individual (t mrcai ) as well as the t mrca for anyone in the control group (t mrcag ) and any of the incident individuals is given by:
"like most contemporary dfss, flexifs decouples metadata from data storage. for each file, an inode block (iblock hereafter) contains the metadata information about the file, e.g., size and user/group ownership. one or more data blocks (dblock) hold the content of the file."
"these algorithms receive as input parameters the area in which the fingerprint was taken and the rssis values measured for each wap. initially we started with a set of 40 waps which involved training the system with 40 (rssis for each wap) + 1 (area) parameters. our first objective was using our algorithm to delete unstable waps (previously described in section 4), discard those access points that do not provide any relevant information to train the algorithms, and verify that the results improved compared to when the process was performed without filtering."
"in this paper we have presented several ways to optimize fingerprinting-based techniques for locating users indoors when only room level accuracy is required. one of the parameters studied is the way to select the most convenient sampling points to measure training fingerprints in order to increase the accuracy while minimizing the memory and cpu execution requirements. previous studies have focused on either an exhaustive grid of sample points validation points 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 estimated area 1 1 1 1 1 1 1 2 2 2 2 2 3 3 2 3 3 3 or a random selection of them. this paper concludes that, for room level accuracy, border points between rooms or areas contain the required information for the prediction algorithm in order to provide a similar or even better accuracy that the one provided by exhaustive or random sampling techniques. the paper also provides a comparative analysis and evaluation of the performance of different decision algorithms identifying under which circumstances each of them work better."
the storage layer is essentially a key/value store extended with a compareand-swap primitive. devising a filesystem on top of this interface is a contribution of our work. the operations of the interface are as follows:
"we have introduced a methodological approach to assess the direct and wider community effect of interventions aiming to control the spread of viral pathogens. the method is based on establishing molecular-clock-based phylogenetic links of newly infected patients to individuals having received the intervention or a control condition. the most important advantage of the suggested method is that it can assess the wider community effect without the necessity of follow-up among an at-risk group. thus it remains strong even in circumstances where retaining participants for follow-up is a major challenge (e.g., among pwid communities). our method provides a new tool for evaluating interventions that previously could be evaluated only with community-randomized controlled trials, with probability sampling of communities, or with (for a narrow subset of interventions like antiretroviral treatment) couples studies (23, 24) (for more examples, see web appendix 3)."
"in what follows, we explore the impact of each fscl on the cost of iblock operations, and file operations. we also investigate the impact of the replication factor on performance. all experimental results are averaged over 10 3 operations, and we present standard deviations when appropriate."
"by means of stochastic modeling and empirical estimates of transmissibility, we can estimate the proportion of individuals that we expect to produce infections (and thus transmission chains) within the time frame (t − t i ). with standard proportion power calculations we can then estimate the minimum number of transmission chains that we need to sample when we visit the site after (t − t i ) years."
"samples at the time point of the intervention, but such early samples are not necessarily required. we may collect samples after the intervention and, by virtue of molecular-clock phylogenetics, still assess phylogenetic links. one obvious adjustment will be on the use of the window a for phylogenetic assignment: the common ancestor might be dated even after the time of the intervention ( figure 3 )."
"our automatic zone abstraction approach is probability preserving for several properties such as reachability checking and ltl checking. a proof sketch is as follows: given a concrete mdp and one of its scheduler, a discrete-time markov chain (dtmc) can be defined; we can always build a corresponding dtmc in the abstract mdp to guarantee these two dtmc are time-abstract bi-similar, and vice versa [cit] ."
"one of the major questions that we want to explore is how many individuals we would need to sample at time t i given that we have performed an intervention on n individuals at time t. if the intervention has no effect, then the transmission rate (r a ) would be equal among background population (r b ), control (r c ), and intervention groups (r i ):"
"in this section, we present experimental results obtained using flexifs, where we observe empirically and in isolation the tradeoffs between sharing semantics and performance in dfss designs."
"tag (3) pred (tag) conf (1) conf (2) 3. for a moving target to be located, the assessed trajectory information could be used to decide in which area a user is in order to improve the accuracy of the algorithms when approaching the border of two areas."
"another strength of the suggested method is that a control group can be formed retrospectively (e.g., by identifying individuals who were infected at the time of intervention and were not part of the risk network of the intervention arm). this is feasible as long as we know that these people were indeed infected at the time of the intervention (thus the risk of transmission is equal between the intervention group and the control group), and nucleotide sequences of such people can be retrieved at any time point. this feature can be particularly important in the event that many interventions have already taken place without the formation of control groups, when assessing their wider community effect can be particularly challenging. such ongoing or recent interventions are unlikely to have collected blood figure 2 . distribution of the length of the transmission chains when simulating the spread of the pathogen in population \"bubbles\" of 10,000 under transmission parameters described in methods. red signifies the control arms and green signifies the intervention arms."
"if we compare the values and reliability intervals obtained in this second test for the metric \"relative accuracy\" we can see a significant improvement as compared to when all locations were used. the increases/decreases in percentage points in those cases where we did not execute our algorithm to delete unstable waps are as follows (we take all the waps from the initial set, as indicated in the first row of combined tables 2-4 and combined tables 5-7 [−4.83] ). although this decline reaches a peak of 4.83 percentage points for the algorithm of k-nearest neighbors, this number seems negligible since generally we get better results, especially for nbc and random tree algorithms. at the same time, again, we are reducing the computational cost substantially, going from 40 to 10 waps. when we compare the results published for the wasp and redpin algorithms with noise filtering and 5 access points [cit] with the ones we get when we run our algorithm to delete unstable waps (and also take 5 waps) we can see that we get significant improvements for the following [cit] . this is an improvement of +4.50 percentage points when using the random forest algorithm. figure 6 compares the results for the metric \"relative accuracy\" when we run our algorithm to delete unstable waps (shown in blue) versus when we do not (shown in green). for most algorithms (11/15) percentages are maintained or improved significantly. figure 7 presents a comparison of the \"relative accuracy\" for algorithms evaluated when the system is trained only with \"border\" locations and by the number of waps selected. for 8 access points the algorithm that provides the best resolution is random forest while for 10 the adtree and ft algorithms ranked first and second, respectively."
"(8) lmt. logistic model trees are classification trees with logistic regression functions in the leaves. the algorithm can deal with binary and multiclass target variables, numeric and nominal attributes, and missing values. for more information, see [cit] ."
"the role of the proxy is to hide both the topology of the distributed storage and the operation logic from the client. in flexifs, any storage node can act as a proxy. when a client executes an operation and contacts a proxy via its web service interface, the proxy accesses the underlying storage system, executes the operation, and returns the result to the client."
"depending on the fscl in use in flexifs, the semantics of the above interface may change. for instance, c&s () is not atomic under eventual consistency. we detail how flexifs implements this interface in section 3."
"trained with \"border\" locations. out of the 15 evaluated algorithms to conduct for this test we chose random forest, as this one gave us the best results overall for 16(99.00), 10(98.50), and 5(90.00) waps, respectively, as it is reflected in the previous paragraph \"training the system with 'border' locations.\""
"for area 3, fingerprint 52, the system also makes the right prediction when it receives the average fingerprint for 5 and 16 waps scenarios with 50% and 80% percent of reliability, respectively. however, when the system takes 10 waps it still fails. why? this is because whether we calculate the average fingerprint using the previous one (51), one previous fingerprint and subsequent one (51 and 53) or two previous and two subsequent fingerprints (50, 51, 53, and 54) the predictions made by the system for these fingerprints are also wrong so there is no way to get an upgrade. tag (2) pred (tag) conf (1) conf (2)"
"enabling further research on dfss to scale and break the petabyte barrier requires developers to understand and be able to compare systematically the multiple components of a design. these components include data distribution and replication (and associated consistency guarantees), request routing, data indexing and querying, or access control. performing a fair comparison of these aspects as supported by existing dfss implementations is difficult because of their inherent differences. indeed, these systems propose not only different filesystem consistency levels (fscls), but they also feature different base performance and optimization levels, which largely depend on the programming language, environment, runtime, etc."
"the authors of pastis [cit] compare close-to-open against read-your-write semantics. levy [cit] surveys dfs designs and four different types of file sharing semantics: posix, close-to-open, immutable files, fully transactional semantics, and survey corresponding implementations. the use of a modular framework for evaluating design choices and establishing performance/tradeoffs in systems software design has been successfully used in various domains. examples include virtual machines construction [cit] or corba-based middleware [cit] ."
"the expected number of transmission chains generated between time t and t i from the background population (n b ), the control group (n c ), and the intervention group (n i ) will be provided through functions of the transmission rate and the size of the population belonging to each one of these groups at time t (n b, n c, and n i, respectively). the same parameters will also contribute to the expected length of transmission chains generated between time t and t i from the background population (l b ), the control group (l c ), and the intervention group (l i )."
"the major contribution of this paper is the study, analysis, and evaluation of the relative accuracy of the different smartphone fingerprinting-based indoor location algorithms when locating users at room level. this has been done using different strategies for selecting the points used to train the system, applying different values for noise and stability filters in order to select the best waps, and taking the average of several measured fingerprints when locating the user. in particular the following algorithms have been used:"
"prts has been integrated into pat, which is implemented with c# and can run on all widely-used operating systems. [cit] plug-in (available at [cit] first, we use a multi-lift system to demonstrate the effectiveness of prts. such system contains different components, e.g. lifts and buttons; it usually has timing requirements in service and users may have random behaviors. an interesting phenomena in such system is that a user presses the button outside the lifts, but one lift on the same direction passes by without serving him/her. this is possible since the lift which is assigned to serve this user is occupied by other users for a long time, and other lifts reach that user's floor first and pass by."
"in all experiments we simulated, using the rapidminer studio 6.0.008 tool, what would be the response of our indoor location system when training with each of the following algorithms?"
"to generate the comparative chart on figure 4 we have considered the case where our algorithm to delete unstable waps gives us 10 access points since for a larger number we did not observe a significant improvement. we have used the green colour to show the results when we have not run our algorithm and thus the number of input parameters that the algorithms receives is equal to the rssis corresponding to the initial number of access points (40) plus the area. in blue we show the results we get when we have executed our algorithm with a given configuration to take 10 access points. in this case the number of parameters that will be passed to the algorithms will be equal to the rssis corresponding to the 10 waps selected plus the area (30 parameters less than the previous case). figure 5 shows a comparison of the \"relative accuracy\" for algorithms evaluated when the system is trained with all locations and by the number of waps selected. for 8 access points the algorithm that provides the best resolution is random forest while for 10 the ft and random forest algorithms are at the same level."
"in the first row of table 1, the number of wireless access points that would result after applying our algorithm to delete unstable waps, with the configurations indicated for nnst and lt, is shown. as can be appreciated, as we expand both thresholds we are less strict; hence, we obtain a larger number of waps."
"flexifs is a distributed file storage service (dfss) offering a transparent filesystem interface. figure 1 illustrates its general architecture. flexifs has been built in a modular way to allow evaluation of different choices of dfss designs. a typical deployment contains two sets of nodes: storage nodes implement a distributed flat storage layer, while client nodes present a filesystem abstraction to the users, and store files and folders hierarchies on the storage nodes. a client node accesses the filesystem through a filesystem in user space (fuse) implementation [cit] . fuse is a loadable kernel module that provides a filesystem api to the user space. it lets non-privileged users create a filesystem without writing any kernel code. in flexifs, each access to the filesystem is transformed into a web service request and routed toward a proxy node that acts as an entry point to the distributed storage. the proxy redirects requests to the adequate storage node(s), which store or return data blocks."
"we select a depending on the natural course of the infection; a reflects the fact that each infected patient has a population of nonidentical viruses, sometimes referred as quasispecies. the role of this genetic diversity in hiv transmission dynamics has been studied intensively and thus we already have a fairly good understanding of how hiv genetic diversity is passed in transmitter-acceptor couples (15) . these studies have shown that at the time of transmission there is a quasispecies population bottleneck suggesting that only a minority of the genetic diversity within the \"donor\" can be transmitted. due to withinpatient evolution and stochasticity, the transmitted strains may or may not contain exactly the same genomic sequence that was isolated from the transmitter at the time of the intervention. this means that the t mrca between the donor and the receiver viral strains is likely to predate the transmission event, which in turn creates the necessity of nonzero a. [cit], that has been described as pretransmission interval (16) explains why t mrca and actual transmissions may not coincide (17) ."
(1) variance threshold (vt). we set its value to 60 dbm 2 (∼8 dbm). this means that we choose ±4 dbm as the acceptable range of oscillation of a set of rssi values for a given wap. above this threshold we consider that it is not possible to achieve stable transfer rates.
"having established the above thresholds, the first step of the algorithm is to calculate the number of noisy samples by wap for each location. a sample is considered a noisy one"
"our second experiment evaluates the cost of fetching an iblock from the file storage. we report the results in figure 5(b) . under linearizability, a get() operation has an identical cost to a c&s () operation, since both operations go through the replicated state machine. on the other hand, the cost of sequential consistency is reduced because the proxy can access any replica to fetch the iblock content. therefore, performance is in that case identical to eventual consistency. figure 5 impact of the replication factor our last experiment measures the impact of the replication factor on performance. in this experiment, a client writes a file of 4 mb under linearizability. we vary both the replication factor of flexifs, and the use or not of the close-to-open semantics. figure 5 (d) depicts our results. in this figure, we observe that increasing the replication factor has a small impact on performance: below 3% without close-to-open semantics, and 7% with. paxos is the most demanding consistency control algorithm we have implemented. thus, this result shows that the filesystem consistency level is contributing more than the replication factor to dfss performance."
"several papers discuss the performance, consistency, and semantics tradeoffs in dfss designs. the andrew file system (afs) [cit] introduced caching mechanisms and the close-to-open semantics for both files and directories. this was inspired by earlier designs such as locus [cit], which relied on a strict-but costly and inefficient-posix semantics. since its second version, the network file system (nfs) [cit] also implements the close-to-open semantics; its fourth version distinguishes data from metadata management, a separation that has been adopted by all dfss designs since then."
"we use a simulator that implements stochastic models of structured and unstructured population dynamics (18) . the simulator generates transmission chains, which we then transform into molecular-clock trees by simulating the sequence evolution of pathogens (19) ."
"proof of concept: simulation of an hiv-preventive intervention among pwid. based on the above-stated power estimation, we have designed an intervention. [cit] we visited a place that had an ongoing hiv epidemic for at least 10 years, with at least 1,000 infected individuals. we randomly selected 120 to participate in the intervention and 120 to participate in the control group. [cit], we revisited the place and sampled 620 individuals who had been infected after the intervention (i.e., we further inflated the number from our power calculation by 20 cases). these 620 [cit] through an approach that we expected to link 50% of the incident cases with either the control group or the intervention group (specifically, targeting the risk networks of the control and intervention groups). these 620 incident cases could be part of transmission chains coming from the study groups or from unsampled people (background infected population). we assumed that our sampling approach would be able to retrieve transmissions from our study groups with equal probability to that of the transmissions coming from the background. the sampling approach was performed in a way that was unbiased with respect to recovering transmissions from the intervention or the control groups. thus, whether the sampled transmissions were the result of the control group or the intervention group is proportional to the number of incident individuals generated from each one of these groups."
"evaluating the wider community effect by measuring the \"length\" of transmission chains. the number of generated transmission chains is a measure of the direct or \"first-wave\" community effect of an intervention. to evaluate an intervention's long-term effect at the community level, another approach is required. as stated above, we are interested in evaluating interventions that are designed to have wider community effects. such interventions are designed to reduce transmissibility even among persons who are socially or distally related to the intervention \"seeds.\" here we propose to assess this wider community effect by comparing the length of the produced transmission chains, which are phylogenetically linked to either the intervention or control seeds. by size or length of transmission chains, we mean the number of the incident individuals linked with a specific person belonging to the intervention or the control group."
"distributed file storage services (dfss) offer a unified filesystem view of unstructured distributed data stores. as for any distributed storage service, the expected properties of a dfss are consistency, availability, and tolerance to partitions. the cap theorem [cit] states that a distributed storage system can fully support at most two of these three properties simultaneously. partition tolerance is usually considered essential for a dfss, as data centers may be temporarily disconnected in a large-scale distributed setting and such events must be supported. as a result, developers of dfss usually decide on a tradeoffs between availability and consistency."
"depending on how the previous four thresholds (noise, number of noisy samples, variance, and location) are configured we can be more or less strict when taking into account a smaller or larger number of waps. for instance, if we keep fixed nt and vt, we take a number of samples ( samples) and locations ( locations) equal to 10 and 12, respectively, and adjust the factors involved in the calculation of nnst and lt; we would obtain the results shown in table 1 ."
"sequential consistency. under sequential consistency, \"the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program\" [cit] . sequential consistency is weaker than linearizability. in particular, this consistency level is not composable [cit] : even if each file is sequentially consistent, the filesystem as a whole is not sequentially consistent (this is also called the hidden channel problem). we illustrate this issue in figure 4 (b) (middle). in this figure, accesses with respect to file f are sequentially consistency, and similarly this property holds for g. however, the execution (i.e., when we consider both f and g as a whole) is not sequentially consistent."
"another mechanism that may be used to improve the performance of the fingerprint based indoor location algorithms is combining several fingerprints measured when trying to estimate the location of an unknown point. the more the fingerprints over a longer period of time the better the impact one expects to obtain in the predicted results. however, there is a trade-off in the number of fingerprints that can be taken into account in a real setting since the user expects to obtain a result from the algorithm in a reasonable amount of time. in this paper we will analyse the impact on the relative accuracy obtained when combining several consecutive fingerprints taken over a few seconds interval. in order to combine the fingerprints we propose two different algorithms (the results will be presented in section 6):"
"when its rssi is less than the value set as nt (note that the wap is not considered noisy if it is not detected at a certain point, only if it is detected with a fluctuating power below the noise threshold a number of times above the nnst). then, for each location and wireless access point we check if the result obtained in the previous step equals or exceeds the nnst. if so, the wap is marked as unstable for that location. if not, the variance of its rssis is calculated. if this number exceeds the vt, the wap is marked as unstable for that location. in summary, we consider that a wap is unstable at a given location when the number of noisy samples for this access point in that location equals or exceeds the nnst or the variance of the rssis for that wireless access point and location exceeds the vt."
"reachability checking the maximum/minimum probability of reaching specific target states could be checked using numerical iterative method. reward checking the maximum/minimum accumulated rewards/costs to reach the target states could be calculated also through the iterative method. in prts we just consider the action reward, that is, assigning each visible action a reward which is a rational number. ltl checking in prts we support ltl-x (ltl without 'next' operator) since in abstract model the semantics of 'next' is hard to define. in our setting, ltl formula can be built from not only atomic state propositions but also events so that it is called se-ltl [cit] . it is very expressive and suitable for prts since our language is both event-based and state-based. we adopt the rabin automata-based approach to calculate the maximum/minimum probability that an se-ltl is satisfied. refinement checking a desired property could be defined as a non-probability model and we can check a trace refinement relation between this model and the system specification [cit] ."
"if the intervention has the anticipated effect, then the transmission rates of the background population and control population would be approximately equal (for a placebo control group), and the transmission rate of the intervention group would be lower:"
"as we see in table 24, when we use the average fingerprint resulting from these three methods (previous, previous and subsequent, two previous, and two subsequent) the system makes the right prediction with a reliability of an 80% percent, 40% percent, and 50% percent, respectively (table 25 )."
"write. the proxy first retrieves the iblock of the file and produces the new dblocks. it uses the storage layer's put() operation to insert (in parallel) the new dblocks in the distributed storage. notice that because dblocks are contentaddressed and immutable, every modification that produces a dblock leads to the creation of a new dblock with a different key. then, the proxy uses c&s () to update the iblock corresponding to a file. if the iblock changed meanwhile, the proxy has to recompute (if necessary) the dblocks, as well as an updated version of the iblock; then it re-executes c&s (). this last sequence of operations is executed until the c&s () succeeds. since a write() operation may access any offset of the file, the above mechanism is necessary to avoid a lost update phenomena when two clients concurrently write the file. close. upon the closing of a file, the proxy checks that the file still exists. if the file does not exist, the proxy returns an error to the client. rename. if the source and target parent directories are the same, the proxy attempts updating the iblock of the parent directory. in case they are different, the proxy first tries adding the file to the target directory, then it attempts removing the file from the source directory. all attempts are perform with c&s (). if c&s () fails at some point, the proxy returns an error to the client. 4"
"this paper depicts a study of the impact of a filesystem consistency level (fscl) on the performance of a distributed filesystem service (dfss). while the fscl offered by a dfss has fundamental impact on performance, it is difficult to systematically evaluate this impact in isolation from other design aspects, due to the design and implementation diversity of existing systems. this paper presents flexifs, a framework for the systematic evaluation of dfss aspects. in more details, we depict a filesystem interface to users and leverage a set of servers implementing a fully distributed storage layer for both data and metadata. we implement three forms of consistency: linearizability, sequential consistency and eventual consistency, together with and without close-to-open semantics. remarkably, a dfss providing all these fscls can be supported with the simple addition of a compare-and-swap primitive to a regular key/value store. our experimental results establish that linearizability under the close-to-open semantics is a sound design choice and a good compromise between operational semantics and performance, while illustrating the tradeoffs offered by the other design options. flexifs has a modular design and we plan on investigating further aspects of dfss pertaining to indexing, client interaction, and semantics. we also plan to release flexifs as a part of the open-source splay framework [cit] ."
"the experiments results are listed in table 1 . we analyze two kinds of task assignment mechanisms: assigning to nearest lift and assigning to a random lift. from the table we could conclude that the first mechanism is better, since it has a smaller probability to ignore users' requests and this is consistent with common sense."
"the confidence is also analysed in order to select the best algorithms, the number of waps to be considered, the values for the noise and stability filters, and the location of the training points. the results are also validated by comparing them to those presented in papers which deal with similar aspects."
"indoor location systems provide a solution for user navigation in places where gps signals are not available. there are many alternatives available for locating users indoors that can be categorized in several ways according to the mathematical algorithms used, radio technologies required, or hardware components used. some of the alternatives are based on the user carrying a mobile device and others are based on device free [cit] . one classical categorization of indoor location systems divides them into presmartphone and smartphone eras. the former are based on specific hardware components like infrared badges, ultrasound tags, laser rangers, and wireless modules such as rfid [cit] . since they require additional equipment to be carried by users, they tend to be difficult to deploy and use in real scenarios. the latter are based on the use of smartphones as the user device for indoor location [cit] . smartphones are reliable and user friendly tools that allow people to have access to information services anytime and anywhere. some of these services may require using the location of the user and, consequently, integrating indoor location services in smartphone based systems is a natural way to minimize deployment requirements. smartphones have access to several radio technologies such as wi-fi, bluetooth, or terrestrial cellular mobile networks. they incorporate builtin sensors such as accelerometer, gyroscope, magnetometer, microphone, or camera. they are, therefore, computing platforms which are capable of sensing the environment and applying the required calculations in order to locate the users in scenarios where the gps signals are not available."
"a plethora of mathematical models aims to evaluate intervention strategies with respect to successful mitigation of pathogen epidemics (8, 9) . it is thought that the evaluation of interventions and their success is dependent on randomized trials (10), which can be costly and unrealistic in many settings. in addition, randomized controlled trials measure the impact of an intervention only on those who directly participate in one of the trial arms (or their first degree contacts) and thus cannot assess the community impact of the intervention (11) . in cases such as harm-reduction interventions (e.g., needle or syringe exchange programs), where randomized controlled trials cannot be applied in practice, a substantial amount of data in support of certain interventions has resulted in considering them as well-supported. however, more research is required (12) ."
"again using the same scenario as described previously, we are looking at transmission dynamics where a point introduction of hiv in a population of 10,000 susceptible persons could produce an epidemic of approximately 5,000 within 5 years. this scenario when stochastically simulated will produce a range of transmission chains (see figure 2) . when transmissibility is reduced by 50%, the length of the simulated transmission chains in a newly introduced epidemic will be significantly shorter than in the full transmissibility scenario. this suggests that the size of the transmission chains that are phylogenetically linked to the intervention group is expected to be smaller than the transmission chains that are phylogenetically linked to the control group. we note that the length of the transmission chains recovered after sampling is much smaller than the actual full length; length is sensitive to sampling effects. however, the relative length between the intervention and control groups should be different, even after sampling."
"in tables 8-13, we have the following: in the first row we point out the number of fingerprints, followed by the corresponding area identifier, the area forecast that the system gives us, and the reliability intervals for area \"1,\" \"2,\" and \"3.\" 35 36 37 38 39 40 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 41 42 43 44 45 46 47 48 49 50 3 3 3 3 3 3 3 3 3 3 3 2 when we train the system with the fingerprints collected on the second day and validate with the ones corresponding to the first, adjusting our algorithm to take 16 waps, we obtain a relative accuracy of 80% (48/60 successes and 12/60 failures) (tables 14, 15 (tables 18, 19, 20, 21, 22, and 23). 51 52 53 54 55 56 57 58 59 60 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 table 14 : system predictions for area 1 when using 10 waps (part i). table 15 : system predictions for area 1 when using 10 waps (part ii). table 16 : system predictions for area 2 when using 10 waps (part i). 21 22 23 24 25 26 27 28 29 30 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 35 36 37 38 39 40 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 if we analyze the results for 5 access points (area 1) we can see that the system succeeds 19 times out of 20 and fails once out of 20 (fingerprint 11). for area 2 the system is wrong in its prediction for the fingerprint 39; it succeeds 19 times out of 20 and it fails once out of 20. as for area 3 it succeeds 14 times out of 20 and it fails 6 out of 20 times (fingerprints 46, 49, 51, 52, 56, and 57). in total the system succeeds 52 times out of 60 and table 20 : system predictions for area 2 when using 5 waps (part i). 21 22 23 24 25 26 27 28 29 30 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 31 32 33 34 35 36 37 38 39 40 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 41 42 43 44 45 46 47 48 49 50 3 3 3 3 3 3 3 3 3 3 3 3 3 and +23.33 percentage points, respectively. this finding is of special interest since it proves that when changes occur in the waps' measured rssis considered day to day we need to be stricter when selecting access points in order to obtain better predictions."
"under posix semantics, almost all file operations shall be linearizable [2, page 58] . in particular, a read shall see the effects of all previous writes performed on the same file. the cap impossibility result [cit] tells us that such a constraint hinders the scalability of a dfss."
"when flexifs implements the close-to-open semantics, the proxy is stateful and keeps track of the files opened by each of its clients. therefore, a client has to access the same proxy during a file session. in more details, upon a successful call to open(f ) the proxy records the iblock of f . this iblock is used for all the operations during a file session: a read () operation accesses the dblocks indexed by the iblock, and a write operation changes only the cached version. when the client closes file f, the proxy stores the iblock of f using put(). it can then forget that f was opened by the client."
"here we propose a new approach that aims to measure and evaluate the effect of interventions to mitigate the spread of pathogens by studying their evolutionary dynamics. we are interested in measuring the \"community effect\" of interventions (i.e., the ability to prevent both the initiation and expansion of transmission chains from people that have received a given intervention)."
"note that the probabilistic real-time systems modeled in prts can be fully hierarchical, since p and q in the above constructors can be any processes. this is different from pta based languages which often have the form of a network of flat pta."
"another intuitive expectation is that if the intervention has reduced transmissibility in the risk contacts of the intervention arm, then a random sample of incident cases should result in reconstructing shorter transmission chains having phylogenetic links with the intervention rather than the control group."
the selection of the values of these thresholds will provide a mechanism to select either a smaller number of more stable waps or a bigger number of less stable waps. the results captured in section 6 show that selecting the best 10-16 waps will provide the optimal results.
"metadata operations in figure 5 (a), we experiment the insertion of a novel iblock in the storage system when both the fscl and the size of the inserted block vary. for the sake of comparison, results are normalized by the time required for executing a noop() rpc operation carrying a payload that equals the size of the block. we observe in figure 5 (a) that eventual consistency is the cheapest fscl as it costs around 2 times more than the baseline rpc call. this is expected since a call to c&s () under eventual consistency requires 2 roundtrips: one to go from the client to the proxy, then one to go from the proxy to a replica. sequential consistency costs 4 roundtrips: once the primary is reached, the update must reach a quorum of replicas. for linearizability, 2 more roundtrips for the \"propose\" phase of the paxos algorithm are executed, leading to 6 times the baseline cost."
"in this paper, we propose a third approach to minimize the requirements for the number of training fingerprints when using the indoor location algorithms at room level. we propose to only measure the points in the border of each room without fingerprinting the interior part of each area. this approach is aligned with the objective of the system to be designed: to differentiate among rooms without taking into account user movements inside each room. the results in section 6 show how this approach can provide even better accuracy than exhaustive fingerprinting of each room with a smaller number of training fingerprints required."
"the results are listed in table 2 . '-' means that experiment takes more than 1 hour. the parameter of each model is the deadline constrain; for prism, iteration means how many refinements the stochastic game approach executes to get the precise result. in these cases we notice prts is much faster than prism's pta since our approach just uses zone abstraction and theirs must have additional refinement procedure."
"for the first experiment 12 post-its (4 for each area) were arranged as training labels and they were separated by a distance of 3 meters. these markers indicated the points that were going to carry out the sampling. in total there were 12 training locations that we grouped and divided into 3 sequential areas: 6 interior area locations and the other remaining 6 were the ones we called \"border\" locations and we defined the three areas designated on the map. for each of the locations we needed to make 5 consecutive measurements (at intervals of 15 s) of the rssi of wi-fi networks. once the 12 points had been scanned we repeated the same procedure for each location; hence the training had involved 10 steps in two batches of 5 spaced in time. this made a total of 120 (12 locations * 10 measures) fingerprints. the next day we performed the same experiment, which made it in total a final set of 240 fingerprints."
"flexifs provides several hooks to tune how files are stored. figure 2 illustrates our current design: dblocks are of constant and configurable size. the current size of dblocks is 128 kb, corresponding to the default maximal block size for the fuse interface. iblocks simply list the dblocks of the corresponding files. compared to the typical redirection-based architecture of unix, the two above mechanisms help in reducing the network overhead [cit] ."
"compared with simulation, automatic verification plays a more important role in system analysis since it indicates the accurate result of whether a property is satisfied in a system. two aspects are quite significant in verification with a model checker. one is the properties it can support, and the other is the efficiency of the verification algorithms. in the following, we review several widely used properties in prts, and some techniques in the verification algorithm to speed up the model checking procedure."
"flexifs is modular and decouples the filesystem logic from the actual storage. the storage layer supports data indexing and provides the api described in the previous section. due to its modular design, flexifs is able to use different indexing and storage layers, such as a multi-hop dht or a central server. below, we detail the design common in flat storage layers [cit] that we use in this paper."
"-put(k, v): writes the value v for key k, -get(k): returns the data stored for key k, -c&s (k, u, v): executes a compare-and-swap on key k with old value u and new value v."
(3) decision stump. this algorithm is usually used in conjunction with a boosting algorithm. it does regression (based on mean-squared error) or classification (based on entropy).
"from each infected individual we generate transmission chains based on: 1) the \"accessible\" population, that is, the size of the risk network (or the population \"bubble\") upon which the pathogen may spread from a single individual within the locality of the study and within a specific time frame; and 2) pathogen transmission assumptions that are based mainly on our expectation of how large an epidemic a pathogen could create in a completely susceptible population of 10,000 people without structure."
"finally, the paper has presented that taking several consecutive fingerprints when locating a mobile device can improve the accuracy of the results. a compromise exists between the accuracy obtained and the time required to locate the user. the results show that 3 consecutive measures are a reasonable trade-off."
"(1) adtree. the basic algorithm is based on [cit] . this current version only supports two-class problems. the number of boosting iterations needs to be manually tuned to suit the dataset and the desired complexity/accuracy trade-off. the trees' induction has been optimized, and heuristic search methods have been introduced to speed learning. (2) bftree. this algorithm uses binary split for both nominal and numeric attributes. for missing values, the method of \"fractional\" instances is used. for more information, see [cit] ."
"for the second experiment, a more complex setting was selected. we used 3 areas physically separated by walls and doors (two connected aisle sections, separated by a connecting door and a research laboratory wall to wall with one of the aisle sections). we took 6 location points in each of these areas following the proposed approach to sample points in the border between areas. the borders sample points have been measured with a 2-3-meter separation criteria."
"(4) finally, those waps marked as potentially unstable must be removed if the number of training points where they are marked as potentially unstable is above a threshold."
"how can we use the estimated necessary number of transmission chains to extrapolate the number of incident cases that we would need to sample in order to get a sufficiently powered study? at the very least a transmission chain can be attributed to 1 incident case. thus, the absolutely minimum number of incident cases would be equal to the minimum number of transmission chains. by simulating the sampling strategy of incident cases over a population of simulated (under the null and the alternative hypotheses) transmission chains, we can then inflate the number of additional incident cases needed to be sampled to get the required minimum number of transmission chains. to illustrate how this framework operates we provide an example of a power calculation below."
"over the last decade our understanding of how pathogens spread has greatly improved, but mitigating pathogen epidemics still remains an enormous challenge. early mathematical models of infectious diseases predicted what reality confirmed: the introduction and spread of pathogens into modern human populations now occurs more easily than ever due to the fast-growing highdensity human population, which may travel long distances more easily than ever (1, 2) . epidemics such as those caused by human immunodeficiency virus (hiv)-1, hepatitis c virus, hepatitis b virus, tuberculosis, or malaria are still holding, and we are still far from containing them. on multiple independent occasions, large and rapid socioeconomic deterioration or, in other cases, a rapid growth preceded epidemic outbreaks, suggesting that \"big events\" could be considered spatiotemporal hot spots for triggering new (3, 4) or fueling old epidemics (5-7). although such epidemics will not stop happening, we endeavor to better understand when, and most importantly, how, we will be able to contain them."
"coda [cit], ivy [cit] (b) with close-to-open semantics linearizability. the most powerful synchronization level for processes in a distributed environment is obtained through the use of atomic, or linearizable, objects [cit] . a linearizable object is a shared object that provides the illusion of being accessed locally. more precisely, this consistency level states that each operation takes effect instantaneously at some point in real time, between its invocation and response. figure 4 (a) presents an execution of linearizable operations. the blue (b) client renames file f to f . concurrently, the red (r) client renames file f to f . since operations are linearizable, one of the two accesses must fail."
"for area \"2\" the system also makes the right prediction when it receives the average fingerprint in the three modalities described before. for 16 waps the system says that it belongs to area 2 with 80% confidence whereas for 5 waps this percentage increases up to 100% (table 26) ."
"both iblocks and dblocks are represented as elements of the same key/value store, where they get replicated according to the different consistency models. int( * create )(const char *, mode_t, struct fuse_file_info * ); int( * open )(const char *, struct feuse_file_info * ); int( * read )(const char *, char *, size_t, off_t, struct fuse_file_info * ); int( * write )(const char *, const char *, size_t, off_t, struct fuse_file_info * ); int( * close)(const char * ); int( * rename )(const char *, const char * ); int( * statfs )(const char *, struct statvfs * ); only iblocks are mutable. the key of a dblock is equal to the hash of its content. this ensures good balancing of the data across storage nodes in order to deliver aggregate performance and increased fault tolerance. in case of an iblock, the proxy generates a unique key at creation time."
"the method we describe here focuses on evaluating interventions among hiv-positive individuals. our approach has been designed with hiv evolutionary dynamics in mind but can be adapted to evaluate interventions for other pathogens and for a broad range of settings. we specifically draw upon the example of hiv-preventive interventions among hiv-infected persons who inject drugs (pwid), where the community effect of an intervention in this group still remains a significant challenge. we focus on risk-network-based interventions such as the transmission reduction intervention program (13), which are designed not only to protect specific individuals recruited by the intervention arm but also to protect their risk network contacts."
"we collect blood samples at time t from both the intervention and defined (also infected) control groups, and we perform hiv genetic sequencing. after some later time t i we revisit the population in order to evaluate the effect of the intervention. an important aspect of hiv natural history is that when someone gets infected they remain infected and carry hiv for life. this allows sequence sampling of the infected individuals many years after the transmission event."
"our tool provides a discrete-event simulator which allows users to interactively and visually simulate system behaviors. in simulation, prts models follow the abstract operation semantics in order to guarantee that each step reflects a meaningful execution of the system. users could choose automatic simulation, which means the simulator will randomly execute the model and generate the random states, or manual simulation, which allows users to choose next event from the current enabled events. through simulation, users could visually check how the model executes step by step, which is very useful in system design and analysis, especially when there are some undesired executions found in verification. simulation is a good complement to verification since users could have an intuitive observation and it makes debugging more convenient."
"indoor location systems focus on solving the problem of best estimating the user location. the exact user location inside a building is occasionally required. in some occasions, however, the location is only required to be estimated at room or area levels. smartphone-based indoor location systems which rely on fingerprinting can be used for either finding the best estimate for the user location inside a building or for finding the best room or area in which the user is located. in the first case it was found that best estimate means that an exhaustive training of the system is required in order to measure fingerprints in as many training locations as possible. trying to estimate the user location at a room level involves, on the other hand, different training requirements for the system. in this paper we focus on best solving this second problem. in the next section we analyse different alternatives for selecting the best training locations for indoor location fingerprinting-based algorithms. the validation results are presented in section 6."
"(11) random tree. this algorithm generates a tree that considers randomly chosen attributes at each node, performs no pruning, and also has the option to allow estimation of class probabilities based on a hold-out set (backfitting)."
"all tests were performed on a cluster of 8-core virtualized xeon 2.5 ghz servers running ubuntu 12.04 gnu/linux and connected by a 1 gbps switched network. we use 3 to 7 servers for the storage layer and one client. our implementation uses the lua programming language (http://www.lua.org/) and leverages the splay framework and libraries [cit] . bindings to the fuse c apis employ the luafuse library (http://code.google.com/p/luafuse/). the flexifs implementation is modular and easy to modify. the conciseness of lua and the use of splay allow the whole implementation to be less then 2,000 lines of code (loc). in particular, the code to support each fscl is very concise and easy to extend, e.g., 62 loc for sequential consistency, and 160 loc for eventual consistency."
"in the following two subsections we show a comparison of the results we obtained when training the algorithms with all locations (12) compared to those obtained when we only use the \"border\" (6) and validate the models generated considering all locations (interior and borders). these results are presented in conjunction with the ones coming from our algorithm to delete unstable waps. this way we get a better overview of the impact of our three major contributions to improve the problem of indoor location based on smartphones and fingerprinting and for the metric \"relative accuracy.\" (a) training the system with all locations. in tables 2-4 we show the results we obtained in the first row for the different algorithms when we train the system with all locations (12) and we have not executed our algorithm to delete unstable waps. in this case we choose all the waps from the initial set (40). then we show the values obtained when we also train the system with all locations (12) but this time we run our algorithm with the configuration previously detailed in table 1 [cit] . this is an improvement of +2.50 percentage points when using the random forest algorithm."
"our intuitive expectation is that if the intervention has reduced transmissibility, then the individuals who received the intervention would have generated a smaller number of transmission chains; a random sample of incident cases should result in finding fewer individuals from the intervention group having phylogenetic links with any of the incident individuals than the control group."
"flexifs implements sequential consistency using primary replication. for each iblock, a primary replica is elected. upon a put(k, v) call, the primary for key k sends to all replicas the value v and then waits until a majority of replicas acknowledges the reception before returning to the proxy. to execute a get(k) call, the proxy accesses any replica of k that contains the version it previously read, or a newer version (this applies only to iblocks). to execute c&s (k, u, v), the primary tests locally if the old value equals u. if it is the case, it executes a put(k, v) and returns the old value to the proxy. a perfect failure detector [cit] ensures the safety and liveness of the above mechanisms."
"at time t an hiv epidemic has n b infected individuals over a population with an overall number of individuals n a (for a list of abbreviations used in equations see table 1 ). at this time t an intervention is applied on n infected individuals and aims to mitigate the spread of hiv over the susceptible population. we believe this intervention has a community effect, so it reduces transmissibility not only by the recruited n individuals but also by the contacts in their risk networks. importantly, the described approach can easily be expanded to include interventions that affect only the recruited individuals to whom the intervention was applied."
"finally, we wanted to analyze how the variations in the measured rssis values overnight affect the predictions made by the system and if there was any way to improve them when applying our noise and stability filters (in our case, the building is dedicated to research activities involving wireless networks and some of the waps were moved of switched off from one day to the other)."
"flexifs classifies the semantics of sharing with (i) the consistency level that governs the fuse interface; and (ii) the use (or not) of the close-to-open semantics. the combination of these two parameters defines a filesystem consistency level (fscl) . in what follows, we list the various consistency levels flexifs supports at the fuse interface and their respective implementation. further, we introduce the close-to-open semantics."
"posix semantics is obtained when file operations are atomic and close-toopen is not supported. to implement sequential consistency, sprite [cit] relies on a cache consistency manager while googlefs [cit] and hdfs [cit] make use of a leasing mechanism. nfs [cit] implements the consistency level offered in andrew filesystem [cit] : the close-to-open semantics is respected and metadata operations are atomic. sinfonia [cit] supports mini-transactions, a generalized form of compare-and-swap operation. to advocate for this paradigm, the authors of sinfonia built a filesystem: sinfoniafs. this filesystem implements sequential consistency with close-to-open semantics. ivy [cit] and pastis [cit] implement an eventually consistent dfss, respectively, with and without close-to-open semantics."
"in this work, we proposed a model checker for hierarchical probabilistic real-time systems. its effectiveness and efficiency are demonstrated through several case studies. as for future work, we are exploring more aspects of probabilistic real-time system such as zeno-check and digitization, and various properties such as real-time property."
"in our model checker, prts models can be analyzed by the built-in editor, simulator and verifier, through which we could investigate system behaviors of the models. in this section we briefly present how the simulator and verifier work."
"finally, for each wireless access point we add the number of locations where the wap is considered unstable. if the result equals or exceeds the lt we eliminate this wap."
"to carry out the analysis of the tests we used 10-fold cross-validation to estimate how accurate a model is before implementing it. using this technique the input data set is divided into 10 equal-sized subsets. of these, one subset is taken to validate the generated model, while the remaining nine are used to train the system and generate the corresponding model. this process is repeated 10 times, using each of the 10 subsets exactly once to validate the model. the 10 results from the 10 iterations are then averaged to produce a single estimation. the sampling type we have chosen is \"stratified\" because not only does it generate random subsets but also ensures that the class distribution in these subsets is the same as in the initial set ( figure 2) ."
"eventual consistency. under eventual consistency [cit], there must exist a monotonically growing prefix of updates on which correct replicas agree. since there is no assumption on the time of convergence, eventual consistency does not offer any guarantee on the return value of non-stable operations (that do not belong to the common prefix). we implement eventual consistency in flexifs using version vectors and the \"last writer wins\" approach [cit] . this optimistic replication schema works as follows: each version of the iblock is timestamped with a version vector. upon updating the value of an iblock (via put() or c&s ()), the proxy contacts one of the iblock replicas. this replica atomically increments its local vector clock, timestamps the iblock with it, and returns to the proxy. replicas then converge using an anti-entropy protocol. if two versions of some iblock are concurrent, we apply the \"last writer wins\" approach. concurrent operations are totally ordered according to the identifier of the replicas that emitted them. upon a read access, a proxy simply returns a version stored at some replica."
"the rest of the paper is organized as follows. section 2 presents a brief review of the major algorithms that have been previously proposed in order to estimate the current user location based on already located fingerprints used when training the system. it also defines the scope of the research in this paper and its major contributions. in section 3 we propose several options for deciding where to take fingerprints in order to best train the system. previously used alternatives are presented together with a novel mechanism that will be analysed and validated in section 6. in section 4 we present a mechanism designed to select the best waps among those available. this section shows how this mechanism provides optimal results in terms of the relative accuracy provided, as well as the computational resources needed. noise and stability filters are presented. section 5 introduces different approaches to combine several fingerprints when locating unknown points in order to improve results. this is done thanks to the analysis of the time fluctuations of the received signal strengths. section 6 captures the validation results using experimental data in two real case scenarios. conclusions are presented in section 7."
"as we saw in the previous tables the system made, among others, the following wrong predictions: for area 1 (fingerprint 11), for area 2 (fingerprint 39), and for area 3 (fingerprint 52). 51 52 53 54 55 56 57 58 59 60 3 3 3 3 3 3 3 3 3 3 2 1 3 3 3 1 1 3 table 24 : system predictions for area 1 when using average fingerprint based on the number of waps."
"in this section we present a comparison of the algorithms described in section 6.2 for the case of the linear configuration in the aisle of the building. the metric we used to evaluate them is called \"relative accuracy\" and is commonly used in fingerprintingbased systems. in our case we aim to measure the number of test fingerprints which are correctly classified according to the area to which they belong."
"(b) training the system with \"border\" locations. the results of this second test are presented in the same format as those already discussed in the previous section. the only difference is that now the system has been trained with only half of the locations (6), those we have called \"border,\" and has been validated with all (internal and border) ( tables 5, 6, and 7)."
"in this paper, we make a step toward allowing the systematic comparison of dfss designs. we instantiate our approach by isolating and evaluating the impact of the fscl on performance. we make the following contributions:"
"it builds a decision/regression tree using information gain/variance and prunes it using reduced-error pruning (with backfitting) and only sorts values for numeric attributes once. missing values are dealt with by splitting the corresponding instances into pieces (i.e., as in c4.5)."
"to implement linearizability in flexifs, we use paxos [cit], which provides a consensus primitive for unreliable nodes. on top of consensus, we implement a replicated state machine executing the three operations listed in section 2.1. notice that, because dblocks are immutable, they are trivially linearizable. hence, to improve performance, we execute a simpler algorithm in that case: operations put() and get() access a majority of replicas, respectively storing and fetching the content from it."
"by introducing the notion of file session, close-to-open semantics [cit] aim at reducing the amount of synchrony required to access a shared file. a file session is a sequence of read and/or write operations enclosed between an open and a close invocation. it has the following properties [cit] : (1) writes to an open file are visible to the client but are invisible to remote clients having the same file opened concurrently. figure 4(b) ). now, when a client executes multiple operations or opens multiple files at the same time, the filesystem is neither linearizable nor sequentially consistent. for instance, execution depicted at the top row in figure 4 (b) is admissible. under close-to-open semantics, linearizability is stricter than sequential consistency, which is itself stricter than eventual consistency (last two rows in figure 4(b) )."
"at time t i we will sample and sequence hiv from n x individuals (we will call them incident individuals) who have been infected after the intervention (see figure 1) . to verify that a patient was infected within the intervention-resampling interval, we may use a combination of approaches and criteria including known time of seroconversion, an hiv-negative test result within the intervention-resampling interval, and/or an avidity test suggestive of recent infection and/or high-throughput deep-sequencing approaches (14) ."
"in conclusion, we have presented a new approach that can evaluate the community effect of interventions that aim to mitigate the spread of pathogens among susceptible populations. the approach exploits molecular evolutionary dynamics of pathogens in order to track new infections as having arisen from either a control group or an intervention group. it enables us to evaluate whether an intervention reduces the number and length of new transmission chains in comparison with a control condition, and thus lets us estimate the relative decrease in new infections in the community due to the intervention. we have also described a framework to calculate sample size and power of suggested intervention designs. this new approach provides a novel formal framework to design and evaluate interventions in settings and situations in which traditional approaches such as randomized controlled trials cannot be applied. figure 3 . theoretical representation of the phylogenetic assignment process as used in the phylodynamics transmission-chain model after relaxing assumption to include samples collected after the intervention (modification of figure 1 ). the molecular sequences are sampled after the intervention. phylogenetic assignment may take place after the intervention, but never before the earlier point of the phylogenetic assignment window a."
"flexifs's indexing and storage layer is a simple yet efficient one-hop dht structured as a ring that relies on consistent hashing to store and locate data. figure 1 presents its general architecture. it supports the following features: routing. for performance reasons and in order to reduce noise in our experiments, we have chosen a one-hop routing design, i.e., every node knows all other nodes in the ring. elasticity. upon joining, a node chooses a random identifier along the ring and fetches the ring structure from some other dht node. it then informs its two direct neighbors that it is joining. storage. flexifs uses consistent hashing to assign blocks to nodes [cit] with replication factor r: a block with a key k is stored at the r nodes whose identifiers follows k on the ring. failure detection. each node periodically checks the availability of its closest successor on the ring, and repair mechanisms are triggered upon a lack of response within a timeout. a gossiping mechanism spreads topological changes throughout the ring. each node notifies its closest neighbor whenever it learns about a leave/join event. if the time to spread a message along the ring is shorter than the time between two leave/join events, this mechanism is guaranteed to maintain the ring topology. in our experience, such an assumption is reasonable for a deployment size of less than a few hundred storage nodes (our typical testbed size)."
"6.1. environment. all the tests described in this paper took place in the first floor of the torres quevedo building (including the hallway and the research lab 4.1.c.03) of the telematics engineering department at the carlos iii university of madrid. in figure 1, the map corresponding to the interior of the first floor of the building is shown and the location of the selected points for the two experiments conducted is presented."
"tag (1) pred (tag) conf (1) conf (2) these are the three scenarios in which we will introduce average fingerprinting to help the system make the right predictions for each of the areas. to do this we will calculate the average fingerprint using (a) the previous fingerprint, (b) the previous and subsequent ones, and (c) two previous and two subsequent fingerprints to the current one."
"(4) ft. functional trees are classification trees that could have logistic regression functions at the inner nodes and/or leaves. the algorithm can deal with binary and multiclass target variables, numeric and nominal attributes, and missing values. for more information, see [cit] ."
"a more complex scenario including 3 wall and door separated areas has also been implemented as presented in figure 1 . in order to validate the accuracy of the algorithms for estimating the area of an unlocated validation point based on the proposed training only with area border points, different validation points from those used for training were selected (randomly selected at each area) as shown in figure 8 . a similar approach to evaluate the 15 machine learning algorithms as in the previous section was used. the same sampling schema as in section 6.3.1 was used for the training points. the average results of the estimated areas for each validation point are shown in table 27 ."
"the \"phylodynamic transmission chain\" model as indicated above we are interested in evaluating the ability of an intervention to prevent both the initialization and expansion of a transmission chain. we will evaluate these 2 properties of an intervention separately even though the expansion of a transmission chain is mechanistically dependent on the initialization."
(1) the way to select the most convenient sampling points to measure training fingerprints in order to increase the accuracy while minimizing the memory and cpu execution requirements. the cost for training the system includes both the number of training points and the number of fingerprints at each point. the paper focuses on reducing the cost associated to the number of points not adding additional constrains on the number of fingerprints at each training point.
"another contribution presented in this paper is the proposal and evaluation of a new way to filter unstable access points in order to increase accuracy and minimize execution requirements. taking into account the quality and stability over time of the received signals from the different waps, the algorithm selects those maximizing both the signal strength and the minimal fluctuations over time. several thresholds are defined. the experimental results show that the algorithms find optimal behavior when filters are configured to select between 10 and 16 waps."
"given globalization and other social phenomena, controlling the spread of infectious diseases has become an imperative public health priority. a plethora of interventions that in theory can mitigate the spread of pathogens have been proposed and applied. evaluating the effectiveness of such interventions is costly and in many circumstances unrealistic. most important, the community effect (i.e., the ability of the intervention to minimize the spread of the pathogen from people who received the intervention to other community members) can rarely be evaluated. here we propose a study design that can build and evaluate evidence in support of the community effect of an intervention. the approach exploits molecular evolutionary dynamics of pathogens in order to track new infections as having arisen from either a control or an intervention group. it enables us to evaluate whether an intervention reduces the number and length of new transmission chains in comparison with a control condition, and thus lets us estimate the relative decrease in new infections in the community due to the intervention. we provide as an example one working scenario of a way the approach can be applied with a simulation study and associated power calculations. community effect; hiv; human immunodeficiency virus; intervention; persons who inject drugs; phylodynamics; pwid; transmission chain abbreviations: hiv, human immunodeficiency virus; t mrca, time to most recent common ancestor; pwid, persons who inject drugs."
smartphone based indoor location systems can be categorized into two major families: on-demand location systems and continuous user location tracking systems. the former normally rely on fingerprinting techniques [cit] while the latter tend to use inertial embedded sensors to calculate incremental user displacements [cit] .
"an important design aspect for a dfss is defining the semantics of sharing, i.e., how clients accessing simultaneously the same file observe modifications by other clients. flexifs has several built-in sharing semantics and corresponding implementations, which we describe in the remainder of this section. additional semantics can be easily added thanks to flexifs's modular design."
"our second challenge was to study the behaviour of the algorithms when we trained the system with only border locations, that is, the ones that delimit an area, and to analyse whether the results were better than when all locations were used (or at least comparable, since training the system only with border locations simplifies the training face and increases scalability). in figure 3, each geometric shape represents an area of the hallway in which the tests were conducted. \"border\" locations are the ones that look darker."
all validation points are properly located except point 15 which is a point very close to the border between areas 2 and table 26 : system predictions for area 3 when using average fingerprint based on the number of waps.
"we present an innovative approach for the automated segmentation, classification, quantification, and visualization of microct images. microct offers the possibility to perform longitudinal studies and thereby provides unique insights into the morpho-and embryogenesis of the live chick embryo. by using obia, image parts (e.g., bones) may be extracted from an image in order to calculate various morphometric feature values. these can subsequently be used to train a classifier that can identify image objects based on these unique values. despite a high classification accuracy, some misclassifications must still be manually corrected in order to yield statistically valid results. human expertise is therefore still required for the interpretation and validation of results. nevertheless, automated systems can greatly expedite image analysis and reduce observer bias."
"in our tests of the wifialertsensor we were able to simulate and detect all relevant attacks detected by kismet and were unable to measure any significant induced delay on event detection in the snbench infrastructure. analysis confirmed the expectation that the amount of time a single kismet message spent in the sensor buffer was directly related to the computation load on the sxe host and the alert generation rate. in general the observed buffer service delay oscillated between zero and 15ms per alert under moderate load with unrealistically high message flooding arrival rates (in practice, kismet can and will throttle alert notification rates, however this was disabled for our performance tests). under heavy load conditions with alert message flooding, we experienced queuing delay as long as 300ms. this gives us a good indication as to the maximum acceptable workload for an individual sxe before it is no longer a viable host for wireless sensing tasks. ultimately any response detection under one second is reasonable as it is unlikely that the attacker would, say, flee the premises (or video frame) within that amount of time."
"i2c distance calculation rather than metric learning, we also propose an acceleration method using spatial restriction for speeding up the nn search, which can preserve or even improve the classification accuracy in most datasets. our objectives for improving the i2c distance are twofold: minimizing the testing time and improving the classification performance."
"wireless intrusion detection systems provide mechanisms to identify, detect and locate dos attacks, yet these systems are generally limited to logging or email alert response mechanisms. many works ultimately recommend dispatching administration personnel to further analyze and respond to a detected attack -a costly and impractical solution in many situations. instead, once the physical area of an attack has been derived it is possible to utilize automated responses from a variety of actuation hardware, if available; e.g., embedded pan-tilt-zoom video cameras to capture an image, wireless detectors on pan-tilt motors to pin-point a signal, programmable robots to triangulate signal, a common message display (virtual bulletin board) in the environment informing users why their service has been interrupted and who is responsible. additionally, there would be a clear benefit from including other, non-network centric inputs to the wireless network security system (e.g., a mac whitelist from bluetooth/rfid tracking, analysis of security camera images or passcard logs)."
"the performance of the trained svm model was evaluated on an unlabeled test set. the svm correctly classified 98.6% of the 1203 object instances that were extracted from the microct images of group 2 eggs. the best classification (table 3) . in contrast to group 1, the ulna (f-measure 0.884), not the carpometacarpus (f-measure 0.933), was the bone with the lowest classification reliability. interestingly, for the femur, the precision is higher for the test set than for the training set (i.e., the application of the model to unknown datasets is highly confident). as with group 1, the noi objects from group 2 also yielded the highest number of false positives (i.e., 12). a maximum of three false negatives were identified during classification of the humerus, radius, carpometacarpus, and tibiotarsus (table 4) ."
"the previous examples are essentially the status quo for a response to the detection of a breach in a wireless network -an entry into a log file or an email alert. the advantage of employing the snbench in the wireless security domain is the wider range of responses possible. nominally, the email operation in program 2 could be replaced with any number of response mechanisms including sending an explicit deauthorization to the detected mac address 4 using the wifiresponder and apdeauth opcode described in section 4. instead, we explore the unique cross section of the network plane (e.g., wireless data frames) with the physical plane (e.g., signal strength and signal loss of signal over distances). for example, an embedded, cross-modal sensor network such as the sensorium can utilize both wireless network sensors (i.e., network plane) and a pan-tilt-zoom video camera network (i.e., physical plane) to catch an image of the attacker \"in the act.\" snafu program 2 e-mail an admin when a specific wireless alert is detected."
"wifialertsensor: the wifialertsensor is a sensorhandler implementation that leverages the kismet [cit] wireless intrusion detector via a self-contained customized kismet client. the java based wifialertsensor class is hosted by a \"non-lightweight\" sxe and translates the proprietary kismet client-server protocol into structured, typed snbench objects (tagged xml) that encapsulate notifications from the kismet server. the decision to use kismet stems from its passive scanning ability, wide range of hardware support, and modular design (described in section 2). while the decision to use this package in particular may be debated, the inclusion of any another functionally-equivalent wireless intrusion detector would be equally straightforward."
"the high-level opcode detectwifiactivity() is compiled into sxe.core-.wifi.get with a wifiactivitysensor as a parameter and blocks until a new activity message is available from that sensor. in addition querywifiactivity() (compiled into sxe.core.wifi.find) searches the wifiactivitysensor's hash table for the latest reading associated with the specified mac address. as with the wifialertsensor, returned data is an snstruct derivative."
"fertilized gallus gallus domesticus (white leghorn chicken) eggs were obtained from a local breeder (geflügel gmbh borna, germany) and incubated in a forced-air egg incubator (grumbach bss300 mp gtfs incubator; grumbach brutgeräte gmbh, germany) at 37.7 ± 0.2 ∘ c and a relative humidity of 60 ± 2%. during incubation, eggs were candled and checked daily for viability. motile embryos were considered healthy. as an additional measure, the buddy digital egg monitor (avitronics, uk) was used to confirm a stable heartbeat."
"this chapter details the inclusion of wireless network monitoring devices in our sensor network infrastructure, snbench (sensor network workbench) to achieve precisely these goals. snbench provides a highlevel programmatic interface to the resources of a sensor network (sn) and thus the inclusion of wireless network sensors enables intrusion detection and response services to be written quickly and easily. snbench has been designed with extensibility and modularity as a central tenet and therefore the changes required to include these new sensing modalities are quite modest. moreover, the framework's modular nature allows a user to swap in any improved emergent wireless surveillance tool or technology (be it algorithmic or a physical turn-key device) with nominal effort and such changes would be transparent to their dependent services. we submit that our programmable, adaptable sn framework is the ideal foundation on which to compose wireless network security services and physical security services alike, providing reciprocal benefit to each. the example programs given provide some insight into the highly customized, cross-modal wireless security behaviors that are possible in this context."
"the network security provided by snbench need not be limited to layer-2 alone. integrating layer-3 detection (e.g., snort) as a sensor would enable the detection of misuse from ip contents that could be used to drive isolation or removal responses at layer-2. including port scanning or other fingerprinting tools as sensors could increase the accuracy of user identification thus further open the possibilities for more \"severe\" automated response."
"image-to-class distance relies heavily on the large number of local features in the training set and test images, which need heavy computation cost for the nn search in the testing phase. however, using small number of features results in poor performance. in this paper, we tried to improve the performance of this distance and speed up the testing phase. we added a training phase by proposing a distance metric learning method to learn the i2c distance. a large margin framework has been formulated to learn the per-class mahalanobis distance metrics, with a gradient descent method to efficiently solve this optimization problem. we also discussed the method of enhancing the discrimination of the learned i2c distance for performance improvement. these efforts made the i2c distance perform excellent even using small feature set. for further accelerating the nn search in the testing phase, we adopted single level spatial restriction, which can speed up the nn search significantly while preserving the classification accuracy. the experiment results on four datasets of scene-15, sports, corel and caltech 101 verified that our i2cdml method can significantly outperform the original nbnn especially for those challenging categories, and such i2c distance achieved state-of-the-art performance on most datasets."
"an example snafu program that provides simple logging is given in program 1. a level trigger is used to assign an event handler to the detection of a high severity wireless alert. the storage.append opcode modifies a named storage entity (i.e., table) by inserting a data object and its corresponding unique key. the storage table is keyed by timestamp and includes entries for each detected violation containing the recorded mac address, the sensor from which the alert was detected, and the type of alert. unlike the logging provided by kismet as an ids, this service records which sensor has detected the event and is backed by an sql server. the logged data is available programmatically via storage access opcodes or direct sql queries, or through a standard web browser via the sxe host's web service that performs xsl translations to render the local data storage."
"embryos from d13 to d15. crural bones are much longer ( figure 5 ) and have a much higher rate of growth than aral bones (table 5 ). the highest increase in length (2.902 mm/day) was recorded for the tarsometatarsus from d14 to d15, while for the same interval, the radius had the smallest increase (0.504 mm/day)."
"in addition, we find in our experiments that a single level spatial restriction at a finer resolution makes better recognition accuracy compared to the top level especially for those images with geometric scene structure, although the accuracy is slightly lower than the pyramid combination of all levels. since the candidate searching set is smaller in a finer level, which requires less computation cost for the nn search, we can use just a single level spatial restriction of the learned i2c distance to speed up the classification for test images. compared to the top level, a finer level spatial restriction not only reduces the computation cost, but also improves the recognition accuracy in most datasets. for some images without geometric scene structure, this single level can still preserve the recognition performance due to sufficient features in the candidate class."
"moreover, we adopt the idea of spatial pyramid match [cit] and learning i2c distance function [cit] to generate a more discriminative distance for improving classification accuracy. since the main computation burden is the nn search in fig. 1 . the classification structure of our method. the rectangular and triangles denote an image and its local feature points respectively. the ellipse denotes a class with images (rectangular) inside it. the i2c distance from image xi to a class c is formed by the sum of mahalanobis distance between each local feature fij and its nn f c ij in class c with the matrix mc learned for that class. the predicted label of image xi is chosen by selecting the shortest i2c distance. section 2.1 gives a full explanation of these notations."
"to evaluate the classification performance for a single day of incubation, an svm that leaves out bones for that specific day, was trained on all remaining objects. the resulting svm was then applied to the image objects of the excluded day of incubation. in general, the classification accuracy increases with the day of incubation (figure 4) . bones become more calcified the longer the egg is incubated, thus allowing for improved differentiation. however, the classification accuracy is dependent on bone type. while the femur can easily be identified, annotation of the carpometacarpus can be problematic. nevertheless, the intrinsic information from the other days facilitates accurate annotation. hence, this model can also be used to transfer knowledge between different days of incubation."
"in this paper, we aim to enhance the performance of i2c distance especially for small number of local features, so as to speed up the testing phase while maintaining excellent result. to achieve this, we propose a training phase to exploit the training information and suggest a distance metric learning method for the i2c distance. our method avoids the shortcoming of both non-parametric methods and most learning-based methods involving i2i distance and descriptor quantization. this leads to a better recognition performance than nbnn and those learning-based methods. for each class, we learn the class specific mahalanobis metric to combine with the corresponding i2c distance. when classifying a test image, we select the shortest i2c distance among its mahalanobis i2c distances to all classes as its predicted class label. since only the metric of the belonging class can well characterize the local features of the test image, such per-class metrics can better preserve the discriminate information between different classes compared to a single global metric."
"for each egg of group 1, the image object data extracted during the automated image segmentation step were annotated and international journal of biomedical imaging 3 classified according to the following categories: humerus, radius, ulna, carpometacarpus, femur, tibiotarsus, and tarsometatarsus. only clearly discernable bones were classified. all remaining image objects, including those representing clotted or blurred bones or bones that appeared anatomically incorrect (e.g., because of image artifacts), were classified as not of interest (noi). finally, all annotated data were combined into a single file that was used to train and test the automated classification process."
"unfortunately, the overabundance of generated image data makes the manual analysis of resulting images a timeconsuming and tedious task. furthermore, the visual interpretation of images is error prone and highly subjective. therefore, automated image analysis systems are highly desirable. the most important tasks of such systems are the automatic detection, segmentation, quantification, and classification of biological structures from various 2d, 3d, and 4d images."
"this is equivalent to the equation (2) . if m c is an identity matrix, then it's also equivalent to the original euclidean distance form of equation (1) . in the following subsection, we will use this formulation in the optimization function."
"using cnt, a rule set that reliably segments in ovo microct images of chick embryos, including those at different stages of incubation, can be created in cnl. the bone objects of interest could be extracted, and their features were used to train an svm that classifies long bones with high accuracy. to present a potential application of our workflow, we studied long bone growth of chick embryos in ovo from day 13 to day 15 of incubation based on daily microct measurements."
"however, the performance of this i2c distance relies heavily on the large number of local features in the training set and test image. for example, the state-of-the-art performance they reported in caltech 101 dataset is achieved by densely sampling large redundant local features for both training and test images, which results in about 15000 [cit] 0 features per image. such large number of features makes the nearest-neighbor (nn) search in i2c distance calculation computationally expensive when classifying a test image, which limits its scalability in real-world application. if only small number of local features is used, the performance of this i2c distance will be poor as shown in the later experiment section, although it needs less testing time."
"generally, attaining such cross-modal interaction within the context of a network intrusion detection tool would require the generation of highly customized, package and deployment specific software (modules, scripts, etc.) that are, by their very nature, cumbersome to maintain. indeed, such an approach is wrong headed. we observe that wireless network security services are specific, narrowly focused instantiations of an embedded sensor network wherein sensory data includes the output of such monitoring tools. rather than \"hack\" a wireless security system to include sensor network functionality, we advocate the inclusion of wireless security within a sensor network. thinking differently about network security, the integration of new sensory data (e.g., motion detection, face detection) and actuation responses expand network security beyond the digital plane and into the physical plane."
"wifiresponder: in addition to the wireless network sensing described above, the layer-2 wireless actuator (i.e., output device) wifiresponder may be used as a retaliatory action against a detected attacker. the wifiresponder invokes a script on a trusted (whitelisted) device running linux with a compatible 802.11 interface and the airreplay-ng [cit] tool. the opcode apdeauth() takes as arguments a wifiresponder that will send a flood of deauthenticate messages to a particular mac address (the second argument) from a particular mac address (the third argument). 2 an actuator is nearly identical to a sensor in its implementation within snbench. the handler for wifiresponder invokes the remote common gateway interface (cgi) script to initiate the deauthenticate \"attack\" against the specified host."
the outer loop was used for global refinement and to control if the segmentation steps performed in the two inner loops contributed a substantial amount of new pixels to new or existing skeleton image objects. the complete bone segmentation step was terminated when the last round of segmentation and classification performed by the two inner loops did not increase the number of pixels classified as skeleton by 0.005% or greater.
"the wifialertsensor's message buffer is configurable in length (where length is measured in either size or time) and alert messages are retrieved from the buffer by opcodes requesting data from this sensor. implementation of the retrieval opcode may impose a blocking or nonblocking semantic, as needed. in our experimentation we implemented a single alert-centric opcode, sxe.core.wifi.get, that performs a nonblocking read from the alert sensor's buffer to populate and return a wifialert. the wifialert data-type is a subtype of snstruct, with tagged fields corresponding to the fields populated by the wifialertsensor and thus accessing the data within a wifialert reuses the existing snstruct manipulation opcodes. a service developer retrieves wifialerts via the high-level function detectwifialert() that is compiled into a call to the opcode sxe.core.wifi.get with a wifialertsensor (or set of sensors) as a parameter. high-level service logic examples are given in section 7."
"persistent triggers extend the basic triggers in that they return a stream of values over their persistent evaluation. a leveltrigger evaluates the predicate p indefinitely (or for some specified length of time or conditional termination) and evaluates and returns a value of q every time p evaluates to true. in practice p may be the detection of a particular mac address being used in the network and q is the recording of an image at the detected locale. an edgetrigger continually evaluates the predicate, but will only evaluate and return the clause q whenever the predicate p transitions to be true (i.e., on the edge of the signal p). consider, if the expression p represents detection of two deauthenticate beacons (indicating the start of a deauthenticate flood) and q is an sms pager alert, we do not want to generate a separate notification for every consecutive deauthenticate beacon for the duration of the flood."
"a very simple wireless cell-of-origin location example is specified in program 3. the program's content is very similar to the previous examples and introduces some pan-tilt-zoom sensor (ptzcamera) specific opcodes, the function of which should be clear from context. this sample streams images of a region where an attack has been detected. the location estimation is explicit in the service logic, selecting an image from the camera that best covers the physical space within the signal coverage region of the relevant wifi sensor, which (in this example) requires some knowledge of the specific physical layout of the sensor deployment. the case expression takes the same syntax as in standardml and is used for readability as syntactic sugar (i.e., a macro) for nested conditionals. connecting this program fragment to either of the previous examples would log or email images that correspond to the attack location."
"to support the wifialertsensor, each of the aps run a kismet drone process, while the kismet server process runs on the same host as the sxe. although the kismet server process could also be run directly on the ap, the ram and cpu limitations of these devices lead to a less responsive system in that scenario. as the kismet server did not distinguish the results from different kismet drones at the time of our experiments, one kismet server process was required per drone, and each wifialertsensor connects to a unique kismet server process thus allowing snbench to distinguish which drone generated a wireless event. running one kismet server per drone also carries the advantage of minimizing the impact of a kismet server process hanging, or failing to process updates from its drones (admittedly a fairly uncommon occurrence)."
"the objective function in our optimization problem is composed of two terms: the regularization term and error term. this is analogous to the optimization problem in svm. in the error term, we incorporate the idea of large margin and formulate the constraint that the i2c distance from image x i to its belonging class p (named as positive distance) should be smaller than the distance to any other class n (named as negative distance) with a margin. the formula is given as follows:"
the in ovo chick embryo is a highly versatile model organism with a long history of use in biological and biomedical research [cit] . the embryonated chicken egg is favored in embryogenic studies [cit] because it allows for easier access and manipulation and is more economical.
"in conclusion: network security (specifically, wireless security) is not a problem that exists in a vacuum detached from the physical space in which the network is deployed. we promote an approach that unifies physical site surveillance and network security under the umbrella of snbench -our general purpose sensing infrastructure. in that regard, we have demonstrated how snbench enables the rapid development and deployment of cross-modal security services. we have shown that with snbench (1) detection of wireless anomalies can be correlated with other sensory inputs providing reciprocal benefit to merging security on the physical and cyber planes, (2) detection and response services may be easily composed and modified without technical knowledge of the specific protocols or implementations of the underlying sensory tools, and (3) adding new intrusion detection tools as input or other devices for response is straightforward given snbench's modular architecture. the illustrative example programs provided include the status quo (simple logging and email alerts) and hint at where we may go from here, in an attempt to spark the reader's imagination to consider what sensors, actuators and new hybrid services may be enabled by the snbench platform."
"wifiactivitysensor: the activity sensor provides data regarding wireless transmissions that have been detected by a passive, promiscuousmode wireless sensor. in particular, we are interested in the mac address of a transmission, the observed signal strength (rssi) and the mode of the transmission (i.e., access points, clients, ad-hoc participants). while determining physical location from rssi is imperfect (as rssi readings themselves may not be entirely accurate depending on the driver implementation and other physical factors), the use of rssi readings can better estimate the physical location of a mac address beyond the simple cell-of-origin. wifiactivitysensor maintains a hashtable of the detected wireless activity (keyed by mac address), which can be used either to report new/updated wireless activity (similar to the alert sensor) or to query the activity log to find information about a particular mac address. like the alert sensor, the activity sensor also communicates with a remote sensor \"server\" process responsible for gathering data."
"snbench is extensible by design insofar as support for new sensing devices may be added to the sensor execution environment (sxe) by providing implementations of two relatively small interfaces; a sensorhandler translates snbench requests to interact with a specific device and a sensordetector module must provide a facility to detect new devices of this type and inspect their state. the sensorhandler is akin to a device driver, abstracting away the specific idiosyncrasies of the particular device's interface and enabling the device to be accessed by higher-level programming constructs. as far as the snbench framework is concerned, the abstracted device becomes just another managed input device/event generator only different from a video camera or motion sensor insofar as the datatype of its output. to enable wireless network security service composition on snbench, two new sensors and a new actuator were added; the wifialertsensor reports wireless alert detection events, the wifiactivitysensor reports mac addresses and received signal strength indication (rssi) for any passively observed wireless activity, and the wifiresponder actuator sends a disauthenticate flood to a particular mac address. rather than implement wireless layer-2 tools from scratch, we opted to leverage several existing open-source software packages."
embryos from d13 to d15. the image object feature length was used for the analysis of long bone growth. the feature is derived from the three eigenvalues of a rectangular 3d space with the same volume as the image object and the same proportions of eigenvalues as the image object. the length of an image object is the largest of the eigenvalues.
"in this step, the skeleton was separated from the rest of the egg interior. because calcified bones have considerably high pixel values that form a pixel subset distinct from the egg interior, the at function could also be applied extensively in this step. the at function extends enough robustness to the entire rule set so that bones and bony structures are correctly segmented for eggs from d13 to d19. however, some additional measures needed to be taken for the correct segmentation of bones located close to the shell. here, the at function fails to directly calculate the best separating threshold, and consequently, initial segmentation often leads to large image objects that are attached to the shell and which need to be further treated. a possible solution that provides robustness, as well as a high segmentation quality, was implemented in three nested loops that perform repeated automatic threshold calculation, image segmentation, and segmentation refinements (figure 1) ."
"as the kismet drone/server cannot retrieve the rssi on all hardware platform, two different physical implementations for the activity sensor server are supported. for kismet's rssi-supported hardware, the client, which is derived from the wifialertsensor implementation, requests and parses network and client messages from the kismet server rather than alert messages. for the openwrt platform, a custom monitoring program sends ioctl's to the wireless device to put the device in passive monitor mode, accept frames, and retrieve data from the frames and device (including the rssi). this program is based on code from the open source wiviz [cit] package for openwrt, which contains the ioctl codes needed to achieve the proper device state and interaction. like the kismet server, this program provides notifications of activity messages which are received and hashed by the wifiactivitysensor."
"our test-bed deployment contains several openwrt [cit] linux enabled linksys wrt54gl access points (aps), each with the kismet-drone, airreplay-ng, and signal strength monitor packages installed. the aps are configured to use their wireless interface in client mode, and are connected to our gigabit research lan by its 100mbit ethernet port."
"several imaging modalities may be considered for the in ovo observation of the live avian embryo: fluorescence microscopy [cit], magnetic resonance tomography (mrt) [cit], ultrasound [cit], and computed tomography (ct) [cit] . both mrt and ct are noninvasive; they do not entail damaging the egg shell. both imaging modalities provide three-dimensional information at high spatial resolutions, thereby allowing for longitudinal studies and the study of long-term processes (e.g., bone growth and ossification) in international journal of biomedical imaging the same chick embryo in ovo. however, only ct provides sufficient bone contrast."
the microct images were acquired during a previous study [cit] in which the bone metabolism of live chick embryos at different days of incubation was investigated using single and repeated 3d and 4d
"all remaining temporary skeleton that could not be classified as skeleton because they did not satisfy the border conditions were then fed into the second inner loop for the refinement of segmentation and the extraction of additional skeleton image objects. the second inner loop had the same basic functional principle as the first inner loop. using an automatically calculated threshold, the temporary skeleton image objects were further segmented into temporary skeleton and temporary no skeleton 2 objects. however, unlike the first inner loop, the classification of the temporary skeleton objects was now performed using the relative border to the shell border and to temporary no skeleton 2 as measurements combined in a fuzzy set with a linear or sigmoidal membership function. the second inner loop was exited when the at function could not calculate a new threshold (i.e., the best separating value was reached). all remaining temporary no skeleton 2 objects were then reclassified as interior and fed back into the first inner loop."
"international journal of biomedical imaging 9 images can be analyzed within the same timespan. moreover, such systems minimize observer bias. the definiens developer xd rule set for automated image segmentation was developed using only one egg at d18. however, it proved to be robust enough to successfully segment the skeleton from the rest of the egg and its periphery on microct images for eggs from d13 to d19 without having to adjust parameters for incubation day. thus, the approach could effectively manage variations in bone size and calcification. in addition for determining feature values for single bones, the segmentation and classification results could be also used to provide an excellent 3d in ovo visualization of the developing chick embryo (figure 3) . the high classification accuracy using an svm greatly facilitates the classification of objects extracted from microct images, although bones with a low classification probability should be reviewed to avoid corruption of length measurements. an iterative approach comprising repeated classification and feature value calculation could, however, refine the existing classification and enable the classification of previously unclassified objects. in the first step, only objects with high classification probabilities would be classified. based on this initial classification, new feature values (e.g., distances) could be calculated for unclassified objects in a subsequent step. in turn, these values could be used to train a new and extended classifier."
"snbench consists of programming support and a runtime infrastructure for sensor networks comprised of heterogeneous sensing and computing elements that are physically embedded into a shared environment. we refer to such a physical space with an embedded sn as a sensorium. the snbench framework allows sensorium users to easily program, deploy, and monitor the services that run in this space while insulating the user from the complexity of the physical resources therein. we liken the support that snbench extends to a sensor network to the support that higher-level languages and operating systems provide to traditional, single machine environments (language safety, apis, virtualization of resources, scheduling, resource management, etc). snbench is designed such that new hardware and software capabilities may be painlessly folded into the infrastructure by its advanced users and those new capabilities easily leveraged by its novice users. snbench provides a high-level programming language with which to specify programs (services) that are submitted to the resource management component which in turn disseminates program fragments to the run-time infrastructure for execution. at the lowest level, each sensing and/or computing element hosts a sensor execution environment (sxe) that abstracts away specific details of the host and attached sensory hardware. sxes are assigned tasks by the resource management components of snbench; the sensorium service dispatcher and sensorium resource manager in tandem monitor sn resources, schedule (link) and deploy (bind) tasks on to available sxes."
"object and an object representing the interior of the egg (figure 2(g) ). additionally, the shell object was surrounded by two layers of pixels classified as shell border."
"in this section, we formulate a large margin convex optimization problem for learning the per-class metrics and introduce an efficient gradient descent method to solve this problem. we also adopt two strategies to further enhance the discrimination of our learned i2c distance."
"during the growth process, a stringent surface tension criterion was applied to prevent the new object from growing back into the initial coarse egg/bed (complete) object. the result of this step was the coarse egg/bed (parts) object (figure 2(d) ). the remaining temporary objects were reclassified as background. consequently, the background consisted only of air and the animal bed, a fact that could then be used to help remove the remaining part of the bed from the coarse egg/bed (parts) object. the strategy was to model the animal bed by segmenting the background into bed and air and then expand the bed through the coarse egg/bed (parts) object using the grow operation. to separate the two image parts of the background, a threshold was calculated by using the automatic threshold function (at) on the original unfiltered ct layer. based on a combination of histogram and homogeneity measurements, this function calculates a pixel value such that intensity differences and heterogeneity increase to a maximum between the resulting pixel subsets. in this case, the respective subsets were the animal bed and air. the background was segmented using the calculated threshold. all pixels with values above the threshold were then classified as bed. this image object was then expanded along the z-axis through the coarse egg/bed (parts) object. as a result, all affected pixels, and thus the animal bed, were reclassified as background and removed from this object (figure 2(e) ) resulting in the coarse egg object."
"automated image segmentation methods [cit] ) and tumorinduced bone destruction [cit] . to our knowledge, these techniques had not yet been applied to the in ovo quantification of bone growth of live chick embryos. we developed an approach for the automated segmentation of in ovo microct images from live chick embryos using obia, followed by automated classification of the extracted image objects. as automated routines heavily reduce processing time, more"
"a kismet client may request to receive several types of kismet messages from a kismet server/drone pair (client traffic, ap detection, sus-picious activity alerts, etc.). in the case of the alert sensor, the client requests notification of all wireless alerts supported by the current stable build of kismet. whenever the kismet server detects an alert condition from its corresponding drone's data feed, an alert is sent to the wifialertsensor client which translates and buffers the alert message. in addition to translating the kismet protocol, the wifialertsensor adds additional fields to the alert message: a local timestamp to measure buffer service delay, a sensor source to identify the physical sensor (drone) that produced the message, and a severity field that indicates the relative threat of the particular attack."
"as it mimics human visual perception, the objectoriented image analysis approach based on the cognition network technology (cnt) offers key advantages over pixel-based approaches. instead of solely relying on pixel information, cnt emulates the segmentation, description, and identification of image objects through context sensitive associations [cit] . based on cnt, rule-based solutions can be created for virtually any question related to image analysis. for rule set creation, a flexible programming language called cognition network language (cnl) has been constructed. recently, cnt and cnl have been used to solve image analysis tasks in such fields as infection, cell and developmental biology [cit] and in clinical and preclinical radiology [cit] ."
"prior to micropet measurements, a blood vessel of the chorioallantoic membrane was catheterized through a small hole in the shell for injection of the radiotracer ([ 18 f]naf). to ensure the normal development of the experimental chick embryos, the beak length (from where the parasphenoid articulates with the palatine to the tip of the upper bill) was measured on microct images and compared with controls."
"the virtual instruction set architecture of snbench is the sensorium task execution plan (step), a tasking-language used to describe complete programs and fragments alike. a step program is a graph of an sn program's data-flow and computational dependency, with the nodes of a step graph representing the atomic computation and sensing operations and edges representing data flow. in execution, demand for evaluation is pushed down from the root of the graph to the leaves, and values percolate up from the leaves back to the root. step nodes describe data, control flow (e.g., repetition, branching) and computation operations that we refer to as step opcodes, and the sxe maintains implementations of the opcodes with which it may be tasked."
"to evaluate the gradient of objective function for each matrix, we denote the matrix m c for each class c at t th iteration as m"
"opcodes do not directly manipulate sensors, but rather manipulate snbench typed data. specific details of the sensor hardware of the sxe are abstracted away by a sensorhandler module that is capable of communicating with and reformatting the data from a specific sensor to produce to snbench typed data; support for new sensor device types require the addition of new sensorhandler modules 1 . in snbench there is a distinction between a sn service developer who uses high-level pro-gramming languages to compose services by gluing together opcodes and sensors (generally without regard for how the opcodes are actually implemented beyond their type signature) and the snbench \"engineers\" who are responsible for expanding the opcode and sensorhandler libraries to enable new functionalities."
"our method also supports the incremental learning. when new training images of existing class or new class are added, per-class metrics do not need to be re-trained from the beginning. the current learned matrices can be used as initial estimates by changing the identity matrix i to current matrix for each class in algorithm 1, and new triplets are added to update all matrices. the updating procedure will converge quickly since pre-learned matrices are relatively close to the optimal. this incremental learning ability shows that our method can be scaled-up to handle large number of classes and support for on-line learning."
"to understand the wireless security services examples it is important to understand the key concepts and unique constructs of snbench programming. 3 the sensorium task execution plan (step) language has a functional-style, high-level sibling called snafu (sensor network applications as functions). snafu serves as a readable, accessible language that is compiled into the graph-centric step for execution. broadly speaking, functions in snafu correspond to the computational nodes of a step graph while terminals represent nodes that convey sensors, actuators, and constant values. snafu provides symbolic assignment and function definition, however it forbids explicit recursion by reference. instead snafu provides iteration constructs called triggers. a trigger takes two arguments: a predicate and a response clause. the predicate is repeatedly evaluated until it evaluates to true, at which point the response clause is evaluated and returned as the result of the trigger expression. for example, consider the expression trigger(p,q) in which p is the detection of an ap intrusion and q is an expression that shuts down the ap. the whiletrigger(p,q) is similar to the previous trigger, except that it evaluates q every time p evaluates to true and when p eventually evaluates to false returns the last value of q (or nil if p was initially false)."
"understanding the mechanisms of bone development is highly relevant for poultry farming, where skeletal deformities in long bones can have a substantial economic impact. insights gained from the chick embryo model also allow for a greater understanding of human bone development and metabolism as well as associated diseases [cit] ."
"since a spatial single level at a finer resolution will reduce the computation cost required for feature nn search, we also compare its performance with spatial pyramid combining multiple levels as well as the original size without using spatial restriction. as shown in figure 4, this spatial single level is able to improve the accuracy compared to no spatial restriction, not only on scene constraint datasets (scene-15 and corel) but also on sports event dataset that does not have geometric scene structure. though the performance is slightly lower than the pyramid combining all levels, it saves the computation cost for both feature nn search and distance metric learning. so this spatial single level strategy will be very useful for improving the efficiency."
"wireless access lists from physical data: with this infrastructure in place, programs may use information detected on the physical plane to (re-)configure the wireless network. for example, an embedded camera network and face detection opcodes can be used to detect the identities of individuals entering or leaving as a trigger to enable the detected user's wireless mac address for service in a physical space. put simply, when we see jane enter the lab we want to enable jane's mac address (added to the whitelist), and disable her mac address when she leaves the lab. a dynamic whitelist would make it more difficult for a malicious user to abuse unused, authorized wireless mac addresses for great lengths of time. modification of the wlan's access control list in this way assumes the presence of a mac whitelist; such an implementa-tion is straightforward on openwrt enabled aps, using a cgi script to modify the device's configuration. in addition, other physical sensors could be used in tandem with face detection as the trigger predicate in this expression; e.g., biometric sensors, magnetic card or rfid readers."
"a variety of wireless intrusion detection systems (wids) have been created to address wireless network security concerns. wids employ wireless probes/sensors to monitor the media access control (mac) frames transmitted on the wireless medium and identify misuse by observing either suspicious characteristics of individual frames (e.g., exhibiting characteristics imprinted by standard hacking tools) or a particular pattern in a sequence of frames (e.g., sequences in violation of protocol standards). wireless misuse includes illegitimate users attempting to gain access to the network (intrusion), man-in-the-middle attacks (e.g., luring legitimate users into communication with a rogue access point), and various denial of service (dos) attacks [cit] (e.g., spoofing a legitimate wireless access point (ap) and sending a disauthenticate beacon to legitimate users). wireless intrusion is often dealt with using layer-3 mechanisms (e.g., content based packet filtering, ip address isolation), essentially ignoring the option of layer-2 detection and prevention. layer-3 idss are likely popular because there is far more data available at layer-3, making it straightforward to respond to attacks, and because detection and response at layer-3 is independent of the layer-2 connection medium. on the other hand, layer-3 response to layer-2 wireless dos attacks is limited given that attackers will likely utilize fictitious or spoofed mac addresses and may not have an ip address to retaliate against. ultimately the only way to respond to these types of attack is to utilize information derived from the wireless medium (e.g., received signal strength) to reconstruct physical location toward the goal of preventing further wireless transmissions from that user [cit] ."
"alternatively user location reconstruction could be implemented within an opcode, resulting in ids logic that is agnostic to the particular location resolution mechanism used. such an approach makes sense if the deployment environment already contains a wireless location infrastructure (e.g., network appliance, all knowing oracle) that could be accessed from an opcode call. an example of this approach is given in program 4. wifilocatemac encapsulates the physical location of mac addresses and ptzlocate determines the best ptz camera (and corresponding angle) to capture an image of that location. the implementation of wifilocatemac is functionally similar to bestptzforviewof in the example in program 3, yet uses a received signal strength from multiple sensors to estimate the target's location between the sensors."
"to exclude additional periphery, only the largest image object (i.e., the egg and the complete animal bed object) was retained. next, this coarse egg (complete)/bed (complete) object needed to be segmented into its two components. since the carbon fiber bed and the interior of the egg have similar pixel values, this separation cannot be performed based solely on these values. therefore, a modeling approach that exploits morphological differences between the animal bed and the egg was developed. this approach is based on the knowledge that the egg is axially aligned in the field of view (fov) of the microct scanner and that in an axial view an egg is much rounder than the animal bed."
"long bone growth during chick embryogenesis has been extensively studied under normal conditions [cit] as well as under various environmental influences such as insecticides [cit], increased temperature [cit], and acceleration [cit] . however, none of these studies provided in vivo data; the chick embryos were sacrificed, removed from the egg, and fixed. bone lengths were either measured while bones were still attached to the limb or after they were dissected from adherent tissue."
"extraction. the image segmentation and feature extraction process is divided into three steps (figure 1 ), which are outlined next: egg detection, shell segmentation, and bone segmentation."
"we describe our large margin optimization problem as well as an efficient solver in section 2, where we also discuss our improvements in addition to the learned metrics. we evaluate our method and compare it with other methods in section 3. finally, we conclude this paper in section 4."
"add to a central log on detection of a wireless alert. this sample snafu program could easily be extended to establish a log of all observed wireless activity (not just attacks) by adjusting the predicate of the trigger from detectwifialert to detectwifiactivity and removing the severity check. another simple example is given in program 2, which automatically emails an administrator when a specific wireless attack is detected."
"to separate the egg from the bed, the coarse egg/bed (complete) object was first split into a series of 2d slices (figure 2(c) ). on each slice, all parts of this object below a certain degree of roundness (calculated as the quotient of the radius of the largest enclosed ellipse divided by the radius of the smallest enclosing ellipse) were reclassified as temporary objects (figure 2 c 1 ) . using a pixel-based grow operation, the remaining round objects (figure 2 c 2 ) were first expanded along the z-axis (i.e., from the blunt to the pointed end of the egg) and then along the x-and y-axes into the temporary objects."
"the extracted image objects and associated properties can be used to train a model for machine learning, which can then be used to automatically classify anatomical units (i.e., bones) in unknown image datasets. as they work on nonlinear problems and can achieve high precision-even with small training sets, support vector machines (svm) have proven advantageous for object-based image analysis (obia) [cit] ."
"any user detected engaged in wireless network intrusion is clearly within a bounded distance from the detecting sensor. this coarse, cellof-origin based physical location of wireless users is available, imprinted in all wireless data returned from the wifisensors (determined by which sensor has detected the user)."
"the first inner loop was exited when a better separating threshold could not be calculated. the number of pixels added to existing skeleton image objects or representing new skeleton image objects was then calculated in order to confirm if the termination condition of the outer loop was satisfied. if the refinement was above the threshold, the temporary no skeleton 2 objects were combined, reclassified as interior, and subjected to another round of bone segmentation. otherwise, after satisfying the termination condition of the outer loop, the final result of the bone segmentation step was a set of skeleton image objects representing the chick embryo skeleton and a number of image artifacts (figure 3 ). for each of these image objects, the following feature values were calculated and exported into a csv file: day of incubation, asymmetry, border length, compactness, elliptical fit, length, thickness, width, length/thickness, length/width, volume in relation to the total volume of all extracted bone objects, radius of largest enclosed ellipse, radius of smallest enclosing ellipse, rectangular fit, roundness, shape index, mean of the ct image layer, standard deviation of the ct image layer, skewness of the ct image layer, minimal pixel value of the ct image layer, maximal pixel value of the ct image layer, number of other bone objects within a range of 50 pixels, distance to the nearest skeleton object, and distance to shell. the typical run time of the rule set was between 6 and 15 min depending on the number of bones, bony objects, and artifacts in the image."
"ideally we could imagine migrating our own campus it departments to use snbench as their network security and intrusion suite, a transition that could be eased by the development of a declarative/ruleoriented domain-specific language (and corresponding step compiler) that is similar to existing network rule specification languages. finally, our work on lightweight sensor execution environments for embedded devices could be used to run the sxe directly on openwrt enabled aps to provide snbench as a turn-key solution for wireless network security services."
"the rule set for automated image segmentation and feature extraction of bone objects was developed with definiens developer xd 2 (definiens ag, germany) on a computer (intel xeon x5650, 2.66 ghz, 24 gb ram) running windows xp professional x64 edition [cit], service pack 2). the rule set is described in detail in the results section."
"a total of 2951 annotated object instances extracted from the images of group 1 were used for training the svm. the grid search yielded an optimal setting of 1.25 for the c and 0.1125 for . after 10-fold cross-validation, the model proved to be highly accurate (table 1) and was able to correctly classify 98.6% of the instances. the femur can be most accurately identified (f-measure 0.981), while the correct annotation of bones of the carpometacarpus is slightly more challenging (f-measure 0.882). the largest class (noi) can also be readily separated from the other bone types. this is integral to the automated annotation of datasets not having bone name labels (unlabeled dataset). the highest numbers of false positives (15) and false negatives (16) were, however, identified during classification of the noi objects (table 2) . to evaluate the classification performance for a single day of incubation, an svm was trained on all objects but the image objects belonging to the corresponding day. the resulting svm was then applied to these excluded objects. the overall classification accuracy increases with the day of incubation."
"the automatic long bone classification system was built using the waikato environment for knowledge analysis (weka) machine learning tool [cit] . it provides a support vector machine (svm) implementation based on the sequential minimal optimization (smo) method [cit] . in general, an svm is a classifier that can separate instances belonging to two classes in a nonlinear space. this is achieved by the kernel trick, which transforms the initial nonlinear problem into a linear one by adjusting the input space. another key feature of svms is that they separate the instances so that a maximal margin between the two classes is achieved. this margin is then expressed by support vectors that define the separating hyperplane. the best parameters, c (the number of support vectors) and (the variance of the kernel function) for the svm, were found using a java implementation of a grid search method [cit] . to use such an svm, it must be trained on a dataset, which results in the specific vectors for the hyperplane. the svm can then be applied on a test set to classify the long bones. the svm was trained using the annotated data from the group 1 eggs. before training, all input data were standardized. to obtain probability estimates of the classification results, logistic regression models were fitted to the outputs of the svm."
"finally, commercial offerings provide turn-key detection and response systems for corporate wireless networks (e.g., [cit] ). responses to wireless attack detection in these systems are more proactive (e.g., disauthenticating malicious users from the network), yet they do not provide integration with third-party tools or offer a programming interface to adjust the sense and respond behavior. commercial sense and respond widss lack the extensibility required to enable cross-modal monitoring (e.g., utilizing video frames)."
"the trained svm was evaluated based on the classification accuracy for the unlabeled data from group 2 eggs. for this test, an additional constraint was implemented; each chick embryo could only have two long bones of each kind. hence, if more than two bones were assigned the same classification, only the two with the highest probability were retained. the superfluous objects were classified as noi. for assessing classification accuracy, the results were reviewed and the classification error was calculated. for the subsequent analysis of long bone growth, misclassifications were corrected."
"directly calculating the gradient in each iteration using this formula would be computational expensive. as the changes in the gradient from one iteration to the next are only determined by the differences between the sets n t and n t+1, we use g(m t c ) to calculate the gradient g(m t+1 c ) in the next iteration, which would be more efficient:"
"the aim of this step was to separate the eggshell from the egg interior. here, the at function could be reapplied, because the shell and the interior form two well-separated pixel subsets. the subsequent segmentation using the calculated threshold value resulted in a shell international journal of biomedical imaging example of image objects above (white) and below (green) the defined threshold for roundness; (d) parts of the animal bed (orange) were removed by retaining only the round (white) image objects from the previous step and reexpanding them by applying a stringent surface tension criterion to prevent expansion too far back into the animal be; (e) the animal bed was separated from the background and expanded along the z-axis through the coarse egg/bed (parts) (white) object; (f) the coarse egg (white) object was segmented using a fixed threshold and further smoothed by three consecutive expansion and reduction operations; (g) using an automatically calculated threshold, the refined egg object was segmented into shell (light blue) and interior (light red). the shell object was surrounded by two additional layers of pixels (shell border, red)."
"is unchanged during the iterations, we can accelerate the updating procedure by pre-calculating this value before the first iteration. the matrix is updated by taking a small step along the gradient direction for each iteration. to enforce the positive semi-definiteness, the updated matrix needs to be projected onto a feasible set. this projection is done by eigen-decomposition of the matrix and truncating all the negative eigenvalues to zeros. as the optimization problem (6) is convex, this solver is able to converge to the global optimum. we summarize the whole work flow in algorithm 1."
"in the first inner loop, a threshold was calculated, and the interior was segmented into temporary skeleton and temporary no skeleton 1 using that threshold. following segmentation, only resulting temporary skeleton image objects were further processed. if it was the first run of the outer loop, all temporary skeleton image objects with a relative border to shell border smaller than or equal to 0.1 were classified as skeleton. in all subsequent runs of the outer loop, a second condition was introduced; new temporary skeleton image objects also needed to share a relative border with existing skeleton objects in order to also be classified as skeleton."
"the last step of the egg detection step was to refine the remaining coarse egg object into its final shape. the object consisting of air and the actual egg was separated into these two components by applying another segmentation process that used a fixed intensity threshold (mean pixel value of the egg object + 500) on the unfiltered image layer. the resulting object was further smoothed into the final egg object using three expansion and reduction steps (i.e., grow and shrink operations) (figure 2(f) )."
"the accuracy of the trained model was evaluated using 10-fold cross-validation, whereby a set of instances (10 percent of the whole set) are systematically excluded from the training and subsequently used as test set. this is done 10 times with nonoverlapping test sets. to judge the quality of the svm per bone class, the f-measure gives a good overview, because it combines recall and precision rates [cit] ."
we demonstrate how automated image analysis and machine learning techniques can be combined to segment microct images and extract object information for the in ovo classification of bones in live chick embryos.
"whenever a wireless alert is detected, pan a ptz camera to that region and return its image. snbench not only eases the composition of such alert services, it also eases deployment by automating the re-use of existing computation/deployments to improve resource utility. all the examples given thus far share the same predicate logic and could share a single instantiation of that portion of the logic."
"noninvasive microct offers quantitative imaging with high spatial resolution as well as the possibility of repeatedly imaging and measuring the same chick embryo in ovo. the excellent bone contrast can be used to investigate bone-related questions, for example, bone formation (in conjunction with table s1 ."
"in parallel computations, one of the most difficult problems is the problem of optimal managing parallel processes. the effectiveness of parcs systems depends not only on its specific structure but also on its implementation on a specific platform, and on many other external factors such as network or hardware peculiarities."
"learning 1: the nfv orchestrator strongly benefits from a task-oriented implementation. to be in accordance with the etsi functional design for mano frameworks [cit], a subset of the sonata service platform implementation included the development of an nfv orchestrator (nfvo). the nfvo is the part of the mano framework that manages and orchestrates everything on the level of the network service, by implementing a range of workflows such as instantiating, scaling or terminating a service. figure 1 shows which sonata components are involved in the nfvo. the workflows are implemented by the service lifecycle manager (slm). looking at the service instantiation workflow, the slm uses the placement plugin to calculate the placement, the function lifecycle manager and infrastructure adapter to deploy vnfs and a storage component to save their records. due to the variety in management and orchestration requirements from services, a straight-forward implementation of these workflows quickly became complex. different services require different placement algorithms, different scaling solutions and in sonata, services might come with service specific managers (ssm), processes provided by the service developer that customise the slm workflow. implementing each workflow as a single process/thread overloaded the code with if-else clauses and boilerplate, and made extending them a complex task. this made the slm an inflexible object, contradicting the sonata requirement for a flexibility and extensibility. to this end, we re-factored the slm into a task-based engine. the overall functionality of the slm was chopped into basic tasks, with each task implemented as a seperate thread of control. workflows are then established by chaining a subset of these tasks together into an ordered schedule, in accordance with the mano requirements for the service. ssms can now customise a workflow by overwriting the functionality of one or more generic tasks, or by adding/removing tasks in the schedule. extending the functionality of a workflow (e.g. interacting with a newly added sp plugins) in the slm now comes down to implementing new tasks, and inserting them in the workflow schedule. the complexity of the code base reduced, making the slm more easily maintainable and extendable. this transition made the slm, and thus the nfvo, the flexible component sonata requires it to be."
"learning 3: the monitoring framework needs to be flexible and scalable. a service platform will collect information about many aspects of its performance -the individual elements of the physical and virtual infrastructure, the vnfs, the network services, and so on. thus, monitoring information is of interest to several parties in the ecosystem, including: the operator of the service platform (to gain a deeper understanding about their service); the end customer (to check that their service level agreement is being met); vendors (who are responsible for various vnfs); network service developers; machine learning specialists (to optimise algorithms that use monitoring data to automatically identify and isolate faults). these factors suggest that the monitoring framework should have two complementary features, it should be flexible and scalable."
"the ci/cd pipeline can be categorised, as shown on figure 2, in three phases: development, integration and qualification. in the development phase, the developer updates the code, creates the docker containers that package this code and expose them to the first line of tests, the unit tests, which runs in the developer's local environment and provide the first feedback. these tests are designed to verify the code in an isolated environment. in sonata, we opted to use github as source code management tool that uses the git version control protocol. once the code passes the unit tests, the developer can commit the code to the github repository and make a pull request which is a petition to the repository owner to add the new code to the master branch. at this point, the selected continuous integration environment, jenkins, kicks in. jenkins provides a range of functionalities that make it easier to set up an integration environment for the software, and to keep track of its quality. once a developer makes a pull request, jenkins will fetch this updated code, generate the new containers with the update and verify they pass the unit tests. once jenkins approves the update, the owner of the repository can accept the pull request and the code becomes part of the master branch. jenkins will also upload the updated containers to the sonata docker registry, making it available for the next phase. the development phase prevents code that fails the unit tests from becoming part of the integrated software, breaking it and blocking other developers."
"learning 4: security for a microservice-based architecture is best obtained with a token scheme. in the sonata service platform, the different modules/plugins are implemented as microservices. microservices are implemented by standalone processes which communicate through remote network calls, such as restful apis or message brokers. to secure this communication, we came to the conclusion that a session based authentication did not fit with requests between different microservices. to this end, the user management tool in the gatekeeper was enhanced to provide a token-based authentication mechanism (jwt [cit] ) for the service platform microservices, just like it does for the end-users. this way, we benefit from a simple and self-contained data way for authentication and authorisation, maintaining statelessness in the platform. tokens are compact, should be strong enough, fit well in apis and should be safely transported, to allow for a secured exchange of information between endpoints. they include digitally signed information that can be verified and trusted. the user management applies the same token mechanism to create user accounts for end-users and service accounts for internal microservices. this approach keeps workflows simple: they both register to the platform, login successfully to get a token which grants access for a certain amount of time. then, end-users and microservices can transmit information including the token, which is used by the user management to extract and evaluate the rights. the interaction with the user management server is minimised, avoiding overheads that session data or state would involve."
"for the uci-had dataset, we found that the unidirectional drnn model with four layers yields best performance results in terms of per-class precision and recall, as shown in figure 8a . the overall classification accuracy is 96.7%, outperforming other methods, such as cnns [cit], support vector machine (svm) [cit], and sequential extreme learning machine (elm) [cit] . figure 8b"
"first, the hidden state h 0 and internal state c 0 of every layer are initialized to zeros. the first layer uses the input sample x t at time t, previous hidden state h 1 t−1, and previous internal hidden state c 1 t−1 to generate the first layer output y 1 t given its parameter θ 1 as follows:"
"traditional sequential programming languages, operating systems, and computer architectures are mainly conformed to each other. therefore, the majority of programs get the mobility property. as far as parallel computations are concerned, positive solutions for conformity could be achieved only for a limited number of local areas. in parallel programming, for the same task an algorithm could be effective in one parallel environment and completely inefficient in another one. meanwhile, the rapid advance of information technologies demands the need to create convenient tools for parallel programming and related technologies. the terms \"parallel\" and \"concurrent\" are increasingly used in relation to architecture of computing systems, operating systems, algorithms, programming languages, data structures, and databases. against this background, the development of descriptive tools well-suited for logical level of algorithmic concurrency is of great importance."
"a memory cell contains more parameters and gate units, as shown in figure 2 . these gates control when to forget previous hidden states and when to update states with new information. the function of each cell component is as follows:"
"as it is though to detect such design issues before an integration cycle and the cost in terms of time and effort of fixing such issues late in the development process is high, we feel that our ci/cd methodology allowed us i) to keep a clear and complete view of the status of the project at all times and ii) to meet software delivery deadlines in an environment of limited resources. while it gives no guarantees about the quality of the software, our methodology makes us confident about its stability and reliability, since it endured numerous cycles in the integration and qualification environment before it was released."
"programming parcs-tools are intended to describe the interaction and recursive-parallel development of the processes which are built on the base of the basic language. low level parcs-tools can be configured for single processor (for simulation) or multiprocessor computers (computer networks, multi-machine complexes, cloud computing, etc.), where message exchange is used to support connections between parallel processes or some other mechanism providing the process synchronization and communication."
"in fact, the use of agent paradigm in our proposal is basically argued by the need for autonomy to initiate the synchronization process of web service bindings between business repositories without the direct intervention of service providers. previously, wooldridge has defined a software agent as \"a computer system that is situated in some environment, and that is capable of autonomous actions in this environment in order to meet its delegated objectives\" [cit] . in our case, and according to this definition, the environment that hosts software agents is the lrbf framework and precisely its underlying business repositories whereas the autonomous action to carry out by these agents in this environment is to pick up newly updated service bindings actively from web service provider, and then to incorporate such updates in business repositories (pbrs, pwsr and lwsrs). accordingly, the delegated objective to meet by software agents in such environment is to maintain bindings of web services that are saved locally in lwsrs coherent and synchronized with original ones available in pbrs and thereby to ensure a reliable local binding of web services whenever they are changed."
we introduce models that are able to classify variable-length windows of human activities. this is accomplished by utilizing rnn's capacity to read variable-length sequences of input samples and merge the prediction for each sample into a single prediction for the entire window segment.
"learning 5: the concept of network slicing is still not clear. within the 5g concept a large amount of effort is focusing on architectures that support network slicing, implying evolution from the network sharing models towards network isolation, multi-tenancy and end-to-end resource provisioning and guarantees. in sonata, slicing is considered at the lower service platform layer via the mano framework, which leverages the ia and slice management components, and a distributed monitoring framework. finally, issues related to service provider peering and recursive operations of the service platform (where slicing is also employed) is tackled by the gatekeeper. in this context, sonata considers an sdn capable wan, managed by a wan infrastructure manager (wim) that supports slicing (by means of provisioning of isolated multi-tenant networks) plus an openstack based vim integrated with an sdn controller (i.e. opendaylight) for the physical and network elements within the nfv infrastructure. in this view the sonata service platform is able to create per domain slices that are interconnected constituting an endto-end isolated network and computing resource slices."
"a couple of issues concern capabilities for discovery and addressing. the upper-sp needs to learn what capabilities can be provided by potential lower-sps. our current thinking is that this is best done by the lower-sp publishing the capabilities it can offer (similar to today's suppliers' information notes about network services [cit] ), instead of a query protocol for instance. on addressing, we need to ensure that packets can flow along a service function chain that spans the sps. at the moment, we think the most likely approach is that the upper-sp tells the lower-sp constraints, so that the lower-sp makes a good choice about the virtual link address and port identifier that it uses for the vnf(s) it supplies."
"learning 11: maintenance and updating of the ci/cd pipeline takes priority over code development. the success of the used ci/cd pipeline is directly correlated with its care and enforcement. in the sonata project, a significant subset of the software developers was also responsible for the maintenance of the integration and qualification environment, causing the description and updating of new tests to slack at times. when the outcome of integration tests is ignored or they are not updated when new features appear in the software, the devops feedback loop is lost causing integration issues to appear in a later stage of the development. it is therefore of the utmost importance that the ci/cd pipeline is strictly followed and enforced, and gets prioritised over code development during every stage of the project."
"the opportunity dataset is very complex and contains a wide range of activities. therefore, the bidirectional drnn model with three layers yields the best performance results. the confusion matrix in figure 10a summarizes the classification results of the proposed model for the test set, along with the per-class precision and recall results. the proposed method outperforms other methods, such as those based on deep believe networks (dbns) [cit], svm [cit], and cnns [cit] . it also outperformed the state-of-the-art method, which is a combination of cnns and unidirectional rnns [cit], for the opportunity dataset. figure 10b presents a performance comparison between the f1 score of the proposed method and those reported by other methods. we used the f1 score as a basis for comparison because the opportunity dataset is imbalanced, manifested by the dominance of the null class."
"implementing, integrating and deploying the sonata nfv platform proves to be a moving target. this is the result of the proliferation of nfv concepts, architectures, technologies, platforms, and standards in flux. the sonata approach is tackling this challenge through the development of an sdk and a mano platform using a microservice design and a devops approach involving continuous development 1 https://sonata-nfv.github.io/ and integration. however, to truly advance the area of nfv platforms, experiences of this undertaking provide a couple of recommendations. a range of nfv-related concepts and models need harmonisation. we argue that the service programming model and descriptor formats need to be unified in the community, reducing the dependency on existing cloud models. the development and adoption of nfv technology would drastically benefit from such a common model, together with openly available network functions and services, as well as commonly agreed functional and non-functional benchmarks. this would allow to overcome the situation where most nfv platforms provide only support for simplistic nfv services, or rehashing existing cloud functionality without any telecom-specific performance guarantees. next, as performance and overhead are of extreme importance when focusing on telecom-specific packet processing tasks on the nfv execution platforms, the community needs increased focus on enhancing vim technology to provide native network control for the interconnection of container instances. in addition, the sonata project confirms that nfv mano architectures can be made significantly more reliable, and scalable, when following a microservice architecture, as well as including a task-based nfvo. scalability and autonomy in orchestration is drastically improved when supporting hierarchic nfvo setups. also, when focusing on the software development and integration methodology, a continuous development and integration strategy has the potential to significantly increase the stability, reliability and thus operational readiness of an nfv platform, provided that adequate (re-)education and reinforcement on this approach is enforced on a regular basis."
"a new qualification phase is triggered every time all the integration tests succeed. at this point, all the docker containers are promoted to our qualification environment, which is guaranteed to have a correctly integrated version of the platform. therefore, it can be used to evaluate the quality of the service platform, e.g. by exposing it to performance, security or stability tests. these tests show us how many requests the platform can manage at the same time, how long it takes to satisfy requests, how the platform behaves when used over a longer period of time, etc. the qualification tests emulate the exposure of the service platform in an operational environment, creating another devops feedback loop to the developer. like the integration phase, jenkins organises the qualification phase by automatically deploying the qualification version of the platform and scheduling the tests. due to its stability, the qualification environment can also be used for demo purposes or to let possible end-users test the service platform."
"as the different modules in the service platform are implemented as microservices, we opted to use docker containers to package each module. the developer defines a docker descriptor that contains the base image of the programming language used for the module, a path to the code that is going to be included inside the container, and all the dependencies needed for the code. based on this descriptor, the docker engine generates the docker container, which can be deployed on any device that is hosting the docker engine. by having the entire sonata service platform code base implemented in such docker containers, deploying the platform becomes real easy, quick, flexible and platform independent."
"the sonata nfv service platform provides the virtualisation infrastructure, as well as the management and orchestration functionality to deploy nfv services. the sonata sp is closely modeled after the etsi nfv model. next to the platform itself, sonata provides a set of development tools (sdk) to assist the developer in developing and packaging 978-1-5386-3416-5/18/$31.00 [cit] ieee nfv services. this section is dedicated to insights gained over the course of implementing the sonata service platform, which are applicable to the design and implementation of general flexible nfv orchestration platforms."
parcscontroller calls methods hostserver api. modulecontroller-starts a new module with given parameters. logcontroller-returns the log-file of the systems work. accountcontroller-is responsible for the user authentication.
"as web services proliferate, the ability to find and bind services of interest across heterogeneous environments becomes a major challenge and raises several concerns such as reliability, availability and performance. solving these problems is key to the ultimate success of web services and the solutions become of paramount importance if complex integration models, such as the service-oriented architecture (soa) [cit], are to be successfully deployed."
"given these facts and according to the infrastructure of the lrbf [cit], the architecture of the proposed model for binding synchronization of web services, shown in fig 1, involves three participating software agents such as public web service synchronizer agent (pwssa), universal web service synchronizer agent (uwssa) and local web service synchronizer agent (lwssa). each agent interacts with one particular business repository according to the tasks to be carried out on this repository. indeed, upon receiving new changes in service binding details from the service provider the pwssa proceeds to save the updated data in the pbr and then notify the uwssa to receive such changes. as soon as the latter agent accepts the reception of changes the pwssa sends a message incorporating the updated data to the agent message queue (amq) of the uwssa. whenever a new message is posted in its amq the receiver agent picks it up first and then stores the included data in the pwsr. to forward these updates to the local repository of service consumer, the lwssa periodically requests the uwssa the new updated data being incorporated in the pwsr. when asked to provide such updates, the uwssa checks if the requested data is in its amq and, in this case, replies the requestor agent with a message that includes the updated binding details. upon receiving this message, the lwssa incorporates the included data in the lwsr. the architecture of the proposed model is shown in fig 1. fig. 1 . architecture of the agent-based synchronization model as depicted in the above figure, the underlying components of the proposed model involve three software agents (pwssa, uwssa and lwssa), two participating roles (web service provider and web service consumer) and three business repositories (pbr, pwsr and lwsr). as we have mentioned previously, each agent interacts with one particular repository to carry out tasks on that repository in response to new updates of service bindings from the web service provider. furthermore, agents interact with each other to synchronize such updates between business repositories in order to ensure that pbrs, pwsr and lwsrs see all the changes that have originated by the service provider and thereby the local binding inquiries from lwsrs will yield results consistent to those made at pwsr or pbrs."
"where w h, u h, and w y are the weight for the hidden-to-hidden recurrent connection, input-to-hidden connection, and hidden-to-output connection, respectively. b h and b y are bias terms for the hidden and output states, respectively. additionally, there is an activation function f associated with each node. this is an element-wise non-linearity function, commonly chosen from various existing functions, such as the sigmoid, hyperbolic tangent, or rectified linear unit (relu). schematic diagram of an rnn node where h t−1 is the previous hidden state, x t is the current input sample, h t is the current hidden state, y t is the current output, and f is the activation function."
"the main difficulty was to get the remote connection to the first machine and to start deployment of the site restapi on it. to solve this problem, we used the afterwards, other virtual machines should be created, the program daemon should be run on them, and all ip-addresses should be registered in the configuration file hostserver. while creating virtual machines, it is important to join them to a single virtual network that allows them to communicate with each other. also, while creating a machine, one should configure the endpoints that are ports to access the machine."
pbrs hold binding details of services provided by the pbrs' owners namely the service providers; pwsr holds binding details of web services available in all pbrs; lwsrs hold binding details of only the frequently used web services required by lwsrs' owners namely the service consumers.
"the key components for the deployment of this architecture are a service provider, a service consumer and a service registry. to find the service they need, service consumers search the service registry where service providers publish the services they offer. currently, web services are the suitable solution to develop soa-based products."
"from the object-oriented programming point of view the functioning of cs is performed by objects and operations. in this paper, we use definitions at the level of specifications for abstract data types. the levels of representation and implementation of these data types depend on specific implementations of parcs programming systems."
"we used an optimization algorithm called adam that minimizes the cost function by backpropagating its gradient and updating model parameters [cit] . training was conducted on a gpu-based tensorflow framework in order to utilize the parallel computation power of a gpu [cit] . the dropout technique was used to avoid overfitting in our model [cit] . although dropout is typically applied to all nodes in a network, we followed the convention of applying dropout to the connections between layers (not on recurrent-connections or intra-cell connections). the probability of dropping a node during a training iteration is determined by the dropout probability p, which is a hyperparameter tuned during training and represents the percentage of units to drop. adopting dropout regularization technique led to a significant improvement in performance by preventing overfitting. figure 7 presents the accuracy and cost of training and testing processes for the unidirectional drnn model using the usc-had dataset. the gap between training and testing accuracies, as well as the gap between training and testing costs is very small. this indicates that the dropout technique is very effective at forcing the model to generalize and be resilient to overfitting."
"the application of deep learning for har has led to significant enhancements in recognition accuracy by overcoming many of the obstacles encountered by traditional machine learning methods. it provides a data-driven approach for learning efficient discriminative features from raw data, resulting in a hierarchy from low-level features to high-level abstractions. the strength of deep learning lies in its ability to automatically extract features in a task dependent manner. it avoids reliance on heuristic hand-crafted features and scales better for more complex behavior-recognition tasks."
"we found that the cascaded drnn model yields the best results for the skoda dataset. the model is built using one bidirectional layer and two upper unidirectional layers. figure 12a presents the classification results for the test set in the form of a confusion matrix, along with the per-class recall and precision results. the proposed method results in an overall accuracy of 92.6%, outperforming other methods such as hmms [cit], dbns [cit], and cnns [cit], as shown in figure 12b ."
"we have presented three novel lstm-based drnn architectures for har tasks. additionally, we empirically evaluated our models by conducting experiments on four miscellaneous benchmark datasets. experimental results reveal that the proposed models outperform other state-of-the-art methods. the reason for this improvement in performance is that our models are able to extract more discriminative features by using deep layers in a task-dependent and end-to-end fashion. furthermore, our models are able to capture the temporal dependencies between input samples in activity sequences by exploiting drnn functionality. future work includes experimentation on large-scale and complex human activities, as well as exploring transfer learning between diverse datasets. investigating resource efficient implementation of a drnn for low-power devices is also a promising future research direction."
"the parcs-environment is provided by the extension of the basic algorithmic language due to the operations with the cs (level of logical structure). further, using the parcs-model, a chosen base algorithmic language, and an implementation platform the corresponding parcs-technology could be created."
"pwssa behaviors: as mentioned previously, the pwssa waits for requests from the service provider to update the pbr with new service binding details. a possible design to achieve that is to make the pwssa execute a one-shot behavior updating the pbr whenever the service provider change service bindings. moreover, we need to make this agent execute another one-shot behavior forwarding updates to the uwssa. uwssa behaviors: upon receiving new updates from the pwssa, the uwssa proceeds to save them into the pwsr first and then waits for requests from the lwssa to serve them. a possible design to achieve this is to make uwssa execute two cyclic behaviors. one dedicated to serve requests for updates reception and the other dedicated to serve requests for updates delivery. lwssa behaviors: as described above, to forward updates of service bindings to the local repository of service consumer, the lwssa periodically requests the uwssa the new updated data being incorporated in the pwsr."
"the second model architecture is built by using a bidirectional lstm-based drnn, as shown in figure 5 . it includes two parallel lstm tracks: forward and backward loops for exploiting context from the past and future of a specific time step in order to predict its label [cit] . in the first layer, the forward track (lstm f 1 ) reads the input window t from left to right, whereas the backward track (lstm b1 ) reads the input from right to left according to:"
"the first one is to build-in basic parallel programming features like in languages c++, java, c#, python. the second approach relates to constructing special addons over standard procedural languages: mpi, openmp, cuda, opencl, javacl."
"flexibility is needed as different parties will be interested in different information. e.g, an operator needs real time information about all the different components; a customer is only interested in summary information. so the framework needs to allow the interested party to describe what they want, and for the owner of that information to approve, adjust or reject the request. in our implementation, the monitoring framework collects and processes data from several sources (vms, docker containers, opendaylight controllers, openstack api, etc.), providing the interested parties the ability to activate metrics and thresholds to capture infrastructure or service-specific performance data. the user can define rules based on metrics gathered from multiple vnfs deployed in one or more nfvis. in general, the user can subscribe to a message queue to get the real time alert notifications and monitoring data, request them through a restful api or directly access them through a web socket url. scalability is needed because there is potentially a vast amount of monitoring information, so it cannot all be communicated or stored in full. approaches that help scalability include: thresholds (only report a metric when its value exceeds some value); averaging, filtering and other aggregation techniques (e.g. averaging information over some time period); creating tailored alerts (for instance, so a help desk can be pre-warned that there is a problem affecting a service); and an emergency button so the service/network manager can quickly reduce to 'skeleton monitoring' (e.g. if some failure means that monitoring data suddenly consumes a significant fraction of resources). to address scalability, several monitoring components in our implementation had to be distributed across nfvis. first, each nfvi needs its own web socket server to accommodate users requests for streaming monitoring data. second, monitoring servers follow a federated architecture. the local servers collect and store metric data from the vnfs deployed in the nfvi, while only the alerts are sent to the federated level for further processing/forwarding to the user. third, alerting rules and notifications can be based on monitoring data from multiple nfvis and thus should be evaluated on a federated level. to enable 'skeleton monitoring', the design of the monitoring probe allows dynamic modification so that in cases where the difference of a monitored metric is below a threshold, it will not be sent to the monitoring server."
"to convey such updates to the pwsr where service consumers can get new bindings of the email validation web service, the pwssa sends an updates message to the uwssa. this message includes in addition to the sender and receiver identifiers and the communication intention which is in this case a cfp (call for proposal) performative, it includes also the new value of the access point (update_value) along with a set of additional parameters, as depicted in table 1, such as service provider identifier (uuid), updates register (usn), updates form (update_form) and so forth. upon receiving the updates message within its amq, the uwssa checks whether this message with both the uuid and the usn values has been already received. depending on the result of this checking, this agent may accept the message and save updates in the pwsr or refuse it. for our example, we assume that the updates message is first time received by the uwssa and therefore this agent will accept it and store the new service access point in the pwsr while waiting for requests from the lwssa to serve them."
"in the vision of a world in which modern enterprises have to accommodate business agility in today's ever more competitive and global markets, web services have emerged as a prominent technology to address this critical issue. indeed, enterprises have become able to expose their existing business processes as web services that can be easily assembled and interact in a highly standardized manner. due to this fact, an increasing number of web services are started to be deployed during the last few years."
"having a ticker behavior the lwssa periodically requests, on each tick (e.g. every second), new updates from the uwssa. upon receiving new requests the latter agent servers them by sending the latest received updates message from his amq to the lwssa. given the fact that these updates may not interest the lwssa, this agent may accept or refuse them. that is, if the local repository of service consumer already contains the uuid key (update_key) of the updated access point, therefore the new updates will be accepted and the new address of the web service will replace the failed one, otherwise the updates will be rejected. finally, by saving the new service access point in the lwsr the service consumer may bind locally the email validation web service from the new recovery site of the service provider."
"conceptually, data transmission over the channels is described by the following set of functions given in table 2 . table 2 . communication functions of parcs."
"in the next years, 5g infrastructure will become a ubiquitous, flexible, broadband and programmable network that will be in the core of every social, business and cultural process, enabling both economic growth and social prosperity. however, the 5g vision poses significant technical challenges that must be fulfilled, including the concept of agile programmability and the introduction of management mechanisms for the efficient instantiation of network services across heterogeneous network components, virtualised infrastructures and geographically dispersed cloud environments."
"to ensure reliable binding synchronization mechanism between business repositories, software agents have to carry out suitable behaviors in response to new requirements of their environment. typically, we can distinguish among three basic types of behavior such as \"one-shot\" behaviors that complete immediately, \"cyclic\" behaviors that never complete and \"generic\" behaviors the complete when a given condition is met [cit] . furthermore, two additional behaviors may be carried out at given points in time namely \"waker\" behaviors that complete after a given timeout expires, and the \"ticker\" behaviors that never complete waiting a given period after each execution. having described the basic types of agent behavior, we move now to analyze which behaviors have to be carried out by pwssa, uwssa and lwssa respectively:"
"due to these facts and to optimize the binding of web services, we have proposed in a previous work a local repository-based framework (lrbf) [cit] that provides users with a local access point to bind web services of interest instead of going through heterogeneous environments (business registries, service portals, search engines, etc.). this framework enables the collection of binding details (wsdl files, binding protocols, endpoints, service operations, functional parameters, etc.) for the most frequently used web services in local web service repositories (lwsrs) where users can bind locally these services in subsequent uses."
"in order to train and evaluate the proposed models, we considered five public benchmark datasets for har. the datasets contain diverse movement data, captured by on-body sensors. they contain various activities performed in different environments and are used to validate the applicability and generalization of our models for a large variety of activity recognition tasks. table 1 summarizes the experimental datasets and the following are brief descriptions of them:"
"the performance results of the proposed models clearly demonstrate that drnns are very effective for har. all of the architectures performed very well on all of the datasets. these datasets are diverse, which proves that our models are effective for a broad range of activity recognition tasks. the unidirectional drnn model yielded the best results for the uci-had and usc-had datasets, the bidirectional drnn model gave better results for the opportunity dataset, and the cascaded drnn model performed better on the daphnet fog and skoda dataset. table 2 contains a performance summary for the four datasets. there are two main reasons for the superb performance of the proposed models for har tasks. first, including sufficient deep layers enabled the models to extract effective discriminative features. these features are exploited to distinguish between classified activities and scale up for more complex behavior recognitions tasks. second, employing drnns to capture sequential and time dependencies between input data samples provided a significant improvement in performance compared to other methods."
"we have developed and tested procedures of the parcs deployment on the amazon ec2 and microsoft azure platforms [cit] . herein, we briefly describe the procedure of the parcs deployment on the microsoft azure platform."
"(15) figure 6 . cascaded unidirectional and bidirectional lstm-based drnn model. the first layer is bidirectional, whereas the upper layers are unidirectional. the number of hidden unidirectional layers is a hyperparameter that is tuned during training."
"the training process of lstm-rnns is essentially focused on learning the parameters b, u, and w of the cell gates, as shown in equations (3)- (6). figure 2 . schematic of lstm cell structure with an internal recurrence c t and an outer recurrence h t . cell gates are the input gate i t, input modulation gate g t, forget gate f t, and output gate o t . in contrast to an rnn node, the current output y t is considered equal to current hidden state h t ."
"returns the number of points which were linked by channels with the point p; the second parameter is used to return these points. // create points and channels-basic elements of the cs. using parcs system we can simulate real computation tasks by constructing different dynamic or static cs configurations (tree, circle, cube etc.)."
"an rnn is neural network architecture that contains cyclic connections, which enable it to learn the temporal dynamics of sequential data. a hidden layer in an rnn contains multiple nodes. as shown in figure 1, each node has a function for generating the current hidden state h t and output y t by using its current input x t and the previous hidden state h t−1 according to the following equations:"
"uci-had [cit] : dataset for activities of daily living (adl) recorded by using a waist-mounted smartphone with an embedded 3-axis accelerometer, gyroscope, and magnetometer. all nine channels from the 3-axis sensors are used as inputs for our drnn model at every time step. this dataset contains only six classes: walking, ascending stairs, descending stairs, sitting, standing, and laying. 2) usc-had [cit] : dataset collected by using a high performance imu (3d accelerometer and gyroscope) sensor positioned on volunteers' front right hips. the dataset contains 12 basic human activities: walking forward, walking left, walking right, walking upstairs, walking downstairs, running forward, jumping up, sitting, standing, sleeping, in elevator up, and in elevator down. we considered 11 classes by combining the last two activities into a single \"in elevator\" activity. the reason for this combination is that the model is unable to differentiate between the two classes using only a single imu sensor. additional barometer readings are required to determine height changes in an elevator and discriminate between the two classes (up or down in elevator). 3) opportunity [cit] : dataset comprised of adl recorded in a sensor-rich environment. we consider only recordings from on-body sensors, which are seven imus and 12 3d-accelerometers placed on various body parts. there are 18 activity classes: opening and closing two types of doors, opening and closing three drawers at different heights, opening and closing a fridge, opening and closing a dishwasher, cleaning a table, drinking from a cup, toggling a switch, and a null-class for any non-relevant actions. 4) daphnet fog [cit] : dataset containing movement data from patients with parkinson's disease (pd) who suffer from freezing of gait (fog) symptoms. the dataset was built using three 3d-accelerometers attached to the shank, thigh, and lower back of the patients. two classes (freeze and normal) were considered depending on whether or not the gait of a patient was frozen when the sample was recorded. we used this dataset to train our model to detect fog episodes in pd patients and prove the suitability of our model for gait analysis using only wearable sensors. 5) skoda [cit] : dataset containing activities of an employee in a car maintenance scenario."
we created the project restapi which allows us to obtain information about the current system state and to fulfill operations such as starting a new am or task cancellation. the project was developed using the asp. net web api technology.
"the remainder of this paper is organized as follows: section 2 provides a brief review of related works employing deep learning for har and section 3 presents a background overview of rnns and lstm. the proposed models and experimental setup are explained in sections 4 and 5, respectively. performance results and comparisons are presented in section 6. finally, discussion and analysis are presented in section 7."
"human activity recognition (har) has recently attracted increased attention from both researchers and industry with the goal of advancing ubiquitous computing and human computer interactions. it has many real-world applications, ranging from healthcare to personal fitness, gaming, tactical military applications, and indoor navigation. there are two major types of har: systems that use wearable sensors and systems that use external devices, such as cameras and wireless rf modules. in sensor-based har, wearable sensors are attached to a human body and the human activity is translated into specific sensor signal patterns that can be segmented and identified."
"in this paper, we describe insights and gained experiences from developing and integrating sonata, a flexible and extendable nfv service platform. this comparison of theory and practice serves as aid for those considering the implementation of a plugin-based mano framework or the adoption of an agile ci/cd methodology for a development project in the telco environment. section ii focuses on the experiences gained from developing the service platform, while in section iii, we detail our ci/cd methodology and the insights we gained from setting it up and using it. finally, section iv concludes the paper and provides some general recommendations for developing and integrating nfv service platforms."
"figure 5. bidirectional lstm-based drnn model consisting of an input layer, multiple hidden layers, and an output layer. every layer has a forward lstm f and a backward lstm b track, and the number of hidden layers is a hyperparameter that is tuned during training."
"in the field of deep learning, there is a growing interest in recurrent neural networks (rnns), which have been used for many sequence modeling tasks. they have achieved promising performance enhancements in many technical applications, such as speech recognition [cit], language modeling [cit], video processing [cit], and many other sequence labeling tasks [cit] . the rationale behind their effectiveness for sequence-based tasks is their ability to exploit contextual information and learn the temporal dependencies in variable-length input data."
"the main purpose of the control transfer is to modify the control part of an active am-copy as a result of interaction with other active am-copies. in particular, am could be assigned to an \"empty\" point."
"gorithm (system) and to represent its dynamic changes. cs consists of points and channels that connect points. it may have the hierarchical structure. a new cs, according to cs structural recursion, can be generated from a cs point."
"in fact as mentioned previously to optimize the binding of such web service, service consumers need to save its binding details locally within lwsrs for subsequent uses. among these details we can distinguish; service access points, binding protocols, service operations, functional and non-functional parameters, service description and so forth. one of the most important binding details from the list above is the access point which enables service consumer to bind web services from his lwsr without the need to access neither the service provider registry nor the pwsr. the access point is a network address, typically a url, suitable for invoking the web service."
"however, the local binding of web services presents an inherent weakness that threatens the performance of the lrbf framework. indeed, as most web services are unstable (updated, removed, replaced, unavailable, etc.), thereby their binding details saved locally within lwsrs risk to be outdated and inaccurate. to maintain reliable binding of web services in such environment, we propose in this paper an extension to lrbf with an agent-based model for synchronizing web service bindings. this model enables the update of locally saved binding details for web services whenever they are changed."
"am sets up a sequential algorithm, either its control or data. points can be connected by channels, which transmit information. the system of points and channels forms cs of the process and specifies its structural organization. points and channels are defined as built-in data types. cs can dynamically change its configuration while an algorithm is running. recursive calls of processes are permitted. such calls could be initialized by any am localized inside the calling process."
another important feature of this system is its ability to simultaneously perform multiple tasks by different users. if there is no enough capacity the system will wait until processors become free and only then it will run new tasks.
"this section is dedicated to the continuous integration and delivery methodology that was applied during the development of the sonata service platform. this way of working aims to improve the quality of the software, reduce the timeto-market of the product and streamline integration actions between different modules of the software. it allows each developer to publish software updates multiple times a day, while assuring that the code integrity is never compromised. to achieve this, our methodology combines a set of software tools with multiple layers of testing to guarantee that each contribution is validated before it enters the master branch of the software. these layers of testing emulate an operational deployment of the service platform, creating a devops cycle that provides developers with fast feedback on how the service platform performs in an operational environment."
"over the last decade, business registries have been the major building blocks required for successful web services. indeed, the uddi business registry (ubr) has emerged as a universal brokerage system that service providers can use to advertise the services they offer whereas service consumers can use to discover services that fit their requirements. however, as web services proliferate size and magnitude of ubr substantially increase, which may threaten its usefulness. furthermore, the development of new operating systems, server applications and apis equipped with built-in functionalities and tools has allowed businesses and companies to create their own public business registries (pbrs) where to publish their home-grown web services."
"as we can deduce from the description above, web service bindings are saved twice. the first time is from pbrs to pwsr for all web services and the second time is from pwsr to lwsrs for only the frequently used web services. therefore, to optimize the binding of web services within the existing framework three categories of business repositories are needed:"
"the performance results of our proposed models are presented in this section. the results are compared to other previously introduced methods, which are tested on the same datasets."
"in the integration phase, the containers of all the modules are deployed and exposed to the whole ecosystem of developers and admins as an entire service platform. this allows each developer to test and validate how their module integrates with the sp. the different containers are exposed to a range of integration tests that validate whether they are integrated correctly. these tests expose mismatches in apis between modules and validate whether the integrated behaviour matches the expected one. integration tests are automatically triggered when docker containers are updated, others are executed every day at midnight. this tight schedule is in accordance with our devops approach. by checking the influence a code update has on the overall integration almost immediately, developers quickly receive feedback on their change. this allows for quicker iterations of the code and a faster progress of the overall functionality."
"y where θ represents the parameters (b, u, w) of the lstm cells for layer, as shown in equations (3)- (6) . any layer in the upper layers uses the output of the lower layer y −1 t as its input:"
"inspired by common best practices for software development within large, distributed teams, the sonata service platform was developed following a continuous integration and delivery (ci/cd) methodology [cit] . such methodologies focus on improving the software quality, while decreasing development time and the gap between developers and the operational deployment of the product. by leveraging tools such as github [cit], jenkins [cit] and docker [cit], and by implementing multiple verificiation layers, sonata created a ci/cd pipeline that allowed developers to frequently update and publish their code to a whole ecosystem of developers and admins, while preventing code updates that break the integration from blocking others. due to a devops [cit] approach, developers quickly receive feedback on their changes, allowing for a more agile development and increasing the frequency of software iterations."
"where tp c is the true positive rate of a class c, f p c is the false positive rate, and c is the number of classes in the dataset. 2) recall (sensitivity): measures the number correctly classified samples out of the total samples of a class. the overall recall is the average of the recalls for each class:"
"traditional sequential programming languages, operating systems, and computer architectures are mainly conformed to each other. therefore, the majority of programs get the mobility property. as far as parallel computations are concerned, positive solutions for conformity could be achieved only for a limited number of local areas. in parallel programming, for the same task an algorithm could be effective in one parallel environment and completely inefficient in another one. meanwhile, the rapid advance of information technologies demands the need to create convenient tools for parallel programming and related technologies. the terms \"parallel\" and \"concurrent\" are increasingly used in relation to architecture of computing systems, operating systems, algorithms, programming languages, data structures, and databases. against this background, the development of descriptive tools well-suited for logical level of algorithmic concurrency is of great importance."
"the first one is to build-in basic parallel programming features into a base language like in languages c++, java, c#, python. the second approach relates to constructing special add-ons over standard procedural languages: mpi, openmp, cuda, opencl, javacl. we propose parcs (parallel asynchronous recursive control system) as a universal add-on extension to the base programming languages."
"the rest of the paper is organized as follows. section 2 outlines the web services synchronization issue related to the architecture of the existing framework. in section 3, we describe the proposed agent-based model for synchronizing web service bindings. prior to the conclusion in section 5, section 4 reviews a usage scenario."
"however, if the service provider has activated a disaster recovery site, by activating new access point for the email validation web service, most of the local binding calls from service consumers will fail when they try to invoke the web service at the failed site. therefore, based on our proposed synchronization model and by updating the service access point the pwssa will automatically update the pbr of the service provider with the new address (e.g. https://tempuri.com/validateemail.asmx)."
"the call execution launches the process of creating a subspace of cs subordinated to the point, where the call occurs. the corresponding starting ams are assigned to the points of this subspace."
"therefore, to achieve this task we need to make the lwssa execute a ticker behavior in which, on each tick, requests new updates from the uwssa. in fact behaviors of software agents are carried out upon receiving messages from other agents. indeed, the format of messages exchanged between agents as specified by the acl language [cit] comprises a number of fields such as the sender of the message, the list of receivers, the communication intention (also called \"performative\") and so forth. for the need of our synchronization model we extend this format with new fields. the description of these fields is shown in table 1 . according to the form of updates (update_form parameter) some parameters of the updates message might be omitted when their value are useless. for example, if the service provider hides or deletes the access point of a particular web service, therefore the update_value parameter is not required in the message. furthermore, some parameters are used in conjunction with other parameters to prevent software agents of some behaviors. for instance, if the pwssa sends twice to the uwssa an updates message with the same uuid and usn values therefore the latter agent will not accept this message as it was already received. further details about the processing of updates message by software agents are provided in the next section."
"learning 6: service platforms cooperate with a hierarchical, recursive architecture. often delivery of a service to a customer will need the involvement of more than one service platform. for example, the customer may want the service in multiple geographical locations, and no operator is present in them all. another example is where some operators specialise in end-customer-facing operations, whilst others specialise in the \"wholesale\" provision of infrastructure, or in providing specific types of vnf."
"we believe that cooperating service platforms should be organised in a hierarchical architecture, meaning that an \"upper-sp\" provides the end-to-end service to the customer, and it chooses to involve a \"lower-sp\" to deliver part of the required capability (in other words, they have a \"north-south\" rather than \"east-west\" relationship). from the customer's perspective, they only interact with, and know about, the upper-sp; from the upper-sp's perspective, the lower-sp is providing a component in their overall network service in a similar manner to the nfvi; and as far as the lower-sp is concerned, the upper-sp is just another customer requesting a service. further, we believe that the architecture should be recursive, meaning that the lower-sp can in turn arrange for some of the service it provides to be delivered by a yetlower-sp (and so on). the advantages of such an approach are commercial and technical: it has clear lines of responsibility, allows autonomy and flexibility in service provision (e.g. different sps could use different orchestrators), and only a single, standardised \"north-south\" api is needed."
"another conceptual feature of parcs is an algorithmic module (am), which is a program (procedure) in a base language extended by special commands for interacting with cs. thus, cs can be viewed as a skeleton of a parallel process carrying executional ams."
"learning 10: there is no clear benchmark that indicates when nfv service platforms are ready for operational deployment. due to our ci/cd pipeline, we feel confident about the stability and reliability of sonata's latest 3.0 release 1 . the service platform has been vetted by a range of integration and qualification tests over a significant period of time before the software was released. it is however a non-trivial task to determine when the platform is ready for an operational roll-out, i.e. which performance, stability and reliability tests constitute a good enough test base. such requirements should entail a range of open-access network services and vnfs that can be used for testing and bench marking, which go beyond minimal test setups with a single pop, a service with two vnfs, few chaining and no scaling. such requirements can create an environment where mano framework performance can be compared against other frameworks and against operational readiness."
another important part of the parcs-conception is the notion of an algorithmic module. a program in parcs consists of a sequence of processes. each process is controlled by a system of ams assigned to some abstract points of cs.
"the parcs model is defined as follows: a) a particular set of basic algorithmic modules (am) is defined; b) during the model functioning it is permitted: creating (perhaps recursively) active copies of am, creating am with \"mutational\" changes which are determined by the current situation; c) the model functioning consists of the creation and destruction of am active copies, their partially-decentralized asynchronous functioning and dynamic interaction."
"the widespread use and availability of sensing technologies is generating an ever-growing amount of data, which along with enhanced computation power have contributed to more feasible applications of deep learning methods. these methods can be utilized to extract valuable contextual information from physical activities in an unconstrained environment. furthermore, many researchers have employed deep learning approaches to build har models in an end-to-end fashion, thereby achieving superior performance compared to previous conventional methods. this strategy has been effective in handling more complex human activities and taking advantage of the proliferating data."
"early work on using deep learning methods in har was based on deep belief networks (dbns) [cit], which were built by stacking multiple layers of restricted boltzmann machine (rbm). subsequent dbn-based models exploited the intrinsic temporal sequences in human activities by implementing hidden markov models (hmms) above the rbm layers [cit] . they performed an unsupervised pre-training step to generate intrinsic features and then used the available data labels to tune the model. however, hmms are limited by their numbers of possible hidden states and become impractical when modeling long-range dependencies in large context windows."
"we found that the unidirectional drnn model with four layers yields the best results for the usc-had dataset. figure 9a presents the classification results for the test set in the form of a confusion matrix, along with the per-class recall and precision results. the proposed method achieved better overall accuracy than other methods, such as cnns [cit], least squares support vector machine (ls-svm) [cit], and random forest [cit], as shown in figure 9b ."
objects such as points and channels are determined by the set of operations presented in table 1. cs is a finite (but dynamically extending) graph representing the architecture of parallel computation.
"learning 9: a good ci/cd and devops methodology allows for quick detection of design issues. as our ci/cd pipeline performs integration tests from the beginning of development, we were able to detect design issues very fast. for example, early in the project we identified a mismatch between the designed schemas, i.e templates, for service and vnf descriptors and the descriptor data required by the service platform to correctly orchestrate the vnfs. detecting descriptor issues late in the project would have implied a huge effort to correct, as already developed services for e.g. pilots and sdk tools that aid in the development of such services would need to be changed."
"to save time and effort of browsing pbrs, service consumers may use web search engines (e.g. google) to find easily and quickly binding details of services of interest. however, the information retrieval model of search engines is not suitable for searching web service bindings."
"learning 2: interfacing a service development kit to a service platform requires a strong integration. an sdk is a design environment that provides a range of tools to aid developers of nfv-based services. such an sdk was developed as part of sonata, and in order to increase its devops capabilities, we learned that it must take into account the specific apis and input formats expected by the sonata service platform. therefore, the sdk requires an explicit linkage to the targeted service platform. the main sdk features regarding validation, profiling and packaging of a network service that are being developed require the implementation of two custom-built interfaces to the service platform: (i) an interface to on-board the service package and deploy it in either a test or production resources of the sp. instead of using an isolated test environment inside the sdk, a better and more devops approach is to use the sp's orchestration framework directly. this way, it is not needed to fully replicate the service deployment functionality inside the sdk and services get tested directly in the service platform they are supposed to run on. (ii) an interface through which monitoring data that is collected by the sp's monitoring framework can be fed back to the sdk and the developer for further analysis that may improve the performance of the service. both test or production environment can have their own, developer customised, set of metrics collected to export. in the sonata service platform, the gatekeeper is the module that mediates interactions between the service platform and the outside world, including these interfaces with an sdk. the gatekeeper authenticates and authorises all requests from developers to instantiate services and to obtain monitoring data."
"in our constructions of parcs, we tried to maximally separate commands for cs from specific peculiarities of a basic procedural language. this allows us to create tools to describe universal parallel extensions of programming languages."
"in this paper, we propose the use of long short-term memory (lstm)-based deep rnns (drnns) to build har models for classifying activities mapped from variable-length input sequences. we develop architectures based on deep layers of unidirectional and bidirectional rnns, independently, as well as a cascaded architecture progressing from bidirectional to unidirectional rnns. these models are then tested on various benchmark datasets to validate their performance and generalizability for a large range of activity recognition tasks. the major contributions of our work are as follows:"
"training regular rnns can be challenging because of vanishing or exploding gradient problems that hinder the network's ability to backpropagate gradients through long-range temporal intervals [cit] . this precludes modeling wide-range dependencies between input data for human activities when learning movements with long context windows. however, lstm-based rnns can model temporal sequences and their wide-range dependencies by replacing the traditional nodes with memory cells that have internal and outer recurrence."
"the fundamental standards that make use of web services are xml [cit], wsdl [cit], soap [cit] and uddi [cit] . although these standards have gained some levels of maturity, web services technology still has serious lacks for providing guaranties to bind web services efficiently. one of the main challenges in binding web services is the fact that users who are looking for relevant web services will have to devote hours searching through potential service resources independently. furthermore, the repeated access to the same read-only web service with the same request may often produce the same response which can be considered a useless work and a waste of time and effort."
"learning 8: selecting the right software tools at the beginning of the project is crucial: i) to protect the devops approach, and ii) backtracking from selected tools is expensive in terms of time and effort. it is important to automate large parts of the ci/cd pipeline, allowing the developers to focus on code development and provide them with quick feedback. to this end, a set of software tools such as github and jenkins were selected. we learned that this selection process is of the highest importance, as the selection of the wrong tool might conflict with the devops goals of the methodology. at one point shortly after the beginning of the project, we adopted owasp [cit], a tool that analyses code for security risks by scanning it for vulnerabilities, performing penetration testing, etc. extending the jenkins job that evaluates pull requests with an additional owasp code check exponentially increased the time it took to analyse the updated code. the addition of owasp significantly increased the duration of the integration cycle, contradicting our devops objective to quickly provide the developer with feedback, which led us to optimise the pipeline and use owasp in a parallel job to not interrupt the first code check iteration. a learning period should be provisioned to allow developers to discover the selected software tools. this puts additional stress on the software tools selection process, as adopting a new software tool in a later stage of the project leads to a new learning period, which is both costly in terms of effort and time, especially in a distributed consortium like sonata."
author contributions: abdulmajid murad and jae-young pyun conceived the idea and research metrology. abdulmajid murad developed the proposed schemes and performed the experiments. jae-young pyun contributed to the conception of the study and analysis by directing and supervising the research.
"due to these facts, the ubr becomes out of use and hosting this registry is no longer necessary as it was reported from microsoft, ibm and sap the original operators of uddi project [cit] . by the closing of ubr along with the increasing demand for web services, pbrs are becoming the convenient way to find services of interest easily. service consumers can search pbrs for web services by categories and tags, or by using a set of advanced search qualifiers. however, the use of either of these methods will always give an incomplete result since the coverage rate in web services of each pbr is limited to few tens whereas the current number exceeds 28,000 [cit] . to fill the gap, hundreds if not thousands of pbrs may exist to cover all the available web services. hence, to find bindings of a suitable web service, users will soon face the challenge of browsing separately a huge number of pbrs which consume time and effort."
"algorithmic models developed on the basis of the parcs-technology (and using the means of a particular parcs-programming system) are called \"parcsmodels\". the development of parcs-models is implemented at two interrelated levels: logical and program. at the logical level parcs programming tools allow a user to describe explicitly the resource allocation, all possible commutation and re-commutation connections (with taking into account the recursive algorithm unfolding) or get implicitly the logical structure of the algorithm, resource allocation and re-commutation scheme as a result of the parcs model functioning. management, data and rules of interaction between the data and management are described at the program level, with taking into account the corresponding logical structure (it looks like as a \"filling\" of the logical structure)."
"in this paper, we have presented an agent-based approach for binding synchronization of web services. this approach follows our previous work which introduced the architecture of a local repository-based framework for optimizing the binding of web services. within this framework, binding details of web services of interest are saved locally in business repositories of the service consumers in a way to reduce time and effort of re-invoking these services."
"newly introduced features, even late in the project, quickly became stable. for example, the authentication and authorisation feature for service platform microservices was lately introduced through the user management concept. it was supposed to have a big impact on our integrated platform, as it added a new security wall between its components, demanding extra adaptation. following the ci/cd loop, it was successfully integrated through a continuous adaptation from the components interfaces with no major impact. once this feature was deployed, it started to gain stability while it was enhanced with additional features such groups, roles and permissions."
"in an effort to address these challenges, multiple consortia are trying to build network function virtualization (nfv) service platforms (sp) and management and orchestration (mano) frameworks. these technologies aid in the 5g adoption by increasing network programmability. one such service platform has been designed and developed in the scope of the eu 5g-ppp sonata project. sonata's service platform [cit] was designed to satisfy the need for flexible and extensible management operations, so it allows telecom operators and communication service providers to cope in a world with rapidly changing technological trends and newly introduced business models. to this end, sonata made the following design choices for its service platform: i) an extendable pluginbased mano architecture implemented through microservices allowing the owner to alter its behavior by adding and replacing plugins on the fly and ii) an infrastructure abstraction allowing the service platform to orchestrate multiple virtual infrastructure managers (vim) ."
"for the daphnet fog dataset, we found that the cascaded drnn model with one bidirectional layer and two upper unidirectional layers yields the best results. figure 11a summarizes the classification results for the test set. the low values of recall and precision for the \"freeze\" class are caused by the dominance of the \"normal\" class. however, our proposed method still outperforms other methods, such as k-nearest neighbors (knn) [cit] and cnns [cit], in terms of f1 score, as shown in figure 11b ."
"to verify the delivery status of email addresses, companies which relay upon email communications use an email validation web service to keep list of addresses correct. in this section we use this sample of web service to show how important it is to maintain its binding details accurate through business repositories to preserve its sustainable use."
"after performing the above action was possible deployment parcs c# in the cloud microsoft azure. we deployed web service system that used a web-interface. as shown in figure 3, the interface displays the current hosts involved in the calculations and tasks performed. also present functions add and cancel tasks."
the parcs-conception appeared in early 80-s of the last century as an attempt to create programming tools for computers of non von-neumann architectures [cit] . the key idea behind parcs is the representation of a complex parallel process as executional activities developing and communicating in some logically connected space. current activities of a process are assigned to some space coordinates that define communication addresses. we call such a communication structure a control space (cs). structurally cs consists of addressable points and channels. both points and channels could be constants or variables.
"in programming languages, such operations have great practical importance for solving a wide class of problems. for example, it is convenient to run the base text of a program as well as several other programs that differ from the basic program in minor modifications."
precision: measures the number of true samples out of those classified as positive. the overall precision is the average of the precisions for each class:
"during training, the datasets were segmented with different window lengths, as outlined in table 1 . the optimal window length of a dataset depends on the sampling rate and the type of activities performed. we tested various lengths by \"trial-and-error\" method, then chose the window length that gave better performance results. training was performed using the raw data without any further data preprocessing or intermediate intervention. the training and testing are generally performed using fixed-length windows, but the inputs of models may be using variable-length windows in the real-time data acquisition scenarios. in real-time application of har, data are captured over the course of time and the delay in drnns is not fixed. instead, the network can emit the corresponding label for a variable-length input segment. this is in contrast to other methods, such as cnns, in which the network must wait until a given fixed-length input segment is complete, before emitting the corresponding label."
"the goal of synchronizing web service bindings within the lrbf is mainly to ensure that business repositories (pbrs, pwsr and lwsrs) see all the changes that have originated by a service provider. an additional goal is that local binding inquiries made at lwsrs yield results consistent to those made at pwsr or pbrs. to meet these goals, we introduce in this section an agent-based model for dynamic synchronization of web service bindings which will overcome limitations of the lrbf."
"due to the fact that ubr, pbrs and web search engines are considered as the major solutions to find binding details of web services and due to their aforementioned limitations we assumed that their interoperability would complement each technology's strengths. motivated by this assumption and to optimize the binding of web services from heterogeneous environments, the lrbf framework was proposed that focuses on binding services of interest locally. the first level of binding optimization is public, that is, to avoid the separate access of various service resources (especially pbrs) we provided a public web service repository (pwsr), as a unique access point for users, to discover bindings of web services instead of going through other service resources. to load pwsr with such bindings, we built a public web service crawler engine (pwsce) that connects each pbr first and then collects binding details of web services before saving them in the pwsr. the second level of binding optimization is local, that is, to avoid the repeated binding to the same web services from the pwsr (accessed remotely from the server host) bindings of the frequently used web services are saved locally in the lwsrs (local web service repositories) of service consumers. this operation is performed by the local web service crawler engine (lwsce)."
"however, since web services are always changing (removed, updated, replaced, etc.) their binding details may need to change too. to address this issue, we have proposed in this work an agent-based synchronization model to update bindings of web services whenever changes occur in order to keep their use in safe. in future work, we envision dealing with the implementation issues of the proposed model."
"we consider recordings from a single 3d accelerometer, which is placed on the right hand of an employee. the dataset contains 11 activity classes: writing on a notepad, opening hood, closing hood, checking gaps on front door, opening left front door, closing left front door, closing both left doors, checking trunk gaps, opening and closing trunk, and a null-class for any non-relevant actions."
"since web services may change their bindings, for instance updating their access points, binding protocols, functional and non-functional parameters and so forth, therefore such updates have to be forwarded from pbrs where updates occur to pwsr and lwsrs respectively where bindings of the updated web services are saved. otherwise, using outdated binding details from lwsrs may not lead to any result since these details no longer much the original ones. therefore, the local binding of web services will not be possible. to sort out this problem, we propose in the next section an agent-based synchronization model to preserve the consistency of web service bindings over pwsr and lwsrs whenever changes occur in pbrs."
"we conclude that visualization of image processing intermediate results, which aims at providing transparency and trust to the physicians, may significantly hamper the workflow and is considered less important for routine integration of medical image processing software into a pacs environment."
"to enable high throughput sample processing, we developed a miniaturized dca (mini-dca) technique that utilizes only 50-100 μl of whole blood samples collected by a finger stick. samples are cultured in a 96-microtube matrix with bar-coded 1.4 ml tubes. since all the chromosome preparatory steps are carried out using multichannel pipettes, the overall sample processing time is reduced by a factor of more than 4 when compared to the conventional dca. to streamline the high throughput sample processing with rapid image analysis, we also optimized the semi-automated metafer dcscore algorithm. we found that a quick manual verification of false positives or negatives considerably reduced the analysis time by a factor of 5 when compared to complete manual scoring. the mini-dca was verified and validated through comparison with conventional dca performed in 15 ml conical tubes. the radiation doses estimated by mini-dca using the triage mode of scoring (50 cells or 30 dcs) were similar (±0.1-0.3 gy) to doses estimated by conventional dca using 300-500 cells. our results suggest that the mini-dca coupled with automated dc scoring can be an effective radiological triage tool for radiation mass casualty incidents."
"our study compares integration variants where the integrated cad-pacs ensemble reports and visualizes more vs. less intermediate steps. our initial hypothesis assumes the more transparent variant, visualizing intermediate steps, to achieve better usability than the \"black box\" variant, which presents the final result or recommendation while avoiding to reveal intermediate steps."
"during a test session, the instructor should only intervene when the subject stops talking. at the session, there should be full audio taping and/or video recording of the subject and, if necessary, video recording of the computer screens to document all important information."
"here f α is fitness of man α and n α,i is a random term. for simplicity, we assume that f and n are uniformly distributed on [cit], and that weight of fitness ω is universal for all men and women."
"attack: adt ii can execute polynomial-bounded numbers of queries for public key creation, full private key extraction, signcryption, and un-signcryption in a systematic way using a standard model similar to the (indàcca2) game. moreover, adt ii is permitted to execute the partial private key-extraction queries since it has access to m sk ."
"we next performed mini-dca and conventional dca on samples that were irradiated with relatively low doses of x-rays (0, 0.25, 0.5 and 1 gy). results of biodose estimation relative to physical dose by both methods are shown in table 2 . metaphase cells obtained by both mini-dca and conventional dca were analyzed by semi-automated method using the dcscore algorithm of metafer after removing the false positives and negatives. the biodose detected by mini-dca was found to be 0, 0.33, 0.39 and 0.92 gy for the physical doses of 0, 0.25, 0.5 and 1 gy. the biodose detected by conventional dca was 0, 0.33, 0.59 and 1.21 gy for the corresponding physical doses of 0, 0.25, 0.5 and 1 gy. with both methods, the estimated radiation doses deviated by 0.1-0.2 gy from the physical doses. these variations are well within the specified limit of ±0.5 gy by iaea for the dca based radiation dose estimation."
"in this phase, we take initial security parameter k as an input. moreover, kgc selects (g 1 and g 2 ) two groups of prime order p, in which g 1 belong to the additive group and g 2 belong to the multiplicative group. while p as a generator of g 1 and a bilinear map: e : g 1 3 g 2 ! g 2 . moreover, four one-way collisionresistance functions are used, which are discussed as follows"
"however, a slight competition will significantly reduce the women's energy. this improvement of women's happiness increases with n (fig.1b), and the competition required for the woman to reach optimal energy decreases as the population grows (fig.1c) . as competition further intensifies, the womans energy will also increase. on one hand, due to the increase of the number of proposals, the number of choices of woman increases. it can be known from the nature of the order statistics that the minimum number of multiple random sampling decreases as the number of sampling increases. the increase in competition between men is beneficial to the women. on the other hand, the increase in the weight of fitness also leads to women tend to favor the men with high fitness, and thus increases the intensity of competition among women. many women have to be matched with the men positioned in the back in their wish lists. this will lead to an increase in the average energy of women and all people. due to the increase of ω in the beginning, the reduction of women's energy is more significant than the increase of men's energy. as shown in fig. 2a, there exists a ω * g that can make average energy of all people reach the optimal value. in other words, a proper competition can help to increase the degree of social well-being."
the study adopted a mixed-methods approach combining the acquisition and analysis of qualitative and quantitative data as well. qualitative data are elicited during a thinkingaloud approach. quantitative data were collected using scaled items contained in a questionnaire as well as time measurements for task solving.
"attack: adt i can execute polynomial-bounded numbers of queries as same as the (ind-cca2) game. moreover, adt i is permitted to execute partial private key-extraction queries."
"the system administrator can set the data-access policies and structures using and, or logic gates and transmitted these access policies toward the patient to authorizes theses polices along with doctor policy for data accessing ( figure 5 ). the biosensor node deployed on a particular patient body can sense the vital signs and then securely communicate these vital signs to bs using ieee 802.15.6 standard. bs further transmitted the received patient information toward the ms in a secure manner using 3g/4g internet technology. now ms used the predefine data-access policies to encrypt the patient information and transmitted to the data pool of blockchain."
"blood samples (~10 ml) were collected from three healthy human donors (35-55 years of age) and the blood collection was performed with the consent of donors in strict compliance with the institutional review board (irb) protocol (orau 000 349). aliquots (1 ml) of samples were irradiated with different doses of x-rays (0, 0.25, 0.5, 1, 2, 3, 4 and 5 gy) using the x-ray irradiator facility of the university of tennessee knoxville (utk) at a dose rate of 2 gy/min."
"performance of dca, however, is time consuming and laborious with an estimated turnaround time of 72-96 h. further, timely evaluation of dcs for dose estimation in a large number of individuals provides a bottleneck due to a restricted number of trained personnel and a prolonged analysis time. to make dca suitable for radiological triage scenarios, efforts have been taken by several laboratories to automate some of the procedural steps in dca: chromosome preparation (2), automated scoring (3) (4) (5) (6) (7) (8) (9) and sample tracking (10) . automated platforms are also commercially available (hanabi, ads biotec, usa) which can perform the steps of cell harvesting and metaphase chromosome preparation with minimal human intervention. however, currently available systems can process only 64 samples at a given time and the steps leading to metaphase chromosome slide preparations take~5-6 h. in order to effectively use dca for radiation mass casualty incidents, improvements are necessary for large-scale sample processing and analysis so that timely assessment of individualized dose is accomplished. the conventional dca utilizes 15 ml conical tubes and processing a large number of these tubes is cumbersome, costly and time consuming as large volumes of reagents are added manually after each centrifugation step during sample fixation."
"one of the interesting questions is, if people's wish lists are correlated, which will induce competition among the agents on the same side, then how will the competition affect the matching results? in previous studies [cit], numerical simulation showed that the exerting of competition will lead to higher energy and lower happiness. here we thoroughly study the impact of competition on bipartite matching, and instead of intuitive result that competition may reduce the well-being of the society, our result shows that proper competition can make society happier. on the other hand, a recent research [cit] shows that random bipartite matching is very unstable because reducing only one woman will lead to a dramatic change in the matching result. however, unstable situations are rarely observed in real life, even though the numbers of matching parties in reality are often different. our research shows introducing competition can effectively increase the stability of the matching result and explain the absence of the unstable situation in daily observation."
"initial: the challenger (ch) executes the setup algorithm on input data, that is, initial security parameters (k), and assigns system parameters spm along with master secret key (m sk ) to the adversary (adt ii )."
"in our proposed scheme, we have used the most popular dolev-yao (dy) threat model. 76 according to this model, the communication among any two nodes is performed on the open channel. moreover, the endpoint of a communication is also not reliable. in bsns, due to the susceptible nature of public networks, the adversary can easily alter or delete the patient-sensitive information for misuse while transmitting it from biosensor nodes to an ms. besides, an attacker can easily imprison some biosensor nodes as well as an ms for illegal activities. moreover, the smartphones of medical experts can be lost or stolen. in this case, the attacker can easily extract the patient's secret information stored in that smartphone and use it for specious activities, such as man-in-the-middle attack, offline password guessing attack, replay attack, insider attack, biosensor node or ms impersonation attacks, and computation of secret key. to maintain the security and privacy of patient data, it is important to secure the bsns against all estimated threats. in our proposed scheme, the kgc is considered as the full trusted server of the entire bsns. furthermore, the computed partial private key is encrypted by the server using advanced encryption standard (aes) algorithm and then transmitted toward the biosensor nodes for the generation of the full private key. in case of ms failure, the sensitive patient data backup is available in the cloud and hybrid blockchain technology is used to protect these data from possible adversaries' attacks. our scheme fulfills the basic security properties of confidentiality, integrity, authenticity, non-repudiation, scalability, and traceability with minimal cost."
"we proposed secure and efficient attribute-based online/offline heterogeneous signcryption scheme for bsns using hybrid blockchain technology, in which we improve the overall security performance of the bsns with lower transmission overhead along with processing cost. in our proposed scheme, kgc is a third-party or a server between certificate-less domain and pki-based domain that is working in a secret key-generation process. in the key-generation process, we compute cryptographic keys for secure transmission of patient's vital sign from source to destination node. kgc contain its own master public key, master secret key and other public parameters for the generation of partial private key. now with the help of partial private key and random number, each biosensor nodes can be able to compute their full private keys for secure communication of patient data from source node to destination node. the biosensor nodes that are deployed on a patient body admitted in a hospital ward can continuously sense patient's vital signs and then securely transmit these patient's vital signs to a bs using the ieee.802.15.6 standards. after receiving the patient's encoded information, the bs used 3g/4g internet services for transmission of patient information in a signcrypted format to the ms. authentic medical doctors can access the patient information stored in the ms directly and also remotely from the cloud. medical doctors analyze patient information and provide treatments based on an analysis of medical data. using hybrid blockchain technology, we securely stored patient-sensitive information on a cloud and multiple types of external users can access the patient information after verification. furthermore, in our proposed heterogeneous scheme, biosensor nodes deployed on a patient body can work in clc domain while for enhancement of scalability on ms side pki-based environment is used. to maintain data security and privacy of sensitive medical information, the concept of blockchain technology is used in our proposed scheme. figure 2 shows our proposed formal heterogeneous model for bsns."
& all items focus on integration-related usability instead of covering all aspects of usability. & all items meet high standards of comprehensibility and scale construction.
"however, in reality the wish lists are often not random, some intrinsic properties, here expressed in fitness, will affect the ranking order of the wish lists. beauty, intelligence, wealth, etc. can be widely viewed as high-fitness, people with those widely accepted attributes are easier to rank in front of the wish lists. in the extreme situation, all people's preferences are strictly based on fitness term, then everyone should have an identical wish list. in the beginning, all men make proposals to the woman with the highest fitness. the man with the highest fitness will be accepted and the others are refused. after that, the remaining men make proposals to the woman with the second highest fitness, the man with the second highest fitness will be accepted and the others were rejected. we continue this process, it is easy to know the men's energy from low to high are 1, 2, 3...n and the average energy (n+1)/2. the energy of women is the same."
"in this phase, the cipher takes an encoded text c 0, full private key fp k, t i, and u i as input and run unsigncryption cipher to get patient-sensitive data in plain text and to check the correctness of that data. un-signcryption (c 0, fp k, t i, u i ):"
"as an additional aspect associated with the examination patterns reported in the literature [cit], the future design of cad needs to improve usability by observing and then taking into account special workflow or examination patterns in order to align the radiologist's workflow and the cad services offered."
"due to extensive use of bsns, designing a secure and efficient heterogeneous scheme is an important requirement for the resource-constrained environment. bsns face three major challenges which are security, privacy, and efficiency. to cope with these problems, in our proposed scheme, we have used a heterogeneous online/offline signcryption and clc environment to overcome the computational burden on biosensor nodes, while pki environment at the ms side to enhance the scalability of the networks. besides, we achieved the advantages of interoperability, transparency, traceability, and privacy using hybrid blockchain technology. we thoroughly examined our proposed scheme for its security analysis with the help of formal security under the standard model along with informal security. using informal security analysis, we proved that our scheme provides resistance against possible attacks. the detailed results of performance analysis show that our proposed heterogeneous scheme provides better security and privacy while consuming less energy during patient data computation and transmission than other heterogeneous schemes discussed in the literature and enhance the overall performance of the networks. therefore, this study is preferable for practical applications of bsns, where two different parties can securely communicate even if their architecture and domain are different."
"the wish list of woman i is generated according to the order of her ratings of all men. similarly, we can obtain the wish list of everyone. with these wish lists, implementing gale-shapley algorithm, we will obtain the final stable matching. now we start to analyze this matching result. as shown in fig.1a, the average energy of men increases monotonically with the weight of fitness, ω. this is because when the weight of fitness increases, it is easier for a woman with high fitness to be ranked in the front of the lists. when men make proposals according to their wish lists, their aims are often overlapping, which requires more proposals for everyone to be engaged."
1. using bonexpert plugin: the images have to be loaded manually via a temporary folder on the hard disk ( fig. 2 ) 2. using bonexpert intray: the digital imaging and communication in medicine (dicom) protocol is used to feed an intray of images or analysis (fig. 3 ).
authenticator contract key: the authenticator also generates the couple of keys and attached with smart contract in a suite used to encrypt the reports from requester system in our network.
"challenge phase. an adversary (ad) defines when phase-1 is completed. the adversary (ad) produces challenge for two same size plain texts (n 0, n 1 ) and biosensor identity (id si ), in which the adversary (ad) wants to be challenged. moreover, the challenger (ch) selects a random number and executes both the offline signcryption along with online signcryption algorithms and submits the obtained result to the adversary (ad). in case the public key has been changed, the adversary (ad) assigns the confidential value to the challenger (ch) for further computation."
"we placed the study in a strictly realistic clinical context, which we regard as a major strength of the experimental setup. the participants had to perform routine tasks in a routine setting and the findings of the study are, therefore, likely to apply to similar real world situations. of cause, it is not justified to generalize the results to different types of radiological diagnostics without considering possible similarities of the radiologist's workflow. fig. 7 answers to the questionnaire: the x-axis corresponds to the scale of the items (−2 \"completely disagree\" to 2 \"completely agree\"). the boxplots show the median, lower/upper quartiles, maximal values, and minimal values. the items of the questionnaire were ordered by their mean values; the original position is given by the numbers at the end of the labels. in case of negatively formulated items (indicated by \"neg\"), results were inverted furthermore, all participants had more than 1 year of experience in the special field of pediatric radiology and roughly the same level of expertise concerning bone age assessment. it has been previously observed and reported that the level of clinical expertise alters the diagnostic process and the related pattern of examination [cit] . while being closely linked to the diagnostic workflow, usability problems may occur only on a special level of expertise. the participants of this study can be assumed to have homogenously reached an intermediate to high level of expertise in the investigated field. thus, the findings are not likely to apply to other levels of expertise and almost certainly not to novices."
"for mini-dca, a 96-microtube matrix (thermo scientific) containing bar-coded 1.4 ml tubes was used. multichannel pipettes were used for all the liquid handling steps involved in culture set up, cell harvesting and fixation. for each mock or irradiated sample, triplicate cultures were set up. cultures were set up in mini tubes by placing 100 μl of whole blood sample and 900 μl of complete growth medium. after mixing thoroughly, the cultures were incubated at 37°c in a 5% co 2 incubator for 48 h. at the completion of 44 h, 10 μl of colcemid (0.1 μg/ml) was added for the last 4 h and the cultures were harvested and fixed as described before. briefly, the microplates containing the 1.4 ml tubes were spun at 1200 rpm for 2 min and the supernatant (~900 μl) was carefully aspirated from the tubes using a multichannel pipette. for hypotonic treatment, 900 μl of freshly made 0.56% kcl was added to each tube and the tubes were incubated for 18 min at 37°c. after hypotonic treatment, the micro-plates were spun and the cells were subjected to fixation using three changes of acetic acid:methanol (1:3) mixture. the fixed cell suspension (25-30 μl) was dropped onto acid cleaned slides and the slides were subjected to fluorescence plus giemsa (fpg) staining technique. briefly, slides covered with a few drops of hoechst 33258 were exposed to short wavelength ultraviolet light for 30 min followed by washing in sorenson buffer (ph 6.8) and incubation at 60°c for 20 min in 2xssc. after staining with giemsa (5% in a buffered solution), the slides were air-dried and subsequently mounted with coverslips using dpx."
"in order to enforce the attention of the participants when answering, the set of items contains negative and positive statements concerning usability aspect [e.g., \"the programs' feedback is comprehensible\" (positive) vs. \"there are too many work steps before starting the analysis\" (negative)]."
"quantitative data figure 7 gives an overview of the answered items of the questionnaire. as stated in the methods section above, the items were ordered by the mean values of the likert-scale, which yielded a ranking ranging from positive aspects of bonexpert to severe usability problems."
"the different handling of emerging problems depends on the massive differences in the participant's computer experience. difficulties with basic computer skills in health care professionals are mentioned earlier and a problem, to be reckoned with by programmers [cit] . the user-specific methods of using bonexpert are really different because some test persons first watch bonexpert's result and then verify it, which actually saves time in some determinations of bone age, and the other watch bonexpert's result after analyzing themselves. in no case, the latter mentioned have modified their own performance if the results disaccorded."
"bipartite matching problem is to study that how the two disjoint groups of agents can be matched pairwise for their personal preferences, such as the matching between men and women, students and colleges, workers and jobs, consumers and products [cit] and many other scenarios [cit] . for convenience we use the paradigm of marriage problem, where n men and n women need be matched. in this problem, each participant is selfish, everyone tries to optimize their own choices and the competition is inevitable. a key question is how to find a stable solution in which there is no such a pair of man and woman who both prefer each other than the assigned partner [cit] . the number of stable solutions is very large [cit] ."
"in this phase, we securely transmitted partial private key s ippk from server side to biosensor nodes using symmetric cipher aes. this algorithm is fast in terms of encryption and decryption and consumes less energy. moreover aes algorithm is secure and suitable for resource-constrained environments like bsns."
"for their assignment, the participants get an instruction manual, 50 consultation papers with patient data, and the promise to get help on enquiry. the information provided is: the introduction given by the testers has been preformulated in order to provide all important information combined with a check list for all important material to be handed out."
"this cipher is computed using a powerful machine, that is, medical server, and after computation, the output of the algorithm is stored in the biosensor node before deployment on a patient body. in the offline phase, we reduced the computational burden of the biosensor nodes. offline signcryption (fp k, s ipuk, pu ms, id i, spm, ms puk, x i ):"
"conventional dca was performed in 15 ml conical tubes (falcon). for conventional dca, 0.5 ml of mock and irradiated samples were mixed with 9.5 ml complete lymphocyte growth medium (pbmax, gibco) containing fetal bovine serum, phytohaemagglutinin, growth factors and antibiotics. to promote an optimal growth, an additional 1 ml of pha was added for every 100 ml of pbmax for both conventional dca and mini-dca. bromodeoxyuridine (10 μm) was added to the cultures to identify the first division metaphases. the cultures were incubated at 37°c in a 5% co 2 incubator for 48 h. colcemid (0.1 μg/ml) was added for the last 4 h and the cultures were harvested using a standard procedure. cells were treated with a hypotonic solution (0.56% kcl) for 18 min at 37°c and fixed in three changes of acetic acid:methanol (1:3) mixture. an aliquot of fixed cell suspension (30-40 μl) was placed at the center of acid cleaned slides. the slides were stained using 5% giemsa for metaphase image capture and analysis."
"running under windows xp or vista, bonexpert (visiana, denmark, version 1.1.4) is a standalone application to determine the bone age of children [cit] . bonexpert features are gp, tw2, tw3, tw japan as bone age scales, the ethnicities caucasian, african-american, hispanic, asian (usa and japan), disorders like short stature and pubertas praecox and more specialties (table 1) . however, it has not been approved for clinical use by the u. s. food and drug administration (fda)."
"the study investigates bonexpert, a cad application introduced to suggest a bone age reading based on the patient's left hand radiograph [cit] . focusing on usability aspects, we design, perform, and evaluate a mixedmethods study combining the acquisition and analysis of both quantitative and qualitative data, which will be adopted to determine the specific usability profiles and differences of the two variants of system integration."
"about half of the participants use bonexpert plugin for the first 15 min of the analysis; then, they switch to bonexpert intray. the other half of the participants start with bonexpert intray instead and switch to bonexpert plugin. the participants are asked to determine as many as possible hand radiographs in half an hour using bonexpert as a second opinion."
"performance analysis consists of two main sub parts. in the first part, we compare the computation cost of our proposed heterogeneous scheme with other related schemes [cit] shown in the below table, while in the second part, we discuss the energy consumption of our proposed scheme with other heterogeneous schemes. [cit]"
"admitted authentic patient in ward of hospital can specifically permit any external users to access their history, that is, medical information, medicine, age, admitted date, and gender."
"the financial assistance received from the us department of energy (#de-ac05-06or23100 and technology integration grant orise-17-cm 989) from na-84 is gratefully acknowledged. reac/ts, an organizational program of the oak ridge institute for science and education, is operated by oak ridge associated universities for the us doe. the content is solely the responsibility of the authors and does not reflect the official views or opinions of either the us doe or orau."
"in this section, we demonstrate the formal security model of our proposed heterogeneous online/offline method. in our proposed scheme, biosensor nodes belong to clc environment and we consider two kinds of adversaries type-i and type-ii for (euf-cma). in type-i adversary model, common users of the system cannot access the master secret key (m sk ) of kgc, but it can try to change the public key of any biosensor node with their own choice. in type-ii adversary model, common user is honest-but curious and knows the master secret key of the kgc, but it does not alter the public key of the any biosensor nodes. however, in our proposed heterogeneous signcryption, we used clc-based signcryption in tier-1 while pki-based signcryption in tier-3 and obtain the security properties of confidentiality (ind-cca2) and unforgeability (eufàcma). moreover, in tier-3, our scheme satisfies the confidentiality property (ind-cca2), while in tier-1 satisfy the unforgeability properties (euf-cma), so we need to prove type-i and type-ii adversaries using games for both (euf-cma) and (ind-cca2)."
"in our proposed scheme, the security of patient's vital signs relies on the following computational hard problems. according to the given additive group (g 1, + ) and multiplicative group (g 1, ã ) of the prime order p and using the generator p of g 1 and a bilinear map:ê : g 1 3 g 2 ! g 2, we discuss the mathematical hard problems of cryptography."
"for the general situation, that fitness has an impact on ranking, and the personal preferences are diversified so causes random deviations. we assume that woman i rates man α with a score s α,i"
"as far as data integration is concerned, there were some positive results, e.g., item 13, stating that the program fits in its conceptual framework, but most of the items yielded no clear decision of the participants against or in favor of the respective statement. the same holds for the other three categories functional integration, presentation integration, and visual integration."
"in our proposed scheme for accessing patient's vital signs, all the external users must be register to the ms for verification or authentication process using their credentials, that is, id, email address, location, and also transmitted the time stamp (t s ). each authentic user can access the patient-sensitive information after verification according to security policies which are predefine by system administrator in access control list (acl)."
"statistical physicists also find areas of interest in the bipartite matching problem because the model is very similar to a system in which two different particles interact [cit] . we assume that everyone's satisfaction is corresponding to the ranking of her/his spouses in her/his wish list [cit] . it can be seen as a cost function, or energy in physicist terminology. if a person just happens to match with the person at the top of her/his wish list as a spouse, then he will be the happiest and have an energy of 1. in the worst case, one had to choose the person at the bottom of the list and the final energy was n. in most of the previous researches [cit], for simplicity, the wish lists are always established randomly and independently."
"definition 1. however, insider security is achieved in the form of confidentiality (ind-cca2) against the attacker in the aforementioned (ind-cca2) game. second, we proved game for existential unforgeability (euf-cma) here we used two types of adversaries: type-i and type-ii. furthermore, type-ii includes (euf-cma-i) game and (euf-cma-ii) game."
"un-signcryption queries. an adversary (ad) can execute the queries of un-signcryption and submits the encoded data. the challenger (ch) executes the un-signcryption algorithm and transmits the obtained result to the adversary (ad). in a pki environment, the adversary (ad) knows the public key of the server."
"proof. the patient-sensitive medical information is stored in a ms for future reference and treatment. each external user, that is, a doctor, who wants to access patient information must be registered with the ms to maintain the real-time patient data privacy."
"methodically, evaluation methods in qualitative research can be categorized as \"participants,\" \"goal,\" and \"time\" (fig. 4) . in our case, expert-based methods, including developer-and user-oriented methods, can be excluded because they massively depend on the skills and expertise of the participant experts [cit] . the result-oriented methods are more suitable for usability evaluations, not for the evaluation of integration [cit] and the time-concentrated methods are remote of an evaluation of applicability, but to perfect the time needed, not the comfort of a user [cit] ."
"in the gdhp, the g 1 calculates (p) xy, given (p, p x, p y ) using dbdhp procedure (p, p x, p y, p z, t) ø or ?."
the study reported in this paper aims at investigating usability aspects of cad-pacs integration in the context of bone age determination. the study compares two different variants of cad-pacs integration in order to compare their effect on the usability of the system in the context of the radiologists' workflow.
"the user-based approach involves end users only, i.e., radiologists regularly and frequently diagnosing x-rays using the pacs and assisting software modules. in contrast to summative evaluation studies normally involving much larger numbers of participants, usability tests can be successfully carried out with a small number of testers (especially when formatively applied, for instance, in order to improve software solutions): it is stated that the number of participants should not fall below five, while more than eight users would not actually yield more [cit] . since an odd number is preferable, we decided to work with seven participants. each usability test is supervised by a person (tester) acquainted to the software and also acquainted to interpret hand radiographs."
we proposed a smarter and optimal technology called blockchain to preserve the medical information of a particular patient in a block and protect the medical data from illegal accessing.
"proof. in our proposed heterogeneous scheme, we used the concept of time stamp (t s ) among communication devices of two different environments in which biosensor nodes belong to clc environment, while ms belongs to pki environment to ensure that the patient data received by the external users, that is, doctors, nurses, insurance companies, and government agencies, are fresh and protect type-i and type-ii adversaries to launch replay attack. moreover, we used the concept of hybrid blockchain, which improves the transparency and privacy of the patient-sensitive medical records and also protects the adversaries to launch replay attack."
"most of the studies addressing cad-pacs integration focus on the technical integration, namely, on system interoperability: [cit] addressed cad-pacs integration focussing on aspects of appropriate data exchange formats and protocols."
"item 16 explicitly addressed the preference of the user according to the type of integration offered by the two different bonexpert modules. item 45 repeated this issue, but here with inverted meaning, confirming the finding that there was a slight preference for the bonexpert intray module (table 3) ."
"with respect to the mixed-methods approach of the usability study, it can be stated that the results of the questionnaire and those of the qualitative text analysis are consistent. due to the small number of participants, the quantitative data can just be used to analyze some errors and not without fail the gravest mistakes or most annoying ones, while the free text questions only partially compensate this. the qualitative analysis of the thinkaloud records gives detailed information about special problems, and the results are expedient and precise. in combination with notes taken during the test session from the interviewer, it can detect all grave errors and the most important disaffections of the end users despite a low number of test sessions [cit] ."
"in this phase, the patient data (m) and the output of previous offline phase is taken as the input and computes the encoded text as follows:"
"in this article we introduce competition in bipartite matching problem by adopting correlated wish lists. while in the traditional model, everyone's wish list is not correlated, the active party can easily acquire the results they want and stop sending proposals so that the passive party are left with little freedom to choose. we show if a proper competition is introduced, men have to make more efforts and send more proposals, which will slightly reduce men's happiness. but at the same time, the happiness of women will be obviously increased, and society as a whole will become happier than the original matching result. in other words, proper competition increases the total happiness of the society. this is not only true for the bipartite matching problem, but also enlightens our understanding of many other social phenomena. besides, the introduction of correlation in the wish lists can also significantly reduce the instability of matching results."
"based on the transcripts of the audio files and the subsequent coding process, a total of 111 text passages was selected and assigned to keywords (codes) following the two-step procedure described in \"methods.\" transcribing and coding of the seven records took about 40 h. the selected passages contained the relevant statements of the participants addressing usability aspects of bonexpert, integrated into the diagnostic workflow using the two different modules for cad-pacs integration. table 2 gives an overview of the code system derived by the qualitative analysis and the number of statements assigned to each code."
"during the test sessions, the probands expressed several times the wish for a direct analysis, which means, when they execute bonexpert, it shall analyze the bone age of the selected hand radiograph directly. contrarily, both versions of cad-pacs integration necessitate to load/select the files form folders or intray lists, and click the \"analyze\" button repeatedly. a version of integration without visualization of bonexpert result window, which just expends the bone age, would solve this problem. this solution also would avoid the problems with the file directory tree and the data sheet of dicom files. all in all, the results of the questionnaire support this important result of the qualitative data analysis: cad should be technically integrated into the pacs/clinical workplace as seamlessly as possible (see, e.g., the items concerning superfluous work steps, difficult data transfer, and unnecessary interrupts) without visualization of intermediate processing steps. thus, the recommended version of bonexpert's integration is a version without visualization and suggested fewer clicks, disregarding the differences between the plugin and the intray methods. the failure indications show which errors are to eliminate before a complete version should be provided. however, a clear preference for one of the both variants of integration was not identified."
"in this era, many researchers have used the concept of blockchain technology for security purposes to enhance the integrity and traceability of sensitive data in different domains like healthcare, 71 cloud computing, 72 iot, 73 and electronic voting. 74, 75"
"as can be seen from the boxplots, the participants saw no problems in understanding the terminology, results and feedback used/produced by bonexpert and had no problems integrating the cad module in their workflow (items 39, 29, 30, 07, 05, 13). they appreciated the practically unhampered access to the pacs (items 28, 12, 06) and the constant availability of the original patient data and radiographs (item 01). the results produced met the expectations of the radiologists (items 43, 31) and the necessary functions could be found when needed, even in the case of rare use (item 08)."
"the think-aloud method assesses end users performing a series of tasks while verbalizing their thoughts. according to jaspers, think-aloud is a verbal report method from the cognitive psychology that is made up of two [cit] :"
"kgc runs the key-generation algorithms to generate secret keys and securely distribute these secret keys among data producers and data consumers for secure communication. data producers, that is, hospitals, encode patient's medical sensitive data using secret keys and send secret data with related description to a data pool of blockchain. data consumers, that is, medical research institutes, insurance companies, doctors, and government agencies, first login, and after verification, if it is declared legal or authentic, only then can access the patient data according to predefine attribute-based access structure or policies. data producers send signcrypted authorized letter to the data pool of a blockchain system to authorize their data-sharing policies. attribute-based security policies are used to provide fine grain access control to all external users, and patients can manage easily their own sensitive medical information and securely disseminate without violating patient data privacy and also enhanced the network life while keeping patient medical records private. data pool securely transfer data to blockchain for further processing and then patient secret data stored on medical cloud. in a blockchain, we apply one-way hash function (md-512) to maintain the integrity and traceability of medical records and improve the security to prevent the patient data from tempering, while the descriptions of patient medical information along with data address are stored in the blockchain in a decentralized manner. now data consumers can access that patient's data and description of data from the cloud and blockchain using internet technology. figure 3 shows the secure transmission between the ms and external users."
"our proposed heterogeneous scheme can also provide the support of smart contract for automatic claims of settlement. 77 the smart contract is just like a reality contract, but the reality contract is in written format and the blockchain smart contract is a small program or protocol inside a blockchain. the process of smart contract can be run between patient, ms, and all external parties. through the smart contract, any two users can communicate securely without a trusted third-party, meaning it removes the trusted third-party. smart contracts are open-distributed ledgers that are impossible to hack. the smart contracts are distributed and immutable: immutable, meaning that once a smart contract is created, it certainly cannot be changed and distributed, meaning that the smart contract is validated by each node of the network (figure 6 )."
"clinically, determination of skeletal maturity (i.e., bone age assessment, baa) is required to track endocrine disorders or pediatric syndromes [cit] and for forensic age assessment of adolescents and young adults [cit] . based on skeletal radiographs of the left hand, the methods of greulich and pyle (gp) [cit] or tanner and whitehouse (tw) [cit] are applied, where qualitative and quantitative comparison to reference images is performed by the radiologist, respectively. more specifically, tanner and whitehouse have presented a complex evaluation scheme, where scores are given according to the shape characteristics of individual distal forearm and metacarpal bones and epiphyses, which are then combined numerically into the skeletal age guess. hence, either the methods are error prone and time consuming, since both require extensive manual interaction of up to 15 min per task."
"the other end of the spectrum indicates some severe usability problems: not all necessary information was available (item 10); there were unnecessary interrupts of the workflow induced by the program and problems with the data transfer between pacs and bonexpert (items 26 and 27). the graphical output produced by the program and the effects of using the same function were rated as partly inconsistent (item 41). the participants could not intuitively start working without help (item 23) and the program increased the cognitive load (item 04). finally, errors could be propagated through the workflow and lead to unexpected errors in different areas (item 20)."
"security and privacy of patient-related data are two indispensable components in bsns. security means that data are securely stored and transferred, and privacy means that the people who have authorization can access, view, and use the data. 6 generally, attributes are used for identity purposes. in a bsn environment, attributes mean vital signs of human body such as bp, temperature, respiration rate, pulse, and oxygen saturation. each attribute is associated with a private and a public key component such that, only the user holding those attributes will be able to decrypt the data encrypted under those attributes. sahai and waters 7 introduced the concept of attribute-based encryption (abe) for the first time. in which encryption and decryption of data are performed using a set of attributes for secure communication among different parties. only that users can access the data which attribute of the public key is corresponding with master key. no need to obtain the certificate for authenticity of public keys."
"public key queries. adversary (ad) can add the identification number (id si ) of biosensor node to the challenger (ch). moreover, the challenger (ch) executes the algorithm of public key generation (id s i, spm) ! (pu k, x i ) figure 3 . secure cloud-based patient electronic record system. and returns (pu k ) to (ad). now the adversary (ad) saves a list for further queries."
"the mini-dca was optimized using a microtube matrix containing 96 bar-coded tubes of 1.4 ml capacity. all the liquid handling steps were carried out using the multichannel pipettes ( figure 1 ). we estimated that each mini culture performed in a 1.4 ml tube containing 100 μl of whole blood sample yielded an average of 100-200 metaphase cells/slide, which was sufficient for a triage mode of scoring. typically, 2-3 slides were prepared from each tube yielding~400-600 metaphases per culture. to verify and validate the performance of mini-dca, conventional dca was performed on the same blood samples. initially, blood samples collected from three healthy donors were irradiated with different doses of x-rays (0, 1, 3 and 5 gy) and aliquots of samples were processed for both conventional (15 ml conical tubes) and mini-dca (1.4 ml tubes in a 96-microtube matrix) methods. the main purpose for developing mini-dca is to increase high throughput sample processing for radiological triage and therefore a triage mode of scoring (50 cells or 30 dicentrics) was employed for dose evaluation by mini-dca while a total of 300-500 metaphase cells were analyzed for dose assessment by conventional dca. the samples processed by conventional dca and mini-dca were evaluated by both manual and semi-automated scoring respectively using metafer. since semi-automated dc scoring is rapid and often preferred for triage scenarios, the accuracy of semiautomated scoring was compared with manual scoring. representative pictures of metaphase chromosomes prepared from 3 gy of x-rays irradiated lymphocytes by both conventional dca and mini-dca are shown in figure 2 . heavily damaged cells were chosen to demonstrate the accuracy of automated detection of dcs by dcscore algorithm. red rectangle boxes in both metaphase cells indicate the dcs that were detected by automated metafer dcscore algorithm. the results of comparative analysis of conventional and mini-dca are shown in table 1 and figure 3 . in all the samples (with the exception of a single dose point of 3 gy in donor 1), the biodose estimated by analysis of either 50 metaphases or 30 dcs in the mini-dca was similar (±-0.1-0.3 gy) to the biodose estimated by the conventional analysis of 300-500 cells. further, radiation doses estimated by both mini-dca and conventional dca were also very similar to the physical dose."
"forgery: adt i can create a forge-encoded text (b ã ) along with a biosensor identity (id si ), and finally, adt i wins the game if the below steps hold."
"in a security model, using a formal security analysis, we have proved that our proposed heterogeneous scheme is secure against type-i and type-ii adversaries. moreover, we show that our scheme satisfies the security properties of data confidentiality (indàcca2) and unforgeability (eufàcma) in the standard model. while using informal security analysis, we also have proved that our scheme resists other potential attacks. now with the help of informal security analysis of the proposed heterogeneous scheme, we have proved the resistance of our proposed scheme against possible attacks."
"it is proved that on average [cit], a total number of n(log(n) + 0.522) proposals need to be sent, to make everyone engaged. every proposal leads to a man's energy increasing by one, the average energy of men is the same as the average number of proposals that he needs to make, i.e. the average energy of man is log(n) + 0.522. on the other hand, for a woman, each of them receives on average log(n) + 0.522 proposals. each time they receive a proposal, they can make a choice and decide the man whom they prefer. obviously, the greater the number of proposals, the happier they are. since each suitor is randomly distributed in the wish list, the optimal one among many choices is equivalent to the first order statistic of multiple random sampling from even distribution. it is easy to obtain the woman's average energy is n/(log(n) + 0.522 + 1). when n is large, the average male energy is approximately equal to log(n), and the average female energy is approximately equal to n/log(n). this conclusion is consistent with the results of the energy distribution function method used in previous study [cit] ."
"we therefore identified the following leading usability questionnaires including items, which refer to integration aspects, by a literature research: cusq [cit], isometrics [cit], isonorm [cit], and sumi [cit] . starting from these questionnaires an item pool was constructed. the pool was iteratively reduced by (1) selecting items addressing integration aspects, (2) excluding redundant (i.e., very similar) items, and (3) carefully adapting the wording of the items (i.e., by changing \"the software\" to \"bonexpert\") in order to further avoid any misunderstanding. the final questionnaire consists of 45 scaled items that were dedicated to four categories of integration, namely [cit] :"
"the relevant detail about patient information is also written in the blockchain. now blockchain verify the transmission source of data pool and also stored the received encrypted data of patient in medical cloud. moreover, all the external users can access the patient data from cloud using their valid user id and assigned password using internet technology."
"questionnaires are a rapid, accepted and recognized option to overview people's sentiments. they are often used for opinion surveys, and there exist a lot of good and bad questionnaires [cit] . the typically used rating score at questionnaires is the likert scale [cit] . sometimes, a group of items is created and scored together [cit] . the most important existing questionnaires addressing usability aspects are the computer usability satisfaction questionnaire (cusq), which is answered after a session with the program, software, or website to be evaluated by the end user [cit], and isometrics and isonorm, which both judge usability of software programs. they are based on the seven basic principles of din en iso 9241-10, which are:"
"all in all, the user-based methods turn out to be a suitable group of methods. especially, the involvement of the end users produces good results in assailable evaluation tasks like tested before [5, [cit] . in this group, think-aloud analysis and questionnaires are suitable methods."
"in one case, the test session was disturbed by abnormal program termination. the situation was not reproducible. the error messages were recorded and annotated with the type of integration for detailed technical analysis. it turned out that program termination was a singular event and could not be explained by the different variants of system integration: it never reappeared in other test cases during the study, and no other participant experienced a similar event. facing various possible explanations for the program termination ranging from hardware problems to failures of the operation system-all independent from the cad application and details of the cad-pacs integration-the event was excluded from the analysis."
"however, while an appropriate research design being crucial to address this important aspect [cit], studies, which systematically assess usability aspects of cad-pacs integration, have rarely been published yet."
"in our proposed scheme using blockchain technology, dictatorial bodies can generate a common flow of deidentified medical patient information. this flow will help to the system to distinguish the pandemics and threats and perform essential action to manage the issue in timely bases."
"in our proposed scheme, we used the attribute-based heterogeneous online/offline scheme that securely transmitted patient information from the biosensor nodes to an ms using online/offline heterogeneous signcryption, while hybrid blockchain technology is used to enhance the overall security and privacy of bsns along with the performance of medical data sharing among various entities and users. the transmission of physiological sensitive data of patient in a wireless channel may cause serious threats to the patient medical records privacy, and illegal users may perform malicious activities on sensitive patient data to disclose the privacy and security of patient for misuses. in our proposed scheme, we design a novel heterogeneous model that highlighted the issue of patient data sharing among various departments, that is, healthcare staff in hospital, researchers, government agencies, and insurance companies in trust-fewer environments. using hybrid blockchain, our system provides data auditing, data provenance, scalability, and control for patient medical record shared in cloud repositories among large data objects. in our proposed cloud-based environment, we share patient information among various entities with minimal risk to patient data privacy. in our proposed heterogeneous model, biosensor nodes belong to a certificate-less environment that allows the biosensor nodes to sense patient's vital signs, such as bp, ecg, emg, and peripheral capillary oxygen saturation (spo2), and then transmitted to the ms via a bs in the pki-based domain. moreover, in the biosensor side, online/offline techniques are used. the biosensor nodes performed all the heavy cryptographic computation in the offline phase, while the receiver credential and patient information are unknown. the online phase is used for minor operations in biosensor side to increase the performance of the networks. our proposed heterogeneous online/offline signcryption scheme comprises the following phases: setup phase, ms key-generation phase, biosensor node keygeneration phase, offline signcryption phase, online signcryption phase, and un-signcryption phase. the following notations are used in our proposed scheme (table 1) ."
"in bsns, the patient should have full authority of his or her attributes (vital signs) such that only those legitimate users he wants to share with can access the patient attributes. the attributes may contain physiological sensitive information of patient such as full information of disease, family relationship history, medication, and dosing, as well as some insensitive information like healthy diet, physical exercise, and secrets of longevity. therefore, for security efficiency, we want to create an acl in the ms, which classifies the patient attributes into different categories according to their criticalness, due to which security and privacy of patients' information can be protected. external users (healthcare staff, researchers, insurance companies, and government agencies) can only access those attributes which are predefined in the acl. healthcare staff need to analyze an individual health profile and identify health threats and then suggest improvements in treatments based on an analysis of drug interaction, current medical practices, gaps in medical care plans, and identification of medical errors. they must have the right to access the patients' full attributes. on the other side, researchers, insurance companies, and government agencies cannot use these full rights. the security and privacy of patients' information are the essential components of secure bsn environments. in case the medical doctors obtain modified data of the patients, it may lead to a misdiagnosis of the patient and may have a serious consequence. malicious users also try to get patient attributes for illegal purposes. in our proposed network model, we maintain the privacy of patients' data with low processing cost and transmission overhead and provide a higher degree of security. our proposed scheme achieves confidentiality against ccas and unforgeability against chosen message attacks in the selective attribute model. attribute-based keys of patients can be revoked if necessary for privacy efficiency. using attribute-based security, only legitimate users can only access the information for which he or she is authorized. our proposed network model is presented in figure 4 ."
"the setting is chosen as realistic as possible: the thinkingaloud sessions are situated in a clinical setting using a typical workplace, where normally radiologists determine the bone age. the workplace contains four computer monitors: the two in the middle support radiographs and the two exterior ones are control processing programs and interfaces (fig. 5) . normally, the pacs client (isite radiology, version 4.41) opens on the left one, so that consequently bonexpert can be watched simultaneously on the right one. the workspace is temporarily equipped with the means to record the users' comments during the thinking-aloud session."
"after two preliminary tests of the basic system functionality, the participants sequentially open and analyze eight hand radiographs. during testing, the probands are advised to talk the whole time as if they would think-aloud. if someone stops talking, he or she is asked to continue by the tester. after finishing the analyses, the participants fill out the questionnaire."
"there were ten statements concerning program failures, which included one indicating a complete program abortion during the test, seven script errors (where the program informed the user about malfunctioning of specific steps) and two problems concerning user authorization. the following code categories of table 2 (namely \"direct analysis,\" \"image handling,\" and \"bonexpert\") address aspects of bonexpert-pacs integration and the specific usability of the bonexpert module. 16 statements voted strongly for an immediate analysis of the images, which would not require the user to trigger the sending of x-rays to the bonexpert module and to close their personal folder in order to avoid access conflicts by the program. another 14 statements expressed problems concerning the dicom-list to be used each time an x-ray is selected and handed over to the bonexpert module (focusing on short-comings of the sorting of list entries). the 20 statements assigned to the category \"image handling\" similarly address integration problems, here concerning the steps necessary to prepare and hand over an image to the bonexpert module. in these statements, the visualization of intermediate image processing results, outlining the bones, epiphyses, and other annotations in different styles and colors (fig. 1) was repeatedly commented as superfluous."
"according to this final finding, a third integration variant of bonexpert has been developed in the meantime allowing the automatic processing or radiographs in batch mode without displaying the intermediate processing result. this will foster applicability and acceptance for clinical routine."
"while the number of seven participants perfectly fits into the range recommended for (formative) usability studies, it is clearly too small for carrying out statistical tests. thus, quantitative data analysis had to be restricted to descriptive statistics only and, therefore, could not produce statistical significance."
"proof. in our proposed scheme for secure distribution of partial private key, we used the concept of aes algorithm. we securely transmitted a partial private key s ippk from the server side to the biosensor nodes using symmetric cipher aes. this algorithm is fast in terms of encryption and decryption and consumes less energy. moreover, aes algorithm protects the secret key during transmission and adversaries cannot guess or alter the session key for illegal activities. hence, our scheme is secure against man-in-the-middle attacks."
"let us firstly consider the matching problem when the two groups have equal size. for simplicity, it is usually assumed that everyone's wish list is completely random. considering the process of gale-shapley algorithm, men make proposals to women. if the proposed woman is unengaged, then the total number of matched pairs will increase by one. if this woman is engaged, no matter the suitor or her current partner is retained, the number of partners will not change."
"the radiologists are asked to analyze hand radiographs in order to determine the bone age. each session lasts 45 min: the introduction and preliminary tests take about 5 min, the application is tested during the following 30 min, and finally, the answering of the questionnaire takes about 10 min. figure 6 presents an overview of the thinking-aloud session. during the session, the tester follows a strict study protocol based on a written guideline. the tester introduces the participants to the test scenario, checks and eventually adjusts the quality of the audio recording, observes the users' actions and is able to help in case of technical problems."
"the evaluators should be a sample of users, representing the expected end users. if there are different types of users, a sufficient number of each user type (approximate eight subjects) should be included in the test sessions. the task examples should be as realistic as possible and representative for end-user performances in daily life situations."
"the questionnaire uses likert-scaled items: users are asked to assess a given statement (e.g., \"terms are used consistently in bonexpert and the pacs, respectively.\") by choosing one of the following grades:"
"private key queries. adversary (ad) can add the identification number (id si ) of a biosensor node to the challenger (ch). moreover, challenger (ch) executes full private key (fp k ) algorithm (x i, s ippk ) ! (fp k ) and returns (fp k ) to adversary (ad). now the adversary (ad) saves a list for further queries while the challenger (ch) can use other types of queries."
"an earlier study (24) described the development of a miniaturized assay for the detection of radiationinduced micronuclei. in this study, we developed and demonstrated the utility of mini-dca for precise dose estimation using a triage mode of dc scoring. our improved mini-dca version can easily support high throughput processing of several hundreds of samples by reducing the turnaround time for personalized dose estimate following a radiological or nuclear mass casualty incident. our mini-dca method, once automated for high throughput robotic platforms, will be an effective radiological triage tool for mass casualty incidents."
"euf-cma-i: in our proposed scheme (euf-cma-i game), the challenger (ch) directly communicates with the adversary of type-i denoted as adt i . initial: the challenger (ch) executes the setup algorithm on input data, that is, initial security parameters (k), and assigns system parameters spm to the adversary (adt i ) and protects the master secret key (m sk ). furthermore, the challenger (ch) assigns the adversary (ad) a server public key (pu k ) and a private key (s k )."
"the audio files recorded during the think-aloud session get transcribed and coded. coding is performed following a bottom-up approach [cit] : all text is read twice: in the first run, text passages, which contain statements referring to the usability aspects of bonexpert are identified, marked, and characterized by keywords. the identification of relevant text passages is based on both syntactic and semantic criteria: in order to preserve the context, we refrain from selecting single words. instead, text fragments are searched that consist of one or more clauses, a subordinate clause or at least a noun phrase (i.e., quite similar to a referenced quotation in scientific literature). furthermore, as an additional semantic criterion, the text fragments are required to represent one coherent concept relevant to the subject of the investigation (i.e., one specific aspect, one relevant factor). the keyword assigned to the text passage represents this coherent concept. afterwards, the keywords are grouped, normalized (in the sense of eliminating synonyms and spelling variants), and assigned to suitable categories. in the second run, this structured set of codes is used to consistently assign all relevant text passages identified before to all suitable codes. qualitative text analysis was supported by maxqda2 (by verbi software-consult-sozialforschung gmbh berlin, germany)."
"body sensor networks (bsns) are a special type of wireless sensor networks (wsns) composed of tiny biosensor nodes deployed inside or outside a patient's body to sense their physiological vital signs, that is, electrocardiogram (ecg), electroencephalogram (eeg), blood pressure (bp), electromyography (emg), respiratory rate, temperature, and oxygen level in a blood, and then securely transmitted these vital signs to a medical server (ms) through a base station (bs) for further data analysis and treatment. moreover, a patient can be monitored in real-time or non-real-time basis remotely from the hospital. in signcryption, the function of encryption and signature are performed in one single logical step. it provides 50% processing efficiency and 76.8%-96.0% less transmission efficiency as compared to ei-gamal encryption and digital signature standard (dss). 1 due to cost efficiency, it is suitable in resource-constrained environments, that is, bsns, electronic and mobile commerce, satellite communication, and mobile ad hoc network (manet). [cit] by satoshi nakamoto; with high security, the blockchain is spread rapidly in different areas like medical, internet of things (iot), smart grid, electronic voting, and wsns. blockchain is a peer-to-peer decentralized distributed network and a secure technology system which protects patient data from the alteration of adversaries. 2 in a blockchain, multiple blocks of data are connected to make a chain of blocks and create encoded digital ledger for each transaction to store patient complete data and related information in a well-organized and a systematic way for future reference and analysis. patient data stored on a server can be compromised, so the blockchain provides you the rights to control the access to the digital ledger and only the legitimate users can access the patient information, and no one can change the data of a block easily because the hash value of each block is stored in the next block along with the current block hash value and time stamp (t s ). valid users add new information in a block, so it is synchronized with each other and provides efficient security and privacy. illegal users cannot add incorrect data in a block of chain easily because the blockchain technology is a writing-once and append-only system. only an authentic user can change the block information and has access rights to the ledger. blockchain provides better security and privacy to the patient data stored in the blocks, and we can check the correctness of patient data using hash and proof of works (pow). both bitcoin and ethereum are public blockchains that have been commonly implemented in practice. 3 in recent times, due to the distributed nature, along with the lack of a third-party, blockchain technologies have achieved a protuberant reputation in the electronic healthcare system. 4, 5 we reviewed many relevant recent research results and determine that the security, privacy, and efficiency of smart health data have not been adequately addressed. the motivation regarding the use of hybrid blockchain technology in bsns is that in the traditional system, the data of the patient are only stored on a single location such as the target hospital or the ms. the traditional approach suffers from certain issues and problems such as (1) monopoly problem, (2) vulnerability problem, (3) privacy problem, and (4) integrity problem. 2 in a monopoly problem, when the data of a patient are needed to be transferred to other locations, some hospitals are reluctant to share such type of data for the sake of interest. in a vulnerability problem, there is a chance of loss of patient data because it is stored in a single location. in a privacy problem, the hospital may disclose the data of a patient to some other persons or organization for commercial purposes or benefits without prior knowledge of the patient. in an integrity problem, the hospital management can make changes in the data of patient for their benefits. our proposed hybrid blockchain technology not only overcomes the aforementioned problems but also provides the advantages of interoperability, integrity, security, traceability, and universal access. therefore, in our scheme, we will use the concept of hybrid blockchain technology because it uses the best futures of two well-known blockchain technologies: private blockchain and public blockchain. a public blockchain is further divided into two major types: (1) bitcoin blockchain, which is commonly used for a smart payment mechanism and (2) ethereum blockchain, which is commonly used for smart contract and other such type of services. in our scheme, for confidential and private data of the patient and online payment transactions, private blockchain technology will be used. similarly, for the general information of a patient, public blockchain technology will be used. in the case of a public blockchain, general payments and transactions are made through bitcoin blockchain technology, and for a smart contract or agreement between the user and the service provider, the ethereum blockchain technology is used. in our proposed heterogeneous scheme, biosensor nodes which are deployed on a patient body can reside in a certificate-less cryptography (clc) environment, while ms resides in a public key infrastructure (pki) environment. moreover, in this article, online/ offline signcryption algorithm reduces the computational burden on biosensor nodes where heavy operations are computed using offline method, and minor operations, such as addition, subtraction, and xor, are performed by using the online method. furthermore, our hybrid blockchain enhances the security and privacy of patientstreamed sensitive medical information. our scheme is suitable for resource-constrained environment of bsns, which consumes fewer resources and provides a higher degree of security and privacy along with efficiency. our scheme fulfills the security properties of confidentiality, integrity, authentication, and non-repudiation as well as provides facilities that the communicating nodes can access the encoded messages by their attributes as an alternative of identities."
"insufficient cad-pacs integration is considered a main cause of this gap [cit], where the short comings do not only refer to the technical integration but also to software usability. usability is defined by an iso standard as: \"the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use\" (iso 9241-11:1998, 3.1) ."
"future cad applications can be expected to increasingly adopt web-based technology, which facilitates the dissemination and maintenance of the respective cad functions. data and functional integration could be based on web service technology (e.g., using the simple object access protocol). nonetheless, while representing rather a design decision than a technological problem, the general alternative between \"black box\" integration and the transparent visualization of intermediate steps will not disappear by applying these new technologies."
"cad aims at speeding up the process supporting physicians with automatic image analysis. beside researchoriented approaches [cit], a commercial system became recently available providing fully automatic baa measurement [cit] . using such methods, the radiologist must majorly be provided with system interfaces transferring the images from the pacs into the cad software and resubmitting the result of automated analysis-after medical verification-into his written reading report."
"descriptive statistics (mean, median, standard deviation, max value, and min value) is calculated for each item of the questionnaire. the answers of items, which contain negative statements to be assessed by the participants (see above), are inverted: their grades get multiplied by a correction factor of −1. subsequently, the items are ordered with respect to their mean values. the resulting ranking represents a spectrum of usability aspects, which reaches from aspects positively rated by the users to aspects found increasingly problematic. box plots are generated in order to give a compact visualization of the results."
"to validate the correctness of our proposed scheme, there are two steps. in the first step, we check the correctness of the decryption, while in the second step, we check the correctness of verification."
"initial ch executes the setup algorithm on input data, that is, initial security parameters k, and assigns master secret key (m sk ) of kgc and system parameters spm to adversary (ad). furthermore, challenger (ch) assigns the adversary (ad) server public key (pu k )."
requester public key: the public key is also generated by the requester and is sent to the authenticator to verify the requester identity for the access of data.
"the cryptographic keys are used to succeed confidentiality for exchange of information between requester and server. through these cryptographic keys, a highlevel security for our scheme is achieved. requester privet key: the private key is generated by the requester, and it is used to sign the data digitally for access."
"after loading the x-ray into bonexpert, the analysis is started by clicking \"perform analysis\" (fig. 1) . to load the images, pacs integration of bonexpert can be achieved in two different ways:"
"forgery: adt ii can create a forge-encoded text (b ã ) along with a biosensor identity (id si ), and finally, adt ii wins the game if the below steps hold."
all entities which are participating in the networks for transmission of information can register their secret keys from kgc. all admitted patients in a hospital wards can transmit patient's medical information in a secure format toward the ms.
"in reality, the numbers of matching parties are often not equal. imagine reducing a member of passive party in two-side matching, such as reducing a woman in the marriage problem, or reducing a job position in the labor market, or reducing one offer in the college admission, under the assumption of random bipartite matching, the pairing result will change drastically with the smallest change of the size of passive side. but in reality, this situation is rarely seen, one of the possible reason is that, the low-fitness agents will be quickly eliminated, and other people's pairing results are almost unaffected."
"certainty and celerity of medical decision making are constitutive criteria for the accurate treatment of patients and cost effectiveness. the use of decision supporting tools such as medical software applications is an appropriate way to improve the decision processes. in diagnostic radiology, computerbased assistance in the interpretation of medical images is of particular value (computer-aided detection/diagnosis, cad) [cit] . the society of computer applications in radiology named cad as one out of six crucial interdisciplinary efforts necessary to overcome the problem of data and information overflow in radiology [cit] . the integration of the cad software into picture archiving and communication systems (pacs) is a central requirement and prerequisite for its efficient usage [cit] . however, there is a huge gap between the number of cad systems reported in scientific literature and those routinely used in radiological practice [cit] ."
"1. data integration avoids repeated entry of same data items (e.g., basic patient data that have been already entered into the hospital information system (his) is then available in the radiology information system and pacs and does not need to be captured for a second time). 2. service integration is achieved if all functions and services can be called from any place or workstation connected within the his (e.g., the radiologist can access baa-cad directly from his reading workstation). 3. presentation integration ensures that all parts and modules of the his present data and user interfaces in a consistent and likewise way (e.g., the same symbol and colour indicates the access point to patient basic data in both, the his and the cad system); 4. visual integration (context integration) means that a task only needs to be done once in the same workflow (e.g., if the patient was selected in the his; he must not be selected again within a called cad application)."
"1. suitability for the task 2. self-descriptiveness 3. controllability 4. conformity with user expectations 5. error tolerance 6. suitability for individualization 7. suitability for learning [cit] and the software usability measurement inventory (sumi), a usability questionnaire, which is mostly used for assessing new products during product evaluation, comparing products or versions of products and setting a goal for future application developments [cit] ."
"a user-centered evaluation study was performed comparing two variants of cad-pacs integration. the study design combines the think-aloud method with a structured questionnaire designed to analyze the four levels of integration, data, function, presentation, and context. the systematic design supports generalization."
"the fusion of images seeks to compensate the disadvantages of the visible and thermal spectra for face recognition. the disadvantages are caused by: lighting, posture, facial expressions, changes in the temperature in the environment, and variations in subjects' metabolic processes. this study therefore proposes the use of a fusion technique based on the pso algorithm, combining the most discriminative features from both thermal and visible images. the pso algorithm maximize the recognition rate by defining an objective function that finds the optimal particle that properly weights the contribution from the different thermal and visible descriptors."
"the system combines the information of the visiblethermal face descriptors, assigning multiplicative weights to each region of the face image. these weights are randomly created and optimized by the pso algorithm using a fitness function to increase the recognition rate."
"once the optimal weights have been obtained from the pso algorithm, the face recognition is then performed by applying the weights to the visible and thermal face descriptors. figure 6 shows the validation process."
"in this study, a novel face recognition approach based on fusing thermal and visible descriptors using the pso metaheuristic was proposed. the presented fusion method was trained with the equinox database to obtain the optimal weights for the pso algorithm, and validated using the pucv-vtf database."
"to conclude, our research adds to the body of knowledge that asserts that distinctions-in form of equivalence classes-made by qualitative formal calculi are not necessarily the ones that are foundational to the human cognitive system. several research approaches have shown that the granularity of formal calculi is inadequate for modeling human conceptualizations of both static and dynamic spatial relations. to address this issue the majority of approaches (e.g., [cit] define formal criteria on how to cluster topological equivalence classes such that the overall number of topological predicates is reduced. [cit] suggest five basic relations that are derived on the basis of the emptiness and non-emptiness of component intersection, inclusion and non-inclusion of one object in another object, and the dimension of the component intersection. [cit] developed a method based exclusively on the emptiness and non-emptiness of component intersection. coming from a database user perspective, schneider and behr developed the concepts of topological cluster predicates and topological predicate groups to reduce the number of predicates in a user-defined or application-specific manner. our research adds to this body of knowledge by having behaviorally evaluated the dline-region calculus that requires, formally, the distinction of 26 relations between a directed line and a region based on an extended version of egenhofer's intersection models. [cit] discuss several approaches on reducing the 26 relations as their primary goal is to model human concepts of motion. while we find some commonalities in the approaches they discuss and the results of our experiments, there is no complete agreement between any of the discussed approaches and our results. this demonstrates once more the importance of behavioral evaluations of qualitative calculi that is often called for (e.g., [cit] ) but rarely delivered."
"in order to perform face recognition, it is necessary to have one or more sets of images, which are contained within a database. for this purpose, different databases have been created to evaluate the performance of local descriptor methods. for this study, we use the equinox [cit] and pucv-vtf databases [cit] ."
"where j is the training image, k is the gallery image, p represent the number of partitions of the images, vd is the descriptor of the visible images and td is the descriptor of the thermal images. the weights used in the pso process are random and complementary values between [cit] and depend on the p partitions of the image."
"before beginning the experiments, it was necessary to find the parameters of the pso. the pso algorithm depends on a series of parameters that determine its performance [cit], including: intelligence constants, maximum velocity, particle swarm size, inertia factor and number of iterations. the parameters were obtained in previous experiments (not detailed here), in which they were adjusted to attain the correct functioning of the pso methodology. a summary of these parameters can be seen in the table 3 ."
"given the ubiquity of qualitative spatio-temporal calculi as tools and means to bridge the gap between a formal systems and human conceptions of space and time, our research has the potential to provide insights into necessary adaptations/modifications of qualitative calculi to deserve the label \"cognitively adequate\". our research methodology, in general, is tailored to calculi based on jointly exhaustive and pairwise disjoint relations that all form conceptual neighborhood graphs [cit] . the use of jepd and cngs in areas such as linguistics [cit], robotics, database query languages and information retrieval, and ontological modeling (e.g. for the semantic web) can be greatly enhanced by behavioral research that provides the necessary bridge between cognitive and formal spatio-temporal semantics."
"it is therefore concluded that the proposed fusion process can be used to optimally exploit thermal and visible descriptors to obtain high recognition rates for multimodal systems, generating a system that is robust to disturbances such as glasses and different types of lighting. by using pso, the method obtains optimal multiplicative weights, and when used with unknown images, it allows recognition of subjects with a high success rate. finally, in the future it should be possible to explore new solutions based on deep learning using deep convolutional networks (inception v3 [cit] and xception [cit] ) to merge visible and thermal images."
"once the descriptors vd and td are obtained from the pair of j-training images, the matching process with the database is performed. the matching process consists of combining each region of the pair of descriptors of the j-training set with each region p of the pair of descriptors of the k-gallery set. the histogram intersection measure hi () is used to combine the descriptors of each regions (see equation 3), where s j correspond to the training vectors and s k to the gallery vectors obtained from the face recognition descriptors. the process combines the visible images of the k-gallery set with all visible images of the j-training set, generating the intersection of hi (vd) histograms, likewise the thermal images of the sets generate the intersection of hi (td) histograms."
"the proposed system was divided into two stages: training and validation. the training stage was the most important process because the system learned to recognize faces using difficult sets. in order to obtain more variation in the images, random face images, images with variable illumination conditions and images with glasses were used to train the pso to obtain the optimal weights. thus, the pso algorithm can be used to efficiently and elegantly combine thermal and visible images to generate a robust face recognition system. the validation stage was then performed using the pucv-vtf database, where the best results were obtained from the following fusion schemes: fd-pso-lbp-lbp-32-c and fd-pso-hog-lbp-256-r, with average recognition rates of 99.34%. these results show a very high level of performance in terms of the recognition rates and also validate the fusion method."
"low values of ϕ 1 and ϕ 2 allow the particle to roam far from the target regions, while high values result in abrupt movements towards or past the target regions. rand 1 are the current and updated positions. for more details of the parameters of pso see [cit] ."
"in addition, the ldp-lbp scheme is a special case, since ldp operates better with 256 regions than 32 regions for the visible spectrum, see [cit] for details. thus, to perform the fusion, groups of 8 regions were taken for the visible ldp descriptor, converting the 256 regions into 32. to evaluate the face recognition system, the fused descriptor was denoted as follows: fd-m-vd-td-p-w, where fd is a fusion of descriptors, m corresponds to the metaheuristic, vd is the visible descriptor, td is the thermal descriptor, p indicates the regions used in the images and finally w indicates the type of weights used during the process: r when the weights are random, and m for complementary weights. the results of applying the fusion scheme are presented in table 4 ."
"unfortunately, the ir spectrum has the disadvantage that the energy captured by the camera not only depends on the body emission, but also from contribution of different elements present in the scene such as: the emission of the target object, the infrared radiation emitted by the background reflected by the object and the infrared radiation from the atmosphere. if these components vary, the estimated body temperature is directly affected. in the case of facial recognition, by varying the emission of the thermal face, the face recognition rate of the system will be affected. in addition, thermal images also present variations given by alterations in the metabolic processes of the subjects (such as physical work or illness), camera sensitivity to external factors and responsivity changes of thermal detectors when worked for extended periods [cit], such as the self-heating effect."
"the second experiment consists of evaluating the proposed face recognition system using the pucv-vtf database with the selected fusion methods and then comparing the results against genetic algorithms. in addition, the results of visible and thermal faces recognition methods are included, without performing the fusion scheme. note that the fusion methods used the best particles obtained by the pso algorithm in the training process, because in the validation stage re-training of the weights for the pso was not performed."
"the best combinations of descriptors were selected considering a face recognition rate over 90%. using the ea-rr sets, two fusion methods obtained the best results: fd-pso-ldp-lbp-32-c and fd-pso-lbp-lbp-32-r, with a performance of over 98%. however, when the glasses sets were used, the performance of the fusion methods decreased, as expected. in this case, the fusion method fd-pso-lbp-lbp-32-r achieved a 94.14% recognition rate, followed by the two combinations of hog-lbp: fd-pso-hog-lbp-32-r and fd-pso-hog-lbp-32-c, with ∼94% performance. in addition, the hog fusion method variants were the most affected by the use of glasses, decreasing performance. thus, for the next experiment, only the methods marked in bold were considered."
"three local matching methods were selected to generate the face recognition system. the methods were chosen considering a high degree of performance and requirements, such as working in real time; the use of only one image per person in a gallery (database) for face recognition matching; and high recognition rates obtained by the algorithms in related comparative studies [cit] ."
"pso is a metaheuristic, population-based optimization technique aimed at finding a solution to an optimization problem in a search space. [cit] . the principal objective of pso is to optimize a given function called the fitness function."
"observing the use of the random and complementary weights, the behavior of the fusion methods is not clear, since in some cases the performance increases and in other volume 6, 2018 cases decreases. thus, there is no clear tendency in the results. however, it can be noted that the combination of ldp and lbp increases the results when complementary weights are used in the fusion process."
"we have developed an approach to characterize movement patterns based on the notion of conceptual primitives [cit] ) that is built on the basic distinctions used to define topological relations between a line and a region: interior, exterior, and boundary [cit] ). [cit] in that basic topological distinctions constitute primitive distinctions that could be additionally annotated by using, for example, direction information. we consider it important to deepen this line of research as it potentially allows for linking linguistic expressions with formal spatial characterizations more flexibly and has the capability of modeling and interpreting continuous movement behavior."
"after that, the positions are updated using equation 2. 8. the fitness value is obtained. if the fitness of x ip is better than pbest, pbest is assigned as x ip . 9. the highest fitness value of pbest is assigned as gbest. 10 . if the iteration number is under 100,000 or the fitness value of gbest is less than 100%, step 7 is repeated. 11. end."
"the synthesis of all three clustering methods allows for identifying two large clusters that are identical across all three clustering methods and one cluster that requires refinement. the two large clusters identical across methods are: cluster 1 with dlineregion relations a, b, m 1, m 2, n 1, n 2, g, h, and l; cluster 2 with dline-region relations c, o 1, o 2, i, j, and k; and cluster 3, which has to be analyzed in more detail, contains on the coarsest granularity the following relations: p 1, p 2, q 1, q 2, r 1, r 2, s 1, s 2, d, e, f."
"face recognition systems usually use two sets of images, the ''gallery set'' and the ''training set''. the gallery set is a set of images stored in the recognition system, which allows the verification and identification of a person, usually having only one image per subject. the training set are images of the subjects with variations, for example in lighting and pose, used to test the identity of a subject through the entire database."
"for the training set, the visible and thermal pair of images will be called as a j-training pair. the local descriptor methods are applied to each pair of images to generate the image face descriptors. the local descriptor algorithm divides the images into p regions, where the histogram is computed for each region. the histograms of all the regions are concatenated to form a single descriptor, which contain the global and local information of each spectra. the visible descriptor will be called ''vd'' and the thermal descriptor ''td''. figure 5 shows a representation of the two descriptors from application of the lbp method. see details of the implementation of the local descriptor methods in section ii. note that the local descriptor process is also applied to the gallery to generate a database called k-gallery images, containing visible descriptors and thermal descriptors of the gallery sets."
"figure 4 reveals-on a coarse level-a three cluster solution using ward's method that has, however, to be somewhat modified if three clustering methods (ward's, average linkage, complete linkage) are compared. the result of this synthesis is indicated by the dashed-line-boxes. to additionally visualize the solution indicated by the combined analysis (i.e. the dashed-line-boxes) we have also depicted these results in figure 5 using a merged cng (compare figure 1) and using dashed lines reflecting the cluster solution shown in figure 4 ."
"the remainder of this article is structured as follows: first, we provide some background information on existing behavioral studies as well as the dline-region calculus. second, we report on the conducted experiment evaluating the divisions (topological equivalence classes) that are inherent to the dline-region calculus. the results show that we need to superimpose a hierarchy onto the 26 primitive relations to reflect human conceptualizations of movement patterns captured by this calculus. we discuss in the outlook some strategies how behavioral results can be transformed into weights for conceptual neighborhood graphs (cngs) that will cognitively adjust their use in areas such as information retrieval, ontology engineering, or the geo-spatial semantic web."
"as stated above, the training stage uses the pso algorithm to find the optimal weights in the fusion process. the training stage is composed of three processes: i) application of local descriptor methods, ii) matching, and iii) fusion of the descriptors using the pso algorithm. the whole training stage can be seen in figure 4 ."
"the proposed fusion approach considers an input pair of training images (thermal and visible) that are compared to the gallery set. for each image, face descriptors obtained from the local descriptor methods are generated: the visible descriptor and the thermal descriptor. the fusion process uses the histogram intersection as the similarity measure to combine the information of the image sets to obtain a fusion descriptor. the fusion descriptor is used to find the weights that optimize the face recognition rates for the pso process."
"the total numbers of vectors obtained in the hi (vd) and hi (td) are j * k, where j is the number of training images and k is the number of gallery images. each vector will have a length of the p divisions of the image. the final vector is the combination obtained from applying hi () to vd and td, generating a vector of size 2p. the fusion process assigns a random multiplicative weight to each region of the feature vector, modifying its information. pso is then used to fuse the descriptors, optimizing the multiplicative weights of each region in order to highlight the most relevant information of both spectra."
"to evaluate the performance of our new fusion approach using pso, we performed experiments with two databases: pucv-vtf and equinox. the experiments, like the methodology proposed, were divided into two stages. firstly, the experiment was carried out to obtain the best combination of weights to fuse the visible and thermal descriptors using the pso metaheuristic. note that the experiments used three local matching methods in order to generate different combinations of local descriptor methods. the steps described in section v were followed to obtain the fusion process using pso."
"finally does seem indicates takes plac fig. 4 nd a compara at is, cluster tually not the re formed by r d figure 5 . our research focused on english speaking participants but we will discuss the opportunity to use our research framework in cross-linguistic studies in the outlook."
"this face recognition system has two stages: a training stage and a validation stage. the training stage finds the optimal multiplicative weights using the pso metaheuristic, which will be applied to each region of the visible and thermal image, maximizing the face recognition rate. after that, the validation stage takes the optimal weights and applies them to the gallery and test images to generate the face recognition."
"the experiments were performed using the top recognition criteria, finding the most similar face image as a success. thus, to determine the performance of this database, different test sets (glasses, frown, vowels, and smile sets) were compared with the normal set (gallery) using the fusion methods obtained from table 4 (marked in bold). see section iv for details about the pucv-vtf database. table 5 shows the results of applying the fusion methods previously trained with the equinox database in terms of average recognition rate (arr). note the high performance obtained from the combinations of lbp-lbp and hog-lbp. the fusion methods based on fd-pso-lbp-lbp-32-c achieve over 99% recognition rates, surpassing the results even when the fusion methods were trained with the glasses sets (eg and vg). the method fd-pso-hog-lbp-256-r also achieved ∼99% recognition. the other fusion combinations such as lbp-hog, hog-hog and ldp-lbp, do not pass the ∼99% recognition rate."
"in general terms, the performance of the fusion methods is high for the validation stage, mainly because the pso algorithm was trained with the equinox database, which has difficult cases (glasses, illumination variations), adding robustness to the system."
"face recognition systems in the visible spectrum have been studied for decades, focusing on the areas of pattern recognition, computer vision, robotics and biometry. this area of research has grown steadily due to its diverse applications in security such as identification and access control [cit] . nonetheless, researchers must overcome the diminished recognition ability of algorithms due to variations in the type and intensity of illumination in the visible images. in realistic applications, where lighting intensities may vary due to outdoor conditions, weather or time, the recognition of faces using visible images may not have the necessary robustness, due to decreased recognition rates [cit] ."
"the importance of this statement is difficult to overestimate. if this statement is true, then we have found-by using qstr-a way to bridge the gap between cognitive (semantic) information processing and requirements of formal systems fundamental to modern information technologies. from our usage of the word \"if\" the reader may derive that it is, unfortunately, not that easy. there are numerous suggestions for qstr that all identify-sometimes contradictory-divisions of quantitative space. additionally, while often a claim is made that qualitative approaches bear some inherent cognitive/commonsense aspects of how spatio-temporal information is processed, there is a paucity of actual behavioral evaluations that would back up the claim of capturing cognitive aspects of spatio-temporal information processing using qstr. hence, we do need an experimental framework that allows for effectively and efficiently assessing qstr approaches cognitively. this article details an experiment assessing the dline-region calculus [cit] while at the same time showing general ways of assessing the cognitive adequacy of qstr approaches that are built on jointly exhaustive and mutually exclusive relations."
"metaheuristics are intelligent strategies to solve, improve or design heuristic procedures of a specific problem. in this study, two metaheuristics based on the behavior of a population were applied. they were selected for their different applications and high performance in face recognition [cit] ."
"the results shown in table 4 correspond to the recognition rates in the training stage of the proposed fusion system. the highest face recognition rates for the different fusion methods proposed with the different combinations of sets are highlighted. each fusion method generates a pbest vector of 30 particles which represents the best weights applied to the regions of the images, with the same face recognition rates. note that each particle contains different weights."
"given the space limitations we will focus on some essential behavioral studies and a brief introduction to the dline-region calculus. of particular importance to the topic of this article are the experiments by mark and egenhofer (e.g., [cit] on topologically characterized relations between a line and a region. in their experiments, the line had no direction and they used static images (rather than animations, see section 3). they focused on both cognitive conceptualization processes as well as the spatial semantics of linguistic expressions. in some of their experiments they employed a grouping paradigm [cit] b), similar to the approach taken in the experiment we will present later on. their findings crystalized in the famous statement that topology matters and metric refines . not as widely discussed but equally important is their finding that the 19 relations between a line and a region are cognitively not primitive relations. in other words, the 19 relations form conceptual groups (clusters) with larger within group similarity as well as larger between group dissimilarity. this aspect has also been addressed in formal research papers and we will come back to this aspect throughout the paper."
"the recognition is performed by finding the maximum similarity value for each image j, among the k values obtained from the comparison of the training image j with each gallery image k. 6. the particle that has the best fitness inside the pbestvector is assigned as gbest which then leads the cluster of particles."
"once the optimal weights were determined, the second experiments used these optimal weights to perform the face recognition. the weights were applied to the pucv-vtf database. finally, the results obtained of the fusion process using pso were compared with a genetic algorithm, in order to compare the face recognition rates with different data sets."
"future research directions are manifold given both the ubiquity of qstr in research and application and the paucity of behavioral evaluations. the following ones strike us as important: we have recently demonstrated that domain semantics has a meaningful influence on the grouping behavior of participants, that is, which original topological relations form cognitive conceptual clusters (klippel in press) . to this end, a theory is needed that would allow for specifying meta-domain characteristics and how they influence cognitive conceptualizations of movement patterns. mark and egenhofer (1994b) raised already the question of the influence of language on the conceptualization of line-region relations. linguistic (and potentially cultural) influences surface in the spatial science sporadically [cit] for a more substantial treatment) but are still not integrated into core theories. a transdisciplinary research agenda is needed to deliver results that could influence spatial theories more fundamentally."
"furthermore, we employ exit charts to design the outer ldpc codes while fixing the inner nltcs. several design examples are provided, and their performances are evaluated. the results indicate that the designed codes operate at about 0.8 db away from the information theoretic limits, and they outperform both regular ldpc codes and optimized irregular ldpc codes for awgn channels when used with nltcs for joint energy and information transfer. in order to have a practical code design, we employ small degrees in both check and variable nodes, however, we expect that by using larger degrees the gap to the information theoretic limits can be decreased. furthermore, our results also show that the designed codes outperform the alternative of using classical linear block codes with time switching and the reference schemes of concatenating ldpc codes with nonlinear memoryless mappers considerably."
"type 1 diabetes is caused by the patient's inability to produce any insulin. the more common type 2 diabetes is caused by the patient's immunity to insulin, preventing its uptake by the body's cells. rates of type 2 diabetes are increasing in developed countries due to an aging and increasingly overweight population. in the united states, for example, according to the centers for disease control, approximately 23.6 million people, or 8% of the population, have diabetes, of which 95% suffer from type 2 diabetes. the total prevalence of diabetes increased by 13.5% [cit] . 8 uncontrolled diabetes can cause severe long-term health problems such as renal failure, blindness, and arterial disease. these problems are expensive to treat, and as the number of sufferers climbs, the medical authorities are finding it increasingly difficult to pay the bill."
"moreover, savings are likely to become even greater if predictions come true that over 600 million people worldwide will suffer from chronic diseases and that spending on such diseases will increase. for example, in the united states alone, without significant intervention, spending is expected to increase from the current $500 billion a year to $685 [cit] . 3 until now, no wireless connectivity technology met all the requirements needed for widespread adoption. these requirements are summarized below:"
"wireless medical monitoring could be used in conjunction with cgm as an emergency communication system for episodes of hypoglycemia. provided the user cancelled the alarm-and ate some food-the wireless monitoring system would take no action. but if for some reason the alarm was not cancelled (for example, because the diabetes patient has collapsed), the wireless monitor could automatically communicate with the diabetes patient's cell phone to send a sms to a nominated contact or even emergency services."
"then, we arrange each set of four branches with consecutive indexes (first one starting with branch with index zero) in two subgroups and represent them using two blocks a l and a * l as follows"
"once communication has been established, the devices begin to rapidly hop between 37 dynamic data channels within the 2.4 ghz band hundreds of times per second in a synchronized pseudo-random pattern to minimize the likelihood of trying to communicate on the same frequency as another radio source. if a clash occurs, the bluetooth low energy transceivers jump to another channel in a matter of milliseconds and \"mark\" the corrupted channel so it is not reused. classic bluetooth uses a similar scheme (albeit with 79 channels) and has been proven to work reliably in thousands of use cases."
"after consideration of required power levels, cell phone ubiquity, required range, and anticipated market penetration, the alliance chose bluetooth low energy (pending finalization of the specification) for medical wireless monitoring of a user's health and fitness levels. additionally, continua chose zigbee health care technology for low-power sensors for local area networks of devices such as motion detectors and bed-pressure sensors."
"the examples above show the importance of the ldpc code optimization for the specific inner nltc, and illustrate that large performance improvements can be obtained by using optimized degree distributions for each inner code."
"appendix a proof of theorem 1 the sufficiency part is straightforward. we assume that either one of these two conditions is true and then we find two different paths of infinite length with finite number of differences in the output and infinite number of differences in the input sequence. first note that starting from any state one can find a path of length m to any other state in the state diagram (due to the specific structure of the evolution of memory). next if condition 1 is true then we consider the two different states of the cycle as state a and b, or if condition 2 is true we consider state a from the first cycle and state b from the second one. we can find two different paths with starting part of length m from state zero to state a or b and final state transitions the other way around, the middle part can be arbitrarily long consisting of traversing around the cycle corresponding to state a or b. the starting and ending parts will result in finite number of differences in the output, and for the middle part which is arbitrarily long the output sequence is the same for both paths but the input sequences have arbitrarily large number of differences."
"wireless medical monitoring with bluetooth low energy will initially be targeted at bg measurement, but applications such as body temperature, blood pressure, pulse oximetry, and heart rate will follow shortly after. [cit] ."
"abstract-harvesting energy from radio frequency signals along with transmitting data through them is appealing for different wireless communication scenarios, such as radio frequency identification (rfid) systems and implantable devices. in this paper, we propose a technique to design nonlinear codes for the use in such systems taking into account both energy transmission and error rate requirements. in particular, we propose using concatenation of a nonlinear trellis code (nltc) with an outer low-density parity-check (ldpc) code. we design the nltc based on maximization of its free distance. we give necessary and sufficient conditions for its catastrophicity; in order to avoid catastrophic codes, we connect each designed nltc to a corresponding linear convolutional code allowing for the use of simpler conditions for verification. furthermore, we use exit charts to design the outer ldpc code while fixing the inner nltc. via examples, we demonstrate that our designed codes operate at ∼0.8 db away from the information theoretic limits, and they outperform both regular ldpc codes and optimized irregular ldpc codes for additive white gaussian noise (awgn) channels. in addition, we show that the proposed scheme outperforms the reference schemes of concatenating ldpc codes with nonlinear memoryless mappers and using classical linear block codes in a time switching mode."
"however, manufacturers would have to embark on a program of testing to ensure their products meet the electromagnetic compatibility requirements of the medical authorities. (note that, because the bluetooth sig has newly adopted bluetooth low energy, no test data are yet available for presentation in this article.) perhaps more importantly, medical wireless monitors need a high degree of immunity from other radio sources to ensure communications are not interrupted. to do this, bluetooth low energy employs a frequency-hopping spread spectrum interference avoidance scheme. when making an initial connection, the two transceivers transmit using one of three fixed channels (trying the other two in turn if no signal is received) in order to establish the link. this is a faster and more power-efficient method of searching for a compatible radio source compared to scanning the whole band."
"to ensure data remain confidential, bluetooth low energy inherits the encryption, authentication, and authorization security from classic bluetooth. the encryption technique uses the advanced encryption standard-128 algorithm. 6 (advanced encryption standard is an encryption technique adopted as a standard by the u.s. government.)"
"bluetooth low energy features two implementations, namely, \"dual mode\" and \"single mode\" (see figure 1 ). single mode devices are compact radio communication units suitable for incorporation into wireless medical monitors measuring just tens of millimeters in size."
"in the case where more than one such subset of labels are available, we select the one that results in the largest lower bound on the minimum distance."
"in a convolutional code with two distinct labels, if we map one of the output labels (0 or 1) to two different output labels (each one-half of the time), then we will obtain an nltc with three distinct labels in which one of the labels is used with twice the frequency of the other two. it is straightforward to show that the resulting nltc will again be a catastrophic code. 1 the above two theorems and the corollary allow us to avoid catastrophic codes in a simple way, and they are utilized in the next section for our specific design examples."
"power consumption is very low, allowing such medical monitors to run for many months or even years on standard coin-cell batteries. for example, in an application transmitting bg level measurements once every minute, on a continuous 24 h/365 [cit] coin cell will have a battery lifetime of at least 1.5 years."
"wireless technology promises benefits for medical monitoring applications by freeing patients from inconvenient and restrictive wires. further, wireless monitoring that can communicate with remote physicians via existing infrastructure could allow patients (especially the elderly) to remain in their homes while still under medical supervision. this promises to reign in escalating health care costs."
"dual mode devices are radio communication devices targeted at handsets and personal computers (pcs). it is proposed that cell phone makers will use these devices when they become available, because they cost only slightly more than the current type (classic bluetooth)-used today in around 70 percent of handsets-but offer consumers much more functionality. the most important added functionality of a cell phone or pc equipped with a dual mode bluetooth low energy device is that it will be able to communicate directly with single mode devices. consequently, medical data can be sent from a wireless monitor to a cell phone or pc and from there to a remote physician (see figure 2 )."
"given the selected subset of binary labels, with the goal of maximizing the minimum distance between the codewords, we perform set partitioning. in order to do this, we first partition the labels into pairs in such a way that the minimum pairwise hamming distance between labels in each pair (d (1) min ) is maximized. then, we partition the pairs into groups of two pairs such that the minimum pairwise distance between the quadruples (d (2) min ) is maximized and continue partitioning in this manner. we denote the minimum pairwise hamming distance of groups of 2 i labels as (d"
"theorem 1: a nonlinear trellis code for which the trellis is defined based on sequence of shift registers is catastrophic if and only if one of these two conditions occur: 1) there is a cycle in its state diagram such that starting from at least two different states of the cycle and traversing around results in the same output sequences corresponding to different input sequences, 2) there are at least two different cycles in its state diagram with different input sequences giving rise to the same output sequence."
"the first version of bluetooth low energy will include hdps for medical (and fitness) applications such as body temperature, blood pressure, weight scale, glucose, pulse oximetry, heart rate, pedometer, speed, distance, cycle cadence, simple remote control, and battery status."
"theorem 2: for the design in section iii, there exists a oneto-one mapping (which is not necessarily unique) from the full set of binary labels with h bits to the group indexes such that assignment of the corresponding binary labels to the branches inside the group results in a linear convolutional code. the resulting convolutional code is called the corresponding convolutional code of the original nltc."
"proof: first assume that the corresponding convolutional code is catastrophic, by definition there is a cycle in its state diagram with a nonzero input sequence which corresponds to the all-zero output sequence. we also know that every convolutional code has a cycle from state zero to itself with zero input and all-zero output. due to the one-to-one mapping between labels, the nltc will have two different cycles with different input sequences but the same output sequence which means that nltc is catastrophic. conversely if nltc is catastrophic, from the definition at least one of the following conditions is true, 1) there is a cycle in its state diagram such starting from at least two different states of the cycle and traversing around, results in the same output sequence, 2) there are at least two different cycles in its state diagram with same input sequence but different output sequences. again because of the one-to-one mapping in either one of these cases, the same condition for the convolutional code will also be true which show that the corresponding convolutional code is prone to catastrophic error propagation."
"case 1: since the trellis has a finite number of states and a finite number of branches, the number of different paths with the same output in the state diagram is finite, this means that at least one of them must repeat infinitely many times in each sequence, and in order to repeat a path we need to go back to its starting point which means we have traversed around separate cycles infinitely many times with each one of the two sequences and these separate cycles having different input sequences but the same output sequence."
"a trade-off between transmission of energy and information emerges when the amount of received energy differs for different channel input symbols (which is not the case for bpsk modulation). a simple model that makes this trade-off clear is using on-off signaling which has already been studied in some information theoretic works [cit] . for a more general case one might consider transmission of any set of symbols with different energy levels (amplitudes) such as qam modulation. here, we consider the case of on-off signaling in which only two symbols \"0\" and \"1\" are used, and with the primary objective to complement the existing information theoretic results, we investigate the joint energy and information transfer from a communication theoretic perspective."
"the main step in the nltc design is to assign output values to the branches of the trellis to maximize the minimum distance of the code while keeping the desired ones' density. in our design, assignment of output labels to branches is performed according to the extended ungerboeck's rule [cit] which maximizes the minimum hamming distance of the code. ungerboeck noted that every incorrect codeword, in its trellis representation, departs from the correct path (split) and returns to it (merge) at least once, so he maximized the distance between splits and merges. one can extend the ungerboeck's rule further into the trellis and maximize the hamming distance between the branches emanating from a split h trellis sections before, where h is a natural number that can be at most m. the same procedure can be followed for the branches that merge h sections later. having the set of 2 h distinct labels partitioned in the previous sections for a rate 1 n 0 code, we can assign the labels according to the extended ungerboeck's rule as follows:"
"classic bluetooth's hdp is a \"one-size-fits-all\" solution that caters to all types of medical products. consequently, classic bluetooth's hdp is a large program requiring a lot of memory and battery power. bluetooth low energy has a number of hdps customized to a given application or applications. a medical product designer can select one or several profiles to suit their specific application, reducing the memory and power overheads. consequently, a bluetooth-low-energy-equipped end product will be simpler, cheaper, and more power efficient than would be possible with an equivalent classic-bluetooth -equipped device."
"in addition to security protection, bluetooth low energy also employs privacy protection in order to stop \"tracking\" by unauthorized receivers. this is done by limiting the ability to track a transmitting device through the use of a random device address that is changed frequently."
"of all the commercially available wireless technologies, only bluetooth low energy is capable of meeting all the requirements for medical applications, specifically, interoperability, low-power operation, electronic compatibility, secure data transmission, and direct communication with cellular and internet infrastructure."
"wireless energy transfer and wireless information transmission have previously been considered as separate problems. however, recent work on dual use of rf signals for delivering energy and information demonstrates that there is a natural trade-off on the design of such systems [cit] . for systems with joint energy and information transfer it is of interest to increase received power levels and information rates at the same time. the authors are with the department of electrical and electronics engineering, bilkent university, ankara 06800, turkey (e-mail: mehdi@ee.bilkent.edu.tr; duman@ee.bilkent.edu.tr)."
"a bg meter measures bg levels from a sample deposited on a test strip. modern units hold information in a memory base for later recall at regular health checks. patients are advised to record bg levels several times a day-more frequent measurements result in better control, as diet, exercise, or insulin injections can be adjusted quickly to stabilize high or low levels (see figure 3) . bluetooth low energy built into a bg meter offers several advantages in the management of diabetes. data from the bg meter could be uploaded frequently to the patient's cell phone and from there to the physician's computer for review. an analysis of bg measurement trends would allow the physician to spot persistent out-of-normal-range episodes much earlier than the typically quarterly reviews allow and to advise on modifications to the diet either via phone call or short message service (sms; also known as \"texting\") to a cell phone, for example, on a weekly basis."
"the aim of this section is to describe an algorithm to arrange the trellis branches in 2 h groups (where h is the number of trellis sections for which the ungerboeck's rule will be extended after each split and before each merge) and to assign partitioned labels to these groups with the same order in which they appear in the partition tree. first, we number the states in natural order starting from zero and assign indexes to outgoing branches from each state as follows: for state number"
. after completing grouping of these blocks we will need to assign the i th pair of labels from the partition tree to the group with index i to complete the label assignment process.
"that said, bluetooth low energy operates at 1 mw (0 dbm) output power-a modest output that falls well below the fcc's guidelines of 125 mw (\"maximum peak conducted output power of the intentional radiator\"). in addition, the technology only transmits for 1% (or less) of the time and then only in short bursts lasting a few hundred microseconds. most modern medical electronics have been designed with a high degree of emi immunity and are very unlikely to malfunction in the presence of such a low power, short duration transmission."
"heal th care reform is a hot topic; governments across the 30-member-nation organization for economic co-operation and development are looking for ways to cut costs without compromising patient care. the organization's statistics reveal that, across a sample of 15 of its member nations, from a baseline of 100 [cit], while gross domestic product had climbed to 205 [cit], health care spending had skyrocketed to 280. 1 a commercial report 2 concluded that the use of body-worn wireless electronics monitors could save the health care industry $25 [cit] . these wireless monitors that could link to existing cellular or internet infrastructureby radio communications in an unlicensed portion of the electromagnetic spectrum (2.4 ghz)-would allow patients to shorten or avoid hospital stays while still being in frequent electronic contact with their health care providers. wireless monitoring could also replace expensive home visits from community nursing staff to take routine measurements, because medical data could be sent via cellular or internet infrastructure."
"assuming that the subset of labels has size 2 h, we obtain a partition tree with h levels. there may be many ways to accomplish this, however, we select one of the possibilities that maximizes"
"proof: see appendix a. catastrophic codes clearly need to be avoided to achieve good error correction capabilities, however, checking for the conditions mentioned in theorem 1 when the memory of the trellis grows can be very complicated. we already know catastrophicity conditions for linear convolutional codes [cit], that is a convolutional code is catastrophic if and only if its corresponding state diagram contains a circuit in which a nonzero input sequence corresponds to an all-zero output sequence. therefore, checking for catastrohicity of convolutional codes and avoiding such codes is much simpler compared to a nonlinear trellis code. fortunately, for the specific design (grouping) that we introduced in the previous section, we can show that checking for the code being catastrophic can be performed in an easy and systematic manner by connecting the design to that of standard convolutional codes. in the following, we introduce two theorems and one corollary in order to detect and avoid catastrophic codes in our specific design in section iii-c."
for the necessity part we know that we have two input sequences with infinite number of differences with corresponding output sequences of finite number of differences. we separate the parts in which two input sequences are different but their corresponding outputs are the same. two situations can occur: 1-there are infinite number of such finite-length subsequences of different inputs that have same corresponding output. 2-there is at least one infinite length subsequence of different inputs for each sequence that have same corresponding output. for each case we can argue that at least one of the catastrophicity conditions must be satisfied.
"in addition, the computing power of the cell phone allied to an internet-downloadable application could highlight trends and advise the patient to modify their exercise regime, for example, to take a 20 min walk just after lunch. as a complement to regular medical consultation, such feedback would help the patient to better manage the condition, and management of diabetes is the critical step in preventing complications and drastically cutting long-term health care costs. bluetooth low energy is capable of communicating with a web-based application without using either a cell phone or pc by using a bluetooth router (a device that acts as a \"gateway\" between the bluetooth low energy device and the internet). the web-based application can send messages back to the bluetooth low energy device. this functionality would be useful if, for example, the diabetes patient has left their cell phone in the car."
"we note that a traditional information receiver architecture designed for information reception is not able to harvest energy 0090-6778 © 2016 ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"the bluetooth sig has developed \"health device profile\" (hdp) software to optimize performance of classic bluetooth for health applications and deliver data in a standard format requested by the medical authorities. similarly, the sig is developing hdp software for bluetooth low energy."
"the fda states, \"wireless coexistence and data latency remain concerns because the data transfer rate can slow slightly or even dramatically with an increase in the number of similar transmitters in a given location. in many cases it is essential that medical data, including real-time waveforms and critical control signals and alarms, be transmitted and received without error.\" 4 worse, bluetooth low energy radios broadcast in the notoriously crowded 2.4 ghz band. licensing bodies such as the federal communications commission (fcc) have attempted to mitigate the effects of emi by restricting the power output of radio devices operating in the license-free parts of the radio spectrum to limit the possible emi with sensitive electronics. 5 operating in the 2.4 ghz band is a major challenge with consequences for product design."
"the second part of the proof follows using induction. as the initial step of the induction it can be shown that the claim is true for any branch inside blocks 0 to 2 h−1 − 1. then in the second step of induction by assuming that the claim is true for any branch inside blocks 0 to 2 i−1 − 1, it can be shown that it is also true for any branch inside blocks 2 i−1 to 2 i − 1. we relegate the details of the proof to appendix b."
"nonetheless, significant testing will be required to ensure emi between bluetooth low energy products and other electronic devices causes no problems. that said, bluetooth low energy is a very low-power transmission technology and uses sophisticated frequency-hopping algorithms that minimize the likelihood of interference."
". we assume that the receiver needs to harvest at least a certain amount of energy on average. in order to provide this required energy at the receiver side, we place a constraint on the average ones' density p at the channel input, i.e., on the coded symbols. therefore, our aim is to design practical codes with a predetermined constraint on the average ones' density of the transmitted codewords."
"the infinite length subsequence needs to contain cycles (since trellis has finite number of branches) that are repeating infinitely many times and have different inputs, hence, there are separate cycles in each subsequence with different inputs but same output (separate cycles can be due to a single sequence of paths starting from different states)."
"the interoperability requirement discounts any of the commercially successful proprietary technologies from being adopted for wireless monitoring in health care applications, because there is little or no chance that products from different manufacturers will be able to communicate. [cit] ."
"we refer to an nltc that is prone to catastrophic error propagation as a catastrophic code for which a finite number of channel errors may cause an infinite number of decoder errors. with this reference, we derive the necessary and sufficient conditions for catastrophicity of nonlinear trellis codes which is stated in theorem 1."
"good management of diabetes is one way to mitigate the cost of treatment, because it delays or even prevents the onset of related health complications. management depends on frequent and accurate measurement of bg to maintain normal levels (a fasting range of 4 to 6 mmol/liter)."
"at the end, after completing the grouping of the blocks, we assign distinct labels to each group. we use the partitioned labels obtained previously and assign them to the groups with the same order in which they appear in the partition tree."
"continuous glucose monitoring using wireless monitoring has major benefits to type 1 diabetes sufferers, because they are more prone to short-term complications, such as very low bg (hypoglycemia), that can occur between routine periodic measurements from finger pricks, leading to coma if not treated rapidly. it would be relatively simple to set thresholds (for example, at a bg level of 4 mmol/liter) to warn the user of danger."
"where a h,i ...a 1,i and its one's complement are, respectively, the corresponding outputs for subgroup (0) and ( in the following, in two parts, we will show that for any branch both generator polynomials and the look-up first we show that if the claim is true for the first branch inside a block then it will also be true for the rest of the branches inside that block. the second and third branches inside a block have the same output which is one's complement of the first branch's output. the same will be obtained by generator polynomials since the only change for second branch is that the input is changed from zero to one for the same state and for the third branch s 1 (lsb of state value) is changed from zero to one for the same input. the fourth branch and the first branch have the same output which can again be obtained by generator polynomials since now both input and s 1 are changed from zero to one and they cancel each other at every bit of the output. hence, without loss of generality, we can check the claim for the first branch inside each block (which corresponds to even states with input zero) and make sure that the rest will be correct if the first one is correct."
"better diabetic control can be achieved by continuous glucose monitoring (cgm). continuous glucose monitoring relies on very frequent measurement (for example, 288 glucose measurements every 24 h) of bg. one fdaapproved cgm device 9 uses a tiny glucose-sensing device inserted under the skin of the abdomen. the system automatically records an average glucose value every 5 min for up to 72 h."
"adio frequency (rf) energy harvesting is a wireless power transfer technique which relies on collecting energy from the radiated rf signals at the receiver for use in information processing and transmission processes. potential applications of rf energy harvesting can be found in different areas including wireless sensor networks, wireless body networks and wireless charging systems [cit] ."
"in this paper, a coding scheme based on concatenation of a nonlinear trellis code with an outer ldpc code for joint energy and information transfer is proposed. in order to design the nltcs, an algorithm based on maximizing the minimum distance of the code is provided. also, necessary and sufficient conditions for catastrophicity of nonlinear trellis codes are obtained; and, in order to avoid such catastrophic codes, each designed nltc is connected to a corresponding linear convolutional code. this allows for the use of simpler conditions for checking for the catastrophicity of the designed nltc."
"the rest of the paper is organized as follows. in section ii, the channel model is described, information theoretic limits for the considered scenario are given and the proposed scheme of concatenation of an outer linear block code with a nonlinear trellis code is presented. the design of nonlinear trellis codes for our purposes is then introduced in section iii. ways of avoiding catastrophic codes are discussed in section iv. exit charts and ldpc code optimization are detailed in section v. in section vi, several numerical examples are provided, and finally, the paper is concluded in section vii."
