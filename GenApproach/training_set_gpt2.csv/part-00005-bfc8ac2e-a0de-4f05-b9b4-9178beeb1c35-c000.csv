text
"in this paper, we jointly investigate the siso and cc transmission schemes in both single-hop and multihop scenarios. the optimal number of cooperative nodes and the broadcasting ber are obtained for the energy efficiency. it is shown that cooperative communication is more suitable for the longdistance transmission in harsher environment. the conclusion of single-hop network is then expanded to multihopclustered network where we study the energy cost of different nodes (cluster head, cooperative nodes, and plain nodes) in the cluster. finally, we prolong the network lifetime by adjusting the transmit ber along the delivery path. an interesting extension is to precisely study the cooperative nodes selection scheme, since the probability is slightly different between nodes to be covered by broadcasting (the node at the core of the circle cluster is easier to be under the convergence)."
"analogy to the relationship of broadcasting ber and cooperative transmission ber is indicated in (13) . we have (19) is approximated to be, +, ⩽ 1 − . expanding this procedure to following hops, we can acquire"
"evidently, the cluster nearest to the sink dies much earlier than the clusters farther away which leads to \"energy hole, \" since the nodes in cluster 1 are burdened with larger amount of data. we notice that the reduction of power consumption at the energy hole leads to the prolongation of network lifetime. to mitigate this \"energy hole\" as well as maintain the statistical reliability, a strategy is proposed to convert the energy consumption at the energy hole to the farther part of the network by adjusting the transmission ber in each cluster. based on the analysis in theorem 3, the sum of ber along the routing path stays stable and the accuracy of the data can still reach the requirement of reliability. by means of this method, the energy consumption of the nearer clusters is reduced although the cost of the external clusters increased. as long as the maximum energy consumption declined, the network lifetime is optimized. through the calculation of multihop algorithm, figure 13 plots the transmit ber of 1 for the data from different clusters (,1 ) compared to the originality. since the energy expenditure of the clusters farther away from sink is lower than cluster 1, ber for 1, to transmit data is switched larger in order to balance the power cost. while to maintain the reliability, the ber of the farther cluster is relatively lower. thus, the energy consumption of peripheral clusters increases and that of 1 has declined as shown in figure 14 . meanwhile, the longevity of network is improved (in case the initial energy of the nodes is 1, the lifetime is optimized by 9.85%)."
"wsn (wireless sensor network), an energy-constrained network, has nodes mainly powered by batteries which are hard to replace even if possible. numerous applications of wsn, such as environment monitoring, always need the network to operate for years without exchange of power suppliers. the prolongation of network lifetime is hence a critical design consideration and the data transmission must be energy efficient. more specially, the sensors near the sink are likely to die earlier since they are burdened with higher data load. their deaths lead to the dysfunction of the network with the residual energy in the outside nodes. this is the well-known \"energy-hole\" phenomenon, the core of many researches in the literature [cit] ."
"mimo (multiple-input and multiple-output) explores the spatial diversity of the wireless channel which can dramatically increase the channel capacity as well as the reliability of transmission. once the transmission distance reaches a certain threshold [cit], the energy conversation performance of mimo systems can remarkably exceed the siso (singleinput-single-output) systems under the same signal-to-noise ratio (snr). the mimo energy-efficiency transmission scheme is particularly useful for wsn due to the limited energy supplied. however, the direct application of multiple antennas technique on wsn is impractical for the insufficient physical size of sensor nodes. fortunately, several individual sensors can cooperate for the data transmission in order to set up a cooperative mimo or miso scheme, which are also known as cooperative communication (cc) [cit] ."
"cluster. during the intracluster process, the plain nodes in cluster transmit bits data to the cluster head with ber, in one round. then, ch aggregates the data and chooses the transmission scheme based on singlehop algorithm. if the cooperative communication is selected, ch broadcasts the data to the neighbors. the internal clusters are responsible for the relay of data stemming from outer clusters ( 2 and 3 in figure 10 ) in intercluster process. notably, the notations in section 3 are expanded in this section."
"proof. nodes undertake the role of ch by cluster head rotation. averagely, every node acts as ch for one time, as plain nodes for − *, times after data gathering round. in particular, the number of cooperative nodes depends on, thus we consider the cooperative nodes in cluster separately according to the intercluster transmission scheme. thus, (28) can be derived."
"after the broadcasting phase, the cooperative nodes and the relay node transmit the data to the destination with ber, the total energy consumption in this phase is"
"(2) in a multihop network, the sensors closer to the sink are more likely to be exhausted earlier due to the heavier data load. based on the analysis of the single-hop scenario, we propose the multihop algorithm to prolong the lifetime of cluster-based network subject to the requirement of statistical reliability. our strategy adjusts the transmission ber higher at the clusters farther away from the sink than the inner ones. this enables the near-sink cluster to lose the requirements of reliability. on one hand, the overall requirement can be met. on the other hand, the energy consumption of the near-sink clusters is shifted to the farther clusters."
"where is the fusion rate. and ag, denotes the energy consumption of data aggregation of cluster head (ch) in cluster . theorem 2 presents the analysis of energy consumption for each type of nodes:"
"the ber in each step greatly influenced the energy consumption performance as we see in section 3. moreover, the overall ber consists of two parts, the ber at data gathering phase and the ber induced by the intercluster data transmission, respectively."
3.2. the energy consumption of cc. the broadcasting radius of the relay node is . the energy consumption for the broadcasting with ber and the reception of the candidates can be derived as
"the rest of this paper is organized as follows. the related work is given in section 2. section 3 presents the analysis of the single-hop network with cc scheme and singlehop algorithm. the numerical and experimental results are shown in section 4. we further evaluate the energy consumption performance in a multihop clustered network, and multihop algorithm is presented to mitigate the \"energy hole\" by adjusting the transmission ber in section 5. section 7 concludes the paper."
"in our paper, we assume that the ch and the cooperative nodes are selected based on the residual energy of the nodes. therefore, it is considered that the energy consumption among the nodes are perfectly balanced, thus all nodes have approximate lifetime. theorem 3 derives the average energy consumption of each clusters. theorem 3. the average energy consumption per node in the ith cluster for an entire data gathering round is presented in the following:"
"cc scheme explores the energy efficiency of multiantennas technique which plays a significant role in the long-range transmission, where the transmission energy consumption dominates in the overall cost rather than that of the circuit [cit] . nonetheless, the decline of transmission energy consumption does not directly lead to the prolongation of network lifetime owing to the existence of \"energy hole\" [cit] . the residual energy in the farther nodes may be up to 50% when the network dies [cit] . thus, the energy consumption balance is also the critical topic in the design of transmission scheme. in this paper, we first propose singlehop algorithm for the minimization of energy consumption in single-hop scenario (see algorithm 1) . furthermore, we generalize the conclusion to the multihop scenario and present the multihop algorithm to mitigate the \"energy hole\" by adjusting the bit error rate (ber) at each cluster (see algorithm 2) ."
"traditional epidemiological research has focused on rate-based differential equation models of completely mixed populations, that is, all the individuals are allowed to interact with each other (see bailey [cit] and hethcote [cit] for a comprehensive discussion on this subject). an attractive feature of this modeling approach is that it allows one to obtain analytical expressions for a number of interesting parameters such as the numbers of infected, recovered, and susceptible individuals in a population. but such a modeling approach does not capture the complexity of human interactions that serve as a mechanism for disease transmission. typically the number of different subpopulation types considered is small (for analytical tractability), and parameters such as mixing rate and reproductive number are either unknown or hard to observe. over the past several years, largescale, individual-based, disaggregated models have been studied [cit] . these new models use an endogenous representation of individual agents together with explicit interactions between these agents to generate and capture the disease spread across social networks."
"in step 2, a set of activity templates for households is determined, based on several thousand responses to an activity or time-use survey. the modeling methodology is called activity-based travel-demand modeling and is now accepted as the de facto standard in transportation science [cit] . our early work in this area [cit] played an important role in the development of this methodology. the activity templates include the activities each household member performs and the time of day they are performed. each synthetic household is then matched with one of the survey households, using a decision tree based on demographics such as the number of people in the household, number of children of various ages, and income. the synthetic household is assigned the activity template of its matching survey household. for each household and each activity performed by this household, a preliminary assignment of a location is made based on observed land-use patterns, tax data, and so on a social network is formed out of these activities as agents mix together with some level of contact, at the locations visited, at various times throughout the day. the dynamic social contact network represented by a (vertex and edge) labeled bipartite graph g pl, where p is the set of people and l is the set of locations. if a person p ʦ"
"three separate studies were conducted for the dod regarding military preparedness and force readiness. the studies elucidated how protecting a small critical subset of a larger population is fundamentally different from public health epidemiology. the studies provided guidelines for military preparedness in the event of an epidemic outbreak. the results showed the importance of early detection in implementing effective sequestration and the apparently counterintuitive result that sequestration, if implemented late, might lead to more infections rather than fewer infections."
"this coevolution is where the classical analogy between percolation and epidemics breaks down. to date, most of the research efforts in building large-scale models have represented this coevolution in an ad hoc manner. recent advances in artificial intelligence and operations research are likely to be useful in representing and analyzing this aspect of the model. figure 2 displays the interaction between multiple networks and the possibility of affecting contagion. two mathematical models are at play: coevolving graphical discrete dynamic systems, or cgddss, form the basis for simulating the coevolving dynamics, while pomdps and n-player games are suitable for representing and reasoning about interventions and individual behaviors. this requires computations over the configuration space of these dynamic systems. note that the configuration space over which pomdps have to reason is exponentially larger than their representations. simulations that implement a cgdds are computationally as well as conceptually much harder than simulations that simply implement basic percolation processes. in general, simulations implementing a cgdds comprise three operations that are repeated: simulate a state transition of disease progression over the network; evaluate the state of the disease, and test whether one or more triggering conditions hold; and apply applicable pharmaceutical or nonpharmaceutical interventions that change the social network structure or individual disease models. the composite system in simdemics facilitates this type of representation and this multistep process. the triggering condition can be based on an individual or on a subpopulation and may involve evaluating a complicated function. interventions can be applied to individuals or to sets of individuals (for example, school-aged children)."
"another recent study [cit] b ) was performed to determine the social and economic impact of public and private interventions, that are typically adopted, during a flulike epidemic. the economic costs included not only the loss in productivity due to sickness but also the indirect costs incurred through disease avoidance and caring for dependents. the results showed that the most important factor responsible for preventing income loss is the modification of individual behavior which reduced the total income lost by 62 percent compared to the base case. this result highlights the importance of behavioral modifications undertaken by the private citizens based on local and global disease prevalence levels."
"from a modeling perspective each vertex represents an agent. here we will assume that the states of the agent come from a finite domain ‫.ބ‬ the maps f vi,j are generally stochastic. computationally, each step of a cgdds (that is, the transition from one configuration to another), involves updating either a state associated with a vertex or modifying the set of incident edges on it. the pseudocode in figure 3 shows the computations involved in one transition."
"vaccination takes effect at the start of the simulation and the other interventions are triggered when a certain percentage of a subpopulation is diagnosed with the virus. it is assumed that 60 percent of those who are symptomatic will be diagnosed. school closure is done on a county by county basis, based on the number of sick children who reside in each county. each person who enters one of the symptomatic x states has a probability of withdrawing to home, depending on the severity of the symptoms (symptom 1-20 percent, symptom 2-50 percent, symptom 3-95 percent)."
"as our society becomes more connected, there will be an increasing need to develop innovative computational tools that will help policy makers and analysts grapple with complex questions. the advances in computing and information science on one hand will likely make our society even more connected and reduce the amount of time we have for decision making. on the other hand, the same technology will form the basis of new modeling and data processing environments that will generate new kinds of synthetic data sets that cannot be created in any other way (for example, direct measurement). this will enable social scientists to investigate entirely new research questions about functioning societal infrastructures and the individuals interacting with these systems. the tools will also allow policy makers, planners, and emergency responders unprecedented opportunities for multiperspective reasoning leading to improved situation assessment and consequence management."
"is research establishes an automatic approach for asphalt pavement pothole detection. image processing techniques including gf, sf, and ip are used synergistically to extract features from pavement digital images. two levels of gf are models to predict the existence of pothole on the pavement surface. experimental results with a repeated subsampling procedure with 20 runs confirm that ann and ls-svm are capable ai methods for pothole detection. it is because the cars of both methods are higher than 85% and auc values surpass 0.9. moreover, ls-svm has been identified as the better approach for the task of pothole detection with a desired accuracy of approximately 89%. with good predictive accuracy, the proposed ai model is very potential to be employed by transportation agencies and road inspectors to enhance the productivity of pavement inspection tasks with the specific focus on the pothole. e first future direction of the current study may include the evaluation of other advanced ai methods and their ensemble learning strategy to meliorate the pothole detection accuracy rate. e second future direction of the current model is to utilize advanced image processing methods for estimating the size of potholes. in addition, the integration of the current ai model with other sophisticated image analysis techniques to enhance the feature extraction stage is also worth investigating."
"ilarly, behavioral models that are used for individual decision making in the event of epidemics are parameterized. an individual's overall representation is composed of a within-host disease model and the individualized behavioral model. the description of the agent, or what an agent does, is not confined to these local functions. the agent's interaction with other agents defines its overall behavioral description. in this sense, the idea bears certain resemblance to the notion of (non)-modularity of functions in cognitive science [cit] ) and the concept of population coding [cit] in neuroscience. so while local functions and the state attributes associated with an agent determine its local dynamic evolution, the phase space encodes the system behavior and is necessary to understand the behavior of an individual agent."
"us, image cropping and resizing are applied if necessary. in total, 200 image samples are prepared within which each class label has 100 samples. in order to establish and verify the prediction models, the collected data set has been divided into two subsets: the training set (80%) and the testing set (20%). e training set is employed in the model construction phase. e testing set is reserved to examine the predictive capability of the trained models."
"here we describe simdemics, an interactionbased multiagent approach to study diffusion processes in very large sociotechnical networks. simdemics is an example of a disaggregated network-based modeling approach in which interactions between every pair of individuals connected in the social contact network are represented. it uses a realistic, synthetic representation of the underlying social contact network. it is based on the idea that a better understanding of the characteristics of the underlying network and individual behavioral adaptation can give better insights into contagion dynamics and response strategies. it should be noted that simdemics by itself does not prescribe a specific level of quality for the social contact networks. the necessary quality (in terms of accuracy, resolution, and fidelity) of the networks is determined by the questions that we aim to address."
"at the start of the simulation, a set of individuals in the social network is initially seeded with an infection. through the use of the scenario scripting language, a researcher can model specific scenarios by developing the set of decision triggers, individual and global interventions, and agents' behavior to be modeled. the scenario can dynamically modify the social network as the simulation progresses."
"scalable multiagent systems. developing scalable multiagent systems for studying a real-world problem remains an active area of research in ai [cit] . we first note that the notion of agents as used here should be distinguished from the notion of entity and actor. agents are actors that have intent or motive and thus require an individuated behavioral representation that is rich enough for the problem at hand and yet lightweight so it can be scaled to large populations of interacting individuals. we require three new ideas to achieve this: (1) parametric representation of individual behaviors and local actions: a single basic algorithm is used for each agent and the behavioral variation is obtained by randomization and agent specific attributes. (2) behavioral decomposition: we use automata theoretic techniques to represent each kind of local function associated with an agent by a separate automaton (algorithm) and then use generalized cross-product-like construction to obtain a composite behavior. thus each agent is a multitheory intentional entity. (3) finally, we introduce the notion of \"unencapsulated agents\": in this notion of agency, intent and behavioral loci are distributed and do not necessarily reside within a single software object. for example, we have only one within-host disease progression model (represented using a probabilistic timed transition system). however the specific manifestation of a disease-within-host is a function of demographic variables associated with the individual, computed elsewhere asynchronously and used as parameters that affect the local state transition properties. sim-"
"let f denote the global transition function associated with . this function can be viewed either as a function that maps ‫ބ‬ n into ‫ބ‬ n or as a function that maps ‫ބ‬ v into ‫ބ‬ v . f represents the transitions between configurations, and can therefore be considered as defining the dynamic behavior of an cgdds . we make several observations regarding the formal model just described."
"simdemics has been developed continually in a spiral r&d process over the last 12 years. it has been deployed and used in a number of userdefined studies, including recent pandemic planning studies undertaken for the u.s. department of homeland security (dhs), department of defense (dod), and department of health and human services (dhhs). the studies have guided the continued evolution of simdemics both in terms of its usability and model development. the studies also helped us identify new research questions at the interface of multiagent modeling, data mining, network science, and high-performance computing. the following are notable case studies undertaken using simdemics."
"rough the supervised training process, an ann model is capable of making inference via a large aggregation of neural units called artificial neurons. ann is very similar to the way a biological brain solves pattern recognition problems with a large number of connected biological neurons [cit] . an ann model consists of multiple nodes, which simulate biological neurons of the human brain."
"ip is a commonly used image processing technique in the field of automatic face recognition system [cit] . due to its simplicity and discriminative power, this technique is very potential to be applied to the task of pothole detection. given a grayscale image i(x, y), the horizontal and vertical ips are defined as follows:"
"the overall approach consists of four distinct models: (1) a model for creating a set of synthetic individuals, (2) a model for generating (time varying) interaction networks, (3) a model for simulating the epidemic process, and (4) a model for representing and evaluating interventions and public policies. mathematically, these steps can be represented by a combination of (1) a discrete dynamic system framework that captures the coevolution of disease dynamics, social network, and individual behavior (first three steps) and (2) a partially observable markov decision process (pomdp) that captures various control and optimization problems formulated on the phase space of this dynamic system. we describe these steps in detail below."
"roads are essential components of the national infrastructure. evaluating road condition is a crucial task of transportation agencies that are responsible for establishing maintenance schedules and allocating maintenance budgets [cit] . e correlation of road deterioration and the increasing number of traffic accidents leads to the fact that road safety has become a common concern in many countries [cit] . e problem of asphalt road degradation has a very negative impact on the economic development for developing countries where financial resource for pavement maintenance is often insufficient. erefore, it is of practical need to improve the effectiveness of the asphalt pavement maintenance process."
"in the next step, we overlay a partially observable markov decision process framework over the discrete dynamic systems framework. this allows us to discuss control and optimization methods. the discrete dynamic system provides us with a computational view of how state transitions are made. [cit] for detailed definitions and complexity theoretic results on this topic. a pomdp m consists of a finite set of states (s), actions (a), and observations (o). s o ʦ s is the initial state of the system. t is the local transition function from one state to another and is probabilistic. o is the observation function that assigns to each state an observation. finally, r is the reward function that tells the reward received when action a ʦ a when in state s."
"] represents a bias vector of the hidden layer; b 2 is a bias vector of the output layer; f a denotes an activation function (e.g., log-sigmoid). generally, the weight matrices and the bias vectors of the ann can be effectively trained using the error backpropagation framework [cit] ."
"occurrence of a fit can be misleading because the inverse being solved is nonunique. in both cases, explanatory power is really at issue. in decision making a causal basis for the choice of the best option is more relevant than any particular kind of detailed prediction of state. this raises fundamental issues for the idea of validation and is the focus of ongoing work. related concerns have been discussed by historians and philosophers [cit] . see table 1 for an overview of our modeling approaches."
"step 3 consists of developing a computational model for representing the disease within individuals and its transmission between them. the model can be viewed as a networked finite probabilistic timed transition system (ptts). each individual is associated with a timed probabilistic finite state machine. a ptts can be represented using appropriate stochastic local functions (with possibly additional nodes to simulate a clock). a ptts representing an individual interacts with or is coupled to pttss of other individuals that are neighbors in the social contact network. pttss are an extension of the well known finite state machines (fsms) with two additional features: the state transitions are probabilistic or timed (or both). in other words, the end point of state transitions may be chosen probabilistically or deterministically; the transi-tions may be timed, that is, they may occur at a specified time after the previous transition, or there may be a fixed probability of transition for each discrete time interval. in principle, each agent can have a different probabilistic timed transition system. more commonly, subsets of people determined by demographics such as age are assigned a single parameterized ptts, and individuals are assigned values for the parameters at random from an appropriate distribution. the coupling between individual pttss is derived from the social contact network. the dynamics of disease diffusion processes can now be simulated using the above coupled model. we have developed fast high-performance computing-based algorithms to achieve this ."
"second, the edge-modification function as defined can modify in one step a subset of edges simultaneously. an alternate model might have allowed a vertex to change exactly one edge at a time. we chose the former due to the specific application we had in mind. in all our applications, when an agent decides to not go to a location (either due to location closure as demanded by public policy or due to the fear of contracting the disease), its edges to all other individuals in that location are simultaneously removed while edges to all the individuals who might be at home are added. third, the model is markovian in that the updates are based only on the current state of the system; it is possible to extend the model wherein updates are based on earlier state of the system. finally, we have assumed that there is exactly one function for each arity for each node. this can be relaxed easily; similarly these functions will, in general, be stochastic."
"a study was conducted on behalf of the office of homeland security (ohs) to develop planning and response strategies for a smallpox-based bioterror attack [cit] . in contrast to earlier results, our experiments showed that early detection and targeted interventions can be quite promising in mitigating the effects of such an attack. the study pointed to the importance of developing realistic models of social contact networks but raised new questions on finding implementable policies for targeted interventions."
"e process of road safety survey generally consists of the detection of the defects (e.g., cracks and potholes) existing on the road section and evaluation of the magnitude of the defects [cit] . among several forms of pavement distresses, potholes are important indicators of the road defects, and they should be detected in a timely manner for the tasks of asphalt-surfaced pavement maintenance and rehabilitation [cit] . e reason is that this form of defect significantly delays traffic and brings about a hazardous condition for drivers."
bene t of this technique is that the number of features is reduced from 300 to 60. is reduction of the number of features is very helpful for the machine learning algorithms since they can avoid the curse of dimensionality [cit] .
"recent review works [cit] have pointed out an increasing trend of applying image processing technique and arti cial intelligence (ai) method for enhancing the accuracy and productivity of the task of interest. moreover, irregular background illumination and complex pavement texture/color are still major challenges that computer vision-based methods have to overcome. hence, other advanced approaches of image processing and ai should be investigated to construct automatic pothole recognition models. e current study is dedicated to establishing a new aibased model for automatically recognizing pothole objects in asphalt pavement images. e steerable lter is employed to create a salient map for distress detection. in addition, the gaussian lter is utilized for image denoising, and the integral projection is employed to exhibit the characteristics of the salient map constructed by the steerable lter. e features extracted by the aforementioned image processing techniques are used by the arti cial neural network and the least squares support vector machine. a data set of image samples with two class labels (nonpothole and pothole) has been collected and used to train and validate the performance of the two supervised learning algorithms. e rest of the paper is organized in the following way: e second section presents the research methodology. e next section describes the structure of the proposed model for pavement pothole recognition. experimental results and performance comparison are reported in the fourth section, followed by conclusions of the study in the nal section."
"besides the sf algorithm, the gf is used as a method of image denoising and ip is employed to extract the properties of the salient map computed by the sf. it is noted that the gf is applied to denoise both the original image (gf level 1) and the sf-based salient map (gf level 2). e gf level 1 aims at removing irregular texture on the asphalt pavement background. meanwhile, the gf level 2 is dedicated to enhance the sf-based salient map by reducing the noisy feature. in the third module, ann and ls-svm are employed to generalize a classification boundary used to recognize pothole patterns. it is noted that sf-ai-pdm has been programmed in matlab environment with the support of the matlab image processing toolbox [cit] ."
"in step 1, a synthetic urban population is created by integrating a variety of databases from commercial and public sources into a common architecture for data exchange. the process preserves the confidentiality of the individuals in the original data sets, yet produces realistic attributes and demographics for the synthetic individuals. the synthetic population is a set of synthetic people and households, located geographically, each associated with demographic variables recorded in the census. joint demographic distributions are reconstructed from the marginal distributions available in typical census data using an iterative proportional fitting (ipf) technique. each synthetic individual is placed in a household with other synthetic individuals. each household is located geographically using land-use data and data pertaining to transportation networks. the process guarantees that a census of our synthetic population is statistically indistinguishable from the original census. since the population is synthetic, the privacy of individuals is protected. the basic process can be extended to assign other demographic attributes using additional data sources. for example, in recent work we assigned individuals' mobile devices using commercially available survey data."
"where α k is a lagrange multiplier; φ(x k ) denotes a kernel function. after the kkt conditions for optimality are applied, the optimization described in (9) corresponds to solving the following linear system [cit] :"
"experiments with various settings of model parameters indicate that ann with the number of neurons of 40 and the learning rate of 0.01 delivers the most desirable outcome. in addition, the regularization parameter of 500 and the kernel function parameter of 100 are found to be appropriate for the ls-svm model. with such parameter settings, the model prediction performances obtained from 20 runs are summarized in table 1 with the average (mean) and standard deviation (sd) values of each performance metric. it is observable that ls-svm (car � 88.75% and auc � 0.96) has achieved a better prediction accuracy than ann (car � 85.25% and auc � 0.92). e tpr and tnr of ls-svm (0.82 and 0.96) are also superior to those of the ann (0.81 and 0.90). e prediction capabilities of the two ai models in the form of rocs are graphically presented in figure 9 . experimental results indicate that ls-svm is a more suitable ai approach for the collected image data set of asphalt pavement."
"step 4 consists of representing individual behaviors and their adaptations, public policies, and interventions. individual behaviors are represented and analyzed using finite pttss and are based on well accepted and recently proposed socioeconomic theories of individual and collective behaviors. public policies are represented and analyzed using partially observable markov decision processes (pomdps). this allows us to capture sequential decision-making processes related to studying the efficacy of various interventions and behaviors of individual agents in response to their perception of disease spread. the pomdp is exponentially larger than the problem specification, and, in general, determining an optimal solution is an intractable problem. we thus resort to efficient simulations. a key concept is that of implementable policies-policies or interventions that are implementable in the real world. that results at the end of these steps. in the real world, disease diffusion and information diffusion are based on different models and potentially use different social networks. although interventions and individual behavioral changes were studied in the past, their implementations were carried out in an ad hoc manner. computational efforts concentrated first on developing fast methods for simulating disease progression. fast methods for disease progression that do not consider the interaction with policies and individual behaviors are insufficient due to coevolution within the underlying system. a modeling environment is required in which simulation of disease spread is carried out in lockstep with the interventions that are instituted, and the resulting effects on the network structure, and individual attributes."
"where r c and r a represent the number of image samples being correctly classified and the total number of image samples, respectively. besides car, the true positive rate (tpr) (the percentage of positive instances correctly classified), the false positive rate (fpr) (the percentage of negative instances misclassified), the false negative rate (fnr) (the percentage of positive instances misclassified), and the true negative rate (tnr) (the percentage of negative instances correctly classified) should also be used [cit] ."
"sixteen scenarios were simulated, specifying all combinations of the four sets of interventions (i, q, s, v). figure 4 shows the number of individuals that are infected for each cell. they can be grouped into four categories: those without vaccination or school closure (labeled a in the figure), vaccination (labeled b), school closure (labeled c), and both vaccinations and school closure (labeled d). selfisolation and quarantine of critical workers have little impact on the overall infection rates. self-isolation happens late in the epidemic (2.5 percent of the adult population is infected on a single day), limiting its effectiveness. in fact, when school closure or vaccination is included, the trigger level is never reached. quarantine of critical workers affects such a small portion of the total population (about a third of a percent) that its effects are not apparent in the total population. however, quarantine reduces the percentage of critical workers who are infected from 40 percent without quarantine to 18 percent when critical workers are both quarantined and vaccinated, which may be vitally important for maintaining a functioning society."
"another interesting phenomenon happens with school closure. schools are closed when 1 percent of school children are diagnosed as ill on a particular day in a county. the schools are reopened, on a county by county basis, when the number of diagnosed children falls below 0.1 percent. even at that low level, there is enough residual infection to cause another wave of infections. this can be observed in the dual peaks of the group of epicurves labeled b in figure 5."
"coevolving graphical models. researchers in ai have been studying graphical models for solving problems pertaining to areas such as bayesian inference, games, and constraint-satisfaction problems. graphical models of bayesian inference and games have been proposed and studied in ai to capture the network structure inherent in certain applications [cit] . dynamic processes such as epidemics on social networks provide a new class of applications [cit] in this regard. two aspects make this a very challenging problem. first, in order to solve realworld problems we need to deal with graphs with 10 million to 300 million vertices and 500 million to 15 billion edges. second, realistic social networks do not have a treelike property that has been exploited in earlier work to obtain tractable algorithms for computing nash equilibria [cit] . they are not even small-world networks or scalefree networks as defined in the current literature [cit] . understanding the structure of these networks and exploiting this structure for designing efficient computational solutions is an important research question. another interesting direction for further research is to extend the notion of graphical games and inference problems to coevolving graphical games and inference problems in which the underlying network is changing due to the decisions taken by individual agents, which in turn change due to the interaction predicated by the network. an example of an important application-motivated inference question is the following: given sparse surveillance information about a small subset of infected individuals in a region, find the index cases, that is, the initial set of individuals who became infected."
"the contagion, the underlying interaction network (consisting of both human and technical elements), the public policies, and the individual behaviors coevolve. this makes it nearly impossible to apply standard model-reduction techniques that have been successfully used to study physical systems. finally, in practical situations, multiple contagion processes simultaneously coevolve."
"image enhanced by gf level 1 sf based response map response map enhanced by gf level 2 e first set is used to establish the model; the second set is reserved for investigating the predictive performance of the models. it is noted that a single run of the experiment may not reliably reveal the model predictive performance due to the problem of randomness in data separation. hence, the performances of the ai approaches (ann and ls-svm) are evaluated via a repeated subsampling process which includes 20 runs. in each run, 20% of the data set is randomly taken out to form the testing data set; the rest of the data set is used as the training set."
"where tp, tn, fp, and fn are the values of true positive, true negative, false positive, and false negative, respectively. furthermore, the four rates of tp, fp, fn, and tn can be graphically summarized in the form of a receiver operating characteristic (roc) curve. e roc curve is drawn based on the sensitivity (true positive rate) and the specificity (false negative rate). using the roc curve, an index called the area under the curve (auc) can be calculated to express the model classification capability. it is noted that auc ranges from 0.5 to 1. auc � 1 indicates a perfect classification model, and auc � 0.5 indicates an incapable classifier with random predictions [cit] ."
"the population of the state of alabama (4.3 mil-lion residents, 1.1 million locations, and 58 million activities of six types) is simulated. about 14,000 of the residents have been marked as critical workers based on the demographics of the actual critical worker subpopulation. a critical worker is someone whose work is essential for the health and welfare of the general population (for example, first responders, health-care workers, powerplant operators, and so on). the contagion that will be spread is the h5n1 influenza virus (that is, avian flu). figure 4 shows the ptts associated with the h5n1 disease model. there are several different interventions that may be applied to different groups at different times: vaccination of adults, children, and critical workers (v), school closure (s), quarantine of critical workers (q), and self-isolation (i). the vaccine is assumed to be low efficacy, meaning that only 30 percent of the individuals vaccinated will be protected. this efficacy is representative of a vaccine for a newly emerging strain of influenza. when schools are closed, an adult caregiver is required to remain home with any children under 13 years of age. sixty percent of the school children are considered to be compliant, meaning that they remain at home for the entire day. the other 40 percent participate in their normal after-school activities. critical workers will be isolated in some type of group quarters in small groups of 16 where they do not come in contact with people outside of their subgroup. when an individual chooses to self-isolate, he stays home and does not participate in any activities away from home, although the person remains in contact with other people in the house. table 2 describes the four sets of interventions. the intrahost disease progression passes through several stages [cit] . when a person is exposed to the disease, it starts off in a latent stage, where the individual is neither infectious nor symptomatic. this is potentially followed by an incubating stage during which the individual is partially infectious but still does not exhibit symptoms. in the final stage of the disease an individual is fully infectious and displays one of four levels of symptoms from asymptomatic to fully symptomatic. the probability of a person staying home instead of participating in his normal activities increases with the severity of the symptoms. once the disease has run its course, the infected individual is considered recovered and cannot be reinfected."
"first, we will assume that the local transition functions and local graph modification functions are both computable efficiently in polynomial time. in agent-based models used in the social sciences these are usually very simple functions. furthermore, the functions g vi need to be specified using a succinct representation, rather than a complete table, which will be exponentially larger."
"after the feature set is determined, the data set of image samples is established in the second module. accordingly, the data set is separated into two sets: training set (80%) and testing set (20%). e training set is used for model establishment; the testing set is employed for model verication. with the separated data sets, the ai methods of ann and ls-svm are employed to generalize a decision boundary that classi es the instances of nonpothole and pothole classes. e training phases of ann and ls-svm require the setting of several model parameters. in addition, to quantify the classi cation performance of these two ai approaches, evaluation metrics must be employed. ese two issues are going to be addressed in the next section of the study."
the string w is a schedule. it represents an order in which the state of a vertex or the possible edges incident on the vertex will be updated. here v i (s) intuitively specifies that the state of the vertex v i is to be updated; v i (g) specifies that one or more incident edges will be updated.
"where label is a record of the type of activity of the visit and its start and end points. the edges between individuals capture physical proximity between the individuals. the existence of such edges and their attributes often depends on the disease we wish to study. for example, in a case of influenzalike illness, two individuals have an edge if they are within certain proximal distance of one another for a certain period of time. in case of sexually transmitted diseases, an edge implies intimate physical contact. see figure 1 for a representation on how a small family's activities during the course of a day are composed to form a dynamic social network. please note that it is impossible to build such a network by simply collecting field data; the use of generative models to build such networks is a unique feature of this work."
"in image processing eld, gf is a widely used preprocessing technique to reduce image noise and remove redundant details [cit] . particularly for the task of pothole detection, gf can be helpful to blur the asphalt background texture and facilitate further analysis of the digital image. e gf is essentially a 2d convolution operator that uses the kernel that represents the shape of a gaussian function. e formula of a gaussian function in a 2d space is given as follows:"
"where θ denotes the orientation of the filter. e filter response for a whole digital image i is graphically presented in figure 2 with θ � [0 : 45 : 360] and r � 1.0, 1.5, and 2.0. it is noted that the response at coordination (x, y) is attained by the convolution operator as follows:"
"all possible vectors of vertex states and edge states (present or absent). each vector of the underlying markov chain ᏹ is specified by a vector of length ( n 2 ) + n, representing all the possible edges and vertices in the graph. the state transitions are obtained by composing the local functions f i and g i as we discussed. if these functions are probabilistic then so is the transition function for the markov chain. thus the chain consists of 2( n 2 ) +n states. actions should be thought of as interventions in our context. policies map observations to actions, and actions in a state yield reward. the reward (cost) function can be a combination of number of infected individuals and the economic and social costs of the interventions. we have two possible classes of reward functions, as is common in game theory: a systemwide reward function and a local reward function associated with each individual. individuals attempt to maximize their local reward function (for example, the probability of the individual or a family member becoming infected), while public policy attempts to maximize the systemwide reward function (for example, the total number of people not infected). an agent-based model serves as the corresponding computational model for the pomdp. partial observability in our context will be captured through various triggering conditions and interventions that are instituted."
"where α k and b denote the solution to the linear system shown in (11) . in addition, the kernel function that is often used in ls-svm is the radial basis function (rbf) kernel [cit] ."
"simdemics can be used to study a much larger class of diffusion processes. these include epidemic processes in ecologies; the spread of certain noninfectious diseases such as obesity and smoking; the spread of fads, conventions, norms, and information in social systems; the spread of worms and malware in communication networks [cit] ). here, we will confine our discussion to the spread of infectious diseases in human populations. besides their obvious societal importance, epidemics serve as an excellent example of diffusion processes over interaction networks. within the infectious disease context, simdemics details the demographic and geographic distributions of contagion spread. it also provides decision makers with information about the consequences of an outbreak, the resulting demand for health services, and the feasibility and effectiveness of various response options. a unique feature of simdemics is the size and scale of social and ecological systems that can be analyzed through its use. planning and responding to the threat of pandemics presents an important societal and public health challenge. public health authorities around the world are far more prepared to respond to pandemic threats now than they have ever been in the past. however, a number of modern trends continue to make this a vexing problem. these include (1) a larger global population and increased urbanization leading to a higher density of individuals within cities; (2) higher levels of long-distance travel, including international travel; and (3) articles 76 ai magazine increased numbers of elderly individuals and individuals with chronic medical conditions."
"pavement pothole detection is section describes the overall structure of the proposed approach ( figure 7 ) which is named as steerable filter and artificial intelligence-based pothole detection model (sf-ai-pdm). e model includes three main modules: (1) image acquisition and feature extraction, (2) data set construction, and (3) ai model training and prediction. sf is an essential part of the first step."
"as we began to answer real-world policy questions, it became progressively clear that easy-to-use webbased systems that employ our models would provide an appropriate mechanism for delivering the results. as a result, we have been developing webbased services so that complex high-performance computing-based models can be used by analysts who are not computing experts. the web-based system is called didactic. in essence, it provides users a way to specify a factorial style adaptive experimental design. using didactic, a public health analyst can carry out a number of what-if experiments, assess the trade-offs between various intervention strategies, and more generally develop a better understanding of how the outbreak is likely to spread over social networks. in many ways this effort is similar to the topic of cognitive prostheses [cit] )-computational systems for analysts to obtain better situational awareness using simdemics-like environments in conjunction with surveillance information. we are extending the system to become more interactive and to allow for computational steering of experiments. this requires methods for coordinating resource discovery of computing and data assets; ai-based techniques for translating user-level requests to efficient work flows; reusing data sets whenever possible and spawning computer models with required initial parameters and coordination of resources among various users. [cit], and . a natural formal environment to represent and assess graph dynamic systems, their composition, and coevolution in both algebraic and computational terms is necessary to deal with these questions."
"our formalization consists of two parts: (1) a discrete graphical dynamic system framework that captures the coevolution of disease dynamics, social networks, and individual behavior, and (2) a partially observable markov decision process that captures various control and optimization problems formulated on the phase space of this dynamic system. the basic framework consists of the following components: (1) a collection of entities with state values and local rules for state transitions, (2) an interaction graph capturing the local dependency of an entity on its neighboring entities, and (3) an update sequence or schedule such that the causality in the system is represented by the composition of local mappings. we formalize this as follows. a coevolving graphical discrete dynamic system over a given domain ‫ބ‬ of state values is a triple (g, Ᏺ, w), whose components are as follows:"
"it is also proper to note that, before the training and predicting phase, the z-score transformation has been employed to normalize the whole data set. e data normalization aims at fending off the situation in which input variables with large magnitude dominate ones with small values. moreover, the implementations of the two ai methods necessitate the specification of several tuning parameters. for the purpose of setting those parameters, the original data set is divided into a training set (80%) and a verification set (20%). e model's tuning parameters corresponding to the best predictive performance on the verification set is selected as the optimal ones. e implementation of ann requires determining the number of neurons in the hidden layer and the learning rate. as suggested by heaton [cit], the number of neurons is roughly selected to be . other parameters of ann including the type of the activation function and the number training epochs are selected to be log-sigmoid function and 3000, respectively. for the case of ls-svm, this ai method necessitates an appropriate determination of the penalty constant and the kernel function parameters. in this study, these two tuning parameters of ls-svm are determined via a grid search algorithm described in the previous research work of hoang and tien bui [cit] ."
"a pothole is commonly defined as a bowl-shaped depression on the pavement surface with a minimum plane diameter of 150 mm [cit] . generally, structure aging, heavy traffic condition, poor drainage, thin asphalt layer substructure, and weak substructure can be the causes of pothole appearance [cit] . in developing countries, the pavement pothole is often detected manually by inspectors of local transportation agencies during periodical field surveys. although this conventional method can help to acquire accurate evaluation of potholes, it also features low productivity in both data collection and data processing. e reason is that one pavement inspector can only inspect less than 10 km per day [cit] . with a large number of road sections needed to be inspected routinely, the automation of the pothole detection becomes a pressing need for transportation agencies. moreover, the productive pavement surveying process significantly leads to economic gain. it is because, if rehabilitation process is performed timely, pavement restoration cost can be saved by up to 80% [cit] ."
"extensive efforts have been made to validate the overall approach and specific components of the model. this includes (1) structural validity of models, (2) matching the data produced to field data, and (3) functional validation and formal specifications of these models for software verification [cit] ). several models used here are based on work done by other researchers (see table 1 )."
"our cyber environment is fully deployed and under continual development. several studies involving human networks, livestock networks, and computing networks have been conducted using this cyber environment. in an epidemiological experiment, an application determines each agent's normative social network. a diffusion process is then run over this social network as agents probabilistically infect other agents connected to them in the social network."
"we first began to work on this class of problems 15 years ago. our goal was to develop new computational and analytical techniques to understand and reason about very large complex sociotechnical systems. solutions to these problems required us to borrow ideas from diverse areas in science and engineering, including network science, highperformance computing, urban planning, economics, nonlinear systems, algorithmics, statistics, and so on. over time, it became increasingly clear that an ai perspective on complex sociotechnical systems was extremely valuable; several ideas and concepts in ai proved to be the conceptual glue that was critical in developing an integrated solution. these ideas include multiagent systems for representing large complex systems; causality, reasoning, and inference in complex systems; and human-computer interaction. we have tried to highlight this perspective throughout the article."
"usually the term model validation is associated with a relatively simple-seeming view of predictive validity -state-space predictions by the model match the measured data. [cit] flu season. although this kind of examination of the model can be useful, it can also be misleading and is not adequate. one immediate implication of the previous section is that the configuration space (which is the essence of causal structure for the evolving system) is important to understanding the system being represented as well as the modeled representation of that system. however, any measured real-world data is incapable of capturing this structural range-only those modes that happened in the real world appear in the measured data. thus the process of postdiction is, alone, inadequate. additionally, usually while postdicting, insufficient information about the context is available to properly specify the initial and structural conditions that would allow the model to predict. as a result, high-dimensional models are often fit to relatively sparse data. in this sense, the schematic diagram depicting multitheory, multinetwork diffusion processes. the lowest level depicts a social proximity network over which aerosol-borne infectious disease spreads. the individual behavioral model captures the host disease progression model. the middle layer denotes the social influence contagion: individuals might comply with nonpharmaceutical interventions (for example, face masks) based on social influences, public information, and policies. the top layer denotes the fear of contagion that results when disease outbreak happens. compliance in the social influence layer depends on the state of disease contagion as well as the level of fear. behavioral change causes change in the networks as well as contagion dynamics."
"the output to be predicted is the patient's condition 30 days after being admitted to the hospital. to simplify the problem, only the differentiation between \"alive\" and \"dead\" has been considered."
"based on the expression pattern of optimal snornas, we concluded that distinguishing eight tumor subtypes by using these snornas is effective and accurate, validating that snornas may also contribute to precise tumor subtyping."
"in this study, we first calculated the prediction accuracy of individual classes by using a 10-fold cross-validation [75, [cit] . for each class, we calculated the individual tpr and fpr as follows:"
"the aim of the experiments is to determine the most interesting set of features to determine the future evolution of the patient. in order to validate and test the use of gas in this study, a classification comparison is proposed. a classification has been performed only with the variables identified as most relevant by the ga, after performing a wrapped search among all the features available on the dataset; then the results are compared with a classification performed using all the variables of the dataset."
"to achieve this, the algorithm extracts the attribute that best separates the given cases into targeted classes. the algorithm uses the statistical property called \"information gain\" to choose which attribute is the best to separate training examples."
"the objective of this study is the identification of the most important patient's characteristics or symptoms in order to determine the future evaluation of their illness. as explained in previous sections, some of those are obtained from medical tests that can take a relatively long time, so it is important to know in advance which of them must be given higher priority. this is therefore, a clear case where the application of feature selection algorithms can be of much use. in the case of this study, a genetic algorithm is employed as a mean for feature selection, enabling to guide the search among the most interesting combination of attributes (or dimensions) to obtain similar results of the ones obtained by using the whole set of attributes or characteristics for each patient."
"in this study, we used the monte carlo feature selection (mcfs) to analysis expression levels of 1524 snornas. each feature was assigned a relative importance (ri) score and all features were ranked in descending order based on their ri scores. the obtained feature list is listed in table s1 ."
where ξ i are positive slack variables introduced to handle the non-separable case and where k 0 and k 1 are typically defined to be +1 and -1 respectively.
"the specific role of snorna in kirc has been widely reported and confirmed [cit] . according to our rules, the tumor samples of the eight tumor subtypes that do not have corresponding expression pattern with any of the rules we have extracted may be the kidney renal clear cell carcinoma samples."
"supplementary materials: supplementary materials can be found at http://www.mdpi.com/1422-0067/20/9/2185/ s1. table s1 . the features ranked by their ri values derived from the mcfs method, table s2 . produced 69 classification rules for classifying samples from the 8 cancer types, table s3 . corresponding accuracies of individual classes, overall accuracies, and mccs by using different numbers of features selected by using the ifs method and svm classifiers."
"according to the two discussion subsections presented above, some extracted optimal distinctive snornas and quantitative rules can be validated by recent publications, validating the reliability of our results. based on the whole blueprint of snorna distribution in multiple tumor subtypes as summarized by a systematic study [cit], we further screened out the distinctive core snornas and built up quantitative standards for the identification of different tumor subtypes based on the snorna expression. this study does not only provide a new tool for tumor subtyping but also deepens the understanding of the different distribution and contribution mechanisms of snornas in various tumor subtypes."
"the next predicted tumor subgroup is thca. we chose an optimal quantitative rule involving two optimal parameters, namely, snord123 and snord114, for the detailed analysis (see rule65). no direct reports confirmed the specific expression patterns of these two noncoding snornas in thca. however, a systematic study [cit] on all noncoding rnas in thca reveals the potential expression tendency of these two parameters in tumor tissues compared with normal controls, corresponding to this rule and partially validating the efficacy and accuracy of our results."
"in the model adjusted for clinically important variables (age, sex, health careassociated acquisition of infection, diabetes, cancer, long-term immunosuppressive therapy, organism (s. aureus infection), previous valve (paravalvular abscess, cardiac surgery, complications (stroke, heart failure, and new conduction abnormality)), variables independently associated with higher mortality among patients with native valve endocarditis were age 60 years or older, health care-associated acquisition of infection, diabetes, s. aureus infection, paravalvular abscess, stroke, heart failure, and new conduction abnormality. in agreement with previous studies, the results point to the fact that advanced age and endocarditis complications (stroke, heart failure, and septic embolism) were associated with greater mortality in patients with native valve endocarditis. independent predictors of in-hospital mortality among patients with endocarditis in the present study included increasing age, systemic embolism, heart failure, prosthetic valvular endocardidtis and clinical delay. so, it can be concluded that the mortality rate may be increased by patient factors such as age and comorbid conditions, rather than by intrinsic qualities of the organism."
"in addition to the qualitative analysis mentioned above, based on the detailed expression level of snorna provided by our reference dataset in section 4.1 [cit], we set up a series of systematic quantitative distinctive rules for further detailed identification of each tumor subgroup among the eight cancer types. in total, we obtained 69 rules for all eight tumor subtypes (table s2 ). however, due to the limitation of the manuscript, no space can be used to analyze each quantitative rule one by one. therefore, to display the whole blueprint of these rules, we screened out one typical quantitative rule for each tumor subtype. the detailed analysis is listed below."
"on the basis of some machine learning methods, we identified not only a group of effective core regulatory snornas that may participate in tumorigenesis and contribute to the distinction of the eight candidate tumor subtypes, but we also set up a series of qualitative rules for the precise recognition of different subtypes according to the expression pattern of the optimal parameters. according to recent publications, some obtained key distinctive snornas and specific distinctive qualitative rules can be verified, validating the reliability of our results."
"the naïve bayes classifier is also a very widespread supervised classifier, known for its simplicity and relatively good performance [cit] . it is based in the probability theory, more precisely in bayes theorem [cit] . this method has the particularity that it will assume that the probability of each of the different attributes, to determine the final class of the sample, can be considered independent of the rest. that is, are conditionally independent given the class label. although this does not always happen to be true, it is a good way to simplify the calculations. it performs its classification by calculating the a priory probability and the likelihood of a sample belonging to a class by using a set of previously labeled training data."
"as a subgroup of noncoding rnas, snornas was also recently confirmed to involve in tumorigenesis in various regulatory modes as reported in recent publications [cit] . first, snornas participate in the regulation of methylation and pseudouridylation. early studies confirmed that some biological components of snornps, such as snora42 and u50, may have specific expression pattern or directly participate in tumorigenesis, indicating that snornps may be functionally related to cancer [cit] . second, snornas may also be functionally connected to cancer through its host genes. a recent study on zfas1 on a tumor-associated nonprotein-coding snorna host gene confirmed that three c/d box snornas might regulate the expression of their host gene zfas1, and further indirectly mediate tumorigenesis [cit] . furthermore, a study on another gene named gas5 confirmed that snornas (c/d box) encoded by its own intron may contribute to the progression of tumorigenesis similar to its host gene, gas5 [cit] . although confirmation is still needed on the topic of whether the snorna functions are independent of their respective host genes, both snornas and the host genes may promote tumorigenesis in a cooperative and synergetic pattern, implying the complicated contribution of snornas on tumorigenesis."
"once the illness has been diagnosed, a rapid initiation of an adequate therapeutic regimen is important to prevent complications such as arrhythmias, brain abscess, brain or nervous system changes, congestive heart failure, glomerulonephritis, jaundice, severe heart damage, stroke,.., and death."
"removing the lgg and lusc interference, the next subgroup of rules to be discussed contributes to prad identification. rule22 involves four effective snornas, namely, snora7, hbi-43, hbii-295, and hbii-52-32. the high expression of hbii-52-32, together with the low expression of snora7 and hbii-295, refers to prad recognition. although no detailed reports have confirmed that these three snornas may directly contribute to prad tumorigenesis, recent publications on c/d box snornas confirmed the potential relationship between snornas and prostate tumorigenesis [cit] . as for hbi-43, such snorna has been identified to have a specific expression pattern in prad tumorigenesis and related cancer subtypes [cit] . therefore, snora7, hbi-43, hbii-295, and hbii-52-32 may be specific monitoring markers contributing to prostate cancer progression."
"patients with this condition usually need to be hospitalized to begin an aggressive treatment [cit] based on intravenously antibiotics. initially, the treatment is empirical and the ideal situation is encountering the specific antibiotic for the organism causing the condition. this is determined by the blood culture and the sensitivity tests, which is not an immediate process."
future work will be focused on the collection and storage of more specific attributes for each patient. results seem to point to the fact that with more detailed data the medical condition of each patient alongside enough amount of different patients better results could be obtained. these results may include better prediction of mortality risk based on detailed data obtained from simple tests performed as close to the admission time of the patient as possible.
"the remaining of this paper is organised as follows. section 2 introduces the decision genetic algorithms techniques used to realize feature selection. section 3 describes classification models; in section 4 the dataset is explained; section 5 shows the experiments and results obtained. finally, in section 6, the conclusions are set out and comments are made on future lines of work."
"among many of the modifications that have been introduced to the basic algorithm, one of the most used is the inclusion of a kernel density estimator to calculate the true density of the continuous variables using kernels in the computation of the likelihood of samples [cit] ."
"the power of gas comes from the fact that the basic technique is robust and can deal with a wide array of different problem statements. they are not guaranteed to find the global optimum solution for the given problem, but can achieve an \"acceptably good\" solution in a relatively low time [cit] ."
"the first ten rules contribute to lgg identification. however, the eleventh rule (rule11) contributes to lusc identification, involving four optimal snornas, including htr, hbi-115, u83b, snora47, and aca31. the high expression of htr and u83b, together with the low expression of hbi-115, snora47, and aca31, is unique in the snorna expression pattern of lusc. according to a recent clinical study on lusc progression, the expression of htr snorna promotes tumorigenesis, corresponding with this rule [cit] . meanwhile, hbi-115, snora47 [cit], and aca31 all have unique expression patterns in lung cancer, especially in lusc, according to a study on the early stage of lung cancer; thus, this result validates the unique indicating effects of these snornas on cancer subtyping even at an early stage [cit] ."
"based on the dataset provided by our reference and the newly applied computational approach, we identified a group of functional snornas that may have distinctive expression pattern in eight tumor subtypes. here, we presented a detailed analysis of a number of snornas."
"in the case of the present work, this algorithm has been used as a way of performing a guided search among the different attributes that could be used to classify future evolution of the patients. this is usually known in literature as a wrapper method [cit] . each individual represents a different subset of the features chosen among the whole of them; while the fitness of each individual is the classification rate obtained by a regular machine learning classifier. in order to test the method in combination with a wider array of models, tests have been performed with three different classifiers: support vector machines, id3 decision trees and naïve bayes with kernel density estimation."
"we also used support vector machines (svms) to classify samples consisting of important features that were selected from the incremental feature selection (ifs) method. first, a set of feature subsets with a step 1 were constructed. after testing the performance of svms on the samples consisting of features from individual feature subsets with 10-fold cross-validation once, we obtained the highest matthew's correlation coefficient (mcc) of 0.881 when the top 443 snornas were used. in addition, we can yield an mcc value of 0.708 when using only the top 72 features. the predicted accuracies for individual cancers, overall accuracies, and mccs by using a different number of features are listed in table s3 . furthermore, we presented the mcc trends corresponding to the number of features involved in building the svm classifiers, as shown in figure 2a, in which the optimal mcc value of 0.881 is marked with a red rhombus. accordingly, we termed the svm classifier using top 443 features as the optimal svm classifier. the confusion matrix generated by the 10-fold cross-validation on this classifier is illustrated in figure 1b, from which we can see that the performance of the optimal svm classifier was much better than that of 69 classification rules. the tprs and fprs of the individual classes yielded by such a classifier are listed in table 1 . compared with those yielded by classification rules, the tprs produced by the optimal svm classifier were much higher and fprs were lower (except one), suggesting the optimal svm classifier gave a much better performance. in addition, we also counted the performance of the optimal svm classifier on each of the ten folds. the highest and lowest accuracies for each cancer type were listed in table 2 ."
"as we have mentioned above, snornas are a group of small nucleolar rnas that generally contribute to the regulation of rna modifications. ribosomal rnas, transfer rnas, and small nuclear rnas are the three subgroups of snornas. snornas generally act as a guide for specific modification on a target rna in the form of rna/protein complex together with multiple protein molecules. such complexes (also called small nucleolar ribonuceloproteins) further contribute to the accurate chemical modification of a target rna sequence. when analyzing the biological functions of snornas, we just screened out the targets of snornas and tried to explain the biological effects of these snornas based regulatory roles on such targets, which may help explain the biological function of snornas. comparing to snornas, mirnas/lncrnas do not directly affect the chemical structure and composition of mrnas. such rnas regulate gene functions by affecting gene expression levels directly or indirectly. thus, when analyzing the biological functions of such two kinds of rnas, we analyzed the expression alteration effects of features in certain physical or pathological conditions. therefore, the analysis of mirnas/lncrnas and snornas are quite different. in this study, we applied the proper functional analysis on key distinctive snornas and rules associated snornas."
"in addition to the above-mentioned four tumor subgroups, the next identified tumor subtype is hnsc. we chose several rules involving snord116, snora73b, scarna4, and hbii-85-20, for further analysis. these four snornas are all functional tumor-associated snornas according to recent publications [cit] . in terms of the tissue specificity of hnsc, a recent publication [cit] confirmed that snord116 is one of the potential transcriptomic signatures for hnsc identification, corresponding to its specific expression pattern in these rules."
"the following snorna cluster is hbii-85-29, targeting the antisense sequence of snurf-snrnp-ube3a (transcription unit) [cit] . hbii-85-29 is specifically expressed in the brain and uterus. thus, it may be functionally connected to lgg and uterine corpus endometrial carcinoma (ucec). recent publications confirmed that hbii-85-29 might be functionally connected to the pathogenesis of hypothalamus, further implying the potential relationship between snorna hbii-85-29 and lgg [cit] . therefore, hbii-85-29 may also be sufficiently specific for further subgrouping of the eight tumor subtypes."
"it is interesting to note that precisely the model combination that obtains best classification accuracy is the one proposed as having selected the most relevant features for the problem. this even outperforms the same classification algorithm when trained with all available features. although this situation does not really add knowledge to what doctors already know, it proves that these models can successfully discard additional unimportant information and help to the prognosis of an illness using the patients' symptoms as they were obtained for the presented tests."
"performance of the optimal svm classifier was much better than that of 69 classification rules. the tprs and fprs of the individual classes yielded by such a classifier are listed in table 1 . compared with those yielded by classification rules, the tprs produced by the optimal svm classifier were much higher and fprs were lower (except one), suggesting the optimal svm classifier gave a much better performance. in addition, we also counted the performance of the optimal svm classifier on each of the ten folds. the highest and lowest accuracies for each cancer type were listed in table 2 . furthermore, we also used a 5-fold cross-validation to replace the 10-fold cross-validation in the above procedures. the mcc trends corresponding to the number of used features are shown in figure 2b . the trends of mcc in the two curves were almost the same. the performance yielded by the 5-fold cross-validation was slightly lower than that obtained by the 10-fold cross-validation. it is reasonable because, in a 5-fold cross-validation, fewer samples were used to train the classifiers. furthermore, we also used a 5-fold cross-validation to replace the 10-fold cross-validation in the above procedures. the mcc trends corresponding to the number of used features are shown in figure 2b . the trends of mcc in the two curves were almost the same. the performance yielded by the 5-fold cross-validation was slightly lower than that obtained by the 10-fold cross-validation. it is reasonable because, in a 5-fold cross-validation, fewer samples were used to train the classifiers."
"the first snorna is hbii-52-14, which is a c/ [cit] . as a transcript that is specifically expressed in the brain, hbii-52-14 may target the serotonin receptor (5ht-2c) in the brain, and regulate its functional methylation status [cit] . recent publications confirmed that the expression pattern of 5ht-2c, regulated by our identified snorna, is pathogenic and may be involved in the initiation and progression of low-grade glioma (lgg) [cit] . this result implies that this parameter may be effective in differentiating glioma from other tumor subtypes. hbii-52-10, hbii-52-15, hbii-52-32, hbii-52-5, and hbii-52-4 have also been confirmed to target serotonin, connecting this regulatory function to the potential glioma tumorigenesis."
"furthermore, the mcfs method can generate some informative features. here, 411 features were produced for the problem addressed in this study. based on them, 69 classification rules were produced by the johnson reducer algorithm and the repeated incremental pruning to produce error reduction (ripper) algorithm, which are listed in table s2 . to evaluate the performance of the rules yielded by the above two algorithms, 10-fold cross-validation was performed thrice, yielding the predicted accuracy of 0.750 and weighted accuracy of 0.751. the true positive rates (tprs) and false positive rates (fprs) of the individual classes are shown in table 1 . the confusion map is illustrated in figure 1a ."
"as mentioned in section 4.2.1, the mcfs method can extract informative features. here, we adopted rule learning algorithms to build classification rules. different from the supervised classifier mentioned in section 4.2.2, which was always a black-box method, that is, its classification procedures were not clear and interpretable. the classification rules can provide a clear procedure of classification, and they can give more information for understanding the expression pattern differences of snornas in different cancer types."
"infective endocarditis is a serious infection and its morbidity and mortality rate is still high, with a reported overall mortality rate ranging from 16 to 37.1% the risk of acquiring infective endocarditis is higher among patients with underlying heart diseases including valvular heart disease and congenital heart disease, among those with prosthetic cardiac valves, and among intravenous drug abusers. substantial questions remain regarding the risk factors for infective endocarditis in bacterial infection. the changing profile of infective endocarditis requires continuous epidemiological updating associated infection.usually, the illness is caused by a growth of bacteria on the edges of a defected heart or on the surface of an abnormal valve; after the bacteria enter the blood stream most commonly from dental procedures, tonsillectomy or adenoidectomy, certain types of surgery on the respiratory passageways, but also from procedures involving the gastrointestinal or urinary tract."
mathematically expressed: if we consider the data samples . in that space (h) the decision rule is governed by a simple hyperplane that separates x i into two different classes:
"the following identified cancer subtype-contributing snorna is also a c/d box snorna named hbii-420. hbii-420, [cit] . although limited reports confirmed the contribution of hbii-420 in tumorigenesis, two studies on lung adenocarcinoma [cit] and multiple myeloma [cit] confirmed that with specific potential pathogenic expression pattern, hbii-420 might be functionally related to these two tumor subtypes. for the eight tumor subtypes in this study, differentiating lung adenocarcinoma from other tumor subtypes using hbii-420 is relatively effective."
"hbi-43, as another identified snorna has also been confirmed to participate in the distinction of different cancer subtypes. according to recent publications, such snorna has been functionally related to a specific effective gene trim25 [cit] . such a gene has been identified to have pathogenic expression pattern in breast cancer [cit] and hepatocellular carcinoma [cit] . therefore, it is quite reasonable to speculate that as the regulator of trim25, the expression pattern of hbi-43 may also be sensitive to distinguish different cancer subtypes."
"we also used support vector machines (svms) to classify samples consisting of important features that were selected from the incremental feature selection (ifs) method. first, a set of feature subsets with a step 1 were constructed. after testing the performance of svms on the samples consisting of features from individual feature subsets with 10-fold cross-validation once, we obtained the highest matthew's correlation coefficient (mcc) of 0.881 when the top 443 snornas were used. in addition, we can yield an mcc value of 0.708 when using only the top 72 features. the predicted accuracies for individual cancers, overall accuracies, and mccs by using a different number of features are listed in table s3 . furthermore, we presented the mcc trends corresponding to the number of features involved in building the svm classifiers, as shown in figure 2a, in which the optimal mcc value of 0.881 is marked with a red rhombus. accordingly, we termed the svm classifier using top 443 features as the optimal svm classifier. the confusion matrix generated by the 10-fold cross-validation on this classifier is illustrated in figure 1b, from which we can see that the"
"the present study describes an ongoing multidisciplinary research in which an application of classical models by means of genetic algorithms to a medical problem has been presented. the features selected through the genetic algorithms presented are consistent with the medical literature found [cit] and the models tested have been able to predict the mortality risk with a reasonable degree of accuracy using a relative small amount of samples. this work proves that is possible to identify and discard the most uninteresting features for this analysis using automated learning algorithms, enabling doctors to concentrate in the remaining -most interesting-ones in the specific case of the endocarditis. in this application field, using this small amount of patients and reducing the features needed for each of them, seems as an advantageous feature; as such kind of real data are so costly to acquire."
another research line is the use of the information and experience gathered in these experiments for the development of a case base reasoning system [cit] to solve tasks related to the ones presented above. these would be able to handle the incorporation of new information with the treatment and monitoring the evolution of more patients.
"according to the quantitative rules listed in table s2, the first identified tumor subtype is lgg. in the first rule (rule1), three effective parameters, namely, snord123, u49b, snora1, and u3 have been proposed. according to this rule, the high expression of u3 and low expression of snord123 and u94b may indicate that the potential tumor is lgg. according to recent publications, snord123 is downregulated in patients with glioma; this phenomenon is associated with personalized prognosis [cit] . moreover, a recent systematic analysis on lgg and high-grade glioma confirmed that u3 and u49b may have identical expression pattern compared with this rule in lgg [cit] ."
the data set contains 50 cases of bacterial endocarditis extracted from the evolution of different patients that were admitted into the complejo hospitalario asistencial universitario de burgos (spain).
"for all these reasons, the correct treatment of the patient in the earliest stages as possible, is considered as an interesting objective. to help achieving this objective, this research proposes the use of genetic algorithms [cit] techniques to select the most important features of this illness once the patient is in treatment, helping to predict the mortality risk."
"in addition to this series of snornas, hbii-336 (with a rank of 3 in the feature list, yielded by the mcfs method) is also validated to contribute to the distinction of different tumor subtypes by recent publications. [cit], hbii-336 guides the 2 -o-ribose methylation of 18s rrna a576 [cit] . recent publications also confirmed that the methylation of 18s rrna may be functionally related to the initiation and progression of certain tumor subtypes, such as breast cancer, colorectal cancer, and renal carcinoma [cit] . comparing these findings with those results for the eight tumor subtypes investigated in this study, the expression of hbii-336 can be helpful for tumor subtyping."
"in addition, the mcfs method can produce some informative features, which were the features with the ri scores greater than a cutoff value. a permutation test and one-sided student's t-test were performed to determine the cutoff value of ri scores. features with ri scores higher than this cutoff value were picked up as informative features, which were deemed to be most important for classification."
"the next identified tumor subtype is ucec. tumor-associated snornas, such as aca56 [cit], hbii-336 [cit], snord19 [cit], and u19 [cit], have been screened out using four quantitative parameters for ucec identification. among these four parameters, u19 is a relatively unique parameter with expression level higher than 119, indicating its potential role in tumor subtyping. according to recent publications [cit], snorna u19 (snora74), as an h/aca box snorna, contributes to the regulation of the famous akt/mtor signaling pathway. the expression level of such gene has been screened out and functionally confirmed to participate in multiple tumor subtypes including ucec and gallbladder cancer [cit] ."
"dimensionality reduction methods [cit] involve processes such as feature construction, space dimensionality reduction, and sparse representations among others, which are achieved by using a wide array of techniques such as genetic algorithms [cit], fuzzy systems [cit] and others that investigate complex real problems in fields as medicine [cit], ecology [cit], engineering [cit] and so on."
"subject to eqs. (1) and (2), where c is a constant and p is usually chosen to be 2. a test vector ( i x ) is then assigned a class label depending on whether"
"snornas may be functionally associated with different cancer types. to identify highly related snornas for different cancer types, the mcfs [cit] was first used to rank all available snornas. then, ifs [cit] with a support vector machine (svm) was further applied to identify important snornas with the strong discriminative power for classifying different cancer types, those snornas are further used to produce classification rules for classifying different cancer samples."
"the endocarditis can be diagnosed by many procedures [cit] such as transthoracic echocardiography, transesophageal echocardiography, duke criteria, magnetic resonance, tomography miltislide and by embolisms, etc."
"thus, as we have analyzed above, snornas have been confirmed to contribute to the tumorigenesis in their specific ways. on the other hand, several studies have been reported that mirnas/lncrnas are highly related to different diseases [cit], including tumor, which gives a strong hint for the associations between snornas and different tumors. the analysis of the expression pattern on them is an essential way. this study gave a computational investigation of the snorna expression pattern of different tumors. recently, a systematic study [cit] on the distribution of different snornas in different tumor subtypes based on tcga database has been presented, drawing a detailed blueprint of snornas during tumorigenesis. according to the dataset provided by such study, we firstly screened out the specific expression pattern of snornas in eight candidate tumor subtypes. then, based on some powerful machine learning algorithms, we tried to screen out the core distinctive distributed snornas among such eight candidate cancer subgroups and further established a qualitative snorna-based recognition standard for further tumor subtyping."
"to define a specific decision making problem, one needs to introduce a last-possibly implicit-function. this latter represents a functional relationship between all three dimensions, more specifically the correlation between the different constraints as illustrated by the design space. thus, it models the interdependence of all three constraints. a simple representation of this interdependence can be expressed through an explicit objective function which numerical value is computed as a function of the equipment parameters, the environment's conditions as well as the values of other objective functions. unfortunately such functions are not always available and might remain implicit. in such scenarios, optimization might prove problematic without using appropriate learning tools."
"taking into account the uncertainty on the environment sensing, we may assert that learning-oriented techniques are more efficient. this is emphasized by the proposed classification based on the a priori knowledge criteria on the environment. hence, we believe that such approaches should be particularly addressed by the cr community in the second decade of cr decision making era. we tackled in this article decision making in the sense of a mono-equipment problem. in a multi-equipment context, a higher level of decision (rule, policy, etc.) should specify how equipments cooperate or not. this is out of the scope of this article. however, at the level of each equipment, decision goes back to what has been stated in this article."
"the expert approach relies on the important amount of knowledge collected by telecommunication engineers and researchers. this knowledge is based on theoretical consideration and practical measures on the environment and radio communication parameters. it was first suggested by mitola in his ph.d. dissertation on cr [cit] . through intensive off-line simulations, expert systems are provided with a set of inference rules. these rules are then used on-line to adapt the equipment depending on the context faced by cr equipments. thus, the more available knowledge, the better the equipment can adapt itself to its surrounding dynamic environment. however, this knowledge is usefully as long as if the cr can represent its knowledge in a way that enables to exploit it and to react to the environment by adequate adaptations of its operating configuration. for that purpose, mitola suggested representing the knowledge of cr equipments using a new dedicated language radio communication: \"radio knowledge representation language\" (rkrl) [cit] . this representation of knowledge uses web semantic such as xml (extensible markup language), rdf (resource description framework), and owl (web ontology language). the expert knowledge based approach had a large success especially due to the xg project (next generation) supported by the darpa (e.g., [cit] and for spectrum sharing: [cit] ). as a matter of fact, if the knowledge is well represented and provided to the equipment as a set of rules, the decision making process becomes very simple. however this approach has a few drawbacks:"
"we consider n sensors monitoring a circular path with unit circumference, called p . in an ideal case, the sensors are precisely located on the circular path, but this is not usually true for a randomly deployed network. in order to take this fact into account, we assume that sensors are randomly spread over a ring containing p (see figure 1 ). we assume a symmetric distribution for sensors, that is, the sensor density does not depend on the polar angle and is determined only by the distance from the center. it is generally desired to have more sensors in the vicinity of p . thus, distributions with larger values close to p are preferred. when no effort is made to put the sensor as close as possible to the path (n sensors are spread totally randomly), the uniform distribution is obtained. hence, in the sense of placement efforts, uniform distribution reflects the worst case. we consider uniform distribution to verify our analysis by computer simulation in section 4. our analysis, however, is presented for any given symmetric distribution. also, notice that since the number of sensors is finite and known, poisson distribution, which has been the focus of existing asymptotic analysis, is not applicable."
"there exists various possible algorithm to explore a large set of potential candidates. the most obvious one is probably \"exhaustive search\", where all possible candidates are computed and evaluated in order to find the best solution. however, when the number of candidates grows large, such approaches can become computationally burdensome and miss the imposed decision making deadlines. usually in such contexts, heuristics are preferred. in the context of cr, finding the best solution might not be necessary. instead, the cognitive engine would rather find, within the imposed limited amount of time, a satisfactory solution."
"to conclude, we usually observe in the literature that these constrained based characterizations are implicitly made. thus, usually the assumptions introduced to define the decision making framework are, unfortunately, hardly explained. these assumptions concern what we refer to as the \"a priori model knowledge\". in section 4, we introduce and explain the notion of a priori knowledge and we present a brief state of the art on decision making for cr configuration adaptation using the dca design space. we show that although the design space is the same, depending on the a priori model knowledge, different approaches are suggested by the community to tackle the defined decision making problems."
"then a large set of decision making tools are possible such as: simulated annealing, gas, and swarm algorithms to name a few [cit] . notice that such approaches did not wait for cr to be used on radio technologies. [cit], article [cit] already suggested simulated annealing as a possible solution to deal with channel assignment for cellular networks. g genetic algorithms [cit], swarm algorithms [cit] and insect colony inspired algorithms [cit] h techniques are usually referred to as bio-inspired or evolutionary techniques."
"opportunistic spectrum access is a particularly interesting framework that illustrates the challenge faced when learning under uncertainty. when tackling the general dca problem, described hereabove, while considering k channels to probe, the problem that consists in maximizing the cumulated throughput of the user over the number of transmission trials appears to be consistent with a mab paradigm [cit] . in a nutshell, based on the analogy with the one-armed bandit (also known as slot machine), it models a gambler sequentially pulling one of the several levers (mab) on the gambling machine. every time a lever m is pulled, it provides the gambler with a random income usually referred to as reward. although we assume that the gambler has no a priori information on the rewards' stochastic distributions, he aims at maximizing his cumulated income through iterative pulls. in the osa framework, the su is modeled as the gambler while the frequency bands represent the levers. the gambler faces at each trial a trade-off between pulling the lever with the highest estimated payoff (known as exploitation phase) and pulling another lever to acquire information about its expected payoff (known as exploration phase). we usually refer to this trade-off as the exploration-exploitation dilemma. if the problem is assumed modeled as a mab framework an interesting way to tackle the problem is to use the class of so-called upper confidence bound algorithms n (ucb) [cit] . the main advantage of ucb methods for cr is to offer a balance between exploration and exploitation phases without interrupting the communication process, i.e., while providing a certain service to the user [cit] . namely, a cr based on ucb can jointly communicate and learn. thus it avoids the instantiation of two steps: a learning step during which the user has to wait. and a communication step that depends on how well the first step performed. it is worth noticing that the suggested illustration, in the article, is based on the so-called ucb 1 . this latter has been selected for its rather low computational complexity compared to other techniques in the literature."
"the paper is organized as follows. section 2 introduces the network model and defines the problem. our coverage analysis is presented in section 3. section 4 includes computer simulations verifying our analysis. finally, section 5 concludes the paper."
"thus, depending on the considered environment, specific sensors are to be designed [cit] . the captured -and/or computed-metrics by the sensors are then processed by the decision making engine. the kind of process highly depends on the quality of the metrics (level of uncertainty on the captured numerical value for instance) as well as the global information held by the cr. finally, the made decisions are translated into appropriate bandwidth occupation and power allocation actions."
"in this article, we suggest to provide a brief discussion on the decision making problems seen from cr equipment's perspective and discussed in the literature as well as the main solutions suggested to tackle these problems. for that purpose, we revisit in section 2 the rise of cr paradigm from which we discuss a basic definition. then, in order to objectively compare the techniques introduces to address cr related decision making problem, we describe a conceptual object referred to as design space in section 3. this conceptual object was introduced in the literature [cit] to suggest that the cr design problem, from the decision making perspective, is better defined by a set of constrains rather than by a set of degrees of freedom. thus, this section reminds us of the three considered dimensions of constrains viz., the environment's constraint, the equipment's limits and the user's needs. moreover, in section 4, we define and use the notion of a priori knowledge, to show that the tackled challenges by the radio community to solve configuration adaptation decision making problems have often the same design space, however they differ by the a priori knowledge they assume available on this design space. consequently, in section 4, we suggest the a priori knowledge as a classification criteria to discriminate the main proposed techniques in the literature to solve configuration adaptation decision making problems. section 5, extends previous classification by adding the impact of observation accuracy and the benefit of learning techniques in such contexts. section 6 concludes this analysis."
"as illustrated in figure 1, a full cognitive cycle b demands at every iteration five steps: observe, orient, plan, decide, and act. the observe step deals with internal as well as external metrics. it aims at capturing the characteristics of the environment of the communication device (e.g., channel state, interference level or battery level to name a few.). this information is then processed by the three following steps: orient, plan, and decide steps, where priorities are set, schedules are planed according to the systems constraints, and decisions are made. finally an appropriate action is taken during the act step (such as send a message, reconfigure, modify power level to name a few). in order to complete the cognitive cycle, a last and final step is needed to enhance the decision making engine of the communication device: the learn step. as a matter of fact, learning abilities enable communication equipment to evaluate the quality of their past actions. thus, the decision making engine learns from its past successes and failures to tune its parameters and adapt its decision rules to its specific environment. learning can consequently help the decision making engine to improve the quality of future decisions."
"i this document is presented as a survey of the various suggested decision making architectures for cr. we notice however, that except the one designed by mitola, during the darpa xg program, and those designed and implemented by virginia tech, the community around this topic seems thin and advances slowly toward efficient architectures. other suggested architectures relying mostly on bio-inspired techniques tackle spectrum resource allocation related problems."
"in this section, we demonstrate the accuracy of our analysis via computer simulations. we have inspected two scenarios for the sensors sensing range. in the first scenario, we assume a network with n sensors all having a fixed sensing range equal to r. the sensors are uniformly deployed inside a ring around the circular path, where p has unit circumference. in the second scenario, the sensors sensing range is also uniformly distributed between 0 and r max . a zero sensing range can represent a dead sensor. we evaluate random properties such as the full coverage, number of uncovered gaps, tightness of the bound presented in (6), the intruder detectability, and the portion of the covered path using simulation, and compare the results with our theoretical analysis."
"our hypothesis is that the dynamics of the reservoir network can enhance the spatial encoding of static inputs by means of a more nonlinear representation, which should consequently improve the task-specific performance. moreover, we expect an improved performance when applying larger reservoirs, i.e. when using an increased dimensionality of the kernel expansion. using attractor-based computation and by considering purely static input patterns, we systematically test the contribution of the network dynamics to the spatial encoding independently from its temporal effects. a statistical analysis of the distribution of the network's attractor states allows to access the qualitative difference of the encoding caused by the network's recurrence indepently of the task-specific performance."
"it is clear from figure 4 that the results from the approximate analysis are fairly close to the exact analysis and the simulations. due to the complexity of the evaluation of exact analysis, we compare the rest of our simulation results with the approximate analysis presented in section 3.2 to characterize the coverage properties of the network."
"k to the best of authors' knowledge such studies have only been conducted when dealing with osa problems. consequently, the presented results are exploratory and need further investigations to fully confirm them. we find however the overall discussion interesting to capture cr related decision making challenges."
"in addition to the cdf of the arc length, we use the mean value of a for our approximate analysis. recall that for an arbitrary random variable z distributed over [a, b],"
"exploiting portions of the spectrum to unlicensed usage was a first step to introducing alternative frequency management schemes. rethinking the main regulatory frameworks imposed for decades is the next step. as a matter of fact, during the last century, most of the meaningful spectrum resources were licensed to emerging wireless applications, where the static frequency allocation policy combined with a growing number of spectrum demanding services led to a spectrum scarcity. however, several measurements conducted in the united-states first, and then in numerous other countries [8, [cit], showed a chronic underutilization of the frequency band resources, revealing substantial communication opportunities."
"in this section, we discuss some of the limits related to the idealized cr concept before introducing the so called dca problem. several questions arise when designing a cr engine. we summarize our conceptual approach, presented in article [cit], to dimension the decision making and learning abilities of a cognitive engine. thus, we introduce the notion of design space as a conceptual object that defines a set of cr decision making problems by their constraints rather than by their degrees of freedom. we identified, in our analysis study, three dimensions of constrains: the environment's, the equipment's, and the user's related constrains."
h to the best of authors' knowledge swarm algorithms have only been exploited in case of resource allocation. no complex configuration adaptation decision making engine was found in the literature based on such techniques.
"in the following, we present an approximate analysis simplifying our path coverage study. the idea of this approximate analysis is to consider a model where a set of fixed-length arcs are spread randomly over p instead of using the actual random-sized arcs. the length of these fixed arcs is equal to the mean value of the randomsized arcs in the original case. we denote the mean value of these random arcs with μ a . in this case, it can be shown that the number of uncovered gaps on p is distributed as follows [cit] :"
"in this article, we presented a brief yet original retrospective view on the first 10 years of cr. more specifically of the different challenges faced by the cr decision making community and the suggested solution to answer them. we state that most of these decision making models have the same design space however they differ by the a priori knowledge they assume available. consequently, we suggested the \"a priori knowledge\" as a classification criteria to discriminate the main proposed techniques in the literature to solve configuration adaptation decision making problems. moreover as a qualitative and prospective analysis, we depicted through an toy example the impact of observation errors and uncertainty on cr decision making engine."
"we rest at the fact that pushing analytics towards the edge is feasible because of the increase of computational power on sensing & actuator devices. their capabilities enables them to reduce network traffic and latency by supporting innetwork real-time data analytics. however, analytics over contextual data at the edge should be imperatively provided with high quality of outcomes, e.g., low prediction errors, avoiding false alarms, taking into account efficient communication due to the above-mentioned constrains [cit] . within an edge network, it is deemed appropriate to introduce a methodology for providing quality aggregation and predictive analytics tasks departing from the traditional innetwork data processing/delivery methods by exploiting the computing capability of the edge nodes."
"as far as we can track the emergence of a cr literature and to the best of authors' knowledge, the today's plethoric publications started with three major contributions: on the one hand, the federal communication commission (fcc) [cit] the inefficiency of static frequency bands' allocation to specific wireless applications, and suggested cr as a possible paradigm to mitigate the resulting spectrum scarcity [cit] . then, haykin in article [cit], suggested a simplified cognitive cycle to represent cr decision making engines as illustrated in figure 2 . haykin's model tackled the particular dynamic spectrum management problem and discussed different possible models to design future cr networks. article [cit] inspired many studies on cr application fields such as theory based cognitive networks. eventually, this two subjects led to two very actives research fields as illustrated in this recent surveys [cit] . on the other hand, while the two contributions [cit] focus on spectral efficiency, rieser suggested, through various publications, synthesized in his ph.d. dissertation, [cit], a biologically inspired cr engine that relies on genetic algorithms (ga). to the best of authors' knowledge, it was the first suggested and partially implemented cr engine presented to the community."
"however, we have to remark that the introduced measure g(d) does not strictly correlate with the task-specific performance. although the esn reassigns a greater amount of information content on the last n − d pcs than the elm (cf. fig. 5"
"within the basic cognitive cycle, we focus in this section on the analysis step, and more specifically on learning and decision making. we mainly find, in the literature two approaches. on the one hand, some of the articles focus on implementing smart behavior into radio devices to enable more adequate configurations, adapted to their environment, than those imposed by radio standards. as a matter of fact, standard configurations are usually over dimensioned to meet the requirements of various critical communication scenarios. this approach mainly focuses on one equipment, ignoring the rest of the network. we refer to the problem related to the first approach as dynamic configuration adaptation (dca) problem. on the other hand due to a more pressing matter, most of cr related articles focus on spectrum management. these latter articles aim at enabling a more efficient use of the frequency resources because of its scarcity. this second problem is usually referred as dynamic spectrum access problem (dsa)."
"the techniques based on expert systems can, however be supported by several other tools (some are discussed later) to help them acquire new knowledge on the environment or help them avoid conflicts between different configuration adaptation rules. a similar approach, based on an ontology to model the knowledge of the decision making engine was recently suggested [cit] . where a common language to radio devices is suggested based on an ontology, expressed in owl and implemented on the usrp card [cit] using gnu radio [cit] ."
"the interaction between all three constraints is further emphasized through the notion of design space. we denote by cr design space an abstract three dimensional space that characterizes the cr decision making engine as shown in figure 4 . it is indeed abstract since it does not have any rigorous mathematical meaning but it is only used to visually and conceptually illustrate the dependencies of the cr decision making engine to the \"design dimensions\": environment, parameters (usually referred to as knobs) and objectives (or criteria defined from the user's expectations)."
"p(x) dx. kl indicates the amount of information lost when en j approximates the actual vectors at sans i due to undelivered vectors. for assessing the quality of predictive analytics, we measure the discrepancy δ in the linear prediction performance w.r.t. rmse and the model fitting discrepancy δ in the actual and approximated linear models as defined in section ii-b."
"there are two major concerns in idm:(c1) if θ is relatively high, san i scarcely updates en j with actual vectors. hence, en j loses significant information, which is expected degrading analytics results. we encounter the same situation if the prediction error e t is relatively small; if for a low θ we encounter e t θ, en j does not follow the data stream. this is happening e.g., when the predictor f i of san i is very accurate. this is counterintuitive, since we desire to have an accurate predictor f i, but its instantaneous outcome for decision making leads to the situation of information loss on en j. figure 1 (upper) shows the case where f i predictor (here, exponential smoothing) at san i produces predictions close to the actual data, thus, resulting in no communication and thus information loss at en j, which re-constructs the data with g j (adopting exponential smoothing). (c2) if there are certain outliers or novel cases/significant events in san i, san i delivers the associated vectors to en j and then transits back to the state of non delivering vectors to en j. in this situation, en j accumulates most of the time outliers and novel vectors and, again, the re-constructed data do not follow the in-between actual data; see figure 1 (lower)."
"as a result of theorem 1 and lemma 1, we have the following corollary. corollary 1. the probability of the full path coverage, p f, is"
"since our analysis studies the effect of the number of nodes on the path coverage of a finite size network, it can readily be used in the design of practical networks. in fact, using our results, one can determine the number of nodes in the network to satisfy a desired level of coverage. an example is provided."
"realistic cr frameworks need to take into account a large set of possible configurations, however, as mentioned hereabove through the gödel-paradox, the decision making engine also needs to be constrained in order to avoid the system to crash. we argue in the rest of this paragraph that, in general, cr decision making problems are better defined by their constraints rather than by their degrees of freedom."
"starting materials were commercially available and were used without further purification. the synthesis was adapted from a procedure reported previously [cit] . the hydrochloric acid catalyzed reaction of 5-bromoisatin (8,83 mmol) and thiosemicarbazide (8,83 mmol) in ethanol (50 ml) was refluxed for 6 h. after cooling and filtering, crystals suitable for x-ray diffraction were obtained from an acetonitrile solution."
"in the ideal case, all sensors are located exactly on the path. this, however, is not a practical assumption for randomly deployed networks. to consider the inaccuracy of the sensors locations, we assume that sensors are inside a ring containing the circular path. as a result, the portion of the path covered by any given sensor is not deterministic. moreover, other factors may affect the sensing range of a sensor. thus, our analysis is not based on a fixed sensing range. indeed, we first develop a random model for the covered segment of the path by each sensor. then, we study the distribution of the number of uncovered gaps on the path. the full path coverage is a special case where the number of gaps is zero. this is used to determine a tight bound on the number of active sensors assuring the full path coverage with a desired reliability. also, we find the probability of having all possible gaps smaller than a given size. this probability reflects the reliability of detecting an intruding object with a known size."
"an arbitrary point x on p remains uncovered when there is no a i covering it. this is equivalent to having none of l i 's within an arc with length μ a whose right end point is x. there are n sensors in the network, hence, the probability of having x uncovered, μ v, is"
"notice that the length of a i 's depends on the location of the sensor within the ring-shaped network area and its sensing range. considering an arbitrary point as the origin on p and choosing the clockwise direction as the positive direction, each a i starts from l i and continues (clockwise) until r i, (figure 2 ). in other words, l i determines the most left point of the arc and r i specifies the most right point of the arc. there are two noteworthy issues here. first, the size"
"g it is indeed a very restrictive case of dca and dsa where a centralized entity, seen as the cognitive agent (ca) assigns frequency channels to its users depending on the channel conditions."
"this definition is quite general. it can incorporate simple designs as well as complex ones. most of the published articles deal however with a restricted problem: spectrum management. in such context, the term environment finds more specific definitions such as the followings to name a few: environment:"
"ideally speaking, cr concept-supported by an sdr platform-opens the way to infinite possibilities. autonomous and aware of its surrounding environment as well as of it own behavior (and thus of its own abilities), any part of the radio chain could be probed and tested to evaluate its impact on the device's performance. this however implies that the equipment is also able, in its reasoning process, to validate its own choices. namely, it must self-reference its cognition components [cit] . unfortunately, this class of reasoning is well known in the theory of computing to be a potential black hole for computational resources. specifically, any turing-capable (tc) computational entity that reasons about itself can enter a göel-turing c loop from which it cannot recover [cit] ."
"wireless sensor networks (wsns) have many applications in security monitoring. in such applications, since it is essential to keep track of all activities within the field, network coverage is of great importance and must be considered in the network design stage."
"using (6), it is straightforward to find an upper bound on n guaranteeing a desired level of coverage, p. later, our simulations show that this bound is in fact very tight."
"due to their lack of flexibility, expert decision making techniques seem to be the most vulnerable to uncertainty. as a matter of fact, their decision making process, based on either rules or predefined policies, leads the cr to consider all observations as being correct. hence a sensing errors provokes a behavioral error. ga based decision making engines rely on explicit relationships between parameters, observations and criteria. consequently, sensing errors can highly impact the selection process as it introduces biases in the performance evaluation of the different candidates. moreover, generation after generation, these errors would probably propagate leading to an inefficient selection process. such decision making engines would probably need to interact with environment to test the candidates and confirm their performance. in such scenarios, the ca might be able to mitigate the impact of sensing errors at the cost however of a burdensome process. ann are usually depicted, when they fulfill given requirements, as universal approximators. in other words, if the neural network is correctly designed to fit the decision making problem, it can efficiently learn the implicit relationship that exists between parameters, observations and criteria. consequently even when sensing errors are present, the learning process can lead to capture average patterns and thus appropriately mitigate their impact. thus, the more learning abilities and flexibility a decision making shows the more robust it become to uncertainty and sensing errors. this analysis is further depicted in figure 6 . thus, we can summarize this intuitive insight view as follows: the more the decision making technique is at the right of figure 6, the more robust to observation flaws it seems to be. notice that the learning process enable the ca to acquire knowledge on its environment. consequently a learning process fully achieved should lead to an expert decision. figure 9 illustrates moreover a vertical axis that suggests, when possible, that collaboration helps cr users to acquire through diversity a better information on their environment. and thus, it enables them to improve the performance of their decision making engine considering a given uncertainty level."
e(g) can be used to find an upper bound on the number of nodes in the network guaranteeing the full path coverage with a given reliability. this is presented below.
"l as mentioned earlier, we consider in this article figure 9 decision-making techniques classification based on a priori knowledge in the context of noisy sensing."
"in section 5.2 an osa scenario based on a mab model, described in article [cit], is summarized and illustrates the impact of observation errors on decision making for cr. in the following section, however, we introduce prior knowledge as a classification criteria among the main learning and decision making tools suggested in cr articles."
"the increase of computational capacity associated with (rather) cheap flexible hardware technologies (such as programmable logic devices, digital signal processors and central processing units) offer a glimpse into new ways to designing and managing future non military communication systems."
"we see in figure 8 the impact of false alarms on the proportion of time a cognitive engine, relying on the ucb 1 [cit] algorithm, choses the most available channel (considered as optimal in this case). this proportion increases as the number of trials grow large, thus as the su learns more on the availability of the bands. we can see that with a probability of false alarm equal to zero, the decision making engine needs 1,000 trials to obtain a selection rate of 72% of the most available channel. this ratio falls to 50% after 1,000 trials for a probability of false alarm of 0.4, which is quite decent considering the scenario and the heavy deterioration of sensing accuracy. in fact, in this case, a little bit more than twice the number of iterations has been necessary compared to a perfect sensing scenario. but after 10,000 trials, the ratio grows to achieve 96 and 92%, respectively. consequently, the cognitive engine is able to communicate and to converge towards the most available band in spite of the sensing errors it is suffering."
"we first prove that the optimal forwarding time t * for problem 1 exists provided in theorem 1 based on the principles of ost [cit] and provide an optimal forwarding rule for evaluating it at theorem 2. then, we report on the two proposed variants."
"since the original definition suggested by joseph mitola iii, several other definitions were proposed to define the edges of cr [4, [cit] . however, defining cognition is, in general, a harsh task. in the context of cr, basic cognitive abilities are considered:"
"analyzing the impact of uncertainty and sensing errors on the performance of a cr decision making engine is very difficult. however due to the importance of this problem to the community, we suggest as a closing point of this article, an intuitive and brief insight view on this matter. within this framework we consider that the sensing information we capture from the environment may contain errors. then we describe the potential consequence of such errors on the performance of class of algorithms previously classified."
"j these same techniques, based on a mab model, prove to be efficient to tackle some dsa related problems as already discussed in section 3.2."
"in the following section we extend the analysis to the specific and practical context of imperfect sensing. as a matter of fact the impact of sensing errors can be significant on decision making techniques. however, unfortunately, very few studies seem to tackle this specific problem within cr contexts. hence, we further discuss this matter hereafter."
"many baseline approaches [cit] (and the references therein) collect all data from iot environments, e.g., wireless sensor networks (wns) to centralized locations for centrally performing analytics tasks requiring, thus, all devices to continuously sensing and communicating. however, due to bandwidth, latency and energy constrains alternative methodologies have been studied [cit] especially for wsns based on selective forwarding. in these approaches data are conditionally transmitted to central locations reducing communication overhead. however, such approaches focus only on communication efficiency and are unaware of the analytics tasks performed at the destination, thus, cannot be adopted to support high quality of analytics. advanced selective forwarding methods [cit] deal with dynamic optimal decisions of finding the best time to deliver data in light of communication efficiency and reconstruction error minimization at the destination. nonetheless, such optimal decision making is limited on communication overhead, not applied on the network edge and not taking into account its impact on the quality of advanced analytics like aggregation and predictive tasks. from the edge-analytics perspective, recent works [cit] exploit the computational power of devices to launch (lightweight) algorithms directly at the data sources. however, such approaches are unaware of communication efficiency in the edge network as supported by the above-mentioned selective forwarding approaches. our previous work [cit] investigates the impact of a predictionbased selective forwarding decision, purely from the communication objective, on aggregation and predictive analytics. this signals the necessity of introducing a hybrid and sophisticated decision making model on when & which data to process and deliver for trading between quality of (advanced) analytics and communication efficiency at the network edge. our proposed method in this paper advances on time-optimized data forwarding and data processing decisions based on the historical patterns of data forwarding decisions and the predictive capability of the edge nodes to determine the best time and the most appropriate data to deliver in light of maximizing the quality of aggregation and predictive analytics tasks at the destination being, in parallel, communication efficient. this is achieved based on a qualityaware, intelligent monitoring scheme over the cumulative reconstruction error at the destination under the principles of the optimal stopping theory (ost) [cit] . our contribution is summarized as follows:"
"our goal in this work is to investigate the quality of the path coverage by the network. to this end, we study the following features of the path coverage which provide us with a concrete insight to the performance of the network."
"to apply theorem 1 for finding the number of uncovered gaps on p, we first prove the uniformity of the arc distribution over p in the following lemma."
"finally, based on the here above presented analysis, all configuration adaptation problems seem to have the same roots. however, to define a specific problem among the set of possibilities in the design space, prior knowledge is important. this latter notion is further detailed in section 4, where a classification of decision making tools as a function of prior knowledge is suggested. nevertheless, the general dca problem can be described as the most general decision making design space that we can state as follows [cit] :"
"(1) observation: through its sensors the cr gathers information on its environment. raw data and preprocessed information helps the agent to build a knowledge base. in this context, the term environment is used in a broad sense referring to any source of information that could improve the cr's [cit] . as illustrated, an agent, usually referred to as ca faces an environment in a broad sense. the ca repeats the cognitive cycle where he observes the environment, analyzes the collected information and decides the next action to take. notice that the arrow action could suggest always an action on the environment. this is possible in order to evaluate the reaction on the environment to given stimuli. however, the arrow also suggests an action on the cr in order to adapt to the environment. behavior (internal state, interference level, regulators' rules and enforcement policies, to name a few). (2) analysis/decision: this macro-step, presented as a black box in this case, includes all needed operations before given specific orders to the actuators (i. e., before reconfiguration in cr contexts). depending on the level of sophistication, this step can deal with metric analysis, performance optimization, scheduling, and learning. (3) action: mainly parameter reconfiguration and waveform transmission. a reconfiguration management architecture needs to be implemented to ensure efficient and quick reconfigurations [cit] ."
"both variants require a vector prediction algorithm f i at san i following the evolving nature of the data streams and a reconstruction algorithm g j at en j that supports the analytics tasks for the pair (i, j). we are seeking to reduce the computational power for prediction and reconstruction at san and en thus using a small fraction of their computing power we adopt the multivariate exponential smoothing [cit], used for time series forecast, as an ideal predictor with computational complexity o(d) in a d-dimensional space 2 . exponential smoothing weighs the current vector with the historic vectors and is adopted as the function f i for predictingx and as the function g j for re-constructing x. at time t, a smoothed vector s t is calculated by using the current vector x t and the previous smoothed vector s t−1 :"
"(iv) distribution of the covered part of the path. the covered part of the path, c, has a stochastic nature and its distribution provides a general view of the entire path coverage. in fact, the covered part of the network reflects the combined effect of the number of gaps and their sizes. we derive the cdf of c, f c (x)."
"in this paper, we studied the path coverage of a random wsn when neither the area size nor the number of network nodes were infinite. hence, the widely used boolean model was no longer valid. moreover, due to the randomness of the sensors placement over the area, network coverage was nondeterministic. thus, a probabilistic solution was taken for determining the network coverage features. our analysis considered the number of gaps, probability of full path coverage, probability of having all uncovered gaps smaller than a specific size, and the cdf of the covered length of the path. all these characteristics were found as a function of the number of sensors n. we also proposed a tight upper bound on required n for full coverage. through computer simulations, we verified the accuracy of our approach. since our study was performed for finite n, using our results on various features of path coverage, one can find the necessary number of sensors for a certain quality of coverage."
"thus, sdr systems are defined only from the design and the implementation perspectives. consequently it appears as a simple evolution from the usual hardwired radio systems. however, with the added software layer, it is technically possible with current technology to control a large set of parameters in order to adapt on the fly radio equipment to their communication environment (e.g., bandwidth, modulation, protocol, power level adaptation to name a few). nevertheless the control and optimization of reconfigurable radio devices need the definition of optimization criteria related to the equipment hardware capabilities, the users' needs as well as the regulators' rules. introducing autonomous optimization capabilities in radio terminals and networks is the basis of cognitive radio (cr), term also suggested and coined by joseph mitola iii [cit] ."
"we believe that this analysis made on the first 10 years of exploration of decision making for cr may help gaining perspective on the topic and thus help addressing this research domain for the next coming 10 years. b it is called full cr to oppose it to other simplified versions suggested in the literature [cit] . c a specific example of such paradox can be illustrated by the following sentence: 'this sentence is false!' [cit] as suggested by mitola during a recent seminar at supélec, http://www.rennes.supelec.fr/ren/rd/scee/seminaire.html. d notice that this assumption introduces the notion of satisfactory behavior. we oppose it to rational thinking where the decision making engine always aims at the most rewarding option. thus when the decision making engine needs to learn in an uncertain environment, satisfaction based reasoning can be introduced to accelerate the convergence rate of learning algorithms for instance. e [...] \"trade, lease, and rent of licenses were possible without incurring excessive administrative procedures and overhead costs\" [cit] ."
"in this article although we cannot avoid mentioning cr applications from spectrum management perspective, we focus on the decision making and learning mechanisms designed to deal with broader frameworks, i.e., configuration adaptation problems. thus, spectrum management problems are, from the equipment point of view, but a subset of configuration adaptation problems."
"with the advent of sdr technology, it became, at least theoretically, possible to design agile systems capable of switching from one frequency band to another depending on given communication constraints. [cit] several task forces and researches suggested new frequency management policies and regulatory frameworks to enable efficient use of the spectrum resource [8, [cit] . the consequences of this new framework are that the spectrum management model of today is abolished for large parts of the spectrum. instead, \"free\" spectrum trading becomes the preferred mechanism and technical systems that allow for the dynamic use and re-use of spectrum becomes a necessity [cit] ."
"proof: since y t are non-negative, problem 1 is monotone [cit] thus the optimal time t * is obtained by the onestage look-ahead optimal rule (1-sla):"
n ucb algorithms are given here as an example of learningoriented approach. but the philosophy and conclusions of this section would match other learning techniques.
"that osa problems are but specific instantiations of dca problems. m from a dca problems perspective, a lever is a specific configuration to be tested. thus in osa, it refers to a band to probe for instance."
"reservoir computing (rc), a well- established paradigm to train recurrent neural networks, is based on the idea to restrict learning to a perceptron-like read-out layer, while the hidden reservoir network is initialized with random connection strengths and remains fixed. the latter can be understood as a \"random, temporal and nonlinear kernel\" [cit] providing a suitable mixture of both spatial and temporal encoding of the input data in the network's hidden state space. this mixture is based upon three key ingredients illustrated in fig. 1 : (i) the projection into a high dimensional state space, (ii) the nonlinearity of the approach and (iii) the recurrent connections in the reservoir. on the one hand, the advantages of a nonlinear projection into a high dimensional space are beyond controversy: so-called kernel expansions rely on the concept of a nonlinear transformation of the original data into a high dimensional feature space and the subsequent use of a simple, mostly linear, model. on the other hand, the recurrent connections implement a short-term memory by means of transient network states. due to this short-term memory, reservoir networks are typically utilized for temporal pattern processing such as time-series prediction, classification and generation [cit] . one could argue that a short-term memory could also be implemented in a more simple fashion, e.g. by an explicit delay-line. but we point out that the combination of spatial and temporal encoding makes the reservoir approach powerful and can explain the impressive performance on various tasks [cit] . however, it remains unclear how the network dynamics influence the spatial encoding of inputs."
. such prediction capability yields san able to decide whether to send x to its en or not for processing based on a θ-based idm rule:
"our target is to compare ovf (hovf) with idm variants in terms of communication overhead, reconstruction error, quality of aggregation tasks, and quality of predictive analytics (regression performance and model fitting). we measure the percentage of communication, i.e., context vectors transmitted by each model for each pair (san i, en j) against the baseline solution, which forwards all actual vectors from sans to en. in terms of analytics quality, we examine the reconstruction error a due to undelivered vectors and the aggregation analytics outcome γ adopting the symmetric mean absolute percentage error (smape) per san. we use smape as a quality metric due to its unbiased properties [cit] . moreover, we adopt kullback-leibler (kl) divergence as a quality metric to measure the the information loss en experienced due to the applied models over each reconstructed vector dimensioñ x from the actual dimension x after estimating their pdfs p(x) and p(x), respectively, defined by:"
"in this work, unlike most existing studies which focus on asymptotic setups, we study the path coverage of a finite random network (in terms of both network size and the number of nodes). as a result, the boolean model is not accurate. alternatively, the methodology of this work is based on some results from geometric probability. our focus is on the path coverage for a circle, but extension to other path shapes is briefly discussed."
"since the early 90s, [cit] in article [cit] : a trend that has the potential to change the current industrial structure is the emergence of alternative spectrum management regimes, such as the introduction of so called \"unlicensed bands\", where new technologies can be introduced if they fulfil some very simple and relaxed \"spectrum etiquette\" rules to avoid excessive interference on existing systems. the most notable initiative in this area is the one of the federal communications commission (fcc, the regulator in usa) in the early 90s driving the development of short"
"path coverage is one of the monitoring examples, where wsns are deployed to sense a specific path and report possible efforts made by intruders to cross it. in a manual network deployment, the desired level of the path coverage can be achieved by proper placement of the sensors over the area. when it is not possible to deploy the network manually, random deployment, for example, dropping sensors from an aircraft, is used. due to the randomness of the sensors location, network coverage expresses a stochastic behavior and the desired (full) path coverage is not guaranteed. thus, a detailed analysis of the random network coverage can be ultimately useful in the network design stage to determine the node density for achieving the desired area/path coverage."
"thus, the purpose of this new concept is to autonomously meet the user's expectations, i.e., maximizing his profit (in terms of qos, throughput or power efficiency to name a few) without compromising the efficiency of the network. hence, the needed intelligence to operate efficiently must be distributed in both the network and the radio device."
"following lemma 1, in order to find the distribution of the number of gaps on p, we need f (·) or in our case f a (·), the cdf of a i 's. notice that a i 's are independent and identically distributed (i.i.d) random variables. we find f a (·) in the appendix for arbitrary distributions of sensor location and sensing range."
"remark 2. in many wsns, the number of active sensors in the network changes with time. this can be due to, for example, sleep scheduling or death of some nodes. since our analysis is provided for arbitrary n, it can accommodate such situations, simply by replacing n with n(t) in relevant equations. consequently, the coverage can be studied as a function of time."
"in addition to studying the number of gaps, we present a simplified analysis for deriving the cumulative distribution function (cdf) of the covered part of the path. this simplified analysis is based on using the expected value of the covered part of the path by a sensor instead of considering the precise random model. we observe that the simplified analysis can provide a fairly accurate approximation of the path coverage."
"motivation: let us consider the following motivating scenario that demands high quality analytics & efficient communication at the edge for car driver mirco-sleep identification [cit] . in-car and driver mood-fatigue detection sensors locally sense the surrounding environment of the vehicle, road, and driver's physiological, facial and driving behavior [cit] and transmit data via 5g towards the cloud for processing/classification and alert the driver/emergency services if irregular patterns are detected. we identify four cases: (a) the driver fell into a micro-sleep and the system identifies that; (b) the driver is awake while the system identifies a microsleep pattern; (c) the driver is awake and the system identifies that; (d) the driver fell into a micro-sleep and the system did not identify that. case (a) demands low latency and realtime reaction to prevent accidents with high certainty on the analytics outcome; using unreliable broadband for data transmission cannot support real-time identification leading to horrible consequences. case (b) encounters a false alarm, e.g., due to missing or obsolete transmitting values, resulting in bad consequences, e.g., the driver got shocked by the risen alarm causing the car go off the road. in case (c) the car sensors continuously transmit data without any action occurrence, thus, resulting in humongous redundant values and unnecessary bandwidth consumption increasing latency. case (d) shows the importance of high quality of analytics as a micro-sleep is not identified, thus inducing disastrous consequences. from such cases, it is challenging to support sophisticated decisions on when and which data to process and deliver for supporting high quality of real-time analytics at the edge taking into account the induced communication overhead and latency. the research challenges this paper focuses on are: (1) deciding which data to communicate at the edge network without loosing quality of data and analytics outcomes at destination; (2) deciding when to deliver data in light of obtaining high quality of analytics; (3) reducing unnecessary communication among edge devices and/or cloud for saving bandwidth and decreasing latency."
"this defined cr decision making framework was first analyzed by rieser and rondeau. they suggested the use of gas to tackle this framework [cit] . gas were first designed to mimic darwin's evolutionary theory and are well known for their capacity to adapt themselves to a changing environment. without using our formalism, their study showed that under what we define as design space and with the described a priori knowledge, the gas provide cognitive radios with an efficient and flexible decision making engine. but we cannot consider their model as a generality for all cr use cases, so that other solutions have to be considered additionally. further details on the different versions suggested and implemented by virginia tech can be found in the following recent survey [cit] . i notice, that once again, prior knowledge can substantially enhance the behavior of these algorithms. an interesting illustration can be found in article [cit] in the case of gas based decision making engines."
"focusing on the increase of sensing & computing devices in internet of things (iot) environments, delivering data continuously towards centralized locations e.g., cloud, is constrained by bandwidth, energy, computational power and data storage. aggregation & predictive analytics at the edge of an (iot) network is an emerging area [cit] trying to overcome these constrains by analyzing data close to the sources. analytics at the edge over dynamic data is different from big data analytics over data at rest. it means carrying out the same kind of analysis, but moving more of it to the edge of the network, e.g., a car, an agricultural equipment in the field, or any other industrial device and exploiting only the local available resources. pushing as much computing workload for analytics (e.g., regression/predictive models, outliers/concept drift detection), as close to the edge as possible brings serious benefits, particularly where communication costs are high or where instant action/decision is needed. but, today's edge capabilities are still relatively unsophisticated in light of quality of analytics, lacking anything like the computing power cloud can provide."
"in the following, we find the cdf of the intersection between the sensing area of the sensors and p, called f a (x). first, we study the situation where sensors have a fixed sensing range r and they are uniformly distributed over the ring. then, we investigate the general case where sensors can have a random sensing range varying from r 1 to r 2 and have any symmetric distribution over the ring."
"now having the cdf of d and using the relation between d and a in (a.5) and (a.6), we will derive f a (x). to this end, one can state"
"the dsa encompasses all suggested approaches that emerged from the early definitions of efficient and \"free\" spectrum access or trading. [cit], article [cit] suggested one possible and simple taxomony f to classify the different suggested spectrum management approaches as illustrated in figure 5 . three main approaches can be discriminated: dynamic exclusive use model, open sharing model (spectrum commons model), and hierarchical access model:"
"as illustrated through the notion of basic cognitive cycle, decision making, and learning rely on prior observations of the environment. consequently, the performance of the implemented decision making tools highly depends on the quality of the observations. unfortunately, we could not find substantial quantitative material evaluating the impact of sensing errors on decision making and learning tools. thus, we suggest to qualitatively k discuss, in this section, the impact of sensing errors on the previously discussed decision making tools for cr. for that purpose we rely on a specific problem borrowed from the osa l community to illustrate this discussion where the problem of decision making in the context of sensing errors is clearly formalized and the impact of such errors on the considered learning algorithm's performance is quantified."
"mitola [cit] defined cr, in his ph.d dissertation as follows: the term cr identifies the point at which wireless personal digital assistant (pdas) and the related networks are sufficiently computationally intelligent about radio resources and related computer to computer communication to:"
"we propose a novel, quality-aware and time-optimized decision making model for achieving high quality edge analytics while being communication efficient. we introduce the fundamental quality metrics and provide two variants exploiting the sensing & computational capabilities of nodes to perform on-line decision making. the edge nodes are enhanced to intelligently decide when and which data to deliver for guaranteeing high quality of data reconstruction, aggregation and linear regression analytics. we provide mathematical analyses based on the principles of optimal stopping theory, while evaluating and comparing the models performance with other methodologies following the instantaneous decision making paradigm. our approach is deemed appropriate to edge analytics being flexible to cope with the trade-off quality & communication overhead. our future research agenda includes leveraging edge analytics by pushing predictive modeling & analytics to sensing/actuator devices expecting limited data transmission."
"when designing such cr equipments the main challenge is to find an appropriate way to correctly dimension its cognitive abilities according to its environment as well as to its purpose (i.e., providing a certain service to the user). several articles in the literature have already been concerned by this matter however their description of the problem usually remained fuzzy (e.g., [6, 14, [cit] ). we summarize their analysis by defining three \"constraints\" on which the design of a cr equipment depends: first, the constraints imposed by the surrounding environment, then the constraints related to the user's expectations and finally, the constraints inherent to the equipment. we argue that these constraints help dimensioning the cr decision making engine. consequently, an a priori formulation of these elements helps the designer to implement the right tools in order to obtain a flexible and adequate cr."
"in some contexts, one can consider that there is a priori knowledge available on the complex relationships existing between, the metrics observed, the parameters to adapt and the criteria to satisfy as described in figure 7 . in this case the problem appears to be a multi-criteria optimization problem. within this framework, the cr decision making engine aims at finding the best parameters to meet the users expectations by solving a set of equations as shown in table two of article [cit] from which is extracted figure 7) . this problem is known to be complex for several reasons:"
"we use f d (x) to derive f a (x). in figure 13, the intersection of the sensing area of an arbitrary sensor with p is denoted by a. by forming a triangle whose vertices are the center of p, sensor location, and one of the points where the sensing circle of the sensor meets p, one can write"
"to fulfill the requirements to enable smart and autonomous equipment, mitola and maguire introduced the notion of cognitive cycle as described in figure 1, [cit], where the cognitive cycle presupposes the capacity to collect information from the surrounding environment (perception), to digest it (i.e., learning, decision making, and predicting tools) and to act in the best possible way by considering several constraints and the available information. the reconfiguration of radio equipment is not discussed in depth, however, it is generally accepted that sdr in an enabling to technology support cr [cit] ."
"in figure 4, we represent two sub-spaces referred to as actual design space and virtual design space. on the one hand, the virtual design space refers to the upper bound support of the design space where all three dimensions are considered independently from each others. its volume can be interpreted as the largest space of decision problems one could define from the three dimensions. on the other hand, the actual design space is included in the virtual design space. it results from the reduction of the design space when taking into account the correlation between the different constraints imposed by every dimension of the design space. for instance, some constraints on the environment such as, \"imposed fixed waveform\" might limit some objectives such as \"find a waveform that maximizes the spectral efficiency\"."
"more recently, article [cit] however assumes that no a priori knowledge is provided and that the performance of the equipment can only be estimated when trying a specific configuration. the associated tools are based on the so-called mab framework. one advantage here is to provide learning solutions while operating, even if the cognitive engine is facing a completely new environment. of course, performance increase while the learning process progresses. note that this approach is also proving its accuracy in the osa context [cit] ."
"using the same approach taken for finding the upper bound on n in (6), one can derive an upper bound on the number of nodes to guarantee having all gaps smaller than t."
"to conclude this section, we would like to emphasize the fact that the proposed classification in this article shows that a cr equipment cannot depend on only one core decision making tool but on a pool of techniques. every time it faces an environment, the equipment needs to have an estimation of its a priori knowledge and on its reliability. to tackle a particular context, the general process can be summarized through three questions: what can't i do (design space)? what do i already know (a priori knowledge)? and what technique should i select to solve the decision making problem?"
"once again these notions know several possible definitions that we do not explicit in this article. however, the basic cognitive cycle considers three macro-steps as illustrated in figure 3 and that we can define as follows:"
if we assume that the previously mentioned off-line expert rule extraction phase has not been (or partially) accomplished an exploration of the space of possible configurations is needed.
"to mitigate this paradox, time limited reasoning has been suggested by mitola. as a matter of fact, radio systems need to observe, decide, and act within a limited amount of time: the timer and related computationally indivisible control construct is equivalent to the computer-theoretic construct of a step-counting function over \"finite minimalization.\" it has been proved that computations that are limited with reliable watchdog timers can avoid the gödel-turing paradox to the reliability of the timer. this proof is a fundamental theorem for practical self-modifying systems [cit] ."
"where f c (x) is the cdf of c. one can also calculate the expected value of c, μ c . to this aim, we first consider the uncovered part of the path, v, and find its expected value, called μ v . then μ c can be found using the fact"
"the correlation maps using counts from the new energy calibration at t3, t3+lsecond, t3+2second, and t3+3second are shown in fig. 27 . the conventional correlation method was not reliable at the low count rates in this study. as one of investigations to improve the correlation maps, we looked into another binary mask."
"coded aperture imaging is based on the shadows made by the mask and incoming x-rays or gamma-rays. however if the count rates are low, comparable to the nwnber of openings in the binary mask, it is statistically difficult to estimate the origins of the radio-active sources [cit] ."
a imci (0.037x i0 9 bq) cs-137 source was placed inside a building as shown in fig. 3 . the place markers show the truck locations at the intervals of 10 seconds in the course of the 50-second period. the truck speed was approximately 5 miles/hour.
"this paper is structured as follows. we begin by presenting the most related work in sect. 2 we introduce the distant n-gram topic model (dntm) in sect. 3, defining the graphical model, the generative procedure, and the inference and parameter estimation details. we then evaulate the dntm on a synthetic dataset in sect. 4 followed by two real mobile phone datasets in sect. 5 . we conclude with a discussion followed by the conclusion and future works."
"topic models have previously been used for n-gram discovery in the context of text and speech. the bigram topic model [cit], the lda collocation model [cit], and the topical n-gram model [cit] are all extensions of lda to tackle this problem. the topical n-gram model is an extension to the lda collocation model and is more general than the bigram model. this approach was developed to be applied to text modeling, and retains counts of bigram occurrences, and thus could not easily be extended for large n (i.e. n [ 3) due to parameter dimension explosion. the multi-level topic model is another extension of lda for n-gram discovery [cit], cascading a series of lda blocks for varying length sequence discovery. the problem of activity discovery from mobile phone data requires n-gram models capable of handling long sequences; we approach this issue by modeling a simplified dependency between labels (or words) within a sequence and adding a dependency to topics; we find that this technique is promising for location sequence discovery."
"localization of target sources requires coordinates of registered counts to estimate the direction and distance of the detected radioactive source. the registered counts include background radiation. in addition, the measured counts only from the target source suffer the counting statistics. therefore repeated measurements do not always observe the same measured counts."
"though only selected results are presented for the discussion here, many extracted topics correspond to human routines. there are topics corresponding to noise, though they do not dominate the extracted routines."
"several of the topics discovered by the dntm for the smartphone data displayed in fig. 8 are shown in figs. 10 and 11. the first parameter the model returns is h, containing a probability distribution of each day in the corpus for each topic. we rank these probabilities for each topic and visualize the 10 most probable days, illustrating which days in the data had the highest probability of the location sequences for the given topic. in fig. 10, the three figures illustrate the 10 most probable days (i.e., max(h m k ) for a given topic k). the x-axis corresponds to the time of day, the y-axis corresponds to days, and each unique color corresponds to a unique place. we can see that sequences of places occurring over particular intervals of the day are discovered by the model. for example, topic 8 for user 1 corresponds to place 1 (home in magenta) occurring over most of the day."
"since the misti truck was moving in 3-dimensional motion between the marker to and t5, the measured counts map affected by the motion is different than that of a fixed truck. the counts that were supposed to be registered in a single pixel could be displaced to neighbor pixels due to the truck motion. in addition, since the detector pixel size is coarse (locm-by-l0cm), it becomes ambiguous to compensate the interaction positions due to the truck motion. in spite of some positioning error due to the truck motion, we accumulated the counts for i-second periods. the i-second counts map at the marker t3 is shown in fig. 14. the infonnation on the possible target source can be decoded using the simple corss-correlation between the 1-second counts map and the binary mask. the alignment of the 10-by-1o array of the nai(ti) pixels and the 12-by-18 array of the binary mask is shown in fig. 15 . the bottoms of the mask and the nai(ti) array were aligned. we can see mask patterns above the nai(ti) detector array and beside either side of the nai detector array. 16 shows the correlation map between the counts map (see fig. 14) and the binary mask (see fig 15) . the correlation value at the cross-point corresponds to the cross-correlation at the alignment shown in fig 15. the nai(ti) counts map is moved around within the mask pattern and the cross correlation correlation is calculated at each alignment. the maximum correlation value in fig. 16 shows the best match between the counts map and the binary mask. the displacement of the maximum value from the cross-point is used to calculate the directional vector in conjunction with the detector-mask geometry."
"a. detection at low count rates measured counts for short periods suffer more statistical counting error. therefore the accumulation period should be adjusted to maximize the detectability of target sources. the speed of the misti truck can be adjusted to collect enough counts. since the detection of source presence is limited by the knowledge of background radiation, the fluctuations of background in the course of time make the detection more challenging [this section is based on unpublished discussions in our group]."
d etection and localization of radioactive target sources have been of great interest. monitoring of radioactive sources using mobile detector systems suffers increased uncertainty due to background radiation; even non-moving monitoring systems are vulnerable to background radiation. therefore understanding of background radiation is one of the keys to the detection of radioactive sources in mobile detector systems.
"each measured count is registered with a time stamp, the deposit energy, and the nai(ti) pixel number. we can use the iterative list-mode data to localize the target source [cit] . since the image space is relative large, the directional vector can reduce the region of interest and the computation time."
"as large-scale mobile phone datasets on human behavior become more readily available, the need for effective methods and mathematical models for analysis becomes crucial. research in reality mining [cit] has led to the need for the development of models that discover patterns over long and potentially varying durations. we address the problem of modeling long duration activity sequences for large-scale human routine discovery from cellphone sensor data. our objective is to handle sequences corresponding to human routines based on principled procedures and to apply them to human location data."
"to correct the location of k-40 photo-peak, we assumed the nai(ti) and pmt gains are linear between 511kev and 1461kev. the corrected k-40 photo-peak location is shown in fig. 7 ."
"in this paper, we proposed the distant n-gram topic model as an alternative to model long sequences for activity modeling and apply it in the context of human location sequences. considering two real life human datasets collected via mobile phone location logs, we tested our model firstly on locations obtained by smartphones based on gps and wi-fi and secondly by cell tower location features. the patterns extracted by our model are meaningful and are further validated by considering a synthetic dataset. we evaluated our model against lda considering log-likelihood performance on unseen data and found that the dntm outperforms lda for most of the studied cases."
"the events shown in fig. 24 were divided into non overlapping subsets which contained 400 events individually. each subset included both compton and photo-peak events. the fust subset contains 24 photo-peak events and the rest are compton events. for each subset, the source localization method was applied to locate the target source. fig. 25 shows a slice of back-projection image space parallel to the binary mask. the distance from the binary mask to the slice is 6m. the scale is i meter/bin. we can see the high-value pixels around the center. since we used both almost ideal motion compensation and no background radiation, in general the localized source is clearly observed. we can enhance the localized source distribution by using the iterative localization method given in eq. 2. since this iterative method requires high computation load, we can use the multiple graphics cards to speed up the computation."
"probabilistic topic models were initially developed to analyze large collections of text documents [cit] . they have been used more recently for other sources of data such as location [cit] and physical proximity [cit] . here, we consider their application to large-scale mobile phone data."
"we focus on probabilistic topic models as the basic tool for routine analysis for several reasons. topic models are, first and foremost, unsupervised in nature. their probabilistic generative nature makes them attractive over discriminative approaches since we are interested in mining the structure of the data. topic models are also intuitive and provide opportunity for extensions with approximate methods for inference. they can handle uncertainty due to the exchangeability of the bag of words property and process large amounts of data [cit] . they can also be extended in various ways to integrate multiple data types [cit] ."
"the graphical model for our distant n-gram topic model is illustrated in fig. 1 . we use a probabilistic approach where observations are represented by random variables, highlighted in gray. the latent variable z corresponds to a topic of activity sequences. the model parameters are defined in table 1 ."
"we used the conventional cross-correlation between the binary mask and the measured counts map to estimate the target source direction. since the measured counts are affected by counting statistics, we plan to optimize the binary mask for the cross-correlation."
measured energy spectra and counts for short periods were obtained. the short time period was set to 1 second to analyze the data at low count rates. therefore the analyzed data were affected by background radiation and poisson counting statistics.
"there are several difficulties to modeling human activities, including various types of uncertainty, lack of ground truth, complexity due to the size of the data, and diversity of phone users. one fundamental issue motivating this work is that we often do not know (or cannot pre-specify) the basic units of time for the activities in question. we do know that human routines have multiple timescales (hourly, daily, etc.); however, the effective modeling of multiple unknown time durations is an open problem. secondly, the problem of mining location sequences quickly results in an exponential number of possibilities, particularly when considering the wide range of locations visited by people and the order in which the locations occur. the focus of our model is to address the issue of modeling long sequences (such as those occurring in mobility patterns) by proposing a novel approach based on latent topics in order to avoid parameter dimension explosion."
"we did not facilitate an available, sophisticated energy calibration due to the time limit. therefore the simple energy calibration we used in the real data study may need some improvement. therefore we re-computed the correlation maps using another energy calibration shown in fig. 7, which used both background 511kev and k-40 (l461kev) assuming that the linear gain between 51lkev and 1461kev."
we note that the truck was originally positioned higher than the building tops at the place marker of to. the target cs-137 source was completely out of fov (field-of-view) between the place marker to and tl (see fig. 4 ). therefore the measured counts between the place marker to and tl were from background radiation.
"like lda, the optimal estimation of model parameters is intractable. the model parameters are derived based on the mcmc approach of gibbs sampling [cit] . the model parameters can then be estimated by solving the following relationship (fig. 2) ."
"lucian mihailescu is a staff scientist in the applied nuclear science group at the lawrence berkeley national laboratory, berkeley, ca 94720 usa (e mail: lmihailescu@lbl.gov)."
"in order to compare our dntm to lda, we adapt the vocabulary used for lda to have a comparable format to that used in the dntm. the vocabulary we use for lda consists of a pair of locations, a timeslot, as well as the distance between the locations. this results in a competitive comparison since the key attributes of the dntm are taken into the vocabulary for lda. the log-likelihood results on 20 % unseen test data are plotted in fig. 19 . we plot the log-likelihood averaged over all the test documents. the log-likelihood results reveal that for small n, lda performs slightly better. however, as n increases, the dntm consistently has better generalization performance."
we used 4 consecutive sets of measured counts maps starting from the marker t3. each counts map is an accwnulation of i-second period. the time lapse between two consecutive counts maps is 1 second. the second counts map that was observed i-second after the marker t 1 was used for the correlation map shown in fig 17 . correlation maps from counts maps at t3+ 2 seconds and t3+3seconds are shown in fig. 18 and fig. 19 respectively. the correlation map in fig. 18 shows some uncertainty in choosing the maximum value. this uncertainty might be caused by counts from background radiation.
system was designed to detect and localize a lmci cs-137 source at 100 meters in 20 seconds. illustration of a binary mask and the nai(t1) array. these pictures were adapted from [i] .
"there are several future directions for this work. the first direction is to explore extensions of the proposed model. one could extend the dntm by taking into account the limitations mentioned and imposing application-specific constraints. one can also further investigate the dependence problem and consider methods to model dependence among labels as opposed to always having the label dependent on the first element, though this could quickly lead to parameter size explosion. for example, there may be effective hierarchical methods for determining the number of previous labels that a given label in a sequence should depend on. the second direction of extensions would be to consider other types of data, for example, in the context of other wearable data and activities. finally, one other relevant line of work future work is a comparison of our method with hidden markov models learned in an unsupervised setting, imposing structure to learn long-term sequential patterns."
"one evaluation criteria in determining the quality of a model is its predictive power. in sect. 6.3, we considered the average loglikelihood of the model on previously unseen data. this is a very general measure giving insight into the predictive capabilities of the model for data that was not previously seen by the model, and the results from fig. 19 are promising for the dntm."
"as shown in fig.2 (b), the bottoms of the binary mask and the nai(tl) detector array are aligned so that the field of view (foy) in the vertical direction is limited to above the horizontal line. therefore if the incoming gamma rays come from below the horizontal line, part of the shadow by the binary mask may not be recorded in the lo-by-lo array of nai(ti) detectors."
the truck is equipped with gps and an inertia gauge. the truck locations and orientations are estimated using the readings from gps and the inertia gauge. the readings were not used for this study due to the uncertainty in the readings.
"we present the dntm results on two real mobile phone data collections. first on the nokia smartphone data considering a scenario with a time coordinate of the day t in the label definition w. then, we consider the modified scenario without a time coordinate in the vocabulary. finally, results are presented on the mit rm dataset."
the (l+r)/r term is roughly related to the simplified solid angle subtended by non-zero pixels in the nai(tl) detector through the 3-dimensional binary mask to an observing point.
"our overall goal of mining location sequences and the latent topic modeling approach in this paper differ from these previous works, which also considered location sensor data for activity modeling."
"mobile detector systems, for example, the misti truck, can track a moving target source. however, one of drawbacks is that the measured counts continuously change due mainly to the varying distance from the target source. the effects of variable measured counts are briefly addressed in terms of detection and location."
"fig . 5 shows that a bus is coming down the hill. the passenger side of the bus is looking down at the source location. we note that since the fov of the coded aperture imaging system in the vertical direction is above the horizontal line, the target cs-137 source is roughly out of fov in fig. 5 ."
"mobility of user 2 [cit] .02.07, fig. 11 topics and location details for user 2. a the satellite view of place 1 is displayed, which corresponds to work for user 2. b the mobility for day 02.07.2010, is displayed. the colors of the places displayed on the map correspond to those displayed in the topic. note that 02.07.2010, is one of the 10 most probable days for user 2 discovered in topic 2 and involved transitions between 3 [cit] 18:223-238 231 coordinates of the place as displayed below the topic. the circle indicates the location of place 1 on a satellite map view. in fig. 11b, we show topic 2 for user 2. below the topic, we display the mobility traces for the day 07 02, 2010, which is one of the 10 most probable days for topic 2. on the satellite view, each color corresponds to a unique location, coordinated with the color scheme of the topic displayed."
we showed some possibility of localizing the target source at low count rates using the mobile detection and imaging system. we plan to optimize the binary mask to improve the cross-correlation outcomes. we may additionally need to incorporate nonlinear pattern search algorithm to take advantage of the embedded information in the measure counts maps.
"once the directional vector to the target source is obtained, we can use a list-mode localization method. we used an iterative equation given below to localize the target source."
"previously, we used existing topic models (probabilistic latent semantic analysis, lda, and the author topic model) [cit] for human activity discovery and prediction using cell tower and a small collection of gps data. this paper extends on this initial work by defining in detail a new model to address the limitation of long duration activity discovery with topic models."
"we temporarily used roughly estimated motion compensation parameters. the accumulated weighted back projections were missing the target source location. however, we note that the source direction information with some error is still valuable to track the target source since the spectroscopic analysis does not provide the source direction information."
"we used the energy calibration shown in fig. 6 (background 511kev only) for this study. since the cs-137 photo-peak (662ke v) is not far from the 511 ke v photo-peak, the calibration error may be small between 511kev and 662kev. we also assumed that the nai(ti) and pmt gains are linear between 511 ke v and 662ke v. i-second energy spectra at the marker to, t3, and t4 are shown in fig. 8, fig. 9, and the count rate within the 6% energy window at the cs-137 peak was 75 counts/second in fig. 9 . we observed 76 counts per-second in fig. 10 . we note that these measured counts within the energy window are less than the number of total detector pixels in the nai(tl) detector. the nai(ti) detector has a lo-by-1o array of nai(ti)-pmt modules."
"since we want to analyze short-period energy spectra and counts in real time, high speed computation is required. we are currently using multiple graphics cards. fig. 22 shows two graphics cards in an external box. each graphics card (nvidia® gtx580) has 512 gpu cores."
"note in b, the sequence is not visible due to a large range between sequence labels (and low probability of the sequence occurring over the entire dataset). however, the most probable days given topics show the location routine learned (color figure online)"
the directional vectors for the target sources were estimated by incorporating the displacement of the maximum correlation value. however the directional vectors have some uncertainty due to background radiation. therefore we do not have high confidence in the accuracy of these directional vectors that are obtained from the cross-correlation. this uncertainty in the directional vectors may be reduced when we have enough counts at a fixed truck position.
"kai vetter is an associate professor of the department of nuclear engineering at the university of california, berkeley and a staff scientist in the applied nuclear science group at the lawrence berkeley national laboratory, berkeley, ca 94720 usa (e-mail: kvetter@lbl.gov)."
"we plot several of the most probable topics for users by displaying the most probable days given topics, h (fig. 13 ) and the most probable sequence given topics, u (fig. 14) . in fig. 13, the five most probable days are plot for each topic, where the y-axis corresponds to days, the x-axis to the time of day, and the colorbar to the locations. in fig. 14, the single most probable sequence is plot for given topics, where the y-axis corresponds to the locations, and the colorbar represents the probability of the location (or label) given the sequence position (x-axis), the first location label and the topic. the probability of each sequence component (indicated by the colorbar) differs and is an indication of the amount of ''noise'' or variation in this label occurring at this position given the topic. a wide range of mobility routines are apparent, particularly by viewing the most probable days given topics (fig. 13 ) for all users, where co-occuring sequential patterns of stay regions are found by the model."
"as one of efforts for mobile detector systems, the naval research laboratory launched a mobile imaging and spectroscopic threat identification (mist!) project [cit] . the mist! system is a hybrid detector system mounted on a truck. the hybrid detector system consists of hpge detectors and a nai(ti)-based coded aperture imaging system. the hpge detctors are used to detect the presence of radioactive sources and the nai(ti)-based coded aperture imaging system is used to localize detected radioactive sources. the mist! sam. s. huh is a physicist postdoctoral fellow in the applied nuclear science group at the lawrence berkeley national laboratory, berkeley, ca 94720 usa (e-mail: sshuh@lbl.gov)."
"instead, we use python scripts to check each step of the iterative method in the simulation study. fig. 26 shows the localized source distribution from the iterative method. the scale is half a meter per bin. we can see the source distribution near the center was roughly improved."
"there is one important change in the five input features we decided on in the previous section. [cit] data set (see table 1 ). instead, we have the cloudiness index in the weather forecast, so we substitute the index for the cloud cover. then, the weather forecast has two items that contain the cloudiness information: the cloudiness index and the weather index. the latter is actually a combination of the cloudiness index and the precipitation index, and has seven values (1: clear, 2: partly cloudy, 3: mostly cloudy, 4: overcast, 5: rain, 6: rain/snow, 7: snow) as we discussed in section 3.1.3. in this paper, we decided to use the combined weather index to replace the cloud cover as it contains more information. [cit] that could generate up to 2.448 [cit] system had 1.224 kw as the maximum power that solar panels could generate. [cit] ranged from 0 to 1.008 kw, and we use it as ground truth for training."
"the main lesson obtained through this study is that solving the weather data granularity and quality problem in pv output power forecasting does not have a single solution, namely fine-tuning it with local measurements using on-site sensors. we demonstrate that deep neural networks (dnns) can achieve comparable or even slightly better forecast quality than our conventional two-stage system that relies on the on-site sensors, by training it with our pv power output history and the corresponding regional weather forecast data from the national weather service for one year. in essence, this finding tells us that most part of the pv power forecat system can be converted to software, by obviating the need of hardware (sensor) modules and their management. it will contribute to the cost, complexity, and reliability aspects of the energy management system in grid-connected buildings. we believe that the dnn-based forecast model can simplify the grid-connected building energy management systems (bems), making it more attractive in future."
"keep the month and the hour because the season and the time-of-day directly affect the solar radiation. however, exclude the date that is less likely to have correlation with the solar radiation or other weather conditions."
"in the pv power output forecast literature, many previous works employ the long-short term memory (lstm), elman network, or the recursive neural network (rnn) in general. they are a popular dnn type to learn the time dependent nature of solar radiation [26, 31, 34, 35, [cit] . however, considering the targeted hourly forecast under the much coarser time grain of 3 h in the weather forecast data, we decided not to rely on the time dependency between adjacent data entries. a positive side of sacrificing the time dependency is that the model will not be affected as much when the weather condition wildly varies between two forecasted hours. without the time dependency between data entries, perhaps except through the time indices (month, date, and hour), we can choose a feedforward network for the dnn type. in this paper, we employ the multi-layer perceptron (mlp). note that our focus is not in proposing a dnn model having a better precision than others, or improving its learning speed. rather, we aim to show that large dnns can replace the traditional sensor-based approach by coping with coarse-grained weather forecast, whatever dnn type is selected."
"mlp is a neural network model that can approximate any nonlinear function (figure 10 ). the mlp consists of an input layer that receives input parameters, an output layer that computes the modeled function value for the input. the hidden layers are where the learning takes place. our mlp consists of l input parameters and m hidden layers each with n neurons that are fully connected between the adjacent layers. the input layer also has n neurons, and the output layer, only one. for the activation functions in the neural network, we employ the hypertangent (tanh) and the rectified linear unit (relu) that are shown in figure 11 . the former is used for all neurons in the hidden layers, and the latter, for the output layer. for the gradient descent optimization algorithm, we employ the adaptive moment estimation (adam) [cit] that computes adaptive learning rates for each input parameter. for learning the weights for the hidden neurons, the back propagation is done based on the loss function value. in this paper, we use the mean absolute error (mae) for the loss function, defined as follows:"
"although we showed that the sensorless forecast using a large dnn has a better prediction quality than the two-stage model relying on on-site sensors, the prediction performances during summer and under cloudy weather conditions are not satisfactory. from the perspective of the building energy management system, it badly needs the assistance of the forecast system when the weather is most unpredictable. to improve the forecast quality in these adverse conditions, we can consider several future directions of exploration. first, we can use more weather forecast data and the corresponding pv output data from the past years (e.g., [cit] as well) to expose our model to more weather patterns. second, we can additionally consider the time dependency of the weather data. note that we completely ignored the time dependency in the current study. we believe that the time-dependent changes around the entry points that lead to the largest errors may be useful to reduce the errors."
"in the current work, we did not explore a few dimensions of the dnn architecture. in particular, we fixed the number of neurons n in the hidden layer. however, some recent works [cit] spearheaded the investigation on the hidden layer dimensioning. when we come to use more training data as discussed above, the dnn dimensioning parameters that we used in this paper may become irrelevant. in our future work, we will also explore the number of neurons in each hidden layer. since it can be a time-consuming process, we will have to use more systematic techniques, in particular, the early stopping criterion [cit] ."
"the kma announces long/medium/short/very short-term weather forecasts. [cit] data set is the accumulation of the short-term forecasts. the bems fetches this data from the kma every three hours. the sensor data are from the direct digital controller (ddc) connected to the sensors, which bems requests every five minutes. for our study, we averaged them into hourly data entries."
"we consider this problem in the context of the grid-connected building that utilizes the solar power as a supplement to the traditional power provisioning, as shown in figure 1 . with the historical pv output power data from our rooftop pv power generation facility and the weather forecast data for the building location, we train a 6-layer feedforward dnn for the day-ahead forecast. we demonstrate that it achieves the average mean absolute error (mae) of 2.9%, comparable to that of the conventional model we have used involving the on-site sensors. we believe that the alternative forecast model can simplify the grid-connected building energy management systems (bems), making it more cost-effective."
"recently, the solar power generation method has been shifting its focus from the concentrating solar power (csp) system to the grid-connected photovoltaic (pv) power generation [cit] . the key aspect of managing the micro-grid environment, such as a grid-connected building, is balancing between the amounts of power generation and the demand. because the solar power generation is strongly affected by weather conditions, forecasting the generated solar power in the face of the weather changes is a critical component for the management [cit] . a difficulty in the forecast is the granularity and quality of the weather forecast used as input to the pv power output forecasting. as the weather forecast is traditionally considered to have coarse granularity, many are compelled to use on-site meteorological sensors to complement it. for such conventional systems, on-site sensors such as irradiance, temperature, and humidity sensors are usually installed together with the solar panels ( figure 1) . typically, these systems use a two-stage approach [cit] . first, they model the relation between the regional weather forecast and the precise on-site measurement at the forecasted time. then, using the precise historical on-site measurement values inferred from the relation and the weather forecast input, they use a pv model (typically implemented in commercial software) to forecast the pv power output. however, this two-stage approach involving on-site sensors has several issues. first, it incurs the cost in the installation, operation, and management of on-site sensors. moreover, it incurs the engineering cost to reflect the sensor readings and the solar panel characteristics to the prediction. second, the physical model of the sensor dynamics itself can be a source of forecast errors. third, it requires an accumulation of sensory data that represent all seasonal variations, which takes time to collect. therefore, in this paper, we aim to show the feasibility of an alternative approach that do not depend on the supplementary on-site sensor hardware modules in the pv output power forecast. specifically, the alternative approach employs a relatively large deep neural network (dnn) to cope with the coarse-grained weather forecast. indeed, by training the dnn with a year's worth of the public weather forecast data and the contemporary power generation history from our testbed, we demonstrate that the dnn forecast model produces a higher level of prediction performance in almost all measures."
"in this section, we discuss the historical data that we use to train our dnn model. then, we compare our model with the conventional system that relies on on-site sensors. we also discuss how we determine the input parameters and the hyperparameters for our dnn model."
"an important aspect of the conventional approach is that the system should be accumulating the various sensor data at the site for the learning of f ml . it means that we need to install the sensors in each facility site. it incurs the installation and operation costs [cit] . in our case, for instance, the capital expenditure for the sensors and wiring, installation labor, and engineering total at $3000. in addition, the operation and maintenance costs keep being added to the overall cost. furthermore, it can take a long time to accumulate a sufficient amount of data until we can use it to train the forecast model, another costly aspect of the conventional approach."
"recollect that we normalized the data items by their minimum and maximum values before we train the models (see figure 9 ). specifically, all values of y i, y i,ȳ i and¯ y i have values between 0 and 1."
"below, we compare the prediction performance of the dnn model with that of the conventional approach that we have been using for our current system shown in figure 1 in detail. however, first, we visually summarize the prediction performance in figure 14 . comparing with the ground truth shown in figure 14c, we observe that the dnn model (figure 14b ) is visually more similar than the conventional method (figure 14a ). figure 15 magnifies a few days from the above forecasts by the two schemes, and puts them against the ground truth. for readability, we plot their predictions for approximately four days in each season. we list the weather and its changes that are forecasted by the kma in table 6 . when the weather is bad, the prediction becomes more difficult for both models. in particular, the summer days (e and f) pose the greatest challenge due to the monsoon climate. the current system tends to more severely underestimate the pv power output under bad weather conditions (6 and 7 february, and 3, 4, 6 august ). however, the dnn model more closely follows the peaks than the conventional model, except on 5 august when it overestimates slightly more than the current system."
"now, we explore the input parameter space, and we try to select the most relevant ones from the original 13. incorporating too many input parameters in a dnn can bring about several issues. first, it increases the training time, particularly for the mlp network with full connectivity among neurons. second, it increases the number of local optima in the error function, resulting in higher risks of suboptimal convergence [cit] . finally, by adding more dimensions, they require a bigger data set to populate the parameter space densely enough to represent an accurate mapping relationship [cit] . with the minimally redundant data, however, the computation complexity becomes more sustainable, and leads to higher forecast accuracy. this is why prior studies strive to pick out the most influential meteorological parameters [cit] ."
"as a result of the scaled-down approach to building a beam-scanning based imaging system, the imaging lens focal length was limited. as a result, this introduces the problem of field curvature and limited depth of field. to address both of these issues, a plano-convex lens was fixed at the focal plane such that the convex surface closely matches the focal plane curvature to ensure that the tissue surface was always in focus. since this lens functions more as a window, we will call this lens, the 'curvature matching window'. in order to select a plano-convex lens of proper curvature, tracepro imaging simulation software (lambda research, ma) was used to optimize the curvature for even distribution of focal spot sizes at all points of scan."
"drsi requires a consistent sampling geometry across the full field in order to accurately capture optical properties across the field of view. this had to be verified in order to be assured that skin image contrast was due only to scattering and absorption heterogeneities. this verification was simply made by measuring the size and center to center separation between the source and collection apertures. to accomplish this, the on-board camera was used to snap images of the sampling geometry at nine points across the field of view. the images were analyzed to see if the size and distance metrics of sds remained constant or not."
"in this paper, we describe the drsi system that provides maps of intrinsic biological scattering and absorption properties in-vivo without any extrinsic contrast agents. we validate this system for biologically relevant combinations of reduced scattering and absorber concentrations. we characterize the spatial/spectral resolution and signal-to-noise ratio (snr). we conclude with a clinical demonstration of the system towards the spatio-chemical characterization of skin cancers."
"in another approach to ccd skin imaging, a modality called spatial frequency domain imaging (sfdi) uses structured rather than spectral illumination techniques to derive quantitative maps of optical and physiological properties of turbid media [cit] . sfdi uses periodic illumination patterns at different excitation wavelengths on turbid materials and calculates optical properties via evaluation of the modulation transfer function. the advantages of this system are numerous: high spatial resolution; the ability to control optical sampling depth; and structured illumination is used to decouple scattering from absorption."
"image processing in this drsi system consists of background subtraction followed by spatial and spectral normalization. the image background consists of back reflections from optics in the system and any constant stray light that travels into the collection arm. the background was acquired as an image of a highly absorbing, non-scattering material in contact with the curvature matching window and was subtracted from all subsequent images. light delivery and signal collection efficiency in drsi is a function of the imaging lens' numerical aperture (na) and, to a lesser degree, of the scan mirror size and position. the scan mirror size affects the na as it dictates the diameter of the collection f-stop. spatial intensity heterogeneities due to scan angle effects are accounted for by imaging a homogeneous sample used to normalize these effects. for this purpose, a pdms sample infused with titanium dioxide (tio 2 ) for scattering (1mg/ml) was used as a reflectance standard to spatially and spectrally normalize subsequent tissue images. equation (1) describes the culmination of these normalization operations that result in reflectance images."
"here we have presented a spatially resolved diffuse reflectance spectroscopy instrument towards the detection of skin cancers in vivo. with this instrument, hyperspectral images of turbid media are acquired and the spectra of each pixel was fit for optical and physiological properties. validation of this system for the fitting of optical properties was accomplished through the use of liquid phantoms that span a range of biologically relevant optical and physiological properties. image quality was achieved through background subtraction, spatial and spectral normalization and cross polarization."
"skin imaging in the clinic was performed in a similar fashion as demonstrated above for the benchtop testing; however, we found it beneficial to apply an index matching gel to the skin surface to avoid some specular reflections prior to acquisition. as stated previously, acquisition time was approximately 50s for the 50x50 pixel image size used, offering a good balance between imaging resolution and acquisition time. figure 8 shows an example image of an ulcerated basal cell carcinoma (bcc) with the fov delineated by a white box. each pixel of the image was fit for concentrations of main chromophores: hemoglobin and melanin where the concentrations are in mg/ml. these optical property maps reveal known details about the biochemical nature of skin cancers, but until now have not been revealed through diffuse reflectance modalities. among these details are the reduction in μ s ' and the increase of hemoglobin concentration, particularly along the lesion margin, which is not always easy for a surgeon to locate. the blue lines surrounding the lesion are pen outlines for demarcation purposes. this pen ink absorption was accounted for in the spectral fit in order to minimize its intrusion into the μ s ' and hemoglobin images. previous to drsi, details like these were only discoverable through intricate, invasive histology procedures such as mohs surgery."
"there are multiple ways to glean optical and physiological properties from a diffuse reflectance spectrum. we employed the look-up table (lut) method [cit] . the lut is essentially a database of reflectance spectra for all physiologically relevant absorption (μ a ) and scattering (μ' s ) coefficients; the lut was numerically fitted to measured reflectance spectra from skin in order to extract μ a and μ s '. we have previously applied the lut method for a variety of applications [cit] . we used the lut method to obtain optical property maps of reduced scattering (μ s ' at 630nm), blood volume fraction and melanin content. at the image fitting stage, the lut was numerically fit to the diffuse reflectance spectrum of each pixel, yielding optical property values for that particular pixel. the fitting was performed using custom algorithms written in matlab that employ non-linear fitting techniques to minimize the error between the lut and the measured data. with the data combined from all the pixels, a two-dimensional map was obtained for each optical property. the fitting step was performed offline, as reflectance spectra typically takes about one second per spectra to fit; although several methods exist to speed this process up several orders of magnitude [cit] ."
"for images of homogeneous phantoms, each reflectance image pixel should ideally be the same in order for optical property maps to be accurate. if this criteria was met (within the statistical variation of the signal due only to shot and dark current noise), then any contrast within the image was attributable only to variations within the tissue. however, in any beam scanning system, there will be artifacts and aberrations; in the case of drsi, the main artifact was field curvature. to mitigate field curvature effects, existing solutions include f-theta scan lenses that effectively reduce the effect of field curvature through a series of meniscus lenses. a cost effective solution used in drsi was to match the field curvature to the surface of the skin to be imaged. this was achieved through using a field curvature matching window (plano-convex lens), which was in contact with the tissue. the convex shape of the matching window provides additional functional versatility; concave skin topography (neck, back) can be imaged. this was visually illustrated in fig. 4, where the results for tracepro simulations for planar and plano-convex matching windows are shown. in this simulation, the difference between the two types of windows was quite pronounced -for the planar case, the beam profiles are very different and highly dependent upon spot location; however, for the planoconvex lens, the beam profile was quite invariant with spot location. therefore, the planoconvex lens was a very useful, cost-effective solution for imaging skin lesions, which are distributed all over the body. to empirically validate the curvature compensation shown in the optical simulation, visual inspection of the sampling geometry at various points of scan was performed. by covering the curvature matching window with light adhesive putty, the embedded camera of the drsi system was used to snap pictures of the source and collection apertures. light was then sent in through the source and collection fiber ports with the shutter open so that the apertures were visible. using nine different scan positions, the images of the apertures were used to measure the projection of intensity in the vertical and horizontal directions. after normalization of the projection profiles, it was clear that the size and location of the source and collector apertures are virtually unaffected by scan position, fig. 5 . the results of these experiments show that the field curvature matching window reduces the effect of field curvature by enabling the sample to be located along a curved surface that closely matches that of the focal plane. the mean diameter spot size was 0.44mm and 0.39mm for the source and detector, respectively. mean sd separation was 0.74mm with a standard deviation of 0.05mm. without a curvature matching window, or a planar window, the sampling geometry would not be the same across the image and there would be noticeable and radially symmetric field inhomogeneity."
"previous clinical studies with point-probe drs have shown skin cancer lesions to have reduced hemoglobin content and increased scattering compared to normal tissue [cit] . as evidenced by the bcc drsi images of fig. 8, the result of the imaging approach is consistent with that of single point-sampling. by being able to visualize lesion margins, drsi could be a powerful tool for image guided skin cancer treatment/removal with minimal involvement with surrounding normal skin."
"where 1/f s was the sampling period and 1/f c was the minimum feature size, or best resolution contained in the image. therefore, for the drsi system as configured, the nyquist condition was met if the sampling period was at least 100µm, or half the full width half maximum."
"over the past 30 years, there has been a higher prevalence of skin cancer than all other cancers combined [cit] . skin cancer screenings, although important, are a highly qualitative and subjective process, suffering from false positives that result in unnecessary invasive biopsies and doctor visits. in fiscal terms, these false positive readings result in an estimated $2b cost to the us health care system and undue stress, pain and disfiguration for the patient. the major factor behind diagnostic accuracy of skin cancer screenings is the clinician's level of experience. studies have shown that diagnostic accuracy is better for experienced dermatologists than general practitioners [cit] . with one in five americans developing skin cancer in their lifetime [cit], there is a growing need to equip healthcare professionals with a tool that will provide quantitative as well as qualitative images that can improve on the subjectivity of skin cancer screenings."
the spectral reflectance of tio 2 is largely invariant with wavelength for the concentration and spectral range (450-700nm) used in drsi images. this point was confirmed by comparing the reflectance of a nist traceable spectrtalon to that of the tio2-pdms standard using a drs point probe (in contact with the tio2-pdms surface). the pdms standard provides a reference that accounts for any spatial and spectral heterogeneities across the field of view (fov). the tio 2 reflectance value was calculated in eq. (2).
"spatial resolution is typically measured with a united states air force (usaf) target of small lines and squares of various sizes. however, due to the nature of the drsi system and its curvature matching window, imaging a usaf target was difficult to perform accurately unless a properly curved target could be found. instead we imaged a single square of electrical tape on curvature-matched pdms so that the edge profile in the horizontal and vertical directions could be measured and the line spread function (lsf) calculated. the lsf was a parameter used to describe image sharpness or resolution in systems where it was less convenient to create a sample containing a point source from which to calculate the point spread function (psf). in order to calculate the lsf, the edge response, or step profile of a sharp edge in the image needed to be measured. then once the step response was acquired, its derivative was calculated, giving the lsf (fig. 7) . as with the psf, the lsf was characterized by the full width half maximum (fwhm) of its pseudo-gaussian shape, revealing the minimum distance that two features can be separated by and still be distinguished as separate features. as a consequence of this resolution measurement, from the nyquist theorem, the minimum image size (that will retain maximum resolution) was determined as half of this value. the image size used for these measurements was 100x100 pixels, which corresponds to a field of view of 10x10mm. therefore, the pixel to pixel distance was approximately 100μm. from the lsf measurement, the fwhm of the system resolution was calculated to be approximately 2 pixels or 200μm in both the vertical and horizontal directions. in order to ensure adequate spatial sampling of images with this resolution, the nyquist condition of eq. (3) must be satisfied, 11 2"
"to test for accurate optical property estimation, we imaged a set of phantoms representing a range of reduced scattering and absorption and used them to build an lut of reflectance. for this set of phantoms, polystyrene beads (polysciences) and red food dye (mccormick) were used to provide scattering and absorption, respectively. the average spectra of all pixels within each phantom image were averaged, and the resulting spectra were used to build the lut. after building the lut model, optical property estimation performance was evaluated after measuring a validation set of phantoms spanning biologically relevant ranges of μ a and μ s ', where hemoglobin was used as the chromophore. using the average reflectance spectrum of each phantom image, we fit the lut to each resulting spectra of known optical properties in order to validate our system. lastly, spatial resolution was empirically measured by imaging a turbid material containing fine, well defined contrasting features. using the edge of the contrasting features as a spatial step function, the step response to that feature edge was analyzed to calculate the line spread function."
"drsi is a widefield imaging technique with image sizes of over one square centimeter and sampling penetration depths of beyond 1mm, which allows for fast and noninvasive in vivo imaging of skin lesions. the diagram shown in fig. 2 illustrates the basic layout of the drsi system with the region in pink highlighting the components of the handheld device. this system uses a 75w xenon arc lamp (newport, irvine ca) coupled to a 100μm optical fiber for source power. at the handheld end, light was collimated with a ½\" diameter 19mm focal length lens (thorlabs, newton nj) and polarized with a polarizing beamsplitter (thorlabs, nj usa.) in order to reject specular reflections. the collimated and polarized source beam was tilted and steered with a pair of galvanometer scanning mirrors (cambridge tech., bedford ma) across a 1\" diameter 30mm focal length achromatic for focusing onto the sample. diffusely reflected light emerging from the tissue surface reverses the optical path through the system. after emerging from the tissue, diffuse reflectance was de-scanned by the galvanometers, and redirected by the beamsplitter down a collection arm where a ½\" diameter 19mm focal length lens focuses this light into a 200μm optical fiber for detection. it was important to note that the tip of the source and collection fibers are imaged onto the tissue, forming virtual sd apertures located at ρ microns away from each other. after the diffuse reflectance signal traverses the collection fiber, it reaches the spectrometer (ocean optics, dunedin fl) which was portable and customized for sensitivity in the uv-vis region of interest. also to facilitate visualization of skin lesions and co-registration to optical/physiological property maps, the drsi imager contains a camera, focus-adjusted to snap an image of the sample immediately prior to imaging. for the camera image collection, the field of view was illuminated by an led ring while a shutter between the beamsplitter and the galvo scanner block the xenon light source from interfering with the image. data acquisition and system automation was performed with labview (national instruments, austin tx) graphical programing software. the system tasks of camera snapshot, shutter and led control, galvo scanning, hyperspectral image processing and visualization was fully automated to and prevent opportunities for measurement error."
"the drsi system as described in this paper was optimized for acquisition time and snr. increasing the source-collector separation will increase the mean penetration depth of sampled photons; however, the trade-off is reduced sampling efficiency as the diffuse reflectance signal is inversely proportional to the square of the source-collector distance. to compromise, the source-collector separation could be brought down to zero (overlapping), with specular and single scattered light rejected by using cross-polarization methods. since overlapping sampling geometry retains the optical confinement of sampled photons, optical property derivation is still possible with this configuration. another benefit of using overlapping source-collector apertures is that shadow artifacts (derived from the sd apertues scanning across a sharp feature) can be avoided [cit] . what makes drsi a unique optical imaging system is that the contrast of each optical property image pixel is dependent on the spectral features derived from the volume it represents. to fit an lut model to each spectra of the image for light transport in turbid media, the levenberg marquardt method of error optimization was used and has been shown to be highly reliable. the snr also affects the quality of property maps and is a function of detector sensitivity, optical efficiency, light source intensity, and the optical properties of the skin under investigation."
"pre-cancerous and cancerous lesions are commonly found on areas of skin that are contoured and difficult to scan with a large, bulky imaging system. earlier prototypes of our design were box-like and bulky with the image plane co-incident with the face of the imager body. as a result, lesion images in areas such as the ventral neck, face and lower back were difficult to image and were commonly out of focus. to solve this problem, a customized imager (fig. 3) was designed from scratch using solidworks drafting software and constructed using a 3-d printer (envisiontec ultra, gladbeck germany). features of this customized imager are: a tapered neck, facilitating access to more contoured areas of skin; easier access to fiber and electrical connections as well as the collection aperture adjustment knobs; and space efficient housing of optical elements. the overall clinical presentation of the drsi system consists of a wheeled cabinet housing the light source, daq, spectrometer and galvo power supply. carried on top was the laptop computer running the drsi program and the imager itself."
"the drsi system operation is similar to that of a confocal microscope with a few exceptions. firstly, the pinhole used in confocal imaging (for removing out-of-focus light) is replaced by the tip of a collection fiber. however instead of collecting single-scattered light from the focal volume, dsri collects diffusely reflected light offset by a distance 'ρ', from the focal point. this is because while confocal imaging is optimized for spatial confinement of collected photon signal from contrast agents, drsi is optimized for snr, and collection of diffuse reflectance. secondly, the light source used in confocal imaging is a laser (typically uv for fluorescence requiring a long pass filter), which is powerful enough to provide enough single scattering (or fluorophore absorption) events in the focal volume for the required snr. in contrast, for drsi, a xenon arc lamp coupled into an optical fiber was used to deliver enough photon flux to the skin to overcome the 1/ ρ 2 attenuation in diffusely reflected light. broadband optical irradiation and detection in this manner allows for the quantification and decoupling of absorption and scattering based on photon transport or diffusion theory. lastly, dsri is relatively inexpensive compared to confocal microscopy (cm); cm requires higher magnifications and more precise optics, which increases the cost of the imaging system."
"the drsi probe has been used to image the optical property characteristics of skin cancer along lesion margins in vivo at the university medical center brackenridge (seton hospital). for each lesion, a camera image was taken for co-registration purposes prior to scanning. each hyperspectral drsi image was post processed to attain optical property maps of scattering, hemoglobin, and melanin concentration. the institutional review board at the university of texas at austin approved the study protocol, and informed consent was received from all patients that participated."
"there are a total of 12 variables and 20 causal relationships between variables in the bn. all of the variables have different probabilities in each state based on the causal relationships. taking the variable project performance for example, there is a probability of 1.46% that the project performance of the analysed firms is low, while the probability of maintaining a high performance level is 68.7%."
"the most significant factor of net profit is project performance, performance. an increase of project termination quality in portfolio management will greatly improve the net profit of firms."
bayesian networks (bns) have been widely applied in the area of both business intelligence and information integration q4 [cit] . nonlinear relationships between variables in uncertain environments can be simulated for prediction and diagnosis [cit] . bayesian learning algorithms can efficiently aggregate the output of members of networks [cit] and handle both nominal and numeric attributes well q5 [cit] .
"meanwhile, project managers' competency in pm cannot be ignored. they can move business strategy through practices and manage an effective strategic implementation effort [cit] . high competency of project managers in the initial stage may link a r&d project portfolio to firm strategy [cit] . a skilful project manager with a clear definition of roles and responsibilities can manage day-to-day activities in portfolio management and deliver high quality projects on time and within budget [cit] .therefore, npd project performance and net profit would be enhanced. in summary, portfolio management practices can be identified with respect to managers and processes. managers' support and involvement activities in portfolio management process will achieve portfolio success and then enhance npd performance. moreover, both the portfolio success, including strategic fit, average project success and synergy, and the npd performance, such as npd success rate, net profit, and sales growth rate are influenced by portfolio management activities."
"top management involvement refers to a group of top managers who participate in portfolio decision activities [cit] . if top managers recognize the importance of pm, they will adopt appropriate methods and implement regular reviews to ensure that a portfolio supports strategic objectives [cit] ."
"portfolio management (pm), the extended application of investment portfolio theory to new product development [cit], has been widely recognized as a crucial technique to improve npd performance, especially for project-based organizations. as a strategic tool, portfolio management is about making important decisions including npd project selection, resource allocation and balancing npd projects [cit] . therefore, firms need to address the issues about how to conduct pm to realize strategic objectives [cit] ."
"lastly, the causal map is delivered to experts for a final review. for those relationships where there are significant disagreements, specific explanations are presented to reach a final consensus. as a result, the dag of portfolio management for bayesian inference is constructed, as shown in figure f2 2."
"with respect to demographic characteristics, the final sample consists of different respondent profiles: top manager (10.1%), midlevel managers (42%), and project coordinators (47.9%). they are mainly from industries of manufacturing (37.3%), it service (26.6%), financial service (6.5%), r&d service (10.7%), construction (5.3%), real estate (2.4%), and others (18.9%). a total of 56% of them have invested more than 5% of their sales in research and development, and 58% have more than 10 projects managed concurrently. a total of 79.3% of the companies are large and medium sized enterprises, as shown in table t2 2."
"process di describes how well portfolio management activities are organized and scheduled. a formal and explicit process provides a platform for communication and decision making, which results in transparent and clear decisions and improves the quality of project evaluation and selection [cit] . in a wellimplemented process, all projects in a portfolio will be regularly reviewed."
"the competency of project managers has been recognized as an important criterion for project success q10 [cit] . it refers to the capability, knowledge, and responsibility of project managers in implementing portfolio management."
"first, we select npd success rate as a target node to investigate its key factors. the sensitivity analysis report shows that the most significant factor affecting npd success rate is strategic fit, which brings a variance reduction of 5.18%. the sensitivity degree of project performance and synergy to npd success rate are 3.58% and 1.1%, respectively. therefore, if a new product portfolio is aligned with business strategy at a high level, the npd success rate will be increased significantly. similarly, when the strategic fit is regarded as a target node, the contribution of portfolio management practices to the strategic fit is obtained from the sensitivity analysis ( figure f4 4)."
"because pm includes a set of multiple interdependent activities which constantly change and develop over time, modelling the relationships between pm practices and npd performance remains difficult [cit] q3 ) . [cit] . portfolio management practices and performance of 205 american companies were reported. the findings show that the best company achieved dramatically better portfolio success than the worst. and then, the consequences of portfolio management decisions on npd performance are examined in marketing simulation exercises conducted with mid-level managers [cit] . the research reveals that the three outcomes of portfolio management decision, including value maximization, balance, and strategic fit, have impact on npd and firm performance."
"we check the issues and de-aggregate variables of the loop into twotime frames to solve the problem. for example, there exists a reciprocal causal relationship between strategic fit and npd success rate, which may be caused by their dynamic relations across multiple time frames. in the first time frame t 1, the portfolio with high strategic fit tends to be allocated sufficient resources and to achieve a high npd success rate. then the high npd success rate will enhance the strategic fit of the portfolio in a future point of the second time frame t 2, as shown in figure f1 1. we retain one of the two relations and exclude the other from the causal map just as nadkarni and shenoy"
"in the optimistic scenario, if the levels of portfolio management practices all increase to high states, there is 60% chance of having a high npd success rate, 59.8% probability of having a high net profit, and 66.6% probability of having a high sales growth rate. as for pm performance, the probabilities of its three measures all increase much more than the prior marginal probability."
"scenario analysis is a decision process of analysing possible future events by taking alternative possible outcomes into account. scenario analysis technique is a tool to tell stories about the future and to explore uncertainties [cit] . it presents decision makers with several possible future outcomes, such as an optimistic, a pessimistic, or a most likely scenario."
"we also develop two scenarios, one optimistic and the other pessimistic, by considering a number of possible events about portfolio management practices and present npd performance changes under the scenarios. in the optimistic scenario, all of the portfolio management practices are assumed at high level states. in the pessimistic one, each of the four factors is assumed at a low level. we analyse portfolio success and npd performance by conducting a predictive inference. and then, the changes on the probability of different states in the bn structure are observed, and the prior and posterior marginal probabilities for the two scenarios are presented, as shown in table t3 3."
"suppose that there are n variables, x 1, x 2,…,x n, included in the dag. for a variable x i, its set of parents can be denoted by pα(x i )."
"the important practices are process di, top management involvement, and project manager competency with the sensitivity degrees of 19.3%, 10.1%, and 5.3%, respectively. hence, process di is the key factor to strategic fit. this indicates that pm process di impacts the strategic fit and in turn has a positive impact on the npd success rate, as shown in figure f5 5. an improvement of a portfolio management process can lead to a growth in both strategic fit and npd success rate."
"consequently, we select four portfolio management factors and two contextual factors to construct a bayesian network for assessing their impacts on portfolio success and npd performance. strategic fit, average project success, and synergy are used to measure portfolio success and criteria including sales growth rate, net profit, and npd success rate to measure npd performance."
"problem structuring is to identify portfolio management factors and criteria of performance. in this stage, all research variables and criteria in bns are presented. because portfolio management is a decisionmaking process involving managers as key players, we identify portfolio management factors from two groups. one is related to processes, including process design and implementation (di) and project termination; the other is associated with managers, including top management involvement (tmi) and project manager competency. additionally, two contextual factors, technology turbulence and market turbulence, which may impact npd performance will be considered."
"the objective of parameter learning is to find the most likely network given the data. it includes an expectation (e) step and a maximization (m) step. in the e step, all of the expected values of missing data are computed by using regular inference with the existing bn. in the m step, the bn with maximum likelihood is found using the extended data."
"bayesian networks have been accepted as a scenario analysis technique in systematic reviews [cit] . variables with probabilities are linked in bns where bayes' theorem and related learning algorithms are used to calculate probabilities of future outcome states. it is common to use bayesian networks to construct scenarios in decision support. [cit] developed a bn scenario to analyse transportation-environment relationships. cinicioglu, önsel, and ülengin (2012) in this paper, key factors of portfolio management will be identified and scenarios with different combinations of portfolio management practices will be studied below."
"finally, project termination is essential to align a product portfolio with business strategies. resources can be released from terminated projects and re-allocated to other prioritized projects. whether an inappropriate project is promptly detected is related to the implementation of portfolio management processes. therefore, it is urgent to enhance the implementation quality of the processes in multiple project environments. a strict selection routine and standardized process will lead to transparent decisions and ultimately to detection and abortion of wrong projects [cit] ."
"the primary contribution of the study is a decision support methodology to assess the effect of portfolio management on npd perfor- portfolio management is a manifestation of business strategy dealing with issues about investment for the future [cit] . in a rapidly changing technological and competitive environment, research and development (r&d) investments are paramount to business survival and future prosperity. r&d project portfolio management aims to obtain portfolio success-that is to maximize portfolio value, to align projects with business strategy and to obtain a balanced portfolio with synergies."
suppose q is a target variable and f is an independent variable. the degree of sensitivity of q to f can be described by the variance reduction (or variance explained) v r of the real value of q [cit] :
"first of all, top managers should select and evaluate projects based on their profitability to maximize the value of an r&d portfolio. a clearly defined and consistently applied portfolio selection and evaluate process can help achieve positive portfolio results [cit] and improve npd performance [cit] ). on the one hand, a well-designed, explicit process provides a platform for managers to communicate and to make effective decisions. new projects or proposals can be evaluated, selected, and prioritized on the platform. exiting projects may be reprioritized and re-allocated resources according to the decisions. on the other hand, a wellimplemented process may achieve high performance by the usage of pm methods and techniques (cooper q6, 1999) . inappropriate projects will be terminated in a timely manner, which will release limited r&d resources and re-allocate them to important projects [cit] ."
"secondly, top managers and project managers are responsible for the strategic fit of an r&d portfolio in order to implement firm strategies well. top managers are important decision makers involved in portfolio management. they make npd project screening, selection, resource allocation and other key decisions [cit] . if top managers actively support portfolio management activities in npd, they will deliver required decisions timely and communicate with project managers effectively to help them understand firm objectives (unger, kock, gemünden, & [cit] ) . strong support from top managers is helpful to align projects with business strategy [cit] ."
"secondly, causal loops in the causal map are detected and removed within similar time frames. because an acyclic graphical structure is essential to bayesian inference, the loops should be eliminated."
"process di, project manager competency, and project termination contribute to project performance at 14.2%, 12.5%, and 11%, respectively. as a result, process di plays the most important role in improving"
"the pessimistic scenario is designed so that all portfolio management practices are at a low level. in this case, portfolio management processes are not well designed and implemented, top managers are seldom involved in portfolio management, inappropriate projects cannot be terminated promptly, and project managers have low competency in steering projects. it can be immediately observed that this case leads to a low portfolio management performance with a probability of 33.3%, declining greatly compared with the prior marginal probabilities (table 3) . obviously, when the situations worsen and the level of portfolio management practices drop to low states in the pessimistic scenario, probability of the npd success rate, net profit, and sale growth rate all decrease greatly to a low level."
"project termination is a type of detection decision to re-allocate resources among projects. if initial goals and objectives of a project are not met or some technical issues cannot be resolved, the project would be terminated [cit] . top managers should dismiss or re-assign the project team, release remaining resources, and accept or reject deliverables. therefore, effective termination indicates the gain of resources and the control over investments."
"the data collection of this study is based on the answers of questionnaires sent to the participants of a survey. we conducted the survey with 169 project/portfolio managers in chinese firms. participants are selected from three different management levels: top managers, mid-level managers, and project coordinators in companies. the questionnaire is constituted by 12 variables and corresponding 45 questions to measure them, besides the basic information of participants."
"portfolio management performance significantly contributes to npd performance and firm profitability. several studies have analysed the influential factors of portfolio management performance based on empirical evidence, such as manager dispositions or management methods. however, portfolio success is also highly dependent on management practices operated by top or mid-level managers. the paper presents the first study that analyses the relationships among portfolio management practices and npd performance and develops a decision making method to model their causal relationships. we expect that it additionally, the intervals adopted to discretise variables are important to model bayesian networks. how to determine the range values of intervals and how to improve the effectiveness of bayesian networks are still to be investigated."
"additionally, portfolio success may be affected by external environments. it is a challenge for organizations to sustain long-term competitive advantages in turbulent environments [cit] ."
bayesian network modelling can further quantify the relationships using probability distributions of connected variables. parameters to be determined in bayesian networks consist of marginal probabilities and conditional probabilities of variables.
"although the optimistic scenario is an idealistic situation that may hardly be implemented, the impressive difference of 21-25% for [cit] . as a result, npd projects will be ensured to align with firm strategies by an effective process. furthermore, a well-designed and well-implemented portfolio management process can stimulate top managers' involvement in new product portfolio management so that sufficient resources can be assigned and portfolio management performance enhanced."
"the parameters of a new bn are then obtained through the learning process. after three iterations when the log likelihood has no change, the probabilities of variables in the bn are determined from the learning process, and the results are shown graphically using bar charts, in figure f3 3."
"because the turbulence is mainly caused by changes in both new technology and customer preference, we identify two external environment factors including technology turbulence and market turbulence."
"meanwhile, the linkages between multiple project portfolio management and npd performance are modelled by linear regression analysis [cit] . because linear regression methods have limited capability of measuring performance, there is no clear explanation about what the consequence of implementing portfolio management in uncertain environments is [cit] . moreover, they cannot diagnose the key portfolio management factors which cause low npd performance."
"project termination quality may positively affect the success of portfolios and prevent portfolios from deviating investments into non-strategic projects [cit] ). consequently, new product success rate would be improved [cit] ."
"finally, we summarize the results of the sensitivity analysis conducted on both the npd performance level and the portfolio success level. top management involvement, project manager competency, project termination, and process di are important factors to npd performance. the contribution of both the external technology and market environments is not significant to npd performance."
"this study also calculated the variability of the two variables in order to find the developmental state of the learner's listening and her adaptability to the changing environment. between-session and residual variability approaches were employed in this study. between-session variability is concerned with the difference between an observation and the preceding observation of a variable [cit] . it is calculated based on the absolute differences between the consecutive measurement points over time. residual variability refers to the distance between an expected value (the value on the loess smoothing trajectory) and the observed value (the raw value) [cit] . as a supplement to the phase transitions revealed by the between-session variability, residual variability measures the extent to which an observed value deviate from an expected value based on the smooth curve. thus, it could reveal the emergence of growth spurts that are much higher than the growth model would predict. the predictive model used to derive estimates and residuals was based on the loess smoothing approximation. the residuals were obtained by calculating the distance between the actual observations and the expected values on the smoothed curve."
"as can be observed, there is a surprisingly downward trend in the relationship between listening strategies and listening performance over the study period. first, the correlation coefficient r started with a relatively high positive relationship in the period from week 0e18 (the first ten measurement points), then it decreased gradually to be zero in the period from week 4e22. the downward trend continued and reached a level of strong negative relationship from the period between 12 and 30 onwards."
(2) how does the efl learner's listening performance develop in the course of interacting with listening strategies over the observation period? (3) what is the dynamic correlation between the learner's listening strategy use and listening performance over the observation period?
"in this derivation, the summation of subapertures is precise only for the beam center point, for which there is no residual error of slant range. when the scene extends beyond a certain scope, the residual range error will increase, and the summation will deteriorate. to control the maximum error, a partition of the range data according to the multiple beam centers is adopted in block_ffbp at each stage of subaperture summation. figure 2 presents an example of the subaperture summation and range data partition in each processing stage. initially, the number of subapertures is eight. in each stage, the number of subapertures is reduced by a factor of two. at each stage of subaperture summation in the conventional fbp algorithm, the steering angle and radius are calculated within the local polar coordinate system where the origin is the new aperture. then, the coordinate of the target and the slant range to the final aperture are determined using subaperture summation. meanwhile, in the block_ffbp, the position of the beam center and the corresponding slant range, as well as the differential range are determined for each range block. the slant range of the other points can then be obtained through linear extrapolation using the range and differential range."
"in conjunction with the listening strategies questionnaire, the listener was also required to keep diaries in chinese for the sake of better elicitation of her reflections every two weeks. the reason for employing listening diaries is that diary is a useful means to elicit learners' reflections and develop their listening process awareness [cit] . the prompts of the listening diary mainly centered on her reflections on the use of the listening strategies in her listening activities, such as \"how do you feel in using the listening strategies\", \"did you have any difficulty in using the strategies? if so, what are they?\" and \"what do you think of the role of strategies in your listening activities?\""
"the dynamic correlation revealed by fig. 7 may also provide clues to resolve the ongoing debate over the effectiveness of training listening strategies. the diverging conclusions regarding the effectiveness of the listening strategy training [cit] in previous literature may result from the fact that the data in previous studies were collected at two measurement points, pre-training or after training in particular, instead of a series of regular observations. thus the limited observation points may overlook the hidden developmental patterns and the potential improvement along the trajectory. particularly if the listening scores happened to be collected during the phase of fluctuations which coincided with the second phase of the present study, there may be a great chance to find that the listening strategy training did not enhance learners' listening performance, even cause the scores to decrease. [cit] found that listening strategy training led to the decrease in students' listening performance rather than raising their listening scores. it is possible that the students in her study may be in the stage of fluctuations and self-exploration when the listening performance was assessed. the finding may, from another angle, reflect the dynamic systems-based approaches have the advantage of examining the relationship between variables from a dynamic perspective, as the frequent and regular observations allow us to have a relatively more comprehensive picture of the relationship between the two variables instead of the snapshot obtained by the traditional approaches."
"equation (6) is a continuous expression of the subaperture summation process. hence, partitioning of the range data is favorably avoided, and the complete range data can be processed in bulk. therefore, the interpolation operation for subaperture data, which is denoted as s m (·), can be performed with an fft to improve the overall efficiency [cit] ."
"to construct the mapping relationship between the echo delay of the \"new\" and \"old\" subapertures, an interpolation function g m (τ i ) can be used for the delay time pair (τ i, τ im ), which can be expressed as:"
"when examining fluctuations of the strategies reported not used before, we also calculated the between-session variability (the sum of the distances between consecutive measurements), and the result is presented in fig. 3 (k) . as illustrated by the trajectory, the four strategies never used before, namely resourcing, key words, elaboration and inferencing underwent great fluctuations. the highest peak emerged in week 10 when elaboration and inferencing were just trained, and the peak of variability continued until week 14. the intense variability of the four strategies in this period shows that the lack of prior knowledge may hinder the use of these strategies in the first few weeks after the strategy training. the second highest peak in variability occurred in week 22, which coincided with the breakpoint between phase 2 and 3 as discussed in section 3.1. the result may indicate that variability tends to work as an indicator of phase transition. fig. 3 (l) illustrates the dynamic developmental variability pattern of the learner's use of cognitive strategies. an indicated by the developmental trajectory, the first peak emerged in week 6 proceeded by a rapid growth in the variability of cognitive strategies, while the highest peak occurred in week 22. as discussed in section 3.1, week 22 was a breakpoint between phase 2 and 3 of strategy use, the co-occurring of variability peak with the phase boundary suggests that great fluctuations and variability tended to emerge during the phase transitional period. the finding corresponds to the hypothesis that the proximity of phase transition is accompanied by great variability in the development of systems [cit] ."
"in the polar format algorithm [cit] where two-dimensional matched filtering is implemented in the phase history domain, the azimuth extension is a reciprocal of the sample spacing in the azimuth frequency domain. similarly, sample spacing in the azimuth time domain and the azimuth frequency span follow the same principle. according to this time-frequency mapping attribute, the computational complexity can be reduced by data segmentation, which is adopted by the fbp algorithm. if the scene extension is divided into two sub-planes in the azimuth dimension, the corresponding frequency sampling space is consequently extended, which corresponds to azimuth de-sampling. after the aforementioned azimuth partitioning, the new datasets can also be divided until a proper data size is obtained, which is the basic concept of the ffbp algorithm [cit] . in general, the computational complexity is o mn 2 log m n when the factor of the factorization of aperture n is m. after data partitioning, azimuth de-sampling arises, which could cause azimuth spectrum aliasing and ambiguity in the final image. for its sake, subaperture processing is introduced in the fbp algorithm."
", respectively; while, r ir is the range between the receiver and p i . thus, the echo delay for each subaperture can be written as: the data of \"new\" subaperture a 0 can also be obtained using equation (6) . for a given range line in the focus plane parallel to the transmitter trajectory, the range to the receiver is different for each grid point. therefore, it is difficult to obtain an analytical expression for the subaperture summation without deriving a relationship for the location of the pivots."
"it can thus be suggested that the overall dynamic developmental patterns of listening strategies represented a dynamic self-adaptation, self-organization and simplification process in the course of employing strategies. as shown by the trajectories of the overall strategy use, the student's strategy use increased after the strategies instruction and then reached a peak. however, the analysis of the strategies used in the final stage shows that the learner ended up using only a few strategies to process the listening tasks. the result may suggest that the learner tended to select or decide on the strategies that suit their own listening activity after being instructed with a wide range of strategies. in other words, in the course of using strategies the student seemed to have tried the strategies, tailored the strategies to her own listening practice and finally developed her own strategy preference. over the dynamic process, she tended to adapt her listening strategies to fit her needs in processing the listening tasks. the simplified set of strategies, in turn, could allow her to allocate more attentional resources to the listening tasks, rather than oscillate around which strategies to choose. [cit] statement that \"emergence produces simplicity from complexity\". the self-adaptation is found to be \"itself evolving\" when interacting with the changing contexts [cit] . the finding also parallels the statement that the development of learners' complexity, fluency and accuracy shows a process of self-adaptation to changing environment [cit] . fig. 5 shows the developmental trajectory and moving minemax graph of listening performance. as can be observed, the trajectory of listening performance shows that the student's listening performance was accompanied by salient fluctuations over the forty-week span. in other words, the trajectory of the learner's listening performance was characterized by an alternation of progress and regression instead of a linear developmental path."
"several noticeable regressions were revealed in the trajectory of the listening performance. the regression in listening performance could be explained by \"resource competition\" [cit] during the interaction with listening strategies. as shown by the diary analysis, the learner tended to allocate her attentional resources to predicting the audio information, picking up the ongoing materials and reading the questions, whilst selecting the suitable strategies in the course of processing the listening tasks. for example, the student commented that \"the listening activity is very intense. during listening, i read the given items, and predict the possible episode and questions. i find it very hard to select the appropriate strategies and capture the audio information at the same time. when i am selecting the appropriate strategies to use, i often miss the audio information.\""
"also, we calculated the residual variability to compare the listening performance with the smoothed approximation, and the result is illustrated in fig. 6 (b) . as illustrated by the residual variability trajectory, the first peak emerged in week 14, and the second residual variability peak emerged in week 24, which were just one observing point after the peaks revealed by the between-session variability trajectory. thus, the residual variability peaks roughly correspond to the rapid growth as shown by the between-session variability in fig. 6 (a) . [cit] . the results reflect that week 14 and 24 were two critical points in the learner's listening performance development when the degree of the variations is much higher than the predictive model would estimate. it is also interesting to note that week 14 and 24 were two breakpoints of phases in the learner's listening performance development as will be illustrated by the spline interpolation in fig. 7 . thus, the result may indicate that the variability in the listening performance development may symbolize the arrival of phase transitions. [cit], variability serves as the \"harbinger of change\" (p.342), as it not only provides a driving force for learners' free exploration but also indicates that the system may move to another phase of development."
"according to the basic principle of the proposed accelerated bp algorithm, this algorithm can also be applied in the bistatic sar mode. in this section, the accelerated bp algorithm is provided for two bistatic sar geometries: the one-stationary bistatic sar mode and the tandem mode. in the former case (including the spaceborne/stationary and the airborne/stationary cases), only the moving platform contributes to the azimuth modulation, whereas the stationary platform introduces a range offset to the range migration trajectories of targets at the same range [cit] . therefore, the subaperture summation can be conducted for the moving platform. figure 4 shows the subaperture summation for one-stationary bistatic sar mode where the factorization factor is set to two. the transmitter runs along the y-axis, and the receiver is fixed. r it1, r it2 and r it are the ranges between p i (x i, y i, z i ) and the transmitter subapertures a 1 x 1, y 1, z 1,"
"where τ is the fast time, ∆τ i is the time difference of signal propagating from subaperture x to p and from x i to p,"
"the trajectories of the affective strategies correspond to the student's reflective diary entries regarding the use of the two strategies. for instance, in week 22 she commented that \"i use self-talk strategy a lot. when i am unable to concentrate, i talk to myself to stay focused. when i couldn't follow the listening information, i tell myself to not to give up and attend to the following information. i find the strategy very helpful, as it makes me more confident.\" however, later in week 32, the student wrote, \"i sometimes talk to myself, but far less often than before, maybe because i become more concentrative. i don't think that i have enough time to talk to myself during listening when i am dealing with the information.\" similarly, the learner wrote about her use of the self-reinforcement strategy in week 16, \"i like using the self-reinforcement strategy. i often reward myself with a small gift when doing the tasks well\". however, towards the end of the study, she seldom mentioned the strategy in her diaries, except in one comment in week 36 \"i do not bother to treat myself with a reward now. it is natural for me to do well in my listening\". clearly, those diary entries provide evidence that the learner employed the social/affective strategies during the initial stage of learning the strategies, but stopped using them towards the end. in this sense, these strategies seem to play a central role in assisting her carrying out listening activities during a particular period of the system development."
"in order to explore the relationship between variability and phase transition in the learner's listening development, we plotted the variability trajectory of her listening performance. as shown in fig. 6 (a), the first peak in between-session variability emerged in week 12, while the highest peak occurred in week 22, showing that the learner's listening performance underwent intense fluctuations in the two weeks. it is surprising to note that the variability peaks coincided with some peaks in the variability trajectories of strategies and also showed agreement with the phase breakpoints as discussed in section 3.1. the result indicates that variability seems to be a critical feature of phase transition in a system development. [cit] assumption that variability indicates a specific moment in the presence of a developmental transition."
"in this line, considerable attention has also been paid to examining the effectiveness of listening strategy training from a longitudinal perspective. [cit] traced the listening strategies of eight pupils over a school year. the study found little change in the students' strategy use, and both the higher-and lower-proficient listeners were found to use fewer strategies in the later stage of his study. [cit] investigated the development of listening strategies and listening performance of two lower-intermediate french learners over six months. their results showed great differences in the strategy used by the higher-and lower-proficient learners, and there was a high degree of stability of strategies used over the period of the study."
"therefore, τ im (τ i ) can be obtained by substituting (15) into τ im in equation (13) . similar to equation (11), the subaperture summation can also be expressed in a continuous form: figure 5 presents the flowchart of proposed algorithm. the green and cyan blocks represent processing with and without pivots, respectively. range compression is conducted in the first stage. the number of factorization stages and the factorization factor in each stage are set. for the general case, pivots are required. according to equation (5), an interpolation function is used to calculate the delay time of the \"new\" subapertures, which corresponds to the propagation time delay reconstruction in figure 5 . specifically, when the platform runs along a straight track, pivots are not required, and in the factorization stage, delay time is determined according to equation (10) . after the propagation time delay relationship is established, range data interpolation is performed using an fft followed by the subaperture summation. when the factorization is done, a conventional bp algorithm is applied to focus the new synthetic data, and a final focused image is obtained."
"the participant in this study was a 23-year-old chinese female postgraduate student majoring in engineering. she had learned english for about ten years when participating in this study, and had not been instructed with strategies systematically and explicitly. she passed cet 4 (college english test band 4) in her second year of undergraduate study, two years prior to the study. her english score for the master entrance exam was 65 out of 100. in comparison with the peer students in her class, her english was at an intermediate level. according to her self-report, listening was the most difficult for her comparing with other skills such as reading, writing and speaking. during the study, she was preparing for cet 6 (college english test band 6). therefore, she had a strong motivation to improve her listening proficiency, which was clearly displayed by her active engagement in this study."
"in conjunction with the spline interpolation trajectories, we applied a moving window correlation over the study period in order to explore the correlation between the two variables. following the phases reflected in the spline interpolation smoothed curve shown in fig. 7, we set the window size of the moving window correlation to be ten consecutive measurement points, because it contains enough data points for reasonable estimation of the relationship. the change of the correlation of listening strategies and listening performance is presented in fig. 8 ."
"thirdly, with respect to the strategies reported not used before, their development trajectories displayed intense fluctuations in the first few weeks. for example, key words, as in fig. 3(f), and inferencing, as in fig. 3(j), underwent great fluctuations in the first few weeks after strategy training, but rose and then remained at a high level towards the end. similarly, the resourcing, as in fig. 3(b), and elaboration, as in fig. 3(g), spiraled upward toward a high level after the strategy training, but ended up with a medium level towards the end of the study period. hence, it could be conceivably hypothesised that the \"never-used\" strategies were more likely to undergo great fluctuations and complexities after the strategy training."
"it is worthwhile to point out that the listener's listening performance displayed the following relationships between the fluctuations and phase transitions. for one thing, the proximities of the phase transitions in listening performance tended to be accompanied by overt fluctuations. for instance, the most obvious variation, which emerged at the around week 20, was followed by a relative stable phase as shown in the max trajectory. [cit] statement that the phase transitions are characterized by high variability. the finding supports the statement that variability indicates a transition towards a new attractor state [cit] . the result is also in line with the finding that large variability increases during the initial stage of learning and then tails off as learners develop more advanced l2 systems [cit] ."
"where c is the speed of light. the range-compressed signals belonging to the \"old\" subapertures a 1 x 1, y 1, z 1 and a 2 (x 2, y 2, z 2 ) are s 1 (τ) and s 2 (τ), respectively. afterward, according to the subaperture summation stage illustrated in section 2.2, the \"new\" synthesized subaperture data s (τ) can be written as:"
"in this simulation, the number of pivots changes from 4-64, and pivots are distributed along the range dimension at the same intervals. first, 3000 points are located along the range direction with different range extensions. the average slant computation error is shown in figure 7a . second, 3000 points are located along the azimuth dimension with different azimuth extensions. the average slant computation error is shown in figure 7b . it can be seen that due to the high accuracy of spline interpolation, the slant range computation is very accurate. the influence of different chosen numbers of pivots is ignorable and varies slightly. in this sense, the choice of pivots is flexible. please note that in the experiment, the smallest number of pivots is four; this is because the spline interpolation method requires at least four sampling points."
"in the data analysis, this study employed the following dynamic systems-based techniques in the process of investigating the development of the learner's use of listening strategies and listening performance, and exploring the dynamic interaction between the two variables."
"previous studies have suggested that listening strategies could be taught to broaden learners' strategy choices and enable them to become competent listeners [cit] . in this strand, researchers have set out to explore the models of listening strategy instruction and validate its effectiveness in enhancing students' listening performance [cit] . as indicated by previous studies, listening strategy instruction could equip students with the appropriate skills [cit], thereby enhancing learners' awareness in listening strategy use and equipping them with the skills needed in carrying out listening activities [cit] ."
"the study investigated the dynamic patterns of listening strategy use and listening performance and explored the interaction of the two variables from a dst perspective. the developmental trajectories reveal that both the listening strategies and listening performance demonstrated non-linear developmental patterns. the moving min-max graph, spline interpolation trajectories and moving window correlation show that the student's listening strategies and listening performance mainly underwent three seemingly co-occurring phases. as shown by the analysis, prior knowledge was found to play a critical role in the acquisition of listening strategies. the developmental path of listening strategies revealed a simplification, self-adaptation and self-organization process in acquiring and using listening strategies. the listening performance development suggests that intense fluctuations and great variability tended to occur in the proximity of a phase transition, and the regression could predict progress to some extent. it was also found that the correlation between listening strategies and listening performance is characterized by dynamic and varied developmental patterns, and the moving window correlation shows a surprisingly downward trend in the relationship between listening strategies and listening performance over the study period."
"where f 0 is the carrier frequency, x i is the location of the original aperture, x is the location of the synthesized aperture and s ( x i, p) is the echo signal at x i ."
"in order to depict the general and underlying developmental trends of the student' s listening strategy use and listening performance, we plotted the loess curve, locally weighted least-squares smoothing [cit], across the data of the listening strategies and listening performance. given that loess is achieved by \"weighting the data proportional to their distance from the middle of the window\" [cit], p.595; [cit] ), it serves as an efficient descriptive and exploratory tool for modelling complex and uncertain processes for which neither developmental patterns nor theoretical models exist [cit] . therefore, this study employed this technique to explore the complex developmental trajectories of the individual listening strategies. in carrying out loess smoothing, this study used pts loess smoothing utility [cit] ). the smoothing parameter alpha a was set to be 0.33, thus the moving window being 7 observation points, 1 to allow the smoothed curves to better display the general patterns while showing the local patterns of the variations."
"one particular strand of the dst empirical studies has centered on exploring language learners' writing development [cit] . these studies provide evidence that students' writing is characterized by a complex dynamic development process in which a variety of factors interact with each other. meanwhile, applied linguists have also focused on the dynamic development of other linguistic features, such as vocabulary development or loss [cit], multilinguistic knowledge [cit], chunks learning [cit], multiple variables [cit], learner agency [cit], chinese numeral classifier system [cit] and english speech [cit] . the results suggest that the process of learners' language development displays great variability, and the variables interact with the internal and external factors within students' language learning system."
"in mobulk_ffbp, the beam center point p in the right panel of figure 3 can also be taken as a pivot. when the subaperture summation is performed using equation (6), the residual phase error of point n can be written as:"
"the listening materials used in the in-class and after-class exercises were chosen from the textbook graduate english for the 21st [cit] . the listening tests used to assess students' listening performance were adapted from the model test of cet 6 released by shanghai foreign language education press. cet 6 is a national english test designed for college students in china. in this study, 21 different test papers were employed to assess the student's listening performance. given that the test papers were all designed following the standard of cet 6, it could be assumed that the test versions were homogeneous in the level of difficulty. the listening test papers consisted of the following four sections: short conversation, long conversation, passage comprehension and compound dictation. one modification was made in the test papers by adding one compound dictation section, because the participant reported this section was the most difficult part for her and had expressed a strong desire to have more practice on this section. one sample of the tests is shown in appendix c."
"according to dynamic systems theory (dst), learners' language development is a dynamic self-adaptation and selfrestructuring process, in which \"a set of variables mutually affect each other's changes over time\" [cit], p.50) . the dst perspective could unfold the development of language learning systems and reveal some features that remain elusive with traditional approaches. its novel methods could also potentially accommodate the individual variations in a complex system, thus allowing us to trace how learners' language competence develops during its interaction with other variables in a complex learning system [cit] ."
"for this mode, let the \"new\" subapertures of a transmitter and receiver be a 0 (x 1, y 1, z 1 ) and"
"due to oversampling in the angular coordinate system, the slant range calculations are transferred from the polar coordinate system to the cartesian coordinate system so that the range calculations are simplified. therefore, this accelerated bp algorithm can be applied in both bistatic and monostatic sar configurations."
"in order to provide statistical support for the negative correlation between listening strategies and listening performance from week 22 onwards as shown in the spline interpolation trajectory and the moving window correlation, we calculated the linear regression for the two variables in this period. fig. 9 illustrates that the listening performance showed an increasing trend, while listening strategies showed a decreasing trend, which corresponds with a negative correlation from week 22 onwards. therefore, the result of the linear regression corroborates the negative relationship between the use of listening strategies and listening performance from week 22 onwards."
"it has been suggested that the min-max graph could show the dynamic stability in learners' language acquisition [cit] . thus the present study analyzed the moving min-max graph of the student's listening performance and the result is presented in fig. 5 . as shown by the min-max value, the learner's listening performance displayed the following noticeable developmental phases. in the first phase, the student's listening performance rose steadily to a relatively higher level. from week 4e18, the student's listening scores developed relatively steadily, despite the mild fluctuations as shown by the listening performance path. the max trajectory from week 20 onwards shows that the student's listening performance moved to a high-level stable development period."
"this section presents the raw data and the loess smoothing trajectories of each metacognitive strategies. considering that variability is \"a potential driving force of development and a potential indicator of ongoing processes\" [cit], p.341), we also explored the intra-individual variability of the learner's metacognitive strategies for the purpose of identifying the peaks in the developmental variability and exploring its relationship with phase transitions."
"an examination of the overall strategies indicates that the student' use of listening strategies was characterized by a complex developmental pattern. some strategies, such as repetition and summarizing strategies, increased and remained stable at a high level, while some strategies, like self-talk and self-reinforcement strategies, were frequently used during the initial stage of learning, then experienced a rapid increase and eventually disappeared towards the end. [cit] overlapping waves model which assumes that learners employ various strategies at any point of their problem solving, thus taking on the appearance of a series of overlapping waves."
"in contrast with the diverse patterns illustrated above, the developmental trajectory of individual social/affective strategies ( fig. 4) shows roughly similar patterns within the four strategies. clearly, the student's use of the four strategies increased notably to a high level of use after the instruction from week 14 onwards. such increase, on the one hand, could reflect the effectiveness of the listening strategy instruction, as listening strategy training not only increases learners' listening strategy awareness but also equip them with the skills in carrying out listening activities. on the other hand, the higher use of the social/affective strategies suggests that the student seemed to have encountered difficulties in carrying out listening activities. as shown by analysis of surveys and diary entries during this period, the student often resorted to the social/affective strategies in order to gain confidence to better fulfil the listening tasks. in this light, the social/affective strategies could be regarded as compensatory strategies students employ to deal with the challenging listening tasks."
"another key concept in dst is 'phase transition', which is defined as \"the coming-into-existence of new forms or properties through ongoing processes intrinsic to the system itself\" [cit], p.38) . phase transition mainly involves discontinuous changes which could usher in a new stage in which some new features are gained [cit] examined the writing fluency development of two efl japanese university students through repetition of a timed writing task. in their study, the data were collected once a week over a school year and analyzed in terms of the sudden jumps, anomalous variance, divergence and qualitative change in the attractor. the study identified some notable phase transitions that the two students underwent in their development of l2 writing fluency."
"this study contributes to our theoretical and methodological knowledge of listening strategy research from a dynamic systems perspective. the findings enrich our understandings of the developmental patterns of listening strategies and listening performance, and the dynamic interaction between listening strategies and listening performance, thus revealing the advantage of the dynamic systems-based approaches in listening strategy research. this study also contributes to resolving the ongoing debate over the effectiveness of training listening strategies and allows us to have a more comprehensive picture of the relationship between the two variables. pedagogically, the results may provide some guidance for english listening course designers in developing curriculum, syllabus and textbooks. it is also hoped that the results may be useful for practitioners to take a dynamic and formative perspective in teaching and assessing students' listening performance, incorporate the dynamic developmental characteristics into the listening strategy training practice, and adjust the teaching practices, materials and assignments to cater for students' developmental learning characteristics and requirements."
"in order to trace the participant's strategy use and listening performance, the two variables were measured every two weeks. the assessment was conducted every other friday, lasting for 45 min. following the assessment, a questionnaire was administered to investigate her listening strategy use in processing the tasks. after that, the student was required to write her reflections guided by the prompts as shown in section 2.2. the first assessment and survey took place in the week prior to the strategies instruction, for the purpose of investigating the student's initial listening proficiency and prior knowledge of listening strategies. then, the assessment and survey were carried out every other week from week 2 onwards. in total, this study collected 63 pieces of data concerning the participants' listening strategy use, listening scores and dairies (2 pieces for each)."
"in this paper, an accelerated bp algorithm is proposed to focus monostatic and bistatic sar data. in this algorithm, the range data are processed in bulk rather than through block partitions. this unified range data processing scheme improves the computational efficiency and simplifies the procedure. a fixed number of pivots rather than beam centers is applied to construct the relationship of the propagation time delay between the \"new\" and \"old\" subapertures. moreover, when the trajectory is a straight line, the pivots are not required, and the analytical expression of the subaperture summation can be derived for the monostatic and azimuth-invariant bistatic sar case. error analysis shows that the accuracy of the proposed algorithm is verifiable. since the algorithm is an improved version of block_ffbp, it satisfies numerical performance standards and retains the processing ability of block_ffbp. moreover, it can also focus the bistatic sar data acquired from the tandem mode and the one-stationary bistatic sar mode. simulation data and real radar data validate the performance of the algorithm."
"the main limitation of block_ffbp is that the range data are partitioned according to the beam centers; therefore, in the subaperture summation process, the synthesized data in each block can only be obtained through interpolation. if the range dimension partition could be avoided, range data from the identical beam are maintained as a bulk, and the interpolation can be replaced by an fft. furthermore, to ensure the interpolation quality of a margin point for a selected interpolation kernel, a fixed backup allowance of the beam data length should be retained. with an increase in the number of factorization stages, the proportion of the interpolation kernel length throughout the entirety of the beam data increases, which indicates that additional memory space is required."
"in this accelerated bp algorithm, a fixed number of pivots, rather than beam centers whose number is variant in each subaperture factorization and summation stage, are applied. like the general interpolation process, values at sample points are provided, and the interpolated values at specific query points can be obtained using an interpolation method. pivots are analogous to the sample points. the correspondence relationship of the propagation time delay from these pivots to current and to synthesized subapertures can be constructed. afterward, the slant range of the other imaging points can be determined using a conventional interpolation method."
the moving min-max graph [cit] was employed to detect the temporary changes and the degree of variation in the development of the two variables. the moving min-max graph is a descriptive approach to visualize the variability and highlight the general developmental patterns of the variability [cit] .
"to demonstrate the focusing ability of the proposed algorithm for the bistatic sar case, an azimuth-invariant bistatic spotlight configuration is initially investigated. the transmitter and receiver run along straight trajectories that are separated by 5 m. the system parameters are the same as the point target simulation in the monostatic sar case. the focused master and slave sar images are shown in figure 11b,e, and the result indicates that bibulk_ffbp can also perform well. due to the relatively short baseline, the results are very similar, but they demonstrate a slight difference in the marked zone. the factorization factor of bibulk_ffbp is four, which means that four aperture positions are summed in each processing stage until the entire block is processed, thereby requiring four stages."
"when analysing the three types of listening strategies, we used between-session variability in order to illustrate the temporary changes in variability and explore the variability peak in the listening strategy developmental trajectory. as for the variability analysis of the listening performance, we used the between-session variability to detect a developmental transition in the learner's listening development and residual variability to visualize the degree of fluctuations of the listening performance with the expected value."
"in this section, the accuracy and efficiency of the proposed algorithm are validated using point target simulation and real data. the time cost for each stage of the factorization is also taken into consideration. due to the parallelizability of the processing scheme, a horizontal comparison between different parallel processing strategies is given. for the azimuth-invariant bistatic sar case, synthesized sar data are utilized to confirm the performance of the algorithm. meanwhile, for the spaceborne/stationary bistatic sar configuration, real data acquired on 31 [cit], using terrasar-x as an illuminator in the staring spotlight mode, are used for validation."
"where τ is the azimuth time, f is the range frequency, r d (τ) is the direct signal path and r tr (τ;r) denotes the signal propagation range for a target located atr. according to the information given in the terrasar-x product file, an xml file, the direct pulse phase history compensation can be performed in the ss-bisar coordinate system, after which the range-compressed signal becomes:"
"in order to obtain a detailed picture of the dynamic developmental features of listening strategies, the 21 listening strategies were examined separately. the following section reports the strategies development from the perspective of three categories of listening strategies, namely metacognitive, cognitive and social/affective strategies in detail."
"after performing the direct pulse phase history compensation and an inverse fourier transformation, the range-compressed signal can be focused using the bibulk_ffbp. in this experiment, the factorization factor is four, and the four subapertures in each of the four stages are summed into a new one. the focused result is shown in figure 15 . the magnified areas a and b are also shown in the right panel. area a is located in the near range of the receiver, from which a high snr is consequently obtained. because of the large incidence angle of the receiver, the tree canopies are clearly identified, and the track of an athletic field is easily recognized. area b has some buildings that were still in construction at the time of data acquisition, and two tower slewing cranes are clearly focused. these details validate the focusing ability of the proposed bibulk_ffbp. as previously discussed, each stage of factorization can be executed in parallel, which further accelerates the processing scheme. to show the acceleration with a multi-thread operation, the factorization and subsequent bp are executed in a single-threaded (st) and a multi-threaded (mt) environment, respectively. the results are shown in figure 16 . the hardware configuration is shown in table 3 . the computation time of the raw radar data containing 1536 msampleson an image grid of 144 mpointsis 203.6 min, 306 min and 400.405 min for mt-mt, st-mt and mt-st processing pairs, respectively. therefore, the parallelization of the processing scheme provides a large increase in computational speed compared to the conventional bp algorithm. moreover, when the program is executed within a gpu platform, the computation speed is even faster."
"as the listening test was adapted from the listening model test, the numbering of the test items was kept as 11e57 following the original order, for there is a skimming & scanning section before the listening section in the cet 6 test papers. the test consisted of multiple choice (sections a and b, items 11e35) single-word cloze (section c, items 36e43 and 47e54), and sentential cloze questions (section c, items 44e46 and 55e57). multiple choice questions counted for two points per question, single-word cloze counted for one point per question, and sentential cloze counted for four points per question. the tests of listening performance were marked by two english teachers who taught post-graduate english courses. in order to determine the agreement between the two raters, the scores given by the two teachers were analyzed using cohen's kappa with spss, and the alpha of intra-rater reliability of the coding was 0.96, indicating a relatively high agreement between the two raters. averages of the two raters' scores were calculated for cloze items and combined with subtotals of the multiplechoice items. a full score on the exam was 90 points."
"the analysis shows that there were some interesting patterns in the smoothed curves of the metacognitive strategies. first, the strategy used prior to the strategy instruction, like planning, as in fig. 2 (a), and problem identification, as in fig. 2 (f), tended to experience steady growth after week 1 and 5 when the two strategies were instructed respectively. in contrast, the strategies not used before, like self-monitoring, as in fig. 2 (e), and self-evaluation, as in fig. 2 (g), experienced intense fluctuations in the first few weeks after the strategy training in week 3 and 5 respectively. interestingly, despite the similar variability experienced in the initial stage after strategy training, the strategies were found to end up with diverse patterns. for instance, self-evaluation, as in fig. 2 (g), underwent great fluctuations since week 5 when it was instructed and then moved on to a phase with relatively high use from week 26 onwards, while directed attention, as in fig. 2 (b), was found to be used at a medium level in the later stage of the study. however, strategies like self-management, as in fig. 2 (d), and selfmonitoring, as in fig. 2 (e), also went through dramatic fluctuations after being taught, but then ended up with a low level of use towards the end. overall, the results indicate that the strategies with prior use tended to develop relatively smoothly after the strategy instruction, while the strategies not used before seemed to experience great fluctuations during the initial stage. in order to find how metacognitive strategies fluctuate over time, we calculated the variability of the metacognitive strategies. fig. 2(h) shows several peaks emerged in the variability trajectory of the metacognitive strategies. the first and the highest peak emerged in week 4 following the rapid growth in the initial stage, and the second major peak took place in week 14, which was followed by two relatively small peaks in week 22 and 28 respectively. it is interesting to note that the second major peak coincided with the finishing point of listening strategy training, which may indicate that the strategy training seemed to have some impacts on the variability of metacognitive strategies. as discussed in section 3.1, week 22 was breakpoint between phase 2 and 3 of listening strategies trajectory, the co-occurrence between the peak and the phase transition boundary suggests that great variability is likely to happen around the proximity of phase transitions. the finding provides empirical evidence for the theoretical assertions that variability is an indicator of a phase transition [cit] . 3.1.2. the developmental trajectories of cognitive strategies the raw data and smoothed trajectories of the individual cognitive strategies are illustrated in fig. 3 (a)e(j). the analysis shows that the cognitive strategies displayed the following notable characteristics. first of all, the participant used more cognitive strategies than metacognitive strategies. one of the possible reasons is that cognitive strategies mainly deal with the materials [cit], and it is possible that some of the strategies may have been taught or acquired in the learner's previous experience, and then transferred into her listening activities. for instance, the student reported that she fig. 3 . the dynamic developmental trajectories of cognitive strategies note:"
"despite the aforementioned studies pertaining to listening strategy instruction, a consensus on the effectiveness of listening strategy instruction has not been reached. [cit] carried out the instruction of selective attention, note-taking and co-operation strategies with 85 intermediate esl learners in 8 days and testified the effectiveness of the strategy training. the study discovered some differences in the means of the student's post-tests scores, but failed to find significant differences. [cit] conducted an interventional instruction of socio-affective, metacognitive and cognitive strategies to efl students but the study found no significant difference in the post-testing scores between the experimental and control group. in recent literature, the controversy continued over the effectiveness of listening strategies [cit] or feasibility of listening strategy training [cit] ."
"when exploring the relationship between listening strategies and listening performance, we used the spline interpolation to generate the developmental trajectories of two variables and plotted the moving window correlation to show the statistical correlation between the two variables over time. fig. 6 illustrates the spline interpolated trajectories of the learner's listening strategy use and listening performance. as can be found, the student's listening strategies and listening performance mainly underwent three seemingly co-occurring phases, namely the first phase lasting from beginning to week 12, the second phase from week 14e22, and the third from 24 to 40."
"in order to consolidate the strategies learned in class, three after-class exercises were assigned to the participant each week. in the assigned listening tasks, the specific listening strategies embedded in the listening tasks were explicitly noted in the first 14 weeks when she was instructed with the strategies. for example, after the first session when planning, directed attention and selective attention strategies were trained, the student was required to practice the three strategies learned in her assigned listening tasks. from week 16 onwards, the student was required to select strategies by herself and report the strategies she used, instead of being assigned with the specific strategies in the listening tasks, for the purpose of practicing her skills in identifying and using strategies in the listening tasks."
"the positive correlation between the listening strategies and performance in the initial stage indicates that listening strategies played an essential role in the student's listening performance. listening strategies could develop self-regulated learning habits [cit] and help users to become more competent in processing the upcoming information. specifically, metacognitive strategies equip students with the skills needed in organizing the listening activity, such as self-evaluating listening process and selecting appropriate strategies, thus enabling learners to pay more attention to the listening tasks. in that sense, it is useful for students with relatively low and intermediate listening proficiency to \"orchestrate the cognitive process more efficiently and effectively\" [cit], p.43 ) and actively engage themselves in capturing the upcoming audio information. likewise, cognitive listening strategies could equip students with effective techniques, such as predicting, inferring and taking note, in dealing with the listening tasks. the social/affective strategies could improve the learner's affective and communication ability needed in carrying out the listening tasks. given that the listening activity is generally regarded as an elusive process, it may be necessary to equip students with a toolkit composed of strategies for the purpose of enhancing listeners' competence and tuning into a favourable affective state to processing listening tasks. overall, listening strategies could provide students with the appropriate and efficient methods to \"orient themselves to the listening task, access their background knowledge, and compare their interpretation of the input with the actual input\" [cit], p.522), thereby enhancing their active engagement and competence in processing the upcoming audio information flow."
"equation (11) represents a case in which the factorization factor is two, and it can be naturally extended to cases with other factorization factors. it is a more simple and practical subaperture summation approach without pivots that facilitates the computational operation. in equation (10), the locations of \"new\" and \"old\" subapertures are only required to be known, and the pivots are no longer required. in this sense, this functional version of equation (10) is known as range determination. this makes a further simplification of the process. in practice, with the use of fine motion compensation, an equivalent straight line can be obtained, and mobulk_ffbp can be applied without pivots."
where g m (τ i ) can be a spline interpolation kernel or a sinc kernel. the subaperture summation can be updated by substituting this interpolation function into equation (4) . the new expression of s (τ) is
"as is shown in figure 5, the factorization is operated sequentially. however, the range data are processed as a bulk set, and the upsampling of this is performed using an fft in each stage of subaperture summation. in this sense, each stage of the factorization can be executed in parallel. moreover, after factorization, the conventional bp algorithm can also be executed in parallel, such as with a gpu [cit] or a multi-thread technical cpu. this will further accelerate the proposed algorithm."
"in order to explore the dynamic relationship between the two variables, this study analyzed the dynamic correlation between the learner's listening strategy use and listening performance by employing a moving window technique and then plotted the moving correlation trajectory over the study period."
"for the purpose of providing statistical support for the correlation shown in the spline interpolation trajectory and the moving window correlation, we calculated the linear regression for listening strategies and listening performance."
"first of all, in order to depict the general trend of the student's listening strategy development, we plotted the loess curves across the data of the metacognitive strategies. as shown in fig. 2 (a)e(g), the student's the metacognitive strategy development trajectories were characterized by noticeable diversity and complexity."
"judging from the above diary entries, it seems that the synchronous operation of the multiple tasks may cause competition in the brain to shift part of attentional resources towards identifying the appropriate strategies and employing them properly. given that attention resources may be decided the moment when listeners' attention tuned into selecting strategies, the learner might neglect some of the upcoming information. this is particularly the case with the audio information, which is stored in short-term memory and might elapse before being transferred into long-term memory. as a consequence of the competition between the conscious use of the strategies and the attention focusing on the upcoming information, the learner may find a temporary drawback in using listening strategies to process the audio tasks."
"english listening is widely acknowledged as a major challenge for efl learners, and it has been reported as one of the most difficult skills in comparison with reading, speaking and writing, especially for efl learners with relatively lower english proficiency [cit] . one of the possible reasons is that listening is mainly characterized as a \"fleeting\" [cit] and \"irreversible and multi-dimensional\" [cit] process. among the considerable studies conducted to improve learners' english listening proficiency, listening strategy is regarded widely accepted as one of the most effective ways."
"when analysing the correlation of the listening strategy use and the listening performance, we used spline interpolation combined with a smoothing operation for the purpose of visualising the dynamic interaction between the two variables. the spline interpolation trajectories also provide clues for defining the window size of the moving window correlation as discussed below."
"it is necessary to point out that most of the studies above carried out from a longitudinal perspective mainly focused on the comparison between the pre-test and post-test scores in investigating the listener's proficiency change. there is little knowledge on how individual listening strategies and listening performance develop when observed over short intervals. also, the dynamic interaction between listening strategies and listening proficiency remains unexplored. given that dynamic description is acknowledged as an effective way of understanding how a system evolves over time [cit], this study set out to take dst as a point of departure, explore the dynamic developmental patterns of efl learner's listening strategy use and english listening performance, and investigate how the two variables interact in the dynamic system. to be specific, the study aimed to address the following questions:"
"as it plots the moving minima, maxima, and observed values of the variables, the moving min-max graph highlights \"the general pattern of variability, while keeping the raw data visible\" [cit], p.75 ). thus, it was applied to examining the general developmental pattern of listening strategy use and listening performance for the purpose of obtaining an overall picture of the developmental patterns of the two variables. the predetermined moving window span chosen in this study was three consecutive measurement points with the aim of obtaining a relatively detailed picture of the developmental patterns."
"in the statistical analysis, the monte carlo (random permutation) technique [cit] was used to calculate if there is any statistical significance in the differences observed in the developmental trajectories. the statistical technique is appropriate for the observations in this study, and the p-value calculated by this technique \"very closely approach[es] the expected p-value and will thus be reliable, irrespective of the strangeness of the sample\" [cit], p.46 )."
"where g m (·) is the interpolation kernel defined by equation (5). moreover, when the platform runs along a straight trajectory, pivots are not required, and points that have the same azimuth coordinates as p have no phase error. the residual phase error can be expressed as:"
"the trajectory of the student's listening strategy use and the min-max values are illustrated in fig. 1 . it is clear that the student's listening strategy use followed a noticeable non-linear pattern. in other words, the development of the listening strategy use did not remain stable in the course of the trajectory. as illustrated by the developmental trajectory, the learner's listening strategy use was characterized by a temporal overshoot in the initial stage of strategy training. then her strategies remained at a relatively high level from week 8e22, which was followed by a gradual downward trend from week 24 onwards, and eventually moved to a final period of stabilization. it is interesting to note that although the strategies leveled off towards the end, it was still much higher than the use prior to the strategy training. overall, the learner's strategy learning trajectory shows periods of progress and regression rather than a neat linear developmental path. the non-linear developmental pattern of listening strategies is in accordance with the findings in previous studies [cit] ."
"the left image in figure 3 shows an example of subaperture summation where the factorization factor is two. the platform runs along the y-axis; the x-axis is the range direction; and the z-axis denotes the height dimension. the new subaperture at a 0 (x, y, z ) is the summation of subapertures at a 1 x 1, y 1, z 1 and a 2 (x 2, y 2, z 2 ). the number of pivots along the range dimension is n, and a pivot can be denoted as p i (x i, y i, z i ). in this accelerated bp algorithm, the number of pivots is fixed during the focusing process, which is different from block_ffbp, wherein the number of reference points grows with an increase in the number of factorization stages. in the left panel of figure 3, the slant ranges r i1, r i2 and r i are the distances between pivot p i (x i, y i, z i ) and the apertures a 1 x 1, y 1, z 1, a 2 (x 2, y 2, z 2 ) and a 0 (x, y, z ), respectively. the corresponding time delay can be expressed as:"
"meanwhile, the trajectory also shows an interesting relationship between regression and progress. as illustrated by fig. 5, two notable regressions took place around week 14 and 22, each of which was followed by remarkable progress in the subsequent weeks. specifically, the student's listening score decreased substantially in week 14, which was followed by an increase between week 16 and 18. similarly, her listening score in week 24 reached a peak, following a temporary decrease in week 22. the result may imply that regression could predict progress to some extent. in other words, regression may be a sign of progressing or updating to a higher level of language development. the dairy analysis reveals some comments corresponding to the fluctuations during the period. for instance, the student wrote in week 14 that \"i find it disturbing to use the listening strategies during listening. when i was trying to find out which strategy to use, i missed the audio information.\" similar worries were also expressed in the diary in week 22, \"at the beginning of the test, i was searching for the suitable strategies to use, and i missed the beginning of the audio information. i became very nervous, and found it very hard to concentrate.\" as suggested in the diary in the following week, the learner seemed to explore a way to improve the situation fig. 5 . the developmental trajectory and moving minemax graph of listening performance. because she wrote \"when self-evaluating my listening comprehension, i realise that if i can find the suitable strategies and feel comfortable at the beginning, i often capture the listening material easily. otherwise, i often ended up with feeling very worried. (…) i made a decision to use whatever strategies come to my mind first. i think this is good because it saves time. now i choose to pay more attention to the listening tasks than to listening strategies\". these diary entries indicate that the regression may manifest the learner's free exploration and resilience in applying the listening strategies to her listening activities. in other words, the temporary drawback in listening performance, from a dynamic viewpoint, reflects the learner's self-organizing and self-adapting to the new phase in the dynamic interaction within systems, as the regression tended to spur learners to take measures to explore the suitable strategies, thus contributing to advancing their listening comprehension to a higher level. [cit] statement that a small push often drives a system to another state. [cit] also find that regression reflects learner's \"flexible and adaptive behaviour\" and allows them to explore freely the strategies to be used in the system development. therefore, fluctuations \"probe the stability of the system, allowing it to discover new and different ways to solve problems\" [cit], p.11 ) and provide the momentum for the development of dynamic systems."
"this section provides the phase error caused by an incorrect slant range when the back-projected data are accumulated during a subaperture summation. since the proposed algorithm is derived from block_ffbp and extended to the bistatic sar case, the error analysis will be conducted through a comparison with block_ffbp using a numerical method. moreover, as block_ffbp was developed for the monostatic sar case, the error analysis is mainly performed for this case."
"as previously stated, nodes have to know their 1-hop neighbors and their degree to construct a ds. algorithm 3 gives the pseudo-code of the neighbor discovery and degree exchange procedure. nodes exchange their degree with their neighbors. for this purpose, a node periodically broadcast a degree_message which contains the sender's degree. during the degree exchange, nodes also discover their neighbors. if a node discovers a new neighbor, then it updates its degree. also, each node stores the node which has-up to that time-sent the highest degree in order to elect this node for becoming a dominator later."
"a maximal leaf spanning tree (mlst) can be used to decrease the number of nodes. we regard nodes that are connected to only one node as a \"leaf\". a mlst is a st with the maximal number of leaves. in terms of network flooding, non-leaf nodes can be used to distribute a packet through the whole network. a mlst contains the minimal set of so-called \"forwarders\". a forwarder is a node which forwards a received packet in the direction of its destination. the mlst problem is equivalent to the mcds problem which also produces the minimal set of forwarder nodes. we discuss cdss in the next section."
"in figure 5, we consider a system where the source is located at alice, the detector apparatuses have efficiencies of 18.81%, the channel to bob has an efficiency of 77.08%, and there is no noise or eavesdropper present. as expected, the key generation rate has decreased, while the percent error has stayed constant. the ber has increased, because with more loss, most of time slots contributing to the key are generated when multi-photon c) absolute error between simulated and analytic key generation rate. d) analytic ber for type i and type ii spdc source. e) percentage error between simulated and analytic ber. f) absolute error between simulated and analytic ber."
random forest is an ensemble model based on decision trees [cit] . an ensemble trees with random feature selection and bagging prevents the model from overfitting and provides more accurate results in prediction [cit] . hence rf has been widely used in a number of studies [cit] .
"the experimental results demonstrate the superiority of the proposed model selection framework in terms of prediction accuracy. in particular, the proposed method almost outperforms all of the candidate models and verifies its effectiveness in model selection. second, the proposed method outperforms the simple model averaging method and further shows that the method is effective in reducing the risk in model selection for a new time series. third, the use of forecast horizon features and the approach to feature reduction are both feasible methods for further improving the performance of the model selection framework. the analysis shows that the optimal model for a certain category varies for different forecast horizons, and the optimal model is also different for different categories. the main reason is that the different distributions of time series features lead to different model selection results."
"for our simulations, we use a network that consists of 100 nodes in three scenarios: we simulate a network with various transmission ranges of 15 meters, 35 meters, and 45 meters, in order to evaluate the performance of cone in different network densities. the nodes are arranged in a square in all scenarios. a larger transmission range results in a higher network density in our scenarios due to the larger number of neighbors of nodes. we expect that a higher network density leads to the occurrence of broadcast storms due to a larger number of simultaneous transmissions in a node's neighborhood. furthermore, a larger transmission range of nodes results in a smaller maximal hop distance of the network. the maximal hop distance of a network is the minimal number of hops needed to distribute packets from the initiator (source of information) to all other nodes in the network (i.e., covering the whole network). each test runs for ten minutes. during those ten minutes, an initiator broadcasts one packet in every 15 seconds. cone uses the first 90 seconds to construct a cds. figure 12 illustrates the cdss which have been constructed by cone during our simulations. the green nodes represent the dominators. due to the arrangement of nodes, the network contains several nodes that have similar sets of neighbors with the highest degree in their local neighborhood. since nodes elect dominators with the lowest known id, the intersection of two or more similar sets of neighbors does not collectively elect one dominator. this causes a larger set size for the constructed cdss in all three scenarios. as shown in the figure, with a lower network density (see figure 12a ), cone requires 55 dominator nodes so as to cover the whole network. while with a higher network density (see figure 12b,c), cone needs much fewer dominators, i.e., 23 and 19, respectively. this indicates that cone is able to decrease the number of nodes (dominators) that are responsible for broadcasting, in order to reduce broadcast redundancy and improve energy efficiency accordingly. the 9-hop scenario in figure 12a has overall the largest number of dominators, since the network diameter is the largest one. furthermore, the number of dominators of the 3-hop scenario in figure 12c is larger than in the one of the 5-hop scenario in figure 12b . the larger number of dominators in the 3-hop scenario occurs because we limit the degree of nodes in our implementation. otherwise, nodes would spend too much time in order to exchange their degrees. the degree limit causes a larger number of nodes with the highest degree in their local neighborhood that also have similar sets of neighbors. cone, in all scenarios, on average causes a smaller rdc and less packet loss compared to the baseline protocol (see figures 13 and 14) . this means that cone is able to effectively decrease packet loss by exploiting a cds for network flooding. the nodes with the highest rdc in the scenarios from figure 12b,c also have a smaller rdc, if we compare cone and trickle (see figure 13) . however, the node with the highest rdc in the scenario from figure 12a has a slightly higher rdc for cone. however, higher rdcs occur only for dominators. due to the lower number of nodes broadcasting packets in cone, dominators receive packets more often compared to nodes in trickle. this is shown in figure 14 . additionally, dominators have to rebroadcast packets more often, which results in higher rdcs. however, the average rdc for cone is still smaller compared to trickle in the scenario of figure 12a ."
"as for the two classifiers used in this study (rf and svm), we find that the forecast error of svm is on average larger than that of rf in all the cases, as can be seen in table 6 and table 7, which indicates that rf is much more suitable for this classification task. moreover, the classification accuracy of rf is higher than that of svm in both msn and msh, as shown in table 8 . the reason may be that rf has advantages over svm when dealing with unbalanced data. the repeated random sub-sampling in rf has been found to be very effective in dealing with an imbalanced dataset [cit] whereas svm assumes that the class distribution in the dataset is uniform [cit] ."
"the spdc source can either be type i or type ii with pair number distributions given by equation 1 and equation 2 respectively as functions of µ, the mean number of photon pairs. 10, 13, 14 it should be noted that the distribution of 14 can be written as equation 1 by rewriting in terms of the mean number of photons."
1: receive token_message 2: if dominator's id equals the received token then 3: set id of my dominator to 0 4: stop timer for election_message 5: else if (my dominator is the sender of the received token_message and sender is a dominator) then 6: set id of my dominator to 0 7: stop timer for election_message 8: end if 9: if value of my token is lower than the one of received token then 10: if sender is a dominator then 11: set value of my token to value of received token 12: else 13: if (id of my dominator equals 0) or (id of sender is equal to my dominator) then 14: set sender as my new dominator 15: end if 16: increase counter by 1 17: if counter equals threshold then 18: set token counter to 0 19: set counter to 0 20: set timer for election_message 21: else if value of my token is greater than the one of received token then 22: set token counter to 0 23: end if 24: end if 25: end if 26: periodically broadcast token_message 27: while ds_is_connected is false do 28: set timer for broadcasting token_message 29: wait until (token counter is lower than threshold or i am a dominator) 30: wait until timer for broadcasting token_message has expired
"in case of cone, only dominators rebroadcast information. as stated in section 2.2.3, each node in the network that is not in the cds is adjacent to at least one node of the cds. therefore, it is sufficient that only nodes in the cds rebroadcast information in order to ensure that all nodes in the network receive the newest information. hence, cone can reduce redundant broadcasts in trickle by using a cds without impairing trickle's ability to keep information in a network up to date. algorithm 7 shows how trickle works in pseudo-code when it uses a cds."
"the tree growing algorithm iteratively adds nodes to the ds until it is connected. this algorithm colors nodes either in white, grey, or green in order to indicate the node's state. white nodes are non-dominators. grey nodes are also non-dominators, but they are adjacent to at least one dominator. green nodes are dominators. algorithm 1 shows the tree growing algorithm in pseudo-code."
"a node proceeds with the flooding process once an event timer for the cds construction has expired. in flooding, an initiator is a node that initiates network flooding by initially broadcasting a message. during the flooding process, the initiator node periodically broadcasts a packet in order to trigger network flooding. for the flooding process, we use the trickle algorithm [cit] . trickle uses polite gossip in order to distribute information through an entire network. polite gossip means that if a node receives new or old information, then the node rebroadcasts the received information. otherwise, the node does not broadcast the received information, because broadcasting the same information again is considered being aggressive. figure 10 provides an example of how trickle works. to determine whether information is old or new, packets in trickle contain a sequence number seqno_x."
"cdss are usually used as a backbone [cit] . as previously mentioned, many nodes in network flooding unnecessarily forward a packet which results in a broadcast storm. using a cds for network flooding can decrease broadcast storms by decreasing the number of forwarder nodes [cit] . furthermore, a node's radio transceiver must be turned off as often and as long as possible in order to achieve a long lifetime of the node [cit] . when a cds is used, non-dominators can turn off their radio transceiver for a longer time period in order to save energy."
"in (4), mape b is the average mape of the predicted value forecast by model b and mape a is the average mape of the predicted value forecast by model a."
neighbor discovery and degree exchange. mark sender's id as known 4: increase degree by 1 5: end if 6 : if received degree is greater than highest known degree then 7: consider sender as dominator 8: set highest known degree to sender's degree 9: else if received degree is equal to highest known degree then 10: if sender's id is lower than the one of current dominator then 11: consider sender as new dominator 12: end if 13 : end if
"over the past decade, wireless sensor networks (wsns) have begun to play a significant role as an enabling technology in a large number of applications, e.g., healthcare, industry, and agriculture, to name a few. a wsn typically contains a large number of sensor nodes. generally, a sensor node consists of a processing unit, a radio for wireless communication and one or more sensors. sensor nodes are usually used to monitor environmental conditions, especially in places where wiring is impractical. presently, wsns act as an important infrastructure in the popular internet of things (iot). the idea behind iot is to fully integrate tiny devices such as sensor nodes with ip-based networks, in order to realize new areas of wsn applications [cit] . the integration of wsns in iot enables users' immediate access to (environmental) data collected by the sensor nodes. this effectively increases the efficiency and the productivity of many processes [cit] . however, in many applications, sensor nodes are powered by a limited power source, and sometimes it is unfeasible to replace or recharge sensor nodes. hence, the network lifetime becomes a critical concern in the design of wsns [cit] . increasing the energy efficiency of wsns is an important topic in the research community [cit] ."
"however, power control cannot always prevent the transmission of redundant packets in dense networks in regard to network flooding. since in dense networks, nodes might be so close to each other that reducing the transmission power would not effectively reduce the number of active links. hence, if nodes transmit packets simultaneously, they still might interfere the transmissions of each other, leading to packet loss. therefore, power control cannot generally simplify a network's topology in order to reduce broadcast storm [cit] ."
"in step 3, the classifiers proposed in the study are constructed by two popular machine learning approaches; i.e., svm and rf. additionally, there are different schemes for developing the model selection framework, which involve a naïve classifier (abbreviated as msn), a classifier with forecast horizon features (abbreviated as msh), and a classifier with the reduced features (abbreviated as msh-fr). therefore, we have a total of five competing classifiers in this study; i.e., msn-svm, msn-rf, msh-svm, msh-rf, and msh-fr-rf. details of these classifiers (including the reason for excluding msh-fr-svm) are provided in section iii."
"for the evaluation of cone, we perform simulations for analyzing the behavior of cone. furthermore, we employ a real-world testbed flocklab [cit] for the evaluation. the evaluation concentrates on (1) packet loss because of ieee 802.15.4 medium access control (mac) contention, (2) radio duty cycle (rdc), and (3) energy consumption. these metrics very well indicate the energy efficiency and reliability of flooding protocols in wsns. besides, we compare cone to the baseline protocol trickle [cit] in order to address benefits brought by using a cds. note, we choose trickle as the baseline because it is one of the most classic algorithms for propagating and maintaining code updates in wsns and several ieee 802.15.4 standards are based on trickle, e.g., rpl [cit] ."
"in our previous work, 12 a trade study of a qkd system over a maritime channel using a weak coherent source was performed. here, a simulation is presented for a qkd system implementing the bb84 protocol using entangled photons. an eavesdropper is included in the simulation, with attacks that are limited to the intercept resend attack and access to all communications over the public channel. this simulation is then compared to analytical equations as well as to previously published results. then, using a theoretical upper bound and practical ldpc codes, the secure key rates of our simulation are compared to experimental results."
"in this section, we describe the design of cone. cone exploits the trickle algorithm [cit] for network flooding. trickle is a popular method for data dissemination in sensor networks [cit] . it has been standardized as the mechanism that regulates the transmission of the control messages used to create the network graph in rpl [cit] . constrained low-power and lossy networks (llns) which includes wsns represent the foundation for iot that deploys rpl."
"to verify the horizon feature performance, two different model selection frameworks were constructed: model selection naïve (msn) and model selection with horizons (msh). msn is the baseline of the model selection framework in this study. in msn, the time series features are the only input to the classifier. for each time series, the average forecast error (aerr) is calculated for each candidate model across four different forecast horizons; i.e., one-step, threesteps, six-steps, and twelve-steps-ahead. comparing the aerrs of all three candidate models, the one with the lowest forecast error is specified as the output label."
"in (3), n is the number of observations in the testing period, y i is the predicted value, and y i is the corresponding actual value."
"the system is also capable of simulating noise from dark counts or background photons. given the probability of a dark count, p dark, and probability of receiving a background photon, p back, the total probability that a single detector clicks due to noise is given by:"
"agricultural commodities are essential to people's daily lives. in recent years, the price fluctuations of agricultural commodities have become more severe and have exerted negative effects on society. for the consumer, an excessive increase in prices will impose a great burden on people's food expenditures, thus impacting their general welfare. for the agriculturalist, large price fluctuations will increase the uncertainty of production, thus adding to the number of risks that must be managed. consequently, an accurate prediction of the price of agricultural commodities is vital for agricultural authorities to make scientific decisions and to guarantee a favorable operation of the social economy."
"anns are data-driven flexible models which are capable of approximating a large class of nonlinear problems [cit] . one of the classic neural networks is the back-propagation neural network (bpnn), which includes feedforward and backpropagation. it is well known for its error learning algorithm in adjusting weights and bias. in general, a bpnn with a single hidden layer can generate the desired accuracy for a time series forecasting application [cit] ."
"one method of topology control is to save energy by reducing the transmission power of individual nodes. this method is referred to as \"power control\". power control reduces the number of active links, but it should also preserve connectivity and coverage of the network [cit] ."
"to the best of our knowledge, forecast models perform differently at each forecast horizon; hence horizon is an important factor in choosing the optimal forecast model. however, this factor is seldom considered in previous studies. moreover, the datasets used in previous studies were mainly m3, nn3, and nn5, which contain few agricultural time series. therefore, there is still a research gap in constructing a model selection framework for forecasting agricultural commodity prices."
"the first aspect to consider is the rate of both alice and bob's detectors recording a click. this happens in one of four ways; both alice and bob receive at least one photon that was transmitted, alice (bob) receives at least one photon that was transmitted while bob (alice) records a click due to noise, and both alice and bob record clicks due to noise. from these observations the probability that both alice and bob's detectors' recording a click is given by:"
"our design does not assume any knowledge of the network topology due to a possibly large network size and random deployment of nodes in an area. for this reason, our algorithm uses only broadcasting for communication. furthermore, cone uses event timers and callback timers to randomly schedule transmission in order to reduce duty cycles and packet loss of nodes during cds construction. an event timer sets a flag to true when it expires. if a callback timer expires, then it triggers a callback function."
the main contributions of this study are as follows. (a) we propose a model selection framework for forecasting agricultural commodity price time series based on time series features and forecast horizons. (b) we verify that the minimum redundancy and maximum relevance method can effectively reduce the redundancies between the features and is a workable approach to improving the performance of the classifier.
"in step 1, twenty-nine time series features are extracted, including complexity features, linearity features, and stationarity features. the optimal forecast model for the time series is specified by comparing the forecast errors of the three candidate models at each horizon. hence, both horizon information (horizon features) and the optimal model for the corresponding horizon will be recorded in the classification sample."
"svr is originally proposed by vapnik and based on the structured risk minimization principle [cit] . it performs nonlinear mappings through the application of kernels, which include nonlinear and linear kernels. it has been applied to forecast complex time series in industry [cit], agriculture [cit] and aviation [cit] ."
"however, network flooding suffers from the broadcast storm problem [cit], leading to low energy efficiency of the network. a broadcast storm occurs when a network is overwhelmed by continuous broadcast traffic. for instance, when different nodes are broadcasting data over a network link and the other nodes are rebroadcasting the data back to the network link in response, then this eventually leads to a network communication breakdown [cit] . correspondingly, the broadcast storm causes collisions, traffic overhead, waste of bandwidth, and so forth. therefore, it results in extremely energy-inefficient network flooding. as figure 1 shows, node 2 and its neighboring nodes suffer from infinite broadcasting traffic. (a) energy efficiency of network flooding can be enhanced by using a minimal connected dominating set (mcds). a connected dominating set (cds) is a connected subgraph of an initial graph that represents a network. each node that is not in the cds is adjacent to at least one node of the cds. a mcds is a cds with the minimal number of nodes. mcds-based approaches reduce communication overhead, radio duty cycles, and overall energy consumption of wsns [cit] . in figure 2, node 2 and 3 form a cds of the network. instead of letting all nodes rebroadcast information, only node 2 and 3 rebroadcast this information. nevertheless, every node which is not in the cds also eventually receives the information. (c) since node 3 is in the cds, it rebroadcasts the packet to node 2, 9, and 13. however, because node 8 and 9 are not in the cds, they do not rebroadcast the packet."
the backward search tries to exclude one redundant feature at a time from the end of feature rank and estimate the classification accuracy to evaluate the feature subset. the features with the highest classification accuracy are the optimal features.
"based on the performance of msh-rf, we employ a feature reduction strategy to further improve the performance of the classifier. it can be seen that the average forecast error of msh-fr-rf is 8.3330, which is the optimal score among all the classifiers. the reason may be that feature reduction is effective in removing the redundant features of a time series, and thus improves the performance of the classifier. it can be seen in figure 4 that features with high correlations are minimized after feature reduction, which again demonstrates the effectiveness of feature reduction. the average ir of msh-fr-rf is 3.9212, which is the best score of all the classifiers. moreover, the acc of msh-fr-rf is 61.85%, which is also the best score for classification accuracy. thus the effectiveness of feature reduction is fully verified in this study."
"nodes use maintenance_messages and is_connected_messages in order to handle those three cases. the state chart in figure 11 shows how cone maintains a constructed cds. when a node initializes cone, then it broadcasts a maintenance_messages to ask its neighbors whether a cds has been constructed. if a node receives a is_connected_messages, then this means that a cds has been constructed and, therefore, the node does not need to initialize the cds construction. instead, it enters the flooding process. furthermore, because the initiator broadcasts flooding packets in a known interval, other nodes in the network expect to receive flooding packets after a certain time. if this is not the case, then nodes broadcast a maintenance_messages to ask their neighbors whether the cds is still intact. nodes which receive a maintenance_messages reply with a is_connected_messages if the cds is still intact. algorithm 8 provides the pseudo-code for cds maintenance. if ds_is_connected is false then 3: proceed with cds construction 4: set timer for construction 5: wait until (ds_is_connected is true) or (timer for construction expires) 6 :"
"all the datasets used here were available on http://www.caaa. cn/market. the research sample includes 522 monthly agricultural commodity price series, covering the commodities of piglet, hog, beef, and so on, as listed in table 3 with the corresponding quantities."
"as future work, we would like to improve cone so that it can be used to construct a cds while causing less overhead and at the same time resulting in a smaller-sized cds. additionally, we are interested in applying cone on the top of concurrent transmission protocols, e.g., glossy [cit] and decot [cit] . then, we are curious to compare cone to the machine learning-based flooding protocol, e.g., lim [cit], which floods packets based on a superset of cds in the network, and to further evaluate the performance of both."
"the above expression comes from using the union of probabilities, where the 1 2 term is applied to applying basis reconciliation. the first term is the probability that alice and bob's bits agree given that the detectors detect photons that had been transmitted. more formally this is given as"
"ann, svr, and elm were used as the candidate forecast models in this study. a single hidden-layer neural network was applied by using the 'nnet' function of the 'nnet' package in r. this is an automatic function that uses a quasi-newton method for optimization purposes; specifically, the broyden-fletcher-goldfarb-shanno (bfgs) algorithm. the neural network parameters were tuned within a maximum of 300 iterations with twice time-series-averageperiod neural units in the hidden layer and a weight decay of 0.05. the optimal pair parameters of svr with rbf kernel were found within the range [10 −3, 10 +3 ] by means of a grid search using the 'tune.svm' function of the 'e1071' package. the elm was tuned within a number of units between 1 and 50 in the hidden layer using the 'elm_train' function of the 'elmnnrcpp' package. as for the candidate forecast models, each time series was divided into a fitted period (80%) and a forecast period (20%). the fitted period was used to fit the candidate forecast models whereas the forecast period was used to identify the optimal model from the range of candidates. each candidate model ran ten times in order to obtain an average performance."
"this study makes contributions to the literature by (1) proposing a model selection framework for agricultural commodity price prediction, by involving forecast horizon as one of the features; (2) using a workable feature reduction method to reduce the redundancy of features and improve the classification performance; and (3) discovering that different distributions of time series features may lead to different model selection results."
"the next step of the protocol is privacy amplifying any information that eve may have of the received key bits, which is dependent of the measured ber. as alice and bob have two detectors,('0' and '1') there are two possible ways to disagree. this implies that the ber is equal to half the probability that a bit is received in error. to calculate the ber however, the probability that alice and bob agree on the same bit will be considered."
"as for classification learners, random forest (rf) was employed using the 'randomforest' package. as for random forest, the number of trees is set to 1000 and the number of randomly seleted features is set to be one third of the total number of features available [cit] . in order to search the optimal cost and gamma for the svm classifier, a grid-search with the range [10 −3, 10 +3 ] and a ten-cross-validation were used in the training set. both rf and svm are trained with a five-fold cross-validation method and ran twenty times."
"in this study, we propose a model selection framework which involves both time series features and forecast horizons for forecasting agricultural commodity prices. within this framework, twenty-nine features are extracted according to the periodicity, nonlinearity, and complexity of agricultural commodity price time series. intelligent forecast models (i.e., ann, svr, and elm) are specified as the candidate models. the relationships between these features and the performances of the candidate models are learned by classifiers, which include rf and svm. feature reduction (the minimum redundancy and maximum relevance method) is also utilized to reduce feature redundancy and improve the forecast accuracy of the model selection framework. we test the effectiveness of considering the forecast horizon as the input feature and apply the feature reduction strategy to improve the performance of the classifier. finally, we use principal component analysis to analyze the relationship between different commodities and the corresponding optimal forecast models."
"as the channels going to alice and bob are independent of each other, the probability that the source sends l photons, and of them alice receives α photons and bob receives β photons is"
"if the distance from the root to any other node of the st is minimal, then the st can be regarded as a shortest path tree (spt). a similar structure is a minimum spanning tree (mst) which is a st with the minimal sum of weights. however, spts and msts do not reduce the number of nodes, but the number of edges of all nodes. because broadcasting still uses each edges of all nodes, flooding by spts and msts is not optimal [cit] ."
"in this study, two criteria were used for evaluating the prediction accuracy: mean absolute percent error (mape) and improvement ratio (ir). classification accuracy (acc) was used to estimate the classification performance. mape is a popular accuracy measure in the forecast community, the definition is shown as follows:"
statistical descriptions of all the features are listed in table 4 . these statistical values indicate that the features have different magnitudes; thus normalization should be employed before classification.
"optimizing trickle has become a popular research topic [1, [cit] . furthermore, simulations have shown that trickle suffers from the broadcast storm problem, especially in dense network. for these reasons, our goal is to enhance trickle by using a cds."
"svm can map nonlinear data into a higher dimension level via a kernel function in order to classify data more accurately. in its early days, svm was only used for binary classification tasks. [cit], however, [cit] employed a oneagainst-all strategy in svm for solving multiclass classification problems. since then, it is applied to a wide range of multiclass machine learning tasks [cit] ."
"in these two experiments, simple model averaging (sma) is considered as a benchmark to verify the effectiveness of reducing the risk of model selection. the predicted value of sma is the average of the predictions of three candidate forecast models."
"in this step, nodes create a ds by electing a particular node to become dominator. algorithm 4 gives the pseudo-code of the main process of a ds construction. besides, on the sender's side, the pseudo-code of the election process is shown in algorithm 5. nodes use an election_message which contains the identification (id) of the sender's dominator. while degree is lower than threshold and timer for degree exchange has not expired yet do 3: broadcast degree_message 4: set timer for degree_message 5: wait until timer for degree_message expires 6: end while 7: end if 8: if receive election_message then 9: set callback timer for election 10: pass election_callback as a parameter 11: proceed with ds connection 12 if (sender's id is lower than my id) or (sender's id is equal to my dominator's) then 4: stop timer for election_message 5: end if 6: else if elected dominator's id is equal to my id and i am not a dominator then 7: mark me as dominator 8: end if 9: election_callback 10: if timer for election has expired then 11: send election_message 12: set callback timer for election and pass election_callback as parameter 13: end if"
set timer for maintenance 8: end if 9: if (i'm not the initiator) and (ds_is_connected is true) and (timer for maintenance expired) then 10: set ds_is_connected to false 11: broadcast maintenance_message 12: set timer for maintenance 13: wait until (ds_is_connected is true) or (timer for maintenance expires) 14: end if 15: end while 16: if receive maintenance_message then 17: if ds_is_connected is true then 18: broadcast is_connected_message 19: end if 20: end if 21: if receive is_connected_message then 22: if (ds_is_connected is false) and (sender is not dominator) then 23: set sender as dominator 24: set callback timer for election 25: pass election_callback as parameter 26: end if 27 : end if 28: if receive trickle_message then 29: ds_is_connected is true 30: restart timer for broadcasting maintenance message 31: handle received message 32: end if
"the forecast performance of the model selection framework is subsequently evaluated by two criteria; i.e., the mean absolute percent error (mape) and the improvement ratio (ir). the classification performance is estimated by classification accuracy (acc). finally, principal component analysis is applied to analyze the relationship between commodities and the optimal forecast model. details of the analysis are provided in section iv."
"both rf and svm are employed in msn and msh as classifiers except for msh-fr. the reason of exclusion is that the training of msn and msh produced a much better performance from rf than svm; thus only rf was used as the classifier in msh-fr. this means that a total of five classifiers were constructed in this study: msn-rf, msh-rf, msh-fr-rf, msn-svm, and msh-svm."
1: initially color all nodes in white 2: while white nodes existing do 3: choose a grey or white node n with the largest number of white neighbors 4: color node n in green and the neighbors of node n in grey 5: end while 6: while dominating set is not connected do 7: choose a grey node m with the largest number of green neighbors 8: color node m in green 9: end while
1: receive trickle_message 2: if received sequence number is equal to my sequence number then 3: stop rebroadcasting 4: else if received sequence number is lower than my sequence number then 5: if i am dominator then 6: broadcast information 7: end if 8: else 9: set my sequence number to received sequence number 10: store the received information 11: if i am dominator then 12: set timer for broadcasting information 13: end if 14: end if
"the efficiency of the paths from the source to alice and bob are defined as η a and η b, where η is the product of the efficiencies of all optical elements in that path. a photon passing through the channel can be modeled by a binomial distribution. from this the probability of getting m photons out of the channel given that l photons entered the channel is:"
"in section 2, we review several existing approaches for controlling a network's topology. also, we clarify the reasons why we choose a cds-based approach in cone. in section 3, we describe the design of cone. for this purpose, we detail the cds construction and maintenance and then describe the portability of the cds construction algorithm of cone to other flooding protocols. in section 4, we compare the performance of cone and the baseline protocol [cit] based on experiments in cooja [cit] and in flocklab [cit] . in section 5, we summarize our work and provide a glimpse into future work."
"a similar approach to cds is called \"clustering\". clustering partitions a network into clusters. a cluster contains several nodes. one particular node in each cluster has the task to manage its cluster-it is called a \"cluster head\". figure 6 illustrates a network that uses clustering. if a node separates two cluster heads, it can assist in the communication between these two cluster heads. therefore, the assisting node is referred to as \"gateway\". clusters might overlap (see figure 6a ), but non-overlapping clusters (see figure 6b ) are also possible. it can also be the case that two nodes separate a cluster head from the next cluster head. in this case, these two nodes form a so-called \"distributed gateway\" (see figure 6c ). two cluster heads can be neighbors, but it is often desirable that cluster heads are separated [cit] . therefore, the set of cluster heads ideally forms an is. however, in terms of reducing the set of nodes, it is more useful to keep the number of cluster heads small. hence, the set of cluster heads should not form a mis, since this results in a larger number of cluster heads. therefore, the cluster configuration without the mis is more beneficial for reducing the set of nodes. the advantages of clustering are similar to the ones of using a backbone. in fact, cdss are sometimes used in order to find the set of cluster heads and the set of gateways [cit] . however, clustering puts more emphasis on enhancing scalability of higher-layer protocols and local resource allocation [cit] . hence, using clustering in network flooding does not provide any added value compared to using a cds."
"in this paper, we proposed a model selection framework for forecasting agricultural commodity prices using both time series features and forecast horizons. generally, three main steps were involved in the proposed model selection framework, i.e. feature extraction, feature reduction and classification. firstly, we extracted twenty-nine time series features of agricultural commodity prices. secondly, we used the minimum redundancy and maximum relevance method to reduce feature redundancy and improve the performance of the model selection framework. finally, five classifiers were constructed to verify the performances of different model selection strategies. additionally, the relation between different commodities and the optimal model was evaluated by principal component analysis. relative to existing studies, this study verifies the effectiveness of the model selection framework in choosing the most suitable forecasting models. with agricultural commodity price series as research samples, several interesting conclusions can be made based on the empirical results. firstly, considering the forecast horizon as one of the features can improve the performance of both classification and forecast, which demonstrates the forecast horizon should be considered as an important factor in model selection task. secondly, mrmr can further improve the performance of the model selection framework, which indicates a workable feature reduction method should be exploited in model selection for increasing the generalization capability of classifiers. thirdly, different distributions of time series features may lead to different optimal forecast models. it verifies the necessity of model selection based on time series features."
we evaluate the performance of cone and compare it to the state-of-the-art in terms of end-to-end reliability and duty cycle in the simulator cooja [cit] and energy consumption in the testbed flocklab [cit] .
"examining equation 18, the factor of 1 4 comes from selecting the correct basis and detector. in the first four cases, the cases are similar to those described above when no noise is present, with the exception that alice and bob both receive photons transmitted, and either alice or bob have a detector click due to noise. in the last two cases, we assume that only one of the users has a detector click due to the transmitted photons, and the other user's detector clicks due to noise."
"in the past 30 years, the model selection approach has been used extensively for choosing the optimal model for various types of input data. that is to say, the underlying relationships between the features of the input data and the performance of a candidate algorithm will be discovered by learners through numerous training samples. once there are some new data, the optimal model will be selected automatically by the trained learners based on the features of the new data. although training for the learner takes much computational time, the pay-off could be a significant gain in being able to choose the optimal model for a new series more quickly. therefore, we propose to use the model selection method to select the optimal forecast model for a time series automatically."
"in step 2, feature reduction is performed using an mrmr approach, with the aim of reducing feature redundancy and improving the generalization capability of the classifier. the ranking of the mutual information (mi) values of all the features will be obtained by the mrmr algorithm, and the ultimate features selected will be generated by the backward search method."
"however, finding a mcds is a non-deterministic polynomial-time (np)-hard problem [cit] . there has been a lot of work done on finding approximation algorithms to solve this np-hard problem [cit] . in this article, we propose the connected dominating set-based flooding protocol (called \"cone\"), to improve the energy efficiency of network flooding. cone exploits an approximation algorithm to construct a cds and subsequently uses this cds to boost the flooding procedure. as a side effect, cone significantly reduces the problem of broadcast storms."
"comparing the three single forecast models (ann, svr, and elm), it can be seen from table 6 and table 7 that ann is the most powerful as it has the smallest average forecast error. consequently, ann is specified as the optimal single model in our study. therefore, in order to verify the effectiveness of the model selection framework, the performances of the forecast models selected by different classifiers will be compared with that of ann."
"parameters can make effect on the model performance. in this study, the artificial intelligent models are executed within a certain parameter range, which is shown as followed."
"if a node initializes the flooding process by broadcasting a packet, then not all nodes in the network are able to receive the packet. the resulting ds from the previous step is not necessarily connected. for instance, the green nodes in figure 8b are not connected to each other via other nodes. to connect the ds, we have to choose additional nodes to become dominators."
"our experiments in flocklab use 22 nodes, since we observed that only 22 of the 25 nodes were available during all experiments. each test runs for ten minutes. during these ten minutes, node 1 (see figure 16 )-as an initiator-broadcasts a packet every 15 seconds in order to flood the messages through the whole network. during the first 90 seconds, cone uses the broadcasts to construct a cds. after the first 90 seconds, cone halts the cds construction and starts the data dissemination by flooding. after ten minutes, we measure the average current consumption of all nodes."
"msh is a model selection framework which contains the features of different forecast horizons. for each time series, the optimal model is specified by comparing the forecast errors of the candidate models at each horizon, as shown in figure 3(b) . that is to say, we have a total of four optimal models according to four different forecast horizons. therefore, the features of both the time series and the forecast horizons are used as inputs to the classifier, whereas the optimal model of the corresponding horizon is regarded as the output label."
"the proposed model selection framework could be improved from the following perspectives. first, the proposed method could be employed as an effective model selection tool for other forecast objects. second, some powerful classifiers such as adaboost and bayesian networks could be utilized to further improve the classification capability. third, this study only considers three popular forecast models in the area of forecasting agricultural commodity prices; however, other techniques could also be introduced to make the framework more workable."
"as figure 16 shows, in our experiment in flocklab, cone constructs a cds with the green node 13, 15, 25, and 28 as dominators. therefore, these nodes are many enough to cover the whole network, thus, continue to forward packets during the flooding process. based on the connections between nodes, we can also see that a packet can flow from initiator 1 through dominators 15, 28, and 13 to dominator 25. furthermore, every non-dominator is in transmission range of at least one dominator. this means that if a packet is forwarded by the dominators, then every other node in the network can receive that packet without retransmitting it. thus, while in a flooding process, the dominators (in a cds) receive packets from the initiator and forward the packets to all other nodes in the network. the non-dominators save a lot of energy during the flooding process by having their radio turned off for a longer time. besides, this is sufficient to ensure that every node in the network is able to receive the packets from the initiator. figure 17 shows the energy consumption of the network in our flocklab experiment after ten minutes. in cone, the energy consumption of node 28, 25, and 15 is higher compared to the trickle protocol. this could be expected due to the fact that these nodes are dominators and, therefore, have to forward packets so that the rest of the network can receive these packets. however, in cone the energy consumption of all non-dominators is lower compared to trickle. this results in an overall 15% lower average energy consumption for the network in cone compared to trickle. because the radio transceiver of nodes consumes a significant amount of power when turned on, the non-dominators save energy by not retransmitting packets. with these results, we conclude that cone decreases the average energy consumption in a network. however, the forwarding of packets during the flooding process is not distributed among all nodes in the network, but only among dominators. this means that dominators in cone may have to forward packets more often compared to individual nodes in trickle. thus, cone does not necessarily decrease the energy consumption of individual dominators."
"after feature reduction, twenty-five features including twenty-one time series features and four horizon features remained. in general, the average mi of each pair of two features has been reduced by 7.45%. the details of the selected features are listed in table 5 . four horizon features have been retained, which demonstrates that the forecast horizon features are important for the performance of the classifier."
"we simulate cone and trickle by using the cooja network simulator [cit] in contiki os [cit] . cooja is often used in the wsn community for debugging and performance evaluation of wsn projects [cit] . to see how cone affects rdcs and packet loss, we compare the average rdc and average packet loss of all nodes in our simulations. furthermore, we compare the largest measured rdc of nodes in cone and in trickle, respectively."
"the correlation diagram based on mutual information(mi) is shown in figure 4 . the dark point at the top right-hand corner represents the maximum mi value of all the twentynine features. the greater the correlation, the deeper the color. figure 4 (a) shows the correlation among features before feature reduction. it can be seen that most of the correlations are light colored, which reveals that these features contain diverse information on the time series. a few points are dark colored, which implies that the information contained in these features is redundant. these redundancies may have negative effects on the generalization performance of the classifiers. thus feature reduction should be employed to eliminate the redundancies. figure 4 (b) shows the correlation of the time series features after feature reduction. compared to figure 4(a), the numbers of the dark colored points have been reduced. this result shows that mrmr approach is a workable approach to feature reduction."
"let a path be a totally ordered set that consists of several nodes. we regard one of these nodes as the beginning of the path and another node as the end of the path. a network can use a spanning tree (st) to find an efficient path by reducing the number of edges in the graph. the path found could then be used to forward a packet to its destination. a st is a connected subgraph of an initial network which contains all nodes (vertices) of the network (see figure 4) . a st contains no circles (loops). many st construction algorithms require that edges have non-negative weights. some algorithms allow negative weights or do not require any weights at all. the weight of an edge represents the cost of transmitting a packet from one node to another node via this edge representing a communication link. the sum of weights of a path is regarded as the distance. some st algorithms also require a \"root\" which marks the beginning of the tree."
"since two mutually unbiased basis are used, there is a probability of 1 2 that a timeslot is used after basis reconciliation. thus the probability that a bit is recorded and used in the key is:"
"in the current study, model selection is a classification problem with the goal of selecting the candidate forecast model with the lowest mape on the test set of a given time series. two widely used classifiers are employed: random forest (rf) and support vector machine (svm)."
"in order to verify this assumption, we perform a principal component analysis (pca), following the method proposed by kang [cit] . the first two principal components of the bean and pig grain price time series are plotted into a feature space as shown in figure 6 . the x-axis refers to the first principal component and the y-axis refers to the second principal component. the red points represent the bean price time series which take elm as the optimal model across all the forecast horizons. the blue points represent the pig grain price time series which identifies svr as the optimal model across all the forecast horizons. it can be seen that the zone of red points is separated from the zoo of blue points. this phenomenon indicates that the features of those two categories are quite different from each other. therefore, different distributions of the time series features can be regarded as the main reason for the different model selection results."
"assuming the system is ideal, with the exception that eve is performing the intercept resend attack 10% of the time, seen in figure 4 . as eve is measuring the photon and then retransmitting it, the key generation rate is not affected, however the ber is affected, since 10% of the photons that are recorded have been measured by eve and are no longer entangled. a disentangled photon has an equal probability of being correct given it enters the correct basis, leading to a minimum ber of 5%."
"the model selection experiments for forecasting agricultural commodity prices were conducted using the research design described above. accordingly, the forecast performances of all the candidate models and the model selection frameworks were evaluated using the two accuracy mea- sures mape and ir, and the classification performance was estimated using acc. table 6 and table 7 show the forecast performances in terms of mape. the last column labeled ''average'' shows the average performances of the models across all four forecast horizons. in order to illustrate intuitively the advantage of the model selection framework, we compare the performance of each selection framework to the optimal single model ann. the results are shown in table 8 . table 9 shows the classification performances of the three model selection frameworks in terms of acc."
"due to the complexity and nonlinearity features of an agricultural commodity price time series, three workable and widely used ai models in agricultural commodity price forecasting are considered as the forecast models in this paper: artificial neural network (ann); support vector regression (svr); and extreme learning machine (elm). the details are as follows."
"the remainder of this paper is structured as follows. the experimental framework, including feature extraction, feature selection, time series forecasting, and classification are presented in section ii. section iii describes the data, experimental design, parameter settings, and evaluation criteria. the experiment results are analyzed in section iv, and section v concludes."
"in (1) and (2), x and y are two variables, s is the selected feature subset, m is the number of selected features, and i (x; y) is the mutual information between x and y."
"the need to generate a secure key between two parties at a distance has increased as communication systems transmit faster and sned larger chunks of data. one solution to generating this secret key is quantum key distribution (qkd), [cit] . 1 ekert then presented the e91 protocol, 2 [cit], and has become known as the bb92 protocol. 3 current qkd systems are implemented using weak coherent sources [cit] and entangled photon sources."
"in this section, we evaluate cone based on results from simulations and from a real-world testbed. for this purpose, we compare the performance of cone and our baseline protocol trickle. note, that we choose trickle as the baseline because it is one of the most classic algorithms for propagating and maintaining code updates in wsns and several ieee 802.15.4 standards are based on trickle, e.g., rpl [cit] . however, cone is flexible to adapt to other flooding protocols in contiki. the difference between both protocols is that cone additionally uses a cds together with trickle for network flooding. we use a tmote sky sensor platform in our simulations and testbed experiments, respectively. we first present our simulation results followed by our testbed results."
"we implement cone in contiki os based on tmote sky sensor nodes. contiki os is an operating system for nodes which are used for the iot [cit] . we use the rime communication stack for network communication. rime is a lightweight communication stack which has been designed for wsns [cit] . furthermore, the implementation of trickle in contiki os employs rime for communication as well."
"1: initially color all nodes in white 2: choose a node m with the largest number of white neighbors 3: color node m in green and the white neighbors of node m in grey 4: while white nodes exist do 5: choose a grey node n with the largest number of white neighbors 6: color node n in green and the white neighbors of node n in grey 7: end while when all nodes are colored in green or grey, the algorithm has successfully constructed a cds, represented by the set of green nodes. figure 7 provides an example of the greedy tree growing algorithm. the advantage of this algorithm is that it keeps the set of dominators connected until a cds has been constructed. the disadvantage of the algorithm is that choosing only grey nodes to become green nodes in line 5 of algorithm 1 results in a larger cds for some topologies. choosing both white and grey nodes to become green nodes would result in a smaller cds in less rounds, as shown in figure 8 ."
"two experiments were conducted to verify the effectiveness of the proposed method within the overall research framework. the purpose of the experiment i was to test whether the forecast horizon feature can improve the performance of the classifier. based on the results of experiment i, experiment ii investigated further the feature reduction performance, which aims to reduce the redundancies between the features."
"to use a cds, cone has to construct a cds in the first place. we describe the cds construction in cone in the following sections. in section 3.1, we explain how cone uses a cds for the flooding process with trickle. in section 3.2, we detail the neighbor discover and information exchange in cone. furthermore, we have to consider node failures [cit], since they can easily disconnect a cds. in section 3.3, we describe how cone maintains a constructed cds in case of node failures."
"the goal of this work was to improve the energy efficiency of wsn network flooding by exploiting a cds on top of the flooding protocol. in this article, we presented the design and implementation of our cds-based flooding protocol cone. cone constructs a cds with only slight information of a network's topology. besides, we compared cone with the baseline protocol trickle, both in simulations and in a real-world testbed, in term of rdc, packet loss, and energy consumption. the results showed that cone successfully decreases the number of lost packages for all nodes in the simulations. testbed results demonstrated that cone decreases the average energy consumption of a network during network flooding. however, cone does not necessarily decrease energy consumption of dominator nodes."
"an independent set (is) contains nodes such that no pair of nodes within the set are adjacent [cit] . if increasing the cardinality of an is breaks the independence property, then the is is regarded as a maximal independent set (mis) (see figure 3) . if a node of a graph is not in the graph's mis, it is adjacent to a node of the graph's mis. hence, every mis is also a dominating set (ds) [cit] . we discuss dss further in section 2.2.3. since miss are not connected, a network cannot use a mis for propagating packets through a whole network like a cds. however, a few cds construction algorithms use a mis in order to find a cds. we discuss this further in section 2.4."
"a ds is a subgraph of a graph g that represents a network (see figure 5) . each node of g is either in the ds or adjacent to a node of the ds. we refer to a node that is in the ds as a \"dominator\". accordingly, we refer to a node that is not in the ds as a \"non-dominator\". a ds is connected and, therefore, a cds if there is a path from each dominator to each other dominator and this path includes only dominators. as mentioned in section 1, a mcds contains the minimal number of dominators that are required to cover the whole network. finding a mcds is referred to as the \"mcds problem\" [cit] ."
"in order to determine the relationships between the selected forecast model and various agricultural commodities, we plot a facet wrap as figure 5 shows. the 1,2,3 and 4 of the x-axis refer to the four different forecast horizons, i. e. onestep, three-steps, six-steps and twelve-steps-ahead. the value of the y-axis indicates the number of times that the forecast model is selected. green, red and blue bars represent the three candidate forecast models (ann, svr and elm). it can be seen that the optimal model for a certain category varies for different forecast horizons, and the superior model is also different for different categories."
"as for different categories, figure 5 shows that the red bar is prominent for the bean facet wrap while the blue bar is prominent for the pig grain facet wrap. that is to say, elm is the optimal model for most of the bean price time series wherea svr is the optimal model for most of the pig grain price time series. the reason might be that these two categories have different features which lead to the different model selection results."
"as the system implements the bb84 protocol, the probability that alice (bob) records a click caused by noise on any of its four detectors, p an,4 (p bn,4 ), is calculated using a binomial distribution, using p n oise for alice's (bob's) detection apparatus,"
"another method of topology control is that a network uses a so-called \"backbone\". a backbone consists of nodes that perform data communication tasks and serve network nodes that are not part of the backbone. in the following sections, we describe a few methods of backbone construction."
"algorithm 6 provides the pseudo-code of how nodes use their token_message in order to connect the ds. each node periodically broadcasts a token_message. a token_message contains the sender's token and a flag which is set if the sender is a dominator. nodes update their token if they receive a greater token from a dominator. however, a node does not accept a token_message that was sent by a non-dominator. figure 9 shows how nodes can use their token_message in order to connect their ds. let the red nodes have a lower token_value than the green nodes. the red nodes will not accept the largest token_value, because there is no dominator that broadcasts the token_message to them. however, the grey nodes are in transmission range of the dominator with the largest token_value. hence, the red nodes receive the higher token_value from the grey nodes. furthermore, selecting one of the grey nodes results in the connection of the ds. to select a grey node, each red node increases its local counter by one if it receives a higher token_message from the grey nodes. if the counter reaches a predefined threshold, the node resets it to zero and selects one of the grey nodes. also, the red nodes record the grey node's id which has the lowest id in their neighborhood to increase the possibility that both red nodes select only one grey node."
"the model selection framework used in this study is shown in figure 2 . first, the features of the time series are used as the inputs to the classifier, and the optimal model is treated as the label of the classifier. both input and output make up the samples for classification. second, all of the samples are divided into a training set and test set. the classifier is then trained by the training set. finally, the test set is used as the input to the classifier, and the trained classifier will determine the selected model."
"an enhanced variation of the tree growing algorithm iteratively adds grey and white nodes to the ds. like the greedy tree growing algorithm, this algorithm colors node either in white, grey, or green in order to express the node's state. white nodes are non-dominators. grey nodes are also non-dominators, but are adjacent to at least one dominator. green node are dominators. algorithm 2 shows the enhanced tree growing algorithm in pseudo-code. when all nodes are colored in green or grey, the algorithm has successfully constructed a cds. however, centralized cds construction algorithms are usually not directly applicable to wsns due to the absence of a central administration and possible large network size [cit] . for this reason, we design and implement a fully distributed tree growing algorithm, which is based on the enhanced variation of the tree growing algorithm. we discuss the design of our algorithm in the next section."
"when using a spdc source, it can be controlled by a third party at any location in the channel, 10 as seen in figure 1 . in our system, it is assumed that eve is only located in the channel going to bob. the spdc source generates pairs of entangled photon, along two different paths. the first photon of the pair is transmitted to alice, through a lossy channel and then through the detection apparatus. the second photon of the pair is transmitted to bob and eve, first transitioning through a channel to eve, then on to bob, and finally through the detection apparatus. eve will perform the intercept resend attack on each photon that reaches her, with a probability of p e.i.r . the detection apparatus is designed to implement the bb84 protocol, using a beam splitter to randomly sort photons into two mutually unbiased basis (hv or ad), then using a polarizing beam splitter to sort into orthogonal polarizations, symbolizing the binary data. if a detection occurs in both the hv and ad basis or in the orthogonal polarizations, the system will randomly select the basis or polarization. b) the detection apparatus used to implement the bb84 protocol, consisting of two mutual unbiased basis, each containing two orthogonal polarizations"
"however, network flooding still suffers from broadcast storms which causes a large number of redundant messages and packet loss. topology control aims to increase energy efficiency of network flooding and to reduce broadcast storms [cit] . in this section, we review several methods of topology control. topology control restricts the topology of a network with the goal to increase the network's lifetime. the topology of a network is defined by its set of active nodes and active links between any two nodes. the optimal solutions to topology control methods are unfortunately np-hard to find [cit] . hence, we also review approximation algorithms."
"to evaluate the feature reduction performance, a model selection framework with horizon features and feature reduction (abbreviated as msh-fr) was developed as a competing model for msh. the main difference between msh-fr and msh is that the input features of msh-fr have been reduced, whereas msh uses the original time series and horizon features."
"we designed cone such that a network can maintain a constructed cds. cone can cover the following cases: (1) new node joins the network after cds construction is done, (2) a dominator leaves the network because of node failure, and (3) a non-dominator loses connection to all dominators."
"to calculate the secret key rate (skr), the security proof given by koashi and preskill, 10, 15 shown in equation 7. the bit error rate (ber), δ is assumed to be equal to the phase error rates, f (δ) is the reconciliation efficiency of the error correction protocol, and h 2 (δ) is the binary entropy function."
where the factor of 1 4 comes from the probability of choosing the correct detector in a basis and the probability of choosing the correct basis.
"some of the advantages to using an entangled photon source are: lower information leakage from multiple-pair photon signals, 9 and channels with higher losses are usable; 10 however, the generation rate of photon pairs is much lower than using a weak coherent source. to generate entangled photons, a spontaneous-parametric-downconversion (spdc) source is used, where a pump laser interacts with a non-linear medium, allowing for entangled photons to be generated. the spdc source can be a type i or type ii, where polarization of the photons in a pair are parallel or orthogonal. these sources can be made by using β-barium borate (bbo) or periodically poled linbo 3 (ppln) nonlinear crystals. a ppln crystal has a much higher coupling efficiency than a bbo crystal."
this defines the way the context aware middleware architecture is constructed. it is the most elementary factor to be initially considered. four classical architectural fashions can come in handy to organize and arrange the inner composition of middleware.
"many context aware middleware architectures [cit] have been proposed to proactively provide adaptive behaviour to the continually changing environment in recent years. thus it is valuable to review these recent publications and attempt to derive the development trend(s). however, a literature review on context aware middleware architectures reveals two major shortcomings: one is the lack of published surveys on this topic and their missing extension & comprehensiveness since many of them are outdated and do not include newer context aware middleware proposals. the other shortcoming is the big overlap between those published surveys as they repeatedly investigate the same knowledge base (several well-known context aware middlewares). due to this background, this paper provides an extensive survey on context aware middleware architectures by means of presentation, comparison and evaluation of newly proposed context aware middleware architectures that have not been mentioned in the review published literature. in this way, readers could refresh their knowledge and stay up on the latest context aware middleware developments. our aim is to present the current status of context aware middleware, point out the challenges behind as well as potential work in order to help researchers and developers to choose a desired middleware for their own use or probably design a brand-new solution inspired by existing proposals."
"the term context awareness is often referred to be sentient, reactive, context-sensitive, environment-oriented, situated, responsive and adaptive, it firstly appeared in the active badge research project of olivetti research ltd. (cambridge, uk) [cit] . from then on, many researchers had interests in generalizing this term and discussing its definition, e.g.,"
"semantics are incorporated into the reasoning procedure [cit] . based on descriptive logic, it is supported mainly by two semantic web languages: rdf and owl. however, it is unable to find missing values or accommodate ambiguous information but compensates with expressiveness."
"to conclude, the selection of context reasoning technique is subject to two factors: the performance and the requirements arising from the modelling technique used. it is shown that ontology-based reasoning is efficient to deduce high level context because of its predominance of knowledge sharing, logic inference and knowledge reuse. to design a specific application, the selection of the appropriate modelling and reasoning technique should be made carefully taking as many criteria and requirements as possible into account."
"definition 4. (the kullback-leibler divergence) if f and g are probability measures over a set s, then the kullback-leibler divergence between f and g is:"
the network is anonymous to alice and bob such that they do not know the connections between input and output links. alice has access to the input links and is able to buffer the packets and release them when she desires to embed fingerprints in packet timings of the flows. adversary willie is between
"context can have a lot of alternatives [cit] . different context can reflect similar or even the same changing aspects of a current state so that a big range of flexibility is available for users to choose the most suitable context to use. for example, both address name and latitude/longitude coordinates can record the location of a person. however, the feature results in an extra concern: context redundancy. the increasing context redundancy will pave the way to improve the efficiency of computing capability, extend storage capacity and refine context filtering algorithms, etc."
"fault tolerance means the adaptive capability to respond to unexpected failures. different middleware may have different reactions to failures. middleware with high-level fault tolerance can keep the intended operations running and get rid of the influences that failures bring or just be affected in an acceptable degree. inversely, for some middleware, a minor failure may lead to a sudden stop."
"none of those seven presented types of classification can be called the best or the worst, as each is suitable and reasonable in a certain situation. hence, depending on certain situation, environment and purpose, a specific classification scheme would be used to handle the context from the desired perspective."
"context can be acquired from diverse sources. generally, obtaining context can be categorized into hardware and virtual sources. from the perspective of hardware, context can be monitored and collected by a variety of sensing devices. the majority of physical context is obtained through sensors or sensor networks. the present trend for hardware development is to become smaller and cheaper from an economic perspective. in the meanwhile, this leads to the restriction of computing capability and storage capacity. moreover, battery capacity should also be increased to support longer work life. in addition, context can be either provided manually or derived from virtual sources such as context agents, context servers, middleware modules, big data, etc."
"context is responsive to mobile objects (or persons). users move around and change from one environment to another which probably is an unfamiliar one. context data is then collected from the mobile persons and reflects their changes. in this manner, a system is able to accommodate its behavior to each environment. in other words, context is obtained from mobile objects as well as used to serve them in the end."
"the remainder of the paper is organized as follows. in section ii, we present the system model, definitions and the metrics employed for the two scenarios of interest. then, we provide constructions and analyses for the two fingerprinting scenarios in sections iii and sections iv. in section v we discuss the results, the extension of the scenarios to distinct flow rates, and the extension of the network model to more general networks. finally, we conclude in section vi."
"refined differentiation of obtainment complexity by sub-categorizations. ambiguity in identifying the complexity of obtainment, difficult to distinguish clear differences of subcategorizations navigation sensed: proximity to an object combined: speed and direction of motion inferred: check distance (rules) learned: compare with similar situations"
"kristian [cit] presented a survey of a few context aware middleware systems, such as aura [cit], carmen [cit], crisma [cit], cooltown [cit], corten [cit], gaia [cit], middlewhere [cit], mobipads [cit] and socam [cit] . a taxonomy of context aware middleware was developed on the basis of several factors such as environment, storage, reflection, quality, composition, migration and adaption. however, these studied middleware proposals are not new and cannot represent the current development status."
"in general, three typical approaches have been of much value to develop context aware applications [cit] : (1) each application interacts, obtains, processes and uses the context of its interest in its own manner; (2) some libraries/toolkits aiming at acquiring and processing context can be added and reused for building context aware applications; (3) applications are built on the basis of context aware middleware in which context management is enabled. the third approach outperforms the other two since it can decrease the complexity of developing context aware applications. hence, context aware middleware is highlighted as an essential requirement for building context aware applications. traditional middleware, acting as a software layer, is usually defined adhering to the metaphor of a black box which hides the heterogeneity of hardware and eases the development of upper applications. to distinguish from the traditional middleware, context aware middleware should provide fundamental context management, such as modelling context, processing context, etc. as a result, applying context aware middleware can free developers from the concern of managing the context and allow them to focus on designing desired application functions and business logic."
"although current proposals for context aware middleware contain different components or modules to manage context, they obey a general rule which is the context lifecycle. the life of context, which is the period of time from its obtainment to destruction, is demarcated by six significant events as context acquisition, context modelling, context reasoning, context distribution, context repository, and context visualization as illustrated in figure 1 . the way to realize context awareness begins with the acquisition of various kinds of context followed by the formalization and inference process, and finally ends up with the distribution of useful context to the corresponding applications. at the stage of context modelling and reasoning, historical context data needs to be recorded for further use or queries and also can be visualized by users. in the following, the major phases of the lifecycle are outlined in detail."
"flow watermarking and flow fingerprinting are two active traffic analysis methods that work by perturbing packet timings of flows according to specific patterns to embed information in them. in flow watermarking, the information embedded in a flow is one bit, i.e., either the flow is marked or not."
"indeed, the concept of context aware middleware/systems is not a buzzword emerging in very recent years. it can be traced back to the early 90s, when the first endeavours to develop context aware systems focused merely on exploiting location data, e.g., the active badge system [cit] and cricket compass [cit] . later on, context aware middleware architectures have evolved to achieve more generality and provide support for more types of context information. many off-the-shelf middleware platforms, including context toolkit [cit], gaia [cit], cobra [cit], socam [cit] just to name a few, can come in handy for developers to build personalized applications. however, they are less likely to be widely used in real implementations due to different limitations, e.g., the adoption of a key-value method in context toolkit considerably restricts this middleware to use in very simple applications. reference [cit] claims that early context aware middleware proposals tend to lack several key capabilities such as fault tolerance, semantic interoperability, distributed data computation, and precise reasoning. along with many technical breakthroughs, such as cloud computing, big data analytics, and artificial intelligence, in the information and communications technology (ict) area, the evolution of context aware middleware is also notable. those aforementioned technologies have favoured the current evolution of context aware middleware. it is thus significant to research the current status of the development of context aware middleware by means of examining the latest middleware proposals."
"consider a network containing m independent, parallel, work conserving, and first in first out (fifo) queues with independent exponential service times where the i th queue conveys the i th flow (f i ) from the i th input link to the i th output link, and conveys interfering flows independent of f i (see fig. 1 )."
"in this section we consider scenario 2. in a set containing m network input flows with equal rates, alice embeds fingerprints into each flow independently with probability p in the time interval and (7), respectively, as long as"
"extraordinary focus should be placed on ensuring security and privacy since a lot of sensitive and private data is used. hence, the middleware should contain security functionalities that can monitor and detect anomalies or unauthorized access to data. two basic mechanisms can be useful to perform security and privacy functions. on the one hand, context data could be encrypted or authenticated in both the transmission and storage process. on the other hand, access control needs to be clearly identified."
"many surveys on popular context modelling techniques have been published like e.g., [cit] . the description for each context modelling technique is quite detailed, however, those surveys are not complete, because some newly proposed modelling techniques are not included, as e.g., multidisciplinary, chemistry inspired etc. in the following, ten modelling techniques including \"key-value\", \"markup\", \"graphical\", \"object-oriented\", \"logic-based\", \"multidisciplinary\", \"domain-focused\", \"user-centric\", \"ontology-based\" and \"chemistry-inspired\" will be summarized to give an overview of the most common techniques. the fundamental scheme to examine the available context modelling techniques is based on the data structure used for representation."
"the storage of context is highly demanded as historical context is still meaningful for further use. based on the context trend, predictions for next actions can be made. the history of context is a good knowledge source for the reasoning process. taking into account the huge data volume, an appropriate storage container should be carefully chosen."
"however, in many applications, more than one bit of information is required to be embedded in the packet timings of the flows. flow fingerprinting provides the solution for such applications by embedding several bits of information in the flows such as the information about the party that has embedded the fingerprint, the source of the flow, or the location at which the flow has been fingerprinted [cit] ."
"however, from our point of view, some statements are too absolute and inaccurate to be extracted as sharing characteristics for context. besides, this analysis is not comprehensive enough to represent all aspects of context. so based on this survey, we refine and summarize the following five features for context."
"the middleware, as a software layer to abstract the heterogeneity of the lower layer (e.g., hardware) and ease the complexity of developing a higher layer (e.g., applications), is often proposed to be enhanced with the ability of context awareness. context aware middleware can be adaptive to the environment and provide relevant services according to the changing needs from the external side (e.g., users)."
"as the judging factor for scalability is intensively focused on the capability of handling context data, the scalability attribute of middleware architectures in this survey is considerably subject to the fact whether cloud-oriented techniques are employed. in this light, cloud-oriented techniques are an elegant solution to solve the scalability issue from different aspects. more specifically, in acoms+, a mining algorithm derived from cloud-based big data analytics is exploited to reconfigure existing resources to operate efficiently in small-scale and large-scale environments. other middleware proposals including fiware, ca4iot, cocamaal, and bdcam achieve multiple benefits from cloud. they extend data storage by means of cloud repository, enhance processing capability by means of cloud computing, and mine data by means of big data analytics. in addition to this, fiware expands its exploration on cloud-oriented techniques to a greater extent, such as visualization of data and invoking cloud-services from different providers."
"graphical diagrams enabled by this model are able to specify relationships. three widely used model examples are unified modelling language (uml), entity relationship model (erm) and object role model (orm). the uml is a standardized general-purpose language which expresses different kinds of context information in an own graphical notation, whereas erm and orm work for designing and querying databases at the conceptual level. however, the interoperability among different storage databases which are used in the actual low level of graphical model poses a challenge."
"secoman [cit], as the abbreviation of semantic web-based context management, is intended to provide a privacy-preserving solution for developing context-aware smart applications. in secoman, ontology is employed to model the description of entities, reason over data to obtain useful knowledge, and define context-aware policies. the whole architecture of secoman is shown in figure 4 ."
"the remaining part of this paper is organised as follows: section 2 provides the background of context aware middleware. herein, the principles of context, context awareness and their role in the middleware are introduced. familiar readers could skip this part. section 3 gives an extensive survey of context aware middleware architectures along with analysis and evaluation. section 4 gives an outlook of open issues and actual challenges. finally, conclusions are presented in section 5."
"the object-oriented model [cit] employs class hierarchies and relationships to represent context data and incorporates encapsulation, inheritance and reusability into context expression. instances can be allowed to access the context by inheritance mechanisms. the core component is called entity and it forms the subject of structured context information. an entity is linked to other entities by means of attributes which are also called associations. this technique stresses developers in terms of being aware of the whole context taxonomy."
"reasoning is also called inference. the demand for context reasoning derives from the fact that context data is imperfect and uncertain by nature [cit] . the task of context reasoning is to deduce high level context from raw context associated with some basic functionalities such as validating the context values, filling in missing values, removing outliers, checking context inconsistencies and applying some calculations to obtain new values. in the following subsections, a brief introduction to five popular reasoning techniques is given."
"context knowledge is deduced from previous similar cases in the case-based reasoning approach [cit] . however, it is difficult to accurately calculate the similarity of different cases."
it is also called notification. applications interested in certain context information can subscribe to the middleware and be notified when updates of the registered context information occur.
"campus provides an effective middleware solution for integrating context awareness to application development. campus could automatically derive context-aware adaptation decisions at run time by means of semantic-enhanced decision making. the initial implementation of campus is built in java se 1.6 along with additional plug-ins such as pellet 1.5.1 for description reasoning and jess 7.1p2 for logical reasoning. however, security has not been mentioned in this proposal and collaborative decision making among multiple campus middleware instances can be a future extension."
"a prototype based on cocamaal was developed in java. the implementation examines the performance of the proposed architecture, such as the influence of increasing context and service load on service response time. the results prove that cocamaal is efficient at collecting, abstracting, and using context from aal environments to provide context aware services. the major novelty is the adoption of cloud computing which provides powerful computing capabilities to process context. however, several concerning issues cannot be ignored. e.g., conflicts in context are not considered and reliability analysis is not accomplished. although [cit] states that the context aware role-based access control and privacy-preserving context service protocol can be adopted to ensure privacy in this middleware, those two mentioned approaches are not included in the test."
"physical context is obtained from sensing devices which are selected according to the requirements of a certain application or system. it is worth noting that simple sensors (temperature, humidity, etc.) and sensor networks are the most widely employed appliance for obtaining context from the surrounding, but with the collaboration of other more complex devices (e.g., snap2play [cit] and cach [cit] ), more types of context can be obtained."
"undoubtedly, ontology has dominated the landscape of modelling context data in all investigated proposals except camph, acoms+, octopus and flexrfid. extra features are enforced by the adoption of ontology such as high-level context abstraction, powerful reasoning, semantic interoperability, and (probably) advanced context awareness. in ontology-based context aware middleware architectures (e.g., fiware, ca4iot, campus, casf, secoman, cocamaal and bdcam), different ontologies are proposed and used to unambiguously formalize the knowledge base so that all entities within/upon the middleware could share the same understanding. however, it could raise a concern when different instances of different middleware want to cooperate and exchange information. for this reason, ontology mapping/alignment among various ontologies employed in different middleware should be discreetly addressed."
"an example of a personalized homecare application mashed from web services running this middleware was developed to demonstrate the working principles of camph. however, this prototype is still far from usable. the usability of camph should be examined in a larger scale field trial. besides, context data exchangeability/interoperability among different context domain is a concern since context data is structured in key-value model. how to manage the massive amount of context data from various spaces is still an unsolved issue. although it is declared that a hierarchical and comprehensive reasoning scheme will be deployed, explanation about detailed procedures is still missing. security and privacy are not considered during the entire conception/design."
"in the acquisition phase, a huge amount of context data which is structured in multiple formats is obtained. to make use of them, the premise is to define and store it in a machine readable and processable form, hereby all data should be converted into a unified format such that the context can be understood and shared. this can be achieved by a model that defines, represents and processes the object \"context\"."
"in the 2nd generation, more kinds of context information are employed to enable systems to know more about the environment. however, due to ambiguity, only a medium level of awareness is reached here. later, common semantic technologies (e.g., ontology) were adopted to unambiguously represent context structures and their relationships in the 3rd generation. here, context awareness can take advantage of semantic techniques to ensure formality, flexibility, interoperability and scalability. the four aforementioned categorizations for context awareness could widen users' vision to observe context aware systems from different aspects. however, those categorizations differ in their generality and rationality. e.g., it is difficult to attribute a context aware system to a specific level from the aspect of acquisition, because context is obtained from both hard and soft sources in most cases. similarly, it is blurry to identify the involvement of user interaction as personalized, passive or active. in general, the last classification is the most applicable proposal compared with the others. in section 3, this classification will be adopted as an important criterion to evaluate different context aware middleware proposals."
"keeping in mind that the architectural style can considerably affect other significant features such as flexibility and adaptability [cit], it is vital to select an appropriate architectural style to conceive the middleware."
"(reliability) similar to the reliability analysis of theorem 1, we can show that the probability that alice runs out of packets for each flow is upper bounded by ζ as long as"
"firstly, a preliminary background about the general approach of realizing context awareness has been presented to provide a starting point for readers to understand context aware middleware. herein, the principal terminologies of context, context awareness and their roles in context aware middleware architectures have been introduced. then, four different classifications of context awareness levels have been reviewed. after a thorough analysis, the categorization which divides context awareness into three levels: location aware, medium and semantic has been chosen as the most reasonable and appropriate criterion to evaluate context aware middleware architectures. an extensive and comprehensive study on existing context modelling and reasoning techniques has been carried out with basic explanations and comparison. the aforementioned study is able to give an insight into the strength and weakness of each modelling and reasoning technique which could help users select the most suitable one for their own use."
"e ) is lower bounded by 1 2 −, and thus the first phase is invisible. the analysis of the invisibility for the second phase is the same as that of scenario 1. thus, the fingerprinting scheme is invisible."
"a recently published proposal called flexrfid [cit] aims to provide a policy-based middleware solution for facilitating the development of context aware applications and integrating heterogeneous devices. ponder is adopted as the policy specification language in this middleware. the flexrfid middleware is a multi-layered middleware consisting of device abstraction layer (dal), which abstracts the interactive operations among the physical network devices), business event and data processing layer (bedpl), it provides context data management like aggregation, transformation and dissemination), business rule layer (brl, it manages policy-related operations), and application abstraction layer (aal, it enables communications among applications and the flexrfid). flexrfid is claimed to provide all data processing capabilities like filtering, grouping, dissemination and duplicate removal. in addition, it is an enabling solution to support simultaneous communication among different applications which are built in this middleware. notably, flexrfid differs from other context aware middleware in the capability of policy enforcement. a plethora of benefits can be achieved by defining different types of policies such as ensuring privacy, constraining access control, and offering customized services. two abstract types of policies are stated in flexrfid: system policies (manage the operations done by the middleware) and application policies (define the way users want the flexrfid services to be delivered)."
"in general, ontology-based context data visualization can be grouped into six types: indented list, node-link and tree, zoomable, space-filtering, focus + context, and 3d information [cit] . apart from those aforementioned ways, the use of web services and graphical interfaces to present context data is becoming more popular. different ways of visualization can be adopted according to the users' preferences. the trend of designing a satisfactory visualization for context should be in line with a more human, interactive, and instinctive manner. besides, it is worth noting that several properties should be guaranteed in a data visualization approach such as real-time, accuracy and contextual awareness."
"it is valuable to provide an overview of the introduced models. therefore, table 3 summarizes the most important features of every technique such as advantages and disadvantages. it is possible to apply this model to applications which require spontaneous interaction and composition of information."
"context abstraction explains how context is expressed and formalized. the increase of the abstraction level can improve the ability of reading, understanding and using context. besides, it can be beneficial to the reasoning procedure."
"a clear and accurate classification of context is helpful to uncover, understand, manipulate and sort out a variety of contexts in an efficient way. also, it can provide great help for users to identify the type of a given context before using it. as context could be categorized from different perspective, the list in table 1 summarizes a few well-known forms of context classification given in the current literature [cit] ."
"as previous active traffic analysis designs are based on ad hoc heuristics (such as moving packets into secret time intervals), they do not offer any theoretical guarantees on the invisibility-performance trade-off. in this work, we take a systematic approach to design a flow fingerprinting system with provable information-theoretic guarantees on invisibility and performance (e.g., number of fingerprints)."
"visualization of ontology-based context data is not an easy task because it means to enrich data with hierarchy, relationships, etc. hence, context visualization ways can be ontology-tailored visualization methods or adapted from other generally used techniques like graph or file system visualization."
"in this paper, we hold the opinion that context is any piece of information that can represent changes of the circumstance (either static or dynamic). further, it could be useful for understanding the current situation and predicting potential changes."
"we analyze the invisibility of the first and second phases separately. in the first phase, the joint pdfs of willie's observations under h 0 (alice did not embed fingerprints), and h 1 (alice embedded fingerprints) are:"
"virtual context refers to context which could not be sensed (e.g., knowledge, preferences, historical data, estimations, etc.) and has to be obtained in different ways. it could be either provided manually or derived from other context. users serve as an important source and provide a lot of useful personal information, such as birthday, age, preference, height, weight, etc. besides, already obtained context data can be used to infer other context meanings by employing certain reasoning rules."
"the core of this technique [cit], which belongs to supervised learning, is based on probabilistic reasoning concepts. entities and relationships among them are represented by directed acyclic graphs and probabilities. two drawbacks limit its usage: huge demands for exhaustive and exclusive hypotheses and exponential computational overhead."
"this is referred to as tagged encoding, as context information is stored within tags, i.e., symbols and annotations which represent and format the data. those symbols and annotations originate from typical markup languages such as xml. typical representatives of this model are profiles. the limit of this model is that its hierarchical structure should be pre-defined and also it is useless to capture context relationships."
"lots of benefits can be obtained by means of service discovery, such as discovering, combining and orchestrating services. repositories can be exploited to store corresponding service profiles including attributes, parameters and locations for further queries, the location of repositories can either be centralized or distributed. some service discovery protocols are available, e.g., the simple service discovery protocol (ssdp) [cit] or service discovery service (sds) [cit] ."
"as far as context awareness is concerned, different levels are reached by different middleware. since secoman is dedicated to provide services according to location changes, it is limited to location aware. the highest context awareness level is achieved by fiware, ca4iot, campus, casf, cocamaal and bdcam."
"it is possible that different kinds of context share several features to some extent. here, a clear and accurate extraction of context features is beneficial for users to achieve a better understanding of context characteristics so that the shortcomings of context can be minimized or even hidden as much as possible. van [cit] have identified eight features of context: context is obtained though individual sensors or networks (1); it is sensed by small and constrained devices (2); it originates from distributed sources (3); it is continuously changing (4); it comes from mobile objects (5); it has a temporal (6) and a spatial (7) character and it is imperfect and uncertain (8) ."
"in the csdl and cprl layers, context data related to sensors are represented in xml. in addition, users submit their requests for querying context using xml data format. in general, this middleware is quite complex and mature with detailed explanations for different functional component. many technical factors such as context abstraction, context process and context dissemination are carefully considered while a significant technical consideration which is security and privacy are missing in this middleware. compared with other middleware solutions, this middleware can act as either a standalone middleware or an auxiliary technique to be integrated with other framework solutions to fulfil the demands from different paradigms. however, only a simple use case is employed to show the specific procedures of developing the proposed middleware, and implementation of this middleware is still missing."
"the aim of context acquisition is to obtain a maximum amount of data, such that the possibilities for applications to be intelligent could be maximized due to richer context information. as stated in table 1, one possibility to classify context is the differentiation into physical and virtual, and the following introduction for context acquisition will be based on this classification."
"three main concepts, which are actors, resources, and applications, are virtualized so as to lay the basic foundation of semantics of this cognitive middleware. the semantic virtualization enablers are responsible for abstracting the heterogeneous actors, resources, and applications by means of attaching homogeneous, context aware and semantic aggregated metadata. on the basis of semantically abstracted metadata available through well-defined restful apis, cognitive enablers are capable of making decisions regarding the best solution of exploiting the available resources to efficiently satisfy application requirements and needs. to meet a specific application's needs, different cognitive enablers can be dynamically orchestrated. in the real deployment, all the functionalities of enablers can be distributed into different physical network entities."
"security in computer networks has emerged as an important area of research. although encryption hides information sent from a transmitter, network traffic analysis can extract important information from the size, count, and timings of the packets. for instance, when attackers relay their flows through compromised nodes called stepping stones, traffic analysis can trace back the attackers [cit] . also, traffic analysis can find the correlations in traffic patterns to link incoming/outgoing flows and break anonymity [cit] ."
"a holistic view on the latest literature shows that, to organize and evaluate existing context aware middleware, many surveys have been made, e.g., [32, [cit] . two representative ones are shortly outlined in the following."
"campus [cit], short for context-aware middleware for pervasive and ubiquitous service, is proposed to automate context aware adaptation decisions with the influence of three key technologies: compositional adaptation, ontology, and description logic/first-order logic reasoning. it has taken an enormous step to advocate automated run-time adaptation decisions instead of depending on predefined adaptation policies that only take limited contextual changes potentially operating in a dynamic situation."
"generally, a sensor is sensitive to only one special phenomenon and monitors some relevant change. then, it converts the change into data (normally electronic signals). in this way, context consumers are provided with the real time information of a particular property which cannot be obtained directly by observing or touching. however, for detecting all necessary aspects of a certain environment, more sensors are needed. a quite complete list of sensors can be found on the wikipedia webpage [cit] ."
"according to ibm data scientists, four characteristics (the 4 v's) can be used to define the concept of big data: volume, variety, velocity, and veracity [cit] . due to the nature of context data, context can fit into the definition of big data and be regarded as big data. context data is of massive volume and presents a big variety of types. besides, it changes continuously in terms of velocity and its veracity can also be satisfied since it can accurately represent the real changes of involving circumstance. therefore, \"big\" context data introduces new challenges but also brings new capabilities to context aware middleware architectures. the realization of context awareness can be supercharged by existing big data techniques such as massive parallel and in-memory databases, deep packet inspection technology etc. here, the outstanding technology that could considerably drive context aware middleware from holding \"big data\" to \"big wisdom\" is cloud-based big data analytics. it can enable context aware middleware to access needed computing resources from the cloud computing, foresee trends over big data and mine more useful information for context aware decision making."
"context consumers are able to actively make queries for their interested context information at any moment. depending on the used modelling and reasoning techniques, different query methods can be employed."
"where the last step is true since the result for scenario 2 indicates a much larger fingerprint dictionary can be generated and employed covertly than in scenario 1. note that (11) implies that in scenario 2, a small portion of the flows are fingerprinted. intuitively, because willie has to investigate a large number of flows to look for alterations in the timings of a relatively (very) small random subset of those flows, in particular in the first phase, this makes covertness much easier to achieve and leads to the significant gains observed."
"in order to be \"smart\", middleware architectures have first to be \"aware\". consequently, apart from a set of general-purposed capabilities, the ability of context awareness is introduced so as to upgrade traditional middleware architectures to context aware middleware architectures. in this light, it is necessary for context aware middleware architectures to provide relevant functionalities to process context concerning the lifecycle of context data. further, systems or applications built on the top of context aware middleware are able to provide services which automatically adapt to the changing environment."
"as revealed from table 4, layered, distributed or layered snd distributed have been popular architectural fashions for recent middleware proposals. the trend to organize middleware compositions tends to be more flexible and reliable while in the early stage the majority of middleware architectures such as cobra [cit] and moca [cit] adopted a centralized approach. structured in a layered or distributed manner, middleware architectures could outperform centralized middleware in terms of extensibility. it is more feasible for layered or distributed middleware to allow the build-up of additional features or capabilities with minimum effort (e.g., without major redesign, with minimum impact on existing components) in order to fulfil a very specialized purpose. in a layered middleware architecture, each layer uses the previous layer to implement new functionalities that will be exported to the layer above. in this way, distributed middleware architectures outperform layered ones in terms of reliability due to the fact that layered middleware architectures are prone to errors because of lower layers' failures. the layered & distributed combined manner allows developers to easily manage the software construction with intuitive conceptual understanding and functionality distribution in a hierarchical (layer by layer) and distributed (inside every layer) way."
"currently, in most proposed solutions, the functionalities of middleware are achieved by distributing the tasks in a layered/distributed architecture. there is a trend to leverage cloud resources to design an efficient and solid middleware. however, to \"shift\" middleware to the cloud will exert more stress on the security and privacy issue. in fact, as seen in table 5, only three middleware architectures take security and privacy into account by applying different strategies. e.g., fiware developed a set of key enablers aiming at guaranteeing security privacy and trust from two levels: generic and optional. in secoman, users' privacy is enforced by adding anonymity and hashing policies to hide and disguise the identity of a user. flexrfid offers basic support for security through the use of access control policies."
"domain-focused context modelling, also referred to as w4 context model, is tailored to model an application domain. therefore, [cit] elaborates the specific mechanism: a four fields tuple [who, what, where, when] (the elements are also called knowledge atoms) is recognized in \"someone or something (who) does/did some activity (what) in a certain place (where) at a specific time (when)\". this model is very expressive and flexible for data usages, and queries, modification and deletion are allowed on context tuples."
"key-value pairs are used to enumerate attributes and values in this model. the model can be written in different formats (e.g., text and binary). because of its simplicity and ease of use, it was widely employed in early research and various service frameworks. for example, schilit [cit] describes the limited number of location information as key-value pairs. however, it lacks capabilities for complex structuring for enabling efficient context retrieval algorithms."
"acoms+ [cit], as an enhancement of acoms [cit], offers a solution of resource efficient and context aware management of sensing infrastructure. the core of acoms+ is composed of a context source manager (models raw context information and performs actions on low-level communication), an application context subscription manager (allows applications to subscribe interested context by specifying quality of information and service), and a reconfiguration manager (reconfigures sensing devices to offer fault-tolerant provisioning of information). context modelling language (cml) is selected as the modelling method which leverages the graphical notations to represent context information."
"we denote by p fa the probability of rejecting h 0 when it is true (type i error or false alarm), and p md the probability of rejecting h 1 when it is true (type ii error or mis-detection). we assume that willie uses classical hypothesis testing with equal prior probabilities and seeks to minimize his probability of"
"the scope of context types and context data is not limited to a fixed number. on the contrary, it has a wide range and the scope is dynamic, depending on different situations. for instance, a set of information including brightness, temperature, humidity and heart rate can be collected as context in a home healthcare scenario. in addition, the same piece of context information could have different importance in different environments. due to the fact of enormous context data on behalf of reflecting changes, the raised concerns are computing burden and storage stress."
"it is known that the majority of context is attributed to the dynamic category. for dynamic context, context data values are only valid for a short period of time until they are replaced by new values. for example, only the real time location is significant for observing a moving person."
"the design of the semantic virtualization enablers and cognitive enablers allows applications to transparently, efficiently and flexibly employ available resources and become customer-tailored. with the collaboration of other enablers such as security enablers, cloud hosting enablers and iot enablers etc., the proposed fiware is upgraded to be a full-fledged middleware with context awareness, interoperability, cloud hosting, big data analytics, and service delivery and composition. what's more appealing for developers is that, so far, all the enablers developed in the fiware project are available as open-source implementations associated with detailed user manuals. however, it still will not be an easy task to move the pilot tests to real commercial usages. more potential constraints introduced by harsh environments such as underwater and off-shore should be taken into account when adapting fiware to actual operations."
"as demonstrated from table 5, all middleware architectures provide technical suggestions for storing historical context. having a deep look at storage means, it can be found that cloud-based middleware architectures (acoms+, fiware, ca4iot, cocamaal and bdcam) utilize distributed cloud repositories for storing context while other proposals keep context in a local knowledge base."
"before digging into the interior composition of context aware middleware, it is necessary to have a fundamental knowledge base regarding context and context awareness. to this end, an introduction to those preliminary concepts is provided in the following subsections."
context distribution is responsible for disseminating useful context information to corresponding applications. two typical distribution mechanisms (subscribe/publish and polling) are widely used in current solutions [cit] .
"the fiware [cit] fp7 project has an ambitious intention to strengthen the competitiveness of the eu economy by presenting a cutting-edge infrastructure in which creation and delivery of services, high qos and security are enabled. this platform is conceived to be considerably generic and could adaptively fit into various usage areas, e.g., safety, logistics, environment, energy, traffic and mobility, and agriculture. this platform is built based on public cloud with a rich library of modules offering various added-value functions (referred as services). these modules, regarded as generic enablers (ges), fulfil all the capabilities of different chapters of this architecture, such as service delivery, cloud hosting, internet of things, support services, developer tools, and interface to the network and devices. among all, it is worth stressing that enablers dedicated to manage context data from heterogeneous resources are playing an important role in the whole platform. these aforementioned enablers could be grouped into two categories: the semantic virtualization enablers and the cognitive enablers."
"in this reasoning method [cit], probabilities are assigned to the context data to make logic assertions. this allows sensor data fusion from two different resources. when conflicts occur, the probabilities can be helpful to make decisions. this technique can only be applied in a scenario with the premise of probability already known."
"it is feasible to analyze the performance of context aware middleware architectures from various points of view. the following list identifies nine of the most crucial technical attributes for evaluating context aware middleware architectures. in addition, these nine features can also act as the primary considerations at the initial design of context aware middleware architectures."
"context is the key knowledge source for systems to achieve context awareness. the oxford dictionary gives a general definition for context as \"the circumstances that form the setting for an event, statement, or idea and in terms of which it can be fully understood.\" nevertheless, many researchers try to define context in their own way, depending on the necessities and investigated environment, e.g., context is…"
"context visualization offers new ways of seeing data. there is a growing need for an effective visualization to provide a visual overview, explore, analyze, and present phenomena which are often difficult to understand or imagine. since ontology-based modelling is selected as the best modelling method in the context modelling phase, research on context visualization will be focused on reviewing current approaches to visualize ontology-based context data."
"due to the rapid development and penetration of ubiquitous computing and the internet of things (iot) into daily life, a boom of context data which continually represents changes of the environment is generated and can be available for further use [cit] . it is believed that the full utilization of this large volume of context data can introduce possibilities for many new applications. taking one possibility as an example, context aware applications have attracted a lot of attention from academia and industry. context aware applications are able to adapt their behaviours to the changing environment with a minimum of human intervention, but meanwhile introduce a new challenge for application developers. therefore, the underlying challenge is to explore an efficient solution to increase the usability of context."
"the definition of interoperability is \"the ability of two or more systems or components to exchange information and to use the information that has been exchanged\" [cit] . two possible types of interoperability can be achieved. on the one hand, inner components can interact with each other and share information. on the other hand, different context aware middleware can communicate with each other and make use of exchanged information."
"this model [cit] involves, as the name says, multiple disciplines like psychology, computer science and linguistics. it demonstrates context from different points of view and specifies the relationships among multiple disciplines. the idea is to widen the vision to examine context and to construct a general model, but the complex modelling process introduces difficulties as it incorporates the information concerning many applications, various types of users, and multiple environments. this proposal still remains at the conceptual level. the specific procedures are not clearly figured out and thus the practical usage of this technique is rare."
"reasoning is a crucial factor for inspecting the middleware performance due to its considerable contributions to improve the awareness and smartness of middleware. although ontology is the primary choice to infer useful information, it can be predicted that hybrid reasoning models will be preferred over ontology in the foreseeable future. for example, secoman proposed a hybrid model which combines ontology and rule. this hybrid model enables user-defined rules via semantic web rule language (swrl) as well as description logics."
"service discovery is missing in octopus and secoman. mechanisms to discover services in other middleware are various. e.g., campus contains a component called service directory acting as a reference to all available services. more specifically, service directory is in charge of registering and locating all services provided by the middleware so that context consumers could find the services needed. in casf, it is possible to search for and combine services which are suitable for the user's purpose by utilizing semantic web services. [cit] declare that cocamaal allows service discovery to build compound services. however, the statement is still blurry and cannot be proven without specifying detailed methodology."
"in a paired psychophysical experiment, we found that human observers were able to distinguish the same set of textures with comparable performance (83 ae 3 percent, mean ae sd). human performance also dropped (although less so) when performance was tested across speeds (77 ae 3 percent)."
"step v: all 4 steps have been repeated till maximum iterations. table 2 shows aco parameters used during practical verification. this methodology is stimulated by foraging nature of ants, which are treated as blind living things, and conversion among them takes place-using pheromone alchemical. it comprises positive feedback affection, which provides better-optimized solutions."
"under low wind velocity also, the hybrid pv-wind system has low battery consumption. the pv and wind energy sources are working independently without influencing one another. compared to pso-based mppt method, the aco mppt techniques has seven times faster mpp achievement with accurate convergence velocity."
"to describe the operation of pv-wind system under intermittent operating conditions, permanent magnet synchronous generator is employed because of zero reactive power consumption. in addition, it does not have need for a gearbox with better power factor and accuracy due to self-execution behavior. the pmsg model has been developed using steady current depicted in figure 2a . voltage and current ( & ) obtained from rectification is expressed [cit] mathematically with regard to stator voltage/current (, )."
"the major disadvantages of switched mode power converters have discontinuity of supply current, low dynamic response and higher power device peak current, which made this less acceptable. in contrast with classical switched mode dc-dc converter, cuk converter comprises less switched power loss, high current behavior with better efficiency, which acts as a power adapter between inverter and renewable sources. the cuk converter operation presented in figure 3b is described in two working modes. when power switch gets short-circuited and energy has been released by capacitor. table 1 presents the specifications used during design of cuk converter. the mathematical expression-governing mode-i conducting state is described as: in case of power switch gets open circuited, the energy flow takes place with forward biasing diode and input supply is responsible to charge capacitor ca. described mathematical relations of this mode of operations are:"
"during the presentation of the motion stimuli, both static pressure and impedance increase rapidly, level off, then fluctuate around a mean value in response to the grating etched on the moving cylinder (fig. 5a )."
"the aco based optimized methodology provides optimal power extraction from solar and wind energy sources for residential applications, which contained high convergence velocity, better-searched performance and simpler implementation as major advantage. the completed hybrid solar wind driven pmsg power system is modeled through matlab and provides hardware interface (dspace) for validation and confirmation of high power generation. under low wind velocity, the hybrid system has low battery consumption, which demonstrates the improved controller performance. inverter regulated with flc-dspace control has power efficiency equated with classical pi-controller. the hybrid integration of solar and wind energy system have been realized experimentally under various conditions to develop novel hybrid power system followed by cuk converter. aco algorithms developed using m-file have complex coding interfaced to a dspace hardware board. the performance of aco based mppt has been evaluated versus pso, fa and abc algorithms. experimental results reveal that the aco based mppt provides seven times faster convergence compared to the pso algorithm for achievement of mpp and tracking efficiency. the limitations of this adopted hybrid control system are a requirement of ideal layout for installation of pv modules and wind turbine with adequate wind and solar insolation. the installation cost is another limitation of this hybrid pv-wind system. included work can be extended to learning framework with internet of things-based intelligent algorithms for pv-wind hybrid system to achieve utmost power tracking efficiency."
the proposed sample process is repeated for optimization of parameters. let z p newly solution are produced and has addition with y p initially obtained solution. total z p + y p solutions have been obtained in which y p best are replicated and overall methodology is recapitulated for several iterations. figure 4 presents the flowchart of aco-based mppt which describes the step by step process.
"). decoder. we implemented three different decoders of increasing complexity. first, we built a simple linear decoder eq. (1), comprising n þ 1 parameters, where n is the number of predictor variables:"
"flc current controller, the inverter current can be regulated using pv-wind systems [cit] . however, in traditional topology the dc-dc converter is employed after pv module for optimal tracking of power."
"the aco based optimized methodology provides optimal power extraction from solar and wind energy sources for residential applications, which contained high convergence velocity, bettersearched performance and simpler implementation as major advantage. the completed hybrid solar wind driven pmsg power system is modeled through matlab and provides hardware interface (dspace) for validation and confirmation of high power generation. under low wind velocity, the hybrid system has low battery consumption, which demonstrates the improved controller performance. inverter regulated with flc-dspace control has power efficiency equated with classical pi-controller. the hybrid integration of solar and wind energy system have been realized experimentally under various conditions to develop novel hybrid power system followed by cuk converter. aco algorithms developed using m-file have complex coding interfaced to a dspace hardware board. the performance of aco based mppt has been evaluated versus pso, fa and abc algorithms. experimental results reveal that the aco based mppt provides seven times faster convergence compared to the pso algorithm for achievement of mpp and tracking efficiency. the limitations of this adopted hybrid control system are a requirement of ideal layout for installation of pv modules and wind turbine with adequate wind and solar insolation. the installation cost is another limitation of this hybrid pv-wind system. included work can be extended to learning number of poles 4"
"a subset of textures (9) was used in a psychophysical experiment involving five human subjects and using the same apparatus. subjects were asked to rate on an arbitrary scale the dissimilarity of pairs of textures presented successively. subjects could not see the textures presented and wore headphones playing white noise to eliminate any auditory cues. scanning speed (40, 80, 120 or 160 mm/s) and texture pairs were varied such that some pairs consisted of the same texture (either presented at the same speed or not), yielding 138 different stimulus pairs (24 of them 'same' pairs). each condition was repeated three times in separate experimental blocks."
"the inverter depicted in figure 5a is controlled with fuzzy logic control (flc)-based intelligent methodology, which provides the smooth maintenance of load voltage and frequency. irrespective of wind velocity, loading conditions and level of sun insolation inverter regulates voltage and table 2 shows aco parameters used during practical verification. this methodology is stimulated by foraging nature of ants, which are treated as blind living things, and conversion among them takes place-using pheromone alchemical. it comprises positive feedback affection, which provides better-optimized solutions."
the finger was fixed firmly in place during stimulus delivery to obtain precise and repeatable sensory outputs. three different experimental setups were used to stimulate the finger. a new biotac skin was used for each of the three experiments.
erik w. [cit] . he is currently working toward the sm degree in computer science at the university of chicago and is a programmer in the bensmaia lab.
"included work, based on aco mppt has no requirement of supplement circuitry with voltage/current sensors and independent system responses compared to different evolutionary techniques used. novelty of this research paper is mppt action with aco technique followed by flc inverter controller for residential pv-wind power generation has neither been discussed nor implemented experimentally under changing operating conditions with single cuk converter as an impedance balancer using dspace (ds1104) platform. the significant contributions of this research work are mentioned below:"
"the equivalent mode of pv cell presented in figure 1b comprises current source, diode and series/parallel resistances. the mathematical equations describing output current (i out ) based on the electrical circuits can be derived with used abbreviations [cit] as:"
"in this research work, electric circuit-based battery model is employed which provides better dynamicity for state of charge approximation. it comprises voltage source (ideal) with series internal resistance, which evaluates battery behavior, depicted using figure 2b . a battery (ni-cd) discharging characteristic is presented with figure 3a [cit] ."
"this paper starts section 1 with a detailed introduction of hybrid pv-wind system using different mppt algorithms followed by several literature reviews. section 2 discusses the complete schematic of pv-wind system controlled through aco-based mppt which is employed with single cuk converter. this section describe the pv modeling pmsg modeling, mathematical modeling of wind turbine system, electric circuit-based battery model and mathematical analysis of cuk converter modes of operations. section 3 explains the aco-based optimized maximum power point tracking algorithm with detailed specifications used. section 4, deals the flc-based inverter control for smooth maintenance of load voltage and frequency, mppt and inverter controller action with single cuk converter and experimental setup. section 5 describes the practical responses followed by conclusions in section 6 and references. the proposed model can be converted to real time application, by applying load requirement of household applications with battery charging during duration of surplus power. battery is responsible to compensate load requirement under hybrid power insufficiency. low wind velocity with high sun insolation operating conditions has also been hybridized by proposed pv-wind power system for real time implementation. the main advantage of the adopted pv-wind hybrid system is minimization of intermittence issues of pv and wind renewable sources which can work under low wind velocity and during night periods."
"case i: the two controllers operations are decided based on the presence of pv/wind renewable sources. in case of generation of power from pv as well as wind sources, the aco-based mppt (controller 1) produces duty ratio for cuk power converter and flc inverter control (controller 2) provides power generation from pv and wind renewable sources."
"interestingly, we repeated the procedure described above after the spectra were normalized by their mean intensity; that is, each element of each fourier spectrum was divided by the mean value of all the elements in that spectrum so that the overall spectral intensity conveyed no information about texture. performance dropped only marginally (by 1 or 2 percent), showing that temporal cues are highly informative about texture."
"punctate indentations produce complex patterns of sensor activation as shown in fig. 2a, b . electrodes near the indentation tend to signal an increase in impedance while electrodes located further away from contact signaled a decrease in impedance. sensors are very sensitive, able to signal the presence of very small indentations (see response to a 100-mm indentation in fig. 2a) . a 50-mm indentation was detected 79 percent of the time and a 100-mm indentation was detected 85 percent of the time (that is, above baseline, chance is 50 percent). the absolute sensitivity to step indentations is thus comparable to that of human and non-human primates (10-100 mm [cit] ."
"benoit p. [cit], 2009, [cit], repectively. he is currently a postdoc scholar with the university of chicago. his research interest includes neuroprosthetics, haptics and neuroscience in the context of touch and sensory-motor control."
"where p is the decoded stimulus feature, s is the vector of sensor outputs, a is a constant, and b is a vector of coefficients. the second decoder was quadratic eq. (2), with ná (nþ3)/2þ1 parameters."
"the inverter depicted in figure 5a is controlled with fuzzy logic control (flc)-based intelligent methodology, which provides the smooth maintenance of load voltage and frequency. irrespective of wind velocity, loading conditions and level of sun insolation inverter regulates voltage and frequency instant. figure 5b depicts the flc regulated inverter controller. for maintenance of voltage output (v out ) and frequency, a phase locked loop (pll) presented in figure 5b associated with synchronized frame of reference is employed. the flc inverter control regulation provides better efficiency, stable operation and less frequency, disturbances with respect to pi-based inverter regulation [cit] . figure 6a -d demonstrate the pwm pulse generation using flc inverter strategy and membership functions used during implementation, respectively."
"case iii: when pmsg is not in operation and only pv sources are generating power, cuk converter has no input and there is no pulse generation using controller 1. controller 2 produces current command signal to obtain optimal pv power generation from pv modules. figure 7a depicts the practical set up developed for proposed hybrid (pv-wind) system controlled through dspace, which comprises pv module, wind emulator, cuk converter, and electric circuit-based battery model. with application of aco model based mppt the sensed (voltage/current) is transformed to digital pulses by analog to digital converter and controller 1 and controller 2 generated signals are collected from control desk i/o of dspace which is processed through insolation interface. la50-p (current transducer) and lv20-p (voltage transducer) are employed during experimentation, respectively. the aco based mppt is modeled in figure 7b using matlab which generates pwm signal for cuk converter linked through dspace hardware based ct60 am igbt, skhi22 ar gating driver, power supplication using programmed dcmagna (programmable dc power supply) and pmsg wind emulators are employed during practical investigation."
"in this research work, aco methodology is implemented by considering vpv and ipv followed by generation of target output (vtarget). ants have been situated randomly and its movement is noted to achieve vtarget which returns to the colony after this process. moreover, the vtarget and colony in case of power switch gets open circuited, the energy flow takes place with forward biasing diode and input supply is responsible to charge capacitor c a . described mathematical relations of this mode of operations are:"
"furthermore, with the development of sophisticated prosthetic fingertips, we show that the analysis of sensor signals provides insights into the mechanisms of touch, from the mechanics of the skin to the decoding algorithms required to extract the various behaviorally relevant stimulus features. finally, we propose that the approach adopted here provides a much more useful metric to assess fingertip sensorization than do the standard specifications."
"in this research work, the proposed hybrid pv-wind system performance have been evaluated with pso, fa, abc and aco maximum power point tracking. compared to other methods, aco-based mppt provides lesser tracking period to achieve mpp."
"case i: the two controllers operations are decided based on the presence of pv/wind renewable sources. in case of generation of power from pv as well as wind sources, the aco-based mppt (controller 1) produces duty ratio for cuk power converter and flc inverter control (controller 2) provides power generation from pv and wind renewable sources."
"previous work investigating the capabilities of prosthetic fingers state these in terms of technical specifications: spatial configuration (density) of the sensors, force resolution, bandwidth, etc. [cit] . however, these specifications are difficult to relate to perceptually relevant quantities. to fill this gap, we test the capabilities of the prosthetic finger using metrics that probe behaviorally relevant stimulus quantities in ways that are analogous to the ways in which human perceptual abilities are probed. we can then assess the degree to which the sensorization of extant prosthetic fingers places limits on the sensory feedback achievable in neuroprosthetics. importantly, we test each sensory dimension while varying others, to assess the degree to which the stimulus information that is available from the sensor output is generalizable across contexts. to extract this information, we implement three decoders of increasing complexity -linear and quadratic functions as well as a neural network -and test these in five different experiments: localization, pressure discrimination, motion direction discrimination, speed discrimination, and texture discrimination. we find that the state-of-the-art in prosthetic fingertips, the biotac (syntouch, llc, los angeles, ca), is capable of conveying this sensory information with a precision that is comparable to that of the native human finger over a range of conditions."
"information about direction of motion stemmed from shearing induced by the frictional force between stimulus and finger. the resulting deformation patterns (and sensor activation patterns) were strongly modulated by changes in motion direction. the output of electrodes on the sides of the finger was particularly sensitive to changes in direction. the responses of cutaneous afferents innervating the skin of human and non-human primates have also been shown to be modulated by shearing direction [cit], in particular those on the sides of nail [cit], mirroring the phenomenon observed with the prosthetic finger."
"our results suggest that the sensorization of the bionic finger does not constitute a significant bottleneck in the development of sensory neuroprostheses, at least in terms of the acuity and precision of the artificial percepts. however, one of the main functions of somatosensation is to support motor behavior and the dexterous manipulation of objects [cit] . sensory signals involved in motor control may be different from those that mediate perception and certainly engage different sensory pathways. then again, it is not clear how to test the sensory capabilities of the prosthetic finger from the perspective of human motor control. only when the robotic finger is implemented in an upper-limb neuroprosthesis and tested on tasks involving grasping and manipulating objects can the adequacy of its sensorization be truly assessed."
"colorni, dorigo and maniezzo invented meta-heuristics-based optimized algorithm to solve difficult non-linear issues. the particular ant to obtain the shortest path optimization generates pheromones. for the searching of foods, the movements of ants take place in different direction followed with generated pheromones. the shortest path should have high pheromones probability as it evaporates in short and methodology is repeated for different iterations to optimize the problems."
"colorni, dorigo and maniezzo invented meta-heuristics-based optimized algorithm to solve difficult non-linear issues. the particular ant to obtain the shortest path optimization generates pheromones. for the searching of foods, the movements of ants take place in different direction followed with generated pheromones. the shortest path should have high pheromones probability as it evaporates in short and methodology is repeated for different iterations to optimize the problems."
"because of abounded necessity of energy harvest and continuous depletion of fossil fuels, demands of renewable energy sources are gaining more attention [cit] . photovoltaic (pv) and wind are the environment friendly renewable energy sources, which has more contemplation for backwoods use [cit] . standalone wind energy conversion system (wecs)/pv system have been remarkably employed to produce electrical power in rural places for agricultural applications [cit] . nevertheless, fluctuations in solar insolation level and wind speed are the major shortcomings of these renewable sources. compared to individual pv/wind system, the hybrid pv/wind integrated system provides high steady power generation. however, implementation of hybrid pv/wind systems is being future assignments for researchers. it is also noted that in contrast to individual pv/wind system, the hybrid system has low cost of implementation with augmented steady operation."
"practical justification is done by comparing power tracking behavior of aco algorithm vs. pso, fa and abc techniques using figure 7c . the average period required to achieve mpp is presented with comparison using table 3 . the aco based mppt method takes lesser time compared to other algorithms mentioned. moreover, the duty ratio performance with aco mppt is better compared to other techniques employed in figure 7d . figure 8a portrays the starting operation before gmp achievements in which pv module voltage level tries to reach preset level of voltage. the steady state behavior of the designed pv system is also examined and is justified with matched pv characteristics depicted by figure 8b . by means of proposed intelligent aco, the inverter current becomes synchronized in figure 8c . the dynamic performance of aco based pv power system has been justified with variations in fluctuating sun insolation and can be depicted in figure 8d . figure 9a demonstrates the responses obtained from hybrid pv-wind controlled aco, which clearly interprets that mppt operation is achieved independently without influencing one another. with application of aco model based mppt the sensed (voltage/current) is transformed to digital pulses by analog to digital converter and controller 1 and controller 2 generated signals are collected from control desk i/o of dspace which is processed through insolation interface. la50-p (current transducer) and lv20-p (voltage transducer) are employed during experimentation, respectively. the aco based mppt is modeled in figure 7b using matlab which generates pwm signal for cuk converter linked through dspace hardware based ct60 am igbt, skhi22 ar gating driver, power supplication using programmed dcmagna (programmable dc power supply) and pmsg wind emulators are employed during practical investigation."
"the ability of proposed hybrid control system is decided on the basis of optimal use of battery voltage by sensing pv and wind dc bus voltage. the pv and wind dc voltage level should be matched prior to battery connection. [cit] has discussed stability realization of hybrid pv-wind system using small signal stability analysis. under fault conditions, the main problems associated with hybrid power system has one or more supply voltages to become zero. moreover, the renewable energy sources should provide required load power under normal operating conditions. during fault situations, the delivered load power are reduced because of one of the renewable energy sources are unable to deliver power to converter [cit] ."
"sliman j. bensmaia received the ba degree in cognitive science from the university of virginia and the phd degree in cognitive psychology from the university of north carolina at chapel hill. he is an associate professor in the department of organismal biology and anatomy, university of chicago, and is a member on the committees on neurobiology and computational neuroscience. his laboratory investigates the neural basis of perception and is leveraging these insights in basic science to develop approaches to restore somatosensation by electrically stimulating the nervous system. he is a member of the ieee."
"case iii: when pmsg is not in operation and only pv sources are generating power, cuk converter has no input and there is no pulse generation using controller 1. controller 2 produces current command signal to obtain optimal pv power generation from pv modules. figure 7a depicts the practical set up developed for proposed hybrid (pv-wind) system controlled through dspace, which comprises pv module, wind emulator, cuk converter, and electric circuit-based battery model. case ii: in case of power generation from only wind renewable sources (not pv), the duty ratio of the cuk converter is generated to make dc link voltage fixed and controller 1 works in voltage control mode. in addition to this controller 2 tries to obtain optimal wind power by generating current signal."
"overall, all three decoders yielded accurate predictions of indentation depth with coefficients of determination (r 2 s) of 0.90, 0.93 and 0.97 for the linear, quadratic and neural network decoders, respectively (fig. 3a) . however, performance was relatively poor for the smallest indentations, as evidenced by deviation from the unity line (median errors of 0.12, 0.07 and 0.06 mm, respectively), as is observed in analogous psychophysical experiments with human subjects [cit] . with a standard amplitude of 0.1 mm, we estimate the jnd to be around 0.15 mm (0.17, 0.15 and 0.13 mm for the linear, quadratic and neural network decoders, respectively), much lower than that of monkeys, 0.50 mm (or 0.33 mm if corrected for the reference according to weber's law) measured in analogous experiments with the same stimulus [cit] (fig. 3b) ."
"that estimates of stimulus location were less accurate than those achieved by human subjects under analogous stimulation conditions can be straightforwardly attributed to differences in sensor density: the density of impedance electrodes on the biotac is relatively low ($4/cm 2 ) compared to the innervation density of the human fingertip ($240 units/cm 2 ). however, the lower spatial acuity of the prosthetic fingertip is unlikely to constitute the bottleneck of neural interfaces given the relatively sparse sampling of the skin's surface that is achievable with current peripheral and cortical implants (see [cit] for reviews)."
"the biotac [cit], designed to closely mimic the mechanical properties and sensing capabilities of the human finger [cit], consists of a rigid core surrounded by an incompressible fluid contained within an elastomeric skin (fig. 1a) . the skin surface is embossed with an ovoid grating that mimics the human fingerprint. the biotac is equipped with three complementary sensor types. nineteen impedance sensing electrodes, distributed over the finger's surface (fig. 1b), sense the pattern of indentations across the skin. a hydro-acoustic pressure sensor measures the fluid pressure. the raw output of this sensor provides the static pressure (referred to as p dc ) but the output is also high-passed and amplified to provide a precise measurement of small pressure fluctuations (p ac ), such as those elicited when scanning a textured surface [cit] . finally, a thermistor measures the temperature and temperature changes (this sensor was not used in this study). the default sampling frequencies are 100 hz for all sensors except for p ac, which is sampled at 2,200 hz."
"the objective was to extract information about the location and depth of the punctate indentations, information about the direction and speed of the moving gratings, and information about the texture of the surfaces. importantly, we wished to extract information about one stimulus dimension while the other stimulus dimensions varied. so, for example, we sought to determine how accurately we could extract information about motion direction, even as the speed and surface texture varied."
"estimates of stimulus location were more accurate than those obtained in previous studies using the biotac [cit], a difference that can be attributed to differences in the stimulation paradigm: first, in the present study, stimuli were applied along the same (gravitational) axis while, in its predecessors, force vectors varied in direction. second, we delivered indentations using the same probe rather than probes with different shapes. third, we applied the stimuli using a highly precise stimulator rather than manually. all of these likely contributed to the better performance of our decoder."
"where c is a symmetric matrix with square terms in the diagonal entries and the interaction terms in the offdiagonal entries. the third decoder was a neural network, with a hidden layer comprising 20 neurons and 20nþ41 weights. the neural network was trained and its predictions derived using the neural network toolbox in matlab (netfit and train, mathworks, inc., natick, ma). fitting procedure. decoder parameters were fit using a leave-one-out procedure. that is, one repetition (out of 5 or 6 repetitions in the indentation or motion experiments, respectively) of each condition was excluded during parameter fitting and this repetition was then used to evaluate prediction accuracy. the same procedure was repeated such that each repetition was left out once and used for crossvalidation. importantly, all conditions were mixed to estimate the decoder parameters."
"psychophysical data analysis. in the complementary psychophysical experiment, dissimilarity judgments were first normalized, by dividing each rating by the mean rating within each experimental block. then, the probability that a pair of different textures was judged as more dissimilar than a pair of same textures (within and across speeds) was computed using ideal observer analysis and used as a measure of discrimination performance."
"case ii: in case of power generation from only wind renewable sources (not pv), the duty ratio of the cuk converter is generated to make dc link voltage fixed and controller 1 works in voltage control mode. in addition to this controller 2 tries to obtain optimal wind power by generating current signal."
"in summary, we showed that simple algorithms, such as linear or quadratic decoders, can robustly extract behaviorally relevant information about tactile stimuli -their amplitude and location, direction of motion direction, and speedfrom the sensor output despite concomitant changes in other stimulation parameters. furthermore, the prosthetic finger and decoders often matched or even outperformed human observers on analogous psychophysical tasks, with the exception of the localization task."
"decoding of amplitude was more precise than that obtained in a previous study with the biotac [cit], a difference that can, again, be attributed to differences in stimulation. in fact, performance was substantially better than that observed in monkeys in an analogous behavioral paradigm. the prosthetic finger is thus well suited to convey information about contact pressure."
"to describe the operation of pv-wind system under intermittent operating conditions, permanent magnet synchronous generator is employed because of zero reactive power consumption. in addition, it does not have need for a gearbox with better power factor and accuracy due to self-execution behavior. the pmsg model has been developed using steady current depicted in figure 2a . voltage and current (v re &i re ) obtained from rectification is expressed [cit] mathematically with regard to stator voltage/current (v st, i st )."
"the equivalent mode of pv cell presented in figure 1b comprises current source, diode and series/parallel resistances. the mathematical equations describing output current ( ) based on the electrical circuits can be derived with used abbreviations [cit] as:"
"abstract-efforts are underway to restore sensorimotor function in amputees and tetraplegic patients using anthropomorphic robotic hands. for this approach to be clinically viable, sensory signals from the hand must be relayed back to the patient. to convey tactile feedback necessary for object manipulation, behaviorally relevant information must be extracted in real time from the output of sensors on the prosthesis. in the present study, we recorded the sensor output from a state-of-the-art bionic finger during the presentation of different tactile stimuli, including punctate indentations and scanned textures. furthermore, the parameters of stimulus delivery (location, speed, direction, indentation depth, and surface texture) were systematically varied. we developed simple decoders to extract behaviorally relevant variables from the sensor output and assessed the degree to which these algorithms could reliably extract these different types of sensory information across different conditions of stimulus delivery. we then compared the performance of the decoders to that of humans in analogous psychophysical experiments. we show that straightforward decoders can extract behaviorally relevant features accurately from the sensor output and most of them outperform humans."
"in this research work, electric circuit-based battery model is employed which provides better dynamicity for state of charge approximation. it comprises voltage source (ideal) with series internal resistance, which evaluates battery behavior, depicted using figure 2b . a battery (ni-cd) discharging characteristic is presented with figure 3a [cit] ."
"the location of each indentation in the first experimental set, decoded using the neural network decoder, are shown in fig. 4a . as can be seen, the sensor output provided generally accurate information about contact location despite large fluctuations in indentation depth. localization performance did vary widely across decoders with the quadratic and neural network decoders outperforming the linear one, particularly for indentations on the side of the finger (figs. 4b, 4c ). the median error was 1.07 mm and 0.77 mm for the quadratic decoder and the neural network, respectively (fig. 4d) . the performance of the decoders was slightly worse than that of a human subject in a two-point discrimination task (fig. 4e) . the two point threshold is 0.33 mm for human subjects and 0.5 mm for the biotac using the nn decoder (1.08 and 0.66 for the linear and quadratic decoders, respectively)."
"classification analysis. textures were classified based on the frequency composition of the sensor output (specifically of the dynamic pressure sensor, p ac ) as the vibrations evoked in the skin during texture scanning have been shown to be highly informative about texture identity [cit] . specifically, the frequency spectra of the p ac were obtained using fast fourier transform (fft in matlab). spatial spectra were obtained by converting the frequency to a spatial period (by multiplying frequency by scanning speed). linear discriminant analysis (lda) was used to classify the textures (fitcdiscr in matlab). the predictor variable was a vector of 15 elements, consisting of the spectra amplitude smoothly interpolated at 15 frequencies spaced logarithmically between 6 and 600 hz (spatial spectra were sampled from 0.1 to 8 mm à1 ). one repetition was left out of the classifier training and used to test classification performance; this procedure was repeated for each repetition. four classification analyses were performed, one for each speed and one with all speeds combined, the latter using spatial spectra instead of frequency spectra. we obtained similar results using welch's method to decompose the vibratory frequency."
"the authors are grateful to hannes saal for his comments on a previous version of the manuscript. this work was funded in part by darpa contracts n66001-15-c-4014 (haptix), and n66001-10-c-4056 (revolutionizing prosthetics), nsf grants 533649 and ios-1150209, and by the kimberly-clark corporation. b.p.d. received funding from fonds sp ecial de recherche (fsr -belgium)."
"speed was estimated solely based on steady-state output of individual sensors. while speed decoders yielded better performance than that achieved by humans under analogous stimulation conditions, complex computations that include the relative timing of activation of adjacent sensors [cit] might provide more accurate estimates. this latter mechanism of speed estimation would more closely mimic its putative counterpart in the visual system of primates [cit] . the neural mechanisms of tactile speed estimation have yet to be conclusively elucidated [cit] but sensorized fingers may provide a fruitful testbed for hypothetical neural codes. a noticeable advantage of our approach is that it could be used to extract both speed (from steady-state output) and texture (from dynamic pressure timing) independently."
"comparison to human psychophysics. published data from three psychophysical studies with similar experimental procedures were used to compare the performance of the biotac to that of human observers: ref. [cit] for localization, ref. [cit] for direction discrimination, ref. [cit] for speed discrimination. for the indentation depth tasks only, decoders' performance was compared to data from a psychophysical experiment carried out with rhesus macaques using an identical stimulator [cit], since no suitable experimental match was found in human literature. note that humans and monkeys have been shown to perform comparably in analogous tasks [cit] . for all the \"psychophysical\" tasks, we computed psychometric functions, just noticeable differences (jnd, difference needed to be perceived 75 percent of the trials), and weber fractions from the biotac decoding accuracy so that biotac and human performance were expressed using the same metrics."
"the estimated maximum value of the objective function e max is set as 10 a, where ϵ + . the value of a is kept increasing from 1 until all individuals' initial objective function values are smaller than e max . other experimental relative data are defined in table 5 ."
"although ends are based on artificial hierarchical structures, they are classified as flat classifiers in the context of this paper because they do not make use of an expert-defined hierarchy. however, we can constrain the construction of the binary hierarchies to be consistent with an expert-defined one by ensuring that class-subclass relationships from the original hierarchy are maintained in the binary hierarchy. this yields a hierarchical classification method based on ends and provides us with a, where multiclass problems have been replaced by two-class problems. this is one possible ensemble member for an ensemble of constrained nested dichotomies (ecnds) for this hypothetical classification problem."
"the iteration number is kept as 100 like the last two tests. the predictive reactive complete rescheduling approach is more flexible in a dynamic environment as it reschedules the new arrival jobs at the beginning of the rescheduling point. however, those jobs could only be scheduled after completing the operations of the original schedule at each stage by the traditional static approach. this impact is more evident when the ratio of the rs to the makespan in the original schedule is small. and it is decreasing and almost disappears when the rs takes place near the end of the original schedule. therefore, we strongly suggest using the predictive reactive complete rescheduling approach with the assistance of gpus when the rs is arranged at the first half part of the original schedule. meanwhile, the traditional static approach may have similar performance if the rs is considered at later half part."
"an example of edffs is presented in table 4 . there are 6 original jobs. each job consists of 3 stages and there are two machines at each stage. jobs are available to be assigned to machines after the release time (rj). each operation is processed on the target machine (mjs) after the start time (sjs). to make it simple, the processing time is set as 1, 2 and 3 for the three stages respectively. the average power consumption"
"the results are taken from ding and dubchak [cit], okun [cit], and shen and chou [cit] . the results by shen and chou were obtained on a different feature set."
"next generation sequencing has enabled the identification of snps and small indels to a high resolution. svs, however, are much harder to detect. one reason is that svs encompass a diverse range of modifications. while snps are simple base pair substitutions, the term \"sv\" summarizes many different phenomena. typically, different classes of svs are distinguished, such as deletions, inversions and insertions. definitions for some of these classes vary in the literature. for the purpose of this work, we define six different sv classes which are visualized in figure 1 : deletions, cut&paste insertions, tandem and interspersed duplications, inversions and novel element insertions. the main drivers behind interspersed duplications in human are mobile element insertions, such as alu, line1 and sva elements. they duplicate using retrotransposition and in total represent approximately 25% of all human structural variation [cit] . dna transposons, although now inactive in mammals (excepts bats) are active in plants and lower-order animals [cit] . they use a cut&paste mechanism to move in the genome and therefore motivated the inclusion of cut&paste insertions as a separate sv class."
"thus energy efficiency is becoming an essential parameter of industrial manufacturing processes, mostly due to new government legislation, customers' environmental concerns and continuously rising cost of energy. because of a growing economical competitive landscape and higher environmental norms, it is now vital for manufacturing companies to reduce their energy consumption and to become more environment-friendly."
"the third component in the workflow analyzes and combines the sv signature clusters to classify events into five sv classes: deletions, inversions, novel element insertions, tandem duplicaitons and interspersed duplications. because the confident distinction of interspersed duplications and cut&paste insertions solely based on sequencing reads is impossible, we classify both as interspersed duplications. nevertheless, we annotate duplications where the region of origin seems to be deleted in the sequenced individual (i.e. a deletion overlaps the genomic origin) as potential cut&paste insertions. while inversions, deletions and tandem duplication signature clusters can be directly reported as inversions, deletions and tandem duplications, respectively, the other three signature classes (ins, dup and brk) are more complex. the reason is that interspersed duplications are not characterized by only one genomic region but twoa genomic origin and a genomic destination. to capture and classify these higher-order events, svim needs to combine multiple signature clusters and therefore makes the following distinctions (see also fig. 3 ):"
"given class probability estimators for all the internal nodes, it is straightforward to obtain probabilities for the leaf nodes. because the subclasses at a particular node are disjoint and complete, the base classifiers' probability estimates along a path from the root to a leaf node can be multiplied to obtain class probability estimates for that leaf class."
"in this paper, we have first studied a dynamic energy efficient flexible flow shop scheduling model using peak power value with the consideration of new arrival jobs."
"recent work on the hierarchical classification of proteins is based on the application of margin-based metalearning schemes that are used to combine the predictions of one-versus-rest binary classifiers. [cit] compare this type of technique to blast-based classification and standard one-versus-all classification and a calibrated version thereof. rangwala and karypis [cit] investigate a very similar scheme but consider information from above and below the target level, as well as the target level itself. they compare to one-versus-all classification and native multiclass classification and observe that, when considering plain and balanced classification error, \"the use of hierarchical information leads to some improvements\" on one of the four data sets they consider, which corresponds to a fold recognition problem."
"in general, one would expect that the learning task becomes easier for a hierarchical classification scheme if the structure of the class hierarchy is reflected by the similarity of the instances from the various classes in feature space. an appropriate class hierarchy can guide the learning algorithm to a good solution. vice versa, an arbitrary structure of classes that is not related to the similarity of instances can increase the complexity of the classification problem. this also applies to the hierarchical variants of ensemble classifiers: if the hierarchy is not sufficiently reflected in feature space there is little to be gained from exploiting it. in the case of ensemble classifiers there is the additional problem that constraining the set of possible ensemble members may simply result in a higher correlation of errors they make, thus degrading classification performance."
"in this paper, we have studied the relative performance of hierarchical and flat machine learning schemes for the classification of proteins based on well-known expert-defined hierarchies. our experiments were designed to evaluate the impact on classification performance when using this additional domain knowledge. as the main instrument for measuring the effect of hierarchical information, we have used hierarchical and flat versions of ends, a technique for solving multiclass classification problems by binarization. we evaluated the performance of these methods on real-world protein classification data sets, as well as an artificial data set where we could ensure that the class hierarchy is actually reflected in the data. we presented results based on 1) using no class hierarchy, 2) using the expert-defined hierarchy for the data set at hand, and 3) using a class hierarchy that has been constructed adversely to the commonly accepted hierarchy."
"as the number of new arrival jobs is equal to the ratio rs to the tmakespan in the original schedule) that multiplies the amount of original jobs, we change the amount of new arrival jobs by varying the ratio of the rs to the makespan in the original schedule. the influence with different ratios to the predictive reactive complete rescheduling approach and the traditional static approach are displayed in table 10 ."
"q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q figure 5 : comparison of recall on a 53x coverage public pacbio dataset and a 6x coverage subset with 2676 high-confidence deletion and 68 insertion calls. for each tool and different thresholds, the number of sv calls with score above the threshold (log-scale) is plotted against the recall. the upper and lower panels show performance on the full dataset and a randomly sampled 6x coverage subset of the data, respectively. svim reached the same recall with fewer calls than other tools. the vertical dotted lines denote the average number of deletions and insertions to expect in an individual as recently reported using a de-novo assembly approach [cit] ."
"there exists a wide variety of tools for sv calling from short reads [cit] but despite ongoing efforts, the discovery of svs from short-read data remains challenging [cit] . studies have estimated that short-read methods suffer from poor sensitivity down to 10% particularly for small svs shorter than 1kbp [cit] . in contrast to snps where discovery and sequence resolution can be performed simultaneously, svs are discovered mainly indirectly using short paired-end reads. their alignments are examined for characteristic signatures, such as inconsistently mapping read pairs, split reads and changes in read depth [cit] . these signatures can only be indirect evidence in favor of certain sv classes but are unable to fully characterize the sv. the main limitation here is that most svs are simply larger than the short reads. the accurate detection of svs is, besides their diversity, hampered by their association with repeat regions, biases in the sequencing technology and the additional complexity of diploidy [cit] ."
"in this appendix, we sketch how hierarchies that are \"maximally\" different from existing hierarchies are constructed. the problem can be framed as a discrete optimization problem. we assume that the structure of the new hierarchy is identical to the structure of the given hierarchy. the goal is then to look for a new assignment of leaf classes such that a scoring function is maximized."
"to be more specific, consider a class hierarchy, for instance, the one shown in fig. 1b . then, we determine, for each internal node i, the set of leaf classes in the leafs below that node and define the class c i of that node as the set of its leaf classes. let c i1 to c imi be the m i subclasses of class c i associated with node i. moreover, let pðc 2 c ij jx; c 2 c i þ be the conditional probability distribution for the m i classes at node i, given an instance x, estimated by the base classifier at that node. then, the estimated class probability for class value c is given by"
"logistic regression models are consistently outperformed by unpruned c4.5 decision trees, which are in turn consistently outperformed by bagged c4.5 trees, and the use of adverse hierarchies harms performance in most cases. however, the impact of using the adverse hierarchy instead of the correct one is quite small on the enzyme classification problem, suggesting that the expert-defined hierarchy does not correspond well to the distribution of the data. among the three hierarchical classification methods, the use of ends in the internal nodes of a class hierarchy (hends) is the best option over a wide range of settings."
"in this study, we introduce svim, a computational method for the sensitive detection and accurate classification of five different classes of svs from long read sequencing data. we describe the three core components of the approach and our methodology for evaluation on simulated and real datasets. our results demonstrate that svim reaches substantially higher recall and precision than existing tools for sv detection from long reads. unlike other methods, svim has been specifically designed to distinguish three separate classes of large insertions: interspersed duplications, tandem duplications and insertions of novel elements. to our knowledge, it is the only tool capable of identifying not only the insertion location of an interspersed duplication but also its potential genomic origin. we demonstrate this capability on a small number of high-scoring interspersed duplications identified in the na12878 individual. furthermore, we compare sv callsets produced by svim on reads from pacbio and nanopore data. finally, we compare the runtimes of different sv callers including svim."
"to simulate heterozygous svs, we adapted the previously described approach only slightly. instead of sampling all reads from the altered reference genome, half of the reads were sampled from the original refer-ence genome. consequently, reads from the original (wildtype) reference genome and the altered genome each amounted to 50% of the total coverage."
"among all tools, svim was the most consistent across the different settings (see fig. 5 ). it recovered substantially more deletions from the high-confidence call set than the other tools with the same number of"
"protein classification is a prominent problem in bioinformatics that can be approached using standard multiclass classification techniques from machine learning. however, in this domain, there is additional background knowledge in the form of expert-defined hierarchies of protein classes that can potentially be exploited to improve predictive performance. in fact, many practical multiclass classification problems are actually hierarchical classification problems: a set of classes can often be more appropriately understood as a set of sets of classes, where subsets comprise classes that are more similar to each other than to classes in other subsets. given a hierarchy of classes, standard machine learning approaches may find it harder to discern similar classes than classes that are unrelated according to the classification system. thus, it can be beneficial to apply a recursive top-down approach to hierarchical classification: first, discriminate the subsets of classes at the top level of the hierarchy and then recursively separate the classes (or sets of classes) in those subsets."
"as there is no a priori reason to prefer one particular binarization, we consider them all to be equally likely. however, although constraining the set of trees based on a given hierarchy reduces the number of possible binarizations-from 2,027,025 to 81 in the above example-it is not possible to consider all of them. the approach we use in this study is to choose randomly among the possible binarizations with uniform probability. this approach has been shown to work well on standard multiclass classification problems from the uci repository [cit] . in that case, a relatively small number of randomly chosen ensemble members, namely, 10 to 20, was found to be sufficient for close-to-optimum performance."
"a typical human genome differs from the reference genome at approximately 4 to 5 million sites amounting to approximately 20 million altered bases [cit] . these variations can be categorized into single nucleotide polymorphisms (snps), small insertions and deletions (indels), and structural variations (svs) affecting a larger number of base pairs. typically, differences larger than 50bp are considered svs although definitions vary and sometimes overlap with those of indels."
"studies have shown that in human more base pairs are altered due to structural variation than due to snps [cit] . additionally, svs are enriched 50-fold for expression quantitative trait loci when compared to snps [cit] . unsurprisingly, svs have a major influence on human diversity and are implicated in a wide range of diseases from autism and other neurological diseases to cancer and obesity [cit] . consequently, the characterization of svs is of major importance to human medicine and genetics alike. it can contribute to the early detection of disorders and can help to elucidate their underlying genetic and molecular processes [cit] . in other organisms such as plants, svs play an equally important role by driving phenotypic variation and adaptation to different environments [cit] ."
"mechanism for testing the benefit of using a given expert-defined hierarchy for model building: we can simply compare the predictive performance of ensembles of unconstrained and constrained nested dichotomies (ends and ecnds, respectively). fig. 1c shows a system of nested dichotomies that is consistent with the n-ary hierarchy in fig. 1b . we learn an ensemble of these trees for prediction because the given tree is not the only possible binarization: the original n-ary class hierarchy can be represented by other binary trees in a valid manner. hence, we construct an ensemble model with a certain user-specified number of these trees. at prediction time, class probability estimates for a particular class are obtained from the different trees in the ensemble according to (1) and then simply averaged."
"first, we present the results for fold recognition and enzyme classification using logistic regression, decision trees, and bagging as the base learners. in addition to the two protein classification data sets, we test the approaches on synthetic data to test hierarchical and flat classifiers under perfect conditions where the class hierarchy is reflected in the relative distribution of the classes in the feature space."
about one half of the world's total energy is currently consumed by the industrial sector [cit] and its energy consumption has nearly doubled over the last 60 years [cit] .
"in the last decade, graphics processing units (gpus) have gained widespread popularity as computing accelerators for high performance computing (hpc) applications [cit] . research on gpu-based approaches for solving scheduling problems [cit] of compute unified device architecture, cuda (a software and hardware architecture that enables gpus to be programmed with some high level programming languages like c, c++ and fortran) [cit] . despite all those advances, the complex problem as dynamic energy efficient flexible flow shop scheduling has not been considered as far our knowledge is concerned. additionally, many parallel genetic algorithm (ga) implementations for solving optimization problems have shown their success [cit] as seen in the literature. therefore, the gpu based parallel ga for solving an energy efficient dynamic flexible flow shop scheduling problem remains an open research challenge based on the previous works, and the one that we seek to address in this paper."
"while the development of machine learning methods for protein classification has made significant progress [cit], many approaches to fold recognition and remote homology detection have been tested only in the binary classification setting. however, due to the existence of expert-defined class hierarchies for proteins, it is natural to consider hierarchical classification techniques [cit] . our aim here is to investigate, for several typical protein classification data sets, whether it is indeed beneficial to exploit this expert knowledge when applied in conjunction with a strong multiclass classification technique. as the main tool for our experiments, we use ensembles of nested dichotomies (ends) [cit], a method for reducing multiclass problems to binary classification task that has been shown to be very competitive with other strong multiclass learning techniques. nested dichotomies take class probability estimates of binary classification models built by a chosen base learner-we use logistic regression, c4.5 decision trees, bagged c4.5 decision trees, and support vector machines (svms) with platt scaling [cit] -and return probability estimates for all classes as output. a crucial feature of this method is that it can be easily adapted to make use of class hierarchies by constraining the nested dichotomies that are used. thus, we can perform a fair comparison of hierarchical and flat classification that enables us to measure the benefit of using a particular expert-defined class hierarchy. the primary aim of this paper is to provide such a comparison. although we obtain results that are competitive with those we could find in the literature, the development of new methods for protein classification is not the focus of this paper. however, the findings of our comparative study should prove useful in the development and evaluation of such techniques."
"brenda-enzyme classes. our third data set contains 10,253 enzymes from the brenda database (http://www.brenda. uni-koeln.de), representing the ratios of the enzymatic main classes (as they are commonly estimated) in this subset. the representation is more abstract than in our second data set: we only use the distribution of amino acids (the percentage of each of the 20 amino acids in the complete chain) and the percentages of three groups of amino acids (hydrophobic, hydrophilic, and neutral) [cit], since the distribution of hydrophilic and hydrophobic amino acids is characteristic for the 3d structure and the cell compartment. thus, for this data, we have a total of 26 features [cit] . as class hierarchy, we utilized the first three levels of the enzyme hierarchy. the enzyme classes at ec level 3 are the leaf classes, levels 2 and 1 are used as superclasses. 2 we selected only those classes that contained at least eight instances. the resulting dataset contains 115 different classes."
"koller and sahami [cit] present results for recursive hierarchical classification with \"hard\" classifications at the internal nodes. bayesian classifiers are built for these nodes, and feature selection is performed individually for each node. increased accuracy compared to a flat classifier is observed for those bayesian classifiers that admit modeling dependencies between features, and this is attributed to the localized feature selection. note that decision trees and bagged decision trees perform feature selection implicitly."
"the results shown in table 4 largely confirm the observations made for the protein classification data sets investigated in the previous section. there is little, if any, benefit in exploiting the expert-defined hierarchies. this can be seen by comparing hclass to multi and by comparing ends to ecnds and hends. considering accuracy, there is only one win for hclass versus multi, and there are no wins for the hierarchical variants of ends versus plain ends. considering the cost-based scenario, the situation is similar: multi always achieves lower costs than hclass. ecnds and hends achieve lower cost than ends on the sf95 data when the fold-based hierarchy is used, but the outcome is reversed on the sf40 data."
"in our experiments, we found that hierarchical classification improved classification performance on the artificial data set considered but provided no clear benefit on the real-world protein classification data sets. in particular, ensembles of unconstrained nested dichotomies outperformed their hierarchical classification variants in almost all cases, regardless of the base classifier used, and delivered the best performance overall. similar conclusions can be drawn from our experiments with remote homology detection. based on our observations we recommend that strong multiclass machine learning algorithms should be used as baseline methods when investigating the benefit of expert-defined hierarchies."
"the precedence among operations due to the jobs' processing cycles is presented by constraints (4) and (5), while constraint (6) establishes the precedence caused by the sequencing on machines. in addition, constraint (7) introduces the power's peak by an upper bound whereas the power consumption during a certain period is expressed by constraint (8) . constraint (9) gives the definition of a boolean variable u jst . it is equal to 1 if job j at stage s is being processed at time period t. finally, constraint (10) imposes the definition of rescheduling."
"considering accuracy, plain ends produce the best results, and table 5 compares their performance with results from the literature [cit] that are also based on 0/1 loss. 5 we observe that our estimates are comparable to results that have previously been obtained on this data. it is interesting to see that the svm-struct-based results do not show an advantage for hierarchy-based approaches either."
"the decision variables in this mathematical model are and . as two scheduling objectives are considered, it is formulated as a single additive objective function (1) by aggregating the total tardiness and the makespan with the weight wt."
"worst of cuda framework. in the first test, a discussion was conducted to obtain the reasonable island size and island amount for the related problem by inhibiting the premature convergence with a faster convergence speed. afterwards, our method displayed in test 2 showed that it could gain better results than the classical ga on cpu through the advantages from both the fine-grained ga and the island ga. but it also reduces the time requirements dramatically. moreover as seen in test 3, the proposed approach has better performance than the traditional static approach because of its flexibility. finally, test 4 demonstrated the response time to achieve the convergence point for large-scale problems. we suggest as well in this case decision-makers to obtain a feasible scheduling by making a trade-off between the solution quality and the time consumption."
"a straightforward approach to exploiting a given class hierarchy is to place a standard multiclass classification model at each internal node of the tree, built from the corresponding portion of the training data associated with a node. in the following, we will often refer to these internal classification models as \"base classifiers.\" we assume that these base classifiers deliver class probability estimates, so that we can obtain a probability of class membership for each of the subclasses at an internal node."
intra-alignment signatures are large alignment gaps in the reference or in the read. they can be found in the cigar strings of individual sam/bam entries.
"although many research works on scheduling problems have been studied in gpu literatures, none of them have so far, and to the best of our knowledge, considered energy saving strategies and dynamic environment completely. the above-mentioned efforts provide a starting point for exploring the gpu based parallel ga for solving an energy efficient dynamic flexible flow shop scheduling problem with competitive results and dramatical time reduction."
"pbhoney comprises two different variant identification approaches [cit] . the first approach, pbhoney-spots, exploits the stochastic nature of the errors in pacbio reads. it scans read alignments (usually produced by the read aligner blasr) and recognizes svs by an increase in error and a subsequent decrease in error along the reference sequence. the second approach, pbhoney-tails, analyzes the soft-clipped (i.e. unmapped) read tails from a blasr alignment. it extracts such tails from the blasr output and realigns them to the reference. then, svs are detected by clustering the resulting piece-alignments based on their location and orientation."
"hierarchical problems are particularly prevalent in the domain of biology due to the evolutionary development of biological objects: one often finds families of objects that share many properties with each other but not with objects of other families. this is also true in the domain of proteins. although they may differ widely in their details, they may share some characteristics in their three-dimensional structure. one of the well-established structural classifications of proteins, scop [cit], organizes the class hierarchy according to various criteria, including secondary structure content (on the structural class level) and evolutionary relatedness (on the fold and superfamily level). the task in fold recognition is then to assign the correct fold to a protein of unknown structure based on the known sequence of amino acids. thus, it is essentially a classification problem, with the classes on the second level of the scop hierarchy. another established hierarchy of classes of a subset of proteins, the enzymes, is the enzyme nomenclature [cit] . enzymes are proteins that exhibit specific catalytic functions (e.g., alcohol dehydrogenase and glycerol dehydrogenase, among others). a hierarchy is given because enzymes can be classified into subtypes. at the highest level, there is the type of enzyme, for example, oxidoreductases (ec 1). considering this group, there is then a certain type of oxidoreductases (ec 1.1: acting on the ch-oh group of donors), and of these, there is a certain subtype with a particular chemical reaction scheme (ec 1.1.1: with nad or nadp as acceptor). another well-studied problem that is more complex than fold recognition, remote homology detection, aims for the classification into the correct superfamily without the help of sequence similarity."
"differences between svim and the other tools were most prominent for insertions (i.e. interspersed duplications and novel element insertions). across all simulations and real data evaluations, svim outperformed the other tools by a wide margin. the difference to sniffles can be largely explained by their approach of analyzing split alignments. from such alignments, sniffles only calls insertions of novel elements but does not detect insertions of sequence existing somewhere else in the genome (e.g. from mobile elements). the detection performance of tandem duplications could only be evaluated in the simulated dataset again due to the lack of a gold standard. what we observed is a big difference in precision between svim and sniffles due to a large number of erroneous duplication calls by sniffles."
"smrt-sv scans pacbio alignments for sv signatures, such as spanned deletions, spanned insertions and soft-clipped read tails [cit] . clusters of such events are validated with a local de-novo assembly of the reads overlapping the locus and subsequent alignment of the assembly to the reference."
"as expected, recall and precision reached by the different tools depend heavily on tool parameters, particularly score or support thresholds. more relaxed thresholds (i.e. yielding more svs) increase recall but decrease precision while stricter cutoffs achieve the opposite. consequently, we ran all four tools with different settings of their most important parameter: for svim we applied different score cutoffs (0 to 100). sniffles was run with different settings of the min support parameter (1 to 60). for pbhoney-spots, we varied the minerrreads parameter and for pbhoney-tails we varied the minbreads parameter (both 1 to 60). we visualized the performance of the tools by plotting each parameter setting as a distinct point in figures 4-6. besides that one parameter, we used the default settings for all other tool parameters except pb-honey spots' spanmax parameter which we set to 100,000 (100kb)."
"we simulated 600 homozygous svs by altering the sequence of chromosomes 21 and 22 in the hg19 reference genome. more precisely, we implanted 200 deletions, 100 inversions, 100 tandem duplications, and 200 interspersed duplications with the r package rsvsim [cit] . the package estimates the distribution of sv sizes from real datasets and simulates the association of svs to various kinds of repeats. the resulting genome contained svs between 50bp and 10kbp in size. subsequently, reads were simulated from this genome to generate 10 different datasets with coverages between 6x and 60x with the tool simlord [cit] . simlord imitates the error model of smrt reads to simulate realistic pacbio reads."
"3. our method can not only achieve better performance than the traditional static approach, but also gain competitive results by reducing the time requirements dramatically."
"inter-alignment signatures are discordant relative alignment positions and orientations of a read's alignment segments. to illustrate this type of evidence, imagine an inversion that is spanned by a single read. the aligner will split the read into three alignment segments: one segment upstream of the inversion, another segment for the inverted region, and a third segment downstream of the inversion. due to the inversion, the middle segment will have a different mapping orientation than the other two pieces. this and other types of inter-alignment signatures are detected by svim in a heuristic fashion."
"what sets svim apart from existing sv callers is not only its improved detection performance but also its ability to distinguish three different classes of insertions purely based on alignment information. svim enables researchers to determine whether an insertion event is due to a tandem duplication, an interspersed duplication or the insertion of a novel element. moreover, svim identifies the genomic origin of duplications which facilitates their functional annotation, e.g. into different classes of mobile elements."
"in addition to the above baseline methods for flat and hierarchical classification, we also tested ensembles of nested dichotomies (ends) [cit] . ends are a general-purpose method for multiclass classification based on artificial binary hierarchical decompositions of the original multiclass problem. an example of an end for three classes is shown in fig. 2 . ends have been shown empirically to deliver performance competitive with error-correcting output codes [cit], a prominent binarization technique for improving classification performance in multiclass settings. ends are closely related to the hierarchical approach discussed above. the difference is that multiple artificial binary hierarchies-called \"nested dichotomies\"-are used instead of an expert-defined n-ary one. this ensemble of nested dichotomies is used to form predictions."
"to solve this np-hard problem, a priority based hybrid parallel ga with a predictive reactive complete rescheduling approach was developed. in order to have a short response in the dynamic environment, we proposed a parallel ga. it consists of a fine-grained ga at the lower level and an island ga at the upper level. this parallel ga is highly consistent with the hierarchy of threads and different types of memory"
"the experimental platform is based on intel xeon e5640 cpu with 2.67ghz clock speed. the gpu code implementation is carried out using cuda 8.0 on a nvidia tesla k40, with 2880 cores at 0.745ghz and 12 gb gddr5 of global memory. all programs are written in c, except for the gpu kernels in cuda c."
"in table 2, we can observe several tendencies: first, hierarchical classification methods outperform ends only on the synthetic data, regardless of whether we consider classification accuracy or misclassification cost as the evaluation metric. for all the base classifiers used, the performance of ends on the fold recognition and enzyme classification problems is consistently better than the performance of any of the hierarchical classifiers. the use of a given hierarchy helps only on the synthetic data, where feature space and class hierarchy were set up to match perfectly. this finding will be discussed in more detail below."
"to measure the influence of the input read alignments on sv calling, we also compared results for two long read aligners, ngmlr and min-imap2 (see suppl. fig. s8 and s9) . the results indicate that svim is relatively robust to the choice of the aligner but benefits slightly from the more accurate alignment of reads covering insertions and tandem duplications by ngmlr. sniffles, however, reaches considerably higher recall for insertions when analyzing alignments by minimap2 compared to ngmlr. visual inspection of the alignments revealed a difference in the way that reads covering insertions are aligned. while minimap2 expresses insertions mainly as long reference gaps in the cigar string, ngmlr tends to split reads at insertions. because sniffles does not call insertions of sequence existing somewhere else in the genome (i.e. interspersed duplications) from split alignments, it reaches higher recall with minimap2."
"we use two evaluation metrics: plain classification accuracy and misclassification cost. both were estimated using stratified 10-fold cross-validation in the case of the first and third data set, where no explicit train/test split was given. a symmetric cost matrix was defined based on the hierarchy of classes to compute the misclassification cost, giving higher costs to errors where the incorrect class is far away from the true class in the hierarchy. more specifically, the misclassification cost for a pair of classes, y i and y j, was computed as the minimum number of edges between the leaves representing y i and y j in the class hierarchy and the node representing their smallest common superclass. considering the hierarchy depicted in fig. 1b, the misclassification cost for confusing classes 1 and 2 would be 1; for 1 and 4, it would be 2."
"simulation cannot reflect all aspects of biological data. therefore, we used real pacbio and nanopore data for the second part of our analysis. this part consisted of three separate experiments. for the first two, we utilized a real 53x coverage dataset of the na12878 individual from a pacbio rs ii machine (genome in a bottle consortium; accession srr3197748) [cit] .. to assess the influence of sequencing coverage on sv detection performance, we produced a corresponding low-coverage subset of the dataset by sampling reads randomly to 6x coverage. with these two pacbio datasets, we performed two separate analyses. firstly, we evaluated our method with a published benchmark sample of 2676 highconfidence deletions and 68 high-confidence insertions [cit] . secondly, we implanted svs into the reference genome and aligned the pacbio reads to this altered reference. implanting an sv into the reference genome causes the original reads to contain the inverse of the sv that was implanted. with this approach, three types of svs were simulated: 1) 200 deletions were simulated by inserting sequence into the reference genome. 2) 100 inversions were simulated by inverting regions in the reference. 3) 200 insertions were simulated by deleting regions in the reference. unfortunately, duplications could not be simulated because this would have required the identification and alteration of existing duplications in the reference genome."
"for the definition of the scoring function, we use the same costs as for the cost-sensitive evaluation of our results (see the end of section 3.2): the distance between two leaf classes sharing a common superclass in the original hierarchy is one, the distance between two classes sharing a common super superclass is two, etc. a distance matrix is computed up-front for all pairs of leaf classes in the original hierarchy. the scoring function is then defined as the sum over all distances between pairs of leaf classes sharing a superclass in the new hierarchy."
"sniffles uses signatures from split-read alignments, high-mismatch regions, and coverage analysis to identify svs [cit] . to overcome the high error rate in the reads, it evaluates candidate svs based on features such as their size, position and breakpoint consistency."
"because svim, similar to other sv callers, analyzes read alignments it depends on the correctness of these alignments and inherits the limitations of the used read alignment method. one of these limitations originates from the repetitive nature of many genomes which keeps repetitive read segments from being mapped confidently. this can affect svim's sensi-tivity but might also cause svim to classify an interspersed duplication as a novel insertion if the inserted segment cannot be uniquely mapped. this might particularly affect mobile element insertions whose individual copies are highly similar. currently, svim is unable to detect chromosomal translocations and nested structural variants. we intend to add this functionality in the future. additionally, we plan to implement genotyping capabilities for the detected variants in an upcoming release of svim."
"structural variation is, besides single-nucleotide variation and small indels, one of the main classes of genetic variation. the influence of svs on human phenotype and disease makes them an important research target but their unique properties complicate their detection and characterization. particularly sv detection methods using short read technology suffer from low sensitivity. long read sequencing technologies such as pacbio smrt sequencing and ont nanopore sequencing have the potential to alleviate these problems. in this study, we introduced the novel sv detection method svim. it employs a three-step pipeline to collect, cluster and combine sv signatures from long reads. a comparison of svim with three competing tools on simulated and real data demonstrated that our method combines high sensitivity with high precision. across all tools, deletions were the easiest to detect. consequently, sniffles and svim reached almost perfect precision and recall [cit] . this level of recall was maintained regardless of sequencing technology and evaluation method (high-confidence callset or altered reference). svim generally required fewer calls to reach the same recall as the other tools indicating that the best-scoring svim calls are more enriched in true variants than the other tools' callsets of similar size. for the identification of inversions, sniffles and svim exhibited equally strong performance although svim showed a slightly higher recall in the evaluation with an altered reference. it needs to be noted, however, that the evaluation of inversions had to rely fully on simulation due to the lack of a suitable gold standard set."
svim's ability to link the genomic origin and destination of an interspersed duplication can yield interesting insights into the dynamics of genomic rearrangements. our analysis of the na12878 pacbio dataset with svim identified 27 high-confidence interspersed duplications with a
"we consider learning problems where we are given a set of instances as input data that each exhibit a class label. the class label defines to which group of proteins the corresponding instance belongs. the aim is to build a classification model using machine learning techniques that can be used to predict the class label for a new instance (i.e., protein). with more than two groups of proteins this is a standard multiclass classification problem (see fig. 1a ). however, the interesting aspect of protein classification is that protein classes can be organized into a hierarchy, giving rise to a hierarchical classification problem."
"in almost all cases, an adversely constructed hierarchy produces worse results than an expert-defined one; the exception is the case of hends on sf95 when used with hierarchies based on both protein classes and folds. considering the expert-defined hierarchy, we can also see that the performance is always worse when using classes and folds rather than using folds only. moreover, the results show that using methods based on ends almost always produce a better outcome than pairwise coupling."
"we explored whether more lenient (1%) or stringent (90%) overlap requirements for the calls would change the results (see suppl. fig. s3, s4, s6, and s7). as it turned out, the overlap requirement had little effect on sniffles and svim. only pbhoney-spots produced substantially worse results for more stringent overlap requirements suggesting that the tool has trouble finding accurate sv breakpoints."
"to characterize the full spectrum of human genetic variation, longread sequencing technologies that generate reads with an average length of tens of kilobases show many advantages. the long reads can be mapped with greater accuracy which enables the sequencing of repetitive and low-complexity regions [cit] . unlike with short reads, svs are often spanned by a single long read. this enables the direct detection and full characterization of the svs. consequently, several studies confirmed that a substantial number of svs that are missed by short-read approaches can be identified with long reads [cit] . two commercial long-read sequencing solutions exist to date: single-molecule real-time (smrt) sequencing by pacific biosciences (pacbio) and nanopore sequencing by oxford nanopore technologies (ont). both technologies have the same drawbacks: high error rates of approximately 5-15% with dominating indel errors and still high costs compared to short read sequencing."
"read alignments alone are not sufficient to detect and characterize svs. dedicated sv callers are needed to collect and interpret evidence from the read alignments. recently, three methods have been developed for calling svs based on long reads [cit] . pbhoney and smrt-sv are designed specifically for pacbio reads while sniffles supports pacbio and ont reads [cit] ."
"when interpreting any of these results, it is instructive to consider confidence intervals for the estimates. this is important because the test sets for sf40 and sf95 are both very limited in size. we calculated 95 percent confidence intervals for the error rate of ends on the two data sets. for the sf95 data, with a test set of 346 instances, we obtained a confidence interval of [12.17 percent, 19. 81 percent] given the observed error rate of 15.61 percent. for the sf40 data, with a test set of 238 instances, the interval is [10.05 percent, 18.84 percent], given the observed error rate of 13.87 percent. intervals of similar size can be obtained based on the other results in table 5 . considering the substantial overlap in the intervals obtained in this fashion, it is not clear whether any of the observed differences actually correspond to genuine differences in performance."
"as tardy jobs typically cause penalty costs [cit] and have a great influence on customer satisfaction, the weight wt indicates the priority of the first objective."
"recently, there has been growing interest in reducing the energy consumption in manufacturing processes. several works tried to reduce the peak power in parallel multi-machine contexts. [cit] presented a multi-objectives mixed-integer programming model of the flow shop scheduling problem that considers peak power load, energy consumption, and associated carbon footprint in addition to cycle time."
"the collection of signatures from the alignments is only the first step to accurately detect svs. subsequently, signatures from multiple reads need to be merged and criteria have to be found to distinguish correct signatures from multiple types of error artifacts (e.g. sequencing error, alignment error). to achieve this, we combine a graph-based clustering approach with a novel distance metric for sv signatures. the aim is to merge signatures of the same sv even if their positions vary slightly due to sequencing or alignment errors. at the same time, signatures from separate svs need to be kept separate even if the two svs lie close to each other."
"the comparison between different tools was complicated by the fact that each tool is designed to detect different sv classes. pbhoney is able to detect deletions, inserted regions, inversions and translocation breakpoints. sniffles is additionally capable of identifying tandem duplications and complex events. because only svim distinguishes between duplications and novel element insertions, we compared the tools on four common basic sv classes in the simulated datasets: deletions, inserted regions (i.e. inserted sequence from duplications and novel element insertions), inversions and tandem duplications. because sniffles tends to call intra-chromosomal duplications as very large deletions or inversions (see github.com/fritzsedlazeck/sniffles/issues/23), we omitted deletion and inversion calls by sniffles that were larger than 100kbp to ensure a fair comparison. to obtain calls of inserted regions from svim, we use the union of its interspersed duplication and novel element insertion calls."
"sv calls. to reach a recall of more than 50%, [cit] / 2577 calls (53x/6x coverage) while sniffles needed 4320 / 6333 calls. pbhoney-spots needed even 5062 calls (53x coverage) and pbhoney-tails did not reach this level of recall at all. a recent study by the human genome structural variation consortium (hgsvc) used a multi-platform de-novo assembly approach for sv detection and found an average of 12,680 deletions per individual [cit] . when we select tool thresholds closest to this mark, svim, sniffles, pbhoney-spots and pbhoney-tails recover 97%, 97%, 80% and 46% of the high-confidence deletions from the full coverage dataset, respectively. all tools miss high-confidence calls across the entire size range (50bp -140kb). but while the false negatives of the first three tools are evenly distributed across the size spectrum, pbhoney-tails particularly misses small events. for instance, it misses all high-confidence calls smaller than 100bp and 69% of calls between 100bp and 500bp but only 24% of calls between 500bp and 1kb."
"all three methods regard structural variation (i.e. deletions, insertions, inversions) as rearrangements occurring in a single genomic locus. however, structural variation often involves multiple genomic loci, such as for a mobile element which is reverse-transcribed from a source region and inserted at another location. the higher read lengths of pacbio and ont reads allow to link both loci much more efficiently and confidently than was possible with short paired-end reads. nevertheless, existing methods ignore this type of information and are only able to detect the isolated destination location of the mobile element insertion."
"in this study, the aim is to investigate whether predictive performance can be improved by exploiting the class hierarchy. an alternative is to discard it and treat the problem as a standard multiclass classification problem, based on the setup shown in fig. 1a . the problem can then be tackled with standard multiclass algorithms."
"the total tardiness and the makespan with a peak power limitation are analyzed in this paper while considering a dynamic environment in the flexible flow shop. a predictive reactive complete rescheduling approach is adopted to represent the optimization problem. furthermore, due to the fact that an adequate renewed scheduling plan needs to be obtained in a short response time in the dynamic environment, a priority based parallel ga on gpus is implemented. the efficiency and the effectiveness of the proposed approach are validated through computational tests. specially, the contributions of our work are summarized as followed:"
"a hierarchical classification problem can be viewed as a problem involving a large number of classes, where some subsets of classes are more closely related than others. as an example, consider the hierarchical classification problem in fig. 1b . the problem consists of three superclasses, a, b, and c. each of these three superclasses contains three subclasses. the given hierarchy states that the subclasses from one superclass are more strongly related to each other than to subclasses of other superclasses. for instance, class 1 is related to class 2 but not to 4. in the following, we will call the classes without subclasses leaf classes."
"svim has been implemented in python and is available at github.com/eldariont/svim. it can be easily installed via bioconda or the python package index (pypi). as input, svim expects either raw reads (in fasta or fastq format) and a reference genome (in fasta format) or already aligned reads in bam format. it outputs detected svs in five separate bed files (one for deletions, interspersed and tandem duplications, inversions and novel insertions, respectively). additionally, a vcf file with all sv results is produced."
"while simulated datasets enable the comprehensive comparison of tools in a controlled and precise manner, they cannot reflect the full complexity of real sequencing data. therefore, we analyzed a publicly available 53x coverage dataset of a human individual from a pacbio rs ii machine and a random 6x coverage subset (see methods section). to evaluate the detection performance of our tool, we first used a published benchmark set of 2676 high-confidence deletions and 68 high-confidence insertions."
"to perform the experiments we used the weka machine learning workbench [cit], enhanced by the hierarchical classification methods discussed above. ten ensemble members where used in each method based on ends. for the experiments in this section, we used logistic regression, unpruned c4.5 decision trees and bagged unpruned c4.5 decision trees as stand-alone classifiers and as base learners for the other methods. for bagging, we used 10 iterations. in addition to the two groups of methods discussed earlier-one that does not use the class hierarchy at all and one that does-we also introduce a third group, namely, one based on using an artificial class hierarchy that is constructed as adversely as possible with respect to the original hierarchy. in this adverse hierarchy, direct siblings exhibit the greatest possible distance in the original hierarchy. details of the construction method are explained in the appendix of the paper."
"svim analyzes read alignments in sam/bam format [cit] from a read aligner. modern aligners, such as ngmlr and minimap2, try to find good linear alignments of entire reads. nevertheless, they will split a chimeric read if its different segments can be better aligned separately. due to these split alignments, the sam/bam output from these aligners can contain multiple alignments for each read (one for each aligned read segment). svim extracts signatures for svs from the sam/bam file by analyzing one read at a time. we define sv signatures as discordant alignments of a read that point to the presence of one or several possible svs in the sequenced genome. svim searches for two types of signatures:"
"the table has results for plain classification accuracy as well as the average misclassification cost across all test instances. costsensitive prediction was employed to generate values for the latter statistic: instead of predicting the class with maximum probability, this approach predicts the class with minimum expected misclassification cost [cit] . because all the learning schemes we use produce class probability estimates, we can compute the expected misclassification cost for each prediction based on these estimated probabilities, and hence, the class with minimum expected cost."
"we tested two stochastic approaches for the maximization of this score by assignments of leaf classes to the original hierarchical structure. in both cases, the starting point is a random permutation of the leaf classes. in the first variant, we \"fill in\" (i.e., assign) the leaf classes in the order of the random permutation. in the second variant, the assignment is made greedily and in a more goaloriented fashion: we assign the next leaf to the one superclass where the overall score is maximized. although this step is done greedily, the whole procedure is still stochastic as it is based on a random sorting order of the leaf classes. both approaches are repeated multiple times (depending on the dataset and the setting for 10 to 1,000 runs), and the overall maximum is chosen."
"figure 3: read signatures for an interspersed duplication and a novel element insertion. a genomic segment (yellow arrow) has been copied from locus 1 to locus 2a in an individual genome. additionally, a novel genomic segment (gray arrow) has been inserted in locus 2b. two reads are generated from the individual (top) and mapped to the reference genome (bottom). the first read (blue-yellow) consists of three segments. they are mapped individually to the reference genome. the two blue segments are mapped to locus 2a exhibiting an insertion signature. the yellow segment is mapped to locus 1 indicating the origin of the insertion. the second read (orange-gray) exhibits a similar insertion signature at locus 2b but as the inserted gray segment is unmapped its origin cannot be determined."
"in our experiments, we compare standard unconstrained ends and ecnds. we also consider a third approach, where we use the standard hierarchical classification approach from the previous subsection and apply unconstrained ends at each internal node of the n-ary tree. we will refer to this latter approach, where ensemble construction and model averaging is performed inside the expert-defined hierarchy, as hierarchies of ensembles of nested dichotomies (hends). it provides us with another method of evaluating the benefit of a given expertdefined hierarchy. an overview of the learning schemes compared in this study is given in table 1 ."
"in this section, we describe experimental results obtained with the above flat and hierarchical classification methods. the first part focuses on fold recognition and enzyme classification and the second part on remote homology detection."
"the results of our experiments are summarized in table 2 . the first two columns contain the results for methods that do not take the class hierarchy into account: the leaf classes are used to define a standard multiclass problem. the first column has results for the native multiclass methods (multi), i.e., plain logistic regression, c4.5 decision trees, and bagged c4.5 decision trees. the second columns shows results for ends using the three different techniques as base classifiers. in the next three columns, results for hierarchical classification can be found. the first of these columns has results for the basic hierarchical approach using the above three native multiclass classifiers in the internal nodes (hclass), and the next two columns have results for the two hierarchical variants of ends (ecnds and hends). the last three columns contain the results of the same hierarchical methods applied to adversely constructed hierarchies. note that, due to the large number of classes in the first and third dataset, logistic regression failed to terminate in a reasonable amount of time."
"there are different ways of exploiting parallelism in ga: master-slave models, fine-grained models, island models, and hybrid models [cit] . fine-grained models can perform well due to the larger genetic diversity obtained by dividing the population into a number of subpopulations [cit] . island models are the most famous for the research on parallel ga. populations on the islands are free to converge toward different sub-optima with a faster improvement of the average fitness [cit] and a migration operator can help mix good features that emerge from the local island. to obtain a good speedup with cuda and to combine the advantage of fine-grained models and island models, we establish the hybrid model presented in fig. 4 with a fine-grained ga at the lower level and an island ga at the upper level. a correspondence between the parallel hybrid ga components and the hierarchy of cuda threads is displayed by table 3 . it turns out that this hierarchy is highly consistent with the hierarchy of threads and different types of memory of cuda framework."
"as described in the methods section, we obtained another reliable gold standard set of svs (deletions, inversions, insertions) by implanting svs into the reference genome and aligning the pacbio reads (53x and 6x coverage) to this altered reference. we evaluated all combinations of the three sv types and the two coverages. svim outperformed the other tools (see fig. 6 ) in all six of these settings. in the recovery of deletions and inversions, svim reached a substantially higher recall than pbhoney. it also needed fewer sv calls to reach similar recall than sniffles while the difference decreased for higher recall. the most striking difference was observed for the detection of insertions. while svim reached a recall of 84% and 43% with 20,000 calls (53x and 6x coverage, respectively), pbhoney-spots reached 61% and 25% and sniffles detected only 57% and 29% with the same number of calls. for full coverage, svim needed 2480 calls to reach a recall of 50% while sniffles and pbhoney-spots needed both more than 10,000 calls."
"the cuda framework is chosen to parallelize the ga on gpus in this paper. it is a single instruction, multiple threads (simt) parallel programming model. the parallel threads are grouped into blocks which are organized in a grid [cit] as shown in fig. 3 using the local memory, the shared memory and the global memory respectively."
"q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q figure 6 : comparison of recall from na12878 reads aligned to an altered reference genome. for each tool and different thresholds, the number of sv calls with score above the threshold (log-scale) is plotted against the recall. the upper and lower panels show performance on the full dataset and a randomly sampled 6x coverage subset of the data, respectively. in all six panels, svim outperformed all the other tools and reached substantially higher recall for similar numbers of calls. the improvement was most prominent for insertions. recall was calculated using a required reciprocal overlap of 50% between variant calls and the original implanted variants."
"there is a significant amount of work on hierarchical classification in machine learning, particularly in the area of document classification. we will review some of the relevant literature in the following. kiritchenko [cit] has an excellent, comprehensive, review of work in this area."
"in this study, we compared our tool, svim (v0.4.1), to three other sv detection methods: pbhoney-spots, pbhoney-tails (both pbsuite v15. 8.24) and sniffles (v1.0.8). all three tools are designed for the application on long read sequencing data. for sniffles and svim, reads were aligned with ngmlr (v0.2.7) or minimap2 (v2.12-r836-dirty). for pbhoney, reads were aligned with blasr (v5.3.4323a52). we did not compare against short read sv callers because they have been shown to exhibit lower recall than methods relying on long reads [cit] . we also did not compare against smrt-sv because it is not a stand-alone tool but a software pipeline applying several alignment, detection, and assembly steps with various other tools. it detects only three sv classes and is computationally more demanding than pure alignment-based tools. we evaluated all tools on two types of data. firstly, we generated a simulated genome from which we sampled in-silico pacbio sequencing reads with known svs. this provided us with a complete set of fully characterized svs for evaluation. secondly, we used publicly available sequencing reads from pacbio and nanopore sequencers. we compared the precision and recall of the three methods. precision is defined as the fraction of detected svs that are correct (requiring 50% reciprocal overlap between detected and correct svs). recall is defined as the fraction of correct svs that have been detected (with 50% reciprocal overlap). results for a more lenient and a more stringent overlap requirement of 1% and 90%, respectively, can be found in the supplementary material. both precision and recall require a suitable gold standard set of high-confidence svs for the given genome (i.e. a set of correct svs)."
"svim implements a pipeline of three consecutive components (see fig. 2 ). first, sv signatures are collected from each individual read in the input sam/bam file (collect). secondly, the detected signatures are clustered using a graph-based clustering approach and a novel distance metric for sv signatures (cluster). thirdly and lastly, multiple sv events are merged and classified into higher-order events (i.e. events involving multiple regions in the genome) such as duplications (combine). the three components are explained in the following."
"the na12878 datasets are more realistic than the simulated dataset but impose the limitation that there exists no complete gold standard set of svs. as a consequence of using an incomplete gold standard for evaluation, precision could not be accurately measured. putative \"false positives\" could have been true but simply not contained in the incomplete gold standard. therefore, we compared the tools only based on their recall in relation to the number of calls."
"the paper is organized as follows: section 2 describes the hierarchical and flat classification methods that we evaluated in our experiments. section 3 has experimental results for synthetic, fold recognition, enzyme classification, and remote homology data. section 4 has a discussion of our findings. in section 5, we briefly review related work on hierarchical classification. section 6 has some concluding remarks."
"the remaining sections of this paper are organized as follows. section 2 introduces related works. section 3 describes the research problem and the mathematical model. section 4 presents the priority based hybrid parallel ga for solving the energy efficient dynamic flexible flow shop scheduling problem. section 5 illustrates the numerical experiments and result analysis. finally, section 6 states the conclusions."
"the predictive reactive method is the most common dynamic scheduling approach used in manufacturing systems [cit] . to solve the edffs, operations are assigned to machines in order, following the original schedule until the reschedule point. new arrival jobs and uncompleted operations of original jobs are processed in terms of the updated schedule executed by the optimization algorithm within a short response. a hybrid parallel ga on gpus is proposed for solving the problem with a complete rescheduling strategy which is better in maintaining optimal solutions, but is rarely achievable in practice due to the prohibitive computation time [cit] . fig.2 summarizes the flow of the predictive reactive complete rescheduling process. the genetic algorithm (ga) is a stochastic search algorithm based on the principle of natural selection and recombination [cit] . however, there is an increase in the required time to find adequate solutions when ga is applied to more complex and larger problems. parallel implementation is considered as one of the most promising choices to make it faster."
"these cases have confirmed that the parallel ga on gpus has good performance in solving scheduling problems. however, it is also revealed that few studies have been conducted to integrate gpus in dynamic energy efficient scheduling problems, because of the complexity that is caused."
"recall was calculated using a required reciprocal overlap of 50% (deletion calls) and 1% (insertion calls), respectively, between variant calls and the gold standard variants."
"as described in the methods section, we implanted svs from four different classes into a reference genome. reads sampled from this synthetic genome were then analyzed with svim, pbhoney-tails, pbhoney-spots and sniffles. results for the 6x coverage homozygous dataset can be found in figure 4 . for a comparison of results across all coverages from 6x to 60x see suppl. fig. s2 . regardless of coverage, svim achieved substantially better results than all other tools in the recovery of inserted regions and tandem duplications. with 6x coverage and homozygous svs, svim reached average precisions (ap) of 86% (inserted regions), and 83% (tandem duplications) for the two classes while the second best tools, pbhoney-spots and sniffles respectively, reached 25% and 54%. in the recovery of deletions and inversions, svim and sniffles reached equal results with average precisions of 94% (deletions) and 90% (inversions), respectively. in our experiments, pbhoney-tails performed very poorly across all settings. it did detect only very few inserted regions, suffered from very low recall for inversions and poor precision for deletions. all these trends remain true for higher coverages as well (see suppl . fig. s2 )."
" the selection operation: on the basis of the value of fitness function, the larger fitness an individual has, the higher the chance it has to be chosen in the next generation. because the 2d grid is adopted as the spatial population structure where each grid point contains one individual, the local asteroid selection is hired to make the selection operation. moreover, since gpu texture caches are designed to gain an increase in performance accelerating access patterns with a great deal of spatial locality [cit], we define the neighborhood on the grid always contains 5 individuals: the considered one and neighboring individuals as displayed in fig, 9 . among these individuals, the tournament selection is implemented where the individual with the largest fitness value is the winner of each tournament and is selected to replace the considered individual. meanwhile, a risk that it converges to the local minima can be eliminated by its cooperation with the local asteroid selection. in details, a 2d single point crossover is executed for the target machine matrix and the priority matrix respectively if a specified probability is satisfied. as the randomly generated values in the priority matrix is unique, a correction step is required to replace the duplicate values by the missing values in ascending order. an example shows the procedure in fig. 11 ."
"in a third experiment, we compared the 53x coverage pacbio dataset of the na12878 individual with a 26x coverage nanopore dataset of the same individual [cit], release 5). we evaluated our method with the highconfidence callset described above and analyzed the overlap between the three callsets (pacbio, nanopore and high-confidence callset)."
"to analyze the performance of the proposed algorithm, test 1 and test 2 are conducted in terms of an energy efficient ffs without considering new arrival jobs. test 1 configures the parameters of the proposed hybrid ga, while test 2 shows its efficiency and effectiveness compared to the classical ga [cit], the cellular ga [cit] and the number of new arrival jobs is decided by the ratio of the rescheduling point on the makespan in the original schedule times the amount of original jobs. this is designed to keep the total amount of jobs waiting to be scheduled roughly consistent."
"therefore, a lot of traditional scheduling strategies considering minimizing the total energy consumption have been studied [cit] . meanwhile, some efforts have been made on taking peak power into account, because electricity consumption and operating costs of manufacturing plants are usually charged based on peak power demand from electricity providers [cit] . however, most of the research works only concentrate on establishing a mathematical model for solving the optimization problem in a static environment. but in fact, scheduling problems are dynamic in the real world with uncertain new arrival jobs after the start time. few works take [cit] reactive approaches into consideration for supporting energy efficient dynamic systems. moreover, they care only about the improvement of algorithms to gain better solution quality, while ignoring the time consumption of the implementation of such approaches. without a doubt, a method proposing an adequate rescheduling plan in a short response time is greatly desired in this case."
"a \"multitiered\" approach similar to hclass has also been used to test various feature subsets and native multiclass methods for hierarchical classification of proteins [cit] . different from a predecessor paper by a similar group of authors [cit], this work does not consider methods for the reduction of multiclass to binary classification problems."
"as a ga converges when most of the population is identical or the diversity is minimal [cit], there is no need to execute the algorithm for more generations after the convergence point. for the edffs, it is important to identify the convergence point and its corresponding execution time for different size problems. three different size problems are considered in this test. the convergence trends of the small size, the medium size and the large size problem instances are described in fig. 15, fig. 16, generations, while the value for the medium size and the large size problems is around 400 and 500. as the complexity increases when we raise the size of the problem, the execution time per 10 generations for those problems is about 1.24s, 223.37s and 4256.14s respectively. therefore, to get solutions after convergence for the small size problem, it takes 6.2s whereas the medium size and the large size problems need much longer time as 8934.8s and 212807s. due to the dramatically increasing execution time for large-scale problems, the parallel ga may get a feasible solution before achieving the convergence based on decision-makers' consideration, namely a trade-off between the solution quality and the time consumption."
"for rerouting schemes using a single path and additional capacity [cit], the limits of the physical resources are quickly reached, which paralyses the network. for loop-free alternative mechanism-based methods [cit], we are not certain of rerouting traffic to all destinations; doing so only helps to reduce the number of lost packets in an ip network. not-via addressing [cit] and tunnelling [cit] mechanisms require encapsulation and de-encapsulation of packets, whereas in a multiple routing configuration mechanism [cit], the packets must carry configuration information. with the appearance of optic networks, methods that modify the packets are not recommended but can instead be used to optimize the usage of resources in network virtualization. multipath rerouting [cit] using spare capacity in the network can induce a capacity saving of up to 11% in randomly generated networks, but lack of spare capacity due to the existence of multiple virtual planes on a substrate network can undermine this result in network virtualization."
this section provides a rerouting mathematical model based on the following assumptions: the graph is assumed oriented and symmetric. there are at least two disjoint arc paths between any two nodes of the graph. there is only one link failure at a time.
"equation (3) ensures that there is no conflict in the rerouting, i.e., the incoming flows to node n to destination d must follow the same rerouting paths. if we use arcs (i, k 1 ), (i, k 2 ), (i, k 3 ), ... for rerouting to destination d, there is at most one output (k s, j s ) for each."
"equation (9) is a constraint for the relationship between two rerouting paths that avoids a conflict (see the definitions of variables x and y). because x and y are binary variables, with the same quadruplet (e, f, g, h) and same destination d, we can deduce from (9) that (x) will take the maximum value of (y). we use the sum of failures divided by the cardinal to reduce the number of constraints."
"when node 4 fails, flows coming from nodes 1 and 7 must be rerouted. the failure of node 4 implies a simultaneous failure of links (1-4), and (4-6). therefore, we must reroute the two flows (1-4) and (7-4) to destination 6 without conflict. to solve this type of failure, two solutions are possible:"
"this approach is similar to the second one presented in section 7 for the node failure problem, and it enables all nodes that detect a failure to initiate the rerouting process. our rerouting scheme for node failure can also be used here. when several link failures occur simultaneously during the rerouting process, we can use flow splitting each time to find spare capacity lacking in the network. consider the example network of fig. 14 to illustrate our rerouting strategy for the case of simultaneous and non-adjacent two-link failures."
"for each destination, we determine the nominal routing tree from each node towards this destination (see fig. 10 ). the failure of node 3 generates simultaneous failures of links (5-3), (6-3) and (3-1) (dotted links). nodes 5 and 6 will detect the breakdowns of links (5-3) and (6-3), i.e., we have two flows to reroute. these failures split the graph into two parts: the blue part and the red part (see fig. 11 )."
"equations (6), (7), and (8) are the flow constraints for the continuity of the alternatives paths. equation (6) is the constraint of flow conservation. referring to (7), the total amount of entering traffic in n is equal to the total outgoing traffic of g; because of flow splitting being used for a given failure and a destination, we could have multiple incoming streams and possibly multiple outgoing streams."
"we speak about simultaneous multiple link failures when several links fail at the same time. the case considered in this section concerns non-adjacent links to the same single node. in this case, there are multiple nodes, each of which detects a link failure as in the simple node failure case. this type of failure can also be handled using either of two methods:"
"the rest of this paper is organized as follows. the next section presents motivations for traffic splitting. section 3 provides a full description of our restoration scheme for a link failure configuration. our mathematical model is described in section 4. section 5 presents numerical results of implementations. section 6 studies the application of our rerouting scheme to the single-node-failure problem. section 7 extends our work to simultaneous multiple-link-failure situations by providing a solution for some conflict problems. finally, section 8 ends the paper."
"we illustrate this last point by an example. figure 1 shows a network with 6 nodes and 8 links. the numbers carried by each arc represent spare capacity available on the links. there is only one path (of traffic) 6-5-2-1 going through link (5, 2). other paths are not shown for clarity. let the traffic of this path be 8 units, and let all links be of equal length. in link restoration, a faulty link is replaced by one alternative path."
"to avoid loops and conflict problems, the alternative paths should not contain any arc of nominal routing in the red part of the network. equation (4) ensures that condition."
"to the best of our knowledge, presently our work is the only one that shows how to effectively solve simultaneous multilink failures using flow splitting methods, thus providing an improvement in qos of computer networks."
"since its creation, the internet has brought innovations and success to industry, economic and research fields; however, deployment of any new, radically different technology and architecture is becoming highly difficult, a situation that cloud computing can mitigate. that effect is what we call internet ossification [cit] . to fend off ossification, studies have proposed rethinking the architecture of the actual internet [cit] . however, network virtualization is the most promising approach to addressing current limitations of the internet and supporting new requirements [cit] . its principle is to implement multiple virtual routers on each physical machine and to interconnect them through substrate network architecture. this implementation allows virtual networks to have different logical topologies from that substrate network, and each of them behaves as a true network in which it is possible to implement different routing protocols and services. as in the substrate network, failures could occur in virtual networks; in this case, rerouting mechanisms can be implemented to forward traffic by using available resources in the virtual network or additional ones taken from the physical network. this additional resource could cause dysfunctional risks inside another virtual plane."
"assume that faulty link (5, 2) is replaced by path 5-3-1. because links (5, 3) and (3, 1) have only 5 and 7 spare capacity each, both links need, respectively, 3 and 1 more spare capacity to make the restoration possible. this example proves that the use of traffic splitting in the link restoration scheme can result in lower spare capacity requirements but in the context of a virtual network, it could be very important to reduce additional capacities added to the links to avoid disturbances due to the rerouting scheme. now, let us consider the traffic-splitting version of link restoration (fig. 2 ). there are two alternative paths, 6-5-3-1 and 6-3-1, for link (5-2), and each alternative carries 4 units of traffic. (as in the model [cit], the general design principles presented in this paper are valid for any unit of bandwidth capacity for virtual networks.) with this second method, there is no need for more spare capacity."
"the nominal routing tree is shown, and the destination node is labelled 1. figure 15 shows two link failures named p1 and p2 occurring at the same time."
"equation (8) ensures that in the blue part, if a path uses an arc of the nominal routing tree, it must continue until destination d."
"we speak of node failure when some flow can no longer go through a given node in the network. this situation can be caused by overflow traffic in this node or a physical failure of the given node. because of the two-link connectivity included in our hypothesis, a node failure leads directly to the outage of at least two or several links; in other words, node failure can be treated as a simultaneous multiple link failure. in this case, failure will be detected by all nodes connected to the failed node. figure 9 presents the failure of node number 4."
"multiple link failures studied in the case of simple node failure involve links adjacent to that node, but we also have cases of simultaneous multiple link failures not adjacent to the same node."
the objective function of our model has the general form: where c is the set arc and f is a function over c giving the additional capacity needed for a chosen arc. the problem described by eq. (15) is convex in the set c and the function f is convex.
"the failures p1 and p2 create the red parts r1 and r2. p1 is detected by the node labelled 4, and p2 is detected by the node labelled 9. the rerouting scheme of p1 can be through bridges (8-3) and leading to the paths 4-5-8-3-1 and 4-5-8-11-7-2-1 (see fig. 16 ). flows can be split at node numbers 8, 11, 2 and 3. concerning rerouting of p2, link (12-13) can be considered a bridge that connects the red part r1 to r2 in addition to and . after the link failure pair (p1, p2), if another occurs (pair (p3, p4) for example), the rerouting will be done based on the previous one to avoid conflict."
"we also perform an additional capacity consumption test for the previous four networks; the results are shown in table 3 . this table consists of five columns. descriptions of the rightmost three columns are as follows: \"unused ca\" represents the additional capacity available in the network; \"our used ca\" represents the total of additional capacity used by our strategy for linkfailure handling; and \"used by x\" represents additional capacity used in the network by method [cit] . the result units are expressed in seconds."
"our aim in this paper was to propose a rerouting approach to handle the single link node failure and simultaneous multiple link failure problems in a network of switches in the context of network virtualization. we proposed a conflict-free rerouting scheme that can ensure that, whatever the case of link or node failure, traffic will be rerouted to the destination. the proposed method is based on local reaction of nodes placed at the extremities of the failed link, whereas the other nodes need not know about the failure or take any particular action. thus, the implementation is particularly easy. the flow splitting strategy used when there is insufficient spare capacity on links helps to reduce additional capacity added to the network. we proved that there exists a restoration scheme without conflict in the network and provide a mathematical model that permits calculation of the rerouting scheme with optimization of the sum of additional capacities needed for one virtual plane. we also proposed a rerouting solution using several planes to solve cases of potentially unsolvable conflicts when we use only one plane. further work will address congestion management into the nodes implied in the rerouting and routing table updates without disturbing the network."
"consequently, the possible rerouting paths will be 5-2-1, 5-8-9-7-4-1 and 5-6-4-1 for flow from the failure of link (5-3); concerning the flow from the failure of link (6-3), the possible rerouting paths could be 6-8-9-7-4-1 and 6-4-1. we can conclude that local reaction required by ipfrr strategy can also be preserved when addressing simple node failure situations through our rerouting scheme."
"to achieve the local connectivity recovery, there is a filter similar to an agent, running inside each switch (example of openflow switches) used in network architecture like ours. this agent detects the port states and acts as needed. for classical switches, there are control mechanisms provided to check that ports status."
"in this work, flow splitting is implemented by building little flow of packets from an original one. to face reordering challenge, we use the numbering packets each time the flows are split, because this method does not modify significantly the packets headers."
"constraint (5) ensures that there will be no loop in the network. for a given destination and a given failure, an alternative path could contain a loop if the flows go from a node with a larger index number to another one with a smaller index. this constraint prohibits this type of problem."
"-reduction of transit delay and packet loss rate, because the flows are more able to reach their destination nodes, thus improving the network qos; -improvement of the packets routing delays: since the original flow is split into several lighter-sized streams, they can be transported more quickly to the destination; -improvement of load balancing distribution [cit], leading to prevent or decrease congestion risk across the network. this is a well-known benefit of flow splitting in computer networks. -extension of the lifetime of a network by allowing more flexible and efficient resources allocation; -better economy of the substrate network resources supporting virtual networks. since virtual networks are built on a physical network infrastructure, it is necessary to avoid an abuse of these resources with the risk of causing the hosted virtual networks malfunctions."
"each link failure because of a node failure is handled locally and instantly by each node that identifies a link failure. the incurred risk in this strategy is the looping problem during flow rerouting; however, assuming our constraint imposing traffic from nodes with a lower label on another one with a higher label, cycles can be avoid. our rerouting scheme for a simple link failure can be used to fix simple node failure situations. recall that we speak about simple node failure when only one node fails at a time. our rerouting strategy for simple node failure problems uses this approach. the following illustrates our rerouting scheme for a simple node failure with an example. fig. 10 shows the nominal routing tree of an example network, and fig. 11 presents link failures (dotted links) resulting from node 3's failure. fig. 12 presents a cyclic problem resulting from a node failure, and fig. 13 illustrates the efficiency of our solution to solve this cycle problem."
"virtual networks use physical network resources to achieve their needs. in the case of link or node failure, spare capacity available in safe links is commonly used to restore traffic. however, this spare capacity might not be sufficient to carry the entering traffic; this situation represents a lack of spare capacity and is the main motivation for using flow splitting in the network. the idea is to split an original flow into multiple parts such that they can be forwarded easily through the network. this method induces numerous potential advantages as:"
"equations (10), (11), and (12) are the capacity constraints. for each failure of edge (n, m), the constraint in (10) assumes rerouted paths for arcs (n, m) and (m, n), and only trees that contain the arc failure are involved. they also consider the released bandwidth on the initial routing paths. equations (11) and (12) are special cases of (9) for the nodes that detect the failure, node n and node m. finally, (13) and (14) indicate that the variables take binary values."
"more precisely, for an incoming flow from a neighbour and a given destination, the scheme will assign the potential output ports. in the case of failure, only the upstream node must react by directing the disturbed traffic to one or many of its neighbours. the traffic is routed according to the filter programmed in each node of the network. traffic can be split anytime at the level of each node if needed. hence, the proposed scheme needs only a local reaction, making its implementation particularly easy in distributed environments. this local reaction helps the network operate normally and can solve the problem of transient failures. a transient failure is a failure whose duration is short, less than 10 min, whereas the duration of a persistent failure is longer [cit] . when the failure is determined to be persistent, the controller can recalculate the routing table for all nodes in the network. to avoid the rerouted traffic of a failure causing disturbances to another part of network, additional capacity is added to all arcs. because this additional capacity is added in the physical network and is exploited by several virtual layers, it is necessary to minimize it. the mathematical model that we propose in this paper can calculate the rerouting paths and optimize the total additional capacity injected into the network."
"in this section, we present our rerouting scheme that addresses the case of a link failure. as presented in section 2, traffic splitting occurs because spare capacity is lacking in the network, but implementation of that approach in our rerouting scheme helps to solve many other problems:"
we illustrate this rerouting scheme in figs. 3 and 4. these figures represent a network with 6 nodes and 8 links. the original graph with thespare capacity of each link is shown in fig. 3 .
"the link and node failure problem has been investigated for a long time in the framework of physical networks but not in virtualized networks because of the new requirements brought by this technology [cit] (e.g., architecture flexibility, mobility, and isolation). our restoration scheme is pre-calculated. therefore, the rerouting paths for all link failure cases are determined and recorded inside the controller in advance. when a failure occurs, the nodes apply the pre-calculated rerouting paths directly. multiple methods based on the ipfrr (ip fast reroute) strategy have been proposed for transient failures. however, they have the following limitations:"
"train camera -test camera figure 2 : accuracy comparison per train-test view pair. we compare with ncte [cit], our method without augmentation, and hankelets [cit] ."
"we further l 2 normalizeȳ to make it consistent with the original descriptor. we refer to n as the transition matrix. we get one such matrix per transition, i.e., per training set d ∆ θ ."
"a. testbed figure 4 shows the data center with 10 rows of racks and 6 crac units. the experimental area is confined to the upper right section with avts fully populated in the cold aisle. it is isolated from the rest of the data center by walls, a removable curtain above the floor and dampers beneath the floor. the experimental area hosts two rows of it equipment racks, row f and row g with 8 and 9 racks respectively. each of the 17 racks is fully instrumented with 5 temperature sensors in the front and another 5 on the back, but only the 17 temperatures located at the top front of the racks are chosen to be regulated. the 20 avts, labeled vf and vg in the figure, line the two rows of the racks in the cold aisle. two chilled water cooled crac units along the right wall, crac #5 and crac #6, cool the experimental area."
"and \"·\" denotes matrix multiplication. the holistic and multivariable model above integrates the zonal cooling actuation of crac units and local cooling actuation of vent tiles, and lays the foundation for the control and optimization work to be discussed next."
"the mpc controller was implemented in matlab running on a linux server. the matlab optimization toolbox [cit] function \"fmincon\" was used to solve the constrained optimization problem. in the objective function j, r v f d and r sat were chosen to reflect the actual blower power and chiller power consumption, and the weights w v f d, w sat, and w tile were chosen carefully to ensure satisfactory transient performance. the control interval was set to allow for sufficient time for computation."
"given this correspondence data, we use a straightforward method to learn a model to transform an action descriptor. more advanced approaches may help improve the results. for instance, rather than using a fixed vocabulary, we can jointly learn the vocabulary and transformations to build a more flexible model. we also make the assumption that all fea-tures transform independently. however, we expect the features in the same frame to be correlated with each other. furthermore, accounting for correspondences which are lost due to occlusion and extending synthetic feature generation beyond trajectories are interesting subjects for further exploration."
"in most cases, the internal crac control can regulate the chilled water valve opening to track the given reference of supplied air temperature (sat). the flow rate of the supplied air can also be tuned continuously if a variable frequency drive (vfd) is available for each crac unit to vary the speed of its blowers. in the previous decoupling based design [cit], one avt controller adjusts the air distribution within each zone to meet the rack inlet temperature thresholds while minimizing the total air flow demand. the pressure in the underfloor plenum is maintained at certain reference value through crac units vfd control to satisfy the cooling needs of the zones. it is difficult to extend this design to include the tuning of sats of the crac units, which have significant effects on both the rack inlet temperatures and the cooling efficiency. it is also challenging to handle the strong interactions between the two separate control loops. more importantly, the reference pressure, which determines the cooling resources provisioned and the blower power, is difficult to optimize."
"we use the inria xmas motion acquisition sequences (ixmas) [cit] to test our approach. this dataset contains 11 action classes, such as check watch, sit down, get up, and turn around, performed by 10 different actors. the dataset consist of synchronized videos observed from 5 cameras with diverse viewpoints. each sequence also has reconstructed volumes and silhouettes of the human subjects, but we do not use this information. for 3d motion examples, we use the cmu motion capture database [cit] which contains over 2600 mocap sequences of human subjects performing a variety of actions. we use the 2200 shortest sequences for training the model. though the cmu dataset contains some action labels for each file, we do not use these annotations."
"equation (3) reveals that the influence of cool and recirculated hot air on rack inlet temperature can be mainly captured by m c (t c −t ) and m r (t r −t ), respectively. this seemingly very simple insight is consistent with the physical intuition and also provides guidance to unite the crac unit sat and vfd control as we will see later."
"many cross-view action recognition approaches transform action descriptors to a viewinvariant space where observations from the source and the target view are comparable [10, (a) (b) figure 1 : (a) a visual comparison between synthesized mocap trajectories [cit] (left) and optical flow-based dense trajectories [cit] (right). we exploit the similarity in their shape to learn a feature mapping between different views using mocap trajectories, and use this mapping to transform dense trajectory based features commonly used for action recognition. (b) to generate view correspondence at the local feature level, the human body is represented using cylindrical primitives driven by mocap data. we project the 3d path of the points on the surface to multiple views and learn how the features based on these idealized trajectories transform under viewpoint changes. 12, 13, 15, 29] . however, learning the invariant space requires supervision. here we present a cross-view action recognition scheme to deal with the unsupervised case, i.e., we have no access to the target view examples. this is a likely scenario when it is not possible to predict the test view in advance. instead of looking for a view invariant space, we learn a probabilistic function to transform descriptors from one view to another using a large collection of unlabelled motion capture (mocap) data. this allows us to generate multiple new descriptors based on each source-view example to account for many possible target-views. we augment the training data using these new descriptors to make our model resilient to viewpoint changes."
our cross-view action recognition pipeline can be summarized as three main steps: a) generate a large number of synthetic trajectory features observed from different viewpoints using mocap data; b) use these examples to learn a feature mapping for each view change; and c) apply this model to generate new multi-view descriptors to augment the training data. we describe each of these steps in detail in the following subsections.
the proposed holistic modeling and control approach was implemented and evaluated through experiments in the experimental area of a production data center. we present part of the experimental results in this section.
"in which v f d stands for the speed of the blower in the percentage of the maximum vfd setting. the coefficient k crac may vary with each crac unit and can be either provided by the manufacturer or determined through experiments. the cool air flow, after leaving the crac unit blowers and traveling through the under-floor plenum, is distributed through the vent tiles. each adaptive vent tile can be treated as an adjustable valve. in order to determine the total cool air flowing through a vent tile, we define the normalized tile opening u tile as:"
"in this section, we derive simplified models from the basic mass and energy balance principles to characterize the complex mass and energy flows within the raised-floor aircooled data centers."
"constraint relaxation method [cit] was applied to address the feasibility problem due to the temperature constraints. in the case of temperature threshold violations, the temperature constraints in the prediction horizon were relaxed to approach the thresholds asymptotically. it was observed in the experiments that the temperature constraints relaxation helped avoid aggressive actuation as well."
"the other sections of this paper are organized as follows. section ii derives the holistic models using the energy and mass balance principles. based on the models developed, integrated cooling control and optimization using mpc is presented in section iii. section iv discusses the controller implementation and experimental results. the paper is summarized in section v with discussion on the future work."
"3d representation of motion is an obvious way to achieve view independence. ramanan and forsyth [cit] track 2d body parts and match them to mocap data annotated with action labels. they also recover 3d pose and camera viewpoint in the process. however, this approach depends on reliable 2d pose estimation. [cit] build a voxel-based 3d representation of human motion using multi-view video data. these methods do not require explicit 2d part detectors, but they still need synchronized and calibrated multi-view videos to construct the model. [cit] removes the dependency on multi-view video data and pose detection by directly matching motion features from video to idealized trajectories generated using mocap data. although this approach is effective in transferring knowledge across views in the unsupervised case, it is limited by the availability of motion examples in the mocap database. hence, it is harder to scale it to a large number of classes. the method is also limited by the accuracy of the intermediate matching step. by learning a direct mapping at the feature level, we are able to address both of these limitations."
"the experiment for integrated control of sat, vfd, and avt lasted for 11 hours. the damper along the left wall was closed, while the curtain and dampers along the curtain were open, thus allowing air flow disturbance from the neighboring area both above and beneath the floor. the control horizon hu was set to 2, and the prediction horizon hp was set to 8 for a more conservative controller in consideration of the disturbance. the control interval was 30 seconds. the uniform rack inlet temperature threshold was set to 25"
"while the temperature of the recirculated hot air is beyond direct control, the cool air delivered to the rack inlets can be adjusted through tuning of sat and vfd of the crac units, and vent tile openings to regulate the rack inlet temperatures."
"in the previous experiments, we saw how augmenting the training data with synthetic examples corresponding to multiple viewpoint transitions improves cross-view action recognition accuracy. we are also interested in the question: how specific are these feature mappings to a particular viewpoint change? this motivates us to consider the problem of estimating the target elevation angle given the source elevation angle and action annotations in both the source and the target view. we achieve this by comparing the action recognition performance gained by different possible transitions. in contrast to the experiments above: a) we augment the training data with the synthetic data generated using one transition at a time and b) weigh synthetic examples the same as the original descriptors. we measure the recognition accuracy on the target view examples and find the transition with the highest gain. using the relative elevation angle change corresponding to the best transition, we estimate the target view elevation angle. for this experiment we consider a denser grid of viewpoints. we choose a 15 degrees (in the range [15 90] degrees) division in elevation angle and 30 degrees (in the range [0 360) degrees) in azimuthal angle giving rise to a total of 142 transitions."
"in which n tile v stands for the number of vent tiles nearby that have significant influence over the cool air flowing into the rack, and the contribution of each vent tile is quantified by b tile ."
"following [cit], we augment our original training data using these new descriptors. in such augmentation schemes, the synthesized data is often weighted less compared to the original data [cit] . we empirically set the weight of the augmented data to 0.01. we train an svm with χ 2 kernel using one-vs-all strategy. the final source code as well as the preprocessed data is publicly available 2 ."
"the holistic modeling and control approach was first applied to the integrated control of vfd and avt. in this case, sat was a fixed given value while crac units blower speeds and avt openings were dynamically tuned by the mpc controller. during the experiment, the rack inlet temperature threshold was set to be 27"
"data center cooling is in essence an optimal control problem, in which the total cooling power is minimized in response to the dynamic it workload while the rack inlet temperatures are maintained at or below the specified thresholds. the temperature thresholds are not necessarily uniform across the entire data center but are dependent on the different functions, such as computing, storage, and networking, that the it equipment serves. service contracts of the it workload hosted in the it equipment also affect the temperature threshold. figure 3 shows the proposed control system structure, in which the three cooling knobs available to the controller are the crac unit sat, crac unit vfd, and vent tile openings. the effects of these cooling actuators on the rack inlet temperatures are captured by the models in the previous section. the objective function of the mpc controller is set up to reflect the total power usage of the cooling system. by comparing the rack inlet temperature measurements t with the temperature threshold t ref, the mpc controller automatically seeks the optimal zonal and local cooling settings in response to the dynamic it workload. the cooling resources provisioning, transport, and distribution are coordinated since they are considered simultaneously in the same framework to minimize the cooling power."
"in raised-floor data centers, the pressure difference below and above the floor drives the cool air flow toward above the floor through the vent tiles. assuming that the air density change is negligible for normal crac units operation, the total cool air flowṁ crac delivered by the blower of each crac unit can be determined by the fan law:"
"among the optimization constraints, t ref is the rack inlet temperature threshold. cooling control inputs including the blower speeds v f d, supply air temperatures sat, and vent tile openings u are constrained by their respective physical or specification limitations. it is found through experiments, for example, that in most cases it is not desirable to turn a crac unit off even if its load is very low since doing so will significantly change the air flows within the data center while resulting in negligible power savings."
"which describes how the rack inlet cool air flow is affected by one specific crac unit and the vent tiles near the rack. for multiple crac units deployment, we can sum up the total cool air flows into a rack inlet from all the crac units as following,"
"the cool air flows leaving the vent tiles are free to mix above the floor. as a result, the cool air flowṁ c that reaches a rack inlet might come from several vent tiles in its vicinity:"
"following the holistic model structure as in equation (11), a multiple-input-multiple-output (mimo) model was identified through system identification experiments. the model takes the 17 temperatures as the outputs, and the sat and vfd of crac #5 and crac #6 as well as the openings of the 20 avts as the inputs."
"recently statistical techniques based on domain adaptation [cit] have become popular because they do not require detection or tracking of individual body parts. however, some of these methods rely on having access to synchronized multi-view video data or labelled examples in the target-view. other recent approaches [cit], taking inspiration from unsupervised domain adaptation [cit], extend these ideas to the case where no labels are available in the target view by utilizing the structure in the data itself. however, they still need unlabelled target view videos which may not be readily available in the general case. now, we discuss approaches that are more closely related to our method."
"among related computer vision problems, learning relationships between features across viewpoints is an interesting challenge for simultaneous object detection and 3d pose estimation in images. some of these approaches use 3d cad models of objects -analogous to mocap trajectories in our case -to learn the local visual features as well as spatial relationships between them [cit] . another approach [cit], similar to ours, solves this problem by establishing connections between features observed from neighboring views (called activation links). however, these methods are not directly applicable to the problem at hand."
"in this paper, a holistic modeling, control and optimization framework is developed to integrate the provisioning, transport, and distribution of the cooling resources. the zonal and local cooling actuation is coordinated, in a unified framework with model predictive controller (mpc), to minimize the total flow and thermodynamic work done by the cooling system. while mpc has been applied to thermal management in various contexts before [cit], the work to be presented is different in its scale since it is targeting the thermal regulation of hundreds of racks in large scale data centers with tens of crac units. moreover, the commonly seen temperature tracking problem is now replaced by an energy minimization problem subject to temperature constraints."
"substitute equation (9) into equation (3) and replace t * and t in equation (3) with rack inlet temperatures at time steps k +1 and k respectively, we have the following discrete model for rack inlet temperatures:"
"the literature of cross-view action recognition can be divided into three major categories: the first utilizes geometric [cit] or dynamical properties [cit] of human motion to develop a view-invariant representation. the second, inspired by transfer learning, treats this as a purely statistical learning problem [cit], and often does not reason about the geometry. the third relies on using 3d exemplars of motions that can be matched with the video evidence [cit] to bring view invariance. other interesting approaches include bilinear modelling of action with viewpoint [cit] ."
"a view-invariant representation of human motion is crucial for effective action recognition. widely popular shape [cit] and optical flow-based [cit] features, which are commonly used to describe actions in videos, are not specifically designed for viewpoint invariance. the aggregated video descriptors based on these local features are robust to variations in size (with multi-scale features) and temporal shifts. however, the representation in general is not view-invariant. this implies that the method's effectiveness highly depends on availability of training data from different views. one way to deal with this challenge is to gather a large enough training set that includes a wide variety of viewpoints. however, this may not be a feasible approach in many real-life scenarios. when the training view (or source view) is significantly different from the test view (or target view), we need to come up with strategies to either transfer knowledge across views or devise a view-invariant description to recognize actions. these techniques are often referred to as cross-view action recognition techniques."
"in the open environment, air flow coming into the it equipment inlet is a mixture of the cool air from the crac units (through the vent tiles) and the recirculated hot (exhaust) air that escapes into the cold aisle. the recirculation of hot air in the cold aisle generates entropy and lowers the cooling efficiency of the data centers [cit] . to determine the effects of both cool air and recirculated hot air on the rack inlet temperature, consider a small control volume in the proximity of the rack inlet with mass m and temperature t, as shown in figure 2 . cool and recirculated hot air flows with mass and temperature (m c, t c ) and (m r, t r ) enter the control volume, mix well with the air (m, t ) already in the volume, leave the control volume altogether and enter the rack inlet with total mass m * and temperature t * . based on mass balance principle,"
"in this paper, we described a simple yet effective approach to add view-invariance to an action recognition pipeline without any additional supervision. we also show that given the labelled examples in two views we can predict the relative viewpoint change along elevation angle. instead of relying on pose estimation, we learn a probabilistic model for feature transformations due to viewpoint changes. for training, we rely on 3d human motion examples in the form of unlabelled mocap data. however, we do not require mocap data with subjects performing specific actions, which makes our approach flexible and widely applicable. in summary, using synthetic motion examples as a proxy for real trajectories allows us to establish a correspondence between features across views, which is a difficult problem given video data alone."
"in which u tile is the vent tile mechanical opening ranging from 0 to 100 percent and n tile is the number of tiles in a cold aisle. for a cold aisle cooled exclusively by a single crac unit, if it is assumed that the cool air mass flow rateṁ tile through each individual tile is proportional to its normalized opening u tile, then we have:"
"given the training data d ∆ θ, the relationship between codewords f i and g i can be modeled within a probabilistic framework. since we assume each feature transforms independently, we can learn a joint probability mass function p(f, g) which captures the probability of having feature pairs ( f i, g i ) in d ∆ θ . we train the model using maximum likelihood estimation. we calculate the empirical probability by counting the co-occurrences of ( f i, g i ) in d ∆ θ followed by normalization. the conditional probability distribution of g, given an observation of codeword f i in the source domain can be written as"
"trajectory features can be efficiently generated for both video [cit] and mocap [cit] . each trajectory descriptor is a concatenation of 2d displacement vectors over a fixed time window normalized by sum of their magnitudes. we use mocap examples to collect a large number of descriptor pairs for each viewpoint change. to simplify the problem, we quantize each descriptor to its closest codeword using a fixed vocabulary. furthermore, we learn a probabilistic mapping between the codewords in two views. for action recognition, we describe each video with a bag of words (bow) of dense-trajectory features. given multiple feature mappings we can \"hallucinate\" action descriptors from different viewpoints and use them as additional training examples."
"in the ixmas dataset, for each camera, we can obtain four estimates for elevation angle by treating other cameras as source views. we show our results in figure 4 . we observe that we are able to get a reasonable estimate based on this simple approach of choosing the best transition. we note that the estimation also includes the quantization error for both source and target views. these results suggest that our approach not only boosts action recognition accuracy when synthetic examples are aggregated, but individual transition matrices are also specific to the relative change in the elevation angle."
"the rest of the paper is organized as follows: in section 2, we discuss the background and related work. we present our main contributions in section 3. section 4 describes the experiments, followed by discussion and future work in section 5."
"based on the hierarchical and execution flow, the most significant part is the scheduling scheme that to spawn the tasks out-of-order by analyzing the inter-task dependencies. at runtime, each function call of do_t_ ã is responsible for the intended behavior of the main program in the scheduler processor. at each call to these functions, the runtime will do the following actions: 1) analyze data dependency including read after write (raw), write after read (war) and write after write (waw) [cit] . the data dependency analysis is based on the parameters used by tasks. 2) eliminate the waw and war dependencies by parameter renaming techniques. 3) identify the target function unit to run current task by a task mapping method."
"parallel task execution models have been studied for parallel computing machines during past decades. first of all, task-based parallel programming model are quite popular to enhance ilp to tlp, such as openmp [cit], mpi, intel's tbb, opencl [cit] and cilk [cit] . most of these state-of-the-art programming paradigms focus on symmetric multiprocessors to significantly reduce the workload of programmers."
"the statistic methods and mrmr were also applied in feature selection in a prad prognosis dataset, with the purpose of training models to predict the prognosis of prad ('good' or 'poor') using somatic gene mutation information (supplementary material). supplementary fig. s5 ). for this dataset, mrmr appeared not better than ebt for different feature size (mrmr best auc: 0.63 at 50 features; supplementary fig. s5 )."
"we illustrate the execution model with the simple sequential program example of fig. 1a, which invokes the two functions t and g within a loop, as well as a head task h in prior to the loop and a tail task e executed after the loop. fig. 1b describes an example dynamic sequence of invocations of t and g after the loop is unrolled during execution, along with their dynamically computed write and read token sets generated in the program. fig. 1c presents the data dependence between the functions. for example, t2 writes objects a and e, and thus it has a war dependence (solid arrows) on t1 and g1, which reads object a as an input. likewise g2 has a raw dependence (dashed arrows) on g1, and t3 has a waw (dotted arrows) dependence on g2. these dependences must be preserved if the dynamic dataflow is to maintain the sequential execution of the static program, otherwise the correct results of the ooo execution cannot be guaranteed. fig. 1d introduces the parallel execution model of the sequential code on different processors. the task h and e standard for the synchronization barriers before and after the loop iteration."
"the computing processor begins execution automatically when all the operands are received. based on the hardware interconnect, results are returned through interrupts. one interrupt controller is integrated to detect results interrupt request and update the task variables. since results from different tasks may be returned at the same time, a firstcome-first-serve (fcfs) policy is used to deal with interrupts, and no interrupt preempt is supported."
"conditional-gan [cit] presents the framework for sampling from a gan based on a conditioning factor, such as image category. most early work on gan had focused on unconditional training of gan. there is not clear evidence as to where in the network the conditioning information is to 978-1-7281-4673-7/19/$31.00 ©2019 european union be provided and different techniques have been proposed, such as providing a one-hot class embedding at the input, or a learnt class embedding in one of the internal layers. conditional batch normalisation [cit] layers have been found to be effective for modulating the activations in a network and have been subsequently used for conditional image generation in gans [cit] ."
"our proposed middleware provides implicit synchronization methods. an implicit synchronization barrier is setup at the end of automatic parallel regions, before the output codes. for example, if the output functions in the main program want to print out the results (e.g., printf) at the end of an automatic parallel region, it will automatically wait until all the annotated functions are finished. at the time of writing this paper, ooo middleware is allowed to deal with synchronizations among all the functions defined in the function library."
"the experimental results are inspiring but there is a lot of work left in the future. first, we plan to extend the renaming technologies to dynamic partial reconfigurable situations where processors and ip cores can be adaptive to fit in different applications at runtime. second, we are implementing the module in rtl which is more capable to handle smaller tasks. in the meantime, new annotations in the fig. 12 . difference between experimental and estimated metrics on speedup, area, efficiency, and power. programming paradigms are taken into consideration by the source to source compiler to identify both the library tasks and the general purpose software tasks. finally, the improvement of the synchronization mechanism using realtime operating system supports is another promising direction for heterogeneous mpsoc research paradigms."
"accelerate tasks execution when all the hardware ip cores are busy. task no.4 and no.9 are distributed to computing processor for execution, which also demonstrated that our hw/sw platform can be used to verify alternative scheduling and mapping methods."
"in order to utilize the hardware resources integrated in fpga platform, drivers for peripherals and memory systems are implemented. similar to devices drivers of a general operating system, we introduce following modules for prototype demonstration: interconnect driver is used to allow applications transfer data and control messages between microprocessor and ip cores, through buses or network-on-chip (noc) infrastructures. file system driver is utilized to provide i/o access to the files. local static and partial configuration bitstreams are also stored into file systems. when task execution is finished, the results are returned with interrupt signals. peripherals such as uart and timer are integrated for user debugging. reconfiguration controller manipulates the software and ip libraries for function units' replacement."
"the whole system is implemented in xilinx virtex5 lx50t fpga, including one microblaze processor, one computing processor, following different ip blocks: adder, idct, aes (enc and dec), des (enc and dec), jpeg, memory and peripheral modules. table 10 summarizes the hardware cost within single fpga. the whole system takes 5,238 slice registers and 19,209 slice luts. considering the resources supplied in fpga, the hardware cost is acceptable for the proposed architecture. by looking further into the synthesis report, most of the resources are occupied by microblaze processors and hardware ip cores, we can get that the scheduling microblaze processor takes 1,650 luts and 1,489 ffs, while the hardware costs of the ip cores distinguish from each other. for example, jpeg function including four phases: color conversion (cc), 2d-dct, zig-zig reordering/quantization (zz/q) and huffman encoding, in which the 2d-dct was identified as the critical section."
"we have investigated the samples from a state of the art generative model, biggan [cit], from the perspective of training a classifier. we saw that the samples generated from this model look quite realistic. however if we want to use these samples for a downstream task, such as training a classifier, there is still a gap in the performance between generated samples and real data samples. on the other hand, from a compression point of view, a generative model in this case is more efficient for storage as compared to storing the same number of real data samples that obtain similar classification error. in order to improve the classification performance we used a pre-trained classifier to filter out samples based on the category and prediction probability. in our experiments we see that using a threshold for the prediction probability is essential in case there are a large number of categories involved. an architectural change was introduced, the concat residual block which helps in the generation of better samples. this is again essential when the number of categories is large. finally, to help the generative model to cover more of the underlying real data distribution, we used multiple gan sampling, leading to significant reduction in the gap of classifier error between training on real data and gan samples."
"a task sequence with 11 tasks is listed in table 8 . type indicates whether this task runs on gpp (s) or ip core (h). the last two columns refer to the ideal start and finish time for current task, taking no account of the scheduling and transfer costs. fig. 7 depicts the comparison between theoretical and experimental results of the task sequence. the curve of experimental value is consistent with theoretical value, but slightly larger. the gap between them stands for the scheduling and communication overheads. the average of overheads is less than 4.9 percent of task running time itself, which proves that the scheduling overhead of the scheduling scheme is quite low."
"a 5-fold training-testing strategy was adopted to assess the performance of models. for each training dataset, the indicated number of genes (10, 20, 30, 40, 50, 100, 150 and 200) with lowest p-values for mutation rate difference between luad and lusc population detected with different statistical methods were used as features for svm model training, respectively. as shown in figure 4a and b, the models based on features selected with ebts (models) consistently outperformed those based on features identified with traditional statistical methods with or without multi-testing correction (chi2, chi2.fdr, binom and binom.fdr models). when feature size was small, the performance of models often increased as feature size increased ( fig. 4a and b) . ebt, chi2 and chi2.fdr all reached the best performance at 40 features with auc of 0.89, 0.87 and 0.87, respectively, while binom and binom.fdr models reached best at 20 and 30 features, respectively, with auc of 0.83 and 0.74 ( fig. 4a and b) ."
"the scheduling module decides when the task can be executed due to data dependencies, furthermore, when a task is ready, only one target function units is selected from multiple options. the task mapping scheme should decide the target for each service, as is described in algorithm 1."
"fisher's exact tests with or without correction. as shown in figure 2a, the 45 significant genes tested by ebts were all detected in the 483 significant results of v 2 and fisher's exact tests, and covered all the 16 significant ones identified by multi-testing corrected v 2 and"
"one of the most related works of our task-scoreboarding scheme is the task superscalar, therefore, we also applied renaming schemes employed in task superscalar to fpga for comparison, which is illustrated in fig. 9 . we use the generated 11 applications in table 8, and then evaluated both taskss and task-scoreboarding algorithm respectively. due to the scheduling overheads, the performance of both approaches gets larger than the ideal scenario, which is normalized as 1.0â. it can be derived that the taskss always achieves higher scheduling overheads than our task-scoreboarding approach, from 3 (t7) to 14 percent (t5). what's more, our approach can get an average overhead less than 5 percent. therefore our approach obtains much smaller overheads than taskss."
1) filtering threshold analysis: at this filtering stage we can analyse which categories are filtered out more. this would be one indication that the gan model has trouble to capture the distribution for these categories. we can further analyse the predictions and features of the pre-trained classifier for the generated samples. similar predictions and features would indicate the repetition of samples and lack of diversity. this can be done both at the inter and intra class level. more details will be explained in next section.
"we see that although the change of is and fid with different number of gan models is small, the error on testing samples of a classifier have a much reduction. from table i, in cifar-10, the value of is and fid is in a range from 8.62 to 8.58 and from 6.84 to 6.47 respectively, however, the error is decreased by 3%. this means that is and fid scores are not the best way to evaluate a gan when the objective is to use the gan for downstream tasks such as training another classifier."
", where the two factors are conditional probabilities for the actual number of positive individuals (x or y) being smaller than n a in a b(n a, p a ) distribution and larger than n b in a b(n b, p b ) distribution, respectively."
a gan sample is used for training the classifier if it belongs to the correct category and if the output probability is above a pre-defined threshold. in figure 5 we plot the relationship between output probability of the pre-trained classifier and the final validation accuracy of the classifier trained on gan samples. we observe that the value of the threshold is not significant in the case of cifar-10. this implies that the pretrained classifier is quite confident if makes a correct prediction. the reason for this could be that the gan is able to generate very good images and the classifier is therefore confident in its predictions. in our experiments this implies that if the pretrained classifier makes a correct prediction in most cases we can use the corresponding sample in our synthetic training set. we need to study this further for different data sets with varying number of target classes.
"the structure of the paper is decomposed as the following. in section 2 we outline the ooo motivation and review the literatures related to this work. section 3 describes a typical reconfigurable mpsoc architecture and the hierarchical middleware. section 4 presents the features implemented in the runtime library, including detailed ooo task scheduling, mapping scheme and synchronization. section 5 illustrates the fpga prototype with experimental results. finally, we conclude the paper in section 6."
"we can view a trained gan as a compressed version of our training data, where the learned weights are the compressed representation of our original data bits. we are interested in compressing the nature of the distribution from which the training data has been sampled, rather than the explicit training data samples themselves. to measure the degree of compression we train a classifier on samples from a gan and measure the prediction error on a held out validation set. the lower bound for the error that the classifier can achieve is equal to the error that it achieves after training on real training data. we compare training on the gan samples with varying amounts of real training data (see figure 1 and figure 2, detailed results are in table i )."
"however, the side effect of fpga based mpsoc has been exposed like a double-edged sword. programmability for mpsoc is still posing serious challenges in particular. since the hardware is adapted to fit in the applications, programming models and middleware architecture support should be invisibly filling the gaps between different architectures."
"we notice that currently used metrics to measure the quality of a generative model, such as is and fid are not completely indicative of the classifier generalisation error. we also saw that some of the techniques led to small or negligible improvements in these metrics but significant improvement in classification error."
"now let's see how the task sequence is issued and executed. the algorithm is divided into five stages: issue stage, read op stage, partition stage, execution stage, and write result stage. each task undergoes five steps in executing, as is shown in table 4 . the whole five stages are similar to instruction level but adding a task partition stage as stage 3. from table 4, we can examine the steps informally and then see in detail how the scoreboard keeps the necessary information determining when to progress from one step to the next. the five steps are as follows: 1) issue-if a functional unit for the task is free and no other active task has the same destination variable, the scoreboard issues the task to the functional unit and updates its internal data structure. by ensuring that no other active functional unit wants to write its result into the destination variable, we guarantee that waw hazards cannot be present. if a structural or waw hazard exists, the task issue will stall, and no further tasks can issue until these hazards are cleared. 2) read operands-the scoreboard monitors the availability of the source operands. if one or more of the operands is not yet available, the scoreboard monitor will wait for the results. a source operand is available if no earlier issued active task is going to write it. when both operands are available, task will be dispatched to certain function units with task partition. the scoreboard resolves raw hazards dynamically in this step, and tasks may be sent into execution out of order. 3) task partition-when in the issue stage, the decided function unit is to make sure that there are no structural hazards. however, after read op stage is finished, maybe there are other available function units, which may provides shorter task execution time. therefore, in this stage a partition strategy is called for task reallocation. if there are other function units available, the task execution time on each function unit will be compared. after the comparison finished, scoreboard will choose the function unit on which current task can finish as early as possible. 4) execution-the functional unit begins execution once receiving operands. when the result is ready, it notifies the scoreboard that it has completed execution. task distribution and data transfer are both performed through on-chip interconnect. task-scoreboarding table 3 variable table in task-scoreboarding 5) based on the hardware interconnect, the execution results are returned through interrupts. one interrupt controller is integrated to detect interrupt request signals from all the interconnect channels. the interrupt handler assigns the variables with results. in our proposed architecture, since results from different tasks may be transferred back at the same time, a first-come-first-serve policy is used to deal with interrupts, and no interrupt preempt is supported. 6) write result-this is the final stage of completing tasks. once the scoreboard is aware that the functional unit has completed execution, it checks for war hazards. the cases of normal result-writing occur when there are no war hazards between current task and its predecessor tasks."
"in this work, our aim is to see how the current state-of-theart gan model samples compare with real data samples, when compared from the perspective of training a classifier on these samples. in order to compare generated samples to real data several metrics have been proposed such as inception score (is) [cit] and fréchet inception distance (fid) [cit] . these metrics aim to characterise how real-like the generated samples. in this work we are interested in training a discriminative classifier model from the samples of a trained generative model. indeed we also investigate if good is and fid scores correlate with training of good classifiers."
"for most heterogeneous mpsoc architectures, application specific instruction processors (asip), digital signal processor (dsp), and intellectual property (ip) cores are introduced to uncover task level parallelism (tlp). through the common feature among those platforms, fig. 2 illustrates a heterogeneous mpsoc hardware platform constructed in fpga, which consists of multiple general purpose processors (gpps), dsp/asip processors, and a variety of heterogeneous ip cores. the responsibilities of the components are as follows: 1) scheduler processor is employed to operate task scheduling and provides programming interface to users. at runtime, each task is mapped and then distributed to certain processor or ip core for execution. scheduler also keeps the running status of processors and ip cores. 2) computing processors provide a runtime environment for software tasks. in general, computing processors can execute different types of software tasks. each processor provides software runtime function library for different applications. 3) hardware ip cores are responsible for a specific kind of tasks to achieve acceleration. in addition, ip cores can be reconfigured and customized according to application demands. specific tasks can be also spawned to dsp/asip or ip cores for hardware acceleration. each hardware module can execute only limited types of tasks for accelerations, due to the rtl functional implementations. 4) interconnect modules between schedulers, computing processors and hardware blocks are in charge of data communication. in this paper, we setup our experiments and simulation on a star network based on xilinx peer to peer fsl [cit] channels. scheduler is connected to every processor or ip core with a pair of fsl bus links. all the interfaces are packaged into unified fsl manner for data communication. note the interconnect structure can be replaced by other topologic modules, such as crossbar, mesh, hierarchical bus or ring architectures. 5) memory and peripherals are integrated to maintain local data storage and peripherals, such as ddr dram controller, ethernet controller, system ace controller, uart, timer and interrupt controller. all these modules are connected to scheduler processor with bus-based interconnects, such as coreconnect processor local bus (plb)."
"we see figure 7 and figure 5 that the distribution of some categories are harder to capture for the gan model. we introduced the idea and intuition behind using multiple gan models for training the classifier in §iii-c. during training of the classifier from synthetic samples, we can construct our dataset by sampling from the multiple gan models which can help us to cover the data distribution in a more effective manner."
"second, the task scale refers to the total amount of different tasks. in particular, as we use multiple loop iterations to construct the intra-loop and inter-loop data hazards between tasks, the task scale indicates the number of loop iterations. in demonstration, we set the task scale less than 4,096 in all the test cases."
"meanwhile, as in this test case, task no.9 is distributed to computing processor for execution, which demonstrates that the current partition method is not an optimal choice for the entire task sequence. alternately, if task no.9 waits until task no.8 finish on aes_dec core, instead of run immediately on computing processor, the entire task running time can be further reduced."
"when the execution encounters a task to be considered for dataflow execution, it requests read (write) tokens for exclusive guarantee in the function read (write) set; it is ready for execution only after it has acquired all its requisite tokens. similarly, upon completion, it releases the tokens which are then spawned to the waiting function(s) if necessary. when a pending function has acquired its requisite tokens, it can be offloaded and submitted for execution immediately."
"corresponding to reconfiguration features, software and ip libraries are introduced at this level. software libraries include functions to be executed locally, while ip libraries consist of back-up ip cores ready to be integrated."
in both datasets we see that the use of the pre-trained classifier is useful for removing samples that do not belong to the correct category. the effect of the probability threshold selection becomes more important as the number of categories increases. it has a significant effect in improving the validation error of the classifier trained on generated samples.
"besides the datasets studied earlier, the advantage of ebt in feature selection for classification over the 'top n' strategy based on the results of traditional tests was further demonstrated with two independent luad prognosis datasets (supplementary material). first, similar as before, a 5-fold training-testing strategy was used to optimize the models with varied size of features and evaluate the performance for the tcga dataset (supplementary material; supplementary table s9) . when the top 30 or 50 features were selected, the ebt models outperformed chi2 or binom ones significantly ( supplementary fig. s6a ). ebt identified 16 genes in the tcga dataset with significant somatic mutation rates generally (supplementary table s10 ); the ebt model trained with these 16 genes was tested in another dataset (imielinski) and compared with the binom and chi2 models based on corresponding top 16 genes. as shown in supplementary figure s6b, the ebt model also showed better performance than either binom or chi2 model."
"on the other hand, potential inter-task dependencies need to be exploited to avoid data hazards from dramatically reduces the task level parallelism. task scheduling mechanisms are employed to detect data hazards (raw, waw and war), and further for out-of-order task execution. in this framework, we tentatively apply both scoreboarding and tomasulo algorithms to task level. of the two approaches, scoreboarding can obtain shorter scheduling overheads, but the waw tasks can only run in sequences. meanwhile, tomasulo algorithm can eliminate waw data hazards by register renaming."
"we can see in figure 6, most bad images already are removed because of incorrect label in cifar-10. this is one reason why the threshold value does not have too much effect. on the other hand, in figure 7 we see that in the case of cifar-100, the number of bad images which is filtered out by threshold value increase remarkably. there are a few categories for which it is easy to generate images belonging to the correct category, while for the majority of categories we need to sample repeatedly to get images of the correct category (green bar in figure 7 ). one concern here can be that for these hard categories, the intra category diversity is reduced when we sample repeatedly since the generator model is inherently of limited capacity. this can be a subject of future work, to investigate and improve diversity for hard categories. we visualise several samples before and after filtering out the bad one in figure 3 and figure 4 ."
"in order to verify the adaptive hardware/software task mapping scheme, we design a task sequence in table 9 . in this case, we introduce task mapping mechanism to operate hardware/software collaboration. for these 11 tasks, task no.4 and no.9 are dispatched to computing processor for execution. this is because that when the two tasks are issued, target ip cores are busy. thus, scoreboard chooses software computing processors as target function units. fig. 8 gives the comparison between theoretical and experimental results of the task sequences in table 9 . similarly, the scheduling and communication overheads are less than 3.3 percent of task running time. moreover, in this case, tasks have been partitioned between hardware and software. a computing processor has been integrated to fig. 7 . experimental results of the example task sequence, the x-axis refers to the length of sequences listed in table 10, while the y-axis is the running time of the application."
"above results demonstrated the possible advantages of ebts in balancing fp (precision) and fn (power) rates, generating a moderate size of interesting features for further analysis or application, and maintaining a general accuracy close to that of commonly used v 2 tests, fisher's exact tests or binomial tests. the robustness and capability of generalization of testing results were further evaluated."
"when a task is issued, it obtains the core currently assigned to its destination core from the table and stores its results to the appropriate output queue upon completion. a side effect of this table based approach is that instructions will not issue to the fabric if the destination core is not available. this prevents the producing task from filling up the fabric if the consumer is not present. even with the table, however, spawned tasks could accumulate in the fabric if the current task forces are switched out while data is in flight to it, which would require the consumer to be switched back into the same core to receive the values. to prevent this situation, the task-to-core mapping table maintains a count of the number of in-flight tasks destined for each core. on a request to switch out, the scheduler checks the number of in-flight tasks bound for its core. if this is greater than zero, the fabric is blocked from accepting any new tasks destined for that core and the core continues to execute until the in-flight counter reaches zero. at this point the application can be stalled and the fabric unblocked."
"in the description of programming model, each task is composed of task name, source and destination operands. as the tasks are treated as macro instructions, then the data dependencies problem can also happen for task level. by reviewing the data dependencies problems (raw, waw and war) at instruction level, scoreboarding and tomasulo are both effective methods for ooo instruction execution. the reasons for which we choose scoreboarding instead of tomasulo are the following: 1) scoreboarding can provide a light-weight task hazards engine for ooo execution. the architecture is simpler, which brings smaller scheduling overheads. 2) for task level parallelization, waw and war happens not as much as at instruction level. most programmers are intended to use different parameters in case of war and waw hazards. therefore introducing a mechanism as complexes as tomasulo is not just fair enough. similar to instruction level, we use parameter renaming techniques to uncover task level parallelism. tomasulo is applied to task level instead of scoreboarding which could do no more than stall when dependency occurs. there are five components of the scheduler: 1) functional unit status-indicates the state of the functional unit (fu). table 2 lists the function unit status. whenever there is a task whose input operands are ready, the task will be dispatched to function units for execution. there are eight fields for each item:"
we experiment on the cifar-10 and cifar-100 [cit] datasets. these datasets consist of 50 000 training images divided into 10 and 100 categories respectively. each image is 32x32 pixels and 3 channel rgb. we choose this dataset because gan training needs large time and computational resources. cifar-100 provides sufficiently complex images while still being possible to train in a reasonable time. we refer to the original dataset as d r . we construct a synthetic dataset by sampling images from a trained gan model and refer to it as d g . sample images from each class are shown in figure3.
"finally, the summary of state-of-the-art parallel execution engines is listed in table 1 . although these approaches provide ooo engine by superscalar or renaming engines, they do not focus on the adaptive mapping for general fpga platform with reconfigurable ip cores, therefore the flexibility across different architectures is still worth pursing."
"cores increases. this overhead prevents the use of barriers at fine granularities. in cases where a barrier is followed by a serial function that is performed by one of the tasks and the output communicated to all participating tasks, the scheduler may directly synthesize the function into the fabric with the output communicated to the participants' output parameters."
"in gwas, tens of thousands of factors, e.g. alleles, are often observed and compared simultaneously for the same samples. the accumulated type i errors in a single v 2 /fisher's exact test or other rate-comparison test caused a large number of false positives (fps). in practice, there are often hundreds to thousands of factors showing significant rate difference between cases and controls. to control the fps, bonferroni proposed a strict correction strategy by multiplying the single test p-values with a constant scaling factor, the testing times [cit] loosened the bonferroni correction, and suggested a false discovery rate (fdr) strategy to control the type i errors. these strategies effectively decreased the false positive rates (fprs), and have been adopted widely. however, the low statistic power also became a major concern. with the multi-testing correction in gwas studies, there were often too few or even no any significant factors identified; many false negatives (fns) were out of detection [cit] . in recent years, various methods were proposed to increase the statistic power of rate comparison tests with multitesting correction, e.g. binning alleles into genes, pathways or networks to decrease the total number of observed factors [cit] . in classificationtargeted machine learning studies, feature selection is an important issue. the features are often identified by rate comparison between cases and controls due to the low computational cost. feature number is also important, since the model performance is low when only a few atypical features are identified for use while the performance would also deteriorate strikingly if too many features are used [cit] . single tests without fp control in gwas generate too many features while fp control leads to too few features, both not suitable for machine learning model training frequently. the binning strategies on original factors often dramatically increase the technical difficulty and costs in practical examination of the significant features. some models only take the top tens to hundreds of significant factors with smallest p-values detected by single tests without fp control, but the feature number is arbitrary and the features are not guaranteed to be most effective [cit] ."
"based on the prototype system, we designed several sample applications to measure the performance, scheduling overheads and hardware cost of the mpsoc system [cit] . in this case, we measured the speedup under four situations: raw, waw, no hazards, and war. in order to evaluate the peak speedup, we define two parameters: first, the task execution time denotes the entire execution time using in different types of data hazards. in the circumstances of no dependencies, waw and raw, the task execution time is configured to the same value (varied from 5k to 100k cycles), while in war and waw, the execution time is configured to different values for heterogeneous computational tasks."
"due to the symmetry, a similar model, probability calculation and statistical inference could be made for n a /n a n b /n b ."
"the classifier is trained on both d r and d g is done for 156 250 steps. in the case of d r this means iterating over the same samples multiple times as is done usually. in the case of d g a batch of samples is obtained from the trained gan model. we expect in the ideal case to have different image samples,however in reality we are limited by the capacity of the gan model with respect to diversity of the data."
"up to now, considerable amount of literatures on hybrid programming models have been conducted at task level. for instances, openmp [cit], mpi, intel's tbb, opencl [cit] and cilk [cit] are very successful programming paradigms. recently, cuda is becoming very popular in gpu based programming models. however, a major weakness of these approaches is the inadequate automatic parallelization degree. in particular, task mapping and scheduling plans are operated manually, which means the achieved speedup is largely dependent on the experiences of programmers. meanwhile, most of these works focus on either the symmetric multiprocessors or gpu based acceleration engines, which cannot be applied to reconfigurable heterogeneous mpsoc scenarios directly."
we use the biggan [cit] architecture as our gan model and it is trained according to the original paper with related settings of the hyper-parameters. for the classifier we use a 18 layer resnet [cit] model with 11m parameters because it provides a good trade off between performance and training time.
"the blue curve depicts training on different subset sizes for 200 epochs. this means that in case of smaller dataset sizes, we have less number of optimisation steps. the orange curve shows training on each subset for the same number of optimisation steps. we see that in the case of less samples, training longer reduces the classification error rate substantially."
"alternatively, there have been creditable mpsoc programming models devoted to specific hardware architectures, such as starss [cit] and cellss [cit] . although both approaches provide superscalar or renaming techniques allowing tasks out-of-order (ooo) execution, the constrained architecture avoids them to be directly applied to the reconfigurable mpsoc architectures. as the system complexity grows, the problem of how to design a flexible programming model is becoming increasing challenging. until now the problem has not been completely figured out yet."
"the ooo middleware proposed in this paper is intended to provide an efficient middleware support between hardware and high level user applications, taking the benefit of the general mpsoc hardware platform with reconfigurable abilities. in this section we propose the hardware architecture and the execution model for the middleware support."
"one of the motivations for training methodologies for classifiers from gan samples is in the area of continual learning. deep generative replay (dgr) [cit] has been proposed where tasks are trained sequentially but the data of old tasks is only available through a generative model. in this case, where we do not have access to old training data, we need a good generative model to maintain good performance for the older tasks. the techniques introduced here are a step towards this direction."
"one of the potential applications of the statistical methods on rate comparison could be the selection of effective features for training machine learning models. therefore, we observed the capability of ebt in such an application in classification of luad and lusc based on somatic gene mutation data, and compared it with other methods. kras 99 kras 99 kras 99 tp53 97 tp53 98 tp53 98 ttn 92 ttn 95 ttn 93 nbpf10 85 nbpf10 85 tcra 89 nfe2l2 81 frg1b 83 frg1b 87 frg1b 81 nfe2l2 81 nbpf10 86 tcra 78 tcra 81 fam75a6 85 fam75a6 77 fam75a6 77 nfe2l2 80 dppa4 76 dppa4 75 dppa4 note: the gene mutually covered by the top significant 50 genes detected from the orginal dataset with individual statistical method was shown with gray background; the genes mutually covered by different statistical methods were shown in italic."
"along with the achieved speedup, the area and power consumption are considered as the two most significant metrics for evaluation. the dct-2d occupies most area and power of the four parts, which are 2.38 mm 2 and 31.1 mw, using xilinx ise synthesis tools and xpower analyzer. in comparison, the hardware cc module takes 1.74 mm 2 and 8.7 mw, while quant module takes 0.1 mm 2 and 8.7 mw respectively. fig. 12 illustrates the differences between the experimental and the estimated metrics. the x-axis in fig. 11 indicates the specific hardware configuration, while the y-axis represents the differential percentage of speedup, power, and resource cost and core efficiency. we can learn from fig. 12 that the actual architectures come up to a speedup of 26.01â for hybrid systems and a speedup of 3.83â for homogeneous systems."
"1) we present a hierarchical middleware on reconfigurable mpsocs, from which programmers are no longer required to gain hardware implementation and task partitioning plans. in order to analyze the annotation based codes, we propose an execution model that translates sources programs to internal functions for parallel execution. 2) we propose an ooo scheduling mechanism that checks the data dependencies, renames the parameters, and issues the tasks automatically when the tasks are ready. the renaming technique is applied from traditional instruction level to task level, regarding each processor (ip core) as a function unit. 3) we introduce an adaptive mapping scheme to map the tasks to target function units at runtime. when hardware architecture is reconfigured, tasks can be remapped automatically without the rebooting. 4) we implement the mpsoc prototype on xilinx fpga. eembc benchmarks (e.g., idct, aes, des and jpeg) are implemented in both software and hardware. multiple microblaze processors are integrated as scheduler and computing processors. the prototype platform can be used for evaluation and verification for task partitioning scheme, scheduling, interconnect, etc."
"additionally, benefiting from current dynamic partial reconfiguration supports, ip cores can be dynamically reconfigured at runtime. after ip cores are reconfigured, the task-to-processor table structures will be updated."
"the two additional datasets with smaller data sizes (117 for luad and 158 for lusc 3-year prognosis dataset) were also used for comparison of the ebts and traditional binomial tests. in total, 8 and 1 significant genes were detected from luad and lusc prognosis dataset with traditional binomial tests, respectively. no significant tables s3 and s4 ). the three significant genes detected with ebts (adamts5, lyst and ptprc) from the luad dataset were also covered by traditional binomial testing results with lowest p-values, and the only significant gene detected with traditional binomial tests (dusp27) from the lusc dataset was exactly the one with the lowest p-value detected by ebts (supplementary tables s3 and s4 ). the composition and significance ranks were quite conserved for the genes with lowest p-values among ebts, traditional binomial tests and v 2 /fisher's exact tests, implicating the high consistency for the different statistical methods for datasets with smaller data size (supplementary tables s3 and s4 )."
"finally, the summary of state-of-the-art parallel execution engines is listed in table 1 . although these approaches provide ooo engine by superscalar or renaming engines, they do not focus on the adaptive mapping for general fpga platform with reconfigurable ip cores, therefore the flexibility across different architectures is still worth pursing."
"before tasks are sent to computing processors, the scheduler must decide which task runs on which function unit, and also when the task is be issued. these two questions are solved by task partition methods and scheduling algorithms."
"case-control studies play important roles in medical research and applications. traditional phenotype-suspicious factor cohort studies have disclosed a large number of important disease risks [cit] . since the human genome was drafted, genome-wide association studies (gwas) have been active, disclosing more and more genetic alleles with significantly high risk for various diseases [cit] . published by oxford university press. all rights reserved. for permissions, please e-mail: journals.permissions@oup. [cit] . these studies also provided a variety of biomarkers and therapeutic targets, which have important application potential in disease diagnosis, treatment and prevention. in the era of precision medicine, case-control studies are exerting more active roles, in disclosing more significant biological features associated with all aspects of medicine and facilitating more precise disease prevention, diagnosis and treatment guidance [cit] . all these studies, however, involve a core analytic procedure, that is, the rate comparison of interesting features between cases and controls."
"the synchronization checks for inter-task data dependencies, and make sure all tasks are returned in-order. the cases of normal result-writing occur when there are no waw or war hazards between current task and its predecessor tasks. therefore, the scheduler consists of a design flow including in-order issue and out-of-order completion."
"acknowledgement this work has been partially supported by the labex persyval-lab (anr-11-labx-0025-01) in the context of the decore project. experiments presented in this paper were carried out using the grid'5000 experimental testbed, being developed under the inria aladdin development action with support from cnrs, renater and several universities, as well as other funding bodies."
"corresponding to the percentage of execution time for different phases, the speedup of the hardware implementation for different functionality modules are illustrated as the speedup term in the legend of fig. 11 . the dct-2d hardware module has been optimized to achieve as high 143.8â, while the cc and quant modules are 66.9â and 15.7â respectively."
"based on the prototype, we designed several example test cases using the specific functions of the implemented ip cores. generally, since the platform integrates both computing processor and hardware ip cores, each application can either run on microblaze processor with software or in specific ip in hardware. in this section, we measure speedups of two typical task sequences through partitioning the test tasks."
"the experimental results for waw data hazards are shown in fig. 6 [e]$ [g] . the maximum experimental speedups for each task sequence are 1.97â, 1.31â, and 0.98â, respectively. it means the scoreboarding on waw hazards can reach 98.70, 98.25 and 98.46 percent of the theoretical peak speedups."
"fisher's exact tests. meanwhile, the top ranked 45 genes with the smallest v 2 and fisher's exact test p-values were retrieved. these genes happened to be mutually covered by those significant ones detected by ebts ( fig. 2a) . for these 45 genes, the significance ranks were compared between the two types of tests (ebt versus v 2 /"
"to implement barriers for synchronization, a barrier table is integrated to ensure that all the returning tasks must not be allowed to issue to the fabric until all participating cores have arrived at the barrier, as is presented in fig. 4 . to achieve this, each core participating in the barrier loads some value(s) into its input queue. once the loads from all of the cores have reached the head of their respective input queues and all tasks have indicated arrival at the barrier. the barrier table also determine that all tasks have arrived at the barrier, with information related to each active barrier. each table contains as many entries as cores attached to a pe cluster, which includes both general processors (denoted in central white block), and heterogeneous accelerators (described in coloured blocks). the table keeps track of the total number of tasks, the number of arrived tasks, and the cores that are participating in the barrier. the number of arrived tasks and participating cores are updated whenever a task arrives, meanwhile the total and arrived task counts are compared to determine when to issue a task. in a system with multiple pe clusters, a dedicated bus communicates barrier updates among clusters. the bus transmits the barrier id as well as the associated application id. all tasks participating in a barrier must be actively running in order for all input data to be available. each table entry maintains a list of the ids of the local tasks that are participating in the barrier as well as a bit indicating if they are actively running. if a barrier is ready to be released but not all participating tasks are active, the scheduler controller triggers an exception to switch the missing tasks back in. once all tasks are available, the barrier can proceed."
"the communication interfaces layer is in charge of data transmission between middleware and reconfigurable hardware platform. generally there are three kinds of primitives: the unified software interface (usi), unified hardware interface (uhi), and unified reconfiguration interface (uri): usi primitive is employed when the information in transferred between two microprocessors. usi is composed of a series of function in libraries. uhi primitive is introduced to model the communication between microprocessor and hardware ip cores. interrupt controller is employed in uhi to detect interrupt requests from interconnections. uri primitive is used only for ip reconfiguration. reconfiguration controller in driver layer is utilized by uri to switch the partial bitstream at runtime. in order to detect data hazards automatically, the scheduler needs to collect all the operands for each issued task. however, we can only keep the limited operands instead of infinite user-defined variables (in most programming models, users are allowed to use any operands as they want). therefore each task operand requires for a table entry allocation. if the table is not full yet, the variable will be renamed to an internal variable implicitly. the internal variable will live for the whole lifecycle of the task execution. furthermore, changing numbers of operands is supported in our programming model. as ã in and ã out indicate the start of the operands array, each operand inside the array will be renamed to a fixed variable implicitly."
"(1) task to core mapping. compared to state-of-the-art middleware and operation systems, this paper utilizes state-of-the-art dynamic partial reconfiguration support at runtime. static core modules and reconfiguration modules (rms) are implemented separately, of which only rms are reconfigured at runtime to reduce the bitstream downloading overheads. in task partition and scheduling layer, reconfiguration core libraries are integrated. after ip cores are reconfigured, tasks mapping and scheduling strategies need to be reconsidered. therefore a task-to-core table is employed to identify the target ip core, as is described in fig. 4 . the table maintains a mapping of tasks to cores to virtualize the selection of the destination core. each table entry contains the task id currently running on that core as well as a count of the number of issued tasks destined for that core. when new an ip core is deployed, the table elements will be flushed and updated."
"the experimental results for raw hazards are shown in fig. 6 [a] $ [d]. the task execution time in the legend illustrates the average execution time for the tasks belong to different series. the experimental results show that the maximum speedup for each task sequence is 3.75â, 1.95â, 1.31â, and 0.99â respectively. this means that the scoreboarding on raw hazards can achieve 93.81, 97.31, 98.45 and 99.05 percent of the theoretical speedups. table 6 lists the task sequences designed to measure the performances for waw hazards. increased from 25 to 100percent, the theoretical speedup of the three task sequences are calculated in (1) to (3), respectively:"
we can choose different threshold values for the prediction probability obtained from the pre-trained classifier. this ensures that we get samples that belong to the correct category and are also informative enough to train good classifiers. we discuss the results of training a classifier and the effect of choosing different probability thresholds in §iv-b.
"one of the major drawbacks of these approaches is that automatic parallelization is not fully supported, which means programmers are required to handle the task mapping and scheduling schemes manually, therefore the speedup achieved is largely confined by the inadequate experiences of programmers. as a side effect, this could also increase the burden of programmers with synchronization and task scheduling on the symmetric multiprocessor architectures. for example, openmp [cit] mainly depends on mutex lock mechanism to achieve synchronization between threads, but mutex locks are managed by the programmer. other parallel programming paradigms such as mpi also needs programmer to find the potential task parallelism and synchronization explicitly, which not only increases the difficulty of multi-core programming complexity, and also led to unsatisfactory task parallelism due to limited experience of programmers. in addition, mapreduce, intel ct [cit], and intel cnc [cit] are approaches facilitating high-level programming paradigms through its inherent support of task-based dataflow execution."
"suppose population a and b, with a sample size of n a and n b, respectively. the number of individuals with an interesting characteristic (positive) is n a and n b for a and b, respectively. let n a /n a ! n b / n b . the null hypothesis (h 0"
"the above example illustrates how the loop based tasks should be analyzed and mapped to general heterogeneous multicore situations for ooo execution, while in the following sections, we will describe how the ooo mechanism is guaranteed in the fpga based mpsoc scenario, especially for massive parallel computing machines."
"for each ip core, the specific task execution time, speedup, area cost and power consumption information are also maintained by scheduler. the information will assist scheduler to make task partition decisions and to achieve better load-balancing status and higher throughputs. since fpga is an area-constrained platform, different ip cores are competing for the limited hardware resources. for task scheduling, tasks are also considered to be arranged in sequences, which should improve the throughput as well as fpga area efficiency."
"moreover, when the results are returned through the fsl bus, scheduler processor is running the subsequent tasks, therefore an interrupt signal is raised to stall the main program. for hardware support to the interrupt mechanisms, an interrupt controller is integrated into the hardware platform, which traces the interrupt events for all the fsl links."
"h igh performance reconfigurable computing technology has emerged and widely applied during the past decades. the remarkable evolution of heterogeneous multicore research paradigms and the invasion of reconfigurable hardware accelerators have made it possible to integrate hundreds of cores into the current petaflop supercomputing machines. the combination of reconfigurable computing and multi-core technologies has been regarded as one of the most promising future processor architectures [cit] . however, critical issues beyond raw computational capabilities are becoming increasingly important, such as programmability, flexibility, scalability, power consumption and so on."
"we present a generative model as a compressed view of real data and show that in low data regimes a generative model can be efficient to store the data as compared to the raw data itself. next, we propose a modification of the building block of the generator and discriminator, which leads to generation of samples that give better classifiers as measured by validation error. we find that in the residual blocks using a concatenation operation instead of addition leads to increased quality and diversity of generated samples. finally, we show that using combining samples from multiple generative models leads to a better coverage of the real data distribution."
"in this study, we proposed an exact binomial test (ebt), which also modeled the rate comparison question with a binomial distribution. it is stricter than traditional binomial distribution based tests so that the power is lowered. however, for a gwas study comparing genes or alleles, the ebt can detect a moderate size of significant factors objectively compared with v 2 /fisher's exact tests or traditional binomial tests with and without fdr control, which are suitable for further experimental investigation or being used as biomarkers or _features for effective prediction model training. to demonstrate the accuracy and reliability of ebt, both modeling and real datasets were tested and the results of different testing methods were compared. a bootstrapping strategy was also proposed to further examine the robustness of exact binomial testing results. as an application, genome-wide somatic gene mutation rates were compared between lung adenocarcinoma (luad) and lung squamous cell carcinoma (lusc) with the new test, and a moderate size of genes were identified with significant difference between groups. we also used the features to build support vector machine (svm) models that could effectively classify the two tumor subtypes, and demonstrated its better performance than the models trained with features identified with v 2 /fisher's exact tests or binomial tests with or without fdr control."
"for raw (25 percent) case, the theoretical speedup is the same as no-hazards. this is because tasks are executed in continuous loops. after loop is unrolled, the first task do_t_adder (a, e) in later loop will run in the meantime of the task do_t_aes_dec(d, h, f) in prior loop, which means the execution time of the adder task will be hidden in the final time. therefore, the theoretical speedups are the same as the test case with only structural hazards."
"and b(n b, p b ) represent the binomial distribution with n a trials and a success rate of p a and that with n b trials and a success rate of p b, respectively. a joint probability, prob joint could be calculated, based on the formula as below:"
"to address the above problems, in this paper we present an architecture support for heterogeneous multiprocessors with hierarchical middleware for ooo execution. we intend to integrate a sound framework that is composed of a hierarchical layer model, an execution flow, an ooo scheduler with a mapping scheme. we claim following contributions:"
"meanwhile, compared to symmetric processors, heterogeneous processors are becoming increasingly dominating in embedded and high performance computing domains. of the cutting-edge researches, fpga based mpsoc is regarded as on of the most promising heterogeneous processor architectures [cit] . with the increasing popularity of reconfigurable computing technology and mpsoc platform, parallelism is shifting from instruction level to task level. one approach is to utilize reconfigurable fpga platform and integrate acceleration engines, such as chimaera [cit], garp [cit], onechip [cit] and other function units [cit] . moreover, there are several creditable general fpga research platforms, such as ramp [cit], platune [cit] and molen [cit] . these studies focus on providing reconfigurable fpga based environments with software tool chains to construct application specific mpsoc. alternatively, products and prototypes of processor are designed to increases tlp with coarser grained parallelism, such as cedar [cit], mlca [cit], multiscalar [cit], trace processors [cit], ibm cell [cit], raw processor [cit], intel terascale and hydra cmp [cit] . these designs present thread-level or individual cores which can split a group of applications into small speculatively independent threads. some other works like trips [cit] and wave scalar [cit] combine both static and dynamic dataflow analysis in order to exploit more parallelism."
"for homogeneous architectures, the difference in resource cost is less than 5.6 percent and that in power consumption is less than 6.8 percent. besides, both speedup and core efficiency difference are less than 2.9 percent. for hybrid architectures, all items have insignificant difference except for the performance metrics with the hardware configuration at 1 mb þ 1 cc þ 1 dct and 1 mb þ 1 cc þ 1 dct þ 1 quant. both of them have a difference up to 14.2 percent. considering that quant and zz/huffman steps in the jpeg 8 â 8 block compression only take a small ratio (2.6 percent depicted in fig. 10 ), therefore any bus delay or communication overheads will have a scaled influence on the system speedup."
"the statistical power of ebts was influenced by sample size, expected success rates of populations (p a and p b ) and significance cutoff (a). the power increases as sample size increases ( fig. 1a and b) . the power also increases as the difference increases between expected success rates or a increases. for example, when the sample size was 70 and a was set as 0.05, the power of 0.45_versus_0.15 (p a _versus_ p b ) reached 0.85, and that of 0.25_versus_0.1 and 0.2_versus_0.1 were only 0.26 and 0.08, respectively (fig. 1a), while the power of 0.2_versus_0.1 increased to 0.38 when the a increased to 0.20 (fig. 1b) . when the sample size increased to 600, the examined tests with different parameters all reached the highest power ( fig. 1a and b) . considering the relatively low frequencies and small rate difference in real genome-scale datasets and the frequently adopted type i error level in practice, 0.2_versus_0.1 and 0.05 were used as representative p a _versus_ p b and a level respectively for later performance evaluation and comparison of different tests ( supplementary fig. s1, distribution of gene mutation rates; [cit] ."
first we train a classifier from the samples of biggan [cit] and compare the training with real data samples. this is to evaluate the extent of correlation between visually convincing samples and the ability to train good classification models.
"each function unit has its own private memory not sharing with other units. the tasks with necessary i/o data are spawned by the messages through hardware interconnects, for example, fsl in the demonstration of this paper. since the tasks throughout this paper are pure functional, the parameters from multiple tasks are synchronized in the synchronization methods."
"in order to evaluate the speedup in raw case, execution time of different tasks is configured to the same value (from 5k to 16k cycles). table 5 lists the task sequences designed to measure the performance for raw data hazards. the four test cases have different percents of raw hazards."
"if current task has no data dependency with previous issued tasks, then it can be spawned immediately. however, dependencies need to be checked and eliminated before they can be spawned. by reviewing the data dependencies problems at instruction level, register renaming techniques are effective methods for ooo execution. traditional approaches, like scoreboarding and tomasulo, are quite successful in the processor architecture designs. one major contribution of task-scoreboarding is an algorithm for ooo task execution."
"(2) barrier synchronization. barriers are one of the most common synchronization operations. however, with a typical memory-based implementation, the overhead of executing a barrier can be significant, especially as the number of table and barrier table, the former table keeps the record of which task is mapping to which core, while the later operates the synchronization between different computational tasks."
"on one hand, for task partition, a task-to-processor table is employed to identify the target processor. the table maintains a mapping of tasks to cores to virtualize the selection of the destination core. each table entry contains the task id currently running on that core as well as a count of the number of issued tasks destined for that core. in most cases, ip core can accelerate task executions by specialized hardware logic or circuit design. so this paper utilizes a greedy strategy: if a vacant idle hardware function unit exists, the task will be sent to hardware; or else, task will be sent to software processor. however, in some cases, this method cannot provide an optimal partition result, therefore the method can be easily replaced by other algorithms."
"in this paper, we have proposed an architecture support for ooo task execution with middleware on fpga based mpsocs. the middleware can automatically identify the parallel region and eliminate the data dependencies with renaming techniques. an adaptive mapping scheme allows the scheduler spawn tasks to a suitable function unit even when the hardware reconfigured. in order to allow ooo task execution, we have proposed task-scoreboarding, a data hazards detecting engine for ooo task execution. regarding processors and ip cores as function units, task-scoreboarding treats tasks as abstract instructions. it can analyze inter-task data dependencies at runtime and issue tasks to heterogeneous function units automatically. experimental results on the state-of-art fpga platform demonstrate that our middleware is flexible to support different ip cores with acceptable hardware costs. the task-scoreboarding can achieve more than 95 percent of the ideal peak speedup. in particular, the task-scoreboarding algorithm costs more than 10 percent overheads than state-of-the-art approaches."
"deep convolutional networks have had success in various image tasks such as image classification, object detection and segmentation. in all these tasks, these models have performed the best as measured by various criteria. on the other side, generative models such as generative adversarial networks [cit] and variational autoencoders (vae) have been proposed and demonstrated to generate images with reasonable similarity to the real training data. recent advances especially in gans, such as biggan [cit] have produced samples which are quite hard to distinguish from real data. in this work, our primary objective is to be able to train good discriminative classifiers from the samples of a generative model. the ability to do this has several applications such as data compression and data privacy protection. a generative model can be thought of as a compressed version of the real data. in a scenario where it is not possible to store or share the real data, we can instead have the compressed generative model. in several applications, such as in the medical domain, data sharing may not be possible due to factors such as protecting data of patients. in these case we can instead share the generative model, and others can reuse this data for other applications, without compromising privacy."
"however, in the case of cifar-100, the probability threshold is important and increasing the threshold leads to a reduction of the classifier error from 39.35% to 37.62%. the threshold makes sure that only good quality image are used for training the classifier. on the other hand, a high value of threshold can reduce the diversity of generated samples which might lead to worse generalisation for the classifier."
"task partitioning and scheduling methods play a vital role in architectural supports. before tasks are offloaded to ip cores, ooo middleware should identify the target processor to run current task, and also decide when the task can be issued."
"the middleware employs user runtime libraries to provide an execution environment for tasks. application programming interfaces (apis) show a high-level view of the internal implementations. after definition, the interfaces should be kept consistent after hardware reconfiguration. moreover, a runtime analysis module is integrated to support application monitoring and bookkeeping techniques. also, the hotspot obtained by profiling indicates the parts executed for high frequencies and can be used to guide ip configurations."
"genome sequencing datasets were also synthesized (supplementary material and supplementary fig. s1 table s1 ). the precision and tpr could not be evaluated with the datasets because no gene with controlled mutation difference between groups was included (supplementary material and supplementary fig. s1 ). taking together, the results indicated that the ebts could improve the power of traditional tests with multi-testing correction but without apparent loss of precision. in other words, ebts could make a better balance between types i and ii errors in multiple rate comparisons, especially in genome-level studies that involve small success rates and more than 10 000 times of comparisons."
"taking together, compared with other statistic tests, ebts could generate a moderate size of significant features in genome-wide rate comparison studies that could be used to train machine learning models with better performance. for some datasets (e.g. prad prognosis), or with a small ( 10) or large (!150) size of features, ebt models could even outperform mrmr models. ebt also appeared better in providing more effective features for classification than traditional tests with 'top n' strategy."
"busy-indicates whether the unit is busy or not. will store values for each variable, if an active task has the variable as its destination. this field is set as blank whenever there are no pending tasks that will write that variable, as shown in table 3, each variable is assigned with a unique id."
"3) variable table- stores the value for each variable. after each task is finished, the results are sent back to variable table through interconnect directly. 4) task partition module-in charge of task partition and mapping. since each task can either run on processor or ip core, thus a good partition method will largely increase the system throughput. for demonstration, we employ a greedy strategy: if there are idle ip cores, the task will be sent to a specific ip; or else, task will be sent to a computing processor. if all the available function units are busy, task must wait until a specific unit is released. 5) function unit monitor-monitors and collects the running information of all the function units. the running information helps task partition module map task to certain unit, and also, can achieve loadbalance of the hardware."
"image samples obtained from a conditional gan may not correspond to the correct category or may not be of a good enough quality. this is due to the fact that we are not able to train perfect generative models. we propose to use a pre-trained classifier, trained on the same training data as the gan, to filter out such samples. this is a general approach that can scale to larger datasets and categories. the procedure for selecting a sample image from the gan that will be used to train a classifier is described in algorithm 1."
"of the cutting-edge gpu and fpga based research approaches, reconfigurable heterogeneous hardware accelerators can achieve both high performance and promising flexibility. on one hand, since numerous processors are being integrated into single chip, reconfigurable multiprocessor system-on-chip can provide increasingly speedups to diverse embedded systems and applications. on the other hand, the involvement of reconfigurable hardware platform like fpga and cpld could efficiently facilitate researchers to decrease the embedded system design time and space costs, as well as to shorten the time-to-market (ttm) simultaneously."
"our training datasets are that of natural images which have significant variation among categories and also within each category. we expect that this implicit distribution is difficult for current generative models to capture. current hypothesis suggests that generative models suffer from mode collapse which leads to the model being unable to capture all aspects of the data distribution. we can train multiple gan models on the same dataset to cover more of this distribution, where each gan model might be able to model different parts of the distribution. we hypothesise that this might be possible due to different starting initialisation and non-convex loss surface during the training procedure of the gan model. during training of the classifier from synthetic samples, we can construct our dataset by sampling from these multiple gan models which can help us to cover the data distribution in a more effective manner. experiments and results are discussed in §iv-e."
"we first profiled the jpeg applications to identify the parts for different phases, as illustrated in the ratio term in the legend of fig. 11 . due to that the zz/huffman phase takes only 2.6 percent of the total execution time, thereby we have not implemented this part as hardware yet. meanwhile, the dct-2d phase is regarded as the major bottleneck of all the four phases, as it takes 73.8 percent of the entire execution time. in contrast, the other two cc and quant steps take 20.8 and 2.8 percent of the total execution time respectively. table 8. table 9 task sequence to test adaptive mapping scheme fig. 10 . reconfigurability study. using jpeg applications to leverage hardware execution with software execution."
"besides the application in new knowledge finding, we also tested the capability of ebt in feature selection for machine learning applications. with two examples, models classifying luad and lusc subtypes and models predicting the prognosis of prad both based on genome-wide gene somatic mutation information, we found ebt almost always outperformed traditional tests with or without fdr control ( fig. 4a and b; supplementary fig. s5 ). therefore, ebt could be advantageous over these traditional tests in selection of features for model training. we also compared ebt with a popular multivariate filter, mrmr ( fig. 4c; supplementary fig. s5 ). the multivariate filters try to reduce the feature information redundancy and therefore improve the model performance [cit] . if the features show little correlation with each other, the filters could not represent their advantages. this could explain the observation of weaker performance of mrmr models compared with ebt models in the luad-lusc classification dataset when feature size is small (e.g. 10), since the few genes could have small information redundancy with each other (fig. 4c ). in such a situation, the relevance of individual features plays important roles. the consistently poorer performance of mrmr models than ebt ones in prad prognosis prediction could also be explained by the low correlation between feature pairs (supplementary fig. s5 ). the general mutation rates of prad cases were low (supplementary table s8), and the information redundancy among the genes appeared lower. taking together, conclusion could not be drawn that ebt is better or worse than mrmr, other multivariate filters or wrappers such as svm-rfe [cit] in selection of features for machine learning application, but instead ebt provides a good option. for a specific feature selection application, ebt could be tried together with other approaches, for different feature size, followed by the selection of most effective ones."
"subsets of the populations a and b with randomly defined sample size (100-500 for the luad versus lusc mutation dataset) were extracted randomly for 100 replicates, followed by independent ebts, v 2 or fisher's exact tests and binomial tests each time. for each replicated experiment, the genes were ranked according to their p-values and the top n genes were recorded (n was set as 50 for the luad versus lusc mutation dataset). for each kind of tests, the times of each gene ranked within the top n among all experiments were counted as testing scores and ordered. the bootstrapping scores (between 1 and 100) reflected the confidence on significance of the rate difference. the most significant n genes detected with a single statistical test were also compared with the top n genes detected most robustly (with largest scores). the mutual coverage also indicated the robustness of the single test."
-2 10 for all svms' cost. the performance of different kernels with best-optimized parameters was then compared and the best kernel (with optimal parameters) was selected for further model training and prediction on the testing dataset. receiver operating characteristic (roc) curve and the area under roc curve (auc) were utilized to assess the predictive performance. an roc curve is a plot of sensitivity versus (1 à specificity) and is generated by shifting the decision threshold. auc gives a relatively objective measure of classifier performance.
"throughout this paper, tasks refer to dynamic instances created when scheduler spawns a piece of code to computing gpps or ip cores. moreover, tasks are regarded as abstract instructions, and each ip core is treated as a dedicated functional unit for a specific hardware task. fig. 3 illustrates the middleware hierarchical architectural model, which consists of four layers in general."
"the heterogeneity in the available sensor platforms result in compatibility issues for the realization of envisioned applications. hence, the need for a standardisation for certain aspects of communication. the ieee 802.15.4/zigbee standard was formed to provide low data-rate wireless transceiver technology with low complexity and longer battery life. in most recent platforms such as cc1000, which is used in mica2 motes [cit], the transmit-receive energy is almost equal. this trade-off has recently tilted toward receive energy consumption in the cc2420 and is therefore used in many platforms and complies with the ieee 802.15.4 standard. receiver electronics dominate amplifier energy consumption due to the increased complexity with spread-spectrum techniques, making receive energy consumption higher than that for transmission. the transmission range of the nodes is assumed to be 10-100 m with data rates of 20 to 250 kbps, depending on the purpose of their deployment. while the physical layer uses binary phase shift keying (bpsk) in the 868/915mhz bands, offset quadrature phase shift keying (o-qpsk) is used in the 2.4ghz band, medium access control layer provides communication for star, mesh, and cluster tree-based topologies with controllers. the symbol rate is 62.5 ks/sec (with 4 bits/s, and bit rate of 250 kb/sec). each symbol is converted into a 32-chip sequence. consequently, the chip rate is 2 mc/sec [cit] ."
"c) hop count: fig. 11 shows the median hop counts per message. as expected, bgr adds several hops to the performance of georouting because of the (intended) redirection of messages along the trail of breadcrumbs. since georouting itself already requires almost twice as many hops as epidemic or prophet (which is also acknowledged as a drawback by its authors [cit] ), bgr naturally \"inherits\" the high hop count. since the underlying geocast algorithm is exchangeable, though, bgr will directly benefit from an improved approach. spray and wait requires the least hops, since the number of copies is limited and nodes must transport messages by locomotion instead of transmission. in terms of propagation delay, bgr is almost on par with georouting in the san francisco scenario, while the additional hops in helsinki lead to a slightly higher delay. in this paper, we present the breadcrumb geocast routing (bgr), a georouting protocol for vehicular networks. bgr not only enables network nodes to route queries into a geographic area, but to route the response back to a moving originator, for which only the initial querying location is known. for this purpose, breadcrumbs are introduced which are a set of messages dynamically kept available for a defined period of time at specific locations along the path of the originator. these breadcrumbs sequentially forward a query response towards the originator. the protocol was evaluated in two different urban scenarios using the one simulator. in the evaluation, the performance of bgr was compared to reference routing protocols such as epidemic, prophet, spray and wait and traditional geocast. additionally, the influence of different parameters such as breadcrumb distribution was evaluated. the results show that bgr avoids up to 97 % of the traffic overhead of epidemic and prophet, while increasing the delivery rate of an existing georouting protocol significantly from about 31 % to almost 100 %. the breadcrumb size has the greatest impact on the performance of bgr, making it a key parameter for the trade-off between network traffic and delivery probability. as future work, we plan to study optimizations, e.g., sending responses to multiple breadcrumbs simultaneously, to reduce the hop count and to provide shorter delays. further, we are going to investigate how breadcrumbs can be dynamically adapted to the street layout, node density and other scenario-specific parameters."
"in existing geocast approaches [cit], if a query response is to be sent to a moving originator, the destination area has typically to be quite large to ensure that the originator is still located within that area when the response arrives. especially in a delay-tolerant environment, however, this approach has several disadvantages, since it leads to a high overhead (if many nodes are involved) or high loss of response messages (if the destination area does not include the originator). in this work, the originator leaves a trail of floating content [cit] areas to ensure that responses can be routed towards it, even if it has already moved arbitrarily far away from its querying location. in analogy to a famous fairy tail 2, we refer to these floating content areas as breadcrumbs, since they are used to keep track of the originator's location and facilitate efficient routing decisions. technically, a breadcrumb is a set of copies of special messages with the same content, which are passed on to each node inside a well-defined and restricted geographic area. hence, the content is kept alive in that area, independent from the actual nodes that are present there, until a certain condition is met or the message lifetime expires. breadcrumbs are based on anchor zones, which are defined by an anchor point p, the anchor zone radius r, an availability threshold a and a time to live (ttl). fig. 1 depicts the according zones and parameters. let h denote the distance of a node n from anchor point p . the distance h determines the probability p f that n forwards its floating content message (in the following referred to as breadcrumb message) to all its neighbors that did not already receive it (which can be efficiently tested using bloom filters [cit] ):"
"in this work, bgr is proposed, an approach that combines the technologies of dtns and geocast and provides a mechanism to not only route a query into a geographic destination area, but also to route the response back to the moving originator of the query, even if its current location is unknown to the respondents. for this purpose, originators leave a trail of floating content [cit] areas, which we refer to as breadcrumbs and which can be used to efficiently forward the response to its destination. to the best of our knowledge, we are the first to facilitate the floating content approach, which we extend significantly, for routing purposes. since bgr can be used on top of different georouting mechanisms, it adds an efficient means to address moving originators even to protocols formerly ignoring the problem. it is evaluated extensively in different traffic scenarios. bgr can be easily extended and is shown to significantly improve the delivery rates of traditional geocast approaches in an efficient way."
"the remainder of this paper is structured as follows. we begin in section ii by studying what other authors have achieved in this field and how it relates to our work. we define relevant concepts and notations in section iii. in section iv, the proposed bgr protocol is presented. our simulation model is introduced in section v. section vi presents our experimental evaluation. the paper concludes in section vii."
"without loss of generality, we assume that the destination area is defined by a geographic center coordinate and a radius which determines the distance of the outline of the destination area to the center. the query q is then sent to the destination area, using a traditional geocast. while we investigated several geocast algorithms for this purpose, in this work we are only describing results using geovanet [cit] due to space constraints. however, the underlying geocast strategy can be easily exchanged."
the combination of the breadcrumb size s and the interbreadcrumb distance d (the distance between anchor points of subsequent breadcrumbs) defines the breadcrumb distribution type. both parameters were evaluated separately and in combination to understand their influence on the routing and to find a satisfying trade-off between cost and benefit.
"in this section, we present bgr, a geographic routing protocol that is not only able to send messages towards a destination area, but also to retrieve information from this area by efficiently routing the query responses back to the (moving) originator. the protocol is divided into the following three phases. in the query phase, the originator sends a query into the destination area. this phase ends when any node in the destination area has received the query. in the response phase, the response from a node inside the destination area (e.g., chosen by the vitp protocol) reaches the querying location. in the tracking phase the response is forwarded along a trail of breadcrumbs until it has reached the originator. if, at any point in time, the ttl of the query or the response expires, the corresponding phase ends, and no subsequent phase is entered. the three phases are shown in fig. 2 . during all phases, each node broadcasts small 1-hop status messages, called beacons, for neighborhood discovery and location detection of neighboring nodes. in the following, the different phases are explained in detail."
"in this work, we consider vehicles (nodes) that are equipped with ad hoc communication technology in order to form a network via vehicle-to-vehicle (v2v) communications. benefits include that no infrastructure installations are necessary and that v2v communications are free of charge as opposed to cellular networks. all nodes are assumed to be able to determine their geographic position. nodes sending a query towards a geographic destination area are referred to as originators, while nodes receiving, processing, and replying to the query are respondents. the location from where a query is sent is referred to as querying location."
"wireless sensor networks (wsns), with a wide range of applications are rapidly becoming an integral part of our lives. recently, considerable amounts of research efforts have enabled the actual implementation and deployment of sensor networks tailored to the unique requirements of certain sensing and monitoring applications. the application of sensor networks are diverse, ranging from habitat monitoring to surveillance and physical intrusion detection and can be categorised into environment, health, military, home, disaster relief, space exploration and other commercial areas. the flexibility, fault tolerance, low cost, rapid deployment characteristics and high sensing fidelity of sensor networks create many new and exciting applications in the field of remote sensing. energy efficiency is crucial because of the scale and application environments in which sensors are deployed [cit] . wsn applications and communication protocols are tailored mainly to provide higher energy efficiency, as sensor nodes carry limited power sources. to provide extensive area coverage, a large number of nodes are required. moreover, to provide a centralised management system of nodes, clustering algorithms are provided as an effective means to extend lifetime and manage wsn's [cit] . significant attention has been paid to clustering strategies and algorithms, yielding a large number of clustering protocols."
"the rest of the paper is organised as follows. section ii presents the background and related work, focussing on ieee 802.15.4 standard, cc2420 radio and clustering. the description of the system and the assumptions considered are presented in section iii. section iv discusses the results obtained using simulation, along with analysis of the results. finally, section v concludes the present work and provides the future directions."
"in comparison, bgr neither requires all nodes to keep routing tables, nor any kind of infrastructure support, nor knowledge about vehicle trajectories or destinations. it therefore provides a versatile and lightweight extension to other geographic routing protocols. as the underlying geocast mechanism is exchangeable, a significant advantage of our approach is that it directly benefits from better or improved georoutings."
"in this section, relevant concepts and notations are defined. in particular, we introduce the concept of breadcrumbs, which are based on the floating content [cit] approach. yet, we significantly extend the original idea by introducing cooperation between and management of multiple floating content areas."
"the following parameters are used throughout this section, unless otherwise stated. a cc2420 chip, compatible with 802.15.4, is used to provide wireless communication, operating at 2.4 ghz and providing a data rate of 250 kbps. the packet size is considered to be 105 bytes [cit] . tmac, which provides both collision avoidance and reliable transmission is used as the mac protocol. each simulation lasts for 100 sec to reach steady state and the results are taken over an average of 100 runs."
"the response phase starts when the message has reached (the first node in) the destination area. inside the destination area, the nodes respond to the message and merge their results into one message that contains all answers to the initial request. this can be achieved by adopting vitp [cit], in which the response message is sent once a return condition is satisfied. it is sent (following the same geocast procedure as in the query phase) to the initial breadcrumb at the querying location."
"the automotive industry is on the brink of equipping modern vehicles with ad hoc communication systems 1, providing a means to share context information and thus enable new types of driver assistance systems. so far, most research in this area has focused on a proactive model to provide such systems with useful information, e.g., by broadcasting awareness and notification messages. while this approach is reasonable for information generally interesting to a large set of vehicles, e.g., safety messages, other applications benefit from a reactive model. the latter allows requests for specific information that need not be disseminated to many vehicles as it would consume both too much bandwidth and processing power in the network, e.g., sharing image data for up-to-date street-view impressions. however, because of the limited communication range of wireless radios and the highly dynamic structure of vehicular ad hoc networks (vanets), often there is no end-to-end path between any two network nodes willing to exchange data. delay-/disruption-tolerant networks (dtns) overcome this intermittent connectivity with a store-carryforward approach and therefore do not require stable links. in a dtn, messages are stored at a node as long as there 1 http://www.car-to-car.org/ is no next hop available for forwarding a message. the node carries the message along its way and forwards it as soon as a new connection for forwarding becomes available. thus, cars can share information that is not time-critical like road closures, parking situation information, entertainment content or even air pollution data. a challenge that arises from the use of geocast for queries is the routing of the response back to the originator of the query. especially in vehicular networks, where messages can potentially have long delays, for example because of missing connectivity, the originator is likely to have moved relatively far away from the location from where the query was started. thus, sending the results to the initial location is not sufficient. enlarging the area to where the response is sent increases the overhead, as more nodes have to be involved in forwarding the message. additionally, since the responding nodes typically do not know about the exact movement of the originator, the area is most likely chosen too large or too small."
"performance modelling and evaluation should consider metrics, such as channel bandwidth and arrival distribution of data packets at the ch, and the introduction of new traffic attributes. in this paper, key issues affecting the practical deployment of clustering techniques are thoroughly investigated. though clustering techniques extend the lifetime of the network, scalability is still an issue, hence the optimality of the cluster size still needs to be thoroughly investigated. in order to have a wider coverage, a trade-off exists between the number of nodes in a cluster being considered and the aggregate packets received at the ch. the results presented show that this analysis can be used to specify the size of a cluster when a specified flow of data is expected from the sensing nodes. in other words, higher packet rates can be accommodated by fewer nodes to attain saturation, as compared to the network with low data rates and higher number of nodes. this directly affects the volume of traffic, the ch can handle. the results presented in order to predict the distribution of interarrival times at the ch follows the exponential distribution phenomenon, thus confirming the markovian nature of the first parameter in kendall's notation."
"for the second scenario, the downtown area around the city center of helsinki, finland is used. the area is 14.67 km 2 in size and includes 119.2 km of helsinki's road network. as in the san francisco scenario, all roads are assumed to be bidirectional. fig. 4b shows the layout of the simulated road network, based on [cit] . 3 http://www.tm.kit.edu/ ∼ mayer/osm2wkt/"
"the san francisco, ca scenario 3 covers the area around the (lower) pacific heights neighborhood and marina district. the area is approximately 5.39 km 2 in size and includes 108.78 km of roads. as is common in u.s. cities, the structure of the road network is for the most part rectangular shaped with two parks in the center of the scenario. fig. 4a shows the layout of the used map."
"a plethora of routing protocols exists in the literature. in this paper, we therefore only consider research work focusing on the problem of georouting in the vehicular domain and, in particular, on the problem of routing messages to moving nodes."
"in recent research focused on energy efficient wsn protocols, clustering is considered as an approach for enabling communication between sensors in a field and the base station (bs) [cit] . group of sensor nodes create clusters with one sensor node amongst them acting as a ch. these ch's are responsible for communicating information from sensor nodes in their clusters to the bs, perhaps through other ch's. the data flow in a cluster network is presented in figure 1 . while the proposed approaches in the research literature are not very specific as to the functionality of the ch's (e.g. they are not like wpan coordinators), in order to distribute the load across sensors in the clusters, cluster heads are periodically changed by an election process [cit] . this improves the number of nodes in the network that are still alive after a given time, thus overall improving the network lifetime. in studies where clustering techniques were primarily proposed for energy efficiency purposes, e.g. leach, heed, eeuc, [cit], the network lifetime was significantly prolonged. clustering is mainly considered to improve scalability and energy efficiency [cit] . besides achieving energy efficiency, clustering reduces channel contention and packet collisions, resulting in better network throughput under high load [cit] . in the wireless domain, a high density of nodes has advantages in terms of connectivity and coverage as well as disadvantages in terms of increased collision and overhead for protocols that require neighbourhood information. as a result, optimising the size of the cluster is an issue in wsn protocols as the nodes increases. this is also related to how much traffic the ch will receive dependent upon it's cluster size. although the protocols developed aim to decrease the contention through either power control or node scheduling, cluster size still is an issue."
"performance modelling and evaluation should consider metrics, such as channel bandwidth and arrival distribution of data packets at the ch, and the introduction of new traffic attributes (such as variations in packet size, rate). minimising the energy consumption and extending the lifetime of the network is possible with the introduction of clustering techniques as they decrease the contention through either power control or node scheduling, however, scalability is still an issue. hence the optimality of the cluster size still needs to be thoroughly investigated. the objectives of this paper are twofold. the first is to identify the bottlenecks in the network, in terms of cluster size scalability, especially while addressing variety of high packet sending rate and real-time applications, such as wearable heart rate and physical activity monitors and holter monitors. this directly effects the volume of traffic, the ch can handle. the second objective is to characterise the distribution of data packet arrivals at the ch, for performance modelling and evaluation and which can be incorporated in energy optimisation studies. in particular, a cluster network is implemented and analysed, using castalia simulation tool, implemented on omnet++."
"in order to evaluate bgr, simulations with the widelyused the opportunistic network environment (the one) [cit] simulator have been conducted. in this section, we present the used scenarios and models."
"sensor nodes have resource constraints in terms of limited energy, limited communication and computational capabilities, and limited memory. a sensor node may belong in one of four categories (1) intel research developed a high bandwidth sensing device such as imote, which has a much broader bandwidth than the earlier ones (bluetooth-based radio) as well as a larger memory; (2) a specialized sensing platform (spec), smaller in size and memory, and has a narrow communication bandwidth and a short radio distance; (3) a generic sensing platform such as the bekeley mote, designed using off-theshelf components and has a bandwidth of 100 kbps or so and more memory, compared to spec; and (4) like sensor node such as stargate, which is a gateway to directly connect mote (or imote)-based devices. these sensor nodes have different levels of resources within them, but they all contain at least the following physical units: a radio unit (transceiver), a processing unit (micro-controller and a memory), a sensing unit, and a limited power supply unit [cit] ."
"at minimum, breadcrumb messages contain information about the position of the preceding breadcrumb (if there is one), the position of the successive breadcrumb, the ids of all queries the breadcrumb corresponds to and the anchor parameters p, r, a. other content is possible, but, in general, the size of the breadcrumb messages should be minimized."
"in this section, we first explore the impact of breadcrumb parameters on the overall performance in order to find good default values. we then present a comparative performance analysis of bgr with other widely-used routing protocols. due to space constraints, we can only present selected results and have to leave others (e.g., effect of ttl on query success ratio and propagation delay) for an extended version."
"in this section, the system description along with the assumptions considered, as well as the queue model of the system is presented. a cluster network with one ch coordinating the cluster operations is considered. sensed information at the nodes is forwarded to the ch which finalises cluster aggregation and transmit all the information to the sink either directly or through intermediary chs. we assume that all the sensor nodes are connected directly to the ch, hence they communicate via their ch. it is assumed that at least one path always exists towards the sink [cit] ."
"as shown in table 4, the above described twophase bootstrapping method has been demonstrated to be beneficial: the learning-based sentiment classifiers t rained o n p seudo-labeled d ata a re superior to lexicon-based sentiment classifiers, including the state-of-the-art unsupervised sentiment classifier problex-dcm [cit] . furthermore, the two-phase bootstrapping method is a general framework which can utilize any lexicon-based sentiment classifier t o p roduce p seudo-labeled d ata. therefore the more sophisticated problex-dcm could also be used instead of psenti in this framework, which is likely to deliver an even higher performance. [cit] ."
"given the induced sentiment lexicon, we propose to use a lexicon-based sentiment classifier to classify unlabeled documents, and then use those classified documents containing at least three sentiment words as pseudo-labeled documents to be used later for the training of a learning-based sentiment classifier. the condition of \"at least three sentiment words\" is to ensure that only reliably classified documents would be further utilised as training examples."
"(1) domain-specific sentiment word embedding, (2) domain-specific sentiment lexicon induction, (3) domain-specific sentiment classification of documents. briefly speaking, given a large unlabeled corpus for a new domain, we would first set up the vector space for that domain via word embedding, then induce a sentiment lexicon in the discovered vector space from a very small set of seed words as well as a general-purpose lexicon, and finally exploit the induced lexicon in a lexicon-based document sentiment classifier to bootstrap a more effective learning-based document sentiment classifier for that domain. the second stage of our approach outperforms the state-of-the-art unsupervised method for sentiment lexicon induction [cit], which is the most closely related work (see section 2). the key to the superior performance of our method compared with theirs is the insight gained from our first stage that positive and negative sentiment words are largely clustered in the domain-specific vector space but these two clusters have a non-negligible overlap, therefore semisupervised/transductive learning algorithms could be easily misled by the examples in the overlap and would actually not work as well as simple supervised classification algorithms. overall, the document sentiment classifier resulting from our nearlyunsupervised approach does not require any labeled document to be trained, and it can outperform the state-of-the-art unsupervised method for document sentiment classification [cit] . the source code for our implemented system and the datasets for our experiments are open to the research community 1 ."
"it has also been observed in our experiments that there is a typical precision/recall trade-off [cit] for the automatic induction of semantic lexicons. assuming that the classified candidate words are added to the lexicon in the descending order of their probabilities (of being either positive or negative), the induced lexicon will be noisier and noisier when it becomes bigger and bigger. table 2 : comparing the induced lexicons with their corresponding known lexicons (ground-truth) according to the ranking of sentiment words measured by au c and kendall's τ . fig. 5 shows that imposing a higher cut-off probability threshold (for candidate words to enter the induced lexicon) would decrease the size of the induced lexicon but increase its quality (accuracy). on one hand, the induced lexicon needs to contain a sufficient number of sentiment words, especially when detecting sentiment from short texts, as a lexiconbased method cannot reasonably classify documents with none or too few sentiment words. on the other hand, the noise (misclassified sentiment words) in the induced lexicon would obviously have a detrimental impact on the accuracy of the document sentiment classifier built on top of it. [cit] which tries to expand the sentiment lexicon as much as possible and thus maintain a high recall, we would put more emphasis on the precision and keep a tight control of the lexicon size. for us, having a small sentiment lexicon is affordable, because our proposed approach to document sentiment classification will be able to mitigate the low recall problem of lexiconbased methods by combining them with learningbased methods, which we shall talk about next."
"a full nonlinear vehicle model in carsim is used to track the optimal path by importing the path data from matlab. figure 4 shows the path tracking performance. it can be seen that the trajectories are in good agreement, which indicates that the optimal path satisfies the vehicle dynamics constraint. figure 5 shows the steering wheel angle of matlab and carsim. it can be seen that the outputs of both are basically consistent, and thus the correctness of the inverse dynamics method is verified."
"in this paper, we have proposed synchronization-free algorithms for parallel sptrsv and sptrsm. these methods completely eliminate the overheads for generating level-sets or colour-sets (in the preprocessing stage) and for explicit runtime barrier synchronization (in the solving stage). meanwhile, they adaptively select optimization paths for best parallelism in the case of multiple right-hand sides. experimental results show that our approach makes preprocessing up to two orders of magnitude faster than level-set methods, and demonstrates significant speedups over vendor supplied parallel routines for forward and backward sptrsv and sptrsm in single and double precision."
"overtaking is a common phenomenon in the process of vehicle driving. inappropriate timing of an overtaking, or operational errors, can lead to traffic accidents. in this paper, the entire overtaking process is divided into three phases: lane changing, overtaking, and merging. taking safety as the premise, aiming at overtaking the vehicle ahead as quickly as possible, and considering the safety distance and relationship between the overtaking and overtaken vehicles during the overtaking process, a new vehicle-overtaking model is established to provide judgment and support. in this paper, the simulated scenarios of a vehicle overtaking are simplified as follows:"
"a domain-specific sentiment lexicon, automatically induced using the above technique, provides a solid basis for building domain-specific document sentiment classifiers. for the experiments here, we would use a list of 7866 candidate words constructed by merging two well-known general-purpose sentiment lexicons that are both publicly available -the 'general inquirer' [cit] . this set of candidate words is itself a combined, general-purpose sentiment lex- icon, so we name it the gi+bl lexicon. moreover, we would set the cut-off probability threshold to a generally good value 0.7 in our sentiment lexicon induction algorithm. comparing the imdb vector space including all the candidate words ( fig. 4a) with that including only the high-probability candidate words (fig. 4b), it is obvious that the positive and negative sentiment clusters become more clearly separated in the latter."
"the rest of this paper is organized as follows. in section 2, we review previous studies on this topic. in sections 3 to 5, we describe the three main stages of our approach respectively. in section 6, we draw conclusions and discuss future work."
"when the vehicle is under a braking force, and all wheels are assumed to be in a lock-braked condition, the constraints on f xf and f xr are expressed as follows:"
"on the other hand, because the parameter q is a split point for column length (recall figure 6 ), setting a proper value for q depends strongly on the test matrices selected. in our benchmark suite, we fix p to 8 of the matrices (specifically, 19 out of 20) do not have columns with more than a few tens of nonzeros, meaning that they are insensitive to a larger q, thus no noticeable performance difference is observed. the exception is the factorized matrix g7jac140sc, which is much denser than the other test matrices (see table iii ). in this case, setting q to values larger than 1024 gradually improves performance, since this scheme exploits more parallelism over the right-hand sides for updating subsequent components with lower latency."
"therefore, considering the control input and response of the vehicle, starting the overtaking process at a lower speed is beneficial to the safe driving of the vehicle."
"the lexicon-based sentiment classifier used in our experiments is a publicly-available system called psenti 2 [cit] . in addition to a customizable sentiment lexicon, it also uses shallow nlp techniques like part-of-speech (pos) tagging and the detection of sentiment inverters and other modifiers (intensifying and diminishing adverbs)."
"we also notice that compared to the kepler-and maxwell-based gpus used in our previous work [cit], the pascal-based titan x gpu offers higher performance. the major reason is that the pascal architecture is equipped with higher bandwidth and improved micro-architectures for atomic operations, which are extensively utilized in our approach. actually, scogland and feng [cit] also confirmed that atomic operations have been continuously improved in the latest generations of modern gpus. moreover, although the amd fury x gpu has slightly higher bandwidth than the nvidia titan x, it is in general slower for our synchronization-free sptrsv algorithm. the main reason is probably the implementation differences for warp/wavefront scheduling in the two vendor's products. to further measure the impact of using the on-chip memory for lower latency, we list the performance improvement obtained by this optimization technique in figure 9 . because an nvidia cuda thread block allows up to 1024 threads (i.e., 32 warps of 32 threads), we report different thread block configurations of 4-, 8-, 16-and 32-warps in figures 9 (a)-(d), respectively. in contrast, opencl on amd cards can use up to 256 threads (i.e., four wavefronts of 64 threads). thus we only test performance of thread block of four wavefronts and plot it in figure 9 (e). in each subfigure, the forward and backward substitution of the 20 test matrices are listed (see 40 sample points on the x axis), and the median and harmonic mean values of the speedups are highlighted. as can be seen, the on-chip memory optimization technique yields higher performance in most of the cases (i.e., with speedups higher than 1.0). specifically, the 32-warp setting (figure 9 (d) ) yields the highest speedups since its diagonal blocks (recall red areas in figures 4 and 5) are larger than those in other settings. however, the absolute throughput of the 32-warp setting (not shown here for brevity) is in general a bit slower than the other three with very similar performance. so in this paper we always set the number of warps to 16. as for the amd fury x, we also notice obvious speedups from the on-chip memory optimization. but because of the limited combinations, we use the 4-wavefront configuration in our test. (e) fury x (4-wavefront) figure 9 . the impact of using the on-chip memory for better performance. the y axis shows speedups with this optimization technique, and the x axis shows forward and backward substitution of the 20 test matrices. the median and harmonic mean of the speedups are highlighted."
"system with multiple right-hand sides (sptrsm) 2.2.1. data-parallel sptrsm algorithm compared to sptrsv with a single right-hand side, the sptrsm kernel has better data-level parallelism since its multiple right-hand sides can be processed in parallel. moreover, since entries in each column are independent of each other, they can be processed in parallel as well. algorithm 2 shows a data-level parallel method for sptrsm. it can be seen that the main for loop (lines [cit] has two optimization strategies for leveraging data-level parallelism: one is to parallelize multiple entries in each column (line 8), the other is to parallelize multiple right-hand sides (line 9)."
"first, we try the induced sentiment lexicons in the lexicon-based sentiment classifier psenti [cit] to see how good they are. given a sentiment lexicon, psenti is able to perform not only binary sentiment classification but also ordinal sentiment classification on a five-point scale. to measure the binary classification performance, we use both micro-averaged f 1 (mif 1 ) and macro-averaged f 1 (maf 1 ) which are commonly used in text categorization [cit] . to measure the fivepoint scale classification performance, we use both cohen's κ coefficient [cit] and also root-mean-square error (rm se) [cit] . as the baseline, we use a combined generalpurpose sentiment lexicon, gi+bl, mentioned previously in section 4. as we can see from the results shown in table 3, using the induced sentiment lexicon for the target domain would make the lexiconbased sentiment classifier psenti perform better than simply employing an existing general-purpose sentiment lexicon. moreover, using the sentiment lexicons induced from the same domain would lead a much better performance than using the sentiment lexicons induced from a different domain."
"one promising way to further enhance the lstmbased sentiment classifier in the proposed approach with the induced sentiment lexicon would be to concatenate word embeddings with an indicator feature which tells whether a current word is positive, neutral, or negative [cit] . we leave this for future work."
"assuming that the tire cornering properties are within the linear range, the vehicle steering motion model is simplified as a 3-degree of freedom (dof) linear vehicle model with lateral motion, yawing motion, and longitudinal motion, as depicted in figure 1, where the differential equations of motion are expressed as follows:"
"most of the early sentiment analysis systems took lexicon-based approaches to document sentiment classification which rely on pre-compiled sentiment lexicons [cit] . various methods have been proposed to automatically produce such sentiment lexicons [cit] . later, the focus of research shifted to learning-based approaches [cit], as supervised learning algorithms usually deliver a much higher accuracy in sentiment classification than pure lexicon-based methods. however, lexicons have not completely lost their attractiveness: they are usually easier to understand and to maintain by non-experts, and they can also be integrated into learning-based sentiment classifiers [cit] figure 1: our nearly-unsupervised approach to domain-specific sentiment classification."
"here, δ sw is bounded by the driver's physiological limit, and the control variable f xf is bounded by the road adhesion. when the vehicle has front-wheel drive:"
"some researchers have also utilized atomic operations for improving fundamental algorithms such as bitonic sort [cit], prefix-sum scan [cit], wavefront [cit], sparse transposition [cit], and sparse matrix-vector multiplication [cit] . unlike those problems, the sptrsv operation is inherently serial and thus more irregular and complex. we also use atomic operations both in on-chip and off-chip memory, and set atomic operations as the central part of the whole algorithm. moreover, we recently noticed that bypassing caches [cit], improving thread-groups locality by clustering [cit] and utilizing on-package high bandwidth memory [cit] can further improve algorithm performance. we leave this extension as future work."
"based on the sptrsv approach described in the previous section, we develop an extended synchronization-free algorithm for sptrsm. the key idea is to adaptively select an optimization strategy (i.e., parallelizing multiple nonzero entries in a column, or parallelizing multiple right-hand sides) at runtime for each column to achieve best performance."
"the optimization method used in this paper is a type of sqp algorithm, namely, the wilson-han-powell method, which is based on the general lagrange-newton method. the sequential quadratic programming method is a commonly used optimization method for solving the nonlinear programming problem. the basic thinking of sqp is to transform the nonlinear programming problem into a series of quadratic programming problems according to the literature [cit] . considering the optimal control problem with general nonlinear constraints, where f (x) and c i (x) are real-valued continuous functions, at least one of which is nonlinear. sub-problems are constructed as where"
"drawing an analogy to the well-known cluster hypothesis in information retrieval (ir) [cit], here we put forward the cluster hypothesis for sentiment analysis: words in the same cluster behave similarly with respect to sentiment polarity in a specific domain. that is to say, we expect positive and negative sentiment words to form distinct clusters, given that they have been represented in an appropriate vector space. to verify this hypothesis, it would be useful to visualize the high-dimensional sentiment word vectors in a 2d plane. we have tried a number of dimensionality reduction techniques including the t-distributed stochastic neighbor embedding (t-sne) [cit], but found that simply using the classic principle component analysis (pca) [cit] works very well for this purpose."
"figures 7 and 8 show the single and double precision sptrsv performance on the 20 matrices measured on the three platforms. overall, the methods in mkl are relatively slow when the parallel degree is high, but behave better when the parallel degree is low (meaning that operations are more sequential). the cusparse library exhibits opposite performance: showing relatively better performance when the parallel degree is high but inferior performance when the parallel degree is low. nevertheless, on average (harmonic mean), the mkl and cusparse libraries show specifically, on the pascal-based titan x gpu, our synchronization-free algorithm demonstrates an average speedup over the cusparse library of 2.42 times in single precision and 2.34 times in double precision for forward substitution, and 1.77 times in single precision and 1.77 times in double precision for backward substitution. the maximum speedups are 6.11, 5.49, 4.65, 4.22, respectively. these best speedups are all from matrices, such as cant and dc2, that have most nonzero entries in diagonal blocks. for those matrices, the optimizing strategy of using both scratchpad and off-chip memory improves the overall performance. also, it can be seen that our method achieves speedups of 2.73, 2.56, 2.8 and 2.65, [cit] . this matrix requires 82,735 runtime synchronizations (see table iii ) limiting its performance for the level-set methods. in contrast, our method avoids synchronizations and obtains much superior performance. for the same reason, our method shows comparable performance compared to existing methods on matrix nlpkkt160, which requires only two runtime synchronizations."
"our approach to domain-specific document-level sentiment classification is built on top of word embeddings -distributed word representations (vectors) that could be learned from an unlabeled corpus to encode the semantic similarities between words [cit] . in this section, we investigate how the embeddings of sentiment words for a particular domain would look like in the domain-specific vector space. to ensure a fair comparison with the state-of-theart sentiment lexicon induction technique sentprop 3 [cit] later in section 4, we adopt the same publicly-available pre-trained word embeddings for the following three domains together with the corresponding sets of sentiment words (i.e., sentiment lexicons)."
"the simulation results of the steering angle, roll rate, and yaw rate of the overtaking vehicle at different speeds are shown in figures 9, 10, 11, respectively. it can be seen from figures 9 and 10 that, at a speed of 160 km/h, the steering angle and roll rate are greater. figure 11 shows that the yaw rate is smaller at 160 km/h, but the integral value of the yaw rate is greater when at a higher speed."
"the first objective of this work is to eliminate the cost of generating level-sets and the barrier synchronizations between the sets. due to the inherent dependencies among components, the major task for parallelizing sptrsv is to clarify such dependencies and to respect them when solving at runtime."
"in this work, we use gpus as the platform for exploiting inherent parallelism when there are many components for a very large matrix. we assign a warp of threads to solve a single component of x (a warp is a unit of 32 simd threads executed in lock-step for nvidia gpus. for amd gpus the warp is 64 threads and is denoted by the term wavefront). to respect the partial order of sptrsv, we need to be sure that the warps associated with dependent entries (if any) must be finished first. thus thread-blocks of multiple warps need to be dispatched in ascending order, even though they can be switched and finished in arbitrary order. since the partial order is essentially unidirectional (i.e., any component only depends on previous components but not on later ones in forward substitution, see figure 1 (b), and vice versa in backward substitution), we can map entries to warps and strictly respect the partial order of the entries so that no warp execution deadlock will occur."
"if one thread is used for solving a batch of components (i.e., a row of the solution matrix x), the two optimization strategies for parallelizing the two for loops are mutually exclusive. that is to say, a program must select either for loop to parallelize. because of the different column lengths and the number of right-hand sides, directly parallelizing any one of the two for loops may not always achieve the best performance. figure 2 shows the distribution of column lengths for the four matrices used above. it can be seen that the distribution varies from matrix to matrix. some matrices only have short columns, meaning that the simd parallelism of column length may not be enough for saturating modern gpus (for the nvidia the warp size is 32 and for the amd the wavefront size is 64). for instance, assume that the number of right-hand sides is 16, then parallelizing the loop on right-hand sides (optimization 2 in line 9 of algorithm 2) may give the best performance for matrix chipcool0 as most of its columns are shorter than 16. but this may degrade throughput for matrix fem/ship 003 since most of its columns are longer than 16, and in such a case parallelizing the loop on column length (optimization 1 in line 8 of algorithm 2) will be expected to be better."
"when setting up a model without a driver, the driver's input can be obtained if the model of the vehicle and the motion state are known [cit] . this method is able to compare the maneuverability of different vehicles operated in the most efficient manner. the specified maneuverability involves accurately reaching a given state variable [cit], tracking a given path accurately [cit], and passing through the given path within the shortest time without deviating from the given path boundary [cit] . the latter situation is the issue studied in this research."
"this adaptive method can be illustrated in a two-dimensional space shown in figure 6 . the two dimensions are the length of a given column (the number of nonzero entries in the column, expressed as col len, i.e., 'column length', on the x axis) and the number of right-hand sides (expressed as rhs, i.e., '#right hand sides', on the y axis). two parameters p and q are used for partitioning the space vertically and horizontally, respectively. hence the space is divided into four areas utilizing different optimization strategies to process a column:"
"when we need to perform sentiment classification in a new domain unseen before, there are usually neither labeled dictionary available to employ lexicon-based sentiment classifiers nor labeled corpus available to train learning-based sentiment classifiers. it is, of course, possible to resort to a generalpurpose off-the-shelf sentiment classifier, or a prebuilt one for a different domain. however, the effectiveness would often be unsatisfactory because of the reasons mentioned above. there have been some studies on domain adaptation or transfer learning for sentiment classification [cit], but they still require a large amount of labeled training data from a fairly similar source domain, which is not always feasible. those algorithms also tend to be computational-expensive and time-consuming [cit] ."
"we have implemented the proposed synchronization-free sptrsv method both in cuda and in opencl and have evaluated it on two gpus: an nvidia geforce titan x gpu of pascal architecture, and an amd radeon r9 fury x gpu of gcn architecture. we also benchmark the most recent sptrsv and sptrsm implementations from two libraries cusparse v8.0 and mkl v11.3 update 3 provided by nvidia and intel, respectively. we execute each method a hundred times and use the arithmetic average of the runtime to calculate the gflop/s rates."
"the induced sentiment lexicon on its own could be applied directly in a lexicon-based method for sentiment classification of documents, and a reasonably good performance could be achieved as we will show later in table 4 . however, most of the time, lexicon-based sentiment classifiers are not as effective as learning-based sentiment classifiers. one reason is that the former tends to suffer from a poor recall. for example, with a limited size sentiment lexicon, lexicon-based methods would often fail to 277 detect the sentiment present in short texts, e.g., from twitter, due to the lexical gap."
"the derivative of the state variable can be obtained by taking the derivative of eq. (12), thereby converting the dynamic differential equation constraint into an algebraic constraint, that is,"
"to simulate the trajectory of the vehicle, we established a ground reference coordinate system (figure 1 ). letting the coordinates of the vehicle's center of mass in the xoy coordinate system be x, y, and the angle between the x' axis of the vehicle coordinate system and the x axis of the ground reference coordinate system be θ, the vehicle velocity in the ground reference coordinate is projected as"
"the driver's control input will be affected by the driver's subjective feelings, previewing time, reaction time, and other uncertain factors, which lead to a lack of uniform and accurate mathematical models in the study of closed-loop systems [cit] . the study of inverse dynamics in vehicle handling is in contrast to the positive problem."
"assuming that the vehicle is traveling in a one-way twolane environment, there are no vehicles in the adjacent lane, and both vehicles are traveling at a uniform speed."
"second, to evaluate the proposed two-phase bootstrapping method, we make empirical comparisons on the imdb and amazon datasets using a number of representative methods for document sentiment classification:"
"considering the influence of the driving/braking force, the lateral forces of the front and rear wheels are (1) where ϕ is friction coefficient of the road surface, and f zf and f zr are the vertical forces of the front and rear wheels, respectively. considering the longitudinal load transfer, the vertical forces of the front and rear wheels are where h g is the height of vehicle's center of mass."
"in this paper, we propose an end-to-end pipelined nearly-unsupervised approach to domain-specific sentiment classification of documents for a new domain based on distributed word representations (vectors) . as shown in fig. 1, the proposed approach consists of three main stages (components):"
"there is often the need to perform sentiment classification in a particular domain where no labeled document is available. although we could make use of a general-purpose off-theshelf sentiment classifier or a pre-built one for a different domain, the effectiveness would be inferior. in this paper, we explore the possibility of building domain-specific sentiment classifiers w ith u nlabeled d ocuments o nly. our investigation indicates that in the word embeddings learned from the unlabeled corpus of a given domain, the distributed word representations (vectors) for opposite sentiments form distinct clusters, though those clusters are not transferable across domains. exploiting such a clustering structure, we are able to utilize machine learning algorithms to induce a quality domain-specific sentiment lexicon from just a few typical sentiment words (\"seeds\"). an important finding is that simple linear model based supervised learning algorithms (such as linear svm) can actually work better than more sophisticated semi-supervised/transductive learning algorithms which represent the state-of-theart technique for sentiment lexicon induction. the induced lexicon could be applied directly in a lexicon-based method for sentiment classification, but a h igher p erformance c ould be achieved through a two-phase bootstrapping method which uses the induced lexicon to assign positive/negative sentiment scores to unlabeled documents first, a nd t hen u ses those documents found to have clear sentiment signals as pseudo-labeled examples to train a document sentiment classifier v ia supervised learning algorithms (such as lstm). on several benchmark datasets for document sentiment classification, our end-to-end pipelined approach which is overall unsupervised (except for a tiny set of seed words) outperforms existing unsupervised approaches and achieves an accuracy comparable to that of fully supervised approaches."
"the longitudinal projection distance between the overtaking vehicle and overtaken vehicle, and the longitudinal projection distance between the overtaking vehicle and the lane boundary, are as shown in figure 7 . when the projection distance is zero, it indicates that the two vehicle projections coincide in the driving direction. it can be seen from figure 7 that the longitudinal projection distance between the overtaking vehicle and the overtaken vehicle is greater at a higher speed, and the longitudinal projection distance between the overtaking vehicle and the lane boundary is smaller at a higher speed. the table 1 reverse is true at a lower speed. the reasons for this are described above. during the overtaking process, the relationship between the longitudinal distance of the two vehicles and the driving distance is as shown in figure 8 . when the distance between the two vehicles is zero, it indicates that the lateral projections of the two vehicles coincide in the driving direction. from figure 8, it can be seen that the absolute value of the slope during the process of a lane change is greater than the absolute value of the slope when driving straight; in addition, the slope of the curve during the process of merging is less than the slope of the curve when driving straight. the reason for this is that the relative longitudinal velocity of the two vehicles increases during a lane change, whereas the relative longitudinal velocity of the two vehicles decreases during the merging process."
"sentiment analysis [cit] ) is a popular research topic which has a wide range of applications, such as summarizing customer reviews, monitoring social media, and predicting stock market trends [cit] . a basic task in sentiment analysis is to classify the sentiment polarity of a given piece of text (document), i.e., whether the opinion expressed in the text is positive or negative [cit], which is the focus of this paper."
"with continuous improvements in traffic conditions, vehicles have entered a high-speed era. it is thus necessary to solve the \"minimum time problem\" in vehicle handling dynamics to satisfy the stability, safety, maneuverability, and high-speed characteristics of a vehicle. with the aim of eliminating the restrictions of slow vehicles or other factors such as road structures, high-speed vehicles will choose when to change lanes to obtain a better driving environment or achieve the desired driving purposes."
the optimal control problem of the time-varying system considered in this study can be solved when transforming it into a finite-dimensional nonlinear programming problem using the direct collocation method above:
"the official performance measures for this short text sentiment classification task [cit] include accuracy (acc) and f 1 . although our approach is nearly-unsupervised (without any reliance on labeled documents), its performance on this benchmark dataset is comparable to that of supervised methods: it would be placed roughly in the middle of all the participating systems in this competition (see table 5 )."
"many real-world applications of sentiment classification (e.g., on social media) are not simply a binary classification task, but involve a neutral category as well. although many lexicon-based sentiment classifiers including psenti can detect neutral sentiment, extending the above learning-based sentiment classifier (trained on pseudo-labeled data) [cit] which is to classify 12379 tweets into an ordinal five-point scale (−2, −1, 0, +1, +2) where 0 represents the neutral class."
"how far can we go in sentiment classification for a new domain, given only unlabeled data? this paper presents our exploration towards answering the above research question. specifically, the main contributions of this paper are as follows."
"1. divide the given partition interval into n equal parts, and n + 1 nodes can then be obtained. 2. using s j as the estimated value of the control variable at the selected node, if the control variable value is known, then use the initial value of the state variable x j to find the state variable value of each node in the sequential iterations, and thus we obtain x n+1 and the performance index j. therefore, the solution to the state equation and the performance index are regarded as the control variable functions of each node."
"the motivation for parallel-sptrsv comes from the observation that some components/vertices are independent and can be processed simultaneously (e.g., vertices 0 and 1 in figure 1 (b)). therefore, the components can be partitioned into a number of sets so that components inside a set can be solved in parallel, while the sets are processed sequentially (i.e., level by level). with this observation, anderson and saad [cit] and saltz [cit] introduced a preprocessing stage to perform such a partition before the solving stage. figure 1 (c) shows that five level-sets are generated for the matrix l. consequently, levels 0, 1 and 2 can use parallel hardware (e.g., a dual-core machine) for accelerating sptrsv. however, between sets, dependencies still exist so synchronization is required at runtime."
"in this paper, we only show forward and backward substitution sptrsm in double precision for brevity. figures 10 and 11 show the performance of the serial/parallel methods in vendor libraries mkl and cusparse, and three optimization strategies: (1) sync-free opt1 (parallelizing column entries in our synchronization-free method), (2) sync-free opt2 (parallelizing right-hand sides in our synchronization-free method), and (3) sync-free adaptive (our synchronization-free method with the adaptive parameter selection illustrated in figure 6 and described in algorithm 4) running on nvidia and amd gpus. it can be seen that our synchronization-free method is in general much faster than cusparse, especially when the number of right-hand sides is large. this trend is the same as that shown for sptrsv. however, there are two exceptions where cusparse behaves better. one is for matrix nlpkkt160, where its parallelism is much higher than other matrices, and so cusparse outperforms our method. but since cusparse requires more space to save level-set information, it cannot process as many right-hand sides as the synchronization-free algorithm can (specifically, cusparse does not work for more than 8 right-hand sides in our test). another exception is from the backward substitution of matrices rajat18 an dc2, where the row/column lengths of their upper triangular part are distributed in a power-law fashion (i.e., several are of size o(n) and the rest are of size o(1)). in this case, the in-degree of some components is relatively high thus causing more load unbalanced traffic for the atomic operations in our method. as a result, the performance of the cusparse method is relatively higher. nevertheless, for most of the cases, our synchronization-free algorithm achieves significant speedups (up to a few tens) over mkl and cusparse. in addition, cusparse fails to solve both the l and u parts of matrices road central and road usa, maybe due to its complex approach analysing their sparsity structures. it can also be seen that the speedups in sptrsm are more noticeable than in the sptrsv test. the reason may be that cusparse does not select the best parallel scheme for multiple right-hand sides."
"in summary, the above studies did not involve the minimum time handling problem while overtaking in a dynamic environment, that is, the lateral and longitudinal safety distances in the minimum time handling problem while overtaking were not considered. the longitudinal safety distance refers to the idea that the overtaking vehicle should keep a certain distance with the overtaken vehicle during the overtaking process, thereby avoiding traffic accidents such as a rear-end collision. the lateral safety distance refers to the lateral lane constraint, as well as the constraint of the vehicle width variation caused by a roll. the lateral safety distance is used to ensure the vehicle completes the overtaking behavior only within its own lane and the left adjacent lane; this can avoid the scraping of two vehicles owing to variations in body width. therefore, considering the influence of the longitudinal and lateral safety distances, with the aim of overtaking the front vehicle as quickly as possible, the optimal overtaking trajectory and the shortest overtaking time are figured out using the inverse dynamics method."
"(10) figure 3 test road for vehicle minimum overtaking time where equations (11a)-(11d) can be obtained based on the objective function, dynamic equations, and constraints of the inverse dynamics problem."
"tables iv and v show the preprocessing overheads of the parallel forward and backward sptrsv and sptrsm implementations from mkl, cusparse and our approach on the three platforms. as can be seen, our method achieves an average speedup of over 48.4 (maximum of 121, from backward substitution of matrix chipcool0) over the sptrsv method in the cusparse library on the titan x card. for the sptrsm operation, the speedups are on average over 322.9 with a maximum of 690 (from backward substitution of matrix cit-hepth). the major reason is that the vendor supplied implementation attempts to find level-sets in the preprocessing phase. moreover, the amd fury x gpu offers comparable cost for preprocessing, due to similar off-chip memory bandwidth. table v . preprocessing cost (in millisecond) of the tested methods for backward substitution on three devices."
"in terms of the longitudinal distance that the vehicle drives during the overtaking process, it can be seen from figures 6, 7, 8 that the distance the vehicles drive is shorter when at a higher speed, and the distance that the overtaking vehicle and the overtaken vehicle drive in parallel is also shorter when at a higher speed. thus, a higher speed is more conducive to driving safety."
"the control variable f xf is constrained by the maximum driving force provided by the power transmission system. according to the relationship between the engine speed and the speed of the vehicles, as well as the relationship between the engine output torque and the driving force, the relationship between the maximum driving force and the speed of the vehicles can be obtained based on the external characteristics of the engine."
"(1) considering the required longitudinal and lateral safe distances during the overtaking process, an optimal control model is established, where the overtaking time is regarded as the performance index. (2) the gauss pseudospectral method is applied to discretize the optimal control problem, which is then solved through sequential quadratic programming."
"the proposed synchronization-free algorithm for sptrsv (forward substitution). figure 3 illustrates the procedure for our synchronization-free algorithm ‡ using an example. suppose there are three warps enrolled, tagged as warp0, warp1 and warp2. they follow the same procedure and are context-switched by the hardware scheduler. for an arbitrary warp, the central region contained in the red dotted box (labelled as the critical section protecting the left sum array) separates the whole procedure into three phases: lock-wait, critical section and lock-update. in the lock-wait phase, the warp iteratively evaluates the status of the lock protecting the critical section of the current warp. if locked, it waits in the loop (known as spinning); otherwise, it stops waiting and enters the next phase. although the lock here is a spin-lock, it does not have the busywaiting problem. based on our observation, if the clock() function is invoked inside the waiting loop, the nvidia nvcc compiler would not start the waiting loop for some 'optimization' reasons, so a signal will be sent to the hardware warp scheduler to switch to the next warp context. this avoids the execution deadlock. in contrast, the amd opencl compiler does not have this risk at all, so 7 the waiting loop in our opencl version does not have to add any functions to prevent deadlock. in the critical section phase, the warp updates the components in left sum that have dependencies on the components that the warp is currently working on. this is done in an order that depends on the partial dependency defined by the sparsity structure. after that, it aborts the critical section and enters the lock-update phase. in the last lock-update phase, the warp updates the dependent in degree array, in the same order as for left sum (so that all the order dependencies are strictly respected). depending on the number of components in that column (line 15 in algorithm 3), it may require one or several updates. when an in-degree is updated to reach the target value (so that all the dependencies of the component are resolved), the lock corresponding to that in-degree is unlocked. consequently, the warp waiting for that lock can abort the waiting phase and enter its critical section."
"given the word embeddings for a specific domain, we can induce a customized sentiment lexicon from a few typical sentiment words (\"seeds\") frequently used in that particular domain. such an induced domain-specific sentiment lexicon plays a crucial role in the pipeline towards domain-specific document-level sentiment classification. table 1 [cit] except for the two additional domains imdb and amazon. the induction of a sentiment lexicon could then be formulated as a simple word sentiment classification problem with two classes (positive vs. negative). each word is represented as a vector via domain-specific word embedding; the seed words are labeled with their corresponding classes while all the other words (i.e., \"candidates\") are unlabeled; the task here is to learn a classifier from the labeled examples first and then apply it to predict the sentiment polarity of each unlabeled candidate word. the probabilistic outputs of such a word sentiment classifier could be regarded as the measure of confidence about the predicted sentiment polarity. in the end, those candidate words with a high probability of being either positive or negative would be added to the sentiment lexicon. the final induced sentiment lexicon would include both the seed words and the selected candidate words."
"neural network (rnn) that can remember values over arbitrary time intervals [cit] . to apply the deep learning algorithms cnn and lstm that have a word embedding projection layer, we fix t he r eview s ize t o 5 00 w ords, t runcating reviews longer than that and padding reviews shorter than that with null values. [cit], the hidden layer size is an important hyperparameter of lstm: usually the larger the network, the better the performance but the longer the training time. in our experiments, we have used an lstm network with 400 units on the hidden layer which is the capacity that a pc with one nvidia gtx 1080 ti gpu can afford and a dropout [cit] ) rate of 0.5 which is the most common setting in research literature [cit] ."
the solution to the optimal control problem transformed from an inverse dynamics problem of vehicle handling is converted into solving the nonlinear programming problem through the transformation mentioned above.
"lines 8-26 in algorithm 3 give the pseudocode for the solving stage of our synchronization-free sptrsv method. we can optimize this by exploiting the gpu on-chip scratchpad memory. when warps in the same thread-block share the same portion of on-chip memory, some components' dependencies may be resolved within the thread block with lower latency. our implementation allocates two sets of intermediate arrays, one set on local scratchpad memory (s left sum and s in degree) and the other set on gpu off-chip global memory (d left sum and d in degree), see line 1 of algorithm 3. when a warp finds a dependent entry (the later entry that depends on the current one) in the same gpu thread-block composed of multiple warps, it updates the local arrays (lines [cit] in the scratchpad memory for faster accessing. otherwise, it updates the remote off-chip arrays (lines [cit], to notify warps from other thread-blocks. the sum of the two arrays (line 11) is used to verify if all the dependencies are satisfied. we can see that entries 0, 1 and 5 can be solved immediately once the corresponding warps are issued since they have no in-degree (see the blue arrow blocks for columns 0, 1 and 5 in the top half of the subfigure), and they update values using their out-degrees (see the bottom half). in contrast, the other entries have to busy-wait (see red and green arrow blocks in the top half) until their in-degrees are eliminated for solving (see blue arrow blocks). figure 5 plots an example that solves an upper triangular system where the matrix is the symmetric counterpart of the lower triangular matrix shown in figure 4 . it can be seen that even though the two matrices are symmetric to each other, the two sptrsv processes have completely different parallelism."
"there are many different approaches to sentiment classification in the natural language processing (nlp) literature -from simple lexicon-based methods [cit] to learning-based approaches [cit], and also hybrid methods in between [cit] . no matter which approach is taken, a sentiment classifier built for its target domain would work well only within that specific domain, but suffer a serious performance loss once the domain boundary is crossed. the same word could drastically change its sentiment polarity (and/or strength) if it is used in a different domain. for example, being \"small\" is likely to be negative for a hotel room but positive for a digital camcorder, being \"unexpected\" may be a good thing for the ending of a movie but not for the engine of a car, and we will probably enjoy \"interesting\" books but not necessarily \"interesting\" food. here, the domain could be defined not by the topic of the documents but by the style of writing. for example, the meanings of words like \"gay\" and \"terrific\" would depend on whether the text was written in a historical era or modern times."
"therefore, to improve the performance of parallel sptrsv, it is crucial to reduce the overheads for preprocessing (i.e., generating level-sets) and to avoid the runtime barrier synchronizations."
the constraint used to prevent a rollover during the process of obstacle avoidance is where b is the wheel track and k is a stability factor.
"to verify the correctness of the model, the optimal path of the inverse dynamics model is tracked using the fully nonlinear vehicle model in carsim. the model parameters of carsim and the vehicle motion model are shown in table 1 . the simulation speed is 160 km/h."
"therefore, before solving for a particular component, we let the processing warp learn how many entries have to be computed in advance (i.e., the number of dependent entries). this number equals the in-degree of a vertex in the graph representation of a matrix (figure 1 (b) ), which is also identical to the number of nonzero entries of the current matrix row minus one (to exclude the entry on diagonal). thus, we use an intermediate array in degree of size n to hold the number of nonzero entries for each row of the matrix. this is all we do in the preprocessing stage. algorithmically, this step is part of transposing a sparse matrix in parallel [cit] . compared to the complex dependency extraction in the set-based methods that have to analyse the sparsity structure, our method requires much less work. lines 3-7 in algorithm 3 show the pseudocode for our preprocessing stage."
"schreiber and tang [cit] first used graph colouring for constructing colour-sets for sptrsv on multiprocessors. when the input sparse matrix is coloured, it is reorganized as multiple triangular submatrices located on its diagonal. because all the submatrices can be solved in parallel, this method can be very efficient in practice. [cit] recently extended the graph colouring method for sptrsv to gpus. however, as graph colouring is known to be an np-complete problem, finding good colour-sets for sptrsv is in general more time consuming. thus it may be impractical for real-world applications. [cit] recently proposed a method that partitions the graph form of an input matrix into multiple sub-graphs to obtain better data locality and higher concurrency. however, its pre-processing cost can be even more expensive than colourset approaches."
"knowing the in-degree information indicating how many warps have to be finished in advance, we can initiate a sufficient number of warps to fully exploit the irregular parallelism. for an arbitrary warp, after finishing the necessary floating-point computation for a component (line 14 in algorithm 3), it notifies all the later entries that depend on the current one by atomic updating (lines 19 and 22) . note that atomic operations are needed here as multiple updates from different warps may happen simultaneously. therefore, a warp only has to wait (lines 11-13) until its corresponding in-degrees are all eliminated, implying that all the dependent components are successfully solved and the warp can start processing safely. due to the warp multi-issuing property of gpus, a warp can start processing immediately after its dependencies have been satisfied, without any false waiting incurred by the hardware."
"also, it can be seen that if the entries in a given column (except the one on the diagonal) are sorted in ascending order, the components affected by the column will be expected to be solved in ascending order (i.e., from left to right). because the left components are in general likely to finish earlier and thus signal their out-degree components earlier, the overall waiting time of our synchronization-free algorithm may be decreased and better performance can be expected. as for backward substitution, the column entries can be accessed in reverse order for similar effects. in this procedure, a fast segment sort [cit] that separately orders a list of columns in parallel will be important to achieve overall best performance."
"differentially substituted norbornene rings have been shown to be useful phenyl bioisosteres in medicinal chemistry; however, their broad implementation is hindered by a lack of methods for their rapid modular construction (unlike the construction of aryl systems) 29 . in collaboration with leo pharma, we prepared a key target for an ongoing program (109; fig. 3d ) from diels-alder adduct 106 via alkene hydration and rcc with the pyrazole-boronic ester 108 in 44% yield. it is worth noting that this is, to our knowledge, the first report of a rcc reaction using a bis(pinacolato)diboron (bpin) derivative rather than a boronic acid. this advance was achieved using an in situ prepared ate complex, prepared by adding one equivalent of n-buli relative to the aryl-bpin donor 30 . we also used this modification of our suzuki-rcc protocol to prepare cyclopropanes 91, 92 and 93."
"in a similar vein, enantioenriched cyclopentenes were easily produced from palladium-catalysed, trimethylenemethane-based, formal [3+2] cycloaddition adduct b 3 21 . such structures are highly challenging to access in any other way. scaffolds c 1 and c 2, representing entry to [2+2]-derived systems, could be similarly processed. modular access to enantioenriched cyclobutanes is important and useful given the strict electronic requirements for photochemical cycloaddition and the documented challenge in achieving general asymmetric induction 22 . access to 1,2-disubstituted cyclobutanes (83-86, derived from c 1 ) compares favourably with photochemical approaches to such systems that frequently give inseparable racemic mixtures of regiomers and diastereomers. furthermore, access to tetrasubstituted, chiral cyclobutanes is highly useful, as multiple families of dimeric and pseudodimeric cyclobutane natural products contain such structural motifs 23 . structures such as 87, which would be otherwise extremely difficult to access through conventional photochemistry, can be easily prepared in enantioenriched manner from maleic anhydride heterodimerization adducts such as c 2 . finally, the strategy outlined above when applied to [2+1] cycloadditions using scaffolds d 1 and d 2 is a major departure from the common retrosynthetic logic normally applied to such structures. conventional approaches, usually involving late-stage cyclopropanation of an olefin, suffer from lack of enantiocontrol in the absence of directing groups or specialized carbenoid donors and complex catalysts 24 . structures 94 and 95 (d 2 -derived) are particularly illustrative of this fact: it would be extremely challenging to access either of these in an enantioselective fashion with current synthetic technology (cyclopropanation or c-c cross-coupling). as testament to the power of this strategy to access diverse libraries, we synthesized an additional 48 enantioenriched compounds in a similar manner (see extended data figs. 1 and 2 for details)."
"as shown in fig. 2b-d, the strategy outlined above could also be applied to [3+2], [2+2] and [2+1] cycloadditions. the vast scope observed with diels-alder chemistry was also seen in these cases, accessing substituents such as substituted olefins, terminal alkynes, homoallyl groups, and heterocycles. pyrrolidine-containing systems could be derived from simple building blocks b 1 and b 2 (accessed through dipolar cycloaddition) 19 to furnish 64-82 with high enantiomeric excess. quick access to pyrrolidine-containing drug scaffolds is useful: historically these have been extremely relevant to medicinal chemists, with around 40 pharmaceuticals containing this motif 20 . as with the diels-alder adducts, none of these structures has been accessed before. therefore, our approach serves as a modular entry to chiral variants of these coveted scaffolds."
"because in most cases the reaction would require both an electron-rich diene and an electron-rich dienophile and therefore is electronically unfavoured. furthermore, controlling the chemoselectivity when there are multiple alkenes present in the dienophile is challenging using traditional diels-alder chemistry; however, with our method, diverse alkenes and alkynes can be easily installed post-cyclization, with excellent isomeric and geometrical purity (accessing 12, 15, 19, 21, 25 and 31). typically, scaffolds derived from diels-alder adducts a 1 -a 6 have a clear retrosynthetic signature, wherein the electronwithdrawing groups on the dienophile have been homologated, alkylated, or degraded. therefore, our approach allows access to the previously unexplored chemical space of electron-rich, chiral, 1,2-disubstituted diels-alder scaffolds. using the tactical combination outlined above, a virtually limitless array of substituents is now easily accessed, including halogenated arenes, lewis-basic hetero cycles, α,β-unsaturated carbonyl groups, cyclopropanes, cyclobutanes, sulfur moieties, and alkenes. in the case of exo and endo isomers a 4 and a 5 (both commercially available), the sequences converged to an identical product using the same coupling partners (27 could be prepared from a 4 or a 5 )."
"building block 3, of hypothetical value in medicinal chemistry, represents the manifestation of this idea (fig. 1b ). although its structure would seem to suggest that it could be formed through a diels-alder reaction, the relevant synthetic building blocks, structures 4 and 5, are not electronically matched and therefore one would expect no reaction to take place. even if a workable enantioselective diels-alder reaction could be invented to achieve this transform, the strategy would suffer from a lack of modularity. in order to solve this problem, one could envisage using a hypothetical vicinal dihaloethylene (6) in place of 5, with a fumarate-type dienophile such as 7 serving as a viable synthetic equivalent. the favourable matched electronics of the dienophile should allow for a facile diels-alder reaction and subsequent radical cross-coupling (rcc)."
"the advance that we have described is largely strategic in nature, and thus the underlying limitations are tied mainly to individual parameters of a particular cycloaddition and ensuing rcc reactions. that said, cis-1,2-disubstituted products are not currently accessible unless downstream isomerization reactions are pursued, which need to be evaluated on a case-by-case basis. although ligand-controlled rcc reactions might eventually address this issue, in its present form this platform for modular molecular assembly holds great promise for accessing new areas of chemical space. the combination of classic cycloaddition chemistry with newly emerging radical c-c coupling offers a powerful way to repurpose the most classic skeleton-building reactions of organic synthesis, to simplify the enantioselective preparation of building blocks, natural products and medicines."
"to further demonstrate the potential of this approach to simplify synthesis, we present six applications in the total synthesis of natural products and in both early-and late-stage medicinal chemistry programs ( fig. 3a-f ). as mentioned above, alkaloid 1 has been a popular target of both academic and industrial scientists. through the application of cycloaddition and cross-coupling, the native carboxylate required for the diels-alder reaction can be used directly to produce epibatidine (1) in five steps (for optimization and in-depth analysis of previous approaches, see supplementary information) with a 38% gram-scale overall yield letter research (fig. 3a) . it is worth noting that the key decarboxylative cross-coupling takes place with 95% isolated yield (gram-scale, 72% yield)."
"studies commenced with diels-alder [4+2] cycloaddition ( fig. 2a ), by which a large variety of enantiomerically enriched scaffolds could be produced in a simple modular fashion. first, scaffolds a 1 -a 6 (a 2 being a diels-alder adduct and all others being derived from diels-alder/hydrogenation) were desymmetrized using deng's conditions, with either quinine or quinidine used as the lewis base to deliver mixed acids/esters in excellent enantioselectivity 18 . next, the mono-acid substrate was subjected to successive negishi 13, [cit] and suzuki 14 type rcc reactions, depending on starting-material availability or individual preferences. in this manner, some of the most simple and inexpensive diels-alder adducts known (a 1 costs us$38.54 per mol and a 2 is us$9.52 per mol) can be transformed into enantioenriched products (11-63) of high value. indeed, none of these products can at present be prepared using a diels-alder reaction (racemic or enantioenriched),"
"here, we sought to combine the innate complexity generation of cycloaddition with the simplicity and modularity of c-c coupling. when applied to structures such as 1, this strategy will permit the rapid generation of analogues, and when applied to medicinal programs such as 2, it will allow for the rapid exploration of otherwise challenging complex chemical space."
"saphris (asenapine, 102; fig. 3b ), an antipsychotic approved by the us food and drug administration (fda), is currently marketed as a racemic mixture (although the (+)-isomer has superior pharmacokinetic properties) 25 . this near-symmetric molecule has been challenging to prepare enantioselectively, as the two aromatic systems differ only in one chlorine substituent. it is therefore hard to envisage a cycloaddition that could be rendered enantioselective for the preparation of 102, and only one enantioselective approach has been reported letter research in 16 steps 26 . however, 102 can be prepared in formally six steps, with complete enantiocontrol, from b 2, through a strategy that could also be used to make an array of near-symmetric analogues. epothilone-a famous natural product that inhibits the dynamics of cellular microtubules and inspired the fda-approved medicine ixabepilone 27 -has been the subject of numerous synthetic studies and analogue campaigns 28 . intermediate 105 (fig. 3c ) has been used during nicolaou's synthesis of a cyclobutyl-containing analogue of epothilone, but required 15 steps (24% overall yield) to be prepared in enantioenriched form from c 1, via enzymatic desymmetrization and a series of carefully choreographed homologations 28 . using the same starting material, c 1, we have prepared intermediate 105 in only eight steps, through sequential desymmetrization, rcc with alkyne 103, hydrolysis, rcc with alkyl zinc 104, protecting-group exchange, and finally hydroboration/oxidation."
"to address the enantioselectivity challenge, a combination of transforms was proposed ( fig. 1c ). maleic anhydride was chosen as a surrogate for the hypothetical chiral (pseudo)dihalide 6 given its inexpensive nature, ready participation in most cycloaddition modes ([2+1], [2+2], [3+2], [4+2]), and known desymmetrization through chiral lewisbase-mediated alcoholysis. our sequence for generating complexity in a modular fashion involves five simple steps: first, cycloaddition to build a scaffold; second, desymmetrization to set absolute stereochemistry; third, rcc to install a new c-c bond; fourth, hydrolysis; and fifth, rcc to forge another new c-c bond. the known versatility of rcc enables a range of functionalities to be installed, from aryl 13 and heteroaryl 14 systems to alkenes 15, alkynes 16 and alkyl groups 17 . we describe here the preparation of more than 80 synthetic examples and multiple applications, covering a range of natural products (including the synthesis of 1) and real-world examples from industrial settings."
"finally, an ongoing program at eisai pharmaceuticals necessitated the preparation of complex scaffold 114 (fig. 3e ), wherein the key structure-activity relationship to be explored resided at the aryl (green) portion of the molecule. this is a particularly powerful application of the strategy outlined herein, because a carboxylate-needed to achieve the diastereoselective diels-alder reaction to construct the decalin framework-served as a gateway for the exploration of chemical space at the desired position. thus, an asymmetric diels-alder reaction using corey's oxazaborolidine 31 catalyst, followed by functional group manipulations (see supplementary information), led to intermediate 112, which could be cross-coupled with boronic acid 113 in a particularly challenging context to deliver 114 and enable biological follow-up."
"2) disparity edges refinement: disparity edges, which correspond to depth discontinuities, may contain disparity outliers [cit] . therefore, a simple and efficient approach to refine the disparity results at the disparity edges is introduced. initially, the pixels that belong to disparity edges are assumed to have absolute disparity difference greater or equal to 1 with at least one of their 4-adjacent pixels. fig. 10(a) shows with red the disparity edges extracted from the disparity map of fig. 9(b) ."
"issue wise discussion issue 1:-controlling method of injection molding machine for new technologies controlling method of injection molding machine for new technologies is one of the issue, some approaches were used for this issue which is injection molding machine controlling process very hard with relay logic, so embedded system controlling process (logic) used for injection molding machine, this process is better than the relay logic & it provides an effective & easy way to control the hydraulic system. the increasing complexity of automation applications needs new framework architecture to development automation control system. by using automation components like that component oriented design, reusability & picture structure is better way for reduces increasing complexity of automation. this approach reduces valuable development time because the component can be tested with their internal test functionality. by using hardware structure, system software architecture & experimental plate form give a better approach to development of a distributed control system for plc based applications. plc based applications & technology is very effective and useful technique to improve product quantity & quality."
"the mean-shift segmentation map (section ii-a) is exploited for selecting the appropriate support window size of each pixel (see section ii-b3). therefore, it is important to verify that small variations to the optimal parameters that adjust the segmentation result do not affect significantly the performance of this method. while for the pair of the method ranks seventh. hence, it is deduced that even varying the segmentation parameters the method remains in the top performers."
"the plastic injection molding machine control system is composed of driving system and electronic controller. there are three forms of electronic controller which include traditional relay controller, plc controller and microcomputer controller. a traditional relay controller has less been applied at present, because this have complexity hardware, low precision and difficult for adjusting and maintaining. the plc controller have many benefits, such as features high reliability, easy programming, strong anti-interference ability and easy maintenance and so on, but the cost is much high. zigbee is the only standards-based wireless technology designed to address the unique needs of low-cost, low-power wireless sensor. ieee standard 1588 is a faster approach for control mechanism and the best part of ieee 1588 that it removes delay of the process. energy storage of the power plant becomes easier by using modern approach of energy storage system. capacitor bank can be extended easily whenever their required value needed to improve."
"2) experiments on the definition of support windows: in order to prove why the exploitation of rectangular support windows of two sizes (as suggested in section ii-b3) enhances the disparity estimation results, we have performed experiments using support windows of either rectangular or square shape."
"and (18) the first condition confirms that a direction has much more non-outlier pixels than the other directions, while the second condition confirms that there is a sufficient number of non-outlier pixels along this direction. in case both conditions in (18) are satisfied, then a higher weight is given to the path cost that corresponds to the direction from which has been derived. for example, if is equal to, which corresponds to direction, then the weights used in (17) will be set as:"
"concluding, the workflow proposed in this paper gives better disparity estimation accuracy compared to the two workflow modifications described before. however, it's worth noting that the removal of \"phase c\" from the workflow leads to the reduction of the computational cost, while keeping the disparity estimation accuracy in high standards. on the other hand, the case of switching \"phase b\" and \"phase c\" in the workflow gives similar results with the case of excluding \"phase c\". however, with increased computational cost compared to the latter case."
"the left disparity map [ fig. 4(a) ] is acquired after applying winner-take-all (wta) to the cost volume, i.e., selecting for a pixel the disparity that minimizes . if the right image is considered as reference, then the disparity map of fig. 4(b) is acquired. the computation of and is fully independent. the disparity maps and are taken into consideration to detect problematic areas, especially outliers in occluded regions and depth discontinuities. a prevalent strategy for detecting outliers is the left-right consistency check [cit] . in this strategy, the outlier pixels have disparity values that are not consistent between the two disparity maps ( and ) and therefore, they do not satisfy the relation (11) where is the location of the considered pixel. the threshold for the outliers detection in (11) is set equal to . fig. 5(a) shows the outliers map that is generated for . the blue regions in denote the outlier pixels for which the relation in (11) does not hold, while the red regions denote the inlier pixels, i.e., pixels for which (11) holds."
"in section iii-b2 the selection of rectangular support windows of two sizes is experimentally justified. provably, more than two window sizes could be used. however, this would increase the computational cost of the algorithm. moreover, two window sizes are enough to achieve high disparity estimation accuracy."
"the content-based guided image filtering in \"phase b\" significantly enhances the initial disparity map. this is evident in the results of fig. 13, where the average percent of bad pixels drastically reduces from \"phase a\" to \"phase b\". the disparity map is further improved in \"phase c\" after applying disparity optimization. the improvement is stronger for the regions near depth discontinuities. outliers handling in \"phase d\" further reduces the average percent of bad pixels. the decrease is more pronounced for all regions, which is rational since all regions include the occluded regions. the disparity edges refinement in \"phase e\" helps to further lower the average percent of bad pixels, but in a less degree than the preceding steps, since it is applied locally to disparity edges. finally, median filtering in \"phase f\" slightly reduces the average percent of bad pixels."
"the disparity maps for the 27 stereo pairs, with their respective disparity error maps for, can be found in the supplementary material that accompanies this paper."
"let and be the left and right color images of the stereo pair, while and are their respective grayscale images. given a pixel on the left image (reference image), the corresponding pixel on the right image (target image) for a candidate disparity value is denoted as . this step defines a matching cost metric for estimating the similarity between two pixels. the proposed cost metric is composed of three individual pixel-based cost terms: i) a gradient-based cost term, ii) a gabor-featureimage based term [cit] and iii) a birchfield-tomasi dissimilarity term [cit] ."
"the detected outliers are filled with reliable disparities from neighboring areas by combining \"background outliers handling\" and \"generic outliers handling\". the filled outliers are then smoothed using bilateral filtering."
"this subsection proposes a novel scheme for exploiting guided image filtering. first of all, the shape of the support window is selected to be rectangular and the largest dimension of the support window to be the horizontal one (width). the window's width is twice its height. a support window elongated along the horizontal dimension, i.e., along the dimension in which disparity varies, is used in order to increase the discriminating ability of the window. this fact is experimentally verified in section iii-b2."
"in this section, the experimental results on multiple datasets are presented. in more detail, the four stereo pairs of the middlebury online stereo evaluation benchmark are used for the evaluation of this method. furthermore, this section presents experimental results on 27 additional middlebury stereo pairs, in order to verify the efficiency of the proposed approach."
"the curves in fig. 12 show the average rank (as estimated according to the online middlebury evaluation) for each of the above four cases, for different values of . an important finding is that between \"case 1\" and \"case 2\", \"case 1\" (rectangular support window) gives better average rank than \"case 2\" (square support window). moreover, by comparing \"case 1\" and \"case 2\" with \"case 3\" and \"case 4\", it is evident that the use of two support windows sizes gives a better average rank. finally, it is shown that \"case 3\" (this is the case proposed in section ii-b3) gives the best disparity estimation results among all cases. the value of for which the best average rank is accomplished is ."
"the initial cost volume is a three dimensional array which stores the matching costs for all pixels and all possible disparity candidates. the initial disparity map of fig. 2 is acquired after applying winner-take-all (wta) to, i.e., selecting for a pixel the disparity that minimizes . the initial disparity map of fig. 2 is heavily corrupted by estimation-error noise."
"for the stereo pairs of the middlebury stereo evaluation benchmark, the proposed method ranks: 12th for the \"tsukuba\" image pair, 6th for the venus image pair, 29th for the teddy image pair and 15th for the \"cones\" image pair."
"as mentioned in section iii-a, the column \"best\" of table ii gives the numeric disparity estimation results using the proposed methodology with the optimal parameters. in the rest of the columns of table ii, we provide experimental results after making some modifications to the methodology."
"afterwards, for the left side the following disparity histogram is generated: (21) where . each bin in the histogram corresponds to a specific disparity value, where the value (height) of the bin is equal to the total number of the left inlier pixels whose disparity is equal to the specific disparity value."
the gradient-based cost term for a pixel and disparity is given by (1) where denotes the gradient in horizontal direction at pixel on the grayscale image .
"the red lines show how the average percent of bad pixels in the disparity map decreases after applying each of the above phases, except for \"phase c\" which has been excluded from the workflow. thought, the percent of bad pixels has been increased compared to the results of the blue lines, it still remains in low levels, even without applying \"phase c\". indeed, the final disparity maps acquired for the \"tsukuba\", \"venus\", \"teddy\" and \"cones\" image pairs, without applying \"phase c\", rank 19th in the middlebury online evaluation benchmark. this rank proves that our algorithm is able to keep high standards of disparity estimation accuracy [this is mainly due to the outstanding performance of the proposed content-based guided image filtering (phase b)], even after removing the disparity optimization step and thus reducing the computational cost of our approach."
"in an analogous manner, the disparity histogram is generated for the right side. let now the maximum values of the left and the right histograms be"
"choosing lab view as the human machine interface for the implementation is a proper decision as it has various types of application s and functions that are easy to understand and use, secondly this approach is more economical as the objectives and system defects can be identified without the implementation of the circuit."
"several methods require iteration cycles in order to improve gradually the accuracy of the estimated disparity maps [cit] . consequently, the number of iterations affects the computational cost of an approach. on the contrary, the proposed method gives accurate disparity results without performing any repetitive refinement."
issue 3:-energy storage in co-generation power plant  conventional energy storage system is not reliable compare to the modern energy storage system available in present scenario solar energy storage system should be replaced with the new and modern storage system.  solar power tower plant discussed and analysis done for verifying various performances of two-stage thermal energy storage system.
", the outliers map [see fig. 8 ] is acquired for . the blue regions in fig. 8 denote the outlier pixels for which above relation does not hold, while the red regions denote the inlier pixels."
"new trends in industrial automation is second issue, some approaches were used for this issue which is simulation approach for speed control of induction motor using lab view software. lab view software is one of the most significant software & with the help of labview, today's industries control applications are done by remote processes only, user normally sits somewhere safe place away from the working environment from there he/she has to control the plant and also make sure that the system parameters should be optimized so here simulation software plays a vital role in industrial monitoring and control system. actually the main issue in the designing field is that most of the problems occur while using large no. of control circuits, since increase in no. of control circuits lead to superfluous no. of wires. to reduce the no of hardwire circuits and to see things moving or happening graphically can be easily seen so we can implement these circuits on simulation level using simulation software. even it is not possible to design control of distant systems as this would involve large no hard wire circuits, simulation programs have made engineering design easy and with lesser amount of material requirement. it claims that choosing lab view as the human machine interface for the implementation is a proper decision as it has various types of application s and functions that are easy to understand and use, secondly this approach is more economical as the objectives and system defects can be identified without the implementation of the circuit. to be precise, this paper falls under the area of controlling an induction motor and its variable like speed and direction with the help of lab view, but there is again a problem occurs with this simulation software as this is not sufficient because it couldn't fetch the desired result and even in controlling the speed there are only3 multilevel references in the vdf, so we need to improve this for the better controlling and simulation."
"combination of \"generic outliers handling\" and \"background outliers handling\": the disparity map of fig. 7(a), after applying \"generic outliers handling\", is visualized in fig. 9(a) . the outlier pixels that have been handled using (22) are considered now as inliers. for the remaining outliers (i.e., none of the conditions in (22) holds), the scheme in paragraph \"background outliers handling\" (section ii-d.i.b) is applied."
"the rest of this paper is organized as follows. in section ii, the proposed method is presented in detail. section iii provides information on the parameters used and presents the experimental results, while conclusions are drawn in section iv."
"the yellow dashed lines show how the average percent of bad pixels in the disparity map increases after applying each of the above phases, with the difference that \"phase c\" is applied before \"phase b\" (when referring to the yellow dashed line, the letter \"b\" corresponds to \"phase c\" and the letter \"c\" corresponds to \"phase b\" on the -axis of fig. 13(a), (b) and (c), respectively) . evidently, the results of the yellow dashed lines after applying \"phase d\", \"phase e\" and \"phase f\" are very close to the corresponding results of the red lines. for example, after applying \"phase f\" the bad pixels error for non-occluded regions, all regions and near depth discontinuities regions are 2.36%, 5.05% and 6.93%, respectively (due to space reasons, numeric values have not been included for the markers of the yellow lines). therefore, it is proved that the case of switching \"phase b\" and \"phase c\" in the workflow, gives similar results with the case of excluding \"phase c\" from the workflow. here, it has to be clarified that when applying \"phase c\" before \"phase b\" the weights of the path costs in the weighted semi-global optimization (phase c) are equal to 1 ( ), since it is not feasible to rely on the noisy initial disparity map (see fig. 2 ) to perform the methodology required for deciding on the weights of the path costs, which is described in section ii-c2."
"in this paper an approach that gives accurate disparity results for stereo image pairs was presented. the approach uses an efficient cost term, composed of three individual pixel-based cost terms, in order to estimate the initial cost volume. the filtered cost volume is acquired after applying image guided filtering to the initial cost volume, using rectangular support regions of two sizes. the optimization of the filtered cost volume is performed using weighted semi-global matching, where an adaptive threshold to identify depth discontinuities is used. outliers handling is improved by introducing a straightforward scheme."
outliers detection: the disparity maps of the left image [see fig. 7(a) ] and the right image [see fig. 7(b) ] are taken into account so as to detect problematic areas. according to the left-right consistency check:
"a c++ implementation of the algorithm is used to report on the required computational time. the algorithm was executed on a desktop pc with core i7-3770 3.40 ghz cpu and 8 gb ram. the low computational time, using as input each of the four stereo pairs of the middlebury evaluation benchmark, 1 is indicated in the column \"proposed\" of table i . the measured time is the average of 10 separate runs and includes both the time 1 [online]. available: http://vision.middlebury.edu/stereo/eval/ fig. 11 . disparity maps generated with the proposed algorithm and their corresponding disparity error maps for error threshold equal to 1. for performing mean-shift segmentation (see section ii-a) and the time for executing the algorithmic steps of the methodology (see sections ii-b -ii-d) ."
"the selection of the appropriate support window size for each pixel, based on its local image content, is discussed in the next subsection. with the term \"local image content\" of a pixel, we refer to the image content within a local region around this pixel."
"based on the above, the new disparity estimate is given from if if (22) the second part of the conditions in (22) ( or ) confirms that there is a sufficient number of inlier pixels on either the left or the right side of that have the most frequent disparity value, before setting equal to this most frequent disparity value."
"the output of the winner-take-all (wta) on, which has been estimated from (17), gives the disparity map [see fig. 7(a) ]. if the right image is considered as the reference image, then the disparity map [see fig. 7(b) ] is acquired."
"the disparity maps after cost volume optimization may contain a large number of outliers in occluded regions, uniform areas and near depth discontinuities. with the algorithmic steps, described through this section, these problematic areas can be handled efficiently in order to get a disparity map of high accuracy."
"wireless data transmission is fourth issue, some approaches were used for this issue which is engineering approach for secure and safe wireless sensor and actuator networks for industrial automation systems which includes the security concept in context of industrial automation and it gives an introduction of a holistic networks but still easy to implement approach for automation networks. justification of problem illustrated through the holistic approaches including security protocols and also works on vest (virginia embedded systems toolkit) focuses on the development of effective composition. but a gap is that a security solution must ensure that the cost of an attacker to break the security solution is higher than his/her potential benefit. solution approach obtained through an engineering methodology to cope with security requirements in context with industrial automation. data collected and analyzed through the three stages of solution -the development flow, inputs of the selection process, mapping from requirements to practical solutions. the proposed development flow promises a reliable objective engineering of proper system solutions. key concepts of the flow are a holistic goal description and an iterative composition algorithm that inherently applies and extends existing knowledge. this holistic approach is reliable, safe and secure. applications of short-range wireless technologies to industrial automation: a zigbee approach. bluetooth, ultra-wideband (uwb), zigbee, and wi-fi are four popular wireless standards for short-range communications. specifically, zigbee network is an emerging technology designed for low cost, low power consumption and low-rate wireless personal area networks with a focus on the device-level communication for enabling the wireless sensor networks. zigbee develops a wireless network which should be low power consumptive with low cost. hence, it has been becoming widely used in many applications, such as home automation, industrial control, location and position, telecommunication, and wireless sensor networks."
"an overview of the proposed method is given in the schematic flow diagram of fig. 1 . as depicted in the diagram, mean-shift segmentation is applied to the input stereo images as a pre-processing step of the algorithm, necessary for several of the subsequent steps. the core algorithm is then divided into four sequentially applied components: 1) matching cost computation, based on the image gradient, gabor features and the rgb information (via the birchfield-tomasi dissimilarity measure); 2) cost filtering using content-based guided image filtering that is applied for rectangular support windows of two different sizes; c) disparity optimization via weighted semi-global optimization, where the path costs of a pixel may have different weights depending on the pixels that precede the considered pixel along a path direction; 3) disparity refinement relying on: i) outliers handling based on background and generic outliers handling and ii) disparity edges refinement using disparity histograms. these algorithmic steps are analyzed in sections ii-b -ii-d."
"stage 3+ deals with evaluation of the details presented and generalization to some extent. this stage deals with synthesis of the data, concept & the results presented by the authors."
"the technology and trends are changing day by day in industrial automation, nowadays simulation software have made their own place. we can see the plant response be far before starting the plants. time is also important; ieee standard 1588 is a fast time based control algorithm to improve the system delays."
"the exhaustive review could finally lead to extract findings in the area of industrial automation, strengths and weaknesses and scope of work during m. tech 1st semester research work."
"give better results compared to the results obtained using balance parameters with values that are slightly below or slightly above the optimal balance parameters. additionally, from the last two columns and ), where the values of the balance parameters cause the elimination of the birchfield-tomasi dissimilarity term from (4), it can be deduced that the disparity estimation accuracy reduces considerably after eliminating the birchfield-tomasi dissimilarity term. this fact shows the importance of using the birchfield-tomasi dissimilarity term in (4). give better results compared to the results obtained using weights with values that are close to the optimal path direction weights. the last column of table iv gives results for weights . for, the weights in (17) are uniformly set as . obviously, the last column of table iv gives worse results than the rest of its columns. therefore, it can be deduced that the use of different values for weights and in (17) helps to achieve better disparity estimation results than using uniform values."
"where, are constant parameters. and are the intensity differences between a considered pixel and the preceding one along the path direction, on the two images, respectively, and they are defined as (14) and (15) where and are the images in grayscale. the idea of using and is based on the assumption that a disparity change (depth discontinuity) often coincides with an intensity edge [cit] . therefore, the intensity difference between neighboring pixels is able to indicate the presence of an intensity edge."
"around each pixel of the disparity edge, a circular region of radius 4 is defined. the color similarity between the center pixel and a pixel within the circular region is estimated as (24) where (25) a disparity histogram is generated for each, where the values of its disparity bins are computed as follows: (26) where . let now the maximum and the second maximum value of be and, respectively, and the corresponding disparity value for be . if then, otherwise the disparity value of does not change. the disparity result after the disparity edges refinement is depicted in fig. 10(b) . a median filter, using a 3 3 neighborhood, is applied to the disparity result of fig. 10 (b) in order to remove spurious disparities before acquiring the final disparity map, which is depicted in the upper-left image of fig. 11 ."
"the parameters used for the experiments are the same for all tested stereo pairs. more specifically, the parameters used for table i computational time in seconds table ii evaluation results table iii balance weights testing the estimation of the cost term (see section ii-b1) are selected equal to"
"this stage evaluates the details in relation to significance of the problem, novelty of the problem, significance of the solution, novelty in approach, validity of claims etc."
"the initial disparity map that is generated from the initial cost volume [which is computed via the matching cost computation step (phase a)] is heavily corrupted with noisy disparities. based on the middlebury online benchmark, this subsection examines how the initial disparity map is improved after applying sequentially: 1) content-based guided image filtering (phase b), 2) disparity optimization relying on weighted semi-global optimization(phase c), 3) outliers handling (phase d), 4) disparity edges refinement (phase e), and 5) median filtering (phase f). outliers handling (phase d) and disparity edges refinement (phase e) constitute the disparity refinement step."
"this subsection describes the definition of the combined matching cost term, which is used for the computation of the initial cost volume. the initial cost volume is then filtered relying on content-based guided image filtering."
"after reviewing 15 research papers on controlling parameters of injection molding machine we have found following issues, which has to be addressed, while the designing and implementation of the injection molding machine these issues are:"
"in the following, the improvement in the disparity map quality, introduced by the aforementioned steps, is visually demonstrated. the initial disparity map, which is acquired via the matching cost computation step, is heavily corrupted with estimation-error noise, as it is obvious in fig. 2 . after applying content-based guided image filtering the noise is removed, as it is evident in fig. 4(a) . disparity optimization further improves the disparity results. this is clearly seen from the comparison between figs. 4(a) and 7(a). fig. 9(b) shows the disparity map after performing outliers handling to the disparity map of fig. 7(a) . the outlier regions (which are denoted with blue in the outliers map of fig. 8 ) of the disparity map in fig. 7(a) have been efficiently filled with reliable disparities in the disparity map of fig. 9(b) . fig. 10(b) displays the disparity map after performing disparity edges refinement to the disparity map of fig. 9(b) . the disparity edges of fig. 9(b) [which are shown with red in fig. 10(a) ] have been effectively refined in fig. 10(b) . after applying median filtering to the disparity map of fig. 10(b) the disparity map of the upper-left image of fig. 11 is generated. the slight improvement in the disparity map quality, introduced by the median filtering, is subtly obvious at locations where the disparity value changes. in the following, two modifications in the proposed workflow are also examined."
"and, where and . that is, a higher weight is given to the direction that has much more pixels that belong to the same surface as, when compared to the other directions, which at the same time are non-outliers. if any of the conditions in (18) is not satisfied, then all weights are set equal to 1. experiments on the selection of optimal values for and are given in section iii-a."
the area of research is a trend in manufacturing towards faster and higher precision part production with the help of an application of ieee 1588 that is time based control system rather than using traditional control techniques.  r-field bus is a radio based physical layer based on the existing and available radio technologies in the lan and wan world.
"the zigbee technology is based on multiple hop topology formation but it is not based on algorithmic study.  zigbee can help continuous monitoring of physical parameters and data transmission in mobile network effectively in terms of cost and power consumption.  zigbee can help continuous monitoring of physical parameters and data transmission in mobile network effectively in terms of cost and power consumption. the modulation technique used in zigbee network is direct sequence spread spectrum (dsss), which effectively avoid electrical interference. solution approach obtained through zigbee technology using xbee trans receiver."
"the blue lines in fig. 13 depicts how the average percent of bad pixels in the disparity map decreases after applying each of the above phases. fig. 13 includes results for non-occluded regions [see fig. 13(a) ], all regions [see fig. 13(b) ] and regions near depth discontinuities [see fig. 13(c) ]."
"tereo reconstruction is one of the most active research fields in computer vision [cit] and it is exploited in a wide range of applications, such as mobile robot navigation [cit], augmented reality [cit], automotive [cit] and telepresence [cit] applications. although various methods have been proposed so far, the estimation of dense disparity maps from stereo image pairs is still a challenging task and there is further space for improving accuracy, minimizing the computational cost and handling more efficiently occlusions, textureless areas and light variations. section i-a reports on existing methods in the field. paper's contribution is described in section i-b. while, section i-c highlights the main differences of the proposed method with respect to other state-of-the-art methods."
"two stage thermal energy storage system plant can be operated in two stages it will reduce the cogeneration problems controlling process is very hard with relay logic, so embedded system controlling process used for injection molding machine."
"energy storage in co-generation power plant is third issue, some approaches were used for this issue which is comparison of two methods of electrical power storage.one of them is conventional method on other side another is modern system. in conventional power storage method has taken the battery storage method and in modern method has taken capacitor bank method for electrical storage. it's not sufficient because new installation of capacitor bank storage contain many difficulties and connection complex-city. by some of the way tried to solve the problem of conventional storage of electricity by giving a better alternative for electrical storage. limitation of the given alternative is so much similar to the conventional method al-thought this is only suggested method of the electrical power storage. with the less complexity of the circuitry and the less economical investment this will become a better alternative for electrical storage in solar plants."
", and the window size is . the disparity map of fig. 9(a) after applying \"background outliers handling\" and bilateral smoothing is visualized in fig. 9(b) ."
"the thresholds and in (13) are adaptive to and, respectively. the rationale behind using adaptive thresholds is that for areas with low intensity it is more difficult to discriminate regions that may belong to different depths, while for areas with high intensity this discrimination is more evident. therefore, the intensity threshold, which denotes a depth discontinuity, should be low for a low intensity pixel and increase as the intensity of the considered pixel increases. more details on how the adaptive intensity threshold is defined are given in the following paragraph."
"the review of 15 research papers has been carried out in the area of industrial automation and find out current challenges and scope of work. after the review, we were found many issues like that controlling method of injection molding machine for new technologies, new trends in industrial automation, wireless data transmission & energy storage in co-generation power plant which should be given proper concern, when the enhancement of security takes place. these papers are a survey of different security issues & controlling related work that carried out in the area of integrity. propose of these models are to reduce the security risks and improve system reliability."
"in the following, the results that justify the selection of optimal values for and are presented. table iii presents the results of testing the balance weights and of (4) (see section ii-b1). the results show that optimal parameters"
"the rest of the paper is organized as follows. in section 2, we briefly describe the data source used in the study. section 3 presents the key methodologies used for data mining and knowledge discovery from train operation data. the experiment and numerical results are presented in section 4, followed by the concluding remarks in section 5."
"here and indicate the passenger od volume and the section length between stations and, respectively. since passenger od is not available from the dataset, equivalently, we can use the sectional passenger volumes (v ) to calculate plf, as in"
"the result in figure 6 (b) also shows that a cluster c, separated from the other two clusters, has large variation in the dimension of pc2. by further analyzing the distributions of dt (a surrogate of pc2), it is found that cluster c is associated with the samples that have early departure time (as shown in figure 8(d) ) and go through fewer sections/shorter distance (as shown in figures 8(b) and 8(c) ). these samples correspond to the extra (temporal) short-distance trains that depart in the early morning. we then rerun the pca and clustering models only for cluster c samples to further explore the patterns of these extra trains. the results are shown in figure 9 ."
the paper makes contributions in two aspects. (i) exploratory data mining techniques are applied to a dataset that contains 3-month long real world train operational data of the beijing-shanghai high-speed railway. such information is usually held by railway companies and is not available to the general public and the academia. (ii) the unique characteristics that affect plf and the underlying behavioral patterns are discovered and further analyzed.
"the railway passenger transport management information system is an official rail operation and management system maintained by china railway corporation (crc). the dataset used for this study was retrieved from the system, which contains 3-month rail operation information of the beijing-shanghai high-speed railway. this railway line is the most important transportation corridor connecting two largest cities of china. the rail-line has a total length of 1318 km and goes through 24 stations. these 24 stations can be further categorized based on their administrative levels, as shown in table 1 . in general, higher level indicates higher population and higher socioeconomic status. the dataset was further processed to extract 33 representative operational features. descriptions of the features can be found in table 2 . the operational features include passenger load factor (plf) that directly indicates the capacity utilization of a train, date, ticketing strategy (ts), run duration (rdr), departure time (dt), train type (tt), number of stops (ns), run distance (rdi), stop schedule (ss), run speed (rs), and load coefficients (lcs) for all sections along the railway line. the authors are aware of other factors such as trip purposes and passenger social-economic status that could also affect tcu, but such information is not available from the crc database. since the ticket prices remain stable during the study period, pricing is not considered as an influential feature in the study."
we first separate the samples into downward trains and upward trains. pca and clustering techniques are then applied to these two datasets. a few interesting findings are generated from the exploratory data analysis and they are discussed in this section.
"in the context of statistical analysis and data mining, exploratory data analysis (eda) is a process of detective work that does not require a predetermined hypothesis to be tested. rather, the role of eda is to explore data in as many ways as possible, until a plausible \"story\" of the data is unearthed. formal definitions of eda and exploratory data mining can be found in tukey [cit] and yu [cit] . in this section, exploratory data mining approaches are applied to gain insights of the structure of the data and the underlying travel patterns. first, principal component analysis (pca) is used to select the most salient features (called principal components) to represent the train operation data. secondly, we use clustering techniques to discover the intrinsic relationship between tcu and the principal components."
"fuzzy partitioning is carried out through an iterative optimization of the objective function shown in (6), with the updated degree of membership calculated using"
"these findings, albeit case-specific, have shown that the proposed approach is a useful tool for data mining and knowledge discovery from train operational data and it can journal of advanced transportation 9 be utilized to facilitate smarter decision-making for train operation and management."
"(1) run distance and stop scheme are found to be closely related to tcu. per the specific dataset, trains with longer run distance and fewer stops result in higher tcu."
"in figure 1, we first show the aggregated statistics of the collected data. figure 1 trend line. figure 1(b) shows the average load coefficients of the upward and downward trains. it can be found that the average plf decreases during the whole study period and the travel pattern may be characterized by two segment trips including s1(bjs)-s12(xze) and s12(xze)-s24(shhq)."
"by the euclidean norm as in (7), wherẽrepresents theth feature of the i-th transformed sample and denotes the location of centroid at the k-th dimension."
pca is a commonly used technique for dimensionality reduction and feature selection [cit] . here we use pca to seek a low-rank approximation of the train operational data. in this
"it is shown that plf and lcs ( 6 ∼ 1 ) are strongly correlated with pc1; date, dt, and lcs ( 16 ∼ 7 ) are strongly correlated with pc2. as in figure 9 (a), cluster c-2 and cluster c-3 are in the higher region of pc1; cluster c-1 is in the lower region of pc1. it is found that early of this quarter and early dt are associated with higher plf with greater lcs ( 6 ∼ 1 ), as illustrated by cluster c-2; late of this quarter and relatively late dt also lead to the higher plf with greater lcs. it is noteworthy that early of the quarter corresponds to the \"golden week\" (chinese national holiday) and late of the quarter is close to the new year. therefore, the extra trains with early or late departure time are better utilized in the holidays seasons compared to those in other seasons."
"this paper proposes an exploratory data mining approach to discover the influential features of tcu and understand the travel patterns using real world train operational data. several interesting findings were reported in the paper, as summarized below."
"by scrutinizing figure 8(c), it is found that the major trip attraction for cluster a trains is beijing (as the load coefficient is high at section 1), and the major trip attraction for cluster b trains is the city of xuzhou (xze station), a medium-level city. combining the patterns in figures 8(a) and 8(c), it can be concluded that passengers traveling to beijing prefer to choose the trains with fewer stops, most likely due to their higher value of time."
"generally speaking, there are two approaches to understanding and improving train capacity utilization. one is model-based approach, which applies analytical models to study the effects of train operation and management strategies (e.g., timetabling and ticketing) on train capacity utilization. the second is data mining approach that empirically analyzes tcu and the interrelationship between tcu and the influential factors."
(2) the marginal effect of travel distance decreases in terms of the gain in tcu. making the short-distance trains into medium-distance trains is more beneficial compared to making medium-distance trains into long-distance trains.
"in the following experiment, we use pc1 and plf for fuzzy c-means clustering. two optimal clusters are found, which are plotted in figure 3(a) . it can be observed that higher plf is associated with higher pc1. since pc1 is positively correlated with rdr, rdi, ss, and 7 ∼ 23, it can be further inferred that longer run distance/travel time, higher level of stop scheme (i.e., fewer stops), and higher sectional loading it is also noticed that cluster b in figure 3 (a) shows the multifurcated lines with different slopes, representing different rates of plf to pc1. to further analyze the pattern, we used rdi as a surrogate of pc1 and applied the clustering model using plf/rdi as the only feature. the results in figure 5 have shown five clusters which correspond to the five linear lines shown in figure 3(a) . the results imply that the marginal effect of rdi gradually decreases; i.e., changing shortdistance trains to medium-distance trains seems to be more beneficial (in terms of the gain in plf) compared to changing medium-distance trains to long-distance trains. this finding can be used to guide train scheduling. figure 6 (a) for the upward trains from shanghai hongqiao (s24) to beijing south (s1). we then conducted clustering analysis using plf, pc1, and pc2. it is found that the optimal number of clusters is 3, as shown in figure 6 (b). figure 7 shows the original features that are strongly correlated with pc1 and pc2. in particular, it is found that pc2. for the upward trains, a few findings of tcu and passenger travel patterns can be put forward. as observed in figure 6 (b), compared to the samples with lower plf (cluster b), trains with higher plf (cluster a) are associated with larger pc1, indicating that higher ss (fewer stops), longer rdi, and higher lcs lead to better train capacity utilization. this is further verified in figures 8(a), 8(b), and 8(c). such finding is consistent with the downward trains."
"due to the vast land span and enormous transportation demand in china, railway transportation plays an increasingly vital role in china's economy. in general, chinese high-speed rails are more preferable compared to other transportation modes, especially for long-distance trips. during the last five years, the railway passenger volume in china has been increasing with a yearly growth rate of 10%. [cit] statistics, the chinese railway passenger volume is 2.8 billion, which has increased 11% [cit] . despite the continuous growth of railway transportation in china, it is found that the train capacity of some passenger lines is underutilized, especially during off-peak seasons. for example, the average passenger load factor of high-speed trains in china is around 60-70%. in extreme cases, the number is less than 40%. and this has motivated transportation researchers to develop methods to reduce such capacity waste. optimizing train capacity utilization (tcu) is challenging. the challenges are mainly bifold: (i) the passenger travel patterns are highly stochastic and unpredictable; (ii) many factors may influence tcu, and the causalities are hard to be captured. to overcome these challenges, it has become an imperative task to find out the factors that affect tcu and to discover the behavioral patterns behind it."
"isps' incentives to implement non-traditional policies are not well-known or understood, but we know that such policies exist [cit] . here is an example scenario of why they could contribute to isps' flexibility. node 7 may agree to announce peer routes to its provider 4, knowing that 4 will not necessarily prefer customer (routes from 7) over peer (routes from 3 or 5). node 4 may find it beneficial to have a greater variety of routes available in exchange for ceasing to always prefer customer routes. peers 3 and 4 could have an agreement to mutually exchange not only routes learned from customers but also from peers for similar reasons. finally, node 3 may have successfully negotiated a discounted provider service with 2 in return for propagating peer routes to its provider."
"the method has been successfully applied to identify the risk of suffering other diseases such as neuroblastoma [cit] . currently, it is also being used to extend the underlying hgdb with information related to crohn's disease, migraine, epilepsy and breast cancer. moreover, a project in collaboration with clinical experts in lung cancer from the \"hospital de clínicas\" in asuncion, paraguay, is exhibiting promising development."
"proof: if pd(s) is acyclic, then there is at least one refinement r(s) of s for which pd(r(s)) is acyclic [cit] . from theorem iv.2, if pd(r(s)) is acyclic then dd(r(s)) is acyclic; from lemma v.1, it follows that r(s) is safe."
"proof: for all the cases specified in theorem v.8 in which pd(s) also has a cycle, it follows from theorem v.5 that pd(r(s)) has a cycle for any r(s)."
"one option is to analyze router configurations to further split equivalence classes of paths according to subsequent steps in the bgp decision process. we then repeat the previous analysis for the new md(s). for instance, suppose that the md(s) created by separating the paths based on local preference values has a cycle, but pd(s) does not. then, we separate each local preference equivalence class into sub-classes based on the as path length, which is the second step of the bgp decision process. each sub-class is a node in the new md(s), i.e., an equivalence class in the new md(s) has paths with the same local preference and as path length. we repeat the procedure of checking for cycles in the new md(s)."
"in conclusion, the competition results show that the most efficient retrieval approach remains the inclusion of textual information, as it provides a higher semantic level of description than audiovisual information. the average map obtained by including textual descriptors is around 30% (e.g., see team tub in table 4 ), which is still hard to achieve using only video information."
"these simulations demonstrate that although many stabilizing solutions to the combination therapy problem exist, the best ones are found when design parameters such as a sparsity, limits on the magnitude of gains, and robustness guarantees are simultaneously considered. experimentally searching for these combinations is infeasible as the number of potential therapies and possible concentrations to consider is experimentally intractable. we propose to guide these experimental activities with our ability to design and synthesize combination therapy controllers. as such, one could generate a family of controllers based on \"design specifications\" tailored not only the (viral or cellular) composition of the disease, but to explore tradeoffs between number of therapies sum of virus populations subject to random time invariant perturbations of 5% in the dynamics for 30 different simulations for (left) a stabilizing closed loop controller comprised of antibody pentamix (0.4687,0.7815, 0.6129, 0.6279, 0.8831) µg/ml of (3bc176, pg16, 45-46g54w, pgt128, 10-1074) synthesized using the convex program (5) and (right) a robustly stabilizing closed loop controller comprised of antibody trimix (0.6891,0.6712,1.0706) µg/ml of (3bc176, 4546-g54w, pgt128) synthesized using the l 1 combination therapy algorithm."
note that lemma iii.1 does not necessarily hold for a strict spp specification s. two distinct paths are allowed to belong to the same equivalence class as long as they have the same next hop.
"as a proof of concept, sile has been applied under the context of searching relevant genes and variations related to the risk of suffering early onset alzheimer's disease (eoad). due to the neuronal degeneration and the early appearance of its symptomatology, specific studies about its genetic characteristics are the key to increasing the quality of life of patients. sile acts as a valuable tool that experts can use to manage the data which are relevant and sufficiently reliable for this task. a summary of the sile method and the dq dimensions used can be seen in figure 10 ."
"temporal descriptors are derived by means of a classic confirmed approach, that is, analysis of the shot change frequency [cit] . unlike existing approaches, we determine the action content based on human perception."
is the convolution of g and w. finally we refer to the ∞-induced robust controller as the l 1 controller as is customary in the robust control literature.
"as an example of use, if we focus on the variation view of the cshg, we can identify what a variation is and which are its main components (see figure 4 ). according to the schema, a variation is a change in the dna sequence which occurs in a certain position inside a chromosome and, depending on its frequency and description, it can be classified into different types (mutant, polymorphism, single nucleotide polymorphism, copy number variation, insertion, deletion, indel and inversion)."
"example iii.2. consider again the specification shown in fig. 2a . paths 140 and 14320 belong to the same equivalence class of node 1, while paths 320 and 32140 belong to the same equivalence class of node 3. ranking the paths of the equivalence classes results in the following four refinements:"
"in the next section we introduce the \"from big data to smart data\" perspective applied to the genomic domain and show how the cshg and data quality methodology are the keys to achieving it."
"-m8: the minor allele frequency (maf) of the variation must be less than the frequency of the phenotype in the population. -m9: the inheritance pattern, penetrance and mechanism of the variant must be consistent with the disease. -m10: the studies provided by the bibliography must have at least 500 participants and it is desirable that they are replicated. -m11: for pathogenic variants the odds ratio must me greater than 1, and for protective variants the odds ratio must be less than 1. -m12: for genome wide association studies (gwas) the p-value must me less than 5x10 -8 ."
"our methodology only examines steps of the bgp decision process as needed. first, we split the paths of each node into equivalence classes according to the first step of the bgp decision process. this step is the comparison of the bgp local preference attribute [cit] . every equivalence class of a node in the specification will correspond to a local preference value of that node. for router implementations that have a different first step, the procedure is similar. then, we create md(s). each equivalence class of paths will be a node in md(s). the ranking arcs among classes that belong to the same node u i of s will have direction from the classes of higher local preference value of u i towards classes with lower local preferences. since all paths of the specification are known, it is also possible to place inflationary arcs between the classes."
"where t ha and t la represent the total lengths of hot and low action segments, respectively, and t total is the total length of the movie."
"the biomedical justification for wanting a simple controller structure with small gains is twofold: first, the number of therapies that can be used simultaneously to treat a disease is often limited, and second to minimize side effects, it is desirable to keep the magnitude of drug concentrations small while being robust to evolution. these design specifications can be expressed with the use of regularization, a common technique used in machine learning and inverse problems for model identification [cit] and increasingly used for controller design [cit] . as such, we introduce 1 and 2 penalties in our design objective to promote controller sparsity and minimize controller gains."
"assume that there is exactly one multi-node m in the cycle in md(s). let a be the node in the md(s) cycle before m and let b be the node following m in the cycle. the part of the cycle from b back to a contains no multi-nodes, so there must also be a sequence of arcs from b to a in pd(s). we examine whether there is a sequence of arcs from a to b in pd(s)."
"the routing system can be modeled as a strict spp by executing all comparisons of the bgp decision process. however, if we stop at an intermediate step, the routing system needs the extended spp model, since there is no guarantee that paths in the same equivalence class will have the same next-hop."
"alzheimer's disease (ad) is a type of dementia, so it mainly affects the capabilities and functionalities of the brain, decreasing them and hindering the patient's normal life development. the early-onset type starts to show symptoms before 65 years old (normally around 50) so it is essential to use truthful and contrasting information of high quality in the context of the clinical diagnosis. due to ad being a degenerative neuronal disorder, stopping it in time is the key to increasing the quality of life of patients, which is only possible through specific studies of its genotype-phenotype relationship."
"extended spp is an spp model without the strictness property. a specification s is still described by the triple (g, p,λ). however, there is no restriction on the preference functions of the set λ regarding which paths can be considered equivalent."
"the following experiment was conducted at application level. we sought to simulate a video browsing environment in which sequences were to be represented using the proposed descriptors. we have developed a client-server architecture which provides a virtual 3d browsing environment for video databases [cit] . movies are displayed in a spherical coordinate system, and each movie is represented by one key frame. the user interface resembles that of google earth (by which we were inspired):"
"we note that due to computational limitations, we were not able to synthesize controllers using the h ∞ algorithm for the full set of thirty five mutants from figure 5 ."
"proof: suppose that there is a cycle in md(s). if the cycle contains no multi-node, then all nodes and arcs that participate in the md(s) cycle exist in pd(s) as well."
"once the relevant variations are determined, the next step is to identify which data must be extracted from each repository, in order to be stored in the hgdb. the information must be sufficient to allow the unambiguous identification of variations in a patient's sample, as well as provide enough data about their characteristics to support a genetic diagnosis. each genomic repository provides different ways of accessing information as well as different data formats (vcf format, tabular text files, xml, etc.)."
"in this work, we develop a variant of the spp model, called extended spp (section iii), that does not require the strictness condition and so allows for an approximation to the full spp model. our extended spp is related to the recently introduced partial spp [cit] that allows for path rankings that are entirely unknown. however, we believe that the extended spp model is better suited for the analysis of existing bgp configuration files. section iv develops data structures derived from the extended spp framework while section v presents the theoretical results needed to apply these structures to the safety problem. finally, section vi describes a methodology of configuration analysis that is based on the theoretical models and structures of the preceding sections."
"coupled with genre labeling provided by a classification mechanism (e.g., svm), this could be a powerful genre-based browsing tool. however, although they illustrate the potential of our descriptors, these results are only preliminary, and more detailed tests must be conducted."
"one of the common problems to be faced when integrating the information from the selected databases is the recognition, of duplicate entries. as can be seen in figure 8, if we compare the original results of the databases that store information about genotype-phenotype relationships (variations), we can observe that 56 of them are common to all the repositories. due to the lack of representation standards, the intersection of results necessitates verification if there are discrepancies in the information associated with each variation. for instance, it is common to find conflicts in the location of the variation in the genome. this happens because of the use of different reference sequences to locate the variations, which leads to discrepancies in the start and end positions. ensembl uses the genome reference version grch38 (the latest one), clinvar uses both versions and alzforum uses the previous one (grch37). as a consequence, there is a requirement to select one of the reference sequences and translate all values to the new coordinates. another common error is related to the gene affected by the variation. sometimes the variation occurs in a sequence between two genes (intergenic) and the database shows the nearest gene to the variation. this leads to confusion because the variation is not located in that specific gene and it may be that its function is not affected. it is important to take this situation into account because knowledge evolves quickly and, in the intergenic region, a new gene can be found at any time. an example of this situation occurs when the variation is mapped according to old reference sequences."
"from the angle of the information sources, audio information proves to be highly efficient compared to visual information, and leads to very good classification ratios (see figure 8) . at genre level, audio features are more accurate at retrieving music, sports, news, and commercials, as these genres have specific audio patterns. using contour and color-action information alone proves to be less efficient. compared to color-action parameters, contour parameters yield better performance for documentaries, sports, and news, which have salient contour signatures such as skyline contours and silhouettes of people (see figure 8 ). compared to contour parameters, color-action features perform better for music, commercials, movies, and news (which can be attributed to the specific rhythm and color diversity, see also figure 8 ). compared to audio descriptors, visual descriptors used in combination are more discriminative for animated movies, movies, and documentaries. finally, the best performance in classifying each individual genre was achieved by using all audio-visual information available."
"in the next subsections we present the main purpose and the steps that are performed at each level of sile, with the aim of determining the genetic causes of eoad."
"although virus replication rates can vary considerably depending on the nature of the mutations a virus may undergo, we choose replication rates to be 0.5 (ml · day) −1 for all mutants. we justify this selection by noting that escape mutants grew to be dominant mutants during selection experiments and assume that replication rate variability due to mutations were negligible."
"the example descriptors emphasize their specificity with respect to video genre, classification tests demonstrate their power for genre classification, and 3d feature-based representation show their potential in real-world browsing applications. finally, we also present a comparative benchmark evaluation."
"we synthesized a nominal stabilizing controller using (4) comprised of an antibody pentamix (0.4687,0.7815, 0.6129, 0.6279, 0.8831) µg/ml of (3bc176, pg16, 45-46g54w, pgt128, 10-1074), and a robust controller using (8) that consisted of antibody trimix (0.6891,0.6712,1.0706) µg/ml of (3bc176, 4546-g54w, pgt128). these both were generated for the evolutionary dynamics of the full, thirty five hiv mutants listed in figure 5 ."
"the adaptability of conceptual models provides a flexible approach to extend them according to the evolution of the domain. as new discoveries are made, new concepts can be included in the schema, new relevant data sources can easily be considered and new attributes can be identified in order to improve the data analysis process."
"first of all, a set of interesting dq dimensions to be checked has been selected: believability, relevance, reputation, currency and accessibility. by using the nar catalog, we performed a research of the databases which belong to the type of genomic data sources considered as relevant and, as a result, 43 repositories have been analyzed: 7 databases of sequences, 6 databases about the human genome, 27 databases about human genes and diseases, and 3 databases about scientific literature. finally, 7 of these have been selected as the most reliable for extracting the required information. the metrics that are used to select the databases are listed below:"
"theorem v.7 proves that it is not possible for pd(s) to have a cycle and md(s) to be acyclic. for the case when pd(s) is acyclic but md(s) has a cycle, we present two options."
"in summary, for a given image with n extracted and partitioned contours, we obtain a list of 7 geometric and 4 appearance attributes for each contour. for each attribute, a 10-bin histogram with n values, is generated."
"in order to make the huge amount of available information affordable, both solutions (cshg and data quality methodology) have been combined to develop a genomic information system (geis) with the aim of supporting the identification of clinically relevant variations in a patient's sample. the consolidation has been made by defining sile, a methodological approach whose main goal is to systematize the search and identification of genomic information to be loaded, analyzed and exploited by a geis."
"we approach video genre categorization by exploiting audio and visual (temporal, color, and contour-based) video modalities. our selection is motivated by the specificity of these information sources with respect to video genre."
"it is important to notice that most of the variations were discarded due to the lack of relevant statistical evidence. this is caused by the characteristics of the disease. eoad is a rare type of alzheimer so the studies are performed over small populations or delimited families. because of that, the evidence currently available is not enough to be used in clinical practice and more research needs to be done. the information associated with the final set of variations is summarized in table 6 ."
"in this article, we present a systematic approach to extract small valuable datasets from the big data lake of genomics, facing ontological and data quality challenges. the aim is to identify relevant variations in genes which are related to the risk of suffering a certain disease, and which will be used to populate a genomic information system (geis). our proposal is based on two principles:"
"in conclusion, the systematic application of conceptual modeling and data quality criteria is a key to creating the link between the big data perspective and the smart data perspective. it provides veracity and value to the final dataset that will be used in clinical practice, as can be shown in figure 3 . in the next section, we will introduce the conceptual schema of the human genome (cshg), an essential component to provide structure to this complex domain and the starting point for identifying valuable genomic information."
the advantage of the cshg over other options is that it covers the entire structure of the genome; so data stored in different genomic repositories can be easily connected by using this structure. the use of the cshg allows researchers to identify the relevant information needed to answer a knowledge requirement and thus obtain a notion of which type of data sources can provide it.
"as has been explained previously, by using the cshg, the most important attributes corresponding to each required piece of knowledge can be identified (the data context). this helps to select the most suitable and complete data sources to interrogate so as to obtain the required information."
"the information about variation-disease relationships comes from different resources, due to the fact that each database stores information about a certain type of variations or diseases. in our case we have selected two databases which belong to ncbi: (i) clinvar 11, a public archive of reports about the relationships among different types of human variations and phenotypes, with supporting evidence; (ii) dbsnp 12, a public archive of short sequence variations, including single-base nucleotide substitutions, small-scale multi-base deletions or insertions, and microsatellite repeats. in addition, two more well-known databases are used to extract information: (i) ensembl 13, a repository which provides comparative information about different species, and tools to support research in many different areas, including all the selected databases fulfill the established dq requirements. as can be shown in figure 6, each database provides information about a specific area of the cshg. this helps to join together, under a holistic view, all the information required to get a better understanding of the disease. some of the reasons why the other databases were excluded are: inactivity of the repository (e.g. ad&ftdmdb), lack of revision from experts (snpedia) or not enough data about the evidence that supports the relationship between the variation and the disease. litvar is a special case, due to the fact that it is a useful database supported by ncbi; but it is still under development and, currently, the available release is a beta version. this affects the believability of the information provided."
"on the other hand, these data sources contain millions of records with different levels of quality due to the complexity of biological processes, the noisy nature of experimental data and the limitations of statistical analysis; besides, there is a bias due to the use of different sequencing technologies and sampling strategies. this is the reason why only part of this \"data lake\" is reliable enough to provide precise clinical diagnosis and treatments."
"due to the specificity of the genomic domain, the first step in determining the relevant dimensions is to be able to understand the issues that affect the information. to accomplish this task, a study of the most common errors present in different well-known genomic data sources has been performed [cit] . the study allowed us to classify them into nine major quality dimensions, which can be seen in table 1 . completeness the extent to which data is not missing and all necessary values are represented [cit] consistency data must be consistent between systems and represented in the same format [cit] redundancy the extent to which the information is redundant or the database contains duplicate records [cit]"
"for displaying movies, we combined all descriptors, since this provided the best classification results. as we are restricted to only 3 axes in selecting the most representative components, we used principal component decomposition. each movie is displayed according to: figure 12 : feature-based 3d movie representation in a spherical coordinate system (inclination-θ, azimuth-ϕ, radius-r). each movie from the data set is represented by a point with which we associate an image vignette. views a to e are screenshots taken from different perspectives (the points of view used are shown in the chart). in views a-e, representative genres are annotated (a demo is available at http://imag.pub.ro/~bionescu/index_files/movieglobe.avi)."
routers execute the bgp decision process to select a single best path. every step of the decision process splits equivalence classes of paths into smaller classes until a single path remains.
"the cshg has been developed in close collaboration with experts in the domain. it is thus based on biological knowledge and is independent of any specific data sources. this characteristic helps the experts in the domain to understand the structure of the information without the need of studying the internal schema of each repository. the cshg has five main parts, each one related to a specific domain view [cit] :"
"in any case, if there is a preference arc from p i to p j in pd(s), there is a sequence of one or more preference arcs from p i to p j in pd(r(s)) for any r(s). we conclude, then, that a cycle in pd(s) implies a cycle in pd(r(s))."
"in this section, we describe a configuration analysis algorithm that leverages our theoretical results from previous sections. these results enable us to reduce the complexity of verifying the bgp safety of router configuration files in several cases. they also provide tools to account for partial information. by partial information, we mean that only a few isps share their configurations, while the rest of the internet has unknown policies."
"due to the complexity of genomics, in this article we focus on a particular use of genomic data applied to clinical practice: the identification of dna variations in genes which are related to the risk of suffering a certain disease. in this case, a huge amount of open data repositories is available and the number of public biological data sources cannot be precisely determined on account of their volatility. online catalogs such as the ones provided by the nucleic acid research journal (nar) [cit] or the human genome variation society 4 (hgvs), are useful to get some idea of the multitude of repositories which are publicly available. furthermore, some repositories are created for a specific purpose or in the context of a particular research and they are not updated or maintained so, as time goes by, they are no longer accessible or useful. in figure 1, an example of the evolution of the number or public repositories during the last 4 years is presented. each data source has advantages and disadvantages, which must be considered thoroughly according to the task to be performed. this means we do not have to query all available data sources, but only those that are relevant, thus reducing the volume of data to manage."
"this center is part of a global consortium that shares information about advances in science and health, by providing access to biomedical and genomic information. most of the databases supported by ncbi are publicly accessible. the information about the structural elements of the dna sequences is retrieved from the ncbi reference sequence database 8 (refseq), an integrated and non-redundant set of reference sequences. these sequences belong to the reference genome (also known as reference assembly) which is a digital nucleic acid sequence database, assembled by scientists as a representative example of a species' set of genes. specific information about genes is retrieved from ncbi gene 9, a repository of integrated information from a wide range of species. the information about bibliography is retrieved from pubmed 10, a repository which stores more than 28 million citations for biomedical literature."
"the gao-rexford guidelines [cit] prevent problematic specifications. the guidelines define the roles (provider/customer, or peer) that an isp is allowed to have with respect to each of its neighbors. depending on the role, the isp then has to follow the corresponding rules that specify (a) the preference it may assign to routes, and (b) the routes it is allowed to announce to other neighbors. safety and autonomy are guaranteed. however, expressiveness is severely limited to those policies that comply with the guidelines. recent studies [cit] indicate that the internet has many more peering relationship types, and thus more policies, than the fundamental hypothesis of the gao-rexford model assumes. fig. 1 illustrates some simple scenarios that violate the gaorexford guidelines, yet we believe these to be quite common in today's internet. fig. 1a depicts isp 2 providing a \"backup route\" service to isp 1, which wants the rest of the nodes to access it through its primary provider, node 4, unless the 4-1 link is down. in that case only, nodes should access 1 through the backup link 2-1. fig. 1b presents a situation where isp 1 selects the peer path 1654 over the customer path 1234 to avoid sending traffic through a specific autonomous system (as). if the rest of the ases follow traditional policies, the specification is still safe. in fig. 1c, isp 4 announces the route it learns from its peer isp 5 to another of its peers, isp 3. although announcing peer routes to peers is prohibited by the gao-rexford guidelines, this policy may be desirable to isps that wish to strengthen their peer relationships and depend less on their providers [cit] . isp 3 now has two choices, paths 3456 and 32156, instead of only its provider path through 2."
"in table 2, the attributes of the cshg have been categorized as \"required\" if the values must be present, \"recommended\" if the values can be missing but would provide interesting information about the variation, and \"other\" if the values can be missing."
"as mentioned earlier, there are no known convex reformulations of this problem due to the additional structure on l. as such, we suggest the following iterative algorithm, based on the convex programs (6) and (7)"
"a challenge inherent to the treatment of certain infectious and non-infectious diseases, such as hiv or cancer, is the risk that the pathogen or tumor will evolve away and become resistant to treatment methods that comprise the standard of care. especially vulnerable to this phenomenon are treatment methods that involve exposing the disease population (such as viruses or cancer cells) to therapies targeting specific molecules involved in disease progression for an extended period of time. while these targeted therapies have the benefit of allowing physicians to tailor treatments to a patient's tumor cell population, they nonetheless establish an environment in which the occurrence of mildly drug resistant pathogens or tumor cells can develop an evolutionary advantage over those for which the therapy is targeted, leading to so called 'treatment-escape'."
"in this article, we have established the importance of using conceptual models and data quality methodologies to define a roadmap in order to move from the big data perspective to the smart data perspective. the lack of an ontological commitment to define core biological terms is solved by using the cshg. the variable level of quality, which affects the information available, is managed by the development of a data quality methodology based on specific dimensions and metrics."
"block-level audio features, temporal-based descriptors, color perceptual descriptors and statistics of contour geometry. these sources of information have previously been exploited, but our approach provides a novel way of computing these content descriptors. the main contribution of our work, however, lies in harnessing the descriptive power of the combination of these descriptors in genre classification. we validated our approach in several experiments using over 91 hours of video footage encompassing seven common video genres (animated, movies, news, sports, commercials, movies and documentaries). [cit] benchmarking campaign proved the superiority of our proposed audio-visual descriptors compared to other validated approaches."
future improvements will mainly consist of approaching sub-genre categorization and consideration of the constraints of very large scale approaches (millions of sequences and tens of genre concepts).
"before one can address the issues involved in analyzing and managing data quality in the genomic domain, it is important to understand well what data quality actually means. data quality (dq) has been defined by wang and strong [cit] as \"fitness for use\", i.e. the ability of a data collection to meet users' requirements. dq is evaluated by means of different dimensions: this definition mainly depends on the context of use. a data quality dimension can be assessed by using specific metrics in order to get a quantitative measure that represents the quality of the data being managed. but to apply this knowledge properly, a sound methodology needs to be defined."
"the paths digraph encapsulates the computational dependencies that exist among possible paths [cit] . formally, given an extended spp s, the paths digraph pd(s) is a directed graph where nodes represent the permitted paths in s. its arcs are of two types: transmission, defined as in dd(s), and preference. there is a preference arc from path p to path q if p is in the immediately preferred equivalence class to q (for paths with the same origin). fig. 4 shows the paths digraphs for all the refinements of the specification in fig. 2 that were described in example iii.2."
"since md(s) was created with partial information, the absence of a cycle can be illusive. there can still be arcs connecting paths that belong to nodes with unknown policies. for instance, nodes p 7x and p 5x may have direct links between them. nodes 1, 3, and 4 (that share their configurations) will not know whether nodes 5 and 7 exchange routes through unsafe policies, but they will be aware of the risk. this is no different from the case that safety is ensured through guidelines: following guidelines is up to each individual isp."
"the principal contribution of our work, however, lies in realizing the descriptive power of the combination of these descriptors in genre classification. extensive experimental tests conducted over 91 hours of video footage spanning seven common video genres yielded excellent classification ratios."
"prior to parameter extraction, we project colors onto a more manageable color palette (initial images are true color). we selected the non-dithering 216 color webmaster palette because of the high color diversity and its efficient color naming system: each color is named according to the degree of hue, saturation, and intensity [cit] . color mapping is performed with a classic dithering scheme [cit], and colors are selected in the l*a*b* color space [cit] . further, the proposed color parameters are computed as follows:"
"the proposed descriptors achieved an overall map of up to 12% (see team raf in table 4 ), which -considering difficulty of the task -is significant. also, these were the best results obtained using audio-visual information alone. using descriptors such as cognitive information (face statistics), temporal information (average shot duration, distribution of shot lengths) [cit], audio (mfcc, zero crossing rate, signal energy), color (histograms, color moments, autocorrelogramdenoted \"autocorr.\"), and texture (co-occurrence -denoted \"co-occ.\", wavelet texture grid, edge histograms) with svm resulted in maps below 1% (see team kit in table 4 ); clustered surf features in combination with svm achieved a map of up to 9.4% (see team tub in table 4 ). we achieved better performance even compared to some classic text-based approaches, for instance, the term frequency-inverse document frequency (tf-idf, map 9.8%, see team uab in table 4) and the bag-of-words (map 5.5%, see team sinai in table 4 ) approaches. compared to visual information, audio descriptors seem to provide better discriminative power for this task."
"the specification in fig. 3a can also be seen as an equalcost multipath routing (ecmp) [cit] example. in this paper, we do not model multipath routing, although the extension to spp and the data structures presented in section iv can prove useful in such a study."
"text-based information may produce high error rates (due to automatic recognition) and is usually computationally expensive to process (e.g., optical character recognition -ocr); object-based information, although also computationally expensive to obtain tends to be semi-automatic (requires human confirmation); motion information is available in high quantities during the entire sequence (object/camera), but is insufficient by itself to distinguish between some genres, for instance, movies, sports, music. in contrast, audio-based information provides good discrimination and requires fewer computational resources to be obtained and processed; color information is not only simple to extract and inexpensive to process, but also powerful in distinguishing cinematic principles; temporal-based information is a popular choice and proves to be powerful as long as efficient video transition detection algorithms are employed."
"all of these variations affect processing or production of beta-amyloid, the protein fragment that is the main component of plaques in the brain. plaques are abnormal clusters of protein fragments, build up between nerve cells. beta-amyloid is a prime suspect in the decline and death of brain cells. several drugs, currently under development, target beta-amyloid as a potential strategy to stop the disease or significantly slow its progression [cit] . this corroborates the importance and relevance of the selected variations."
"for instance, most common video genres have very specific audio signatures, for instance, music clips contain music, news contain many monologues/dialogues, documentaries have a mixture of natural sounds, speech, and ambient music, in sports there is crowd noise, and so on. to address these particularities, we use audio descriptors related to rhythm, timbre, onset strength, noisiness, and vocal aspects."
"we conclude that regardless of the acyclicity of pd(s), if md(s) has a cycle there always exists a refinement r(s) such that pd(r(s)) has a cycle. from theorem iv.2, dd(r(s)) has a cycle as well."
"to asses the representative power of the proposed content descriptors, we conducted several experiments. for validation, we selected seven of the most common video genres, namely animated movies, commercials, documentaries, movies, music videos, news broadcast, and sports. classifying these genres is challenging due to content similarity: animated movies include natural scenes (we used not only cartoons but also artistic movies, see [cit] ), commercials also include cartoons, news include scenes from sports and scenes resembling documentaries, music clips tend to have visual patterns similar to those of commercials, and so on."
"the interest in moving from a big data to a smart data perspective comes from the need to extract relevant data that can be used in daily work. this is especially important in fields such as genomics applied to clinical practice, due to the increasing number of public resources that are becoming available, as well as their variable level of quality."
"instead of checking whether the current configuration leads to a safe refinement by executing additional steps, the second option is to select one of the safe refinements and configure the network accordingly. lemma v.10 states that there are refinements which are safe and refinements that are not. in other words, depending on the decisions made in subsequent steps of the decision process, the specification can be guaranteed to be safe or not. from the proof of theorem v.8, we know which refinements are guaranteed to be safe: the refinements which split the multinode in fig. 7d so that path p b is more preferred than p a . thus, we know what decisions subsequent steps in the bgp decision process need to make in order to have a system that is guaranteed to be safe."
"the vast majority of the information generated by biological research centers or biotechnological world-wide consortia is publicly available so that it can be used by the community: over one thousand repositories of open genomic data, which help biologists and clinicians to extract meaningful gene-disease associations, improving their ability to tackle complex diseases in a multidisciplinary and individualized way (precision medicine). however, genomic repositories have commonly been developed in an ad-hoc way, focused on addressing specific knowledge requirements, but not designed to share information among them."
"-m3: the information about the variations is defined by using standard vocabularies and verified ontologies to determine critical attributes such as hgvs expressions, pathogenicity or functional effects. -m4: there must be no conflicts in the clinical interpretation of each variation."
we also address the requirement that the synthesized controller be not only robust to unmodeled dynamics but also exhibit sparse structure and small feedback gains. this is motivated by the fact that the number of therapies commonly used in combination to treat a disease is often small while the number of potential usable therapies are often very large [cit] . targeted therapies such as small molecule drugs or antibodies exhibit a maximum effective concentration beyond which side effects are likely to worsen and no additional drug benefits are seen.
"we averaged thirty simulations of closed loop evolutionary dynamics subject to 5.5% random time invariant perturbations in the plant dynamics using both sparse and full support h ∞ and l 1 controllers. the sparse controller found by the l 1 algorithm performed better than the one found by h ∞ algorithm, whereas the situation was reversed for the respective synthesized full support controllers. as previously mentioned, the motivation for generating sparse controllers for combination therapy is that number of therapies commonly used in combination to treat a disease is often limited for clinical reasons. therefore, the potential for the l 1 algorithm to synthesize controllers that are not only sparse but more robustly stable than h ∞ algorithm is a desirable feature. figure 2 shows the relationship between gain sparsity and both h ∞ and ∞-induced norms in the synthesis of h ∞ and l 1 controllers. although closed loop h ∞ norms remain constant with respect to sparsity for both controllers, the closed loop ∞-induced norm decreases with sparsity with the suggesting that the l 1 synthesis algorithm, as expected, finds better performing sparse controllers."
"involved in analyzing bgp safety from configuration files. this allows us, in many cases, to only evaluate parts of the bgp best path selection process, without losing accuracy. we propose a new data structure, the multipath digraph, that is well-suited for detecting problematic conditions, and we prove properties that allow us to demonstrate the feasibility of applying it for verifying bgp safety in practice."
the remainder of this paper is organized as follows: section 2 discusses several genre classification approaches and situates our work accordingly. section 3 deals with extraction of features:
"the article is structured as follows: in section ii, we recall the extended quasispecies evolutionary dynamics model that encodes replication, mutation and neutralization and summarize relevant results in controller design of positive systems. in section iii, we present our l 1 combination therapy synthesis algorithm. section iv illustrates our al-gorithm in the context of an hiv antibody therapy design problem previously studied in an experimental setting [cit] . in this section we also compare performance and scalability properties of our l 1 algorithm and the previously developed h ∞ algorithm [cit] . section v ends with concluding remarks and directions for future work."
global weighted color histogram captures the global color distribution of the movie. it is computed as the weighted sum of each individual shot average color histogram:
"in this section, we address the aforementioned nonconvexity of the optimal control problem by formulating an iterative algorithm for finding effective drug concentrations. our main result addresses the issue of synthesizing a stabilizing controller subject to the constraints imposed by the quasispecies model (2), with acceptable robustness properties characterized in terms of its ∞-induced closed loop norm."
"two different classifiers were evaluated: the nearest neighbor (k-nn) classifier, to indicate retrieval performance, and a support vector machine, to indicate the achievable classification performance."
"contour characterization. contour processing starts with edge detection, which is performed with the canny edge detection algorithm [cit] . for each contour, a type of curvature space is created. this space is then abstracted into spectra-like functions, from which a number of geometric attributes, such as the degree of curvature, angularity, circularity, symmetry and \"wiggliness\", are derived. in addition to these geometric parameters, a number of \"appearance\" parameters are extracted. they are based on simple statistics obtained from the luminance values extracted along the contour, such as contrast (mean and standard deviation, abbreviated c m, and c s respectively) and \"fuzziness\", obtained by convolution of the image with a blob filter (f m, and f s, respectively)."
"for each selected pair, a number of geometric attributes is determined: the angular direction of the pair (γ p ), the distance between the proximal contour endpoints (d c ), the distance between the distal contour end points (d o ), the distance between segment center (middle) points (d m ), the average segment length (l), the symmetry of the two segments (y), the degree of bendiness of each segment (b 1 and b 2 ), the structural biases (ŝ) which express to what degree the pair alignment is an l feature (ŝ l ), a t feature (ŝ t ), or a \"closed\" feature (two curved segments facing each other as '( )',ŝ () ). in total, 12 geometric relational attributes are extracted for the selected pairs. again, for each attribute a 10-bin histogram is generated. figure 4 shows an example of the representative power of these descriptors in image-based categorization. we present the results obtained by similarity-based contour search in the corel collection (60000 images) using three concepts: \"landscape\", \"entrance\", and \"people\". the contour of the first image in each row is the selected sample contour, the remaining images in each row contain the most similar contours. regular objects can be associated with particular salient contour signatures (see the blue contours in figure 4 ) and retrieved accordingly with good recognition rates [cit] . this information can be very useful in tackling our genre classification problem. the structural information is extracted only from a summary of the movie. in this case, we retained around 100 images that are evenly distributed with respect to video transitions. as previously mentioned, at image level, contour properties are captured with histograms. to address the temporal dimension -at sequence level -the resulting concatenated feature vectors are averaged to form so the structure signature of the movie (see also the examples in figure 7 )."
"this article is structured as follows: in section 2, we introduce the state of the art. then, in section 3 we present the \"from big data to smart data\" perspective applied to the genomic domain. in section 4, we show how the conceptual schema of the human genome (cshg) is useful to provide the ontological ground required to understand the key concepts of the domain. in section 5, we explain the principles of data quality (dq) which help to ensure the reliability required to apply the results to clinical practice. in section 6, we present a practical example of how this approach can be applied: the sile method, which acronym refers to the stages that make it up (search, identification, load and exploitation). finally, we present the conclusions and suggested future work in section 7."
"the task to be performed must drive the choice of the particular pieces of information (variables) which are critical to achieve it. the description of the variables required is determined by the cshg; and the specification of the ones which are going to be used in the dq assessment process, is determined by the selected dimensions and their corresponding metrics. for instance, to measure the believability of a variation we must focus on the number of publications related to it. according to the cshg, the most suitable metric to measure this attribute is \"pubmed_id\", which is a unique identifier, provided by the bibliographic repository pubmed, to each publication it stores. if the variation has at least one pubmed id, then it passes the quality filter."
"one of the biggest concerns in any investigation is missing data (incompleteness) because they can compromise the validity of the resource and any conclusions obtained by using that information. it is important to determine which variables are more or less likely to be missing, to define a priori an acceptable percentage of missing data, and to be aware of the effort that would have to be made to minimize the amount of missing information."
"also, [cit] benchmarking campaign proved the superiority of the proposed audio-visual descriptors compared to other approaches. further, we tested our descriptors within a practical application, namely automatic genre categorization of video documents for potential use with media platforms (e.g., video rental, selling). we propose a prototype 3d browsing environment in which movies are displayed according to descriptor-based coordinates. preliminary results show that movies tend to regroup according to similarities in content and genre, which is a very interesting result."
"it must be noted, however, that the results presented in table 4 cannot be definitive, as the classification approaches were not trained and set up strictly comparably. teams were allowed to access other sources of information than those proposed in the competition). for instance, we used 648 sequences for training, whereas team kit used up to 2514 sequences. most text-based approaches used query expansion techniques (e.g., wordnet -see http://wordnet.princeton."
"advances in the theory of policy-based routing (for example, [cit] have yielded sufficient conditions that ensure bgp safety -that is, guarantees of convergence to a unique stable state. yet flexibility is typically seen as the more important concern in the operation of real-world networks. we can break flexibility down into the distinct concepts of autonomy and expressiveness. autonomy here refers to the ability of each isp to configure its own network with little or no global coordination, while expressiveness refers to the ability to innovate in the policy domain in response to evolving and unforeseen customer needs. an open research question in inter-domain routing is whether or not it is possible to design protocols that guarantee safety while maintaining levels of flexibility acceptable for isp operations."
"on the other hand, the use of databases with large amounts of missing information, or that do not have rigorous and standardized data editing, cleaning, and processing procedures, increases the risk of inconclusive and potentially invalid results. after all, the value of the results is only as good as the quality of the data used. in respect of this issue, one question arises: how can a database be considered as relevant for the task at hand? the use of data quality management techniques is helpful in determining the most suitable data sources, and the application of them will be explained in section 5."
"(b) avoiding sending traffic through a particular as may require preferring a peer or provider route over a customer route. as 1 prefers the 1654 peer route over the 1234 customer route, because it does not wish its traffic to traverse as 3."
"volume, velocity and variety are the three vs in the original definition of the key characteristics of big data according to the research report published by meta group [cit] . volume refers to the size of the data, velocity refers to the speed of data generation and variety refers to different types/sources of data. since then, other factors have also been considered, such as veracity (trustworthiness of the data obtained) and value (usefulness of data) [cit] ."
"-m6: each variation must have significant medical or genealogical consequences and be reproducible (e.g. the reported consequence has been independently replicated by at least one group, besides the first group reporting the finding)."
"in other words, under extended spp, two distinct paths p 1 and p 2 which do not share the next-hop node can still be equally preferred by a node u i :"
"we apply our methodology to the partial information problem, since we expect our tool will never have complete configuration information. when checking configurations for a few isps, outside paths are at best partially known. furthermore, isp configurations may not always be complete. our future work includes completing the implementation and evaluation of a tool that applies our methodology to real isp networks."
"the most reliable video genre classification approaches, which also target a wider range of genres, are multi-modal, that is, multi-source. in this section, we discuss the performance of several approaches -from single-modal (which are limited to copeing with a reduced number of genres) to multi-modal (which are able to perform more complex classifications) -we consider relevant for the present work."
"to achieve what we refer to as the \"smart data\" perspective presented in this article, we need a conceptual structure in which to store each piece of genomic data in the right place, regardless of the data's origin. to accomplish this goal, we propose the use of the conceptual schema of the human genome (cshg) [cit] ."
"we synthesized a nominal stabilizing controller using (4), a robust controller that minimizes the h ∞ closed loop norm using (8), and a robust controller using (7) that minimizes the l 1 closed loop norm for the evolutionary dynamics of the eighteen hiv point mutants listed in figure 5 . we found similar gains and robustness properties for both sparse and full controllers using either algorithm with the notable difference seen in computational time. not surprisingly, the l 1 algorithm has far superior performance, beating the runtime for the h ∞ synthesis algorithm by four orders of magnitude."
"another problem is that, due to the lack of standards in representing biological information, it is common to find different ways of representing the same concept. for instance, there are different ways to determine which nucleotides are affected by a variation:"
"let (p i, p j ) be a preference arc in pd(s). then, p i and p j belong to different equivalence classes p i and p j in s, respectively. each of the paths in p i has a preference arc towards each path of p j in pd(s). for any refinement r(s), the least preferred path of p i has a preference arc towards the most preferred path of p j in pd(r(s)). every refinement will also specify strict preferences among the paths in p i and p j . therefore, there will be a sequence of zero or more preference arcs from p i to the least preferred path of p i in pd(r(s)). similarly, there will be a sequence of zero or more preference arcs from the most preferred path of p j to p j ."
"if md(s) has a cycle, we need to create pd(s) and check if it has a cycle. theorem v.8 provides an alternative to the pd(s) construction. checking for a cycle in pd(s) is equivalent to checking the type of cycles in md(s). if there is a cycle in md(s) that does not satisfy the conditions specified in theorem v.8, then that cycle is guaranteed to appear in pd(s) as well. if pd(s) has a cycle, then sufficient conditions for safety are not met for any r(s). again, it is unnecessary to consider other steps of the bgp decision process. the specification will not meet the sufficient conditions regardless of the decisions made by bgp in subsequent steps."
". since any refinement respects the preferences in s by definition, there is a ranking arc from m i to m i+1 in md(s). in either case, if there is an arc from p i to p i+1 in md(r(s)), m i ≡ m i+1 or nodes m i and m i+1 are also connected in md(s). therefore, the existence of a cycle in md(r(s)) implies the existence of a cycle in md(s). since this conclusion contradicts the hypothesis, there can be no cycle in md(r(s)) if there is no cycle in md(s)."
"our goal is to partly execute the decision process and still be able to decide on the satisfaction of sufficient conditions for safety of the routing system. in other words, we are interested in answering the following question: given an extended spp specification, are any of its refinements safe? we also wish to provide the network operators with information on how to configure their networks in order to reach a safe refinement."
"once the errors have been corrected and the load has been finished, the hgdb will store a set of variations selected according to the dq established and that are ready to be analyzed by specific tools, in order to extract the underlying knowledge."
"another common source of errors is related to the reference and alternate alleles that indicate the change which occurs in a certain position. the dna is composed of two complementary chains called forward strand and reverse strand. depending on the strand, the alleles are different, but not all the databases provide information about the strand used. this identification is the key, when we try to identify the variations present in a patient's sample, to identify precisely a specific variation leading to an important problem of missing information."
"on the one hand, there is a vast amount of data ready to be explored, but, on the other hand, only part of them is sufficiently valuable to be applied in clinical practice. big data essentially means all data, but data lakes by themselves are meaningless for biologists. in order to obtain true benefits, big genomic data needs to be turned into actionable small datasets, clearly focused on the purpose, insights and resulting outcomes that can be used in daily work: e.g. to understand the genomic nature of a particular disease. this is why there are also the \"smart\" data required to manage adequately the information in such a complex context as genomics. the core of our work is to provide a systematic approach to handle the huge amount of open genomic data, in order to get a subset, of whatever size, which is valuable for biologists and also cross functional; this is what we mean by 'from big data to smart data perspective'."
"in this paper, we address the global classification task. since it is related to data mining, video genre classification involves two steps: feature extraction and classification. feature extraction and selection is one of the main critical steps determining the success of the classification task. in the literature, many sources of information have been exploited [cit] ."
"edu/, wikipedia -see http://en.wikipedia.org). nevertheless, these results provide a good overview and (crude) comparative ranking of the performance of various methods and in consequence of the proposed descriptors."
"during the last two decades, advances in research technologies such as next generation sequencing (ngs) have allowed us to read (sequence) dna in a faster and cheaper way. [cit] but, nowadays, is becoming a routine research tool. this has revolutionized our understanding of human biology and improved the study of how changes (variations) in the dna are involved in the risk of suffering a certain disease [cit] ."
"the aim of the exploitation level is to extract knowledge from the information system. nevertheless, this is not a trivial task. several tools have been developed to support researchers in genetic data analysis. however, the lack of intuitive and interactive-usable mechanisms of such tools, transforms the analysis activity into a complex and time-consuming task. in order to provide a solution useful for clinical purposes, the data exploitation tools must enhance data discovery, enlarge visualization, allow the performance of data analysis operations and contextualize data by augmenting it [cit] . one of the tasks that can be performed is related to the enhancement of precision medicine (pm). as it has been explained in the introduction, pm is an emerging approach for disease treatment and prevention that takes into account, for each person, individual variability in genes, environment, and lifestyle. this approach allows doctors and researchers to predict more accurately which treatment and prevention strategies for a particular disease will work in which groups of people. this is in contrast to a one-size-fits-all approach, in which disease treatment and prevention strategies are developed for the average person, with less consideration for the differences between individuals."
the task previously determined dictates the type of data required; and the researcher must best match the data to the question; i.e. variations in the dna sequence of genes related to the risk of suffering the early onset alzheimer's disease (clinical significance).
"in this work, we propose a methodology that allows isps to check their router configurations for safety. we leverage prior work on the stable paths problem (spp), but we bridge the gaps needed to make spp applicable to a real-world implementation. specifically, we extend spp to reduce the complexity (a) specification s."
"in previous sections the needs of the cshg and the description of a dq methodology to determine relevant information has been described. but, how can both proposals be combined in order to move from the big data perspective to the smart data perspective?"
"finally, in the next section we are going to explain how the cshg and the dq methodology proposed can be joined together in order to accomplish our proposed \"from big data to smart data perspective\" for the genomic data management domain."
"-m8: it is highly recommended that the database provides ways to allow programmatic access to the information stored. in table 5, we have summarized the results of the analysis of the top 10 repositories in order to clarify how the selection of the final set of databases has been performed. the results have been sorted according to the number of filters passed."
"once the information from each database is identified, the next step is to map it to the structure of the hgdb. additionally, the mechanisms to solve every possible inconsistency must be clearly defined in order to be implemented in the next level of sile."
"note that in previous work [cit], the definition of a preference arc does not require p to be in the immediately preferred class to q, it can be in any class that is preferred to q. this means that additional preference arcs were present. such additional (p, q) arcs do not affect the presence or absence of a cycle, since p and q are in any case connected by a chain of other preference arcs. consequently, theorem iv.2 holds for the pd definition in this paper."
"in this section, we prove the results on which our methodology is based. specifically, we show how the graphs of section iv can be used to verify the safety of a refinement r(s). note that we make no assumptions regarding the strictness property. specification s is modeled as an extended spp, which makes our algorithm less sensitive to the vendorspecific implementation details of the bgp best path selection process. fig. 6 summarizes our key results."
"it is important to note that this relaxation to the guidelines does not always create an unsafe system. however, when multiple isps choose such \"non-standard\" policies, the interactions can lead to unexpected results. for example, rfc 4264 [cit] describes how scenarios like that depicted in fig. 1a can lead to multiple stable states, some of which violate the intent of the policy writers. it is exactly this kind of unintended policy interaction that motivates our work."
"finally, we tested the potential of the proposed descriptors at the application level. movies displayed according to feature-based coordinates in a prototype 3d browsing environment tend to regroup according to similarities in content and genre. coupled with genre labeling provided by a classification mechanism (e.g., svm), this could be a powerful genre-based browsing tool."
"genomics research is under constant evolution and data are generated quicker and quicker. the perspective of big data to smart data requires that our strategy must be able to adapt to any changes and new findings. the refinement of the tasks to be performed at each level of sile, the evolution of the cshg and the quality controls that are applied, conform to a cyclical process that must be continuously refined. this ensures the fulfillment of knowledge needs as long as the evolution of the domain continues."
"of course, most video genre classification approaches rely on visual elements. they exploit both static and dynamic visual information in the spatial domain, for instance using color, temporal structure, objects, or motion or in the compressed domain, for instance, using mpeg coefficients [cit] . color information is generally derived at image level and quantified with color histograms or other low-level parameters such as predominant color, color entropy, and variance (various color spaces are used, including rgb -red green blue, hsv -hue saturation value, or ycbcrluminance, chrominance) [cit] . temporal-structure-based information exploits temporal segmentation. a video sequence is composed of several video shots which are connected by video transitions, which can be sharp (cuts) or gradual (such as fades, dissolves) [cit] . existing approaches basically exploit their frequency of occurrence in the movie. although some approaches use this information directly [cit] (e.g., rhythm, average shot length), others derive features related to visual activity, for instance, by defining the concept of action (a high frequency of shot changes is usually related to action content) [cit] . object-based features in genre classification are generally limited to characterizing the occurrence of face and text regions in frames [cit] . motionbased information is derived either by motion detection techniques (i.e., foreground detection) or by motion estimation (i.e., prediction of pixel displacement vectors between frames, see [cit] ). common features describe motion density, camera movement (global movement) and object trajectory [cit] . finally, less common are features computed directly in the compressed video domain, for example, using dct coefficients (discrete cosine transform) or embedded motion vectors from the mpeg stream [cit] . their main advantage is their immediate availability with the video file."
"this strategy allowed us to identify 24 clinically relevant variations as the most meaningful ones. the process that was followed can be seen in figure 7 . starting from the databases that store information about variations and diseases, the metrics were applied in a certain order to ensure the effectiveness of the process. because the same variation can be stored in different databases, it is important to spot duplicate entries before starting the identification process. the issues associated with this task are explained in the next subsection. the refseq, ncbi gene and dbsnp databases were used to complete the required information, necessary due to the nature of these repositories. as the identification process progresses, the number of variations is reduced. finally, the variations from the original dataset are classified into 4 different categories: variations discarded due to contradictory evidence, variations discarded due to the lack of evidence associated to the disease, variations discarded due to there not being enough statistical relevance and variations accepted as relevant. this classification improves the traceability and replication of the results."
"where t x represents the total duration of all the gradual transitions of type x. this provides information about editing techniques which are specific to certain genres, such as live action feature films and artistic animated movies."
"is the color histogram of frame j from shot i, c is a color index from the webmaster palette, and elementary color histogram. this feature is computed by:"
"determination of the information context, required to solve a concrete need, as well as the selection of data sources from which to extract information (i) identification determination of a reliable and relevant dataset to be used to populate a database which structure is delimited by the cshg (l) load population of the database with the data identified in the previous level (e) exploitation extraction of knowledge from the database by using tools to analyse and interpret genomic data"
"in individual genre retrieval (binary classification), we achieved average precision and recall ratios of 87% − 100% and 77% − 100%, respectively, while average correct classification was up to 97%. using only audio information proves to be -compared to visual information -highly efficient in tackling this task. audio features are more accurate when classifying music, sports, news, and commercials. visual descriptors are more discriminative than audio for animations, movies, and documentaries. the best performance, however, is obtained when all audio-visual descriptors are used in combination. retrieving all genres simultaneously (multi-class classification) produced good results: the f score achieved ranged from 95.3% for news to 73% for commercials."
"if md(s) is acyclic, bgp safety is guaranteed for the specification. it is then unnecessary to consider additional steps of the bgp decision process. note that the local preference attribute is typically used to configure policies which override shortest path routing and are therefore more prone to bgp anomalies [cit] . due to the presented extension to the spp theory, we may be able to conclude the safety of such policies by examining this attribute alone."
"at this level, the inconsistencies identified previously must be solved so as to be adequately stored in the database. this is the objective of the subsequent transform process which ensures the consistency of the system. the difficulty of the transformation process depends on the complexity of the field and its representation within each data source."
"due to the lack of consensus in the use of standard terminology, it is common to find inconsistencies in the nomenclature related to the type of variations. for instance, ensembl considers 28 types of variations and clinvar considers 31 types, based on the type of change (insertion, deletion, etc.). nevertheless, alzforum classifies the variations according to their molecular consequence as well as the nucleotides changed (e.g. \"point, missense and gac to cac\" is type of variation d678h). this situation requires understanding of the ontology used by each data source, in order to create a mapping of the different terms that would guarantee the use of a unified terminology. a similar problem occurs with the name of the disease (phenotype) associated to the variation. there are different ontologies used to classify traits and diseases such as the human phenotype ontology [cit], human disease ontology [cit] and medgen 16 . in this case the mapping of terms is not a trivial task due to the complexity of the disease types and subtypes."
"color information is a powerful means of describing visual perception. most existing color-based genre classification approaches are limited to using intensity-based parameters or generic low-level color features [cit] such as average color differences, average brightness, average color entropy [cit], variance of pixel intensity, standard deviation of gray level histograms, percentage of pixels with saturation above a given threshold [cit], lighting key (measures how well light is distributed) [cit], object color, and texture."
"in addition, varsearch allows the researcher to go into detail about the characteristics of the variations found and the evidence that corroborates their relationship with the disease of interest. as all the information has been extracted from public repositories, it confirms ncf as being a valuable diagnosis tool for an advanced medicine in a precision working environment."
"for a preliminary analysis of the representative power of the proposed descriptors with respect to video genre we show the average audio, color-action, and contour descriptors in figures 5, 6 and 7, respectively, for each of the seven genres. in general, each genre behaves differently."
"we propose a more sophisticated strategy which addresses the perception of color content. a simple and efficient way to accomplish this is using color names; associating names with colors allows creating a mental image of a given color or color mixture. we project colors onto a color naming system [cit], and color properties are described using statistics of color distribution, elementary hue distribution, color visual properties (e.g., percentage of light colors, warm colors, saturated colors, etc.), and relationships between colors (adjacency and complementarity). color descriptors are extracted globally taking the temporal dimension into account."
"several screenshots taken from different angles are presented in figure 12 . interestingly, although we use only the first three principal components (which account for up to 94% of the initial data variance), movies from particular genres form clusters. due to similarity in content and structure, the most clearly grouped genres are news (see view b in figure 12 ) and sports (see view c in figure 12 ). other genres tend to be more \"interleaved\", as one might expect, since even human observers find it difficult to draw sharp distinctions between genres (see also the observations at the beginning of section 4). nevertheless, sequences of the same genre tend to regroup around a basis partition (see the examples in figure 12, e.g., animated movies -view d, documentaries -view e)."
"on the other hand, data quality has not been given due attention even though analytics and outcomes are highly dependent on the quality of the data on which they are based. for instance, there are types of genomic databases which are different as regards to their level of curation 1 : from redundant and non-curated data warehouses that store millions of genomic sequences, such as trembl 2, to highly accurate databases manually annotated and reviewed by experts in a specific field, for instance swiss-prot 3 . in such a critical context, as genomics applied to clinical practice is, this aspect becomes especially relevant. the use of data quality management principles helps to select the appropriate repositories and the most valuable data, in order to ensure the higher veracity of the results."
"many of the underlying principles of big data have been explored by the research community for years in different domains. nevertheless, theories and approaches for analyzing big genomic data are relatively recent [cit] . ngs requires more and more sophisticated algorithms and high-performance parallel processing systems to analyze and extract knowledge from a huge amount of genomic and molecular data. in this context, emerging deep learning algorithms help biotechnology researchers to perform big data analysis [cit] . but these technological requirements are expensive, time consuming and commonly out of reach of biologists and experts who use these data for clinical purposes. in this case, big data is useful to them only if they can do something with it in their everyday jobs. for many problems and questions, smart data itself is sufficient, as it creates and integrates small data \"packages\" and partitioning problems in a way that works across a wide range of people and organizations."
"example vi.3. fig. 9a shows a specification where the nodes follow policies that are not recommended by the gao-rexford guidelines [cit] . in particular, nodes 1 and 4 prefer peer routes equally to customer routes, while nodes 3, 4, and 7 announce to their providers or peers (nodes 2, 3 and 4 respectively) routes they have learned from a peer (4, 7, and 6 respectively). md(s), shown in fig. 9b, is acyclic, therefore s is safe."
proof: pd(s) and pd(r(s)) have exactly the same nodes. the transmission arcs of the two graphs are also the same. we examine whether two paths that are connected through a preference arc in pd(s) are also connected in pd(r(s)).
"automatic labeling of video footage according to genre is a common requirement in indexing large and heterogeneous collections of video material. this task may be addressed, either globally or locally. global-level approaches aim to classify videos into one of several main genres, for instance, cartoons, music, news, sports, documentaries or into more fine-grained sub-genres, for instance specific types of sports (football, hockey, etc.) and movies (drama, thriller, etc.). local-level approaches label video segments according to specific human-centered concepts such as outdoor vs."
"some databases such as dbsnp 5 provide schemas for explaining the structure of the data they store, but they are usually very complicated to understand. additionally, they are focused on satisfying the needs they were created for, at which time the option of interoperating with other repositories was not considered."
"our approach exploits both audio and visual modalities for genre classification. use has previously been made of these sources of information, but we approach computing these features in a novel way. the proposed audio features are block-level-based and have the advantage of capturing local temporal information by analyzing sequences of consecutive frames in a time-frequency representation. visual information is described using temporal information, color, and structural properties. temporal descriptors are derived using a classic confirmed approach, that is, analysis of the shot change frequency [cit] . however, we use a novel way of measuring action content that assesses action perception. color information is extracted globally. in contrast to existing approaches, which mainly use local or low-level descriptors such as predominant color, color variance, color entropy, and frame based histograms [cit], our approach analyzes color perception. using a color naming system, we quantify color perception with statistics of color distribution, elementary hues distribution, color properties (e.g., percentage of light colors, cold colors, saturated colors), and relationships between colors [cit] . the final type of visual descriptor is related to contour information, which has rarely been exploited in genre classification [cit] . unlike most existing approaches, which describe closed region shapes (e.g., with mpeg-7 visual descriptors [cit] ) we break contours down into segments and describe curve contour geometry both individually and relative to neighbor contours."
"conventional big data processing can be adapted to the genomic domain in order to solve most of the problems related to heterogeneity, data cleaning and data integration. but it leaves an important problem unsolved: the lack of an ontological commitment to define basic biological concepts; for instance, changes in the dna sequence have been traditionally named as \"mutations\", but this term has become increasingly problematic because usage by scientists is not uniform and has developed a negative connotation [cit] . substantial discrepancies in the meaning and use of key biological terms constitute an issue of concern, because they guide the understanding and processing of genomic data. if this issue remains open, the link between big data and smart data cannot be efficiently established."
"a data quality methodology can be defined as \"a set of guidelines and techniques that, starting from the input information concerning a given reality of interest, defines a rational process for using the information to measure and improve the quality of data of an organization through given phases and decision points\" [cit] . we propose to use a data quality methodology specifically for the genomic domain in order to (i) ensure veracity (selection of high quality repositories) and (ii) provide value (selection of high quality data from each repository)."
"where p and q are both probability distributions. for the block-level features, the manhattan distance was used. table 1 compares the classification accuracy (i.e., the number of sequences which were correctly labeled) obtained with a single gaussian model over mfcc (denoted sg) to the proposed blocklevel feature set (denoted blf ) for the two classifiers. for our experiments we used the data set that is introduced in section 4. the results clearly indicate the superiority of the block-level features over mfcc, as the improvement in classification ranges from 8% to 20%. [cit] evaluation campaign presented in section 4.3. table 4 shows that the video genre classification system that is based only on the proposed feature set achieved a mean average performance of 10.29%. interestingly, this approach not only outperformed an approach based on standard audio features (see team kit), but also some systems based on textual and visual modalities. thus, we can conclude that the proposed block-level features are more powerful in terms of music genre classification [cit] and also provide an adequate audio representation for video genre classification [cit] . in section 4 we investigate whether this approach in combination with visual descriptors can help to further improve the quality of video genre classification."
"the dq literature provides an extensive classification of data quality dimensions. however, there are discrepancies in the definition of most of them due to the contextual nature of quality. the most important classifications of quality dimensions are provided by wand and wang [cit], wang and strong [cit], redman [cit] and naumann [cit] . nevertheless, no general agreement exists on which set of dimensions defines the quality of data, or on the exact meaning of each dimension. thus, it is very important to compose a detailed description of the dimensions that best fit our data quality requirements."
"consequently, these repositories lack the holistic conceptual view required by a field as complex as genomics, leading to inconsistencies, redundancies, dispersion concerning data about a specific topic, different representations of the same concept and thus a high variability in their quality. the identification of novel disease-causing genes is highly dependent on our ability to gather and join all the relevant puzzle pieces together, reducing the noise as much as possible, which has become a challenge for biologists."
"the term \"smart data\" contrasts with the term \"big data\", which usually refers to a combination of structured and unstructured data that may be measured in petabytes or exabytes. smart data, in contrast, consists of usable datasets derived from big data repositories."
"in terms of visual information, we first extract temporal information by assessing action content and video transitions. this information is related to genre-specific cinematic principles."
"consequently, to provide a consistent training data set for classification, we extended the data set to up to 648 sequences. the final classification task was performed using a test set consisting of 1727 sequences (approximatively 350 hours of footage). after testing various machine learning techniques (we used the weka environment, see http://www.cs.waikato.ac.nz/ml/weka/) on the development data, the most accurate results were achieved again with a linear svm approach using all audio-visual descriptors. therefore, we used this approach for the final classification run."
"978-1-4673-2447-2/12/$31.00 [cit] ieee (a) the backup relationship requires coordination [cit] . the backup paths (21, 321, 4321) need to be propagated with a \"depreference\" community on each hop in order to ensure safety."
"using the previously determined histogram information in conjunction with the color naming dictionary, we define several color ratios. for instance, the light color ratio, p light, which reflects the percentage of bright colors in the movie, is computed by:"
"one of the pillars of pm is the genetic diagnosis which consists of the identification of potentially damaging variations in the dna of a patient (see figure 9 ). following this approach, a tool called varsearch [cit] has been developed, in order to point out genetic variations present in a patient's sample. the information about the variations presented in the sample are stored in variant call format 17 (vcf) files, a standard widely accepted by the biological community. the vcf files are processed by varsearch in order to determine which variations within the file are also among those stored in the hgdb. as a result, a personalized report is generated, indicating the risk of suffering the disease."
"(c) propagating peer routes to peers may become a routine policy in a flatter (less hierarchical) internet [cit] . node 3 depends less on its provider node 2, since it has the additional route 3456."
"theorem iv.2. for any specification s that can be modeled as a strict spp and has a single refinement r(s) (i.e., r(s) ≡ s), pd(s) has a cycle iff dd(s) has a cycle [cit] ."
"in the meantime, we must live with legacy bgp and the fact that safety is sacrificed (knowingly or unknowingly) in favor of full autonomy and unconstrained policy expressiveness. in this context, one possible approach to safety is to check router configurations for possible safety violations. there are two difficulties with this approach, both of which may be overcome to a limited extent. first, it has been shown that determining safety from configuration files is in the worst case an intractable problem [cit] . however, we feel it may be practically feasible to do this type of analysis when the number of isps involved is small (dozens, not thousands). second, isps consider their routing polices to be private. for this, we imagine that isps may be willing to share their configuration files with a trusted third party in return for information that facilitates debugging. this approach may provide isps with more control over the balance between safety and flexibility. we will present modifications to existing theoretical models of bgp that are specifically aimed at supporting this approach."
"proof: it follows from theorem v.3 that md(r(s)) is acyclic for any refinement r(s). from lemma iv.3 pd(r(s)) is also acyclic, and based on theorem iv.2 dd(r(s)) will have no cycle either. from lemma v.1, r(s) is safe."
"each participant was provided with a development set consisting of 247 sequences, unequally distributed across genres (some genre categories contained very few -even just one or two -examples). this initial set served as a reference point for the development of the proposed solution."
"assume that isps 1, 3, and 4 of fig. 9a share their configurations with a third party, while other isps do not. we select these nodes because they use non-traditional policies, as described in example vi.3. for this reason, they are more likely to be concerned about the safety of their policies than the other isps of fig. 9a . fig. 10 . md(s) constructed with partial information. it corresponds to the specification in fig. 9a when only nodes 1, 3, and 4 share their router configuration files. fig. 10 presents the md(s) that can be constructed with partial information. the notation p ab denotes the set of paths that node a learns from node b. similarly, p ab,ac denotes the set of paths node a learns from either b or c. nodes 2, 5, and 7 are isps whose policies are unknown. their paths are represented as p 2x (p 5x and p 7x respectively), where x stands for any of known or unknown neighbors through which they learn a route. all paths of the p 2x set are placed in the same equivalence class, a single node of md(s), since there is no information on 2's preferences. there exists an inflationary arc from p 2x to p 32, because the configuration of 3 permits paths announced from 2. similarly, there is an inflationary arc from p 41 to p 7x since node 4's policies announce to 7 any routes learned from 1 and it is not known whether 7 permits or denies this announcement. the equivalence classes and ranking arcs of the isps which share their configuration files are known. for instance, 4 ranks last any routes announced from node 1, while node 1 equally prefers all its available paths. although the equivalence classes are split based on the neighbor announcing a path in this example, the methodology is independent of the criterion on which paths are ranked."
"metrics m1, m2 and m3 help to identify errors that must be solved before the information is stored in the database and presented to the user. metrics m4, m5, m8 and m9 help to identify conflict in the information provided from different repositories that could affect the veracity of the information, for instance, there could be different interpretations of pathogenicity for the same variation. the rest of the metrics help to ensure that the information selected is sufficiently relevant for the task at hand."
"in order to accomplish the proposed task, the types of genomic data sources required are: databases of sequences (genes and chromosomes), databases with information about genotypephenotype relationships and databases that store scientific literature."
"a relatively recent discovery is that a minority of hivinfected individuals can produce broadly neutralizing antibodies (bnabs), that is, antibodies that inhibit infection by many strains of hiv [cit] . these have been shown to inhibit infection by a broad range of viral isolates in vitro but also protect non-human primates against infection [cit] . recent experimental results conducted in the nussenzweig lab at rockefeller university have demonstrated that the use of single antibody treatments can exert selective pressure on the virus, but escape mutants due to a single point mutation can emerge within a short period of time [cit] . although antibody monotherapy did not prove effective, it was shown that equal, high concentrations of an antibody pentamix effectively control hiv infection and suppress viral load to levels below detection. the goal of this example is to demonstrate how our proposed algorithm offers a principled way to design combination antibody therapies that control hiv infection and prevent evolution of any set of known resistant mutants. in a realistic setting, the ability to do this relies on the knowledge of what resistant viruses may be selected for with single therapies, and so this algorithm would be most effective in conjunction with single antibody selection experiments."
"in particular, through 1 and 2 regularization, we induce sparse structure in the feedback controller while bounding the magnitude of the feedback gains. this leads to a socp formulation of the combination therapy synthesis problem. the main contribution of this paper is a scalable algorithm for the systematic design of sparse, small gain feedback strategies to stabilize the evolutionary dynamics of a generic disease model."
"using the cshg as the conceptual core and the previously mentioned concepts of \"dimension\" and \"metric\", the proposed data quality methodology is divided into 5 phases: dimension description, metric description, variable selection, minimum dq requirements and dq assessment (see figure 5 ). the methodology must be based on a detailed description of the knowledge requirements to be achieved. in the next subsections, each phase of the methodology is going to be thoroughly explained."
"definition iii.4 (refinement). let s be a specification modeled as a strict or extended spp. specification r is a refinement of s, written as r(s), if each node u i in r has a strict order of path rankings which follows the rules:"
"the final set of parameters provides information based on structure, that is, on contours and their relations. so far, contour information has been exploited to a very limited extent in genre classification. for instance, some approaches use mpeg-7-inspired contour descriptors [cit], such as texture orientation histograms, edge direction histograms, edge direction coherence, [cit], which do not exploit real contour geometry and properties."
"as a huge amount of research has been done in big data processing, usually focused on volume, velocity and variety, we are going to focus on how we can reduce the noise and identify the most reliable data that are useful for clinical practice (veracity and value). we propose that the correct path to achieve this goal, is to add conceptual modeling techniques and data quality management to traditional big data processing, as can be shown in figure 2 . most databases gather information from different biological contexts such as epigenomics, proteomics or pharmacogenomics. the researchers may link datasets to combine information from multiple sources, in order to increase the richness of the information available to answer a research task. but the lack of consensus when defining basic biological concepts can be a huge problem when integrating information from different repositories. the use of a conceptual model (cm), provides the ontological basis to unambiguously define each biological concept, needed to identify the data in the repository, whatever the term is used to represent it. this helps to provide structure to the heterogeneous data managed, as well as making easier the access to an integrated dataset that can be used in daily work. the conceptual model can be used as a solid ontology representation to address issues of semantic integration between different datasets."
"proof: it follows from theorem v.5 that pd(r(s)) has a cycle, where r(s) is any refinement of s. from theorem iv.2, dd(r(s)) will also have a cycle."
"once the context is established, data sources suitable to provide the required data must be selected from among all the publicly available repositories. at this point, the previously defined data quality methodology is useful to determine those sources with the higher quality according to our dq requirements."
"example iii.1. the specification shown in fig. 2a is an example of an spp. paths 40 and 20 are more preferred than paths 4320 and 2140, respectively. paths 140 and 14320 are equally preferred by node 1, and since they have the same next-hop (node 4), the preference of node 1 satisfies the strictness property. similarly, node 3 satisfies strictness."
"in this section we present our proposal to provide a systematic methodological approach in order to answer this question for the genomic domain. it is the so called search-identificationload-exploitation (sile) method. its main goal is to systematize the search and identification of genomic information to be loaded, analyzed and exploited by a genomic information system (geis) based on the conceptual schema of the human genome (cshg). a summary of the activities taking place at each level of the method is defined in table 4 ."
"thus, each available color is projected in h e () onto its elementary hue, while saturation and intensity information are disregarded. this mechanism removes susceptibility to color fluctuations (e.g., illumination changes) and provides information about predominant hues."
"where t p, f p, and f n are the average numbers of correct detections (true positives), false detections (false positives) and non-detections (false negatives), respectively. averaging was performed over all repetitions for a given amount of training data."
"the most visible differences can be found in the audio descriptors, where measures such as logarithmic fluctuation pattern, spectral pattern, and delta spectral pattern discriminate well between all genres. in contrast, color-action and contour descriptors tend to emphasize the specificity of some genres. for instance, commercials and music clips have a high visual rhythm and action by the following classification experiments."
"previous approaches to configuration analysis [cit] have been based on the stable paths problem (spp) framework [cit] . however, the spp contains a \"strictness condition\" which essentially requires every step of the bgp decision procedure be captured -all the way down to the \"last gasp\" tie-break on router identifiers. the problem with this approach is that it requires too much information regarding the internal details of an isp (like igp distances, and the use of the troublesome med attribute) and it may depend on many tricky vendorspecific details of bgp policy implementation."
"iv. data structures given a strict or extended spp specification s, we can construct a dispute digraph dd(s) [cit], a paths digraph pd(s) [cit], or a multipath digraph md(s). we introduce md(s) in this paper, but we also use pd(s) in our analysis. the relations that pd(s) and md(s) have to dd(s) are important, because of the connection of dd(s) to the sufficient condition for safety in strict spp."
"one of the least common approaches (in the context of image processing) is the use of textbased information, mainly due to its limited availability with video information. text is retrieved either from scene text (e.g., graphic text, sub-titles), from the transcripts of dialogues obtained with speech recognition techniques, or from other external sources such as synopses and user tags. bagof-words model approaches [cit] are very common in genre classification. audio-based information is more widely available than text and derived from both time and frequency domains. common time-domain approaches use the root mean square of signal energy (rms) [cit], sub-band informa-tion [cit], zero-crossing rate (zcr) [cit], or silence ratio. frequency-domain features include energy distribution, frequency centroid [cit], bandwidth, pitch [cit], and mel-frequency cepstral coefficients (mfcc) [cit] ."
"we combine these regularization techniques with controller synthesis results for positive systems and present an iterative algorithm that yields a suboptimal l 1 controller. this formulation of the combination therapy problem allows the designer to explore explicit trade offs between closed loop performance, sparsity in controller structure and gain minimization."
"if there is cycle in md(s) that only involves paths which belong to the isps sharing configurations, then these isps know their policies may create routing anomalies. having this information, they can decide what is the economically and operationally best method to resolve such a cycle, if they choose to do so."
"using an extract-transform-load (etl) process, the relevant data related to the variations identified at the previous level are loaded into the hgdb. by using the application programming interface (api) provided by the selected data sources to access their content, a specific wrapper for each repository has been developed to extract the required data."
"several supervised strategies were used for classification. as the choice of training data may distort the accuracy of the results, we used a cross-validation approach: for each experiment we tested all possible combinations of training and test data. additionally, we varied the amount of training data (percentage split from 10% to 70%) and tested different combinations of descriptors."
"constructing the complete md(s) requires router configuration files from all isps. in our model, we have assumed that only a set of neighboring isps are sharing such information. therefore, there will be parts of md(s) which are unknown."
"the aim of this phase is to specifically determine the minimum levels of quality that the selected variables must fulfil according to the metrics specified in phase ii. concrete acceptance criteria must be assigned to each metric, e.g. the number of different submitters providing information about a gene-disease association must be at least two."
"in order to select clinically relevant genes and variations, a new set of data quality dimensions has been defined: accuracy, completeness, consistency, believability and relevancy. the metrics used to determine relevant variations are listed below."
"the proposal has been validated by populating the geis with relevant variations related to the risk of suffering the early onset type of alzheimer's disease (eoad), and made ready to be used by researchers in their clinical practice."
"color relationship. the final two parameters are related to the concept of perceptual relationships between colors in terms of adjacency and complementarity. the parameter, p adj reflects the number of perceptually similar colors in the movie (neighborhood pairs of colors on a perceptual color wheel, e.g., itten's color wheel [cit] ), and p compl reflects the number of perceptually opposite color pairs (antipodal)."
"to identify novel detoxin and rimosamide-related natural product bgcs, we selected all bgcs containing a taud homologue. the resulting 1175 gene clusters were then subjected to a combined big-scape/corason analysis. this revealed that the detoxin and rimosamide gcfs are part of a larger gene cluster clan related to peptide biosynthesis that also comprises unexplored families across the phylum actinobacteria (fig 5) . metabolomic data was available for 40 of the 152 strains identified to encode these bgcs. tandem mass spectrometry molecular networking analysis of these strains indicated the presence of three known detoxins, four known rimosamides, and 103 putatively novel detoxin and rimosamide analogues (fig. 3b), confirming the vast natural product chemical diversity suggested in the big-scape/corason data."
"if we included l sym in the reward function, it will change the return of the rollout when the θ is changed. instead, we include l sym in the objective function and calculate its gradient separately from that of the l p po, which depends on the policy gradient theorem."
"hence, mapping the evolutionary relationships between bgcs within and across gcfs is crucial for the discovery process. to this end, we introduce the core analysis of syntenic orthologues to prioritize natural products biosynthetic gene clusters (corason) software, written in perl and available as open source from https://github.com/nselem/corason. given a query gene inside a bgc of interest, the corason pipeline identifies other genomic loci that contain homologues of this gene and calculates a multi-locus phylogeny of all loci based on their conserved core (fig. 4) ."
"to verify that big-scape is able to group together bgcs that are known to be related, we constructed a chemical similarity network from all products of bgcs in mibig, and used this to derive a curated set of 376 compounds, which were manually classified into 92 groups (e.g. 14-membered macrolides, benzoquinone ansamycins, quinomycin antibiotics etc.) and 9 classes (e.g. polyketides, nrps, ripps etc.). we then used big-scape to group the corresponding bgcs into gcfs and observed good correspondence between manually curated families and those predicted by big-scape ( supplementary fig s5) ."
"a representative example of sampling-based method is cma-es, which iterates between evaluating a set of sampled parameters and improving the sampling distribution [cit] . this class of policy search methods is relatively robust to local minima and does not require gradient information. however, the sample number and memory requirement usually scales with the number of model parameters, making it less suitable to optimize models with many parameters such as deep neural networks. to combat this constraint on policy complexity, researchers resort to specialized controller design and a fine-tuned reward structure."
"similar to the standard policy gradient method, at each learning iteration, we generate rollouts from the current policy, use the rollout to estimate the gradients of the objective function of policy optimization, and update the policy parameters θ based on the gradient. with curriculum learning, we introduce a virtual assistant to provide assistive forces to the learner during rollout generation. the virtual assistant is updated at each learning iteration such that it provides assistive forces appropriate to the current skill level of the learner."
"the third detoxin clade that we targeted occurred almost entirely within the genus amycolatopsis and was named the 'amycolatopsis/p450 clade' (fig. 5, in blue) . this clade drew our interest as its bgcs contained a cytochrome p450 gene unique among the detoxins and rimosamides that was predicted to be responsible for novel hydroxylation patterns. though we did not have strains in our library with bgcs in the big-scape-defined clade, corason analysis allowed the selection of a strain with a very similar bgc also containing the desired p450 gene (fig. 5) . tandem ms analysis of this strain, amycolatopsis jejuensis nrrl b-24427, revealed detoxin isomers p 1 (4; figs. 5, s21) containing a tyrosine, p 2 (5; figs. 5, s26) featuring a hydroxylated valine, as well as detoxin p 3, a closely related analog free of hydroxylation (6; figs. 5, s31). as before, validation of amino acid assignments observed in ms 2 fragmentation data was achieved through several metabolic feeding experiments using stable isotope-labeled amino acids (figs. s22-s25, s27-s30, and s32). detailed structural analysis of 1-6, including detailed interpretation of tandem ms spectra and results from feeding studies using stable isotope-labeled amino acids, can be found in the supporting information."
results can be further processed in downstream analysis or immediately visualized using an interactive html-based interface that permits dynamic exploration of the gene cluster network (see further below for more details).
"policy gradient methods have demonstrated success in solving such a high-dimensional, continuous mdp. in this work, we use proximal policy optimization (ppo) ] to learn the optimal locomotion policy because it provides better data efficiency and learning performance than the alternative learning algorithms. our method can also be easily applied to other learning algorithms such as trust region policy optimization (trpo) [cit] ] or deep deterministic policy gradient (ddpg) [cit] ."
"in this work, we intentionally avoid the use of motion examples to investigate whether learning locomotion from biomechanics principles is a viable approach. in practice, it would be desirable to use our policy as a starting point and improve the motion quality by further training with motion examples or additional reward terms. for example, we took the network of the humanoid running policy to warm-start another policy learning session, in which the character learns to walk using a bigger stride length (figure 12 ). because the starting policy is already capable of balancing and running, the refinement learning takes only 200 iterations to train. our experiments show that both learner-centered and environmentcentered curricula are effective in training locomotion controllers. the learner-centered curriculum is designed to bootstrap learning from the character's existing skill, but the curriculum might end up taking a long and winding path to reach the origin of the curriculum space. on the other hand, the environment-centered curriculum takes a straight path to the origin, but it presents the character with predetermined lessons and ignores the character's current skill, which might result in an overly aggressive curriculum. however, our scheduling algorithm for the environment-centered curriculum ensures that the lessons in two consecutive learning iterations overlap, reducing the chance of the character being thrown into a completely unfamiliar environment and failing immediately. while the learner-centered curriculum requires less hyper parameter tuning, we prefer the environment-centered curriculum for its data efficiency. the main design choice of the environment-centered curriculum lies in the formula of reduction from x beдin to x end . our arbitrarily designed step function works well in all of our examples, but it may be possible to design a different reduction formula that can lead to an even more data-efficient learning algorithm."
"quadruped. quadrupeds exhibit a large variety of locomotion gaits, such as pacing, trotting, cantering, and galloping. we applied our approach to a quadruped model as shown in figure1(b) . the model has 13 links and 22 dofs, with a height of 1.15m and weight of 88.35kg. as quadrupeds can typically move faster than biped, we trained the quadruped to move at 2m/s and 7m/s. the results are shown in figure 5 . the trained policy results in a trotting gait for low target velocity consistently. for high target velocities, the character learns either trotting or galloping, depending on the initial random seed of the policy."
"given the current lesson x i, the goal of algorithm 2 is to find the next point in the curriculum space that is the closest to the origin while the current policy can still retain some level of proficiency."
"in addition to the parameters of the reward function and hyper parameters in the network, the style of the motion also depends on the kinematic and dynamic properties of the character. a comprehensive sensitivity analysis is beyond the scope of this work. however, we notice that the resulting motion is particularly sensitive to the range of joint torques that each actuator is allowed to generate. in this paper, the torque range is set heuristically based on the perceived strength of each joint. our preliminary results show that different locomotion gaits can result when different torque range settings. for example, we observed hopping behavior when we greatly increased the leg strength of the humanoid character."
"based on inspection of genetic features unique to detoxin/rimosamide bgc phylogenetic clades identified by corason (fig. 5, in red), there were three that captured our interest. the first was named the 'p450/enoyl clade' and contained two p450 genes and an enoyl-coa hydratase/isomerase within its bgcs. tandem mass spectrometry analysis of extracts from streptomyces sp. nrrl s-325, which contains a bgc within this clade, led to the discovery of detoxin s 1 (1; figs. 5, s15-16) with a heptanamide side chain, a unique feature among the detoxins and rimosamides whose installation likely depends on the enoyl-coa hydratase/isomerase. the second clade of interest, termed the 'supercluster clade' (fig. 5, in light green), comprised bgcs that encode detoxins in a 'supercluster' that included the known spectinomycin bgc. two detoxin-like bgcs within this clade in the genomic dataset, identified in the genomes of streptomyces sp. nrrl b-1347 and actinomycete sp. nrrl b-1348, were syntenic to a spectinomycin bgc (virtually identical to bgc0000715 in mibig) in a supercluster configuration (fig. 5) . interestingly, the mibig spectinomycin cluster was included on the detoxin/rimosamide corason tree just beside the big-scape-defined supercluster clade due to the presence of a taud gene at its periphery that is homologous with those from the detoxins. the taud gene is not known to be involved in spectinomycin biosynthesis, so we hypothesized that the strain from which the mibig entry was sourced, streptomyces spectabilis nrrl 2792, must have a detoxin bgc beside the spectinomycin cluster and would also produce a detoxin-like product. with this, we acquired the producing strain to determine if corason analysis was powerful enough to predict metabolite production solely based on the presence of a query gene but in the absence of full genomic data or a complete bgc. tandem mass spectrometry analysis of a s. spectabilis nrrl 2792 extract revealed production of six detoxin-like natural products, including detoxin n 1 (2; figs. 5, s17) and its acetoxylated analog, detoxin n 2 (3; figs. 5, s19). structural features unique to this novel detoxin subclass included the incorporation and formylation of tyrosine resulting in a formamido group. lc-ms analysis of cultures supplemented with stable isotope-labeled amino acids corroborated these structural predictions based on analysis of the bgc and tandem ms data (figs. s18, s20)."
"where b is the number of simulation samples per iteration. we use 20, 000 samples in all of our examples. since equation 6 is differentiable with respect to the policy parameters θ, it can be combined with the standard reinforcement learning objective and optimized using any gradient-based rl algorithm. incorporating the mirror symmetry loss, the final optimization problem for learning the locomotion policy can be defined as:"
"there are many different ways to design the virtual assistant, and some are more effective than others. for example, we tried to provide lateral balance using two walls, one on each side of the character. the initial lesson presented a narrow passage between the two walls so that the character could not possibly fall. as the curriculum proceeded, we then widened the gap between the walls. the results showed that the character learned to intentionally lean on the wall to decrease the penalty of falling. as the gap widened, the character leaned even more until the character fell because the walls were too far apart. we have also attempted to provide propelling assistance through the use of a virtual treadmill. this experiment was unsuccessful even when learning the initial lesson x 0 . our speculation is that the treadmill does not provide the necessary assistance for learning how to move forward; the learner still needs to put a significant amount of effort towards matching the speed of moving ground. in other words, it is arguable that learning with assistance (x 0 ) is just as difficult to learn as without assistance."
"our approach is also inspired by works in curriculum learning (cl). the general idea behind cl is to present the training data to the learning algorithm in an order of increasing complexity [cit] ]. researchers in machine learning have shown that cl can improve performance and efficiency of learning problems such as question-answering [cit], classification [cit] ] and game playing [cit] ."
"imaging a person who is standing in front of a floor mirror with her left hand behind her back. if she uses the right hand to reach for her hat, what we see in the mirror is a person with her right hand behind her back who is reaching for a hat with her left hand. indeed, if the character has a symmetric morphology, the action it takes in some pose during locomotion should be the mirrored version of the action taken when the character is in the mirrored pose. this property can be expressed as:"
"we compare the learner-centered and environment-centered curriculum learning algorithms on the simplified biped model. as demonstrated in the supplementary video 1, both methods can successfully train the character to walk and run at target velocities with symmetric gaits. we further analyze the performance of the two algorithms by comparing how they progress in the curriculum space, as shown in figure 7 . we measure the progress of the curriculum learning with the l2 norm of the curriculum parameter x, since the goal is to reach 0 as fast as possible. we can see that environment-centered curriculum learning shows superior data-efficiency by generating a successful policy with about half the data that is required for the learner-centered curriculum learning."
"microbial specialized metabolites are key mediators of interspecies communication and competition in the environment and in the context of host microbiomes 1, 2 . their diverse chemical structures have been critical in the development of antibiotics, anticancer drugs, crop protection agents and ingredients for manufacturing. while tens of thousands of natural products have been discovered in past decades, recent evidence suggests that these represent a fraction of the potential natural product chemical space yet to be discovered [cit] ."
"we use pydart [cit] ], a python binding of the dart library [cit] to perform multi-body simulation. we simulate the characters at 500 hz, and query the control policy every 15 simulation steps, yielding a control frequency of 33 hz. we use the ppo algorithm implemented in the openai baselines library for training the control policies. the control policies used in all examples are represented by feed-forward neural networks with three fully-connected hidden layers, and each hidden layer consists of 64 units. we fix the sample number to be 20, 000 steps per iteration for all examples. the number of iteration required to obtain a successful locomotion controller depends on the complexity of the task, ranging from 500 to 1500, yielding a total sample number between 10 and 30 millions."
the goal of curriculum scheduling is to systematically and gradually reduce the assistance from the initial lesson x 0 and ultimately achieve the final lesson in which the assistive force is completely removed. designing such a schedule is challenging because an aggressive curriculum that reduces the assistive forces too quickly can fail the learning objectives while a conservative curriculum can lead to inefficient learning.
"to showcase the power of our workflow for the analysis of large bgc collections and highresolution mapping of gcf biosynthetic diversity, we focused on the detoxin and rimosamide gcfs 38 . our comprehensive analysis of the selected actinobacterial genomes revealed these bgcs to be taxonomically widespread and architecturally diverse (fig. 5) . the conserved gene core of detoxin and rimosamide bgcs is composed of three genes: one nrps, one nrps/pks hybrid, and a homologue of taud, presumably recruited from the tauabcd operon in escherichia coli 39 . as suggested by evomining analysis."
"designing a reward function is one of the most important tasks in solving a mdp. in this work, we use a generic reward function for locomotion that has three objectives: move forward, balance, and use minimal actuation."
"humanoid. finally, we trained a locomotion policy for a full humanoid character with a detailed upper body. the character has 13 links and 29 dofs with 1.75m in height and 76.6kg in weight. we trained the humanoid model to walk at 1.5m/s and run at 5m/s. in addition, we trained the model to walk backward at −1.5m/s. we kept the same reward function parameters between forward and backward walking. results of the humanoid locomotion can be seen in figure 4 . during walking forward and backward, the character learns to mostly relax its arms without much swinging motion. for running, the character learn to actively swing its arms in order to counteract the angular momentum generated by the leg movements, which stabilizes the torso movements during running."
"symmetry is another important characteristic of a healthy gait. assessing gait symmetry usually requires at least an observation of a full gait cycle. this requirement poses a challenge to policy learning because the reward cannot be calculated before the end of the gait cycle, leading to a delayed reward function. we propose a new way to encourage gait symmetry by measuring the symmetry of actions instead of states, avoiding the potential issue of delayed reward."
"we propose two approaches to the problem of curriculum scheduling ( figure 2) : learner-centered curriculum and environmentcentered curriculum. the learner-centered curriculum allows the learner to decide the next lesson in the curriculum space, resulting in a piece-wise linear path from x 0 to the origin. the environmentcentered curriculum, on the other hand, follows a series of predefined lessons. however, instead of focusing on one lesson at a time, it exposes the learner to a range of lessons in one curriculum learning iteration, resulting in a set of co-linear, overlapping line segments from x 0 to the origin of the curriculum space."
"hexapod. we designed a hexapod creature that has 13 links and 24 dofs, inspired by the body plan of an insect. we trained the hexapod model to move at 2m/s and 4m/s. as shown in figure 6, the hexapod learns to use all six legs to move forward at low velocity, while it lifts the front legs and use the middle and hind legs to 'run' forward at higher velocity."
"while most existing methods in motor skill learning penalize the use of energy in the reward function, the weighting of the penalty is usually relatively low for fear of negatively impacting the learning of the main task (e.g. maintaining balance). as a result, the energy term serves merely as a regulator and has negligible effect on preventing the agent from using excessive joint torques. we introduce a new curriculum learning method that allows for a high energy penalty while still being able to learn successfully using the existing policy learning algorithms. the curriculum provides modulated physical assistance appropriate to the current skill level of the learner, ensuring continuous progress toward successful locomotion with low energy consumption. our algorithm automatically computes the assistive forces to help the character with lateral balance and forward movement. the curriculum gradually relaxes the assistance, so that eventually the character learns to move entirely without help."
"our evaluation shows that the agent can indeed learn locomotion that exhibits symmetry and speed-appropriate gait patterns and consumes relatively low-energy, without the need of motion examples, contact planning, and additional morphology-specific terms in the reward function. we further show that the same reward function with minimal change of weighting and the learning methodology can be applied to a variety of morphologies, such as bipeds, quadrupeds, or hexapods. we test our method against three baselines: learning without the mirror symmetry loss, learning without the curriculum, and learning without either component. the comparisons show that, without the curriculum learning, the trained policies fail to move forward or/and maintain balance. on the other hand, without the mirror symmetry loss, the learning process takes significantly more trials and results in asymmetric locomotion."
"learning locomotion directly from the principles of minimal energy and gait symmetry is difficult without additional guidance from motion examples or reward function shaping. indeed, a successful locomotion policy must learn a variety of tasks often with contradictory goals, such as maintaining balance while propelling the body forward, or accelerating the body while conserving energy. one approach to learning such a complex motor skill is to design a curriculum that exposes the learner to a series of tasks with increasing difficulty, eventually leading to the original task."
"although our characters demonstrate more natural locomotion gaits comparing to existing work in drl, the quality of the motion is still not on a par with previous work in computer animation that exploits real-world data. further investigation on how to incorporate motion capture data, biological-based modeling, and policy refinement (see figure 12 ) is needed. in addition, our work is only evaluated on terrestrial locomotion with characters represented by articulated rigid bodies. one possible future direction is to apply the curriculum learning to other types of locomotion, such as swimming, flying, or soft-body locomotion."
"on the other hand, gradient-based methods naturally fit into the stochastic gradient descent framework, an algorithm that has been successfully demonstrated to train deep neural networks with hun-"
"to map and prioritize this complex biosynthetic diversity, several groups have devised methods to compare architectural relationships between bgcs in sequence similarity networks and group them into gene cluster families (gcfs), each of which contains bgcs across a range of organisms that are linked to a highly similar natural product chemotype 4,28,29, . the presence or expression of such gcfs can be correlated to molecular families (mfs) identified from mass spectrometry data to match genes to their product molecules in a process termed metabologenomics 4, 30, 31 . however, current methods fail to correctly measure similarity between complete and fragmented gene clusters (which frequently occur in metagenomes and large-scale pan-genome sequencing projects based on short-read technologies); do not consider the complex and multi-layered evolutionary relationships within and between gcfs; require lengthy cpu-time on supercomputers when processing large datasets; and lack a user-friendly implementation that interacts directly with other key resources. these shortcomings preclude adoption by the broader scientific community and impede significant advances in natural product discovery."
"to demonstrate the effect of curriculum learning and mirror symmetry loss, we compare our method with environment-centered curriculum learning and mirror symmetry loss (ecl + msl) to three baseline methods: with environment-based curriculum learning only (ecl), with mirror symmetry loss only (msl) and using vanilla ppo (ppo) with no mirror symmetry loss nor curriculum learning. the baseline methods are trained on the simplified biped character and the humanoid character for both walking and running. the learning curves for all the tests can be seen in figure 8 . in all four of these tasks, our approach learns faster than all of the baseline methods. without curriculum learning (i.e. blue and cyan curves), the algorithm typically learns to either fall slowly or stand still (as is shown in the supplementary video). on the other hand, without mirror symmetry loss, the resulting policy usually exhibits asymmetric gaits and the training process is notably slower, which is mostly evident in the running tasks, as shown in figure 9 and figure 10 . in addition to the three baseline methods described above, we also trained on the simplified biped walking task using vanilla ppo with a modified reward function, where w e is reduced to 0.1. this allows the character to use higher torques with little penalty (\"ppo high torque\" in table 2 ). while the character is able to walk forward without falling, the motion appears jerky and uses significantly more torque than our results (see supplementary video)."
"where x l and x r are the average of joint torques produced by the left and right leg respectively. the smaller the value si is, the more symmetric the gait is. the results can be seen in table 2 . as expected, policies trained with our method uses less joint torque and produces more symmetric gaits."
"corason is available for users as a downloadable and easy-to-install software that allows tracing the evolutionary history of biosynthetic genes using customizable databases. any subset of genes from a bgc may be selected to identify other genomic contexts across a set of prokaryotic genomes in which homologues of these genes are found. the selected query genes are then visualized on a multi-locus approximate-maximum-likelihood phylogenetic tree 46 that allows the user to identify all functional genomic contexts in which the corresponding gene families are found. in this way, the evolutionary relationships of each gene within a bgc across large numbers of genomes can be comprehensively analyzed. a version of the corason algorithm, called 'familymode', was also integrated with big-scape, allowing to generate a multi-locus phylogeny of all bgcs within each gcf using the sequences of their common domain core."
"arguably, the greatest value of big-scape lies in the practical utility of the predicted gcfs for discovery applications. hence, we assessed the accuracy of correlations of big-scape-predicted gcfs to ms ions from known natural product through metabologenomics 31 . first, we performed a big-scape analysis of 74,652 bgcs from 3,080 actinobacterial genomes (see methods), including 1,393 reference bgcs from mibig 20 . big-scape grouped these bgcs into a total number of 17,718 gcfs and 801 gccs using default parameters. extracts from 363 actinomycete strains were analyzed using untargeted high-resolution lc-ms/ms 4 . the gcf annotations for these 363 strains from two big-scape modes (global and glocal) at two similarity cutoffs (0.30 and 0.50) were used to generate four rounds of metabologenomic correlations utilizing a binary scoring metric as described previously 4, 30 . big-scape's gene cluster family annotations were then assessed against ion production patterns via two methods. first, a 'golden dataset' of nine known ion signals and their characterized gene clusters were manually tracked across the four correlation rounds. these ion signals corresponded to the following natural products; ce-108, benarthin, desertomycin, tambromycin, enterocin, tyrobetaine, chlortetracycline, rimosamide, and oxytetracycline. second, a target-decoy approach was applied to estimate the false discovery rate (fdr) for each round to provide an overview of correlative power for the unknown ion to gene cluster family hypotheses generated (see fig. s14 ). decoy databases used for the target-decoy"
"we have demonstrated a reinforcement learning approach for creating low-energy, symmetric, and speed-appropriate locomotion gaits. one element of this approach is to provide virtual assistance to help the character learn to balance and to reach a target speed. the second element is to encourage symmetric behavior through the use of an additional loss term. when used together, these two techniques provide a method of automatically creating locomotion controllers for arbitrary character body plans. we tested our method on the lower half of a biped, a full humanoid, a quadruped, an a hexapod and demonstrated learning of locomotion gaits that resemble the natural motions of humans and animals, without prior knowledge about the motion. because our method generalizes to other body plans, an animator can create locomotion controllers for characters and creatures for which there is no existing motion data."
"the comprehensive computational workflow introduced here enables effective exploration of biosynthetic diversity across large strain collections, pan-genomes of entire bacterial or fungal genera, and metagenomic datasets with thousands of metagenome-assembled genomes. the combined big-scape/corason platform overcomes key computational bottlenecks inherent in previous approaches, as it facilitates building gcfs with both partial and complete bgcs, accounts for class-specific differences between bgcs, incorporates sequence identify information within limited compute time, charts out evolutionary relationships between and within gcfs, and provides an interactive user interface to explore the outputs. hence, we anticipate that it will soon facilitate metabologenomic correlation studies to systematically assign many gene clusters to many molecules at unprecedented scales. [cit] 41, will constitute a key technology to facilitate fundamental studies on the evolutionary origins of natural product chemical innovations. for example, it provides a stepping-stone to perform detailed analyses of how gene cluster architectures have evolved (and are evolving) from their constituent independent enzymes and sub-clusters. a logical next step will be the unified classification of the millions of bgcs within publicly available genome sequences, and a pfam-like database for the assignment of biosynthetic gene cluster families to known and unknown areas of natural product chemical diversity."
"one benefit of encouraging symmetric actions rather than symmetric states is that it allows the motion to appear asymmetric when desired. as shown in figure 11, we trained a humanoid that is walking while holding a heavy object (10kg) in the right hand. the character uses an asymmetric gait that moves more vigorously on the left side to compensate for the heavy object on the right side. if we chose to enforce symmetry on the states directly, the character would likely use a large amount of torque on the right side to make the poses appear symmetric."
"big-scape and corason connect seamlessly with antismash and mibig, as genbank outputs of antismash can be used directly as inputs for the workflow, and mibig reference data can be included in the analysis automatically. although calculations on hundreds or thousands of genomes are too compute-intensive to provide them on a free public web server, we still wanted to make the results available in an interactive user-friendly html visualization that enables efficient exploration of biosynthetic diversity across large datasets for non-programmers. hence, we constructed a powerful javascript-based visualization that provides an interactive output for every big-scape run, which can be viewed offline on any web browser. in a single view, the visualization displays bgc nodes colored by gcf in interactive sequence similarity networks, side-by-side with arrow visualizations of the gene clusters, which contain gene annotation and pfam domain details that appear on mouse-over. networks can be searched by compound names of mibig reference clusters, pfam domains of interest, or species names, with resulting match nodes instantly highlighted within the network. each gcf is given its own view panel, which shows the corason-based multi-locus phylogeny of the underlying bgcs and includes links to related families within the same gcf. finally, an overview page is provided that displays statistics on the bgcs identified, as well as a gcf absence/presence heatmap of the most frequently occurring gene clusters within the input dataset."
"one of the most encouraging results of this work is the emergence of different gaits for different target velocities. most previous work obtains different gait patterns through contact planning or constraint enforcement [cit] . in contrast, with different target velocityv, our characters learn speedappropriate gaits using nearly identical reward functions (table 1), without additional engineering effort."
"our results show how big-scape can effectively identify sets of related bgcs across large numbers of genome sequences. moreover, using corason to systematically map bgc evolutionary diversity and assemble gene cluster phylogenies proved to be a powerful approach for the discovery of novel clades of bgcs that are responsible for the biosynthesis of uncharted natural product chemistry. when focused toward the specific detoxin/rimosamide discovery effort in \"query mode,\" corason guided the discovery of six new detoxins by way of taud phylogeny. thus, corason enables rapid mapping and visualization of bgc relationships for more effective genome mining from large genomic libraries. additionally, variation in bgc domain architecture corresponded to variation in chemical structure -presence of an enoyl-coa hydratase/isomerase corresponded to the fatty acid amide detoxin s 1 and presence of a unique p450 gene corresponded to evidence of hydroxylations in detoxins p 1 -p 3 . some highly similar bgcs and those that were the most remote from the big-scape-defined clades were removed from the original tree for readability. the representative structures for each clade illustrate the correspondence between molecular and genomic variations. the bgc tree is rooted with a taud encoded in an unrelated (non-nrps) bgc from streptomyces sp. nc1. the genes encoding the nrps, the nrps/pks hybrid, and the taud homologue are highly conserved among detoxin and rimosamide bgcs. highlighted sections on the tree indicate big-scape-defined clades. bolded strain/bgc names are those investigated in this study with dotted lines indicating those which were just outside the big-scape-defined clades. clusters in the 'p450/enoyl clade' contain a pair of p450 genes and an enoyl-coa hydratase/isomerase which are putatively involved in biosynthesis of alkylated detoxins, the 'supercluster clade' is comprised of detoxin clusters and is immediately adjacent a spectinomycin bgc, and the 'amycolatopsis/p450 clade' possess a unique p450 enzyme that corresponds to the first examples of hydroxylation in the detoxin/rimosamide class. strain names are followed by their genbank accession number when available. genes not found in the reference cluster are colored based on blast analysis."
"both lateral balancing force and propelling force are produced by a virtual proportional-derivative (pd) controller placed at the pelvis of the learner, as if an invisible spring is attached to the learner to provide support during locomotion. specifically, the pd controller controls the lateral position and the forward velocity of the learner. the target lateral position is set to 0 for maintaining lateral balance while the target forward velocity is set to the desired velocityv for assisting the learner moving forward."
"simplified biped. bipedal locomotion has been extensively studied in the literature, and it is a familiar form of locomotion to everyone. thus we start with training a simplified biped character to perform walking and running. the character has 9 links and 21 dofs, with 1.65m in height and weighs in total 50kg. the results can be seen in figure 3 . as expected, when trained with a low target velocity (1m/s), the character exhibits a walking gait. when trained with a high target velocity (5m/s), the character uses a running gait indicated by the emergence of a flight phase."
"creating an animated character that can walk is a fascinating challenge for graphics researchers and animators. knowledge from biomechanics, physics, robotics, and animation give us ideas for how to coordinate the virtual muscles of a character's body to move it forward while maintaining balance and style. whether physical or digital, the creators of these characters apply physics principles, borrow ideas from domain experts, use motion data, and undertake arduous trial-and-error to craft lifelike movements that mimic real-world animals and humans. while these characters can be engineered to exhibit locomotion behaviors, a more intriguing question is whether the characters can learn locomotion behaviors on their own, the way a human toddler can."
"an evomining 8 analysis of the taud dioxygenase protein family showed specific specialized metabolism-related expansions of paralogues across genera such as streptomyces, rhodococcus, frankia and amycolatopsis (fig s13) . within this expansion, one clade was shown to contain fifteen taud homologues that are part of experimentally characterized bgcs from mibig, including the detoxin and rimosamides bgcs (table s6 )."
"our virtual assistant provides assistive forces to simplify the two main tasks of locomotion: moving forward and maintaining lateral balance. the lateral balancing force is applied along the frontal axis (left-right) of the learner, preventing it from falling sideway. the propelling force is applied along the sagittal axis, pushing the learner forward to reach the desired velocity. with these two assistive forces, the learner can focus on learning to balance in the sagittal plane as well as keeping the energy consumption low."
"the recent disruptive development in deep reinforcement learning (drl) suggests that the answer is yes. researchers indeed showed that artificial agents can learn some form of locomotion using advanced policy learning methods with a large amount of computation. even though such agents are able to move from point a to point b without falling, the resulting motion usually exhibits jerky, high-frequency movements. the motion artifacts can be mitigated by introducing motion examples or special objectives in the reward function, but these remedies are somewhat unsatisfying as they sacrifice generality of locomotion principles and return partway to heavily engineered solutions. this paper takes a minimalist approach to the problem of learning locomotion. our hypothesis is that natural locomotion will emerge from simple and well-known principles in biomechanics, without the need of motion examples or morphology-specific considerations. if the agent can successfully learn in this minimal setting, we further hypothesize that our learning method can be generalized to agents with different morphologies and kinematic properties. our approach is inspired by the recent work that applies reinforcement learning to train control policies represented as neural networks, but aims to address two common problems apparent in the motions produced by the existing methods. first, we observe that the motions appear much more energetic than biological systems. second, an agent with perfectly symmetrical morphology often produces visibly asymmetrical motion, contradicting the observation in biomechanics literature that gaits are statistically symmetrical [cit] . therefore, we propose a new policy learning method that minimizes energy consumption and encourages gait symmetry to improve the naturalness of locomotion."
"genome mining has emerged in the past decade as a key technology to explore and exploit natural product diversity. key to this success is the fact that genes encoding natural product biosynthetic pathways are usually clustered together on the chromosome. these biosynthetic gene clusters (bgcs) can be readily identified in a genome. moreover, in many cases, the chemical structures of their products can be predicted to a certain extent, based on the analysis and biosynthetic logic of the enzymes encoded in a bgc and their similarity to known counterparts 9 ."
"biosynthetic potential of broad taxonomic groups of organisms, as well as entire ecosystems. these large-scale analyses easily lead to the identification of thousands of bgcs with varying degrees of mutual similarity, ranging from widely distributed homologs of gene clusters for the production of well-known molecules to rare or unique gene clusters that encode unknown enzymes and pathways."
"in addition to energy consumption, gait symmetry offers both stable and adaptive locomotion that reduces the risk of falling [cit] ]. the symmetry of walking trajectories can be measured by established metric used in clinical settings [cit] ]. however, directly using this metric in the reward function leads to two complications for policy gradient methods. first, the requirement of evaluating an entire trajectory introduces a delayed and sparse reward, resulting in a much harder learning problem. second, the policy, especially at the beginning of learning, might not be able to produce structured, cyclic motion, rendering the symmetry metric ineffective in rewarding the desired behaviors. our solution departs from the conventional metrics that measure the symmetry of the states. instead, we measure the symmetry of actions produced by the policy. we propose a mirror symmetry loss in the objective function to penalize the paired limbs for learning different control strategies."
"to illustrate big-scape/corason usage, we provide an example output of a run with antismash-predicted bgcs from 103 complete streptomyces genomes, including as outgroups the genomes of catenulispora acidiphila and salinispora arenicola: http://bioinformatics.nl/~xnava009/streptomyces_out/. to connect the absence/presence map of gcfs across these genomes to species phylogeny, a high-resolution multilocus whole-genome phylogeny (fig.s11) was inferred from the streptomyces conserved-core (online data: streptomycescore .), and the gcf absence/presence patterns were plotted onto the structure of the tree (fig. s12) . as has been observed before in other genera like salinispora 29, this shows high conservation of select gcfs across larger numbers of genomes, combined with large numbers of rare gcfs that are specific to one or a few genomes."
"genetic diversity of bgcs within gcfs is directly related to structural differences between their molecular products, and even small chemical variations can lead to different biological activities 37 ."
"two questions remain in our locomotion curriculum learning algorithm. first, what is the most compact set of parameters for the virtual assistant such that locomotion skills can be effectively learned through curriculum? second, what is the appropriate curriculum schedule, i.e. how much assistive force should we give to the learner at each moment of learning?"
"where ψ a (·) and ψ o (·) maps actions and states to their mirrored versions respectively. we overload the notation π θ to represent the mean action of the stochastic policy. enforcing equation 5 as a hard constraint is difficult for standard policy gradient algorithms, but we can formulate a soft constraint and include it in the objective function for policy optimization:"
"ecole polytechnique fédérale de lausanne i. experimental results figure 2 represents the example reconstructed images from kenya dataset using greedy, coherence based, and single-image masks at 25% sampling rate. ground truth fig. 2 . masks obtained and example reconstructions using coherence based, single-image and greedy mask at 25% sampling rate. the quality of reconstruction is measured using psnr. in the last row, we present ground truth images."
"the rimosamide bgc differs from those of the detoxins by having an additional nrps, which encodes for further elaboration of the common detoxin/rimosamide core scaffold with isobutyrate and glycine 38 . aside from the more common nrps and pks genes, the fact that the taud gene was present across all members of this family caught our attention. the product of the taud gene belongs to the fe(ii)/α-ketoglutarate-dependent hydroxylase enzyme superfamily and is named for the commonly encoded α-ketoglutarate-dependent taurine dioxygenase involved in assimilation of sulphite by oxygenolytic release from the amino acid taurine 40 . interestingly, this family also includes enzymes across fungi, bacteria and plants that catalyze hydroxylations, desaturations, ring expansions, and ring formations, among other transformations. to date, the role of taud in detoxin and rimosamide biosynthesis is unknown, but it has been suggested to be responsible for the proline oxidation observed in some analogs 38 ."
"where the reference r(k+1) is replaced by y(k+1). generally, it is difficult to find the analytical inverse function f -1 (.). therefore, the method exploited here makes use of the identified fuzzy ts of the process under investigation for providing the particular state x(k) at each time step k. from this mapping, the inverse mapping (5) is easily identified as a model in the form of (3), provided the controlled system is stable."
"the first control scheme proposed for comparison purposes relies on uios [cit], which were used for estimating measurements used by the control system. the uio-based scheme is chosen since it enables the possibility to include robustness towards the uncertainty of the wind speed, which is difficult to measure."
"t are the control inputs and the monitored output measurements, respectively. f c (.) represents the continuous-time nonlinear function that describes the complete behaviour of the wind turbine process. regarding the input and output signals,  g (t) is the generator speed measurement, p g (t) the generator power measurement, and  g (t) generator torque measurement. finally, the model parameters, and the map c p ( r, ) are chosen in order to represent a realistic turbine, which is used as benchmark system in this study [cit] ."
"the nomadicbts experimental prototype cell setup is shown in figure 2 . at this stage of the study, the prototype cell was not linked with the traditional gsm network. the usrp b200 adopted for the sdr hardware front-end covers 50 mhz to 6 ghz frequency range, which falls within the microwave portion of the rf spectrum as shown in figure 3 . apparently, the usrp b200 can adequately operate within the gsm 900 and gsm 1800 bands. the device provides interfaces for two dualband vert900 antennas with 824-960 mhz and 1710 [cit] mhz frequency bands. the technical specifications of the device as related to this study are presented in table 1 . the device can stream up to 56 mhz of instantaneous data bandwidth over a high-speed usb 3.0 bus operating at 4.8 gbps full duplex to the host pc. the rf/if signal processing tasks on the device are carried out by the ad9364 radio frequency integrated circuit (rfic), which is a direct conversion transceiver along with the spartan 6 fgpa. the host pc shown in figure 2"
"also the stability properties of the overall control strategies were checked by means of a monte-carlo campaign based on the wind turbine benchmark. in fact, as pointed out above, the monte-carlo analysis represents the only method for estimating the efficacy of the developed control schemes when applied to the monitored process."
"t and the current input u(k). the output is a prediction of the system's output at the next sample y(k+1). the objective of the control algorithm is to compute the control input u(k), such that the system output at the next sampling instant is equal to the desired (reference) output r(k+1). in principle, this can be achieved by inverting the model of the process. given the current state x(k) and the reference r(t+1), the control input is given by:"
"because wind turbine control is often achieved using two distinct control loops for the working regions between partial load and full load conditions, the transition between these regions can be problematic. for some turbines, the maximum structural damage occurs due to extreme and fatigue loads during this transition. often, the act of switching between these region controllers contributes to the problem."
"the signal strength obtained at an approximate distance of 30 cm from one of the test mobile phones (on which the network signal info pro app was installed) to the prototype cell in this study is shown in figure 10 . the signal strength is −57 dbm with 28 asu. based on the description of asu in table 2, this is a perfect signal strength. other relevant descriptions of the prototype cell are shown in figure 10 ."
"in this section, further experimental results were reported. they regard the performance evaluation of the developed control schemes with respect to modelling errors and measurement uncertainty. in particular, the simulation of different data sequences was performed by exploiting the wind turbine benchmark simulator, and a matlab ® monte-carlo analysis. in fact, the monte-carlo tool is useful at this stage as the control strategy performances depend on the error magnitude due to the model approximation and uncertainty, as well as on input-output measurement errors."
"sdr is a modern radio engineering approach in which components that were hitherto implemented in hardware are now implemented using software on an embedded platform such as field programmable gate array (fpga) or personal computer (pc) [cit] . with sdr, it is now possible to have malleable reconfiguration of wireless systems with attributes such as efficient spectrum access, rapid development, inexpensive implementation, easy upgradeability, high flexibility, enhanced radio frequency (rf) signal analysis and inter-operability among heterogeneous wireless standards [cit] . modern hardware and software platforms have been developed for rapid prototyping based on the sdr architecture. a leading example of such platforms for open-source software development is the gnu radio [cit] . different versions of universal software radio peripheral (usrp) are major hardware platforms for sdr [cit] . with the combination of these evolving technologies, complete sdr implementations of different wireless standards from 2g to 5g can be achieved. however in the past, it was mandatory to carry out hardware design of separate custom-built radios for different standards, which could be large and expensive. nowadays, a single versatile sdr hardware can be modified to carry out multiple functions depending on the code it is running at a comparatively low cost."
"the sdr software back-end comprises of the soft base station subsystem (softbss) and the voice over internet protocol private automatic branch exchange (voip pabx) software running on a single-board embedded computer or pc with an operating system (os). the softbss provides the necessary interconnection between the sdr front-end hardware and the voip pabx. softbss implements a software transceiver which performs functions such as frequency tuning, gaussian minimum shift keying (gmsk) modulation and demodulation, clock synchronization, control command transaction as well as transmission of receive and transmit bursts. it also implements the session initiation protocol (sip) mapping functions in order to establish sip connections for processing by the voip pabx. for instance, the international mobile subscriber identity (imsi) stored on the subscriber identity module (sim) card of a mobile station (ms), which is the end-user phone, is presented to the voip pabx as an sip client. the location of the ms is mapped to sip registration, call connection is mapped to sip transactions, while short message services (sms) function is realized through instant messaging extension to sip [cit] . the softbss, as shown in figure 1, also contains a graphical user interface (gui) to provide access for configuring the nomadicbts by technical administrator in a user-friendly manner. as a proof of concept, the softbss was realized, in this present study, using a combination of open-source technologies such as ubuntu linux, openbts and gnu radio [cit] ."
"the architecture of nomadicbts, which is an sdr-based cellular system and the prototype cell developed based on the architecture have been presented in this paper. we have been able to demonstrate the possibilities of evolving a cost effective cellular communication system with the results and the open-source technologies adopted for this work. different tests were carried out to and satisfactory results were obtained to justify the choice of usrp b200 for implementing the hardware front-end of the nomadicbts. in the future, we hope to incorporate other features such as voice and data communication, interconnection among multiple nomadicbts, connection with existing gsm networks, surveys for path loss propagation modelling and implementation of the architecture for 3g/45/5g mobile technologies."
"the three-blade horizontal axis turbine considered in this paper works according to the principle that the wind is acting on the blades, and thereby moving the rotor shaft. in order to up-scale the rotational speed to the needed one at the generator, a gearbox is introduced."
"note however that, with the fuzzy control strategy proposed here, disturbances acting on the process, measurement noise and model-plant mismatch can cause differences in the behaviour of the process and of the model. a mechanism to compensate this error can be exploited e.g. via on-line adaptation of the process model. on-line adaptation can be applied to cope with the mismatch between the plant and the fuzzy model. in many cases, a mismatch occurs as a consequence of (temporary) changes of process parameters. therefore, section iv motivates the adaptive strategy based on linear models, whose parameters are adapted on-line and exploited for the controller parameter estimation."
"in particular, the nonlinear wind turbine simulator originally developed in the simulink ® environment [cit] was modified by one of the authors in order to vary the statistical properties of the signals used for modelling possible process parameter uncertainty, and measurement errors. under this assumption, table iii reports the nominal values of the considered wind turbine model parameters with respect to their simulated but realistic uncertainty."
"the increasing dimensions of wind turbines lead to the increase in the loads on wind turbine structures. because of increasing rotor size and spatially varying loads along the blade, individual blade pitch control can reduce the negative effects of sub-rotor-sized turbulent structures. additional pitch control loops can be used to damp the tower motion or additional structural vibrations in the full load working condition [cit] ."
"in order to provide a brief but clear insight into the abovementioned techniques, the comparison was performed in the same previous working conditions, and based on the nsse% index suggested at the beginning of section v. table v summarises the results obtained by comparing the three control techniques recalled above with the ones proposed in this study. table v shows that the scheme using the vas and the lpv strategies allow to achieve better performances in terms of tracking error. however, the lpv controller can increase the computational time considerably with respect to the other solution, without any gain scheduling, whilst the lpv and uio control methods can require larger computational effort at the design stage."
"with the conventional cellular communication networks, the cost of deploying and running mobile applications and services in most developing countries is relatively high and unaffordable. however, software-defined radio (sdr) architecture offers an effective and efficient platform for mobile network operators to achieve backward compatibility of legacy digital mobile standards (2g/2.5g) with state-of-the-art (3g/ 4g) and emerging standards (5g) at a very low capital expenditure (capex) and operating expenditure (opex). in this work, open-source technologies were adopted to implement a proof-of-concept prototype of the sdr architecture named nomadicbts. the results of the tests conducted on the prototype proved that the architecture can be leveraged on to develop and deploy low-cost cellular communication networks for the underserved areas in emerging economies."
"finally, controller performance depends on modelling accuracy. for instance, as shown in section v, realistic modelling error in the optimal tip-speed ratio can cause an energy loss of around a few per cent in the partial load condition [cit], which can be a significant loss in this industry. even disregarding model errors, the dynamical behaviour of a wind turbine changes over time due to wear, debris build-up on the blades, and environmental conditions. as such, adaptive methods shown in this work can be used to tune controllers to improve performance compared to time-invariant methods [cit] ."
"phones were able to detect all the licensed operators within the vicinity of the study location in agreement with the detected traffics. in addition, the test phones were also able to successfully establish connection to the prototype cell whose network id is nomadicbts as shown in figure 12 ."
"finally, the paper has the following structure. section ii provides an overview of the wind turbine system considered in this work. section iii recalls the strategy exploited for the identification of the fuzzy controller. on the other hand, the second parameter-varying controller design is described in section iv-a. the achieved results are summarised in section v. section vi ends the paper by highlighting the main achievements of the work."
"the introduction of the fifth-generation (5g) mobile networks has become irrefutably essential in meeting the global mobile data traffic of the future. also, backward compatibility of 5g networks becomes highly imperative, given the fact that gsm has been acclaimed as the most successful wireless standard in the world with an extensive coverage of above 90% of the global population [cit] . in order to achieve backward compatibility of legacy digital mobile standards (2g/2.5g) with state-of-the-art (3g/4g) and emerging standards (5g) at a very low cost, the software-defined radio (sdr) architecture becomes very handy."
"with reference to the second adaptive control design, the two outputs p g (t) and w g (t) of the wind turbine continuoustime nonlinear model (2) were approximated by 2 timevarying miso discrete-time second order prototypes of the type (7) with 2 inputs. the approach described in section iv for siso models can be easily extended to the miso case. using these two on-line identified prototypes, the modelbased approach for determining the adaptive controllers shown in section iv-a was exploited. thus, the parameters of the adaptive controllers were computed on-line. in particular, the adaptive regulator parameters in (13) were computed analytically at each time step k. simulations were performed in the same conditions of the fuzzy controllers, and 2 adaptive regulators were used. as an example, the initial values for the parameters of the on-line estimation algorithm are listed in table i ."
"in this section, the results obtained from the various tests carried out on the usrp b200 hardware module are presented. the module was tuned to the licensed gsm 900 and gsm 1800 bands in nigeria in order to verify its capability for real-time signal transmission and reception. the radius of the prototype cell was computed using the measured signal strengths. furthermore, the connections of smart gsm phones (test phones) to the prototype cell were verified and the intercommunication between the phones is reported based on sms transmission and reception capabilities."
"in this study, ts fuzzy models are exploited [cit], as they are able to provide the mathematical description of the nonlinear system. the switching and the scheduling between the submodels is achieved through a smooth function of the system state, the behaviour of which is defined using fuzzy set theory."
"the effectiveness of the proposed control strategies has been assessed on data sequences acquired from the considered benchmark. several simulation results show the achieved performances with respect also to different control methods specifically developed very recently for the same wind turbine benchmark [cit] . in particular, three alternative control schemes are considered in this work, which are based on unknown input observers (uios) [cit], virtual sensors/actuators (vas) [cit], and lmi-based lpv controllers [cit] . since it is necessary to evaluate the impact on the designed control systems of modelling uncertainties, disturbance, and measurement errors, the overall scheme verification uses extensive monte-carlo simulations for the analysis and the assessment of the robustness, the stability, and their final performance evaluation. in fact, as shown in the following, the wind turbine system may contain elements that cannot be described by any analytical model obtained via first principles."
"thus, the recalled scheme is used for the on-line identification of the process modelled by the following transfer function g(z): hence, an alternative expression for the considered difference equation is given by:"
"according to these simulation results, good tracking capabilities of the suggested controllers seem to be reached, and the adaptive solution seems better than the fuzzy one."
"on the other hand, this work describes the application of two control methods, which are quite direct and straightforward, as well as their testing through extensive simulations for a wind turbine prototype, which is freely available for the matlab ® and simulink ® environments [cit] ."
"the approach suggested in this section employs fuzzy clustering techniques to partition the available data into subsets characterised by linear behaviours. relationships between clusters and linear regression are exploited, thus allowing for the combination of fuzzy logic techniques with system identification tools. in addition, an implementation in the matlab ® toolbox of the fuzzy modelling and identification (fmid) technique presented in the following is available [cit] . www.ijacsa.thesai.org"
"the monte-carlo analysis describes these variables as gaussian stochastic processes, with zero-mean and standard deviations corresponding to the maximal error values in table iii. therefore, for performance evaluation of the control schemes, the best, average, and worst values of the nsse% index were computed, and experimentally evaluated with 500 monte-carlo runs, as shown in table iv. table iii. realistic wind turbine uncertainty. in particular, table iv summarises the values of the considered performance index nsse% according to the best, worst and average cases, with reference to the possible combinations of the parameters described in table iii. table iv shows that the proposed control schemes, and in particular the adaptive solution, allow for good control performances www.ijacsa.thesai.org even in the presence of considerable error and uncertainty effects."
"the paper is focused on two examples of control designs for a nonlinear wind turbine prototype. the proposed control designs represent viable and easy-to-use methods for the straightforward derivation of proper controller models, as datadriven and system identification from data approaches are exploited. tests on the considered benchmark process and monte-carlo analysis were the tools for assessing experimentally the properties of the proposed control schemes, in the presence of modelling and measurement errors. the developed control methods were also compared with different approaches, in order to evaluate the considered techniques. these comparisons highlight that the proposed design methodologies can constitute reliable and robust approaches for application to real wind turbine processes."
"on the other hand, the second control approach uses the idea of virtual sensors/actuators (vas) [cit] . an estimation of the uncertainty acting on the process is provided on the basis of a batch least squares approach. the use of on-line disturbance estimation is essential for all compensation approaches."
"the coercion test was carried out using appropriate uhd command to ensure that the usrp b200 daughter boards can successfully tune to all specified frequencies and gains. the outputs of the test are shown in figure 6 . these outputs indicate that both the transmitter (tx) and the receiver (rx) sections of the device can operate within the specified frequency range of 50 mhz to 6 ghz. thus, the device is considered suitable for both gsm 900 and gsm 1800, which operate at 900 mhz and 1.8 ghz, respectively. furthermore, benchmark test, which involves the use of appropriate uhd command to obtain the throughput capability of the host pc with the usrp b200 device, was carried out. the result of the test is shown in figure 7 . the output comprises of the number of sent frames, overflows, dropped frames, received frames, sequence errors, and underflows detected. the result showed that the \"transmit\" and the \"receive\" components of the device are working optimally. therefore based on the foregoing tests, the usrp b200 is considered satisfactorily adequate for implementing the hardware front-end of the nomadicbts architecture in figure 1."
"the usrpb200 device was further validated in this study by investigating its signal reception capability from neighbouring licensed base stations. this is to ensure that the module can successfully process real-time rf signals. a spectrum browser flow graph in gnu radio companion (grc) was used to accomplish this task on an ubuntu linux 16.04lts os. this investigation was conducted at the downlink frequencies. in nigeria, the downlink frequency ranges of the licensed operators by the nigeria communication commission (ncc) in the 900 bands as at the time of this study are as follows: etisalat (935-940 mhz), mtel (940-945 mhz), globacom (945- (b)). in figure 8 (b), the band 945-948 mhz, which corresponds to the globacom spectrum, is also very active. figure 9 shows the spectrum of (a) 947-955 mhz and (b) 952-960 mhz bands obtained from the spectrum browser. in figure 9 (a), the region from 950 to 955 mhz, which is the mtn band, is conspicuously active with traffic. similarly, the region from 955 to 960 mhz, which is the airtel band, is also active with traffic. these results imply that out of the five licensed operators in the gsm 900 band in nigeria, only mtel does not have installed bts in the proximity of the laboratory where this study was carried out. in addition, the results provide a basis to utilize the usrp b200 along with the pc running the sdr software for a prototype nomadicbts cell site in this study."
"wind turbines can also be damaged when they are stopped as a result of supervisory control action due to high winds or fault conditions [cit] . however, little or no active control is performed when the turbine is stopped, although the yaw angle can be changed to accommodate changes in wind direction, which can prevent some damage."
"this section describes the recursive approach exploited for obtaining the mathematical description of the wind turbine system, which is used for the design of the second control strategy. a modification of the frisch scheme algorithm is proposed here to identify dynamical errors-in-variables (eiv) models [cit] . for the update of the estimated model parameters, a recursive bias-compensating strategy is also www.ijacsa.thesai.org implemented. thus, a recursive frisch scheme identification approach is extended to enhance its on-line applicability. it is shown that by incorporating adaptation via the introduction of exponential forgetting, the algorithm is able to compensate for the systematic errors, which arise in the original scheme [cit] . therefore, this adaptive recursive frisch scheme is able to deal with linear time-varying systems, and it is used in connection with the design of an adaptive control scheme, shown in section iv-a."
"in particular, the controller parameters p k and i t are computed using the ziegler-nichols relations depending on the (time-varying) critical gain and the critical period of oscillations [cit] . also these variables are functions of the time-varying model parameters θ(k)."
"in addition to the possibility of improving control when the turbine is stopped, advanced fault detection and turbine protection schemes are of interest to the wind industry [cit] . stopping the turbine in the case of emergency, which might entail pitching the blades to a predetermined stop position at maximum pitch rate and setting the mechanical brakes with which the rotor is equipped, can also cause damage to the machine and must be done only when a turbine failure is suspected."
"as illustrated in figure 1, the voip pabx carries out functions like call control and mobility management. it also implements voice mail server and voip gateway. the voip gateway provides interconnection to the pstn and to the traditional gsm network using sip and real-time transport protocol (rtp) via an internet telephony service gateway (itsg). in this study, the voip pabx section of the nomadicbts architecture was realized using asterisk, which is a leading software voip pabx [cit] )."
"are computed for the designed fuzzy controllers. table ii refers to the full-load operation, where the performance depends on the generator speed,  g, with respect to the nominal one,  nom ."
"the evolution of wireless communication technologies is rapid and the number of mobile devices, applications, and services is growing in an unprecedented dimension [cit] . the evolution of wireless cellular communication started with the first-generation (1g) cellular technology. the analogue system utilizes frequency division multiplexing (fdm) and circuit switching for its operations. however, the power consumed by the system is large while the quality of calls is low (del peral-rosado, raulefs, lópez [cit] ) . the introduction of the global system for mobile communications (gsm) standards marked the birth of the second-generation (2g) technology. gsm technology adopted a simplified encryption to overcome the security challenge of eavesdropping in 1g cellular systems [cit] . furthermore, the third-generation (3g) technology improved the voice quality with better quality of service (qos) delivered at a data rate of 2 mbps [cit] . continuous demand for enhanced data rate by mobile users led to the development of the fourth-generation (4g) networks which feature higher data rate of 50-100 mbps and internet protocol (ip) capability [cit] ."
"it is worth noting that the work [cit] provided an analytical demonstration of the stability of an adaptive control scheme for wind turbines. however, model parameter variations, recursive frisch scheme adaptive methods, or complete wind models were not taken into account there."
"wind turbines are complex nonlinear dynamic systems forced by gravity, and stochastic wind disturbance, which are affected by gravitational, centrifugal, and gyroscopic loads. their aerodynamics are nonlinear, and unsteady, whilst their rotors are subject to complicated turbulent wind inflow fields driving fatigue loading. therefore, wind turbine modelling and control are challenging tasks [cit] . accurate models should contain many degrees of freedom in order to capture the most important dynamic effects. moreover, the rotation of the turbine adds further complexity to the dynamics modelling. in general, off-the-shelf commercial software usually is not adequate for wind turbine dynamics modelling, but special dynamic simulation codes are required. it is clear that the design of control algorithms for wind turbines has to take into account these complexities. on the other hand, control algorithms must capture the most important turbine dynamics, without being too complex [cit] ."
"after installing the uhd software on the host pc in windows 8 os, different tests were carried out on the usrp b200 module. firstly, to ensure that the device is recognized by the host pc and obtain the default configurations of the device, appropriate uhd commands were invoked. the results presented in figure 4 show that the host pc recognized the module, the serial number of f5ff55, the receiver and transmitter frequency range of 50 mhz to 6 ghz, and host of other parameters."
"finally, once the time-varying parameters θ(k) of the discrete-time linear model approximating the nonlinear process (2) have been computed at each time step k, the adaptive controller is designed as described in section iv-a."
"few further comments can be drawn here. when the modelling of the dynamic system can be perfectly obtained, in general model-based control strategies are preferred. on the other hand, when modelling errors and uncertainty are present, alternative control schemes relying on adaptation mechanisms, or passive robust control methods, showed interesting robustness properties. the fuzzy logic-based scheme relies on the learning accumulated from off-line simulations, but the on-line estimation stage could be computationally heavy. finally, regarding the proposed methods using lpv or fuzzy tools, they seem rather simple and straightforward, even if optimisation stages can be required."
"in this paper, a novel cellular communication architecture, known as nomadic base transceiver station (nomadicbts), was proposed, designed, and prototyped based on sdr paradigm to achieve an efficient, dynamic, and cost-effective wireless communication. the architecture comprises of hardware front-end and software that are compatible with general-purpose processor. opensource technologies were adopted to implement a proof-of-concept architype of the architecture. the rest of the paper is organized as follows. section 2 presents the materials and methods. section 3 presents the results and discussion. finally, section 4 concludes the paper. the sdr hardware front-end comprises of the receiver and the transmitter stages for rf signal processing. at the receiver stage, the signal at the receiving antenna is forwarded to the low noise amplifier (lna) via a duplexer. the rf signal is translated to an intermediate frequency (if), which is then digitized by the analog-to-digital converter (adc). decoding, data rate conversion, timing, and various access schemes are carried out by the fpga. at the transmitter stage, the digital signals are converted to analogue waveforms by the digital-to-analogue converter (dac), upconverted from if to rf, amplified through the power amplifier (pa), and routed via the duplexer to the antenna. the hardware front-end of the sdr can be realized with any of the commercially available options which include hackrf, bladerf, ettus usrp, matchstiq, and zeptosdr [cit] . in this work, the ettus usrp b200 was used to realize the sdr front-end because of its admirable features [cit] ."
the signal strength of the nomadicbts prototype cell that was set up in a laboratory was measured using the network signal info pro tool. this software is a signal strength measurement mobile app. the prototype cell was configured to operate at the 900 mhz band using an absolute radio frequency channel number (arfcn) of 1. the signal strength measurement was used to determine how seamless the mobile device can connect to the nomadicbts prototype cell in the context of this study. signal strength was represented with received signal strength indicator (rssi) measured in dbm in network signal info pro. arbitrary strength unit (asu)-an integer value that is proportional to rssi was also used to present the signal strength alongside the rssi by network signal info pro. the interpretation of the asu values is shown in table 2 .
"where rc is the cell radius, r is the distance from the test mobile phone to the prototype cell site, and dbm represents the measured signal strength at r. using the signal strength at approximately 30 cm from the prototype cell, which is −57 dbm, we have"
"given the complexity of the wind turbine system, the stability of the complete plant plus control system cannot be proven. the multiple control loops interact, as do the multiple degrees of freedom of the turbine, especially as wind turbines become larger and have lower natural frequencies. a unified multiple-input multiple-output (mimo) framework for individual blade pitch control can achieve significant load reduction for floating offshore wind turbines with strong coupling across degrees of freedom [cit] ."
"the testbed for the nomadicbts prototype cell for this study was set up within our laboratory and it has a radius of 7.54 m. this is a femtocell as established, and consequently, there is a negligible interference with active commercial traffic. as shown in figure 12, the two test phones used for this study are window's phone nokia lumia 530 and android phone blu dash 5.0. the two figure 10 . nomadicbts signal measurement at close proximity (30 cm) with one of the test mobile phones using network signal info pro mobile app."
"with reference to the second control method proposed in this work, the application of an on-line identification mechanism in connection with a model-based adaptive control design is considered. this control scheme belongs to the field of adaptive control. on-line parametric model identification www.ijacsa.thesai.org schemes represent an alternative for developing experimental models for complex systems, such as wind turbine systems. therefore, this paper suggests the implementation of controllers based on adaptive identification schemes, used for the on-line estimation of the controlled process, which can be affected by uncertainty and errors. the recursive frisch extended to the adaptive case making use of exponential forgetting is considered here [cit] . it also overcomes potential numerical difficulties with the existing recursive scheme. the ability of the adaptive scheme to track changes in the system parameters is exploited here in connection with the on-line computation of time-varying controller parameters, in order to maintain the required control performances. the use of this identification procedure is motivated by its easy integration into the simulink ® toolbox for the design of on-line controllers [cit] ."
"in particular, the first proposed strategy consists of a scheme relying on a fuzzy identification approach to modelbased control design. in contrast to pure nonlinear identification methods, fuzzy systems are capable of deriving nonlinear models directly from measured input-output data without detailed system assumptions, with arbitrary degree of accuracy. in particular, takagi-sugeno (ts) fuzzy prototypes are exploited, whose parameters are obtained by identification procedures from the data of the monitored process. the suggested fuzzy approach is motivated also by previous works by one of the same authors [cit] . it is worth noting that the works by one of the same author [cit] presented a totally different solution to the design of the fuzzy regulators. in fact, even if the papers [cit] and the present study share the common fuzzy clustering methodology, this contribution focuses on the direct fuzzy regulator identification, whilst [cit] were based on fuzzy pi controllers, whose parameters were computed from the identified fuzzy prototypes."
"therefore, the approximate cell radius of our prototype cell is 7.54 m, which can be described as an approximate femtocell with potential for cellular communications within homes or offices [cit] ."
"this section recalls the approach exploited for obtaining the fuzzy description of the wind turbine controller, whilst the proposed controller model estimation is shown in section iii-a, which represents one of the main contributions of the paper."
"the procedure of running csann to produce a schedule is summarized in algorithm 4. [cit], 2001) . these heuristic algorithms, algorithm 5 and algorithm 6, are integrated in algorithm 4. they are described in the following section."
"from fig. 8, it can be seen that when γ becomes tighter, the mean ips and hence the mean tps required by csanns increases. and for each jssp when γ is decreased from 0.5, the ips and tps by csanns increase slowly at the early stage. when γ is decreased below a certain value (i.e., the threshold value), they increase significantly in a power law. for example, on la21, the mean ips of csann-ii increases approximately linearly from 34.43 to 138.80 when γ decreases from 0.5 to 0.3. then, the ips increases from 186.54 to 1019.40 approximately in a power law when γ decreases from 0.28 to 0.22. finally, it increases exponentially from 1019.40 to 10672.38 when γ decreases from 0.22 to 0.2. the threshold value varies with the jssp. the larger the problem size, the smaller the threshold value."
"in this case, rcb qikjl represents a sequence constraint described by the second disjunctive equation of (2). if a violation exists, the activation of rc qikjl and the feedback adjustments are calculated by"
"regarding the effect of m on the performance of csanns, from fig. 8 it can be seen that when m is doubled, the tpi of csanns is approximately doubled. and from fig. 9, it can also be seen that the value of m has no clear effect on the value of r t p i (and r t p s ). this result is consistent with our analysis in sect. 4.2: the computational complexity per iteration for csann and csann-ii is linear with m while the improvement of csann-ii over csann is of the order o(n/ log n) without an explicit impact from m."
"there are several avenues to pursue for future work regarding csann-ii. first, just as the three classical heuristic algorithms studied in this paper, csann-ii is a working tool for jssps. whenever the initial start times for operations are given, csann-ii will return a deterministic schedule. it is important to see that the qps generated by csann-ii is much higher than the three classical heuristic algorithms studied, which are widely used as the fundamental tools for advanced jssp systems [cit] . just as the three classical heuristic algorithms, csann-ii can surely act as a good fundamental tool for constructing advanced hybrid intelligent systems for jssps. for example, combining it with a simple local search scheme has shown some promising results for jssps [cit] . combining it with other meta-heuristic methods may also produce promising results. for example, we may integrate csann-ii into gas and compare the performance with other state-of-the-art ga based scheduling systems for the jssp. this is an interesting work now under investigation."
"the design of algorithm 8 is based on our experience of running csanns. a key factor relevant to the computational cost is the number of iterations that csanns require to produce a schedule. during our preliminary experiments, it seems that, if the expected makespan is properly set, the number of iterations that csanns require to produce a schedule has an approximately linear relationship with the total number of operations in the jssp. when the number of iterations required by csanns to produce a schedule is approximately linear with the total number of operations, the computation time for a schedule is always reasonable. hence, algorithm 8 is designed to produce a proper expected makespan that makes the mean number of iterations that csann or csann-ii requires to produce a schedule to be approximately linear with o. this is realized by cyclical trials. for each cycle, we try to reduce the factor γ (and hence tighten the expected makespan) while ensuring that the mean number of iterations required to produce a schedule is still within a linear relation with o approximately."
"it is also quite common that the priority rules can be combined into the giffler and thompson algorithm: when dispatching an operation, one priority rule is first randomly selected from a pre-defined set of priority rules and then is applied to select an operation. this hybrid method is denoted gt-rule in this paper and is described in algorithm 3."
"2 the job-shop scheduling problem 2.1 formulation of the jssp traditionally, the jssp can be stated as follows [cit] ): given n jobs to be processed on m machines in a prescribed order under certain restrictive assumptions, the objective is to optimally arrange the processing order and the start times of operations to optimize certain criteria. generally speaking, for jssps there are two types of constraints: sequence constraints and resource constraints. the first type states that one operation of a job must finish before another operation starts. the second type states that no more than one job can be handled on a machine at the same time. job-shop scheduling can be viewed as an optimization problem, bounded by both sequence and resource constraints. traditionally for a jssp, each job may consist of a different number of operations, which are subject to some precedence restrictions. usually, the processing order of each job by all machines and the processing time of each operation are known and fixed. operations cannot be interrupted once started (i.e., nonpreemptive). this kind of scheduling is called deterministic and static. in this paper, we focus on the deterministic and static jssp."
"all neurons in csann are structured into two problemspecific constraint blocks: the sequence constraint block (sc-block) that deals with all sequence constraints of a given jssp and the resource constraint block (rcblock) that deals with all resource constraints of a given jssp. each sc-block unit has two st-neurons that represent two operations of a job and one sc-neuron that represents whether the relevant sequence constraint is satisfied, see fig (1) between o ikp and o ilq, with b sc ikl being its bias. i st ikp and i st ilq represent the initial value for t ikp and t ilq, which are taken as the initial net input to st ikp and st ilq, respectively. the weights and bias are valued as follows:"
"second, csann-ii itself may be further improved. the knowledge that becomes available during the solving process can be further integrated to improve its performance. for example, with the solving process of csann-ii, more and more neuron units will correspond to already satisfied sequence constraints and resources constraints and hence become irrelevant in terms of solving constraint violations for a feasible schedule. this knowledge may be used to avoid calculating such irrelevant units and further reduce the computational cost."
"the calculated results regarding r tpi and r tps are plotted in fig. 9(a) and (b), respectively. from fig. 9(a), it can be seen that r t p i is roughly bounded in the range of [0.14, 0.2] over all values of γ for csanns and on all the jssps. in other words, we have the following relationship between tpi(csann) and tpi(csann-ii):"
"in step 4 of the above gt-act and gt-nd algorithms, an operation to be selected from the conflict set c has an important effect on the final active and non-delay schedule respectively. researchers have developed a large number of heuristic priority rules to be"
"units have finished their calculations and stored their activations. in the next calculation cycle, the activation of a unit is calculated using the stored activations of the connected units."
"it has been demonstrated [cit] ) that job-shop scheduling is usually an np-complete prob-lem. because of the np-complete characteristics of the jssp, it is usually very hard to find its optimal solution. fortunately, an optimal solution in the mathematical sense is not always necessary in practice. hence, researchers have turned to search for near-optimal solutions to jssps, utilizing many different heuristic algorithms [cit] ) and the near-optimal solutions that are produced usually meet the requirements of practical problems. several knowledge-based scheduling systems have been presented [cit], which are much more general than traditional methods because they systematically use constraints, implement heuristic knowledge, and represent a framework for stating and solving combinatorial optimization problems."
"during the running of csann, due to conflicts resulting from sequence and resource constraint violation feedback adjustments, the phenomenon of deadlock may occur [cit] . deadlocks stop csanns from producing a feasible solution. a heuristic algorithm was proposed to break the deadlock (and hence make it possible to produce feasible schedules) by exchanging the order of two adjacent operations on the same machine via exchanging their start times under a certain condition."
"usually, a neural unit (or neuron), say neuron i, in a neural network consists of a linear summator and a nonlinear activation function f (·), which are serialized [cit] as below:"
"from figs. 7 and 8, several results can be observed and are analyzed as follows. first, csann-ii greatly outperforms csann with respect to the quality of schedules produced, either the best makespan or the mean makespan, under almost all values of γ on all the test jssps (see fig. 7 ). on la21 and la26, when γ is set to loose values, e.g., bigger than 0.3, even the mean makespan achieved by csann-ii is better than the best makespan achieved by csann. under almost all settings of γ, csann-ii has reached the optimal schedule for jssps la01, la06, and la11."
"for each csann-ii iteration, sorting the st-neurons for each machine requires o(n log n) calculations by a quick sort algorithm [cit] ). it also requires n(m − 1) sc-neuron calculations and m(n − 1) rc-neuron calculations, resulting in a computational complexity of o(mn log n). in contrast, each iteration of csann requires n(m − 1) sc-neuron calculations and mn(n−1)/2 rc-neuron calculations, which is in the order of o(mn 2 ). hence, for each iteration of the neural network, csann-ii achieves a reduction of magnitude o(n/ log n) over csann with respect to the computational complexity."
"in the t-test results regarding csann-ii -csann. this result is consistent with our first set of experiments. in order to see whether algorithm 8 for csanns works, we also recorded the tightness factor γ achieved after the pre-processing stage of each run and report the results in table 6, where min/ave/std means minimum, average and standard deviation over 50 runs of csanns, respectively. from table 6 and fig. 7, it can be seen that algorithm 8 achieved a slightly loose value of γ for la01, la06 and la11 and a good value of γ on larger jssps, la16, la21 and la26. here, a \"loose\" or \"good\" value of γ is relative to the \"threshold value\". for example, for la01, it is \"good\" to set γ to the range of [0.35, 0.4] since in this range tps is low but the solution quality is high, see fig. 6 and 7. algorithm 8 reaches an average value of 0.46 for second, comparing the performance of csanns with the three heuristic algorithms regarding the quality of produced schedules, it can be seen that csann-ii also significantly outperforms gt-act, gt-nd and gt-rule on all test jssps and that csann outperforms them on la01 and la06 while performs similarly as or is beaten by them on the other jssps. this result can be more clearly viewed from the dynamic performance of methods in fig. 10 . fig. 10 shows that csann-ii performs much better than the other methods on the jssps. on la06 and la11, csann-ii even achieved the optimal solution within 100 schedules on the average."
"this paper investigates an improved constraint satisfaction adaptive neural network, csann-ii, for the jssp. in csann-ii, the topology corresponding to the resource constraints is simplified according to the online resource constraint satisfaction situation when it is running via a simple sorting algorithm. consequently, csann-ii's computational time per schedule is reduced over the original csann model. some heuristics are also proposed to improve the performance of csann and csann-ii, including producing a proper expected makespan and improving the quality of produced schedules. based on a set of benchmark jssps, we experimentally studied the computational complexity of csann-ii over csann and compared the performance of csann-ii over csann and three classical heuristics. from the experimental results, we analyze the strength and weakness of csanns for jssps. according to the experimental results and analysis, the following conclusions can be drawn on the test jssps."
"third, when considering the computational cost of different methods, both csann and csann-ii spend significantly more time than gt-act, gt-nd, and gtrule. this is easy to understand because csanns, under the values of γ produced by algorithm 8, need tens or hundreds of iterations for one schedule while gt-act, gt-nd and gt-rule produce one schedule in just one iteration."
"from table 10 and table 4, it can be seen that csann-ap significantly outperforms csann on almost all test jssps, as indicated in the t-test results in table 11 . that is, algorithm 9 significantly improves the quality of solutions produced. another observation is that csann-ap outperforms csann-act, especially on large jssps. this result indicates that algorithm 8 also contributes toward a better quality of solutions produced by csann. the third result is that csann-ii performs statistically equivalent or similar to csann-ap regarding the solution quality, except on la21 where csann-ii is significantly beaten by csann-ap."
"for csann-ii, for a traditional jssp with m machines and n jobs where each job passes through all machines in a certain sequencing order, it requires mn st-neurons, n(m − 1) sc-neurons, and m(n − 1) rc-neurons. in total, csann-ii consists of 3mn−m−n neurons, which is in the order of o(mn) instead of in the order of o(mn 2 ) for csann. this is a reduction of magnitude n regarding the network complexity."
"finally, the jssp studied in this paper is basically an academic problem in scheduling. for the sake of simplicity and clarity, we have focused on jssps that are not oversubscribed, have no (tight) deadlines for jobs, 21 and do not allow one job to be processed more than once on a machine. a modest future effort will extend the csann approach to practical jssp-type situations. the application of csann-ii can also be extended to stochastic and dynamic jssps, which are closer to realworld scheduling problems. to address these jssps, the structure of csann-ii may be more flexible and change over time according to the stochastic or dynamic conditions of the jssp."
"where the feedback adjustments put backward the start time t ikp of o ikp and put forward the start time t ilq of o ilq . thus, the sequence constraint violation between o ikp and o ilq may be solved. figure 3 (b) shows an example rc-block unit rcb qikjl, representing the resource constraint of (2) between o ikq and o jlq on machine q. at time t during the run of csann, the connection weights and bias are adaptively valued according to the following two cases. (13) holds"
"third, regarding the effect of the tightness factor γ on the performance of csanns, from fig. 7 it can be seen that the value of γ affects the best schedule produced by csann, but it shows no clear effect on the best schedule produced by csann-ii. however, γ has a significant effect on the mean makespan produced by both csann and csann-ii. the smaller (and hence the tighter) the value of γ, the better the schedule produced on the average. the trade-off for this gain is, not surprisingly, the computational cost."
"where c i represents sc i or rc i and b ci is the bias, which equals the processing time of an operation corresponding to st 1 or st 2 (more details are explained in sect. 3.2). the st-neurons, st 1 and st 2, represent two operations of the same job for an sc-neuron, or two operations sharing the same machine for a rc-neuron. the activation function is linear-segmented. when the activation of an sc-neuron or rc-neuron is greater than zero, it means the relevant sequence constraint or resource constraint is violated and there will be feedback adjustments from c i to relevant st 1 and st 2 with adaptive weights. in the following section, the connection weights and biases for sc-neurons and rc-neurons and feedback adjustments from an sc-neuron or rcneuron to st-neurons are explained."
"the job-shop scheduling problem (jssp) is one of the most difficult problems in scheduling. it aims to allocate a number of machines over time to perform a set of jobs with certain constraint conditions in order to optimize a certain criterion, e.g., minimizing the makespan [cit] . the jssp has been much studied in the academic literature due to its importance as a typical combinatorial optimization problem and its potential expansion to a wide range of industrial problems. traditionally, there are three kinds of methods to solve jssps: priority rules, combinatorial optimization, and constraints analysis [cit] . the first category has the merit of being computationally efficient and easy to implement on real cases, but there is no guarantee with respect to the quality of the solutions produced. optimization methods are much more rigorous but are not tractable when the problem size is large and the optimal solution is required [cit], looks for a set of feasible solutions that meet several technical constraints from which the user may choose the final solution."
"where w is a positive feedback adjustment factor (it is the same in other equations in this paper). at time t during the run of csann, if the sequence constraint between o ikp and o ilq is satisfied, the activation a sc ikl (t) of sc ikl equals zero; otherwise, the activation of sc ikl will be greater than zero and can be calculated by"
"given a jssp, any feasible solution to the above formulation is called a feasible schedule. given a feasible schedule for a jssp, if an operation can be left-shifted, i.e., started earlier, without altering the processing sequences, such a left-shift is called a local left-shift. if a left-shift of an operation alters the processing sequences but does not delay any other operations, it is called a global left-shift. based on the concept of local and global left-shift, feasible schedules for a jssp can be classified into four types: inadmissible, semi-active, active and non-delay. inadmissible schedules are those that contain excess idle time and can be improved by local leftshift(s). semi-active schedules are those that allow no local left-shift. active schedules are those that allow neither local left-shift nor global left-shift. non-delay schedules are those schedules in which no machine is kept idle when it could start processing some operation."
"first, csann-ii speeds up in the order o(n/ log n) over csann, including the time per iteration and time per schedule measures. csann-ii outperforms csann with respect to the quality of solutions primarily due to its use of algorithm 9, which is better than the semiactive algorithm used in csann."
"for each csann on a jssp, 50 runs were carried out with 10 5 schedules produced per run and the intermediate best-so-far schedule recorded every 100 schedules. the experimental results regarding the best makespan produced by csann-act and csann-ap and the corresponding t-test results of comparing them and csann and csann-ii are shown in table 10 and table 11 respectively."
"third, csann-ii outperforms three classical heuristic algorithms with respect to the quality of solutions but requires more time for a schedule. when the run time is fixed, csann-ii can reach better solutions than the three classical heuristic algorithms with fewer schedules. hence, csann-ii has a much higher qps, which is an important feature."
"based on the general neuron model, csann contains three kinds of neurons: st-neurons, sc-neurons and rc-neurons. each st-neuron represents an operation with the activation representing the start time of the operation. each sc-neuron or rc-neuron represents whether a relevant sequence constraint or resource constraint is satisfied, respectively."
"the relationships between the different kinds of feasible schedules are illustrated in fig. 1 . non-delay schedules are necessarily active and hence also necessarily semi-active. an optimal schedule with respect to makespan is guaranteed to be an active one but not necessarily a non-delay one [cit] . however, there is strong empirical evidence that non-delay schedules show a better mean solution quality than active ones. nevertheless, scheduling algorithms typically search the space of active schedules in order to guarantee that the optimum is taken into consideration."
"during the running of csann, if two adjacent operations of the same job are placed in an order that violates algorithm 6 swap two adjacent operations on a machine"
where log n is 10 based. we also calculate the weighted ratio of the tps by csann to the tps by csann-ii using the following formula:
"in this case, rcb qikjl represents a sequence constraint described by the first disjunctive equation of (2). if violation exists, the activation of rc qikjl and feedback adjustments from rc qikjl to st ikq and st jlq are calculated by (17) holds"
"the performance of csann and csann-ii for jssps. the gt-rule algorithm studied in this paper uses the six rules in table 1 as the set of priority rules. the following two sections will describe the details of the original csann model and the improved csann-ii model, respectively."
"swap the order of o ikq and o jlq on machine q: this heuristic algorithm aims to accelerate the solving process. in fact, (21) and (22) form a more direct method of removing sequence constraint violations than the feedback adjustment scheme in csann. hence, the adjustment time for removing sequence constraint violations is shortened and the solving process is speeded up."
"for each run of a method on a test jssp, 10 5 schedules 4 were calculated with the intermediate best-so-far schedule recorded every 100 schedules. and for each run, the final best schedule and time used were also recorded. in order to avoid the effect a random seed may have, 50 runs with different random seeds were carried out for each method on each test problem and the mean results over 50 runs are reported."
"the gt-act, gt-nd, and gt-rule algorithms have become the basis for many state-of-the-art hybrid scheduling systems for jssps [cit] . for example, in hart and ross's hga [cit], each gene in a chromosome represents a pair of values (method, heuristic), where method denotes either gt-act or gt-nd should be used at each iteration of a scheduling algorithm to calculate a set of schedulable operations, and heuristic represents the priority dispatch rule to be used to select an operation from that set. the priority dispatch rules used in hga are similar to those shown in table 1 ."
"the above two aspects together validate the importance of selecting a proper γ, i.e., a proper expected makespan, for csanns to run. there is a big trade-off between the quality of the schedules and the computational cost. ideally, γ should be set at the end of the linear growth region for a good schedule quality and reasonable run time of csanns. this is the aim of the proposed algorithm 8, which tries to produce a proper expected makespan by iterative trials. csanns with algorithm 8 are the concern of the second set of experiments to be presented in the next section."
"the rest of this paper is organized as follows. the next section presents the basic concepts of classical jobshop scheduling, the mathematical formulation and the classical giffler and thompson methods for jssps [cit] . sect. 3 [cit], 2001) for jssps. in sect. 4, the improved model of csann-ii is described in detail together with the heuristic algorithms that can be combined with csann and csann-ii for a better performance. sect. 5 presents the computer simulation study based on a set of benchmark jssps to investigate the computational complexity of csann-ii over csann and show the performance of the combined approach of csann-ii and heuristic algorithms for jssps. finally, sect. 6 concludes this paper with discussions on relevant future work."
"in algorithm 8, τ is the total number of runs for each cycle of the pre-processing stage, i i (k) is the number of iterations that csann or csann-ii uses to produce a schedule in the ith run of the kth cycle of the pre-processing stage,ī(k) is the mean number of iterations that csann or csann-ii uses to produce a schedule in the kth cycle of the pre-processing stage, ρ is a coefficient that roughly determines the mean number of iterations required by csanns to produce a schedule after the pre-processing stage, and δ is the decreasing factor for the tightness factor γ."
"in sect. 4.2, we have analyzed that for each iteration of the neural network, csann-ii achieves a re-, ips csann-ii, ips csann, tps csann-ii, tps csann, tpi csann-ii, tpi fig. 8 experimental results on the mean iterations per schedule (ips), mean time per schedule (tps), and mean time per iteration (tpi) used by csanns against different values of γ on the jssps. the y-axis is log-scaled and the unit for tps and tpi is 10 −3 second and 10 −6 second, respectively. duction in the order o(n/ log n) over csann regarding the computational complexity. in order to see whether this is the case in the experiments, we calculate the weighted ratio of the tpi required by csann to the tpi required by csann-ii for each value of γ and on each jssp using the following formula: (a) (b) fig. 9 results of (a) r t p i and (b) r t p s against different γ on the jssps."
"next, let us consider the average number of schedules produced by different methods within the fixed time on each jssp. from table 9, it can be seen that gt-act and gt-nd produce a similar number of schedules while gt-rule generates fewer schedules. this is natural due to their similar computational complexity. csann-ii generates about 6 to 18 times fewer schedules than gt-act and gt-nd on the jssps while dominating them in mean solution quality. in other words, the peak quality per schedule (qps) generated by csann-ii is much higher than other methods. this is an interesting result because both csann-ii and gtact aim at active schedules with uncertainty: csann-ii starts from random initialized start times of operations while gt-act schedules a random operation from the conflict set c iteratively. the difference lies in that gt-act searches in the whole domain of active schedules randomly while csann-ii searches from the larger domain of feasible schedules, which are properly filtered by the expected makespan imposed (the produced schedules are then mapped to active ones by algorithm 9). this gives csann-ii an advantage over gt-act (and gt-nd and gt-rule similarly). the result that csann-ii has a much higher qps is also important since for many advanced scheduling systems, the qps is a key issue. for these systems, csann-ii appears better suited than some widely applied heuristics like gt-act, gt-nd, and gt-rule."
"for csann-ii, algorithm 5 and algorithm 6 can also be used to speed up the solving process and enhance the convergence. in addition to these algorithms, this paper presents two other heuristics to be combined with csanns (both csann and csann-ii) to achieve even better performance. they are described as follows."
"first, csann-ii significantly outperforms csann with respect to both the quality of produced schedules and time used on almost all test jssps, as indicated fig. 10 the dynamic performance of methods regarding the best-so-far makespan against schedules on the jssps. the data were averaged over 50 runs."
"just like csann, the feasible schedules produced by csann-ii are usually inadmissible. in this paper, a heuristic algorithm is proposed to generate an active schedule from the feasible schedule produced by csann-ii as follows: first, sort all operations in a non-decreasing order of their start times; then, from the first to the last in the ordered operation list, each operation is moved forward to its earliest start time as follows: if possible, performing a global left-shift; otherwise, if possible, performing a local left-shift. the details are shown in algorithm 9."
"fortunately, when we further consider the running of the rc-block, a potential improvement can be obtained. when we run csann for a jssp, during each iteration of the rc-block, in fact usually only a part of the n(n − 1)/2 resource constraints with respect to one machine are violated and hence are relevant to our algorithm 7 construct the rc-block adaptively 1: before each iteration of the rc-block, sort the st-neurons related to each machine according to their activations (i.e., present start times of relevant operations to be processed on the machine) in a non-decreasing order. 2: from the first to the last in the ordered st-neuron list, construct one rc-block unit for two adjacent st-neurons."
"the experimental results regarding the makespan of final best schedule and time used in seconds are given in table 4, where min/ave/std means minimum, average and standard deviation over 50 runs of algorithms, respectively. the statistical comparison of methods regarding the best makespan produced and the run time used over 50 runs by one-tailed t-test with 98 degrees of freedom at a 0.05 significance level is given in table 5, in table 5, if the t-test value regarding method 1 -method 2 is negative or positive, it means that method 1 is better than or worse than method 2, respectively, regarding the corresponding performance measure. if the absolute t-test value is greater than 1.660, the performance difference between method 1 and method 2 is significant. those t-test values that mean significant differences of performance between methods are shown in bold font in table 5 . the experimental results regarding best-so-far makespan produced by different methods against schedules are plotted in fig. 10, where the data were averaged over 50 runs. from table 4, table 5, and fig. 10, several results can be observed."
"second, the proposed adaptive scheme for producing a proper expected makespan works well for csanns. the experimental results show that reasonable values can be found for the expected makespan automatically. this eliminates the need and difficulty of manually setting the value for csanns to solve jssps since setting a reasonably good value for the expected makespan can usually lead to good schedules. of course, we may still manually tune the value with human expertise for optimal results."
"in this paper, csann is run iteratively using the first asynchronous running mechanism. each iteration first calculates each sc-block unit in the sc-block and then calculates each rc-block unit in the rc-block in a fixed order, e.g., starting from rc-block units corresponding to the first machine, to rc-block units corresponding to the second machine, and so on. this iteration continues until the activations of all sc-neurons and rc-neurons become zero. the final activations of st-neurons form a feasible schedule to the given jssp."
"a sensitivity analysis was conducted to observe the influence of each architectural design parameter -bore, stroke, hot and cold temperatures on power, efficiency, and cost. as an example, figure 4 illustrates the main effects of design parameters on the capital cost of the engine. engine cost is less sensitive to the change of stroke, for instance, than to the change of bore size. change of temperatures in hot and cold spaces produce close to similar effect on the engine cost, with hot space being more effective. the next section represents two case studies of applying the framework to design stirling engine for two different applications."
"heat transfer coefficients are used as primary drivers for the cost estimating relationships for the engine, which form the economic model for engine capital cost prediction. the economic model estimates capital cost associated with four key elements of the engine: cooler, heater, regenerator, and the bulk engine itself. these cost equations are derived from the work of ferreira et at. (2014) 30 . the cost model derived from ferreira estimates capital cost of the entire engine unit, as well as the cost of its key elements -bulk engine, heater, regenerator, and cooler -as a function of main engine design parameters."
"in all infants, the cortico-spinal tract was finely reconstructed by tractography, similarly with all five correction strategies. all strategies also globally provided equivalent quantification of mean fig. 5 . evaluation of rgb maps quality in infants. rgb maps are presented for the same three infants as in fig. 3 .2 (a: quiet infant; b, c: moving infants), and were computed according to dw images obtained 1) without correction, 2) with visual rejection of corrupted volumes, 3) with resampling of outlier slices, 4) with 3d registration of motion, 5) with our 2-step correction strategy and 6) with restore approach. the 2-step strategy (5) corrected most artifacts (arrows) and provided the best rgb maps on visual assessment. with restore (6), images appeared smoother, but the comparison remained difficult because of differences in orientations (images were not resampled to ac-pc referential). some motion artifacts remained, particularly in b (arrows)."
"there is a number of works that tackled the interdependencies of technical parameters and their impact on capital costs in the design process of stirling engines. [cit] 16 has analyzed key issues to be addressed when designing a stirling engine, including ideas on thermodynamic and kinetic coupling. [cit] 17 has performed detailed analysis of design parameters interaction in a stirling engine. the design parameters they considered include heating temperature, cooling water temperature, charge pressure, and the operation time. however, while previous research developed sophisticated thermal, kinematic, or mechanic models in support of the design process, there is a gap in system design models for tradespace exploration applied to the stirling engine design. a comprehensive analysis of this design problem needs to account for the economics of the engine, intimately integrated to the understanding of the thermodynamics and mechanical aspects involved in the design. being thus a multi-criteria design problem, the early stage study of stirling engine systems could benefit from the insights obtained by formulating and solving a corresponding multi-objective optimization problem 18 . however, the key disadvantage of multi-objective optimization is a need for a priori set of constraints and optimizing objectives with their relative ranking by a decision-maker. [cit] 19 allows decision-makers to gain an understanding of underlying relationships of different design variables prior to forming a final set of requirements and constraints. to support this approach, balling states the need for research in the following two areas: 1. interactive graphical computer tools to assist decision-makers in the shopping process 2. efficient methods for obtaining pareto frontiers. [cit] 20 focuses on developing a graphical user interface that allows decision-makers to implement a design by shopping paradigm. in this work we focus on the second objective proposed by balling and develop a framework for obtaining pareto frontiers for stirling engines design. it is a common problem to formulate decision-maker requirements that will have similar metrics to be applied in system model analysis. [cit] 21 proposes a math-based approach of aggregating decision-maker requirements. the approach quantifies and tracks decision-maker requirements instead of assuming a decision-maker requirements based on invalid metrics and fixed requirements. this work assumes that the decision-maker requirements is to maximize power and efficiency, and minimize capital cost of the engine. [cit] 22 [cit] 23 improves model-based system engineering by proposing a tool that allows to connect system modeling (sysml) and domain models analysis using an interface provided by the process integration and design optimization (pido) framework. this allowed to connect sysml with various engineering tools such us cad/cae, legacy codes, mathematical solvers, and spreadsheets. in our work, domain models and the system model are developed from the outset as one framework. however, future work will require the implementation of multidisciplinary tools. at that point a pido framework will need to be developed. it will require also implementing sysml language to efficiently execute system and domain level analysis. the following section proposes a possible approach to stirling engine systems analysis, which in this paper is formulated as a stirling engine design framework."
"this experiment required only 11 rounds to produce populations showing substantial binding during bulk reactions. many of the selected aptamers bound with various levels of specificity to gpc3; others appeared to bind to different proteins on the cell surface, perhaps newly emerging on the host cells as a consequence of their being engineered to express gpc3. interestingly, the kd for these aptamers were in the low to high nm range when tested on a hgpc3-positive human liver cancer cell line, hepg2. one aptamer in particular, lg5, which carried one z residue but no ps, showed a high level of specificity for the intended target regardless of the cell line expressing it, and had an apparent kd (for hepg2) of 6 nm. again, aegis nucleotides were found to be essential for binding. several aptamers were obtained from deep sequencing, and a quantitative improvement in dna performance with added nucleotides could definitely be seen. indeed, the aptamers with the lowest k d contained z and/or p residues, and were as low as 14 nm, while actg-only aptamers had k d ranging from 326 nm to more than 1 µm (figure 4) . moreover, the aptamers were very specific for the cell line used during selection, and did not bind to other cell lines. this feature was not fully achieved in the first attempt at aegis-cell-selex, where some aptamers also bound to some other cell lines."
"in terms of angular errors in the main eigenvector direction (fig. 4.b), the strategy of outliers resampling performed better than the strategy of outliers exclusion, both for randomly distributed outliers and for outliers along a specific orientation in the case of vibrations. errors were particularly small when resampling the random outliers (for instance, for 5 outliers: 5. for both the random motion and vibration outliers, errors in fa estimation were of the same order of magnitude as errors in dw signal (fig. 4.c) . no significant difference was observed between the strategies of outliers resampling and exclusion in terms of mean normalized deviations to the reference (up to 8% ± 0.3%) and percentages of voxels with differing fa values (5%: up to 51% ± 1%; 10%: up to 27% ± 1%). as for angular errors in the main eigenvector direction, these deviations increased drastically when the outlier number increased from 1 to 5 (fig. 4.c) . this reflects the high dependence of the tensor estimation on the acquired number of orientations. for an equivalent number of outliers, the mean normalized deviations and the voxel percentages were again higher for outliers along a specific orientation (vibrations) than for outliers along orientations randomly distributed over the whole space (random motion) (fig. 4.c) ."
"in filter-binding assays, pa1 was found to be strictly specific for the 63-kda version of protective antigen, and unable to bind to pa83, indicating that the pa1 epitope might at least in part reside on the portion of pa83 that becomes exposed after cleavage of the 20-kda subunit. extensive mutational and enzymatic analysis revealed that the two p were essential for binding to pa63, and that the folding of the molecule included a four base-pairs stem surmounted by a large highly compact and nuclease resistant loop ( figure 5a ). while the parent molecule that was originally selected had a k d of~2.3 µm, subsequent truncations of the molecule reduced the binding constant to~50 nm (pa1t4). most interestingly, although expected from the structural studies mentioned above, when two base pairs in the stem were substituted with two p-z pairs (pa1t4ppzz), the molecule's k d lowered further to~35 nm ( figure 5b ). this is one example of the effectiveness of post-selection design, where a molecule's features can be improved by the addition of strategically placed non-natural elements."
"the thermodynamic cycle model includes equations to estimate performance for three different stirling engine types: alpha 25, beta 26, and gamma 27 . the key difference between engine types is in kinematic schemes. without considering in details the dynamic relations for the schemes, the equations allow calculating cycle thermodynamic parameters that include cycle pressure, temperature, engine power, and cycle efficiency. these parameters will be later used for estimations of the engine capital cost. [cit] 25 and from the first-principle mechanics modeling."
"aegis nucleotides complete the watson-crick pairing concept by shuffling hydrogen bond donor and acceptor groups, forming additional orthogonal nucleobase pairs. they resemble natural nucleotides in size, shape, and pairing geometries, are independently replicable, and they do not interfere with dna double helix structures that are fundamental for the recognition of the polymer by many natural enzymes. most importantly, aegis nucleotides increase the information density of a nucleic acid molecule of a given length, and have the potential to increase functionality."
"however, synthesis as an activity was necessary to determine whether nucleobase pairing was as simple as the watson-crick model implied. in this undertaking, it soon became clear that more than one heterocyclic system would support, or \"implement\", any particular nucleic acid hydrogen bonding pattern. for example, among natural nucleobases, uridine and pseudouridine both present a acceptor-donor-acceptor hydrogen bonding pattern. those seeking to meet the grand challenge of creating an artificial genetic system needed to decide which heterocyclic system to synthesize to implement each of the orthogonal hydrogen bonding patterns."
"supports laboratory darwinism, in order to allow the power of darwinism to \"finely tune\" molecular systems to convert poor binders or catalysts into good ones. darwinism cannot act prospectively. therefore, it cannot anticipate what functional groups will be needed to solve future problems in binding and catalysis. thus, the limits of darwinism (it is a bad innovator, but an excellent refiner) complement the limits of design (a good innovator, but a bad refiner)."
"when an outlier is detected using the previous criterion, our strategy consists of resampling it instead of discarding the corresponding diffusion orientation from the set of available dw data. corrections are performed through resampling from the nonoutlier dw images in the q-space. a decomposition of the dw signal is performed over the non-corrupted orientations, by using the modified spherical harmonics basis (sh) proposed by frank [cit] : for acquisitions performed on a single shell q, the signal of each voxel can be decomposed on this basis ψ:"
"the framework is validated with a benchmark with empirical data on the v161 stirling engine 25, 26, 27, 30 . table 4 illustrates the results of the validation and shows that the accuracy of the model is suitable for the purpose at hand of conducting tradespace exploration in a preliminary design phase. a bottom-up cost estimating approach was also used to increase confidence on the validation, using cost data available to the authors provided as a courtesy of the russian start-up company thermal motors llc that develops 5 kw prototype stirling engines. the small difference in results provided by the model and the bottom-up approach provided additional confidence on validity of the model."
"at this point, the stage was set to attempt experiments that applied laboratory darwinism to create aegisbodies, which are dna-like molecules built from expanded genetic systems that we hoped would parallel protein antibodies, but without requiring the animal."
"in the relations (3) and (4) it is assumed that the wall of heat exchangers is a perfect thermal conductor and therefore has no temperature change through the wall. for system level model this assumption is considered as acceptable but has to be revised for a more detailed design. the two heat transfer coefficient estimates are then compared; if the difference between estimates fall within a predefined threshold, the result obtained with the first model is used; otherwise, the results from the model derived from garcia is used. the rationale behind this choice is the fact that garcia's model was derived from empirical data of high output stirling engines that are closer to the applications considered in this paper. reconciliation of these two models with additional data will form part of future work. the outputs of the model are validated using available data from the v161 engine. this empirical data is used to validate modeling for alpha-type engines, while the engine design guide provided in martini's work 26 is used to validate the models of beta-type and gamma-type engines. table 2 shows the results of calculation by the model for the engine with input design parameters of v161 stirling engine."
"the combination of both the resampling of outlier slices and the registration between orientation volumes was critical [cit] . however one may wonder which step should be performed first. on the one hand, if an orientation volume still presents irregularities in the diffusion signal, it will be difficult to realign it in 3d. on the other hand, the resampling of corrupted slices may be wrong if the volume is spatially shifted in comparison with the reference. we performed the outlier resampling first, because this step is made slice-by-slice, and would thus be wrong anyhow should the slices be tilted previously by the 3d registration. moreover, the initial 3d misregistration that we observed between volumes in all infants was verified to be small enough to guarantee the reliability of the outlier slice resampling."
"considering the infant dataset where motion-related artifacts were introduced, the slices that were corrected for random outliers presented a mean normalized deviation in dw signal of 5.1% ± 0.6% in comparison with the reference and on average over the different outlier numbers. 34% ± 4% (resp. 11% ± 3%) of voxels within the corrected slices showed signal differences higher than 5% (resp. 10%). the number of random outliers had no influence on these deviations (fig. 4.a) . concerning the resampling of outliers stemming from vibration-related artifacts, the mean normalized deviations and the percentages of voxels with differing signals were higher and increased with the number of outliers (fig. 4.a) ."
"another direction to deal with motion in dw images is to apply post-processing correction strategies, which definitely help improve the precision and accuracy of the metrics estimation in dti [cit] and hardi imaging [cit] . the most common registration technique corrects for eddy current distortions and 3d motion [cit], and is based on mutual information between diffusion orientation volumes and a reference volume, with a subsequent rotation of the b-matrix before analysis of dw images [cit] . integrating motion in the signal model used for the tensor estimation seems to perform superiorly compared with the conventional method [cit] . in pediatric patients, an automated reconstruction software has recently been implemented [cit], but it requires a dedicated acquisition for nyquist ghost calibration and parallel imaging grappa weight. these post-processing strategies hardly correct for within-slice artifacts, which are frequently observed in rapidly moving subjects like infants or due to mechanical vibrations or spike noise. the easiest solution to deal with these artifacts is to exclude the whole corrupted volumes on a simple visual basis, but it is time consuming, dependent on the experimenter and it also potentially removes uncorrupted slices. automatic detection of outliers has previously been performed through linear correlation coefficients between dw volumes [cit], during a robust estimation of the diffusion tensor [cit], or by finding the local maxima on the laplacian of dw signals across diffusion orientations [cit] . for the correction of detected outliers, methods include removing such voxels [cit] or volumes [cit], fitting the signal using linear regression methods [cit], or interpolating the q-space signal directly on the spherical shell [cit] . recently, an algorithm which detects and removes outliers prior to 3d resampling, while taking misalignment into account, has been proposed [cit] . despite their respective advantages, all these approaches also present some drawbacks: rejecting instead of correcting the corrupted data, making hypothesis on the diffusion model, etc."
"this strategy further improved the quality of resulting dti maps, as outlined by rgb maps (fig. 5) . when outlier slices were either not rejected or resampled (strategies #1 and #4), remaining artifacts were seen with a color-code corresponding to corrupted orientations (see arrows in fig. 5b ). the red color (right/left orientation) corresponds to the read echo-train axis that often shows artifacts because it is highly solicited when the dw orientation is along the x-gradient axis. when 3d motion registration was not performed (strategies #1, #2 and #3), the bundles' delineation was blurred and questionable (see arrows in fig. 5c ), particularly in the sub-cortical white matter and the corpus callosum. in quiet infants, all rgb maps were relatively similar, except that the correction of eddy current distortions (strategies #4 and #5) enabled to reduce artifacts over the whole brain and obvious discrepancies at the frontal and occipital borders (see arrows in fig. 5a ). finally, the 2-step strategy provided the maps with the highest quality (fig. 5.5 ), appearing more reliable and less artifacted than maps obtained after correction with restore ( fig. 5.6 )."
"in addition to this parametric estimate, the heat transfer coefficients are also calculated using non-dimensional characteristics of the v161 [cit] 29 . this model assumes the following relationship between wetted area and swept volume :"
"the remainder of this paper is structured as follows. section 2 provides further background on previous research and development efforts on stirling engines and system design methods. section 3 defines the design framework that has been developed for the optimization of the design of stirling engines under performance and cost evaluation metrics; the section also discusses the validation of the framework through benchmark with known stirling engine designs. section 4 describes results of tradespace exploration based on the proposed framework, and successively uses the tradespace to identify optimal stirling engine designs for the whr and chp case studies described previously. the section illustrates the results for these two cases and derives recommendations for engine designers. lastly, section 5 draws conclusions from the paper and lays out avenues for future research."
"with this setting-up, the detection was fully automated and reproducible, and the factor f was the only parameter to be tuned once for a specific protocol. the detection was performed on a sliceby-slice basis. given the acquisition time of a slice (200 ms), intraslice motion generally corrupts the whole slice: artifacts may not be visible in some regions of the brain; nevertheless it does not imply that the signal sampling is reliable in such regions. besides, vibration-related artifacts corrupt clusters of voxels within a slice, but it remains difficult to limit the borders of the regions with impaired signal. consequently methods which correct the artifacts locally (either on a voxel-by-voxel basis [cit] or with a moving average window) may fail to correct the wholeness of such artifacts, and rejecting the whole slice may be more reliable. in this study, only a qualitative comparison between our method and restore was performed, but it appeared that using the diffusion tensor model was quite unsuccessful to detect outliers in both infants and an adult dataset locally corrupted for a specific orientation of the diffusion gradients. combining the two approaches (mi criteria on the whole slice and local constraint on signal intensity) would be a perspective to improve the outlier detection."
"this section describes the formulation and the underlying assumptions of the proposed stirling engine system design framework. the framework allows to obtain pareto frontiers and study the effect of different design parameters of the engine, namely heating and cooling temperature, engine bore and stroke, and engine type -alpha, beta, gamma -evaluating their effect on system-level technical and economic figures of merit, including power, efficiency, and capital cost. the proposed design framework integrates four interdependent domain models ( fig. 1) : (i) a thermodynamic cycle model for engine cycle parameters estimation, (ii) a heat exchange model to estimate cooling and heating surface areas of heat exchangers, (iii) an economic model for the estimation of capital cost of stirling engine designs, and (iv) a tradespace model 24 identifying tradeoffs between different engine architectures. the models i, ii, and iii feed each other as indicated by the arrow in figure 1 . the tradespace model enumerates possible engine architectures, generate a tradespace based on models i, ii, and iii, and provides graphics for downsizing the tradespace to identify pareto optimal engine architectures. the following paragraphs describe the four building blocks of the framework."
"2.2.5. evaluation of the correction strategies: optimization over the infant group 2.2.5.1. implementation of motion correction strategies. for each infant, experimenter jd performed visual rejection of corrupted volumes (strategy #2): whole volumes were rejected if they presented typical signal dropout (\"black stripes\" when viewed from the side), while volumes with minor irregularities in the diffusion signal were kept (see fig. 2 for examples) ."
"we were not the first to hypothesize that an expanded genetic alphabet might be obtained by shuffling hydrogen bonding units. alex rich, some 20 years earlier, had recognized that isoguanine (a natural product) and isocytosine might possibly form a third pair [cit] (the first generation s: [cit], 6, 53 4 of 13 in figure 1 ). independently, geoffrey zubay proposed another alternative pair [cit], not recognizing that his hypothetical structure for the small component lacked the aromatic planar geometry that the watson-crick model suggested was necessary for nucleobase stacking. even the writer of et. the extraterrestrial understood the possibility; et has dna built from six nucleotides, \"inosine and a pyrimidine we cannot identify\" [cit] ."
"the most interesting results from this first aegis-live experiment were that: (1) binders appeared after 9-12 rounds of selection, compared to the 15-20 rounds that have been reported as necessary to observe the growth of binders in previous cell-selex experiments with standard nucleic acid libraries; and (2) the z and p nucleotides, when present, were necessary for binding. [cit], contained 1 p and 1 z, which are both necessary for binding, and had a k d of 30 nm."
"resampling of the detected outliers can be performed from the non-outlier dw images, either in the image space, or in the q-space. the former strategy is not adequate because the tissue local microstructure can change significantly between neighboring slices: interpolating the signal can introduce incoherence, partial voluming effects, and consequently lead to the mixture of heterogeneous populations of fibers. resampling the corrupted data in the q-space is more robust. in a recent study sharman and collaborators [cit] also considered this correction strategy after manual detection of the outliers. two smoothing steps were used: a first spatial smoothing in the image space, using a gaussian filter, which can only be applied to low b-values acquisitions and a second q-space smoothing applied to the five closest neighbors using a weighted mean, which restricted the smoothed estimates to a very small quantity of dw data and imposed that they remain of good quality. following smoothing, q-space interpolation was performed directly on the spherical shell in the native dw signal space [cit] . in our approach, the resampling of outlier slices relied on the decomposition of the dw signal on the modified spherical harmonics basis [cit], which is a natural basis on the sphere. contrarily to the geodesic resampling approach [cit], it made use of all non-corrupted data, rather than on a restricted neighborhood. thus, it required a smaller number of valid data to compute a robust diffusion model estimation, and increased robustness to local corruption on the spherical shell, for instance when dw data contained artifacts around a specific orientation because of gradient system instabilities. since using a nonparametric model may lead to overfitting of the signal, the spherical harmonics decomposition was limited to the 6 th order. in addition, a"
"this has a parallel in standard live with four nucleotide alphabets. sequences rich in g appear this molecule is the first example of an aptamer that has evolved towards a specific protein target from an expanded genetic alphabet using the molecular biology and analytical chemistry that has been developed to support it, but without the escamotages used in other systems to circumvent the limitations of extant biology."
"however, it is clear that polymerases that have evolved over billions of years to manage g:c and a:t pairs are not yet entirely \"up to speed\" when challenged to reproduce aegis dna. thus, while results from aegis-live do not suggest a preference for certain regions of the expanded aegis sequence space, they do suggest a slow loss of aegis pairs during the pcr amplification that occurs between selection steps."
"the study of system design requirement using the developed stirling engine system design framework was conducted for two potential applications: waste heat recovery (whr) and combined heat and power (chp) generation. stakeholders and potential customers interviews were conducted to formulate system-level and design-level requirements that have been used as inputs to the design framework. the following section discusses the results that have been obtained by the application of the framework for the two applications being considered, and illustrates how results drive the formulation of recommendations on high-level design decisions to engine developers."
"these second generation improvements are summarized in figure 1, in the right panel. the effort produced much new knowledge in heterocyclic chemistry. we applied chemical synthesis to create several generations of heterocycles that implemented the aegis concept on heterocycles with increasingly improved chemical stabilities [cit], tautomeric ratios [cit], study melting temperatures, and duplex stability [cit] . within the framework of the watson-crick pair, a rather comprehensive view of what heterocycles are possible in genetic systems emerged."
"the fourth model integrated in the design framework is a tradespace model 31 of the stirling engine. the tradespace model allows the identification of pareto optimal se architectures based on desired system requirements. table 3 illustrates the design parameters that have been considered in the model. the total number of engine architectures generated by the model illustrated in table 4 is 750. the small size of this tradespace allows a full combinatorial exploration of all design alternatives. engine cost (to minimize), engine power (to maximize), and engine efficiency (to maximize) are used as evaluation metrics for the tradespace in order to identify pareto-optimal designs. figure 3 in the results section illustrates the tradespace under these different dimensions."
"for example, it seems unusual to those familiar with the design of artificial molecular recognition systems to have genetic information entrusted to flexible molecules. most exercises in molecular recognition seek rigid size complementarity in the binder and the bindee. aegis allowed us to explore the flexibility of the dna backbone and, by implication, its ability to enforce size complementarity. for example, a pair between two small nucleobases (a small:small pair) could stabilize the duplex, if they were joined by three hydrogen bonds, but less than a size complementary small:large pair. indeed, a small:small pair joined by three hydrogen bonds stabilized the duplex as much as a small:large pair joined by just two hydrogen bonds [cit] . these biophysical studies were followed by crystallographic studies that showed that aegis components do, in fact, pair with watson-crick geometry. for example, the introduction of a single z:p pair into the stem of a riboswitch marginally increased the stability of that stem; a crystal structure showed essentially no geometric perturbation [cit] (figure 2, right) . in dna, the crystal structure of a single z:p pair likewise showed no substantial deviation from watson-crick geometry [cit] . indeed, duplexes with two or six z:p pairs retain their overall watson-crick geometry [cit] (figure 2, left) . with these tools in hand, we created a molecular biology to support laboratory in vitro evolution (live) using aegis components. this includes in particular polymerases that are able to replicate aegis dna. this challenge was approached two ways: by screening and optimizing commercially available polymerases [cit], and by engineering polymerases by directed evolution to incorporate non-standard nucleotides [cit] . although neither approach produced enzymes or protocols that were absolutely immune to aegis losses and/or aegis/standard dna mismatches (see the sections below), these studies set the stage for the development of laboratory in vitro evolution experiments with aegis libraries (aegis-live). rna polymerases [cit] and reverse transcriptases [cit] that interconvert aegis dna and rna, and restriction enzymes that cut specifically around aegis dna [cit] were also developed."
"comparison of methods to correct outlier slices (strategy #3): adult dataset. our method to correct outliers was qualitatively compared with a widely-used approach (restore [cit] ). because the steps for outlier detection and 3d motion registration are not applied in the same order in these two approaches (outliers are first corrected with our method, and secondly excluded with restore), we focused on data without 3d motion by considering the adult dataset corrupted with vibration-related artifacts for 1 orientation."
"with these motivations, the benner group developed parts of an artificially expanded genetic information system (aegis, figure 1 ) [cit] . aegis is a biopolymer similar to dna [cit], but with 12 building blocks, which in turn creates the opportunity to attach a number of functional groups that might be useful in binding and even catalysis. figure 1 shows some examples of what has been made. especially noteworthy is the use of the nitro group, which is a polyvalent general binder (remembering for example the ability of nitrocellulose to bind biomolecules)."
"in filter-binding assays, pa1 was found to be strictly specific for the 63-kda version of protective antigen, and unable to bind to pa83, indicating that the pa1 epitope might at least in part reside on the portion of pa83 that becomes exposed after cleavage of the 20-kda subunit. extensive mutational and enzymatic analysis revealed that the two p were essential for binding to pa63, and that the folding of the molecule included a four base-pairs stem surmounted by a large highly compact and nuclease resistant loop ( figure 5a ). while the parent molecule that was originally selected had a kd of ~2.3 μm, subsequent truncations of the molecule reduced the binding constant to ~50 nm (pa1t4). most interestingly, although expected from the structural studies mentioned above, when two base pairs in the stem were substituted with two p-z pairs (pa1t4ppzz), the molecule's kd lowered further to ~35 nm ( figure 5b ). this is one example of the effectiveness of post-selection design, where a molecule's features can be improved by the addition of strategically placed non-natural elements."
"dti and hardi techniques are sensitive to motion in two ways. first, because they are based on 2d acquisition, motion artifacts may be observed on isolated slices. in addition, misregistration can occur between dti volumes because of the successive acquisition of several diffusion orientations, and because of eddy current distortions in epi images. it seems intuitively important to correct images corrupted by such motion-related artifacts before estimating the diffusion models. actually, it has been shown that the contributions of motion and noise are of the same order of magnitude at 3t, and that both are influenced by the choice of sampling scheme [cit] . motion is generally due to the subject, some of which being more susceptible to move than others, like infants, children and patients. but artifacts can also result from technical problems: in many mri systems, instabilities of the gradients can lead to spikes [cit], and the table may vibrate due to low-frequency mechanical resonances, which are stimulated by the low-frequency gradient switching associated with the diffusion-weighting [cit] . this leads to corrupted data along specific gradient orientations."
"an adult moved on purpose in a first acquisition and remained motionless in a second scan to get a reference unbiased dataset. for another adult, because of technical problems, data were corrupted with vibration-related artifacts (located in the occipital lobe over 25 slices) for orientations of the diffusion gradients along the x-axis. from this dataset we selected 5 datasets of 25 orientations including 1 to 5 artifacted orientations."
"the advancement in development and commercialization of stirling engines has been historically prevented by two primary limiting factors, namely limited commercial applications, and technological issues 12, 13 . two exception areas where stirling engines have been more successful are chp and solar power generation due to raised awareness in climate change by industry 14, the deregulation of the power industry, and the aging of the centralized power infrastructure currently in use in many countries 15 . technological issues hindering the development of stirling engines include lubrication, weight and size, and control of the engine 12, 13 . the technological issues mentioned are factors that lead to high capital costs for the engine, hence making this solution non-attractive for applications where traditional internal combustion engines have been historically more successful. nevertheless, investments in stirling egine technology hold the promise to overcome these issues, hence lowering capital costs and making these prime movers attractive to the market."
"the 2-step correction strategy was validated on datasets with various motion-and vibration-related artifacts, and it was successfully applied to dti data of the infant brain. since no hypothesis on the diffusion model is made, it can be used to correct any dataset acquired over a single shell in the q-space (e.g., dti and hardi local models) and could be easily extended to multiple-shell acquisitions. so it is worth applying this correction in all dw data with potential sources of artifacts. as an example, it here enabled to reliably study the developing cortico-spinal tract, in agreement with previous studies of unmoved datasets [cit] ."
"alternatively we here propose a global post-processing methodology for automatically correcting all motion-related artifacts in dw images before computing the diffusion model. it is based on two successive steps: 1) automated detection and 2d resampling of slices corrupted by motion or technical problems (mechanical vibrations, spike noise); 2) 3d realignment of orientation volumes misregistered due to inter-volume motion and distortions stemming from eddy current. this correction strategy was applied on dti data from 20 non-sedated infants, aged from 6 to 22 weeks. first, the two steps of the methodology were validated by simulating motion in an uncorrupted dataset. second, we applied this strategy to all infants, and we studied quantitatively the immature cortico-spinal tract, because its development has already been detailed over this age range."
"in comparison with the strategy based on visual rejection of whole corrupted volumes, our method presents several advantages. it is fully automated and quick, independent from the experimenter, and is performed slice-by-slice rather than volume-by-volume, which is a particularly suitable when a single slice is corrupted. by introducing outliers on purpose in a set initially not corrupted by motion, we observed that filling in the missing data was equivalent to rejecting these data in terms of fa estimation, but it performed better when considering the estimation of the main eigenvector direction, particularly for outliers along a specific orientation (vibration). furthermore, over the whole infant group, the final numbers of corrected volumes were smaller, and the resulting dti maps were equivalent for both strategies. the main advantage of filling in the missing data was to make the number of dw measurements constant from voxel-to-voxel before the computation of the dti model. nevertheless it remained that the number of recovered orientations differed across slices. besides, as the accuracy of the sh decomposition estimation increases with the spatial distribution of the diffusion orientations, it is important to combine our correction method with dw acquisition strategies that optimize the spatial orientation distribution according to the acquisition time and the motion hypotheses [cit] ."
"moreover, we established analytical chemistry to support aegis-live, including tools to analyze folding in functional aegis molecules (circular dichroism, x-ray crystallography) [cit], and computational platforms to model the fold of functional aegis molecules based on biophysical studies of aegis dna [cit] . of particular importance was the development of a strategy to convert aegis dna to standard dna in a traceable manner, which allows for aegis high-throughput sequencing [cit] . this method takes advantage of the ability of polymerases to mismatch aegis nucleotides with standard nucleotides when the firsts are missing from the amplification mixture. removing one aegis nucleotide at a time in parallel amplification reactions of the same target produces a \"mismatch pattern\" that allows for the identification of the type and location of the aegis nucleotide in the original sequence."
"moreover, we established analytical chemistry to support aegis-live, including tools to analyze folding in functional aegis molecules (circular dichroism, x-ray crystallography) [cit], and computational platforms to model the fold of functional aegis molecules based on biophysical studies of aegis dna [cit] . of particular importance was the development of a strategy to convert aegis dna to standard dna in a traceable manner, which allows for aegis high-throughput sequencing [cit] . this method takes advantage of the ability of polymerases to mismatch aegis nucleotides with standard nucleotides when the firsts are missing from the amplification mixture. removing one aegis nucleotide at a time in parallel amplification reactions of the same target produces a \"mismatch pattern\" that allows for the identification of the type and location of the aegis nucleotide in the original sequence."
"the library used in this selection contained a 20-nt long random region composed of actgzp nucleotides, and surrounded by primer binding sites made of standard dna. this library was subjected to 12 rounds of selection that gradually reduced the number of cells and incubation times per cycle in order to increase the selection pressure. the surviving pool of cycle 12 was subjected to aegis to standard dna conversion, and submitted for high-throughput sequencing. with these tools in hand, we created a molecular biology to support laboratory in vitro evolution (live) using aegis components. this includes in particular polymerases that are able to replicate aegis dna. this challenge was approached two ways: by screening and optimizing commercially available polymerases [cit], and by engineering polymerases by directed evolution to incorporate non-standard nucleotides [cit] . although neither approach produced enzymes or protocols that were absolutely immune to aegis losses and/or aegis/standard dna mismatches (see the sections below), these studies set the stage for the development of laboratory in vitro evolution experiments with aegis libraries (aegis-live). rna polymerases [cit] and reverse transcriptases [cit] that interconvert aegis dna and rna, and restriction enzymes that cut specifically around aegis dna [cit] were also developed."
"second, in the adult dataset with intentional movements, the two steps (strategy #5) were combined to correct motion artifacts. for both the corrected and the uncorrected datasets, errors in terms of dw signal, direction of the main eigenvector and fa were computed relatively to the reference dataset without motion and compared in order to evaluate the correction effects."
"for the adult dataset with vibration-related artifacts for 1 orientation over 25, our approach could provide correct rgb maps contrarily to restore (supplementary fig. 1 )."
"however, this also indirectly supports the hypothesis that added pairs and added functionality are valuable during the selection step. thus, the best aptamers that these four experiments have delivered retain aegis z or p (or both) despite the bias towards z/p loss during the amplification step."
"with these motivations, the benner group developed parts of an artificially expanded genetic information system (aegis, figure 1 ) [cit] . aegis is a biopolymer similar to dna [cit], but with 12 building blocks, which in turn creates the opportunity to attach a number of functional groups that might be useful in binding and even catalysis. figure 1 shows some examples of what has been made. especially noteworthy is the use of the nitro group, which is a polyvalent general binder (remembering for example the ability of nitrocellulose to bind biomolecules)."
"the schematic of this aegis-based cell-selex is shown in figure 3 . in this case, the synthetic aegis-library contained 25 random nucleotides (actgzp), and a negative selection step was added to each cycle to increase specificity to the target cell line. again, only 13 rounds of selection were needed to observe the growth of binders. several aptamers were obtained from deep sequencing, and a quantitative improvement in dna performance with added nucleotides could definitely be seen. indeed, the aptamers with the lowest kd contained z and/or p residues, and were as low as 14 nm, while actg-only aptamers had kd ranging from 326 nm to more than 1 μm (figure 4) . moreover, the aptamers were very specific for the cell line used during selection, and did not bind to other cell lines. this feature was not fully achieved in the first attempt at aegis-cell-selex, where some aptamers also bound to some other cell lines."
"this experiment required only 11 rounds to produce populations showing substantial binding during bulk reactions. many of the selected aptamers bound with various levels of specificity to gpc3; others appeared to bind to different proteins on the cell surface, perhaps newly emerging on the host cells as a consequence of their being engineered to express gpc3. interestingly, the k d for these aptamers were in the low to high nm range when tested on a hgpc3-positive human liver cancer cell line, hepg2. one aptamer in particular, lg5, which carried one z residue but no ps, showed a high level of specificity for the intended target regardless of the cell line expressing it, and had an apparent k d (for hepg2) of 6 nm. again, aegis nucleotides were found to be essential for binding."
"for the adult dataset moved on purpose, the deviation errors computed according to the reference unmoved dataset were relatively high in terms of dw signal (mean normalized deviation: 18%; percentage of voxels with differences n5%: 73%; percentage of voxels with differences n10%: 51%), tensor main eigenvector ) and fa estimation (mean normalized deviations: 38%; percentage of voxels with differences n 5%: 90%; percentage of voxels with differences n 10%: 80%). the 2-step correction strategy enabled to significantly reduce these errors for dw signal (mean normalized deviation: 10%; percentage of voxels with differences n5%: 62%; percentage of voxels with differences n10%: 35%) and tensor main eigenvector direction (angle errors: 31°for voxels with fa [0.1-0.3]; 32°for voxels with fa [0.3-0.5]; 33°for voxels with fa [0.5-1]). for fa estimation, the errors remained high (mean normalized deviations: 32%; percentage of voxels with differences n5%: 89%; percentage of voxels with differences n 10%: 80%), probably because the corrected dataset did not match entirely the reference dataset, which was not corrected for 3d motion and eddy current distortions. these results highlighted the correct performances of our correction approach but also the difficulty to compare successive acquisitions that present intrinsic variability: spatial variability due to varying head position, and signal variability caused by different technical tuning."
"we proposed in this study a post-processing approach relying on two successive and uncorrelated steps, which were first validated by introducing selected motion artifacts or discrepancies on different datasets, and comparing the corrected datasets with reference ones. it was further tested on dw data obtained in non-sedated infants, who frequently move during mri acquisition: it successfully corrected sets corrupted by motion, while it had lower influence on uncorrupted data."
"we were not the first to hypothesize that an expanded genetic alphabet might be obtained by shuffling hydrogen bonding units. alex rich, some 20 years earlier, had recognized that isoguanine (a natural product) and isocytosine might possibly form a third pair [cit] aegis nucleotides complete the watson-crick pairing concept by shuffling hydrogen bond donor and acceptor groups, forming additional orthogonal nucleobase pairs. they resemble natural nucleotides in size, shape, and pairing geometries, are independently replicable, and they do not interfere with dna double helix structures that are fundamental for the recognition of the polymer by many natural enzymes. most importantly, aegis nucleotides increase the information density of a nucleic acid molecule of a given length, and have the potential to increase functionality."
"the schematic of this aegis-based cell-selex is shown in figure 3 . in this case, the synthetic aegis-library contained 25 random nucleotides (actgzp), and a negative selection step was added to each cycle to increase specificity to the target cell line. again, only 13 rounds of selection were needed to observe the growth of binders. the most interesting results from this first aegis-live experiment were that: (1) binders appeared after 9-12 rounds of selection, compared to the 15-20 rounds that have been reported as necessary to observe the growth of binders in previous cell-selex experiments with standard nucleic acid libraries; and (2) the z and p nucleotides, when present, were necessary for binding. [cit], contained 1 p and 1 z, which are both necessary for binding, and had a kd of ~30 nm."
"in many cases, the first heterocyclic system that was prepared to implement each of the four additional hydrogen bonding patterns turned out not to be the best heterocycle to support genetics. several first-generation aegis components suffered from chemical defects, which are indicated in magenta in figure 1 . for example, the pyrazine heterocycles that were first used to implement the pyadd and pydda hydrogen bonding patterns epimerized rapidly [cit] . the purine ring system used to implement the pudda hydrogen bonding patterns had a substantial amount of a minor tautomer that created nucleobase pairing ambiguity [cit] . in a long process documented in the literature, second generation implementations of various bonding patterns were then synthesized to fix problems in the new dna."
"this has a parallel in standard live with four nucleotide alphabets. sequences rich in g appear to be favored in functional xna molecules, which is likely because g can form a series of non-watson-crick interactions that allow the xna molecules to fold. however, polymerases have difficulty replicating g-rich regions, causing pcr to be selectively biased against them. this fight between what is desired in the selection step, and what is tolerated in the amplification step in natural xna may be a parallel to what we see with aegis alphabets."
"wetted area and engine swept volume are derived for the heater, regenerator, and cooler of the v161 stirling engine, as illustrated in table 1 . the wetted area for regenerator is calculated using only the relation described in table 1 due to the lack of empirical estimations for the regenerator heat transfer coefficient. the wetted area surfaces for the heater are estimated as:"
"combining the resampling of outlier slices and the registration for 3d motion and eddy current distortions (strategy #5) modified a similar volume number than the 3d registration alone (strategy #4): 16.5 ± 7.1 volumes on average over infants for differences larger than 1% (range: 6.5-28.5); 4.4 ± 6.4 for differences larger than 5% (range: 0-23.7) ( table 1 ). in comparison with the other approaches, this strategy enabled to drastically homogenize the mi coefficients over the 30 orientations in moving babies, while it implied no changes in quiet babies (fig. 3.2 ): the standard deviation over the 30 orientations (normalized by the mean) was the smallest with strategy #5 for most infants (except for 2 for whom differences were less than 0.2%)."
"in a third aegis-cell-selex experiment, cell engineering was used to place glypican 3 (gpc3), a possible marker for liver cancer diagnostic, on the surface of a murine cell line that does not express gpc3 (1mea). this engineered cell line (1mea hgpc3 ) was used in positive selection cycles, while wild-type 1mea cells were used in counter-selections [cit] . in this case, the library's randomized aegis region was 35 nt long."
"these results suggest promise, with much more work to be done. first, it is clear that aegis represents something new in the field of molecular recognition, and in synthetic biology. the \"new\" feature is its ability to evolve across what appears to be unconstrained sequence space. this is likely because the aegis pair still fits into the \"aperiodic crystal structure\" proposed by schrödinger as being necessary to support darwinism [cit] . as the crystal data show, even if multiple aegis pairs are adjacent, the structure of the double helix survives."
"correcting the 3d-motion after introducing translations or rotations on purpose to the uncorrupted dataset triggered relatively small errors in comparison with the reference, and there were no influence of the motion kind or amplitude (for up to 5 mm and 5°). these errors were smaller in comparison with the outlier resampling, in terms of dw signal (on average over all translations and rotations, mean normalized deviations: 2.7% ± 0.5; percentage of voxels with differences n 5%: 14% ± 4%; percentage of voxels with differences n 10%: 5% ± 2%), tensor main eigenvector direction consequently, correcting such 3d motion with our approach appeared to be worthwhile and efficient."
"the library used in this selection contained a 20-nt long random region composed of actgzp nucleotides, and surrounded by primer binding sites made of standard dna. this library was subjected to 12 [cit], 6, 53 6 of 13 in order to increase the selection pressure. the surviving pool of cycle 12 was subjected to aegis to standard dna conversion, and submitted for high-throughput sequencing."
"this hope was greatly magnified by the view that life on earth itself had experienced an episode of natural history where nucleic acids (likely rna) were the only genetically encoded compounds of biological catalysis, performing many of the roles that are now performed by proteins [cit] . the existence of protein biosynthesis machinery (the ribosome) that was rna in its catalytic part reinforced this enthusiasm [cit] . likewise, the role of rna fragments in cofactors with a wide distribution across the terran biosphere that makes their antiquity likely, further leads to the apparent reasonableness of this \"rna world\" model."
"simulations of motion. we further selected the data from a single infant who had not moved at all during the acquisition (subject #8 from table 1, middle age of 11.7w) and compared the corrected datasets (after simulating different kinds of motion) with the reference dataset (the real uncorrupted dataset). on the one hand, random intraslice motion was simulated by introducing different numbers (from 1 to 5 over 30) of outlier orientations in a given slice (by making the dw signal aberrant). ten random sets of corrupted orientations were tested for each number of outliers, and 3 random slices were independently corrected. on the other hand, vibrations and miscalibration of a gradient were simulated by corrupting gradient(s) around a specific diffusion orientation: the read axis (along which the echoplanar echo-train is collected) was considered because it is highly and frequently on demand in mr scanners, and it may induce mechanical vibrations due to the coupling of the gradient coil, the subject itself and the table [cit] . all slices of the corresponding volumes were considered as outliers since such artifacts affect the whole volume. the strength of the vibrations was taken into account by increasing the conic angle of the corrupted dw orientations. for our specific set of 30 orientations (siemens package vb15), it concerned 1 orientation (angle 0°), 2 orientations (up to 11.5°around x), 3 orientations (up to 23.5°), and 5 orientations (up to 31.5°). for both the simulated random motion and the vibrations, the corrected datasets (with resampling of the outlier slices-strategy #3-or by exclusion-strategy #2) were compared with the acquired reference dataset. first, the resampled dw signal within each voxel of the outlier slices was compared with the reference signal in order to investigate the impact of the number of outliers and of the kind of motion (random or vibration) on the resampling performance. mean normalized deviation was computed as"
"the development of the framework helped to support complex system level recommendations in stirling engines design for the two intended whr and chp applications. alpha-type stirling engine designs are advantageous over beta and gamma to minimize capital cost while generating more electricity at the same time. nevertheless, the framework shows how alpha-type engines are not advantageous in all ranges of engine output powers. each engine type appears to be power-cost pareto optimal for a specific output power range. it is therefore important to know early in the design the power output range of interest to the customers to be targeted. another important conclusion of the study conducted the beta-type engines feature advantages in terms of maximizing efficiency while minimizing capital cost of the engine design. this conclusion is valid in almost the entirety of the range of calculated efficiencies, with the exception for low efficiency -low cost engine design, where gamma engines hold an advantage. wide commercialization of low power -low cost gamma engines already in place in industry 33, 34, 35 is an empirical proof for advantageous architectures of gamma type engines for low power applications (such as model toys and tool kits)."
"when both a corrupted volume and a 3d-moved volume were simultaneously introduced in the uncorrupted infant dataset, the outlier detection step finely detected the corrupted volume, whereas the 3d-moved volume was not detected for translations up to 5 mm. less than 3 peripheral slices were detected for rotations up to 5°( 1 slice for 1°and 2°rotations, 2 slices for 3°rotations), except for 5°r otation along z (14 slices detected). since smaller amplitudes of 3d motion are generally observed in infants, this simulation justified to perform the outlier detection step before the 3d volume registration."
"besides, a resampling of dw images was performed for all strategies (even if no correction or registration was performed) in order to align the anterior and posterior commissures (ac-pc) in a single slice. this realignment aimed to provide a consistent color coding on directionality rgb maps across all infants, despite the variability in brain positions that resulted from how the infant fell asleep. for strategies #4 and #5, this resampling was applied jointly with the 3d-motion registration by composing the two transformations. after the ac-pc placement all orientation volumes resulted in 60 slices, the first and last of which being possibly cut or empty when the anterior and posterior commissures were already well-aligned in the initial brain orientation. thus only the 40 central slices were considered when a global estimation of the corrections over the brain was required."
"where u ! represents a normed vector coding for the diffusion orientation. this decomposition is limited to the 6 th order to avoid overfitting, and some regularization is introduced by descoteaux and colleagues to improve its reliability [cit] . the resulting sh coefficients are used to compute the \"theoretical\" signal values along the orientations corresponding to rejected outliers. thus, in a given slice, corrupted dw images are replaced by these images interpolated onto the sh basis computed from the set of non-corrupted dw images. such interpolation should be applied to images with relatively high signal-to-noise ratio (greater than~4) as the laplace-beltrami regularization imposes a gaussian noise model. note that the outlier detection and resampling are performed first, independently for each slice and before the 3d volume registration, because rapid-motion artifacts generally corrupt 2d slices. consequently, we cannot exclude potential contributions from \"inter-volume\" motion. nevertheless, these contributions are expected to be small in comparison with the potential impact of 2d outliers on the 3d registration, and not the whole volume is modified when only a single slice is corrupted (this hypothesis was tested by simulations in section 2.2.4.5). furthermore, this approach does not rely on strong hypothesis concerning the diffusion model, except that it can be decomposed onto an sh basis, contrarily to a previous approach which considered a diffusion tensor model [cit] ."
"the heat exchangers model includes equations that are used to calculate heat transfer coefficients and consequently the area of heat exchangers. heat transfer coefficients are estimated using two different models. [cit] 28 who based results on experimental testing of small stirling engines. this work provides empirical equations for heat transfer coefficients in engine heat exchangers linked with engine cycle parameters, as follows."
"the proposed system framework for stirling engine design allows to couple technical and economic parameters of the engine to conduct tradespace exploration and formulate recommendations to engine designers to identify nondominated engine designs. the framework allows a reduction of the amount of time and resources required to find non-dominated engine design, while comprehensively looking at a broad array of engine alternatives, compared to the heuristic \"trial and error\" approach commonly used in industry. the study presented in this paper reveals a preference on alpha-type engine designs for industrial waste heat recover, and beta-type engines for combined heat power generation for residential use, under the requirements elicited through stakeholder interviews of potential customers for the engine. gamma-type engine designs prove to be effective and efficient design solutions only at the range of small power outputs, as also witnessed by the wide use of this type of stirling engines in the toy and residential tool kit industries in the market."
"the development of aegis was supported by synthetic chemistry [cit], mechanistic chemistry [cit], enzymology [cit], protein engineering [cit], structural biology [cit], and natural darwinism [cit] . here, these worked together to not only improve aegis, but also to lead to a deeper understanding of how natural genetic biopolymers work."
"in a third aegis-cell-selex experiment, cell engineering was used to place glypican 3 (gpc3), a possible marker for liver cancer diagnostic, on the surface of a murine cell line that does not express gpc3 (1mea). this engineered cell line (1mea hgpc3 ) was used in positive selection cycles, while wild-type 1mea cells were used in counter-selections [cit] . in this case, the library's randomized aegis region was 35 nt long."
"in this experiment, the six-letter actgzp aegis library contained only 25 randomized positions flanked by 15 nucleotide-long primer binding sites composed of standard dna, with the aim of selecting for the smallest binders available in the~10 14 sampled different sequences (out of~10 19 total possible sequences). the target, pa63, was presented to the library immobilized to magnetic beads; binding oligonucleotides were recovered magnetically; and aegis-pcr with a single biotinylated primer was performed directly on survivors bound to the bead-coupled pa63. after 14 cycles of selection, the binder population had grown by~30%, and the library from this cycle was subjected to transliteration and deep sequencing. one sequence named pa-apt1, or pa1, dominated the surviving pool, with 96% of the total reads. this contained two p residues and, similar to the other five most represented sequences, no z nucleotides, despite z being present at 17% frequency in the starting library."
"our post-processing strategy takes advantage of a high diffusion orientation number to correct for corrupted (also called outlier) images. it relies on two successive steps: 2d resampling of the outlier individual slices, followed by 3d registration and correction of the eddy current distortions in the resulting volumes (fig. 1) . it is implemented within brainvisa [cit] in the connectomist toolbox [cit] ."
"imaging the diffusion of water molecules by mri enables the noninvasive exploration of the tissues' microstructure. this is done by making the mr signal sensitive to spin motion through the application of diffusion gradients during acquisition [cit] . to explore the anisotropic structure of tissues, such as the fiber organization of white matter, diffusion-weighted (dw) images are currently acquired along several orientations of the diffusion gradients taken on a single shell in the q-space, for a fixed b-value, with models like diffusion tensor imaging (dti) and high angular resolution diffusion imaging (hardi). in dti, mr measurements are performed along at least 6 orientations of the diffusion gradients. in comparison with data averaging, increasing the number of orientations also improves the signal-to-noise ratio (snr) of the resulting diffusion maps. on condition that orientations are uniformly distributed over the space [cit], it further enables a more precise spatial and angular estimation of the diffusion model, thus improving the local estimation of the spatial organization of tissues. but it also increases the acquisition time and thus the risk of motion artifacts. increasing the b-value improves the reliability of diffusion models, but decreases the snr. hardi models, such as q-ball imaging (qbi), better explore the tissue microstructure and anisotropy, but the acquisition of a high number of diffusion gradient orientations is required. therefore, a compromise between image quality and acquisition time must be found."
"in this experiment, the six-letter actgzp aegis library contained only 25 randomized positions flanked by 15 nucleotide-long primer binding sites composed of standard dna, with the aim of selecting for the smallest binders available in the ~10 14 sampled different sequences (out of ~10 19 total possible sequences). the target, pa63, was presented to the library immobilized to magnetic beads; binding oligonucleotides were recovered magnetically; and aegis-pcr with a single biotinylated primer was performed directly on survivors bound to the bead-coupled pa63. after 14 cycles of selection, the binder population had grown by ~30%, and the library from this cycle was subjected to transliteration and deep sequencing. one sequence named pa-apt1, or pa1, dominated the surviving pool, with 96% of the total reads. this contained two p residues and, similar to the other five most represented sequences, no z nucleotides, despite z being present at 17% frequency in the starting library."
"stirling engines (se) are prime movers that employ an external combustion mechanism to generate heat and power for a variety of uses. the stirling engine architecture has been considered for a wide array of applications, including combined heat and power generation (chp) 1, 2, solar power utilization 3, 4, underwater vehicle 5, 6, and deep-space exploration 7, 8 . compared to other prime movers, the main advantages of stirling engines include an external combustion process that allows operating the engine with higher reliability, cleaner emissions, lower noise and vibrations 1 . these characteristics make the stirling engine an interesting solution for residential and small-size business chp-generation where reliable, fuel-flexible, and cleaner power generation is required 9 . as part of their operations, stirling engines can utilize an external heat supply from hot gases. this characteristic makes these engines suitable to be used for waste heat utilization 10, a commercial application currently receiving an increased interest from industry 11 ."
"laplace-beltrami regularization term was used to better deal with noise removal and avoid any kind of overfitting with spurious spikes that may be present in the signal acquired on a shell of the q-space. the resampling was validated for 1 to 5 outlier orientations over 30, leading to small errors in diffusion signal (~5%, fig. 4a ) in comparison with acquired images. but it may fail in case of severe motion when most orientations are corrupted, and then prospective strategies that adapt the acquisition according to the subject's motion should be preferred [cit] . this correction approach is well indicated for any dw acquisition involving spherical samplings. it can be applied not only to the tensor model, but to most hardi and multiple shell models since the spherical harmonics decomposition can be generalized to acquisitions with multiple q samplings [cit] . for such protocols, our method would be particularly useful because the long acquisition times lead to more probable motion. besides, the regularization relying on the laplace-beltrami operator makes the decomposition implicitly robust to noise, and the novel implementation using a modified lmmse approach [cit] also makes it robust to rician and non-central chi noise, which is observed in high b-values acquisitions."
"however, it is clear that polymerases that have evolved over billions of years to manage g:c and a:t pairs are not yet entirely \"up to speed\" when challenged to reproduce aegis dna. thus, while results from aegis-live do not suggest a preference for certain regions of the expanded aegis sequence space, they do suggest a slow loss of aegis pairs during the pcr amplification that occurs between selection steps."
"these results suggest promise, with much more work to be done. first, it is clear that aegis represents something new in the field of molecular recognition, and in synthetic biology. the \"new\" feature is its ability to evolve across what appears to be unconstrained sequence space. this is likely because the aegis pair still fits into the \"aperiodic crystal structure\" proposed by schrödinger as being necessary to support darwinism [cit] . as the crystal data show, even if multiple aegis pairs are adjacent, the structure of the double helix survives."
"however, overall, the performance of nucleic acid aptamers has not rivaled the performance of other binding molecules that can be generated with respect to specific targets [cit] . most notable among these, of course, are antibodies raised against antigens in living animals. over the same time period, antibody replacements that are still proteins, but are created by diversity science that does not"
"where n is the number of voxels in the outlier slice excluding voxels in the surrounding noise. the percentages of voxels with signal values different for more than 5% or 10% of the reference were evaluated. second, we assessed the errors in the estimation of the direction of the main tensor eigenvector e v ! : the averaged angle between the reference and the corrected eigenvectors was computed third, we focused on the corrected fa maps generated from the 30 diffusion orientations after resampling or exclusion of outliers, and we reported differences within slices relatively to the reference fa map in terms of mean normalized deviation and percentages of voxels with fa values differing by more than 5% and 10% as previously mentioned. for each measure, the average values and standard deviations were computed over all the considered slices and over all the corrupted sets of orientations within a given number of outliers."
"2.2.5.3. dti quantification over the infants group. because the range of ages was restricted to a short developmental period, linear models between dti parameters in the cortico-spinal tract and age provided the best fits across the infants group as compared with quadratic models. for each correction strategy, we computed correlation coefficients r, as well as mean, minimal and maximal values over the group, and standard deviations after taking into account the significant linear age-related effects. for the strategies comparison, note that lower standard deviations mean better registration across babies, and thus better motion correction. higher fa values also mean better delineation of the fasciculus according to surrounding tissue and less partial volume effect."
"two different applications for stirling engines, drawn from industry and the residential market, are considered as use cases for this paper. this discussion hence illustrates how the stirling engine design framework may be used to set system-level requirements for stirling engines. table 5 illustrates the needs of the two user categories of interest, and their mapping to system-level requirements for the engine design. two primary needs have been identified as characteristic of industrial applications: optimize technological processes, and to reduce electricity payments to the centralized power grid. needs can then be mapped to system level requirements, namely the maximization of efficiency and maximization of power outputs. maximization of efficiency means that the maximum amount of useful energy is recovered for each unit of waste heat supplied. on the other hand, maximization of power outputs means that one converter will need to produce as much power as possible in a given range of design parameters. in a similar fashion, figure 2 helps to identify the remaining design parameters of the alpha-type engine. the model identified higher temperature values for this engine architecture to be optimal; this result thus suggests that this engine type could be optimally employed for waste heat industrial applications -heat treating furnace and cement kiln. these applications were derived from the classification of waste heat potential for different industries, provided by the us department of energy 32 ."
"unfortunately, a quarter century on, this hope has not been realized as comprehensively as had been hoped. for sure, numerous binding molecules have emerged from selection applied to both dna and rna libraries [cit] . these \"aptamers\" have seen use in research and, to a limited extent, medicine [cit] . some of these successes have been reviewed in this special edition [cit] ."
"this molecule is the first example of an aptamer that has evolved towards a specific protein target from an expanded genetic alphabet using the molecular biology and analytical chemistry that has been developed to support it, but without the escamotages used in other systems to circumvent the limitations of extant biology."
"however, this also indirectly supports the hypothesis that added pairs and added functionality are valuable during the selection step. thus, the best aptamers that these four experiments have delivered retain aegis z or p (or both) despite the bias towards z/p loss during the amplification step."
"the study was performed on two adults and 20 healthy infants born at term (details in table 1 ). the mri protocol was approved by the regional ethical committee for biomedical research, and all subjects or parents gave written informed consents. infants were non-sedated and spontaneously asleep at the beginning of mr imaging, but some of them moved during acquisition (see the results section for details). particular precautions were taken to minimize noise exposure, by using customized headphones and covering the magnet tunnel with special noise protection foam."
"at this point, the stage was set to attempt experiments that applied laboratory darwinism to create aegisbodies, which are dna-like molecules built from expanded genetic systems that we hoped would parallel protein antibodies, but without requiring the animal."
"correction strategies were evaluated on brain dti images of adults and of non-sedated infants, as these subjects are particularly prone to motion during mr acquisitions. five strategies were compared: #1 no correction, #2 visual rejection of corrupted volumes, #3 resampling of outlier slices alone, #4 3d motion registration alone and #5 2-step correction strategy (corresponding to strategy #3 followed by strategy #4). first, the outlier detection was evaluated in adults' data with vibration-or motion-related artifacts. motion was also simulated to further validate the method with ground-truth knowledge stemming from an uncorrupted infant dataset: corrupted slices were introduced randomly (simulation of random \"intra-slice\" and \"intra-volume\" motion) or around a specific orientation (simulation of a systematic equipment vibration effect) to test the resampling of outlier slices (strategy #3); random translations and rotations were also introduced to test the 3d motion registration (strategy #4). second, the two steps (strategy #5) were combined to correct real motion on an adult who moved on purpose. third, the five strategies were applied to the whole infant group, and we focused on the cortico-spinal tract, as an example of a welldescribed fasciculus, relatively mature in the developing brain."
"all these results together suggest that artifacts due to technical problems (vibrations) may impair the robustness of dti quantification more than a reasonable random motion. even if the strategy of exclusion is less sensitive than the signal geodesic interpolation on the diffusion shell, errors cannot be avoided when the signal sampling is missing around a specific orientation. in our analyses, resampling outliers according to the spherical harmonics basis appeared as the most reliable strategy."
"in a stepwise type of analysis, aegis-pa63 aptamers were also tested against anthrax protective antigen biological activities. first, aptamers were found to bind to pa63 when this is already associated with its natural cell-receptor cmg2 (capillary morphogenesis protein 2, also known as antxr2), further narrowing the possible epitope of pa1 on its target surface, excluding the pa63 binding site to cmg2. further, electrophysiology assays showed that pa1t4 competed with the lethal factor (lf) for binding to pa63 when this is assembled in its channel state across an in vitro membrane system ( figure 5c ). in a stepwise type of analysis, aegis-pa63 aptamers were also tested against anthrax protective antigen biological activities. first, aptamers were found to bind to pa63 when this is already associated with its natural cell-receptor cmg2 (capillary morphogenesis protein 2, also known as antxr2), further narrowing the possible epitope of pa1 on its target surface, excluding the pa63 binding site to cmg2. further, electrophysiology assays showed that pa1t4 competed with the lethal factor (lf) for binding to pa63 when this is assembled in its channel state across an in vitro membrane system ( figure 5c )."
"the infinite-time lqr quadratic cost function is of the general form (9) where is the state gain matrix, is the control effort gain matrix and is the state vector. the infinite-time cost function was selected to allow convergence duration to be guided by the varying optimal gains. although the process duration is allowed to be infinite, in practicality the close proximity maneuver duration is finite. also, in a majority of close proximity maneuvers the final state conditions are zero (near origin) which results in a negligible end-point cost component. therefore, in our research we eliminated the end-point cost component and adopted the infinite-time lqr cost function."
"comprehensive performance evaluation of the lqr/apf and apf control algorithms was conducted for close proximity docking maneuvers. results are shown for docking operations involving the simultaneous maneuvering of six chase spacecraft to a target spacecraft. docking maneuvers require precise convergence to the outer boundary of a target spacecraft while avoiding collision. all close proximity operations begin when the chase spacecraft are within 1.0 km of the goal. for comparison, close proximity docking operations are subdivided into relatively near and far maneuvers based on the chase spacecraft initial position. in the near maneuvers each chase spacecraft starts approximately 100 m away from the goal, [cit] ."
"in this paper we have presented a design methodology that enables the deployment, parameterization and integration of hardware ips into soc platform at multiple levels of abstraction. for this, we have introduced ip deployment capabilities in marte, which aim at facilitating the import of selected low-level features into the high-level models, their modification, and the creation of an ip-xact design description that is used to parameterize and integrate the underlying ip descriptions."
"the proposed methodology ( figure 2 ) is based on a cnn model, known as u-net, to outline agricultural plots boundaries automatically. to overcome the lack of labeled data sets needed to train the model, in this work, open-access lpis data was used. the main steps of the proposed methodology are detailed below."
"the main disadvantage of recent efforts in applying mde to soc design has been precisely in the passage from the high-level models to the code generation. the parameters and interface information of the ip have to be readily available in the high-level models. therefore, the deployment phase must also provide a mechanism for retrieving the information of ip cores that are usually coded in languages such as vhdl. some approaches have performed this mapping manually, whilst others have defined deployment meta-models to link both levels. however, these meta-models remain highly methodology dependant and do not promote ip reuse."
"the final stage and ultimate goal of rendezvous is assumed to be here the docking of multiple spacecraft.. in particular, the case of six chase spacecraft docking on the six sides of a cubic target is considered, with each chaser able to dock on any of the simulation results for the six chase spacecraft simultaneously sample docking maneuvers with collision avoidance are listed in table iii, with the far docking results listed in table iv . for these collision avoidance maneuvers, the goal positions are docking ports on the surface of the target spacecraft. the maneuver is completed when the assigned chase spacecraft approaches within 2.0 mm of the center of its target docking port. for these maneuvers, the docking ports are centered on each side of a cubic target stationary spherical obstacles are placed at positions along each chase spacecraft's unobstructed path. these obstacles have a diameter of 2.0 m and are placed directly along the chase's spacecraft path when the chase spacecraft is at maximum relative velocity. in each case, collision avoidance of the stationary obstacles and other spacecraft was successful. the apf algorithm forces a strict return to the desired velocity once an obstacle is avoided. so, the apf tends to pull the spacecraft around obstacles faster than the lqr/apf, but risks saturating available actuation. the lqr/apf maneuver durations tend to be slightly longer due to smoother transitions in and out of obstacle regions. comparison of the control requested and the saturation limits for both algorithms, illustrates the smooth performance of the lqr/apf algorithm. the relative velocity and acceleration of one of the six chase spacecraft for the docking maneuver of the second row of table iv is shown in fig. 3 . the upper left plot shows the rsw relative velocity response with the lqr/apf control. the upper right shows the rsw velocity response of the apf control. the lower left plot shows the rsw relative acceleration response of the lqr/apf control. the lower right plot shows the rsw relative acceleration response of the apf control. the central spike in velocity and acceleration, at approximately 600 s, is due to the stationary obstacle along the path. the acceleration oscillations toward the end of the maneuver are due to the collision avoidance of the target and the other simultaneously docking chase spacecraft. the dashed line on the acceleration plots show the thruster saturation limits. the desirable performance of the lqr/apf with collision avoidance is evident due to the excellent control effort response."
"imultaneous autonomous control of multiple spacecraft maneuvers will be required for several planned space missions in the near future [cit] . large spacecraft formation tracking and station keeping has received a great deal of study, but research in the area of multiple spacecraft close proximity operations is limited [cit] . there are numerous mission scenarios that involve the convergence of multiple spacecraft in close proximity [cit] . present close proximity path planning and tracking algorithms are in general computation- ally expensive, and often require manual backup [cit] . therefore, a relatively simple and completely automated control algorithm is desired which allows for multiple spacecraft close proximity operations. research and experience with terrestrial-based robots have matured the application of artificial potential function (apf) robotic navigation and control algorithms. the simplicity of the apf-based control algorithms is a good match for spacecraft applications with limited proximity sensors and processing capability. indeed, for these applications, global knowledge is assumed not to be available to each spacecraft [cit] . also, a centralized controller is assumed not to exist, such that each spacecraft must perform their portion of the operation with local information and limited communications. previously proposed spacecraft apf-based controllers have been very task specific and not applicable to the full range of possible close proximity operations [cit] . also, studies of their efficiency have primarily been focused on maintaining spacecraft formations [cit] developed a decentralized control algorithm for multiple agents using navigation functions under the assumption that the environment is perfectly known and stationary. the algorithm makes an assumption that each agent disappears once it is sufficiently close to its goal. this assumption excludes its use for the multiple spacecraft docking application considered in this paper."
"dimarogonas and kyriakopoulos [cit] investigated the rendezvous problem of multiple nonholonomic unicycles. since smooth feedback laws do not exist for nonholonomic systems, the proposed control law is discontinuous and time-invariant."
"the attractive and repulsive velocity shaping functions, and, respectively, allow for velocity damping around regions of concern. this ensures that the chase spacecraft slows as it approaches the goal position and avoids obstacles. balancing these parameters allows the goal position to be placed in the center of a spacecraft and the control algorithm to converge to the surface of the target spacecraft. this is vital capability for docking maneuvers."
"the broad use of ppp requires a faster convergence than today's l1/ l2-based gps solutions. this paper has provided a ppp solution that exploits the highly accurate position, clock and bias estimates of next-generation gnss kepler and the low noise level of the wideband signals on e1, e5 and e6. thereby, the convergence time was reduced from 30 minutes to less than 5 minutes without any prior information."
"critical evaluation of multiple spacecraft control algorithms requires high fidelity six degrees of freedom (6-dof) spacecraft models. most proposed spacecraft control algorithms have not been fully assessed with realistic spacecraft dynamics, kinematics, and constraints. the spacecraft physical characteristics and actuator constraints must be included in order to determine if a spacecraft control algorithm is practical and valid. therefore, we use a high fidelity nonlinear model for validation of the multiple spacecraft proximity control algorithm."
"performance evaluation requires that each maneuver is successfully accomplished without collision. the overall time duration of the maneuver, in s, and control efficiency measured by the required, in m/s, are used as the primary metrics for evaluating the performance of the control algorithm. these two metrics are roughly inversely related to each other. however, since these metrics are a result of the minimization of a cost or potential function with numerous constraints the relationship is not simple. in this research, the maneuver duration for the close proximity operations is desired to be approximately 30 minutes. the control effort was desired to be both efficient and reasonable with limited actuation. heavily saturated control effort in the collision avoidance environment is a safety concern. control effort that heavily saturated the realistic and limited actuators is denoted with an asterisk in the tables."
"in order to help determine the threading of the protein sequence through the ssts, a computational method, such as jpred [cit], is used to predict the subsequences (sequence segments) of the protein sequence that are likely to be the secondary structures. these subsequences are then mapped the ssts. the topology of the ssts refers to their order with respect to the protein sequence and the direction of each sst. for example, in fig. 1, d 1 through d 18 represent ssts and s 1 through s 18 represent the subsequences on the protein chain that correspond to the secondary structures. in this case, ssetracer was able to detect 18 of 20 helices (red sticks in fig. 1a) fig. 1a and the dots/crosses shown in fig. 1b ), since the sequence of a protein has a direction."
"several works have tackled the use of marte in soc design, specifically at the deployment level, such as the mopcom [cit] and gaspard [cit] frameworks. they are both based in a y-schema co-design approach. the main disadvantage is that, as with many other mde methodologies, both approaches make use of nonstandardized deployment representations. this means that the deployment models obtained by these methodologies are not interchangeable, making them highly methodologydependant. another issue is that the parameters are retrieved from the implementation files in a non-automatic way (e.g. annotated as comments in uml)."
"we have modeled the system in figure 7 using papyrus. the platform model is converted into an xmi file, which is parsed in mdworkbench and then used to perform the mode transformation described in section 5.3, in order to obtain an ip-xact design description. then, by using the xpsf meta-models and models transformations, we obtain the mhs file, which is fed to xps, to obtain the vhdl description which, after synthesis, can be used as an input for the partial reconfiguration xilinx design flow. table 4 summarizes the required times to achieve each of the transformations, from the marte model conversion into a top level netlist, along the number of lines of each of the intermediate representations. it must be noted that the longest time corresponds to the synthesized top-level netlist which relies completely on the xilinx tools. each of the steps described in table 4 has been automated, facilitating the task of the creator working in high-levels of abstraction, one of the objectives of using marte in our approach."
"at this point, and for a better interpretation of the results, it is important to noted that the available gt does not fully cover the 21 tiles, which could be a handicap for the correct evaluation of the performance of the methods compared."
"the radiation being emitted by the sun exerts a force on the spacecraft. the magnitude and direction of the solar-radiation force is dependent on several factors, such as the position of the sun relative to the spacecraft, the attitude and shape of the spacecraft, the intensity of the solar-radiation and the reflectivity of the spacecraft [cit] . for this research, the spacecraft is treated as a \"black body,\" which absorbs all radiation. with this assumption, the solar pressure on a spacecraft orbiting earth is . solar pressure and atmospheric drag impart, in general, both a disturbance force and a disturbance torque."
an obstacle repulsive region of influence may cause a local minimum or saddle point to occur in the area between the obstacle outer region of influence and the surface of the obstacle. the location of this local minimum or saddle depends on the obstacles location with respect to the goal position. this local minimum or saddle can cause difficulty if the overall potential function is the only driving function for determining control effort.
"tanner and kumar studied the formation problem of multiple agents using navigation functions [cit] . a local navigation function is built for each agent. trajectories of individual agents also decrease a centralized navigation function, thus converge to the formation goal."
"this section begins with a brief description of the traditional model for absolute carrier phase and pseudorange measurements for precise point positioning (ppp). subsequently, we adapt our model to the proposed next generation gnss -kepler, i.e. we exploit the benefits achieved through optical inter-satellite links."
"however, the computational management of this large amount of data has required a reduction in the spatial resolution of the original data. this has made it possible to feed the network with images covering an area of about 20 ha, which provides contextual information on the environment of the agricultural plots which, with the original resolution and current means of calculation, is difficult to approximate."
"numerous simulations of multiple spacecraft close proximity maneuvers were conducted in order to generate a sample distribution of maneuver parameters. using monte carlo methods, estimates of the mean and standard deviation of and were determined. two hundred convergence, rally, rendezvous, and docking maneuver simulations were conducted for both the apf and lqr/apf control algorithms [cit] . the docking maneuver results are presented in this paper. each simulation involved six chase spacecraft performing simultaneous maneuvers. the law of large numbers permits the approximation of sample statistics via monte carlo methods. as expected, the large sample size generally approaches a gaussian distribution. the normalized data distribution allows for estimates of the maneuver parameter means and standard deviations. the statistical data is presented in both per spacecraft and per maneuver format. the per spacecraft statistics use each chase spacecraft of each maneuver for a total sample size of 1200 spacecraft. the per maneuver statistics use the maximum parameters of each maneuver for a total sample size of 200. the average close proximity maneuver the video clip, created by using the software package agi satellite tool kit [cit] s (33 minutes). for presentation purpose, the animation is running at the speed of about 100 times faster than real time."
"our previous methods were mostly tested using the true positions of secondary structures. in this present study, we have taken a significant step by establishing an efficient algorithm to address the increased computational cost due to the alternatives. fig. 4 . topologies derived from the cryo-em density maps. secondary structure positions derived from the true structure and those predicted using sympred [cit], jpred [cit], and psipred [cit] are shown from the outer circles to the inner circles for protein 4v68_br(pdb id) in (a) and 3iz6_k (pdb id) in (b). the a-traces (thicker sticks) and the b-traces (thinner sticks) were detected from the experimentally-derived cryo-em map emdb_5030 in (a) and emdb_1780 in (b) using ssetracer [cit] and strandtwister [cit] . the true topology (shown in rainbow colors from blue/n-terminal to red/c-terminal) is ranked 47th for emdb_5030 and 2nd for emdb_1780. the connecting trace was identified from the skeleton using dp-toss. the true structure (ribbon) is superimposed for each."
"in order to obtain the boundaries of the agricultural plots of a complete tile, the following post-processing strategy is followed. first, a padded version of the tile obtained through a mirror padding strategy is divided into several overlapping patches, from which the u-net model performed the segmentation. during the prediction phase, random transformations are applied to the test patches. in this way instead of showing the regular images only once to the trained model the various versions of the images are shown several times improving the chances of identifying the target and predicting it accordingly [cit] . in this work, we used the eight transformations of the dih4 group. once the model provides the segmented images, the inverse transformation is performed on them, in such a way that all the pixels agree. the final segmentation of a tile is given by a soft voting that returns the final class labelȳ i as arg max of the sum of predicted probabilities:"
"xilinx psf files are structured a textual format, which can easily be understandable by machines by defining a parser, but the first mandatory step if the meta-model definition. the ecore formalization of these meta-models does not exist by nature, and has to be entered, in uml for instance. we have created meta-models for the different xilinx files used in edk, such as the mhs and the mpd, among others. these meta-models reside in the sodius tool, where the models transformations take place."
"in this paper, we introduce a deployment approach used for deploying xilinx platform studio (xps) [cit] ip cores. we make use of ip-xact as an ir between the marte models and the files used by xps to implement the soc platform. we transform the xps files into ip-xact components that are subsequently converted into marte models, used in the deployment phase of our approach."
"the ppp solution is determined with a standard linear kalman filter. in this section, we briefly describe the state space model and the initialization of the state parameters."
"to the receiver clock offset. the receiver phase bias is also mapped to the receiver clock, and the receiver code bias is mapped to the pseudorange multipath errors. the obtained parameter mapping is given by:"
"in general, the apf of each spacecraft is determined by the arithmetic superposition of the goal and all obstacle potential functions in its working area [cit] . the overall potential field will serve as the performance surface for the control algorithm, of the form (13) where is the attractive potential of the goal point and is the repulsive potential of obstacles. the gradient of the attractive potential is essentially the position vector pointing from the chase spacecraft to the goal, with its magnitude being shaped by a number of parameters to obtain the desirable behaviors."
"in order to make easier for the reader to visualize the 3-d trajectories of six spacecraft in the field filled with obstacles, a video clip has been submitted with this paper of the complete docking maneuver using lqr/apf, listed in the bottom row of table iii (see also fig. 4 )."
"considering that deep learning (dl) techniques have proven useful as a tool in understanding agricultural processes [cit], dl methodologies for outlining agricultural plots seem to be a solution to overcome the drawbacks of the aforementioned approaches."
"several atmospheric models are available. in this work, an exponential atmospheric density model based on the u. s. [cit] was used [cit] . this model, adopted by the u. s. committee on extension to the standard atmosphere (coesa), is valid for low earth orbits (leo) orbits with altitudes between 100 km and 1000 km [cit] ."
"the balancing factor between spacecraft relative position and control effort efficiency is the relative convergence rate. however, the relative spacecraft dynamics causes rendezvous challenges if the relative convergence rate is too slow or rapid. if the rate of convergence is slow the goal position is spirally orbited as the minimal control actuation is used. the slow convergence can dramatically increase the maneuver duration as the spacecraft approaches close to the goal position. on the other hand, if the rate of convergence is too rapid limited actuation will result in collision danger due to relative position overshoot and oscillation. for this research, converge maneuvers were required to be of an over damped nature. this ensures safety upon arrival to goal locations which are being approached by other spacecraft."
"chase spacecraft acceleration due to collision avoidance is determined from the summation of all obstacle influences, such as (33) this is similar to (27), however now there is a velocity and an acceleration term."
"xilinx xps makes use of a series of files defined in the platform specification format (xpsf) document [cit], which formalizes the description of different components in the xilinx design flow for processor-based systems. these files are used as an abstraction of the ips implementations, and as a means to configure the ips used in the platform via a top-level description. the vhdl description of an ip contains only information about the in/out ports, and in the best case, generics allowing the designer to parameterize and customize it. if the vhdl implementation had to be associated with a high-level description (typically containing parameters and bus interfaces), there will not be an easy and automatic way to determine which ports of the ip belong to a bus interface, and to use additional information important for the design flow."
"the apf obstacle velocity function, represented in (23), is a gaussian function which is equal to one at the obstacle boundary. this function serves as our lqr/apf velocity shaping parameter due to obstacle position (30) it will be multiplied by the component of the chase spacecraft relative velocity toward obstacle, . this ensures the chase spacecraft slows to zero at the boundary of the obstacle."
"future work should focus on: (1) evaluating the generalization capacity of cnn models to be used in other agricultural land cover, and (2) developing an automatic dl tool for spatial-temporal characterization of agricultural land uses, allowing a sustainable agricultural management, and (3) develop semi-supervised dl approaches (expertin-the-loop approach) to update agricultural parcel boundaries in order to reduce labor-intensive use of labor for this task."
"afterwards, the components in the marte platform are linked to those in the library, obtaining automatically a description that is converted to an ip-xact design. this description contains the information of the deployed components, which is fed to a model transformations tool. in this way, an xps platform which is used by the xilinx tools to obtain synthesizable vhdl code is created."
"currently, the lqr/apf multiple spacecraft close proximity control algorithm is undergoing ground and flight testing. it was successfully implemented and tested at the synchronized position hold engage and reorient experimental satellites (spheres) facility at the massachusetts institute of technology space systems laboratory [cit] . this successful ground testing enabled execution of spheres flight testing inside the international space station (iss) [cit] ."
"the dynamic nature of agricultural activities (e.g., crop rotation, subdivision or consolidation of fields, temporary fallow) generally makes agricultural plots unstable both in boundaries and land use [cit] . therefore, in order to reduce the risk of paying sanctions due to the improper identification of agricultural lands [cit], the lpis should be updated regularly (i.e. once a year). in general, the creation and updating of each lpis is mainly done by photo-interpretation using very high resolution orthophotos [cit], which makes it a laborious process which is susceptible to human error [cit] . in this respect, efforts should focus on the automation of workflows for the outlining parcels in cadastral maps [cit] . in the literature on remote sensing, automatic and semi-automatic methods have been proposed to outline the boundaries of agricultural plots. most of them are based on image segmentation, edge detection algorithms and classification models, or combinations of these techniques."
"these changes are automatically updated in the mhs files by parsing the corresponding mpd file and checking for any dependencies on parameters. however, the creation of the platform in xilinx edk is not completely automated, and a lot of steps still need to be performed manually; for instance, importing ips into the platform, their interconnection and parameterization. all these steps require a great deal of design effort and expertise of the tool and this is precisely one of the advantages of using the proposed methodology: by using a high-level description, the designer does not to know all the specificities of the used tools, which often are difficult to grasp by people who are not proficient into fpga design and vhdl."
"many secondary structure prediction methods are available and some of them provide online services. although certain methods are more accurate than others, overall, we observed that no single method is superior to any other for all the secondary structures. for example, certain helices are predicted more accurately by sympred, but others are predicted more accurately by different methods (fig. 2) . to utilize the advantage of all the prediction methods, it is always important to use all of the predictions. however, doing so results in significant computational overhead. we present a two-step approach in which alternative positions are generated from all predictions in the second step. a dynamic programming placement method is devised to quickly find the best alternatives that fit the constraints from all of the ssts. we tested the two-step approach on 12 large proteins. the rank of the true topology was used to evaluate the accuracy for each of the seven methods. the difference among the seven methods resides in the input of the secondary structure positions on the sequence obtained by (1) pdb, (2) sympred, (3) jpred, (4) psipred, (5) pred-ator, (6) sable, and (7) all five of the online predictions. intuitively, the best accuracy comes from the use of the true secondary structure positions on the sequence. amazingly, this is true for only four of the 12 cases (columns 3, 9, table 2 ). since the ssts detected from the image are not 100 percent accurate, using the true sequence position for the helices in the matching is not always the best approach. for example, the correct topology was ranked 4th for 3ltj when all five predictions were used, but it was ranked 9th when the true sequence segments of the helices were used (row 4, table 2 ). since small helices are generally harder to detect from both the image and the sequence, missing them from both sources appears to be more favorable than having them in only one of the two sets. our results clearly indicate that the two-step approach that utilizes all five secondary structure predictions is the most accurate approach from among the seven different methods. the true topology rank is the highest among the other five methods (use of sympred, jpred, psipred, predator, and sable) when all of them were used for all 12 proteins in the test (column 9, table 2 ). for example, true topology was ranked 15th for 2xb5 when all five of the online secondary structure prediction methods were used to generate alternatives. the true topology of 2xb5 was ranked 40th, 44th, 40th, 75th, and 53rd, respectively, when sympred, jpred, psipred, predator, and sable were used individually. we observed substantial enhancement in ranking of the true topology when multiple secondary structure predictions were used for 10 of the 12 cases. we noticed that these 10 cases are the largest 10 of the 12 cases, with their lengths ranging from 201 to 585."
"the associate editor coordinating the review of this manuscript and approving it for publication was marco anisetti . nutritious food for a growing world population; and, at the same time using natural resources more sustainably while making an effective contribution to climate change adaptation and mitigation [cit] . for agriculture to be sustainable, agricultural practices must take full advantage of technology, research and development and adapt to local requirements. adequate statistics, geo-spatial information, maps and qualitative knowledge are needed for the planning and volume 7, 2019 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/ management of the agricultural sector [cit] . on the other hand, a symbiosis between technical and investment-oriented organizations is also necessary. at the european level, the common agricultural policy (cap) fosters green and sustainable agriculture 1 by offering compensation payments to farmers. the geographical location and size of the agricultural parcel play a key role in determining which payment entitlements may be eligible and for which payment may be claimed for [cit] . in this regard, the land parcel identification system (lpis), a gis inventory similar to cadastre, has been created as a control mechanism to verify the eligibility of subsidies, and monitoring of plots with respect to selected environmental rules and rural development programs [cit] . the european union (eu) member states have their own lpis which follows common guidelines. in addition to its original purpose, lpis information has proven useful for studying various aspects of agricultural activity [cit], such as the effect of land fragmentation and carbon dioxide emissions."
future work may include research into the control algorithm robustness with random initial configurations and measurement uncertainty. the control algorithm may be further evaluated in a hardware-in-the-loop laboratory test-bed. development of the nps autonomous multi-agent physically interactive spacecraft (amphis) is being conducted and should allow for future testing and validation of multiple spacecraft control concepts [cit] .
"the major time in the two-step approach occurs at the mapping step in which the initial 1,000 top-ranked topologies are generated. the second step is a placement step, and it can be quickly done using the dynamic programming algorithm given in this paper. the time it takes to compute the initial top-ranked topologies and the placement of those topologies are shown in table 2, column 10. apparently the time is quite little. for example, to produce the top 1,000 topologies and to derive the optimal placement for those topologies only takes 14.89 seconds for 3ods, in which 16 of the 23 helices were detected from the image. the experiments in this paper were executed on a 2x intel xenon e5-2660 v2, 2.2 ghz server machine. the factors affecting the run-time include the number of secondary structures and the quality of the skeleton. we noticed that the skeletons produced from the simulated density images are often much better than those produced from experimentally-derived images."
"in order to establish the equations of motion between spacecraft we will consider one of the spacecraft as primary spacecraft (target) and each one of the secondary spacecraft (chasers). the hill-clohessy-wiltshire equations are the linearized relative motion equations, which assume that the two following conditions are met. first, the relative distance between the two spacecraft is assumed to be much smaller than the orbital radius of the target spacecraft (for instance, a relative distance of few kilometers versus an orbital altitude of several hundreds of kilometers). second, the target spacecraft is assumed to be on a circular orbit (angular velocity of the orbital coordinate system is constant)."
"finally, this research could play an important role in agricultural decision-making by converting remote sensing data into timely and accurate information, contributing to the development of sustainable agriculture and reducing the gap between information technologies and users."
"in this section we elaborate on the design effort required to implement the system detailed in figure 7, especially if we compare the proposed approach with a purely vhdl approach and, as in the case of the generation back end of this methodology, using xilinx edk. let us consider for instance the obtained vhdl top level design, which is generated in around 30 seconds by platgen; the top-level vhdl description contains 7986 lines of code, and mainly contains components instantiations, parameterization and signals declarations for interconnection. it is evident that creating such a design (composed of several components, and multiple sub-components) would take not only hours, but maybe days, in a process very prone to errors, as shown in table 5 . xilinx edk, using the platform specification format (psf, notably mhs and mpd files), makes the design process more amenable: the designer can start creating a design through an easy to use graphical user interface (gui), and then parameterize the design by choosing different options through ip specific tcl files and guis."
"for this research, the fundamental system is a 6-dof spacecraft orbiting the earth. the translational control algorithm employs linearized relative motion equations, while it is applied, in the numerical simulations, to the full nonlinear multiple spacecraft dynamics and kinematics model."
"the chase spacecraft desired velocity, expanded from (15)- (18) is (19) where and are the independent variables and all other terms are spacecraft maneuver dependent constants."
"in this section, we analyze the performance of ppp with kepler. the first subsection describes the simulation parameter. subsequently, we show the achievable accuracy and convergence behaviour for the absolute receiver position and clock offset, tropospheric zenith and ionospheric slant delays, carrier phase ambiguities and pseudorange multipath errors."
"in the figure 9 it can be observed that in these tiles the proposed model also provides a much greater precision than gpb-ucm for the identification of the agricultural plots, demonstrating their capacity for generalization. specifically, the bde values obtained by gpb-ucm for case 1 and case 2 were 12.45 and 10.45, respectively, whereas for the proposed approach the errors were 3.93 and 4.86. in addition, the proposed methodology, based on cnn models, successfully distinguishes between those areas with urban cover and those areas with agricultural cover, without postprocessing requirements. (fig. 9(e) )."
"the lqr/apf control algorithm's promising results, derived from short maneuver durations with limited control effort, paves the way for potential application to a wide range of multiple spacecraft close proximity operations. this research is based on conservative estimates of relatively far initial positions, short maneuver duration, modest volume of propellant, and a dense obstacle environment. the average chase should be able to perform several close proximity maneuvers. therefore, the lqr/apf algorithm appears to be a promising new development for the field of multiple spacecraft close proximity maneuver control."
"angel garcía [cit], and the ph.d. degree in advanced computing for science and engineering from the universidad politécnica de madrid. he is also an assistant professor with the universidad politécnica de madrid, spain. his research interests include object-based image analysis, image understanding, and image interpretation (remotely sensed and medical images). dionisio rodríguez-esparragón received the b.sc. [cit], the m.sc. [cit], and the ph.d. [cit] . he joined the departamento de señales [cit], where he is currently an assistant professor. his current research interest includes processing of high-resolution remote sensing imagery."
"the rest of the manuscript is organized as follows: section ii describes the data set used as well as its geographical location. section iii describes each stage of the proposed methodology. the results obtained as well as their discussion are presented in section iv. finally, the conclusions and problems to be addressed in future efforts are presented in section v."
"due to inaccuracy in the estimation of secondary structures, the determination of topology for ssts requires the exploration of alternatives. effective methods are needed to explore the large solution space that results from those alternatives. we propose a dynamic programming algorithm to find the optimal placement when a topology is given. this algorithm is combined with our previous mapping algorithm and the shortest k paths algorithm to form a two-step approach. a test using 12 proteins showed that the two-step approach improves the ranking of the true topology in comparison to using single consensus prediction. we demonstrate for the first time that computationally-detected helices and b-strands from an experimentally-derived cryo-em density image can be combined with multiple secondary structure predictions to rank the true topology near the top of the list."
"secondary structure prediction was performed using five online servers (sympred [cit], jpred [cit], psipred [cit], predator [cit], and sable [cit] ). sympred and jpred are consensus servers. the initial positions include the predicted positions using either sympred or jpred, whichever predicted a greater number of helices. these initial positions were used to obtain the initial topologies using dp-toss. alternative positions of each secondary structure were generated based on the results from the multiple secondary structure predictions."
"world food production needs to grow by 70% in developing countries to meet food demands of 9 [cit] . hence, the agricultural sector faces a critical global challenge: ensuring access to safe, healthy, and"
"to show the generalization capability of the proposed approach, two tiles (figure 9a and 9b ) corresponding to a geographical location different from that of the tiles used the training and testing (i.e., external set) were evaluated. the boundaries of the agricultural plots of these tiles were drawn by hand using gis software. despite the operator's knowledge of the study area and the digitization process, drawing the boundaries of the agricultural plots was a major, error-prone and time-consuming challenge."
"considering the state-of-the-art and the results obtained in this work, the methodologies for the automated outlining of agricultural plot boundaries should be considered as a complementary tool for the expert operator. one of the main limitations for the complete automation of the delineation process lies in the discrepancy between what is seen in the images and what is generally recorded as a plot, which can be influenced by issues such as land ownership (i.e., cadastral register) and the agricultural practices developed on the land (e.g., mixed crops)."
"the desired chase spacecraft acceleration due to obstacles is determined from the summation of all obstacle influences, such as (26) where the summation is with respect to all obstacles within the sensor range of the chase spacecraft. obstacles may be either other spacecraft (additional chase spacecraft converging toward a goal within the same region) or stationary obstacles in fixed positions relative to the goal location (for instance solar panels or thruster plume exclusion zones)."
"in each of the simulations, high fidelity 6-dof spacecraft orbits are propagated by numerical integration. in particular, a fourth-order runge-kutta method was used with a time increment of . this conservative 1.0 hz sampling rate was selected to allow for slow actuation cycles and sensor update rates. in our simulations, the thrust along each spacecraft axis is limited to a maximum acceleration of, which is based on a thrust force of and a spacecraft mass of . the maximum relative chase spacecraft velocity was selected to be . this is rapid enough to allow for timely convergence, while being manageable with limited spacecraft actuation."
"the accuracy and efficiency of the two-step approach were tested using 12 a-proteins and two cryo-em proteins that contain both a-helices and b-sheets. while a-proteins do not contain b-sheets, they provide test cases for large proteins. the length of the a-proteins ranged from 142 amino acids (1flp) to 585 amino acids (2xvv). therefore, the a-protein dataset is suitable for testing the efficiency of the method and its capability of handling large complicated cases in topology determination. for the a-protein dataset, the atomic structures were downloaded from the protein data bank (pdb), and they were used to simulate density maps at 10 a resolution using eman software. the two cryo-em test cases use experimentally derived cryo-em density maps (emd-5030-4v68_br and emd-1780-3iz6_k) downloaded from the electron microscopy data bank (emdb) [cit] . the atomic structures of chain br of 4v68 (pdb id) and chain k of 3iz6 (pdb id) were used to extract the density regions that correspond to the chains."
"the seven spacecraft simultaneous docking maneuver requires that all six chase spacecraft avoid each other while converging to within 2.0 millimeter of their assigned docking position on the target spacecraft outer boundary. the docking ports, on the center of each face of the target spacecraft, are randomly assigned to each chase. the individual spacecraft distribution statistics for and are listed in table vii . the lqr/apf performs well, with the simultaneous docking maneuvers being accomplished in longer durations and slightly more control effort. this is as expected due to the highly tuned and regulated velocity control during the apf's collision avoidance. any apparent apf control efficiency is only achieved by saturating the available control actuation. in contrast, the lqr/apf controller consistently demonstrated desirable control effort with tighter standard deviation. the lqr/apf's docking spacecraft distribution is shown in fig. 7 . the lqr/apf's docking spacecraft distribution is shown in fig. 8 ."
"this file is used by the xilinx tool to create the top-level description of the hardware platform the mhs description contains the same elements of the mpd file, with one difference: only the parameters, bus interfaces and ports that have been used by the user for the top-level description of the ip are displayed, as shown in figure 8 . the mhs file is created from an ip-xact design description, which contains as well component instances, parameters associated with them (named configurableelements in ip-xact jargon). the components in edk are associated to the implementation files using the instance name and hardware version values. ip-xact provides a mechanism, the vlnv value to link the components from the marte models to their ip-xact and edk/vhdl counterparts. the bus interfaces are inferred from the busref tags associated with the bus interconnection between the component instances. regarding the individual, top-level ports (typically used for ad-hoc connections between components or to external fpga pins), their values and tags are retrieved from the ah-hoc connections elements in the ip-xact description. table 3 shows the list of mapping between ip-xact and mhs."
"the attractive velocity is toward the goal position and the repulsive velocity vector due to obstacles is away from each obstacle. the commanded total chase spacecraft acceleration is determined by vector addition of (20) and (26), such that (27) selection of the repulsion shaping parameter must be related to the attraction shaping function in order to achieve desired critically damped performance. proper selection allows for safety in selecting goal positions and efficiency when avoiding obstacles. for instance, if the region of influence of the obstacle is too small and the slope of the repulsive potential shaping parameter is too steep then a thrust limited actuator may not be able to avoid collision with the obstacle. on the other hand, if the obstacle region is too large then the chase spacecraft may be less efficient in both control effort and maneuver duration as it avoids obstacles."
"an autonomous distributed lqr/apf control algorithm for multiple spacecraft in close proximity operations is proposed. the control algorithm combines lqr efficiency with apf-based collision avoidance. the developed lqr/apf multiple spacecraft close proximity control algorithm offers robust close proximity performance and establishes a reliable baseline for control effort efficiency, while maintaining collision free maneuvers. the multiple spacecraft simulation results are promising. the developed lqr/apf multiple spacecraft close proximity control algorithm allows for convenient inclusion of known or estimated sensor uncertainties and actuator response into the control parameters."
"in order to reduce the gap described above, in this paper we explored the use of a dl methodology for the automated mapping of agricultural plot boundaries over a large area with a heterogeneous landscape. in particular, we performed an experimental analysis using the lpis open-data of an extensive region of spain. the proposed approach opens up the possibility to create a framework to a support human operator task, as a complementary tool for the systematic updating of agricultural cadastral boundaries on a large scale."
"in this section, we will discuss the proposed back-end meta-models and outline how the model transformations are carried out. for questions of space, we will concentrate in the mhs file for obtaining the top-level implementation description from the ip-xact description."
"next, we developed a multiple spacecraft apf control algorithm with collision avoidance. our research explores the use of potential functions in relation to velocity error, as opposed to only position errors, for controlling spacecraft. the apf control algorithm's collision avoidance capability is essential during simultaneous multiple spacecraft close proximity maneuvers."
"the number of multipath parameters is kept equal by this mapping, but the total number of clock offset and ambiguity parameters is reduced from 1 + m k to m k. the pre-corrected phase measurements of eq. (3) are expressed in terms of the reduced parameter set of eq. (7):"
"apf theory has been used extensively in robot navigation and control [cit] . apf control algorithms are effective in simple obstacle environments and safer than most path planning algorithms in highly dynamic environments. [cit] . it has been expanded to consider distributed control [cit], autonomous rendezvous with fixed obstacle avoidance [cit], autonomous control of on-orbit assembly [cit], and fuel efficiency constraints for cluster formation maintenance [cit] . recent application of apf for swarm control of micro-utility spacecraft also shows promise [cit] ."
"the lqr/apf multiple spacecraft close proximity control algorithm proposed combines desirable characteristics of the lqr and apf. it uses the lqr response as the attractive force and apf-based repulsion for collision avoidance. the apf is an artificial construct which is conceptually useful. however, the apf limitation is that it is not intended to represent the dynamics. in contrast, the advantage of lqr consists in the incorporation of relative dynamics in the control algorithm. using the dynamics, the lqr generally improves the performance of the control algorithm with little additional computation. meanwhile, the repulsive apf can provide collision avoidance capability that lqr may not readily offer in a dynamic environment. first, the inclusion of all necessary collision avoidance constraints in the lqr for a static environment is challenging. next, if a dynamic environment is considered the necessary states and constraints for the lqr may not be readily available. finally, for lqr collision avoidance computations, numerous states for each object in the environment must be added. each additional object would add six variables and the requisite computations. consequently, in a high density obstacle environment the computation cost and time may limit implementation. our lqr/apf control algorithm utilizes the optimal lqr in the obstacle free environment and employs practical apf-based collision avoidance as necessary."
"although it is not possible to distinguish amino acids, most secondary structures, such as a-helices (red sticks in fig. 1a ) and b-sheets, can be computationally identified from a density map with medium resolutions, such as 4-8 a [cit] . once a b-sheet density region is identified, b-strands may be predicted using strandtwister by analyzing the twist of a b-sheet [cit] . a helix detected from a cryo-em image can be represented as a line-referred to here as an a-trace-that corresponds to the central axis of a helix (shown as red sticks in fig. 1a ). similarly, a b-strand can be represented as a b-trace that corresponds to the central line of the b-strand (see section 3.5 for more details). the term, secondary structure traces (ssts), refers to the set of a-traces and b-traces detected from the three-dimensional (3d) image (fig. 1a) ."
"moreover, since our approach is based on a machine learning process, even though discrepancies between images and sigpac information are present in some parcels, the generated dl model has the ability of detecting the borders of a parcel not included in sigpac. these results confirm the capability of working with out-of-date cadastres for updating them."
"the results obtained from the classification for the three classes (background, parcel, and buffer) of the 21 test tiles are shown in the confusion matrix of table 1 . the inaccuracy of the edges of the lpis plots caused the pixels belonging to the buffer class to be confused with the other classes, particularly the parcel class. on the other hand, the background and parcel classes showed a success rate of more than 89%. since the transitions between what is an agricultural plot and what is background, represented by the buffer class, is not so clear, it is not possible to obtain a good representation of the boundaries of agricultural plots considering only a classification of a single class i.e. the agricultural plot boundary. it is important to point out that although the precision obtained in the buffer class is not excellent (69%), this is not a disadvantage to the aim of this work, because agricultural plot boundaries are obtained from the parcel class."
"dl techniques, and in particular convolutional neural networks (cnns) are capable of exploiting the unknown structure in the input data distribution to discover good representations, often at multiple levels, with higher-level features defined in terms of lower-level features; reducing large and complex data sets to a predictive output [cit] . this provides greater learning capacity and therefore higher performance and accuracy compared to traditional methods. two types of cnns are generally used in remote sensing applications [cit] : patch-based convolutional networks and fully convolutional networks. in the first approach, a typical cnn model receives fixed-size patches centered on each image pixel as input and, consequently, the network's response (prediction) to every single pixel is represented by the image region corresponding to that particular patch [cit] . these models work particularly well in sparse annotated data sets; however, when dense predictions are required (i.e., one for each pixel of the image) they require a lot of computational power [cit] . although there are approaches to reduce these resources such as the use of superpixels instead of pixels [cit], they present problems such as inaccuracies at the edges of the objects in the image. on the other hand, fully convolutional networks are built solely from locally connected layers, such as convolution, pooling, and upsampling [cit] . unlike patch-based architectures, they can offer dense predictions because they do not contain fully connected layers with fixed dimensions, depending only on the size of the input image. thus, a fully convolutional network tries to learn representations and make decisions based on local spatial information [cit] . these features make them more efficient than patch-based approach [cit] ."
"the lqr/apf control is much more efficient than the apf control in the absence of obstacles. however, the efficiency gained by lqr/apf control tends to decrease as the number of obstacles in the region increase, since the collision avoidance capability alters the iterative optimal solution. the addition of the robust collision avoidance capability is considered to be worth some loss in efficiency in a dense obstacle environment."
"visual examples of buffer classification problems are shown in the figure 5 . as can be seen, in some cases the model has problems in identifying the different classes which results in missing sections of plots or the lack of separation between them (buffer). it should be noted that in some cases it is even difficult to visually distinguish the separations between them. discrepancies can also be observed between what we would visually consider to be the boundaries of a plot and what is reported by sigpac. this highlights the importance of having automatic/semi-automatic methods to update the boundaries of agricultural parcels for operational use."
"since the earth is not symmetrical in its shape and mass distribution, its gravitational field is also not symmetric. the earth's shape and mass distribution can be described by the zonal harmonics coefficients in conventional legendre polynomials [cit] . the first three zonal coefficient terms in the legendre polynomial are j2, j3, and j4, where is the second-order zonal harmonic. the third-order zonal harmonic is and the fourth-order zonal harmonic is . j2 is the equatorial bulge term which has the most significant effect on spacecraft orbits. j2 effects are often classified as short period oscillations, while j4 results in long period variations."
"the first step in our control algorithm research is to develop a close proximity multiple spacecraft lqr controller. the lqr algorithm serves as the principal convergence force during close proximity operations. the multiple spacecraft lqr algorithm uses the linearized state dynamics from (2). the iterative lqr allows for efficient control. each lqr solution is optimal for each iteration of the cost function. the cost function is based on variable gain matrices, which allow for steady convergence to the desired goal state."
"for each simulation, the chase spacecraft initial positions were uniformly randomly distributed while all other simulation parameters were maintained. each of the six chase spacecraft was initially positioned within a 1.0 km sphere with respect to a target spacecraft position. however, the chase spacecraft initial position was assumed to be at least 10 m from the target. this stand off range, representing the center-to-center distance between the target and chase spacecraft, was used so that spacecraft size variation of a few meters could be readily simulated. the target spacecraft was assumed to be in a circular leo of 500 km altitude. the initial relative velocity was assumed to be negligible. this neutral velocity state in the relative frame, suggests an elliptical orbital phase for the chase spacecraft. for the sake of control evaluation, this neutral situation is reasonable and serves to avoid bias due to favorable velocity conditions. the relatively high velocity management of both the apf and lqr/apf allows for some initial velocity. table v . first, the initial range mean and standard deviation is listed for all 1200 chase spacecraft in the 200 simulations. the chase spacecraft are initially uniformly randomly distributed within the rsw orbital frame, as shown in fig. 6 . next, the mean and standard deviation is listed for the maximum initial chase spacecraft range of each of the 200 simulations. this maximum initial range metric drives the overall and for each multiple spacecraft maneuver. both the lqr/apf and apf control algorithm are analyzed for the same random range distribution. this allows for direct comparison of performance for each maneuver."
"in spain, the implementation of lpis, called sigpac, is supported by the ministry of agriculture, food and environment of the government of spain. sigpac is a public data source that allows the geographical identification (e.g., land use and boundaries) of parcels declared by farmers and stockbreeders throughout the spanish territory. all sigpac data 2 has been generated from digital orthophotos (rgb images), cadastral information and on-site field visits."
"the proposed mde methodology, in terms of models and model transformations, is presented in figure 3 . we make use four abstraction levels, each making use of its corresponding component library. the entry point is a marte deployment model (a composite structure diagram, csd), which is created by choosing components from a marte model library (mml). the marte model is obtained in the deployed allocation phase, where sufficient information of the components to use is available. at this phase, components are seen as simple ip blocks containing interfaces to be connected and parameters to be set by the designer. we have created an extension to the marte profile for defining the features of the deployed ips that allow us to link them to their ip-xact counterparts and to obtain their properties automatically."
"in order to understand these differences between the two aforementioned method, tiles 4 and 17 are shown in figure 7 . at first glance, there are differences between the types of plots that appear in each of the tiles, while tile 4 (figure 7a ) mainly includes large, well-defined plots as well as an urban area, tile 17 (figure 7b ) contains small plots. these differences are reflected in the gt, where many more boundaries are observed in tile 17 (figure 7d ) than in tile 4 (figure 7c ). in the case of tile 4, it can be seen that while the results produced by our approach (figure 7e ) are more in line with the gt, the gpb-ucm method (figure 7g ) not only generates the edges corresponding to the boundaries agricultural plots, but also edges within urban areas. in this regard, it is important to highlight that gpb-ucm is an unsupervised method, therefore, a further step (e.g., classification) is necessary to eliminate areas that are not of interest. in the case of the proposed approach this step is not necessary as classification is implicitly included. the less clear the natural boundaries of the agricultural plots, the greater the possibility of discrepancy, resulting in a higher bde index value. this usually occurs on large farmlands, which are generally divided for operational reasons (e.g. different crops or other uses). on the other hand, the results obtained from the two methods in tile 17 (figure 7f and 7h) are better because the boundaries of the plots are clearer due to over-fragmentation of the land."
"in table 6, we provide a summary of the several languages, models and tools. for models transformations we have used the model query language (mql) and java. the high-level models have been created using papyrus, and the xpsf meta-models have been implemented using rhapsody, and then imported to mdworkbench. we have used ip-xact editor for creating ip-xact golden rule models manually; these models are compared with those generated by mdworkbench. table 6 . used languages, models and tools"
"these results are shown in much more detail in figure 8 . the boundaries obtained by the proposed approach (figure 8c and 8d) fit the gt boundaries better (figure 8a and 8b) than those generated by the gpb-ucm method (figure 8e and 8f ). it should be noted that the boundary transitions of our results obtained in two hand-delineated tiles are different from those used in the training and test sets. volume 7, 2019 approach are also smoother than the edges obtained by gpb-ucm. in addition those obtained by gpb-ucm present oversegmentation."
"xilinx solves the aforementioned problems by providing an intermediate representation layer, the microprocessor peripheral description (mpd) file, as depicted in figure 1, which shows a section of such a file. the mpd file contains basic information of underlying ip vhdl/verilog implementation (generics, ports), adding flow dependent attributes, used for configuration. the ports can be bundled together using the concept of \"bus interface\", allowing the designer to customize the use of certain interfaces by setting attributes such as datatype, isvalid, permit, etc. the ip implementations abstracted by the mpd files need to be parameterised at a higher level; this is done through the components instantiation in the microprocessor hardware specification (mhs) file. as shown in figure 2, platgen (a xilinx tool) reads a mhs as its primary design input. the tool also reads various hardware microprocessor peripheral description (mpd) files from the edk library and any user ip repository referenced in the mhs file. platgen produces the top-level hdl design file for the embedded system that stitches together all the instances of parameterized ip blocks contained in the system. in the process, it resolves all the high-level bus connections in the mhs into the actual signals required to interconnect the processors, peripherals and on-chip memories the edk intermediate description, based in the mhs and mpd file (among others), represents an improvement over a purely vhdl description, since the textual representation has a formal semantic. therefore, in our methodology there is an interest in being able to integrate these models for platform generation; for this, we have proposed several meta-model of the xilinx psf files. however, an mde methodology should be platform independent before the deployment model; it is at this phase where back-end and technology dependant information is added to the platform components."
"from authors point of view, the outstanding results obtained have been achieved thanks to the combination of the dl model, which has provided really good results in different image processing tasks, with a large amount of annotated training data set, it being another strength of our work. the use of sigpac public data has provided a large volume of training data, which has been increased with the application of augmented techniques. the final result has been a dl model with a high generalization performance as has been showed in the delineation results of data with a different geographical localization that the tiles used in the training and test phases."
"the gradient of the attractive and repulsive potentials are related to desired control forces. although various potential functions are utilizable, quadratic functions are generally used based on their desirable geometric characteristics [cit] . however, for close proximity maneuvering, it was discovered that more linear functions tend to offer better convergence performance, since they do not tend to flatten out in the same manner as the bowl shaped quadratic functions."
"the reduced state vector includes the receiver position, the combined receiver clock/ reference ambiguity, the tropospheric zenith delay, the slant ionospheric delays, the differential integer ambiguities and the combined pseudorange multipath/ reference ambiguity parameters. it can be obtained from the full state vector of eq. (5) by a linear transformation:"
"linking the components directly to the xpsf descriptions would tie the approach to an specific back-end, making it difficult to adapt to updates or to adapt it to other vendors and flows. therefore, we use ip-xact, as it will be described in section 5, with vendor extensions to support specific attributes to generate the edk files, and to support features such as customization in parameters, bus interfaces and ports, which are not supported in the current ip-xact specification. we have created a series of edk metamodels in order to perform these transformations."
"5) thrust: jet thrusters are assumed to be used in order to control the spacecraft translation, while the spacecraft attitude is controlled by reaction wheels. the thrust of each thruster is (6) where is the specific impulse of the thruster, is the mass flow rate, and is the gravity acceleration at the earth surface. the change in velocity, also referred to as delta-v, experienced by the spacecraft due to a single thruster pulse is (7) where is the mass of the spacecraft before the pulse and is the mass of propellant expelled during the pulse. for a given thruster pulse lasting for a time interval, the can be approximated as (8) thruster sizing and amount of propellant to be stored on board (related to the total required ) depend on the particular mission."
"), the atomic structure can be derived directly, since the backbone is mostly resolved. however, it is computationally challenging to derive atomic structures when the backbone of the protein is not resolved from the density maps, such as those with resolutions lower than 5 a . in current approaches, a known atomic structure or a model built from known atomic structures is fit into the cryo-em density map [cit] . however, those approaches are limited by the need for atomic structures that are either components of or homologous to the atypically-sized protein. when there is no template structure with sufficient similarity, de novo methods must be devised and used. these methods do not rely on templates and they aim to derive the structure from the intrinsic relationship among the secondary structures visible in the density map."
"consuelo gonzalo-martín received the ph.d. [cit], she has been with the computer school of universidad politécnica de madrid. [cit], she has been carrying out her research activities with the center for biomedical technology of the upm. her main contributions have been in the areas of image processing, and the development of new artificial neural networks algorithms in remote sensing, medical imaging and recognition of faces. at present, her main line of research is new methodologies for image analysis based on objects (obia) for satellite images and medical images. also, she is involved in text and image mining in the health care domain."
"in this section, the absolute phase and pseudorange measurements are corrected for the broadcast orbits and clocks and other parameters known through accurate models. subsequently, we re-parameterize the corrected measurements to obtain a full-rank system of equations, which allows the joint estimation of the absolute receiver position and clock offset, tropospheric zenith delay, ionospheric slant delays, carrier phase ambiguities and pseudorange multipath errors."
"finally, the total chase spacecraft acceleration determined by the multiple spacecraft lqr/apf with collision avoidance is (34) where is determined iteratively from (10). the control algorithm only decreases velocity and acceleration toward obstacles. it does not actually push away from obstacles. therefore, densely packed stationary obstacle regions could possibly cause the control to settle into regions other than the goal. however, the relative dynamics generally results in forces that cause the control algorithm to escape local minimums. the outcome is similar to that achieved by apf wall-following methods [cit] . if a local minimum or saddle point exists in persistence, a local minimum avoidance method such as the random walk method is to be activated. in the course of extensive simulations with realistic multiple spacecraft environments, no persistent local minima were encountered."
"although the proposed method provides great precision to find the visible edges of the plots automatically, a successful update of the agricultural cadastre still requires the participation of owners, surveyors and other actors with spatial knowledge of the sites to be analyzed [cit], allowing the inclusion of non-visible edges (e.g. those that depend on legal aspects). however, starting from the edges obtained by our method would undoubtedly improve the efficiency of the updating process."
"the paper is organized as follows: in section ii, we describe similar approaches and its limitations in the ip deployment phase. then, section iii describes the targeted back-end, and the needs in terms of models generation for hw specifications. in section iv, we present the proposed approach, the role of the proposed meta-models and the model transformations. in section v we discuss how ip-xact is embedded in the ip deployment and generation phases of the methodology. the next section introduces the marte extensions for ip deployment, and describes how the models are obtained from ip-xact; then, we discuss ip-xact design is obtained through model transformations. the transformations for obtaining the xps models are described in section vii. in section viii we embark in a discussion of the reduction in design effort provided by the methodology, and section ix concludes the article, providing some avenues of future work."
note to practitioners-use of multiple spacecraft for close proximity operations is expected to increase in future space missions. a challenging problem is how to automate motion planning and control of multiple spacecraft in close proximity. this paper presents a distributed control algorithm for simultaneous docking maneuvers of multiple spacecraft.
"the physical characteristics of each spacecraft in the simulation are assumed to be similar. the target spacecraft orbital altitude is assigned a predetermined, or randomly distributed, range of 300-1000 km. the number of chase spacecraft is assigned, or randomly selected, from one to six. each chase spacecraft is assigned a predetermined, or randomly distributed, initial position from the target spacecraft initial position. this initial range between the target and chaser spacecraft is within 1000 m in rsw coordinates. initial velocities of the chase spacecraft are assumed to be the same as the target spacecraft. this neutral initial velocity allows for practical controller performance evaluation. the simulation condition ranges are summarized in table i ."
"after having chosen an ip in the deployment model, the system designer can create a system description by using two views, the \"parameterization view\" and a \"platform view\"; these views contain a set of component instances to be parameterized and a csd for interconnecting the different ips in the platform, respectively. the parameters and interfaces for the components in these models are obtained from the ip-xact library. both views are parsed in the marte model parsing phase to obtain an ip-xact system description, using of a simplified version of the model transformations proposed in previous works. the components in the ip-xact library are parsed to gather the valid parameters and interfaces used to create the complete ip-xact design description; this description contains the chosen component instances, bus interfaces, configurable elements, the connections between the component instances and hierarchical connections. the ip-xact design is imported to our chosen \"design environment\", sodius mdworkbench [cit], in which the model transformations from ip-xact to the xpsf models take place. the xml schemas have been processed by an improved xsd/ecore meta-model importer in mdworkbench, which leads to a java/emf implementation of the ip-xact meta-model."
"none of the three close proximity maneuver failure conditions considered were experienced by either the refined apf or the developed lqr/apf control algorithm. first, no spacecraft collisions were detected. second, all spacecraft maneuvers were successfully performed within 90 minutes. finally, no spacecraft was required to use all of its propellant during maneuvering. therefore, both control algorithms proved to be effective in performing close proximity operations. a statistical analysis of both the lqr/apf and apf during the seven spacecraft simultaneous docking maneuvers follows."
"based on this monte-carlo analysis, the lqr/apf control algorithm appears suitable for application to emerging multiple spacecraft operations. both the control efficiency and maneuver duration are reasonable for current spacecraft designs. based on the required, the average chase spacecraft could perform 20-40 close proximity maneuvers. the lqr/apf algorithm appears to be practical for multiple spacecraft close proximity maneuver control. variation in spacecraft physical characteristics and orbital assumptions may cause some fluctuations in the total number of close proximity maneuvers which can be performed. however, the lqr/apf control algorithm performs reliably for a wide range of simulated maneuvers."
"the purpose of this tool is to generate several files used by edk configure the hardware and software component of a soc platform. the configuration of the components is performed through the creation of a microprocessor hardware specification (mhs) file, which contains a set of component instances and their parameters. we have defined transformations from ip-xact to this proprietary format, which is used by the xilinx tools to obtain the top-hdl description, and the references to the hdl ips, that are configured in this phase."
"as regards the comparison with methods that do not require annotated data, such as the gpb-ucm method, which has been used as a reference in this work, it has been shown that the reference method presents oversegmentation in some areas, and that the boundaries obtained by the proposed approach fit the sigpac boundaries better than those generated by the gpb-ucm method and which are also finer, reducing uncertainty in the label assignation."
"our research proposes a control algorithm which combines the efficiency of linear quadratic regulator (lqr) with an apf-based collision avoidance [cit] . in particular, the proposed apf-based collision avoidance relies on both the relative positions and velocities of spacecraft. the merged lqr/apf control algorithm utilizes simple goal commands and obstacle sensory data [cit] ."
"the absolute carrier phase measurement of eq. (1) is precorrected for the known earth tides, satellite position and clock estimates, phase-wind up, phase center offset and variation, and satellite phase biases, i.e."
"from an identified helix voxel. for the simulated dataset shown in table 1, ssetracer was able to detect most of the helices with an average specificity of 82.24 percent and an average sensitivity of 84.83 percent. this suggests that the ability of ssetracer to detect the helices in this dataset was fairly accurate. we noticed that secondary structure prediction servers, such as sympred and jpred, predicted a greater number of helices than ssetracer (columns 4 and 5, table 1 ). dp-toss was designed to incorporate non-identical numbers of the secondary structures predicted from the sequence and from the 3d image."
"in this section we explain in more detail the ideas introduced previously. here, we embark in a thorough description of our approach and how it is embedded into the design flow of xps embedded systems."
"model-driven engineering (mde) [cit], in tandem with uml has been used in soc co-design methodologies recent years with relatively success [cit] . many of these approaches make use of the uml profile for \"modelling and analysis of real time and embedded systems\" (marte) [cit] . uml models are used not only for communication purposes but, using model transformations, to produce concrete results such as a source code. for this purpose, mde methodologies make use of a deployment phase in which the building blocks of the high-level models are linked to the low implementations that embody the related behaviour. in the context of uml for soc, this is basically an ip reuse problem; in this way the components can be configured and, through a system composition phase, a synthesizable implementation can be obtained."
"the system of eq. (3) -(6) is under-determined, i.e. it is not possible to separate the receiver clock offset from all ambiguities and pseudorange multipath errors. therefore, we choose one satellite as reference satellite (being indicated by the upper index ref ) and map the respective ambiguity n ref"
"performance validation is a critical part of control algorithm development. an effective validation scenario is one which accurately simulates the environment in which the control algorithm is expected to operate. the application of the control algorithm for use on multiple spacecraft in proximity operations drives the requirements that it be tested with computer-generated orbital dynamics and kinematics. for this research a high fidelity 6-dof spacecraft dynamics model is used. given the initial values of the position and velocity of each spacecraft, the orbits are propagated by numerical integration."
"the statistical analysis is extended to a per docking maneuver basis, with the means and standard deviations of the maximum and listed in table viii . the lqr/apf control algorithm performance continues to maintain a tighter standard deviation. this confirms the lqr/apf control algorithm's more predictable performance. while on the other hand, the apf control algorithm significantly saturated the control actuation in approximately 10%-20% of all docking maneuvers. this increase in actuator saturation is undesirable and increases the risk of collision in high density obstacle regions. the lqr/apf's docking maneuver maximum distribution is shown in fig. 9 . the lqr/apf's docking maneuver maximum distribution is shown in fig. 10 . the lqr/apf's maximum is lower fig. 7 . lqr/apf spacecraft docking maneuver duration distribution. on average with a tighter distribution. the lqr/apf's total docking maneuver distribution is shown in fig. 11 ."
"this section introduces the dynamic model used by the control algorithm. as typical, the earth centered inertial (eci) coordinate system and the orbital local vertical-local horizontal coordinate system, as depicted in fig. 1, are used to describe the motion dynamics [cit] . the eci coordinate system, represented by ( ), has origin at the earth center. the axis points toward the vernal equinox, the axis is aligned with the earth rotation axis and is directed through the north pole, the axis completes the dextral cartesian coordinate system [cit] . for scaling reference, the radius of the earth spheroid is approximated as . the orbital coordinate system, represented by ( ), is used to express the relative dynamics among the multiple orbiting spacecraft. the axis is aligned with the radial direction from the earth to the spacecraft, the axis is normal to axis on the orbital plane along the direction of the spacecraft orbital motion, and the axis is normal to the orbital plane. it is worth noting that the target spacecraft velocity vector is only aligned with the axis when the orbit is perfectly circular, and at the apogee and perigee of elliptical orbits."
"the concept of merging the lqr and apf control algorithms concepts is proposed as an efficient and capable combined algorithm. the recursive lqr is used as the attractive force and the apf-based repulsive forces are determined by obstacle locations. for the apf, relative position from goal and obstacles is used to determine desired velocity. residuals from the desired velocity are used to command thruster firings. however, the lqr control effort varies the position and velocity based on the linearized system dynamics. this more complicated relationship requires a modification to both velocity and acceleration in the region of influence of obstacles. the result is an iterative spacecraft control algorithm which is driven by optimal lqr cost convergence, with associated dynamics, and apf-based smooth collision avoidance responses."
"given a specific set of secondary structure traces and a specific set of predicted secondary structure sequence segments, k best mappings were determined using dp-toss. in the first step, a set of sequence segments predicted by a consensus secondary structure prediction server was used. the idea is to use the best estimation of the secondary structure positions in the first step in order to obtain a small number of possible topologies. for each possible topology, the best placement of the secondary structures will be determined in the second step."
"another advantage of using uml and marte is the maintainability and improved updatability of the models; this means that, contrarily to purely vhdl or edk flows, a change in the platform requires much less effort: since every step of the design flow is automated, the designer does not even need to make use xilinx edk or ise. the ip-xact descriptions also facilitate the updatability of the approach by changing the vendor extensions or the target meta-models, but not the implementation files. in table 5, we show the design efforts for each of the aforementioned methods. it must be noted that we consider design capture times by non experts."
"the results of the evaluation using the bde index that compares the boundaries obtained using the proposed methodology and gpb-ucm with those of sigpac for each of the 21 tiles in the test set are shown in the figure 6 . as can be seen, for all tiles, the value of the bde index is lower for the boundaries generated by the proposed methodology than for those obtained using the gpb-ucm method. the greatest difference between the two methods with respect to the gt is observed in tile number 4, with bde values of 14.64 for gpb-ucm and 5.53 for our approach; while tile volume 7, 2019 figure 5. comparison between sigpac ground-truth and convolutional neural network output. as can be seen there are some discrepancies between the image and the classes obtained from sigpac. in addition, some inaccuracies in the predictions of the neural network can be appreciated. the background, parcel and buffer classes are shown in navy blue, green and yellow, respectively. figure 6. bde index values of the results obtained by our approach and the gpb-ucm method for the 21 test tiles. number 17 shows the smallest difference between them with bde values of 2.26 for gpb-ucm and 2.08 for our approach."
"an ip-xact design object describes an actual top level implementation as a set of component instances, which can be configured through configurable elements. the subelements in a design are connected between bus interfaces (that conform to predefined bus definitions). there are three kinds of connections in ip-xact: interconnections, ad-hoc and hierarchical connections."
"the actual relative velocity is subtracted from the desired velocity to determine the required by the control effort, and the related spacecraft acceleration is (20) the goal potential allows for convergence to the goal position; however an obstacle potential is required to avoid collision with other spacecraft and sensed objects. the repulsion potential curve is a smooth function that increases from the boundary of the region of influence to the surface of the obstacle. the obstacle potential is selected to be a function of the vector from an obstacle to the chase spacecraft, (21) the obstacle potential is selected such that gradient of the obstacle potential determines the desired velocity away from the obstacle. the resulting chase spacecraft desired velocity modification, based on the repulsive potential gradient way from an obstacle, is (22) where is the velocity shaping function from (16) and is the obstacle velocity function. the obstacle velocity function is a gaussian function of the form (23) where is the obstacle spatial region of influence, is the radius of the smallest sphere enclosing the obstacle, and is the standard deviation for obstacle region of influence. this selection of ensures that the magnitude of equals at the surface of the obstacle."
"it is generally more challenging to determine the topology for proteins with b-sheets than a-proteins. first, the detection of b-sheets is generally more challenging than the detection of helices. second, the close spacing of b-strands makes it more challenging to identify the correct topology. we applied the two-step approach to two experimentally-derived cryo-em density maps, (emdb_5030 and emdb_1780) that were downloaded from the electron microscopy data bank (emdb). each density map corresponds to an atomic structure; thus, they can be used to test the accuracy of our approach. in the case of emdb_5030, all three helices and three b-strands were detected using ssetracer and strandtwister (fig. 4) . the true topology was ranked 47th when multiple secondary structure predictions and dynamic programming placement were used. amazingly, the rank (47th) is even better than the rank (55th) derived using true secondary structure positions on the protein sequence. in the case of emdb_1780, the rank of the true topology is the 2nd when either multiple secondary structure prediction methods or the true sequence segments of secondary structures are used. although the two cryo-em proteins are smaller than most of the other proteins in the test, they are the first two cases that successfully demonstrated topology determination directly using computationally-obtained b-traces and multiple secondary structure predictions."
"the deployment phase of any mde methodology is instrumental, since enables the generation of a synthesizable soc description from a high-level marte model. more precisely, sufficient information must be provided at this stage so that the code integration and parameterization on the ips can be performed."
"for the multiple spacecraft rendezvous problem, a damped relative position response with limited control effort is desired. as with all spacecraft maneuvers, control efficiency during multiple spacecraft close proximity operations must be considered. however, the close proximity maneuver is assumed to be operationally significant and must be performed in a finite duration. for this research, approximate maneuver duration of one quarter orbital period was assumed. the close proximity maneuver is considered successful when the chase spacecraft converges within a precise range of its goal position. the precision used in this research is modified to evaluate various multiple spacecraft close proximity maneuvers, with the intent that the developed lqr/apf control algorithm performs docking maneuvers."
"besides initial position, all other chase spacecraft parameters were held constant for each simulation. the general monte carlo simulation parameters are summarized in table vi. table v chase spacecraft range distribution statistics table vi monte carlo simulation parameters table vii docking spacecraft statistics"
another contribution of this paper was extensive numerical simulations performed with a nonlinear high fidelity orbital model using monte carlo method analysis. this allows for statistical estimates of the mean and standard deviation of maneuver and . both the stability and robustness of the close proximity control algorithm is demonstrated implicitly by monte carlo simulation results.
"we initialize the state vector by least-squares estimation using the pre-corrected pseudorange and carrier phase measurements of a single epoch. the pseudorange multipath errors can not be estimated with single-epoch measurements. thus, we determined the full-state vector of eq. (5) except of pseudorange multipath parameters by minimizing the sum of squared measurement residuals:"
"this issue has been addressed by the academia, with ongoing efforts aiming to integrate the ip-xact standard [cit] in mde-based methodologies, as an intermediate representation (ir) . the standard describes a set of xml schemas used to document ip meta-data for ip packaging and soc integration. the goal of the specification is to provide an interchangeable description of hw components."
"we make use of design descriptions as a means of describing the top level architecture in a flow agnostic way. the ip-xact design description contains most of the information required to generate systems described in languages such as vhdl, verilog or even systemc. the descriptions are tailored by adding vendor extensions or flow dependant configurableelements; ip-xact defines the concepts of generators and generator chains for accessing the meta-data contained in these descriptions. they are used configure the ips in the component library, to generate drivers or customize components. this task is carried out by sodius mdworkbench, in which we import the ip-xact descriptions of the top-level to generate the xilinx mhs file, effectively decoupling the intermediate representation (ir) from the intended front and back-ends. therefore, we can envision scenarios in which the departing model is not described in marte, but in aadl, to cite and example. the back end can also be customized by choosing a different view in the components description."
"we undertake a simpler approach: the ip-xact components in our approach are created in such a way that allow us to extract information of the ip components (in another ir, used by xps) by using a deployment package, which includes only the necessary information of the ip for parametrisation and interconnection. the components obtained in this manner are used to compose an architecture diagram, which is then parsed to obtain an ip-xact design description. the ip-xact design allow us to be back-end independant, but to generate the edk files by using vendor extensions in the component descriptions."
"the convergence of the ionospheric zenith delay estimation is shown in fig. 4 fig. 3 . accuracy of tropospheric zenith delay estimates delay for each satellite and the much larger process noise compared to the tropospheric zenith delay result in a slower convergence and higher errors of the ionospheric zenith delay estimation. however, these errors do not affect the absolute receiver position, which is determined much more accurately. fig. 5 shows the accuracy of the float ambiguity estimates. the errors of different satellites and frequencies are significantly correlated, which could be exploited by an integer decorrelation and a subsequent ambiguity fixing [cit] . fig. 6 shows the simulated pseudorange multipath errors and fig. 7 provides the accuracy of their estimates. the errors are again significantly correlated. fig. 8 and 9 show the residuals of the phase and pseudorange measurements of all satellites and frequencies. all residuals are unbiased and the magnitude is in the order of the measurement noise, which confirms the consistency of our state estimation and measurement models. vi. conclusion"
"the main contribution of this research is the development of an automatic dl tool for the automatic tracing of agricultural parcels, using a public data source such as lpis, which opens up the possibility of a systematic updating of agricultural cadastral boundaries. the use of lpis data reduces the main drawback of dl, the availability of annotated data sets to carry out the training of cnn models."
"as mentioned previously, the boundaries are extracted from the segments belonging to the class parcel, which in this work provide the boundaries of the agricultural plots and whose accuracy is discussed below."
"the ip-xact standard defines four central object descriptions, which are bus and abstract definitions, component, and design descriptions. these four elements are sufficient for structurally describing a system and the ip cores the compose it. the main goal of this paper is to present a framework for the parameterization and integration of the modules that comprise the dpr platform."
"1545-5955/$26.00 © 2010 ieee this paper offers the following primary contributions: 1) a unique close proximity control algorithm that combines lqr goal convergence with apf collision avoidance. 2) numerical simulations through monte carlo analysis of the multiple spacecraft control algorithm performed with a nonlinear high fidelity orbital model. this paper is organized as follows. first, the relative dynamic equations of motion between spacecraft in close proximity and a high fidelity 6-dof spacecraft model are discussed. second, the lqr/apf proximity spacecraft control algorithm, based on lqr and apf concepts is developed. the development starts with the lqr control algorithm foundation, and then proceeds to the development of the apf control algorithm. once these two theoretical components are fully explained, the combined lqr/apf multiple spacecraft close proximity control algorithm is presented. finally, the control algorithm performance is evaluated via numerical simulations of proximity docking maneuvers of seven spacecraft, by using monte carlo analysis."
"the presented ip-xact component descriptions contain vendor extensions that allow us to integrate our methodology in the xilinx design ecosystem for dpr systems, but in such a way that allow us not to impact the interchangeability of the models. therefore, the ip-xact models can be extended for targeting different back-ends, allowing to easily evolve the methodology to changing requirements or to adapt it to other vendors. using marte and ip-xact makes the design or dpr more amenable, and at the same time, helps in decoupling the high-level models from the intended back-end. this his achieved through the use a generic ip deployment meta-model, which does not make particular assumptions of the nature of the low-level implementation details."
"only devices connected to the free wi-fi provided by the mall are logged. the logs do not track users but rather mobile devices through the device's wi-fi mac address, which was replaced by a hash key. thus, there is no ground truth about the identities of the users (e.g. shoppers or mall employees). the term users is used to refer to unique devices appearing in al, a subset of such users are browsers who appear in the bl, and searchers are those users who appear in the ql. the bl includes traffic to the web originating from the mobile web browser, as well as from other apps. traffic from apps currently cannot be easily filtered out. the ql was extracted from the bl (it is a subset of bl), by isolating searches pointing to search engines, including google(110148, 92.4% of ql), yahoo (6915, 5.8% the al captures information about user physical behaviour through the following parameters: (1) users' location in the mall defined by the location of the wi-fi access point associated with the user's mobile device; (2) timestamp and duration of users' association with the access point. the bl includes the users' information behaviour, characterised by: (1) the timestamp of the web request; (2) the uniform resource locator (url) of the requested web page."
"large-scale indoor spaces are increasingly equipped with free internet via wi-fi, the use of which can be logged and studied. with such infrastructure in place, it is possible to collect a variety of parameters about user behaviour. these environments are visited on a regular basis by a large number of visitors. for instance, the mall in dubai attracted 75 [cit] . tracking the location of users in permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. time and space along with their web activity allows for the study of their needs, establishing whether these are appropriately supported by the environments, and exploring how the indoor spaces cope with the presence of diverse visitors; an important consideration in, for example, hospitals [cit] ."
"we study an anonymized dataset of internet accesses taken from a free wi-fi network operated by a large inner-city shopping mall. the mall is covered by around 70 wi-fi access points (ap). the dataset includes three kinds of logs: a wi-fi access-point association log (al), a web browsing log (bl) and a web query log (ql), [cit] . table 1 shows summary statistics of the three logs. for this research, all user identifiable information was replaced by a hash key in an non-invertible way."
"an indoor space imposes a range of social, technical, and physical constraints. while web query logs have been widely studied, both on desktop machines [cit] and on mobile devices [cit], there are few published studies that analyse web search in large-scale indoor spaces. by analysing a log of web activities of more than 120,000 users in over 1 year period, we study how people behave on the web in indoor spaces and whether information access trends can be predicted. such predictions have practical applications, e.g. for predicting consumer behaviour."
"the ql was processed as follows: (1) search queries were treated as case insensitive; (2) a query term was defined as any unbroken string of characters in a query delimited by the whitespace symbol (u+0020); (3) following [cit], sessions were defined as \"a series of queries by a single user made within a small range of time\". following a recent study in mobile web search [cit], we also use 30 minutes as the threshold for maximum session duration; (4) session length was defined as the number of queries in a session; (5) query length was defined as the number of terms in a query."
"overall, we conclude that indoor users of the many category tend to be much more active in the web browsing. what indoor users browse for is different from what they search for. the web categories of the searching activity are more related with the retail environment. moreover, while the user groups differ in the number of urls per visit in their web browsing patterns (for a deeper analysis of bl, see [cit] .), the difference is small in the average number of search queries per session. we focus on the analysis of ql in the following section 4."
"the prediction of the query popularity can then be modelled as the propagation process of query searching on the given graph g. let n j l−1 denote the number of users who reach qj after searching l − 1 queries, then the number of users who reach qi after searching l queries is defined as:"
"from al, we can observe that (table 1 and table 2 ): (1) around 66% of users stay in the mall between three and four hours per visit, with 26% staying less than two hours and 8% exceeding three hours; (2) while once users outnumber many users, many users tend to contribute more to the al logs than once users, due to their repeated visits to the mall (56.99% of many visited twice, 16.13% of them visited 3 times, and 26.88% visited 4 times or more); (3) many stay in average longer than once users (2.9 hour vs 2.4 hour), while closed users stay longest (3.2 hour). overall, mall users are most likely to stay in the mall for around 3 hours on average across different user groups."
is defined in eq. 1. δ l is interpreted as the fraction of users who search more than l queries over users who search more than l − 1 queries. this propagation process stops when the majority of modelled users stops searching.
"in this study, we analyse the physical behaviour and web information behaviour for indoor users by dividing them into four groups (number of unique users in brackets): (1) open (112,322) denotes the users who appear during the mall's business hours (when shops are open); (2) closed (8,226) denotes the users who appear when the mall is closed (there are some restaurants which are still open to the public after the retail part of the mall is closed); (3) once (80,900) denotes the users who appear only once in the collected period when the mall is open; (4) many (31,422) denotes the users who appear more than once in the collected period when the mall is open. we observe that the majority of users come from the open group, and there are more once users than many users. the closed group was not split due to its small size. the majority of this group visited only once. table 2 shows the comparison of the physical, browsing, and query activities among these user groups."
"from the bl, we observe that: (1) around 60% of wi-fi users in the al actively browsed the web, while the rest visited the mall but did not use the web; (2) around 60% of these users accessed fewer than 100 urls; (3) many is the most web-active user group (around 144 urls per visit, significantly more than values for groups once and closed ); (4) the number of bl logs from closed is fairly low, with users only averaging 13 urls per visit. table 3 shows a breakdown of the content these users browse for on the web. the top-10 popular web categories of indoor browsing 1 . while some of the top categories are related with retail activities (e.g. business and economy and web advertisements), many are related to social networking, infotainment and personal information management (pim). these results indicate that frequent visitors tend to be the most active on the web, where they use diverse, and not only retail related content."
"in this paper, we report the key characteristics of users' physical and web behaviours in large indoor spaces. this shows an initial understanding of how mall visitors behave on the web, and provides a chance to improve their web and shopping experience. we have established that the indoor query behaviour is predictable in terms of query popularity, which has a practical application for the detection of search trends and recommendation services. in future work, we will further characterize and model location-based indoor search behaviour in order to better suggest contextualised results."
"consequently, we explore what indoor users search for on the web. is this content similar to their general search? from the ql, we find that: (1) around 10% of overall users in the al (16% of browsers in bl) are searchers; (2) there is no big difference for indoor searching among different user groups in terms of the number of queries per session. (3) around 50% of the searchers issued three or more queries per session, which is different from general web searchers [cit] : most general web searchers issue only one query per session. potential explanations for these differences include: (a) people search differently in indoor retail spaces; (b) mobile search behaviours have changed since the publication of earlier studies; or (c) the behaviour of the modern interfaces has altered the patterns detectable in the logs (e.g., query suggestion by google); (4) the top 10 indoor search categories (query-click) ( table 3, right column) are dominated by retail-related activities, e.g. travel, shopping, 1 the web page categories were generated through the public webroot content classification service bcws.brightcloud. com. although dmoz has higher accuracy, its coverage is to limited for our study. e.g. www.gumtree.com.au is not categorized in dmoz but correctly categorized as shopping by brightcloud. reference and research and business and economy; (5) the search patterns of indoor users are different from their browsing activity, e.g. while social networking is the most popular browsing category (consistently with mobile internet usage [cit] ); travel is the most popular query-click category. specifically, travel makes 1.4% of browsing but 12% of searching and social networking takes 20% in browsing but only 6% in searching. these differences imply that indoor browsing and searching should be treated differently to improve users' web experience, because users are likely to satisfy different information needs via browsing and searching, respectively. for example, top browsing activities tend to be retail irrelevant, and may be accessed to satisfy common information needs not directly linked to the retail environment; while top search activities tend to be linked to retail, which may indicate tighter dependence on the spatial context of the retail environment. the role of specialised apps is also likely contributing to the difference in browsing vs. query traffic -users are more likely to use pre-installed specialised apps (e.g., facebook) for social interaction rather than using the mobile version of the sites, and in any case the related websites represent known target that need not be searched for. moreover, while adult-related search was popular in general mobile search [cit], it is not popular in both indoor browsing and searching. we suspect one main reason is because the data were collected in a public indoor retail space."
