text
"hiv-1 group m -a line drawn from a random point in sequence space is more likely to intersect 384 the branch relating either of these distant taxa to group m. similarly, branches leading to subtype 385 u sequences tend to be longer and to intersect the hiv-1 group m tree at a basal location 4 . this 386 artificial example implies that real hiv-1 sequences that do not readily fit into any of the defined 387 subtypes or circulating recombinant forms may result in incorrect predictions with misleadingly 388 high confidence scores."
"overall, these experiments demonstrate our method is nearly identical in both accuracy and 303 running time to the top third-party tool, comet. our tool differs from comet in that it is 304 open-source and freely available for commercial use, and is available in a standalone application which 305 can be run on any computer, while comet is closed-source and freely available for non-commercial 306 research use only, and is publicly available only in a web-based system. 307 table 3 . classification accuracies for all tested hiv-1 subtyping tools, for each testing dataset; average accuracy both with and without weighting datasets by the number of sequences they contain."
"in some cases, these datasets had few examples for some classes. training on classes with very 173 few examples would unfairly lower accuracy since the classifier does not have enough information 174 to learn, so we wish to omit such classes from our analysis. however, the minimum number of 175 examples per class to achieve proper training of a classifier is difficult to estimate; this number is 176 known to be dependent on both the complexity of the feature vectors and characteristics of the 177 classifier algorithm being used [cit] . since we vary both k and the classifier algorithms in this 178 study, this makes it especially challenging to determine an adequate minimum class size. here, we 179 arbitrarily selected 18 as our minimum, so we omitted from analysis any subtype with fewer than 18 180 sequences. it may be that specific values of k and some classifier algorithms work well in scenarios 181 with very small datasets, and we leave this as an open question."
"we have developed a software package called kameris which implements our method. it can 155 be obtained from https://github.com/stephensolis/kameris, and may be used on windows, 156 macos, and linux. kameris is implemented in python, with the feature vector computation parts 157 implemented in c++ for performance. it is packaged so as to have no external dependencies, and 158 thus is easy to run. the package has three different modes: first, it can train one or more classifiers 159 on a dataset and evaluate cross-validation performance; second, it can summarize training jobs, 160 computing summary statistics and generating mds plots; and third, it can classify new sequences 161 on already-trained models. detailed documentation, including usage and setup instructions, can 162 be found at https://github.com/stephensolis/kameris. all running time benchmarks of our 163 software were performed on an amazon web services (aws) r4.8xlarge instance with 16 physical 164 cores (32 threads) of a 2.3ghz intel xeon e5-2686 v4 processor. we also note that many of the 165 implementations of the classifier algorithms we use are single-threaded and that performance can 166 almost certainly be substantially improved by using parallelized implementations."
"primary dataset 183 the primary dataset used was the full set of hiv-1 genomes available from the los alamos (lanl) 184 sequence database, accessible at https://www.hiv.lanl.gov/components/sequence/hiv/search/ 185 search.html. in this database, the option exists of using full or partial sequences -in our analysis, 186 we consider both full genomes and just the coding sequences of the pol gene. for the set of 187 whole genomes, the query parameters \"virus: hiv-1, genomic region: complete genome, excluding 188 problematic\" were used; this gave a total of 6625 sequences with an average length of 8970 bp. 189 for the set of pol genes, the query parameters \"virus: hiv-1, genomic region: pol cds, excluding 190 problematic\" were used; this gave a total of 9270 sequences with an average length of 3006 bp. 191 in both cases, the query was performed on may 18, 2017, and at the time, the lanl database 192 reported a last update on may 6, 2017. after removing small classes (see preceding section), this 193 dataset contained 26 subtypes and circulating recombinant forms (crfs). this dataset was used to 194 determine the best value of k, the best classifier algorithm, to compare the performance of whole 195 genomes with pol gene sequences only, and to produce the modmaps of hiv-1. in those experiments, 196 cross-validation was used to randomly draw training and testing sets from the dataset."
"running time is another important performance indicator, so we also compare the performance of 296 these five tools for the dataset of van [cit], and the four fastest tools for all datasets together 297 (see table 4 ). we observe that our tool matches or outperforms the competing state-of-the-art. 298 note that, for these comparison experiments, castor, comet, scueal, and rega were run 299 from their web-based interfaces, and therefore the exact specifications of the machines running each 300 programs could not be determined. for this reason, the running times presented here should be 301 taken as rough order-of-magnitude estimates only."
"for some of the results that follow, we required a method for measuring classification accuracy 131 without the need for a separate testing dataset. to do so, we used 10-fold cross-validation, a technique 132 widely used for assessing the performance of supervised classifiers [cit] . n -fold cross-validation is 133 performed by taking the given dataset and randomly partitioning it into n groups of equal size. 134 taking each group in turn, we trained a classifier on the sequences outside of the selected group, 135 and then computed its accuracy from predicting the classes of the sequences in the selected group. 136 the outcome of the cross-validation are n accuracy values for the n distinct, independent training 137 and testing runs. we report the arithmetic mean of those accuracies as the final accuracy measure. 138"
"we manually searched the genbank database for large datasets comprising hiv-1 pol sequences 201 collected from a region with known history of a predominant subtype, and evaluated the associated 202 publications to verify the characteristics of the study population (table 1) . after selection of the 203 datasets, we wished to obtain labels without relying on another subtyping method. to do so, first 204 we made use of the known geographic distribution of hiv-1 subtypes, where specific regions are 205 predominantly affected by one or two particular subtypes or circulating recombinant forms due 206 to historical 'founding' events [cit] . next, we screened each dataset using a manual phylogenetic 207 subtyping process to verify subtype assignments against the standard reference sequences. this 208 was done, essentially, by reconstructing phylogenetic trees to identify possible subtype clusters. 209 a cluster was identified as a certain subtype if it included a specific subtype reference sequence 210 we had initially provided in our datasets. thus, the first step was to download the most recent 211 set of subtypes reference sequences for the hiv-1 pol gene at the lanl database, accessible at 212 https://www.hiv.lanl.gov/content/sequence/newalign/align.html [cit] . we loaded the resulting fasta file in the eleven datasets from table 1 . we then aligned the 214 datasets with muscle v3.8.425 [cit], implemented in aliview 1.19-beta-3 [cit], where we also visually 215 inspected the alignments. to avoid overfitting, we searched for the nucleotide model of substitution 216 that was best supported by each dataset using the akaike information criterion (aic) in jmodeltest 217 v2.1.10 [cit] . for the dataset us. [cit], the large number of sequences precluded this model 218 selection process, so we chose a general time reversible model incorporating an invariant sites 219 category and a gamma distribution to model rate variation among the remaining sites (gtr+i+g); 220 this parameter-rich model is often supported by large hiv-1 data sets, and was similar to the model 221 selected by the authors in the original study [cit] . phylogenetic trees were reconstructed by maximum 222 [cit] 0207 [cit] with a related bootstrap support analysis. the resulting 223 trees were visualized and their relative sequences were manually annotated in figtree v1.4.3 [cit] ."
"next, the distance matrix is visualized by classical multidimensional scaling (mds) [cit] . mds takes 148 as input a pairwise distance matrix and produces as output a 2d or 3d plot, called a modmap [cit], 149 wherein each point represents a different sequence, and the distances between points approximate the 150 distances from the input distance matrix. as modmaps are constrained to two or three dimensions, 151 it is in general not possible for the distances in the 2d or 3d plot to match exactly the distances in 152 the distance matrix, but mds attempts to make the difference as small as possible."
"in order to benchmark performance on this manually curated testing dataset, we required a 225 separate training dataset. since the subtype annotations from the full set of hiv-1 genomes in the 226 lanl database are typically given by individual authors using unknown methods, they may be 227 incorrect at times, potentially negatively impacting classification performance. thus, we trained our 228 classifier on the subset of hiv-1 [cit] web alignment from the lanl database, 229 accessible at https://www.hiv.lanl.gov/content/sequence/newalign/align.html. this web 230 alignment dataset is a more curated set of pol sequences, and is more likely to be correctly annotated. 231"
"evaluation datasets 198 to evaluate classifiers trained on hiv-1 sequences and subtype annotations curated by the lanl 199 database, we needed testing sets but wanted to avoid selecting them from the same database. 200"
"to more clearly demonstrate this last issue, we generate a random sequence of length 10,000 with 377 equal occurrence probabilities for a, c, g, and t, and we ask the five subtyping tools evaluated in our 378 study to predict its hiv-1 subtype. as expected, rega gives a result of 'unassigned' and scueal 379 reports a failure to align with the reference. our tool reports subtype 'u' with 100% confidence, 380 castor predicts hiv-1 group 'o' with 100% confidence, and comet reports siv cpz (simian 381 immunodeficiency virus from chimpanzee) with 100% confidence. these outcomes are consistent 382 with the disproportionately large genetic distances that separate hiv-1 group o and siv cpz from 383"
"the k-mer based supervised classification method we propose in this paper has several advantages 338 compared to other popular software packages for the classification of virus subtypes. first, we 339 have shown on several manually-curated data sets that k-mer classification can be highly successful 340 for rapid and accurate hiv-1 subtyping relative to the current state-of-the-art. furthermore, 341 releasing our method as an open-source software project confers significant advantages with respect 342 to data privacy, transparency and reproducibility. other subtyping algorithms such as rega [cit] 343 and comet [cit] are usually accessed through a web application, where hiv-1 sequence data is 344 transmitted over the internet to be processed on a remote server. this arrangement is convenient for 345 end-users because there is no requirement for installing software other than a web browser. however, 346 the act of transmitting hiv-1 sequence data over a network may present a risk to data privacy and 347 patient confidentiality -concerns include web applications neglecting to use encryption protocols 348 such as tls, or servers becoming compromised by malicious actors. as a concrete example, the 349 webserver hosting the first two major releases of the rega subtyping algorithm [cit] was recently 350 compromised by an unauthorized user (last access attempt on november 27, 2017) . in contrast, our 351 implementation is available as a standalone program, without any need to transmit sequence data 352 to an external server, eliminating those issues. in addition, our implementation is released under 353 a permissive open-source license (mit). in contrast, rega [cit] and comet [cit] are proprietary 354 'closed-source' software, making it impossible to determine exactly how subtype predictions are 355 being generated from the input sequences."
"in this paper, a variety of different datasets were used to validate the performance of the method. 169 straightforward reproducibility of results was a priority in the design of this study, and to that end, 170 every sequence and its metadata from every dataset referenced here can be easily retrieved from our 171 github repository at https://github.com/stephensolis/kameris-experiments."
"running time for datasets from table 3 kameris less than 2 seconds 16 seconds comet less than 2 seconds 14 seconds castor 3 seconds 46 seconds scueal 3 18 minutes 8 hours rega 3 31 minutes 19 hours 1 the rega and scueal web servers have limits of 1000 and 500 sequences per run, respectively. thus, 3 batches of sequences were needed for rega, and 6 batches for scueal to classify all sequences. comet, castor, and our tool have no such limits."
"relying on a remote web server to process hiv-1 sequence data makes it difficult to determine 357 which version of the software has been used to generate subtype classifications, and by extension 358 difficult to guarantee that classification results can be reproduced. there is growing recognition 359 that tracking the provenance (origin) of bioinformatic analytical outputs is a necessary component 360 of clinical practice. for example, the college of american pathologists recently amended laboratory 361 guidelines on next-generation sequence (ngs) data processing to require that: \"the specific version(s) 362 of the bioinformatics pipeline for clinical testing using ngs data files are traceable for each patient 363 report\" [cit] . in contrast to other tools, our standalone package makes it easy to use exactly the 364 desired version of the software and thus enables precise reproducibility. 365 we now discuss some limitations of our approach. like many machine learning approaches, our 366 method does not provide an accessible explanation as to why a dna sequence is classified a certain 367 way, compared to a more traditional alignment-based method. in some sense, the classifiers act more 368 as a black box, without providing a rationale for their results. another issue is our requirement for 369 a sizable, clean set of training data. as opposed to an alignment-based method that could function 370 with even a single curated reference genome per class, machine learning requires several examples 371 per training class, as discussed previously, to properly train. finally, one issue common to any hiv-1 372 subtyping tool is the fact that recombination and rapid sequence divergence can make subtyping 373 difficult, especially in cases where the recombinant form was not known at the time of training. 374 other tools are capable of giving a result of 'no match' to handle ambiguous cases, but our method 375 always reports results from the classes used for training."
"generally, the redundancy level is determined with a feedback-based method like above [cit] . however, this algorithm ignored that the bitrate of a video streaming and its redundancy varies considerably and the plr cannot be estimated correctly. the real-time bitrate is sometimes two times than abw and sometimes much less than it due to i pictures. therefore, we design iafec to enhance the feedback based algorithm."
"the goal of afec is to adjust the level of redundancy, increasing or decreasing it according to the network performance [cit] . in order to simplify the algorithm and improve efficiency, a parameter based on packet loss rate and rtp fec repair rate is applied to measure the performance of a video streaming with redundancy."
"to make sure the system could be adapted to various kinds of networks, we should monitor transmission channel state and feeds back them to the sender. in [rfc 3550 ], rtcp protocol is defined to monitor the transmission state. through analyzing rr reports from the receiver, the system will be able to calculate channel state parameters [cit] . they are bandwidth, round-trip delay, one-way delay, jitter and packet loss rate (plr)."
"now we present in this subsection the foundation of polynomials optimization, which is instrumental in solving the stability conditions derived in this paper. [cit] for detailed discussions on this subject."
"to partially verify the results in table 4, we apply the sigma function 5 in matlab, which can calculate the singular values (min γ) of a dynamical system over a fixed frequency range. by extracting the peak value produced by sigma, it yields that the system (47) with (48) and"
"the packets transport between the sender and the receiver are routed via an emulator called wanem which can emulate a wide area network. by means of this method, we can test the system under different kinds of packet lost rate conditions. while testing, we will also use other tools such like iperf to monitor the real-time network status to ensure the emulator works normally."
"rbw (mbps) is the real-time useful bitrate of video streaming at time t app(i) which represents the time when a rtcp app report responses to the sender. s represents the packet size of rtp-udp packets. moreover, sabw (mbps) is the available bandwidth for the whole channel left which is not concerned here. the estimation of sabw is not involved here."
"in addition when the original plr becomes greater such as 20%, the performance of iafec is still better than the other two schemes. it shows that a greater ri is no more useful under congested packet loss network and the ri 0.3 may be the balance between performance and congestion under the particular test environment. therefore, we can conclude that iafec is adaptive to packet loss and bandwidth limitations caused by congestions."
"given an initial r 0 together with a prescribed performance objective (no decision variables in (13)) which renders (39) to be feasible, we are interested in the following problem. problem 1. finding the minimumr or maximumŕ which render (1) to be stable and dissipative over [r, r 0 ] or [r 0,ŕ], where the dissipative constraint in (13) is given and it is satisfied at r 0 ."
"if any one of the above three conditions is satisfied, the system will begin to generate repair packets (k is equal to the number of source packets stored). another parameter n can be calculated as follows:"
"on the other hand, almost all existing krasovskii functionals in literature are based on constant matrix parameters, which is a very conservative choice when it comes to range stability analysis. this motives one to propose new functionals to specifically tackle the problem of range stability analysis considering performance objectives or even further potential constraints."
". the following lemma allows one to solve sos constraints numerically via semidefinite programming. unlike the original lemma 1 [cit], we only need to consider the univariate case."
remark 9. note that the number of decision variables of theorem 1 in table 2 -3 might be further reduced by simplifying the sos certificate variable in (20) for each case when a sos condition needs to be solved.
"the test video is encoded with h.264/avc baseline. it has a resolution of 1280x720 and duration about one minute. the system runs in three modes: non-protection, static fec and iafec with tm. the test result is shown in fig. 5 . in this figure, p means the performance of the three modes and ri represents the cost. certainly, there is no cost for the non-protection mode. the ri line of shannon limit represents the theoretical minimum cost for all fec schemes."
"proof. first of all, we will demonstrate that the feasible solutions of (24)-(25) infer the existence of (21) satisfying (12) and (10) with (11). differentiating v(r, x(t), y(t + ·)) alongside the trajectory of (1) and considering the relation"
are polynomials matrices with respect to r in line with a 3 (r) and c 3 (r) in (1). this demonstrates that the choice of legendre polynomials ℓ d (τ ) in (2) together with the forms of a 3 (r) and c 3 (r) in (1) can handle standard polynomials matrix distributed delay terms.
"when it comes to real-time calculation, one can only obtain a numerical result q ≻ 0 instead of q ⪰ 0. consequently, the membership certificate produced by numerical calculations in reality is"
"where φ d (r) and χ d (t) have been defined in (26) and (30), respectively. based on the structure of (32), it is easy to see that if"
"as shows in fig. 1, the system can be divided into 2 parts: the sender and the receiver. the sender includes 5 components and they are described as follows: the hardware is based on tms320dm6467 of texas instrument [tm] and the software is based on live555 streaming media. hence, we should fulfill our obligations under the lgpl that make the modified source code available."
"the control interpretation of this problem is straightforward: given a specific performance objective, we want to obtain the largest stable delay interval of a delay system over which the system can always satisfy the given performance objective."
"then we should determine the redundancy level for the video streaming. in real-time network applications, the quality of service (qos) will be decreased by packet lost and delay. in this paper, the two parameters are end-to-end and only caused by network bottleneck and congestions what means it has nothing to do with equipment troubles etc. hence, the redundancy is only to make the video streaming robust."
"the conditions, requirements and assumptions are shown in table ii as follows: the test is only about the performance of iafec under congested network environment. generally, we should use psnr to measure the performance but its value cannot be calculated in real-time. therefore, the rate of useful video source packets received and repaired is used instead."
"since (40) and (41) are all of range stability conditions, thus the usage of bisections will not produce false feasible solutions even (40) and (41) are not necessarily quasi-convex. furthermore, as what have been elaborated in subsection 4.3, if any inequality in (38) and (34) is affine (convex) with respect to r, then it can be solved directly for (40) and (41) (40) and (41), cannot be automatically merged together due to the nature of dissipative analysis."
"many thanks to prof. [cit] . the author also thank prof. johan löfberg for his help on the usage of yalmip löfberg (2004) . finally, the authors would like to thank the associated editor and two anonymous reviewers for their constructive comments."
"according to our statistics, about five kinds of nalus are used in network applications as shows in table i . in iafec scheme, every kind of nalu has its own weight and the system decides the level of redundancy on the basis of their weights. the initial value of index a is an experience value related to b. we define rpi (repair performance index) to measure the performance of video streaming and repairing as follows: once a rtcp app packet is received and identified, the system will calculate the rpi value and compare it with the last rpi value. then index a will be adjusted according to the result of comparison. after that, rs encoder will work. the h.264/avc video encoder applies adaptive video encoding (ave) technique while compressing data. once the transmission manager passes in a new set of parameters, ave will help the encoder producing proper video streaming which meets the bandwidth requirement of the working network. though many parameters of the encoder may affect the quality of the video streaming, we only decide to adjust the bitrate b as shown in iii.c. this method is simple and effective."
"the paper is organized as follows. in section 2 we formulate the linear cdds model to be analyzed in this paper. subsequently, theoretical preliminaries are presented in section 3 which provide necessary tools to derive the main results in the following section. in section 4, the main results on range stability analysis incorporating dissipative constraints are presented, including remarks and detailed explanations. finally, we present several numerical examples in section 5 to demonstrate the advantage of our proposed schemes."
"remark 5. note that the structure of (21) [cit] . because all the matrix parameters in (21) are related to r polynomially, thus it might be anticipated that less conservative results, in terms of range delay stability analysis, can be produced by (21) in comparison to a krasovskii functional with only constant matrix parameters."
"in this paper, we designed a network h.264/avc video transmission system prototype and tested it for applications in adaptive forward error correction (afec). a new afec scheme based on reed solomon code working with rtcp app packets is developed and validated in laboratory. the test results demonstrate the system's performance under strict bandwidth constraints and high packet loss rates. future work is to make more research in the transmission characteristic between the streaming characteristic and improve the performance of network video transmission system."
"in this paper, the solutions concerning range stability analysis for a cdds subject to dissipative constraints have been presented. the advantage of the proposed methodologies is rooted in the application of a krasovskii functional with delay dependent parameters, which leads to dissipative range stability conditions expressed in terms of sos constraints. the tests of numerical examples have demonstrated that less conservative results with less computational burdens can be produced by our methods compared to existing approaches. in addition, the proposed scheme is able to handle distributed delays with polynomials kernels when dissipative range stability analysis is concerned. meanwhile, it has been demonstrated that our approach can also handle delay margins estimation problem with prescribed performance objectives."
we assume that in one rtp-udp transmission channel the original packet lost rate is plr. then the average packet lost rate can be calculated as follows:
"the program of iafec is shown in fig. 4 . at first, an incoming h.264/avc nalu will be encapsulated to one or several rtp source packets according to [rfc6184] [cit] ."
which is now in line with the cdds form in (1). now consider a linear neutral delay system with the parameters apply theorem 1 to (47) with the parameters in (48) and
"in this paper, we propose methodologies which allow one to conduct range stability analysis for a linear coupled differential-difference system (cdds) [cit] subject to dissipative constraints. the linear cdds model considered in this paper contains distributed delay terms with polynomials kernels, which is able to incorporate many models of time delay systems as special cases. a novel liapunov-krasovskii functional, with delay-dependent matrix parameters, is applied to be constructed together with a quadratic supply function to derive stability conditions. the resulting sufficient conditions, expressed in robust lmis, are able to ensure the range stability and dissipativity of the linear cdds over a known delay interval. [cit] to equivalently transfer the original polynomials optimization problem into semidefinite programs with finite dimensions, without introducing any potential conservatism. furthermore, the proposed scenario is extended to handle the problem of estimating the margin of a stable delay interval with given dissipative constraints. finally, we also prove that the resulting stability conditions in this paper exhibit a hierarchical feasibility enhancement similar to the one in ."
"usually, afec researchers need some more information like average burst loss length to help determining the level of redundancy in their schemes [cit] . similarly, we use customized rtcp app reports to establishing an interaction between the sender and the receiver and feedback the extension information. the format of app packets is shown in fig. 3"
the aim of the nsga-ii algorithm is to obtain a good estimation of the pareto front of a multi-objective problem through a genetic optimization process. it finds an evenly distributed range of solutions along the pareto front by combining ga with the nondominated sorting algorithm and the crowding distance calculations.
"securing communications in manets is one of the biggest challenges for system architects. there are a variety of attacks that can occur in every network layer (the attacks that manets are vulnerable to are classified in table 1 ). although it is desirable to introduce effective security solutions that can reveal attackers in a network and there are several existing manet security solutions, attack prevention is even more desirable. to minimize malicious actions, it is necessary to introduce an effective trust system in manets to achieve various system objectives, including reliability, scalability, and availability [cit] . to introduce trust into communication networks under critical conditions, system designers should consider various perspectives (e.g., node heterogeneity, rapid changes in network topology, a lack of predefined trust relationships, and the resource-constrained environment of mobile nodes [cit] ). in manets, trust management has been introduced in various forms, including trust establishment [cit], trust updating, and trustworthiness [cit] . some researchers have introduced an authentication mechanism [cit] to ensure links are safe prior to communicating within the network. however, because manets are continuously changing [cit], the owner of a secret key could be a malicious entity, even if there are no malicious outsiders with unauthorized identities. furthermore, it is difficult to control the authentication process through a single node. generally, trust operations are limited to local access, which introduces the problems of incompleteness and vagueness. hence, it is critical to reconfigure networks seamlessly and consistently."
"the optimization program, coupled with the emulator and the component database provides a tool that allows the exploration of the design space and the study of the impact of different architectural choices and parameters. it is found that the performance improvement introduced by operating the processor regions at different clocks is offset by the necessary delay introduced by wrappers needed to communicate between the asynchronous regions. with a two clock-periods delay, the minimum processor delay of the asynchronous case is 311% of the delay obtained in the synchronous case, and the minimum consumed energy is 308% more in the asynchronous design when compared to its synchronous counterpart. the instruction region has been also identified as a major design bottleneck. for the synchronous case, the pareto front contains solutions with 4 regions that minimize delay and solutions with 7 regions that minimize area or energy. a minimumdelay design is selected for hardware implementation, and the fpga version of the optimized processor is tested and correct operation is verified for aes and rc6 encryption/decryption algorithms."
"moreover, this paper considers to reduce the unnecessary bandwidth and energy usage by introducing the collaborative detection process based on predefined principles. most security solutions in olsr, including dcfm, force nodes in the network to perform excessive operations based on frequently updated topology information, which is periodically exchanged and sometimes contains additional messages, which incur unnecessary bandwidth and energy usage. furthermore, nodes are suspicious of each other since they are on their own without any authentication process. this gives attackers a chance to compromise networks easily and manipulate residential nodes repeatedly which makes the overall network efficiency is decreased."
"suppl. fig. 7 shows the results of running the aes encryption program on the fpga-implemented crypto processor. the resulting waveforms are screen captured from logic analyzer model agilent 16851a as fed from the hardware through the chipscope module. in suppl. fig. 7, signals configdataout_0_obuf through configdataout_10_obuf show part of the data memory output bus carrying the results of encryption. this result is for a case where plain text is 32 43 f6 a8 88 5a 30 8d 31 31 98 a2 e0 37 07 34, the key is 2b 7e 15 16 28 ae d2 a6 ab f7 15 88 09 cf 4f 3c and the expected (and obtained) result is 39 25 84 1d 02 dc 09 fb dc 11 85 97 19 6a 0b 32. aes encryption bits match the results of the software emulator and -in addition -the number of cycles in the fpga implementation matches those obtained from the emulator. another test is carried out which involves the encryption/decryption of a stream of blocks, and this passed successfully."
"here, we propose integrated copy number variation caller (icnv), a statistical framework for cnv detection that can be applied to multiple study designs: wes only, wgs only, snp array only, or any combination of snp and sequencing data. compared to existing approaches, icnv improves copy number detection accuracy in three ways: (i) utilization of b allele frequency (baf) information from sequencing data, (ii) integration of sample matched snp-array data when available and (iii) integration of improved platform-specific normalization for sequencing coverage. icnv produces a cross-platform joint segmentation of each sample's genome into deleted, duplicated and normal regions and further infers integer copy numbers in deletion and duplication regions."
"the main contributions of this research is the proposal of a modified version of the genetic algorithm -known as nsga-iiand linking it to a component database to perform design space exploration, building a processor emulator that is invoked to calculate the solutions cost and building estimation models for the design metrics used in the optimization process."
"-at generation # 1, processor area ranges between 6.7 mm 2 and 8.0 mm 2, its delay performance ranges between 10.0 ls and 22.5 ls, and its energy consumption ranges between 9.0 lj and 18.0 lj. when reaching generation # 50, range of variation narrows for all objective functions. starting from generation #200 and thereafter, the range of variation stabilizes at a still narrower window, where area ranges between 6.7 mm 2 and 7.5 mm 2, delay performance ranges between 9.5 ls and13.0 ls, and energy consumption ranges between 8.0 lj and11.0 lj."
"the olsr [cit] protocol is a proactive routing protocol for manets. it is an optimized version of the classical link state routing protocol that reduces control message overhead. the nodes in a manet select an mpr node to provide maximum coverage for their two-hop neighbors and reduce the number of control messages. with a minimum number of mpr nodes, a node can reach its neighbors through a small number of transmissions without any duplicated messages, as shown in figure 1 . mpr nodes are responsible for relaying both control messages and data messages."
"therefore, from attack transaction validation to block dissemination, the process requires approximately 10 s, as indicated in the figure 13 . depending on the time at which an attacker is caught by a victim node, the latency could be a slightly lower or higher than 10 s. for example, if an attack detector node becomes the delegate, the communication time required to relay attack information to the delegate is reduced, leading to even better results in terms of block latency. additionally, when colluding attacks occur in a network, the block time and transaction ratio are even lower. for example, if two different attackers launch an attack in different places simultaneously, two attack transactions will be included in a block and the effectiveness of the proposed scheme will increase."
"unlike conventional networks, mobile ad-hoc networks (manets) are cooperative networks composed of mobile nodes. without any fixed infrastructure or centralized administration [cit], the nodes randomly move and communicate with each other both directly via wireless connections and indirectly via communication with other nodes outside the wireless range through relay nodes. this mode of communication induces the multi-hop phenomenon [cit], where intermediate nodes act as routers for relaying packets to destination nodes. consequently, several routing protocols for manets have emerged, including optimized link state routing (olsr), ad-hoc on-demand distance vector routing (aodv), and dynamic source routing (dsr). based on these routing protocols, the nodes themselves choose a routing path to the destination and forward packets [cit] ."
"as standard cryptographic algorithms -such as des [cit], rsa [cit], ecc [cit], and aes [cit] -were adopted, researchers and technology firms devoted considerable effort and time to develop efficient implementations in software and hardware. initially, attention was directed to achieving high throughput as well as low cost and/or low power consumption for specific algorithms such as aes (see [cit] and the references therein). considering the fact that cryptography -by its very nature -is always ever-changing, the need for a flexible platform that can implement a wide range of cryptographic primitives, algorithms, and protocols was soon recognized. since the late 90s, activities concerning the implementation of multiple security algorithms have centred around three main approaches: customized general purpose processor (gpp) [cit], crypto co-processor [cit], and crypto processor [cit] . while throughput was almost always the prime metric, other figures of merit were sought such as flexibility and security. this trilogy was used for evaluating various proposals, along with the usual design considerations of surface area, cost, and power consumption, [cit] ."
section ii of this paper describes the proposed coders. their performance is evaluated in section iii through experimental results. the last section provides concluding remarks.
"the proposed coder codes binary symbols using codewords of bits. with some abuse of notation, let the lower bound of the interval be denoted by . the size of the interval minus one is denoted by . both and are stored in integer registers. initially, and . the operation to partition the interval uses integer arithmetic since the latency of integer multiplications in modern processors is (almost) one clock cycle [cit] . when the coded bit is 0 (i.e., ), the size of the interval is reduced to (1) and is left unmodified."
"a novel data flow-oriented crypto processor based on the transport triggered architecture (tta) was proposed [cit] . the architecture comprised function units (fus) which were selected to cover all arithmetic/logic functions typically encountered in encryption/decryption algorithms. a fu would not store its output in a common memory (as is typical with the von neumann architecture), but rather it would feed its output directly to one (or two) fus waiting for such output as an operand. to allow execution of security algorithms in a parallel mode, the fus are distributed among several execution regions (ers). each of the ers, as well as an instruction region (ir) and an interconnection region (called global interconnection network -gin) operates synchronously at its own clock frequency, while regions communicate asynchronously. this gives rise to a globally asynchronous locally synchronous (gals) architecture. this architecture allows higher throughput, and the decoupled structure of the gals units makes it possible to clock gate idle regions, thereby reducing the amount of dissipated power. finally, the asynchrony of regions, in addition to a novel data scrambling technique can render the processor more immunity against side channel attacks."
"an integer copy number is assigned to each cnv region after the detection step. the assignment is based on maximizing a likelihood function that quantifies the probability of the observed normalized intensities and bafs for each copy number state. the data input (normalized intensities and bafs from all platforms) are shown in figure 7, along with their copy number assignments shown as contours. the marginal densities of the normalized intensity values for each platform seem to be well modeled by a mixture of normals with platform-specific mean and variance. the baf is much noisier and do not show any platform specific trend. thus, the likelihood model we use is based on a mixture of normal for the normalized intensities, with platform and copy number specific means determined by an initial k-means clustering step, and a mixture of truncated normal for the baf with pre-fixed means and standard deviations. the maximum likelihood copy number state is assigned to each segment."
"most arithmetic coders employed for image and video coding produce variable-to-variable length codewords. this is, a variable number of input symbols are coded with a codeword of a priori unknown length. practical realizations of arithmetic coders operate with hardware registers of at most 64 bits, so the generation of a single -and commonly very long-codeword is carried out progressively. the main idea to do so is the following. let denote the current interval of the coder, with and being the fractional part of the lower and upper bound of the interval stored in hardware registers. assume that the leftmost bits of the binary representation of and are not equal in the current interval. when a new symbol is coded, this interval is further reduced to . if the leftmost bits of and are then equal, all following segmentations of the interval will also start with those same bit(s) since . this permits to dispatch the leftmost bits of and that are identical and to shift the remaining bits of the registers to the left. this procedure is called renormalization. it represents a non-negligible part of the coder's workload since these operations are executed intensively."
"also, since the instruction region (ir) could represent a bottleneck for the processor operation, the effect of speeding up the ir is considered. thus, for each of the above cases, the optimization process is repeated assuming that the ir clock period is reduced to nearly 50% and 30% of its original value (integer values are used)."
"to verify the effectiveness of the proposed scheme, the period of vulnerability to nias in a network with the proposed scheme was compared to that in a network with dcfm. figure 11 presents the overall vulnerability periods based on the time required to detect attacks versus the percentage of newly introduced attacks in the network. according to the standard olsr protocol, the hello and tc message intervals are two and five seconds, respectively. therefore, attack detection can take up to five seconds depending on when an attack is launched. in this simulation, there were ten different potential attacker nodes in the network. we simulated ten sequential attacks at different time intervals. the newly introduced attack ratios varied from 0% to 100%, as shown in figure 6 . according to the dcfm, the nodes are guarding themselves and there is no information being shared between nodes regarding the attacks. therefore, the nodes must perform the detection process whenever a new attack is introduced. in the proposed scheme, attack information is shared through the network and nodes do not have to repeat attack mitigation tasks that were already performed by other nodes. therefore, even when the same attacker targets a different victim, a node in the blockchain olsr scheme is not vulnerable to the attack. in contrast, multiple nodes in the dcfm scheme can be subjected to the attack until it is detected. as a direct result, the vulnerability period of the network with the blockchain olsr scheme is reduced, as is the ratio of new attackers. table 3 shows the percentage of messages received by the victim node when there is an attack and when there is not in the network. in this scenario, we use an attacker node to launch near two different victims. when there is no previous block and also dcfm rules are not applied, it is sure that the adversary had successfully launched the attack and the victim got very few messages. when the rules were using in the absence of block, victim can receive nearly the same percentage of messages as in normal scenario without attack. however, in the third and fourth columns, the percentage of message received is even a little higher than the case of detecting with dcfm. because of the block distributed in the network, the nodes are directly neglecting the attacker regardless of using dcfm or not. with our proposed scheme, even when the same attacker attacks the different victim, the victim node can still receive the same percentage of messages with no attack scenario. with new attacker in the network, the victim receives the same percentage of messages with dcfm even though it could be a little less than no attack scenario. therefore, the proposed scheme is proved as an effective solution in terms of security value, too. figure 12 presents an overhead cost comparison between the olsr and dcfm schemes. overhead is measured based on the average tc message size. our scheme creates a block after detecting an adversary, so it incurs overhead for distributing the block through the network via tc messages. because adversaries can be detected within five seconds after an attack begins and the tc interval is five seconds in the olsr scheme, a block will be disseminated within 10 s after an attack. in every second of the tc interval after a malicious node is identified, the overhead will be slightly greater than that of the dcfm scheme. however, consider a scenario in which six attacks are launched in a network. we launched three different attacks in the first part of the simulation (100 s). the three attackers were detected and three blocks were disseminated through the network. when those attackers attacked different victims during another 100 s period, the detection process was no longer necessary and the attackers were completely ignored by the nodes in the network. even though six attacks occurred in the network, no additional detection overhead was incurred for the same attackers. therefore, the overhead of our scheme is significantly reduced for repeated attacks from the same attackers. as shown in figure 12, the overhead of the proposed scheme is lower than that of the default olsr scheme because the number of nodes in the network is reduced. this is because previous attackers are ignored in the network. therefore, it can be assumed that the overhead of the proposed scheme is the same as that of the conventional olsr scheme."
"when the victim node detected adversary a, according to the dcfm scheme in section 2, rule 1 incurred computational complexity of o(n) during the hello interval and rule 2 incurred computational complexity of o(n 2 ) during the tc interval. the same process is used to detect new attackers in the proposed scheme. because there were no blocks in the network, our scheme incurred the same complexity. next, attacker b launched an attack and was detected by a node in both schemes. subsequently, a different victim was attacked by b again. attacker b was re-detected by the new victim in dcfm, but not in the proposed scheme. because there was a block containing attacker b in the network, every other node in the network was aware that b was an adversary. therefore, they did not need to perform the detection process, which reduced the computational complexity to o(1). because the dcfm checks if a specific node is an adversary, the necessary computational complexity is o(n) in every hello interval. however, in our scheme, there is no additional complexity incurred for repeated attacks from the same attackers."
"with the above design goals in mind, we adopted a blockchain architecture to manage trust evaluation and maintenance in a manet environment. specifically, we implemented a public blockchain architecture to eliminate the intensive resource consumption and long validation times of current blockchain technology in a dynamic and latency-sensitive environment. based on the requirements of manets, the proposed blockchain-based distributed trust establishing system can be divided into the components presented in figure 5 ."
"calculated trust information is included in blocks such that every node in a network can consistently access the information simultaneously. in a manet environment, because there is no centralized party that handles blockchain processes, the consensus process must be performed by routing nodes in a distributed manner. delegated proof-of-trust (dpot) is introduced to compensate for the downsides of other consensus mechanisms by considering manet characteristics. for example, proof-of-work (pow), which uses computationally complex operations to perform validation, is not a suitable choice when energy consumption is a primary concern. although another candidate mechanism called proof-of-stake (pos) works well in resource-constrained environments, the \"nothing-at-stake\" problem cannot be ignored. because this algorithm uses predefined tokens (stakes) to identify validators, it is impractical to apply it to a temporary ad hoc network. furthermore, its biggest limitation is that it gives the richest nodes in the network get the greatest chance to become validators, meaning it lacks fairness. therefore, we propose dpot as a new consensus algorithm for dynamic and resource-hungry environments."
"the objective functions of processor area, energy consumed in executing the aes encryption algorithm and total delay for executing this algorithm, will be used to compare between solutions in the design space. for a given design, the total area of the processor can be estimated as:"
"in this section, the asynchronous operation of the proposed architecture is compared with the synchronous operation, [cit] . in a number of previous works [cit], the advantages of gals architectures including the avoidance of clock distribution problems and the possible reuse of ip components that have independent clock requirements were shown. however, the asynchronous communication between regions using mechanisms such as fifos or wrappers could incur delay and power overheads that may offset the benefits obtained by using various clocks. this can cause the synchronous design to outperform the asynchronous design in delay and power consumption. in this section, this phenomenon is studied as applied to the proposed architecture by comparing the optimal performance measures in the cases of asynchronous and synchronous operation when executing the aes encryption algorithm."
"the first set of experimental tests assess the coding efficiency and computational throughput when coding artificially generated symbols. the symbols are generated assuming that they are independent and identically distributed. a generalized gaussian distribution (ggd) with parameter and support in the range is employed to generate the probabilities of the symbols. through this method, the probabilities of the symbols are from almost 0 to almost 1, though most symbols have a probability close to . the experiments below report the performance achieved with and separately to appraise the coders in different conditions. the sequences employed in the tests have and symbols to evaluate coding efficiency and computational throughput, respectively. all coders are programmed in java. all tests are performed with an intel core i7-3770@3.40 ghz using a java virtual machine v1.7. fig. 2 evaluates the coding efficiency achieved by the proposed coders when using codewords of different length. [cit] and the m coder of hevc. the vertical axis of the figures is the coding rate, expressed in bits per sample (bps), whereas the horizontal axis is the codeword length (i.e., ). fig. 2(a) and (b) report results when the real probability of the symbol is fed directly to the coder, i.e., when is used instead of probability estimates. fig. 2(c) and (d) report results when the probabilities of the symbols are estimated through the context-adaptive mechanisms described before. the experiments that employ context-adaptive mechanisms utilize eight contexts. the probability of a symbol is always in the range . this range is divided into eight uniform intervals and each one is assigned to a context. all symbols whose probabilities fall within an interval are coded with the corresponding context. flw and fl2w use and (see below). the results of fig. 2 indicate that the longer the codeword employed by flw/fl2w, the higher the coding efficiency achieved. this is because the longer the codeword, the less often the interval is reset. when is utilized, the coding rate achieved by fl2w for codewords of 32 bits or longer is almost that of the source's entropy. for long codewords, flw (fl2w) achieves a coding efficiency 1% (1.4%) higher than that of the mq coder when and almost equal when . compared to the m coder, flw and fl2w achieve virtually the same coding efficiency. these results suggest that arithmetic coders can be implemented without the renormalization procedure while achieving high efficiency."
"in this paper, we proposed a novel approach for generating distributed trust in manets by adopting the blockchain concept. simulation results demonstrated that distributed trust (i.e., quickly making every node in the network aware of an attacker and any relevant tvs) provides strong network security. even when an attacker changes their location and attacks different nodes, the network is safe. no information or additional time is lost and overall complexity is reduced. additionally, based on collaborative detection, each node's individual responsibility is significantly diminished. the denser the network, the lesser the detection responsibility of each node. furthermore, by using blockchain-based distributed and tamper-proof access of the trust levels of residents in the network, manets can fulfill their system goals such as reliability, scalability, and availability. in future work, we wish to test the feasibility of our proposed scheme with various routing protocols in manets."
"with regard to the efficiency of the context-adaptive mechanisms, the results of fig. 2 indicate that the variable-size sliding window employed by flw/fl2w is competitive. for long codewords, flw (fl2w) achieves a coding efficiency 3.3% (3.6%) higher than that of the mq coder when, and 2.2% (3.1%) higher when . compared to the m coder, the performance of flw and fl2w is 0.8% and 1.6% higher, respectively. another observation of fig. 2 is that the selective interval coding employed by fl2w works well. the coding efficiency achieved by fl2w when using two codewords of length is higher than that achieved by flw when using a codeword of length . fig. 3 evaluates the computational throughput. the vertical axis of the figure is the execution time, whereas each column reports the results for one coder. in this test, the coders employ the context-adaptive mechanisms for probability estimation. the results indicate that the flw coder always achieves a higher computational throughput than that of the mq coder. compared to the m coder, the computational throughput of flw is, in general, slightly superior. as seen in the figure, the longer the codeword employed by flw, the lower the execution time. a similar behavior is obtained when context-adaptive mecha- nisms are not employed, though the execution time of all coders is slightly lower (not shown in the figure) . though it depends on the probability distribution of the source, approximately 20% of the total time spent by the proposed coders is devoted to probability estimation. similar results are obtained for the decoder (not shown due to page constraints)."
"translation, mapping, and placement and routing are performed on the synthesis results with the addition of a chipscope component used to probe certain signals from the design for verification purposes. the constraint file demands a clock that fits the critical path and assigns pin locations so they can be probed in the hardware setup. although the fpga is almost entirely utilized by the design, judicious choices of constraints and optimization effort lead to a timing closure and successful generation of bit file. a clock speed of 13 mhz is attained with moderate or high optimization effort. this translates into a bit rate of 1.66 mbps for a single processor running aes encryption."
"based on their flexibility and robustness, regardless of geographic location or proximity to infrastructure [cit], manets are applicable to various interactive application scenarios, including military battlefields, commercial sectors, and other civilian environments. in military scenarios [cit], while manets can provide information network technology for soldiers and vehicles through scattered devices on the battlefield, an enemy can compromise devices and make the network vulnerable, allowing them to disseminate malicious information, discard sensitive information in multi-hop scenarios, or extract critical data. additionally, in after-war scenarios, an enemy can continue to scatter devices in an attempt to obstruct recovery operations or coordinate sudden attacks. manets are also widely applied in the commercial sector, as well as in emergency and rescue operations following natural disasters. during emergency and rescue operations, manets are also prone to vulnerabilities. for example, when multiple countries cooperate to recover from natural disasters, the data that some of the countries prefer to secure as private and sensitive could be exposed through manets [cit] ."
"to infer the integer copy number, we use a maximum likelihood procedure. for each cnv region, we infer deletion copy number and duplication copy number separately. deletion can only be 0 or 1 copy. for duplication, we only characterize two cases: three copies or greater than three copies. we find it very difficult to separate copy numbers 4 or greater because (i) such events are rare, thus making it hard to infer the mean of their distributions; (ii) baf distributions among four, five or more copies are too similar to be separated. the likelihood calculations are shown in the supplementary method. we assign the maximum likelihood copy number to each cnv region."
"based to the dynamic and limited features of manets, even though a node is chosen as the validator, there could be the time when it cannot do the block generation process. therefore, we consider a delegation process for the validator node that is influenced by the delegated proof-of-stake (dpos) algorithm, where the validator from the pos mechanism can vote on whether or not other nodes should be trusted as delegates. similarly, in the proposed scheme, the original validator can select a delegated node to perform the role of the original validator on its behalf. if the validator node is subjected to the following conditions, the delegation process will occur. otherwise, it will be put in blacklist and cannot get the chance to join this network anymore."
"where p ir (n) is the power consumed by the ir as function of the number of ers, p gin (n) is the power consumed by the gin as function of the number ers, p is (f i ) is the power consumed by the is in er i as function of the number of fus in er i, p lin (f i ) is the power consumed by the lin in er i as function of the number of fus in er i, p fu (j) is the power consumed by the fu number j, and d is the total delay for executing the program. the values for the power terms in eq (2) can be expressed in terms of a dynamic component and a static (idle) component, along with the busy period and idle period for each term. specifically, each power term can be expressed as:"
"in figure 6, we demonstrate phase 1 with two attackers in manet. after phase 1 is completed, the trust tables of the nodes will be updated. for example, in node d's trust table, all of its neighbors will be evaluated according to the equations presented above. because node x is an attacker, its old value of 0.1 will be updated via multiplication by −1. nodes e and f receive an α value 0.1, while nodes b and g receive values of 0.5 because they are mpr nodes for node d. furthermore, in this network, nodes d, g, and f can perform collaborative detection according to the principles outlined above."
"blockchain technology has become popular based on its strong security. [cit] . based on its strong security and decentralized management, it has been applied to various types of monetary transactions. in summary, several transactions are performed in a transaction pool. several nodes attempt to create blocks of unspent transactions to perform a mining process, resulting in a proof-of-work (pow), which indicates that a block has been created, accepted, and chained into the existing blockchain. after chaining a block into the chain, it is difficult to change information in the block because new blocks can be chained only when a miner obtains the correct nonce that can generate a valid hash value. hash values are generated based on the nonce that is mined and the previous block hash."
"another approach is to handle one objective function and to treat the value of other objectives as constraints in the optimization problem, e.g. minimize the power consumption under a delay constraint or vice versa. again, it is usually difficult to determine the bounds a priori and repeating the solution for different values is often necessary."
"for wgs data, bin length affects analysis results. in the tradeoff between sensitivity and specificity, larger bin size increases specificity while decreasing sensitivity. we have found 1 kb to be a good default value. users can customize longer or shorter bins depend on analysis goal and sequencing coverage. icnv is implemented in r and is available on bioconductor (under revision) and github (https://github.com/zhouzilu/icnv). for chromosome 22 on 75 samples, the run time for joint detection (wgs and snparray) is less than 5 min on a 16gb-ram laptop. runtime scales linearly with number of samples and genome size. icnv allows parallelization across samples once the codex normalization step is finished, and the entire procedure can be parallelized across chromosomes. collectively, icnv provides a systematic framework and an efficient, scalable toolset for single and cross platform cnv detection."
"consequently, crypto processors have been proposed to retain the flexibility of customized gpp and still approach the higher throughput exhibited by crypto co-processors, by implementing the additional cryptographic functional units in a separate tightly-coupled or loosely-coupled processor. the function units are either realized using customized alus or systolic arrays. other designs that exhibited flexible implementation of cryptographic algorithms using crypto processors have been also proposed. in one instance, the alu is replaced by a set of function units (fus) connected to a common bus so as to allow data flow implementations of a cryptographic algorithm [cit] . it is called transport triggered architecture (tta), deploys move instructions, and allows improved throughput performance when compared to the classical von neumann approach. by using a reconfigurable fpga engine, the type and number of deployed fus are adapted to various security algorithms."
"both the ir and gin are allowed to have clocks that best suit their operation. according to the gals paradigm, signalling inside each er is controlled by a single clock (synchronous operation), but the different ers may have independent clocks and therefore operate asynchronously. this necessitates the use of wrappers between the various regions. in fig.1, asynchronous communication uses request (r)/acknowledge (a) signals through a double ff mechanism."
"further, one of these operators is chosen to generate an offspring from each selected solution with a specific user selection probability. thus, ga is used in combination with local search, which has been reported to be efficient in a number of similar problems."
"the final rule is that node n should check every hello message containing its neighbors if the sender is suspicious. these rules should be applied sequentially. in this scheme, all nodes in the network perform these processes repetitively in the hello interval and tc interval whenever a hello message is suspicious. because nodes perform the attack mitigation process individually without trust considerations for other nodes, in the case where there is no attacker in the network, it is wasteful for every node to be protective all the time. furthermore, some nodes in the network must add virtual nodes under certain conditions when an attacker is intelligent and makes negligible changes to the control messages exchanged."
"the multi-objective optimization problem generally, it is not expected that a single design minimizes all the objective functions simultaneously. in fact, these objectives may conflict with each other and a solution that minimizes one objective may result in unacceptable values for other objectives. a trade-off between objectives is thus necessary. one way to handle multi-objective optimization is to combine objective functions into a single composite function, e.g. a weighted sum or product. the problem with this approach is that a designer has to determine a priori the weight given to each objective. it is usually very difficult to quantify the desired trade-off, in terms of weights that are assigned to objectives having different dimensions and units. also, in this approach a single solution is obtained without giving the designer a view about the range of possible trade-off solutions."
"the main drawback behind the use of codewords of fixedlength is that the coding efficiency is penalized when the size of the interval is small. let us illustrate this point with an example. assume that the size of the current interval is 2 and that the next symbol to code has a high probability estimate, say 90%. although the coding of the most probable symbol should spend a fraction of a bit, its actual coding spends a full bit because the interval can only be divided in two subintervals of equal size. so, in practice, it is like if the coding of this symbol had employed a probability estimate of 50%."
"the function of the ir is to hold the instructions of the algorithm to be executed, and to dispatch instructions to appropriate ers. an instruction fetcher within ir fetches instructions from the instruction cache. to allow regions to work in parallel, each fetch operation can get up to n instructions, where n is the number of ers. an arbiter unit then forwards the fetched instructions to the appropriate ers, after checking that such ers are ready to handle a new instruction. to enhance the processor's ability to combat sidechannel attacks, the order by which instructions are executed is randomly altered, and a random selection is made among internal signals targeting the same fu."
"for efficiency, this string is actually stored and handled in program as a string of integers. the initial population is generated randomly, i.e. individual solutions have random distribution of fus on regions. generation of initial population thus takes relatively insignificant time. when a random initial population is generated, a minimum and maximum number of regions are specified (e.g. 3 to 10 regions). a number of individuals is generated for each size between these limits to cover a large range of the solution space. for a random solution with n regions, the region in which each fu is placed is selected at random in the range from 0 to nà1."
"algorithm 1 details the encoding procedure of the proposed arithmetic coder with fixed-length codewords (flw). the notation is that employed in the previous discussion except for the variables of probability estimation, which are arrays accessed via the context for which they are computed [cit] . the probability estimation is carried out in lines 1-10. is initialized to so that the size of the window is extended to at the beginning of the coding. we note that after this point, the actual number of symbols within the window is, though it is not considered in line 2 since it does not affect coding efficiency. the interval division is performed in lines 11-18, whereas the dispatching of the codeword is carried out in lines 20-24. the decoder has a structure similar to that of the encoder (not shown due to page constraints). as well as most context adaptive coders, the bitstream generated by this (and the following) algorithm does not have error recovery properties. if needed, they could be included by using segment markers."
"in this paper, we propose a blockchain-based trust establishment system that provides distributed, consistent, and tamper-proof trust to nodes in manets. blockchain as a potential solution for trust management, it has been actively researched in various fields, including wireless networks [cit] and the internet-of-things (iot). to take advantage of the decentralized nature of blockchain technology, one must consider the limited resources of manets when designing a trust system. for example, implementing a blockchain implementing a blockchain with proof-of-work (pow), the complex computational consensus mechanism and long validation time in manet nodes without any centralized party would be highly impractical. therefore, we propose a lightweight consensus algorithm for trust management in manets called delegated proof-of-trust (dpot), which achieves reasonable validation times based on peer-to-peer (p2p) concepts."
"the encryption/decryption program instructions are stored in the instruction cache of the ir, while the input data (such as plaintext, ciphertext, encryption keys, decryption keys, and substitution box values) are stored in the writereg and readsbox fus."
"step 2: remove candidate solutions that appear more than once. table 3 depicts the outcome of this step which shows three candidates with minimum area (1, 2 and 3), two candidates with minimum delay (4 and 5), and one candidate with minimum energy (6) . the number of regions for these candidates is, respectively; 7, 4, and 7. table 4 depicts the mapping of fus for the six candidates."
"each fu stores data and instructions in its mibs, where each instruction is matched to corresponding operands according to their tags. when an fu receives the appropriate operands, along with the associated instruction, the fu executes its operation and forwards its outcome to the destination fus. mibs allow the results to wait for their instructions and also the instructions to wait for their data to arrive. compared to the common register file used in tta architecture, this requires much less address decoding time and buffers can be read and written back in parallel. typically, the size of fu buffers, specified at the design stage, is limited. it is to be noted that if the fu mib size is too small, some instructions may be lost, and if the fu mib size is too large, there will be wasted area in the processor."
"in order to estimate the execution times, as well as the busy and idle periods of different components in a given configuration, the programmable data-flow crypto processor is emulated using the c# programming language. the following specifications have been targeted during the development of the emulator: it is to be observed that the logic used by the various hardware modules of the crypto processor has been mimicked when developing the software for the emulator. for example, the determination of the program counter values follows the exact logic used by the hardware circuit. moreover, in order to have a cycle accurate emulation of the crypto processor, the delays experienced by all signals as they propagate through the ir, er, and the gin have been inserted into the program. it is to be noticed that the emulator software has been modified to allow its integration with the nsga-ii algorithm. this necessitates that the emulator reads text files with specified formats describing the input parameters, and then outputs the results in text files of appropriate formats."
"the crowding distance is a measure of how an individual is close to its neighbours in the objective functions space. it assigns a further fitness value for each solution, which is higher as the solution is farther from its neighbours. selecting solutions with larger crowding distance results in a better diversity of outcomes, and thus the obtained solutions are evenly spaced along the pareto front."
"as a starting point, the attacker positions itself within the broadcast range of the victim and identifies its two-hop neighbors by exchanging hello messages with the victim. it then produces a fake hello message indicating that it is a one-hop neighbor of the victim's two-hop neighbors. consequently, the victim will choose the attacker as its sole mpr based on the routing protocol, which requires a minimal set of mprs. therefore, the attacking node is the only node that can forward link information from the victim and it simply discards the victim's messages. as a result, other nodes cannot receive any information from the victim, so will be removed from the network topologies of any other nodes."
"we generated a network of 30 nodes with random positions. sender, node isolation attacker, and victim nodes were predefined and the attacker node was positioned one node away from the victim along the route from the sender node to the destination node. the simulation environment and parameters are summarized in table 2 ."
"speed is the first important primary objective of the design. the speed of a design can be expressed by different metrics such as the throughput achieved for computations or the latency/response time for certain events. the average and/or worst-case latency needed to perform the encryption or decryption algorithm on a block of data is an appropriate performance measure for the crypto processor. this follows since these operations will have to be performed repeatedly in a typical usage of the processor and will affect the overall system performance. since the advanced encryption standard (aes) - [cit] -is the most popular algorithm among all symmetric key cryptographic techniques, it has been chosen as the reference security algorithm for optimizing the programmable data-flow crypto processor."
"the energy consumed by the processor to perform the encryption or decryption process on a block of data must also be emphasized in the design. this is important for embedded batteryoperated systems to prolong the battery life-time. even for highend systems optimized for speed, one needs to consider the generation of heat within the system that degrades the life-time of the components. again, the consumed energy will be affected by all the considered design choices."
nsga-ii sorts a given population based on non-domination into a number of fronts. the first front is the non-dominated set in the current population and the second front is dominated only by individuals in the first front and so on. each individual is assigned a rank (fitness) value based on front on which it lies.
"the bit file is programmed to the target fpga. the program and input data are first fed to the storage memories through a test mode setting. functionality is confirmed in two ways. first, onchip assertion is performed through a hardware test bench that performs bit matching on the plain text and encryption results. secondly, a logic analyzer attached to specialty software and interface allows real-time display of signals from the chipscope."
"the number n of ers in the processor is a major design parameter. it will determine the degree to which programs can benefit from parallelism and asynchrony. the number n will also determine how many instructions are fetched within the ir whichin turn -determines the complexity of the instruction fetcher and the instruction arbiter. it will also determine the width of buses for data and control interconnecting ir and ers, as well as how many ports and input buffers the gin should deploy. this will have an important impact on processor area and consumed energy. the manner in which fus are distributed among ers is another major design option. the fus mapped to an er will operate at the clock frequency dictated by the slowest fu in the er. this may slow down some fus and increase the delay of executing some algorithms. also, the number of fus within an er will determine the number of ports of this region's is and lin, hence affecting their complexity, area, and energy consumption."
"the architecture of the novel crypto processor proposed previously [cit] inherits features both from the transport-triggered paradigm and the dataflow paradigm. thus, instructions control the bypass circuits rather than the fus; the fu operation is triggered by the presence of its operands; and the results are passed between the fus instead of returning back to a register file. all fus can work in parallel and fast fus do not need to run at the same speed as the slow fus, which can lead to further improvement in the processor performance. fig. 1 shows a block diagram of the processor. the design includes 27 fus needed in the execution of major encryption/ decryption algorithms. table 1 shows the 27 fus included in the processor together with the clock period of their asic implementation. the fus are grouped into a number of ers, each operating at a given clock frequency. normally an er will include fus with close latencies and will operate at a clock period dictated by the slowest fu it includes. within the er, an instruction switch (is) is used to forward an incoming instruction to the designated fu. matched input buffers (mibs) are provided inside an fu to store incoming instructions and operands. meanwhile, results from an fu can be either sent to fus inside the er using a local interconnection network (lin), or else sent to other fus in other ers through the gin."
"the ga method is a general method that can be applied without particular requirements in the characteristics of the search space. it incorporates knowledge of the design space acquired gradually through iterations, which results in faster convergence toward desired solutions. ga method evaluates a number of solutions rather than a single solution -in each iteration -and thus it could be readily modified to obtain pareto fronts for multi-objective problems. in this paper, the solution of the optimization problem is based on the non-dominated sorting genetic algorithm-type ii (nsga-ii) [cit] ."
"to validate the functionality of the deduced architecture in hardware, the entire design is written in vhdl and is implemented on a xilinx virtex 6 fpga. the fpga of choice is xc6vlx240t package ff1156 speed grade à1, as found in the xilinx evaluation board ml605. verification of functionality and performance is ensured in a variety of ways. first, bit matching is confirmed between the software emulator and the results of behavioral simulation for the vhdl. second, bit and cycle matching is confirmed between behavioral and post-synthesis simulation using the clock period indicated by the critical path in the synthesis report. this ensures that the vhdl is properly written and free of synthesis unfriendly syntax. it also confirms that the critical path produced is true."
"we next applied icnv to 75 individuals with both wgs and array data, performing a similar analysis as above. figure 4 shows, for a region of chromosome 22, a heatmap of the integrated hmm z-scores output by icnv run on joint platform mode. regions reported by icnv as deletions and duplications are also shown as thick black or white lines on the heatmap. in this dataset, the joint method detects more cnvs than a simple intersection of a separate analysis of the two platforms and less than their simple union (table 2, supplementary fig. s4a ). compared to wgs, snp array has much lower resolution and thus detects significantly fewer cnvs. more than 71% of the snp calls and 96% of the wgs calls overlap with icnv joint platform calls (supplementary table s2 ). on the other hand, around 92% of the joint platform calls overlap with calls made on wgs alone. since wgs already has very good coverage across the genome, there seems to be, as expected, a much smaller power gain achievable by adding snp arrays to the joint calling procedure."
"the remainder of this paper is organized as follows. in section 2, we discuss background information and related works. in section 3, we introduce the proposed method. in section 4, we describe our simulation environment and test results. our final conclusions are discussed in section 5."
"within the guiding design principles mentioned above, still many design variations are possible. these variations may affect the different performance aspects of the processor and hence need to be considered within the design space exploration (dse) process."
"in order to evaluate the objective functions, it is necessary to know the area, dynamic power, and static power of different components as function of corresponding parameters. these depend on the used circuit designs as well as the technology used to build these circuits. for optimization purposes, a database of the characteristics of different modules using real asic technology data has been constructed. with the help of synthesis tools for 130 nm asic technology, the vhdl files for the design of the ir, fus, lin, is, and gin are used to obtain the required data. the clock periods, area, and power consumption of different components are then used to evaluate equations (1)-(3) for every candidate design."
"the results achieved by fl2w in fig. 3 indicate that the computational complexity of fl2w is higher than that of the mq and m coders. the use of long codewords helps to improve the throughput of fl2w slightly. clearly, the competitive coding efficiency achieved by fl2w comes at the expense of high computational costs."
"in a manet trust blockchain, blocks consist of block transaction data and the metadata described above (timestamp, hash of the transaction, delegate id, and the nonce). when a transaction is hashed, the transaction generator id and the tvs proposed by the transaction generator, as well as the delegate id, will be included to provide non-repudiation for the block transactions proposed by any nodes. when the network is formed, the first block in the blockchain, which is called a \"genesis block\" (blockchain jargon), is defined with an empty list of transactions. a sample configuration for a block is presented in figure 9 ."
"in the dynamic environment of a manet, trust management plays an essential role [cit] . therefore, several studies have developed various approaches, such as reputation-based frameworks, trust establishment frameworks, and policy-based trust management. in reputation-based trust management [cit], trust values are calculated by aggregating and distributing reputation among residents. in this scheme, a destination node rates the reputation values of its upstream nodes by checking the packets delivered and acknowledges the source node. based on the occurrence of transactions, each node that participated in the message transmission process maintains the trust values of other participants in their distributed routing table. however, trust values are not verified, meaning if the rating node is malicious, all nodes that participate in message transmission can be manipulated in this scheme. additionally, the rating process occurs after packet transmission, meaning sender nodes cannot be sure of the trust values of their relay nodes."
"suppl. figs. 8 and 9 show the results for rc6 encryption and decryption, respectively. running conditions are identical to those for testing the aes program. again, bit-matching is achieved with the software emulator and cycle-matching agrees with the behavioral simulation. functionality is confirmed in processing several consecutive blocks and when switching between encryption and decryption."
"the asic implementation of the optimized programmable dataflow crypto processor, as well as the redesign of different performance bottlenecks, are the subject of future extensions of this work."
"where p d . the dynamic power of the term, p i . the static (idle) power of the term, t d is the busy period of the term during program execution, and t i is the idle period of the term during program execution."
"on the other hand, crypto co-processors attempt to avoid the shortcomings of customized gpp's by detaching the special cryptographic function units from the alu of the main processor. they execute cryptographic algorithms or cryptographic protocols on a completely separate co-processor which is either tightly-coupled or loosely coupled to the main-processor. such co-processors provide hardware implementation of selected cryptographic algorithms or protocols and hence can exhibit a higher throughput than customized gpp (especially for tightly-coupled implementations). however, they lack the flexibility of customized gpp's. to partially circumvent this drawback, reconfigurable designs -such as reconfigurable fpga cores -have been proposed [cit] . further enhancements of crypto co-processors were realized by deploying several such engines within an array (called crypto array) [cit] . multi-core versions of crypto co-processors were also proposed [cit] . still, crypto co-processors cannot be easily adapted to the wide range of existing and expected future cryptographic algorithms and protocols."
"in a blockchain environment, there are two types of nodes: full nodes, which maintain the blockchain, and light nodes, which largely rely on information from full nodes and do not maintain the entire blockchain. according to the nature of manets, we also adopted this concept in our environment. whenever a new node joins the network, it will have access to the blockchain information. however, a node should initially join the network as a light node, meaning it can simply download the header of the block, as shown in figure 10 . although a new node will act as a light node immediately after joining the network, it is still able to generate transactions (attacker detection/tv calculation) in the network. the host node of the network will serve as a temporary full node to relay block headers until the new node becomes a full node."
"in this section, we present a performance evaluation of the proposed scheme based on the ns-3 simulator. because the implementation stack in ns-3 is similar to a real-world implementation, it can be assumed that the simulated scenario will also function properly in a real-world environment. furthermore, it has built-in implementations of most of the routing protocols in manets. therefore, we used this simulator to implement the proposed scheme to test its feasibility and performance."
"the overall design includes the crypto processor plus a xilinx clock manager unit used to generate the appropriate clock from the on-board 66 mhz clock. the design includes all functional units, whether or not they are utilized by the algorithms under investigation, thus inflating the resource utilization but ensuring full flexibility. to ensure the design fits on the target fpga, the size of matched input buffers (the memory component of the content addressable memory) is optimized per functional unit, ensuring enough but not excessive entries are included."
"block transactions update the trust values of all nodes in a network, including malicious nodes. all transactions are sent to the validator node or a delegated node, which will generate a block. this scheme is conducted based on the olsr protocol, where transactions are disseminated through mpr nodes. each node n will send an encrypted transaction (n, transaction)prkey n, where transactions are encrypted by the private key of n."
"multi-objective optimization problems typically have multiple solutions, where each would behave well for one or more performance measures, but none would behave well for all measures. in order to reduce the candidate solutions to a small set, the aes algorithm has been chosen to be the basis for optimization of the programmable data-flow crypto processor. to further reduce the set of candidate optimal solutions, it is proposed to differentiate between members of the optimal solutions for the aes algorithm, based on the metrics obtained when executing other security algorithms. earlier results following this approach have been reported before [cit] . however, more refined results are presented in this section by using an updated component database."
"additionally, we evaluated the computational complexity for an arbitrary node when it is detecting a malicious node (table 4 ). in this scenario, there were six attacks in the simulation."
"to bring snp array intensity (x snp;j ) and sequencing coveragederived plr (x wes;j and/or x wgs;j ) to the same scale, we standardize each to produce a normalized intensity score:"
"in silico spike-in is a useful way to assess sensitivity. spike-in differs from simulation in that signals are inserted into real data matrices, which retain the true noise structure and probe distribution, thus giving more realistic projections of detection power. in our spike-in design, we start by removing cnv regions detected by our program from original datasets, using a lenient threshold. we then add cnv signals randomly to the presumed diploid region with lengths ranging from 100 bp to 500 kb. exons, bins or snps that overlap with the added cnv regions have their intensities and bafs changed according to the simple and standard model described in the supplementary method. as a result, not all of the spike-in cnvs are detectable, especially when the data come from wes and snp arrays with low-resolution target set. icnv is then applied to the spike-in dataset, using the single-platform mode for each platform as well as the integrated multi-platform mode combining the platforms. results are compared with the underlining truth for sensitivity and precision assessment."
"for example, table 1 shows the clock periods for the fus used in the database, while table 2 shows the area of the ir for different values of n."
"suppl. fig. 1 shows the pseudo-code for the modified nsga-ii algorithm, while suppl. fig. 2 shows a flowchart for the evaluation of the objective functions for a generation of solutions."
exhaustive evaluation of every design point is prohibitive in problems with huge design spaces such as the design problem under consideration. different approaches are used for multiobjective optimization in the context of dse [cit] . one particularly successful approach is to use evolutionary approaches and in particular the genetic algorithm (ga) [cit] .
"similarly, in a manet blockchain, only the node with the highest tv can become the validator. there is a threshold value θ in the network that determines if a specific node is sufficiently trustworthy to become a validator node. unlike in bully election, a node cannot claim to be a trustful node on its own, meaning it requires a neighboring node. if node i has a neighboring node j with a tv above the threshold, it sends a claim message (i, j, tv-claim, one-hop-count)prkey i, where i is the follower node of j, j is the validator node claimed by i and trust value and one-hop neighbor count of j is put in tv-claim and one-hop-count, respectively. prkey i is the private key of i, which signs the claim message. similarly, every node with neighbors with tvs above the threshold can broadcast a claim message to the entire network by piggybacking on a tc message. if the tv of node j is the highest and there are no malicious claims on nodes i and j from other nodes, j becomes the validator. in the claim message, one-hop neighbor count is added to avoid the situation in which two or more nodes have the same trust value and cannot be concluded which node should be selected. based on the count, the node which has more one-hop neighbors could have more chance to be validator. here, node i sacrifices its energy to broadcast a claim message for node j. because a manet is a resource-hungry environment, there should also be a reward for the voting node i (possible rewards for node i will be discussed in the following subsection)."
"the optimization algorithm is applied on the processor architecture executing the aes encryption algorithm, with all the regions running at the same clock frequency. synchronous operation simplifies design as it does not require special communication mechanisms between regions operating at different clocks. suppl. figs. 3 and 4 show sample results for the simulation at three stages: generation # 1, generation # 50, and generation # 200."
"the demonstration of how to select the validator and how it could delegate to another node is shown in figure 7 . among the validator claim messages broadcasted by every node in the network, according to node e and f, d has the highest trust value. even though d is chosen as the validator node, since it is a rich node with trust value ''1\" it has to perform the delegation process consequently. among its neighbors, b, e, f and g, it compares their trust values. however, for node e and f, the ones which claimed it as the validator, it compares their value by adding the temporary value, 0.5 to them. node f with the highest trust value among d's one-hop neighbors becomes the delegate to create the block."
"the operations carried out by the renormalization procedure can be avoided if, instead of producing a single codeword, the coder produces short codewords of fixed length. to this end, the coder uses an integer interval of range, with denoting the length of the codewords. the coding of symbols is carried out by segmenting this interval as it is previously described. when the size of the last selected subinterval is 1, the number that it contains (which represents the fixed-length 1520-9210 © 2015 ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"1. decentralization: based on the flexible nature of manets, where nodes continuously enter and leave a network without administrative action, trust evaluation for resident nodes is an essential component. however, it is impractical to assess nodes within multi-hop distance using only a centralized node or set of nodes. therefore, decentralized trust evaluation is crucial for manets, meaning the nodes in a manet should perform mutual assessment that is decentralized and straightforward."
"-at generation # 1, the population has number of execution region ranging between 3 and 10. when reaching generation # 50, the range for the number of execution regions is reduced and becomes between 4 and 8. starting from generation #200 and thereafter, the range for the number of execution regions stabilizes at four values, namely: 4 (delay is minimized), 7 (area is minimized or energy is minimized), and 5 or 8 (neither area nor delay nor energy are minimized, but these candidate solutions are on the pareto front implying no other points in the design space dominate them)."
"in this paper, the optimization of a novel programmable dataflow crypto processor dedicated to security applications has been considered. its architecture is based on distributing function units needed for executing security algorithms over a number of execution regions that operate in parallel. processor optimization is formulated as a combinatorial multi-objective optimization problem with the objective functions being area, delay and consumed energy. the evaluation of objective functions relies on a database of component specifications as well as a cycle-accurate emulation of the processor. the optimization problem is solved using a modified version of the nsga-ii algorithm."
"the third quantifiable primary objective function is the cost. the cost of the design is essentially determined by the area consumption in the target technology and the packaging costs. fixed costs, such as the fabrication costs of mask sets cannot drive the optimization process and are often not included [cit] ."
"context-adaptive binary arithmetic coders employed in image codecs are commonly implemented with variable-tovariable length codes. this paper introduces two contextadaptive binary arithmetic coders that employ codewords of fixed length. they are referred to as flw and fl2w. flw employs one interval arithmetic, whereas fl2w employs two to enhance coding efficiency. the proposed coders avoid renormalization, which simplifies their implementation and reduces their computational complexity. an important point disclosed in the experimental results is that the renormalization procedure of arithmetic coders can be removed without affecting their coding efficiency. the context-adaptive mechanism integrated in these coders employs a low-complexity technique based on a variable-size sliding window."
"for creating a block, the information that will be included in the block and the manner in which it will be configured by the delegate node should be specified. in a blockchain system, transactions in the pool are collected as a block and the block is chained through the network. for providing immutability in a blockchain, a hash value (sha-256 algorithm) that is directly derived from the transaction data is appended to the block. therefore, even a small change in the data will change the hash value. the hash of the previous block will be included as data in the current block for chaining, meaning a data change in one block can disorganize all blocks in a blockchain. the block hash only accepts a specific format (e.g., a hash signature starting with 10 consecutive zeros). in compliance with this rule, there is a piece of data called a nonce. the nonce value is repeatedly changed until an eligible hash signature is acquired."
"the evaluation of the computational throughput considers the speedup achieved in the tier-1 [cit] codec. the tier-1 implements the bitplane coding engine and the entropy coder. it spends of the total execution time. the widest columns of for decoding, fl2w slows down the decoding process by approximately 10%. these results slightly differ from those obtained with artificially generated symbols due to the different probability distribution of the source."
"step 3: deduce the metrics for the candidates obtained in step 2 when running security algorithms other than aes encryption. the following four algorithms have been considered using the fus' mapping shown in table 4 : aes decryption, rc6 encryption, rc6 decryption, and sha3 hashing. using eqs. (1)-(4), along with the emulator output, values for area, delay and energy have been deduced. the resulting metrics for the six candidates when using the aforementioned security algorithms are listed in table 3 ."
"the problem of selecting the best distribution of fus among ers falls within the class of grouping problems, for which special solution encodings and genetic operators are needed [cit] . the encoding and genetic operators used in the processor optimization programs are based on a modified version of those used in refs [cit] ."
"we have proposed a method, icnv, to improve cnv detection and genotyping accuracy using high throughput sequencing data, allowing for integration of snp-array data. the distinguishing features of icnv compared with existing methods are as follows: (i) icnv adopts codex to improve the normalization of sequencing data, removing biases due to target length, mappability, gc content and other latent systematic factors; (ii) icnv utilizes baf information from sequencing data, which is valuable for cnv detection and exact copy number inference; (iii) array data, if available, are combined with sequencing data to allow more sensitive and robust cnv detection than either platform alone; (v) icnv outputs a z-score from an integrated hmm that summarizes evidence across multiple fig. 7 . raw intensity score and bafs distribution of cnv genotype inference. contour plot shows distribution density. each margin, respectively, shows density distribution of intensity score and baf in each cnv genotype category platforms, allowing for easy visualization and quality assessment; and (vi) even though we combine cross-platform data for cnv detection, we use platform-specific parameters for exact copy number estimation and thus minimize noise effect due to platform specific latent variables. how much does snp-array data add to ngs data for cnv detection? our results, based on spike-ins and pedigree-based quality evaluations, show that snp-arrays give a significant boost in accuracy to wes but relatively little gains for wgs. for snp-array data, although we applied illumina platform as example, we could incorporate any data as long as both lrr and baf are available. for cnv detection and genotyping using wgs alone, we compared icnv against other read depth-based cnv detection methods including cn.mops [cit] and cnvnator [cit] . germline cnvs detected by icnv have higher within-family sharing than the other methods being compared, suggesting a higher accuracy."
"the proposed arithmetic coder with two fixed-length codewords (fl2w) addresses this drawback by using two intervals. they are stored in the integer registers and . all symbols are coded employing the first interval while its size is greater than a predefined threshold, i.e., while . when, then the symbol's probability estimate is tested to check whether it fits well in the first interval or not. the closest probability to that can be employed in this interval is (5) with the division carried out in the integer domain. is expressed in the same range employed for, i.e., . the absolute difference between and the probability estimate (i.e., ) determines whether the symbol is coded in the first interval or not. if the difference is smaller than a predefined threshold, say, then the symbol is coded in the first interval. otherwise it is coded in the second. when the first interval is exhausted, its codeword is dispatched and replaced by that of the second interval, which is reset. this selective interval coding increases the efficiency of the coder since it avoids coding symbols in intervals in which the probability estimates do not fit well. we found that and are good choices for a large variety of sources. evidently, the procedure described before can only be performed if the size of the second interval is . if not, both intervals may not have an appropriate size to code the symbol without loss in coding efficiency. when both and are, the interval with a closest to the probability estimate is chosen. the implementation of fl2w must also take into account that if the second interval is exhausted (i.e., ), then all symbols are coded in the first until it is exhausted too. this happens rarely in practice."
"suppl. fig. 6 shows the minimum energy of solutions on the pareto front for different values of link delay and ir speed. again, asynchronous case is better than the synchronous case for zero link delay and faster ir. for 50% ir clock period the energy is 83% of that in the synchronous case, and for 30% clock period it becomes 81% of the energy in the synchronous case. as link delays increase, the asynchronous case consumes more energy as a result of increased delay and region activities. at original ir speed, for 1 and 2 clocks link delay, the energy becomes 201% and 308% -respectively -compared to the energy in the synchronous case."
"in this paper, the above architecture is enhanced and extended by investigating the effect of assigning fus among arbitrary number of synchronous ers in order to optimize the processor performance. the optimum design will be based on a trade-off between the primary objective functions of implementation area, execution delay and consumed energy. thus the problem will essentially be one of multi-objective optimization."
the gin is a high speed interconnection network that allows exchange of intermediate results between the ers. incoming data to the gin wait in input buffers and are forwarded through the proper output port to other ers.
"it is yet uncertain in such studies how to combine data from multiple platforms and unclear how such multi-platform integration can improve accuracy. there is also ample room for improvement in the detection of cnv from ngs data alone. sequencing data are subject to multiple sources of experimental noise such as gc bias and batch effects [cit] . numerous cnv detection tools have been developed for sequencing data, but they often make contradicting detections on the same dataset [cit] . on snp-array platforms, utilization of b-allele frequency improves cnv detection accuracy, but few of the cnv detection tools currently available for sequencing data make use of information from allele-specific reads."
"other design options, such as the possible duplication of some fus, and changing the number of instruction busses and data busses may also be considered but will not be addressed in this paper."
"if a malicious node is revealed by dcfm, it will receive a negative tv and be eliminated from the network. however, one node could erroneously assign a negative value to another node, even if it is not an adversary. to prevent this issue, transactions related to malicious node information must be validated by neighbors before they are sent to the delegate node. the attacker information and any inconsistencies generated by the attacker must be reported through two-hop neighbors because attackers can claim that two-hop neighbors of a victim are their neighbors in nia. specifically, a report message (v, x, reportattack)prkey v is sent, where the victim id (v), attacker id (x), and reportattack (identified inconsistencies) are encrypted by the victim's private key, as described in figure 8 . this message is sent by piggybacking in hello message until two-hop range since the malicious hello message contains the two-hop neighbors of the victim. if the neighboring nodes agree on the transaction, they will reply (i, ackreport)prkey i and the transaction will be validated. since the attacker can also include the fake nodes, to get agreement from all nodes is impossible. moreover, the node requesting the agreement could also be the attacker which is trying to isolate a good node. thus, if half of the neighbors included in attacker's hello agrees with the report, the transaction is validated. in addition, \"agree\" means that they don't have connection with that attacker even though it claims that they are its neighbors. thus, every node is evaluating with the same criteria to agree or not. only nodes with tvs above θ will send normal tv transactions that are not attack transactions. the delegate node will count the number of nodes voting for a particular transaction. based on the count, the delegate will determine the transaction sequence and generate a block. consequently, the delegate will broadcast the new block (dl, block)prkey d l through mpr nodes in the network. when all nodes receive the block, each node replies with a confirmation message (n, blockack)prkey n . if the majority of nodes agree with the new block, each node chains it to a local blockchain."
"offspring individuals are obtained by first selecting parents from the current population. parents are chosen using binary tournament selection based on both the rank and the crowding distance. thus, two random individuals are compared and the one with lower rank is selected. if ranks of the two solutions are the same, the one with higher crowding distance is selected. traditional crossover and mutation are next applied on the selected individuals to generate a child population. giving better solutions a higher probability of being selected for breeding allows keeping good solution attributes and results in faster convergence of the algorithm."
"step 4: analyze the metrics of the six candidates. the following observations can be made: -among the three candidates yielding minimum area for aes encryption, candidate 1 has smallest delay for aes encryption, smallest delay and energy for rc6 encryption, smallest energy for rc6 decryption, and smallest energy for sha3. meanwhile, candidate 2 has smallest delay and energy for aes decryption and smallest delay for sha3. on the other hand, candidate 3 has smallest energy for aes encryption, smallest delay for aes decryption, and smallest delay for sha3. consequently, one may rank candidate 1 as the best -followed by both candidates 2 and 3 -among the three candidates exhibiting minimum area for aes encryption. -among the two candidates yielding minimum delay, candidate 5 has smaller energy for aes encryption, smaller delay and energy for rc6 encryption, smaller delay and energy for rc6 decryption, and smaller delay and energy for sha3. candidate 4 has smaller area for aes encryption and smaller energy for aes decryption. consequently, one may rank candidate 5 as the best -followed by candidate 4 -among the two candidates exhibiting minimum delay for aes encryption. -candidate 6 has lowest energy among all 6 candidates for aes encryption, aes decryption and rc6 decryption."
"the olsr node maintains the topology information for the network through periodic exchanges of control messages. there are two main types of messages that describe the topology information of the network, namely the hello message (described in figure 2 ) and topology control (tc) message (described in figure 3 ). every node includes its neighborhood information (one-hop neighbor nodes) in a hello message and broadcasts it to all one-hop neighbors. subsequently, each node collects the topology information of the network until reaching two-hop range. tc messages are advertised by mpr nodes. these messages include a list of mpr selectors that have chosen the broadcasting nodes as their mpr nodes. every routing node acquires the network topology by utilizing hello and tc messages. finally, the shortest route to a destination node is calculated and data packets are forwarded efficiently."
"to evaluate the effect of the delay of the links between the processor regions through the wrappers, separate optimization runs are made for the cases of zero, one, and two clock link delay. signals for which link delay is introduced are request/acknowledge signals between ir and ers, and request/acknowledge signals between ers and ports of gin. each of these delays has been put as 0, 1 or 2 clock periods."
"copy number variations (cnvs) are large chunks of dna that have been deleted or duplicated during evolution, leading to polymorphisms in their numbers of copies in the observed population. studies have shown that cnv is an important type of variation in the human genome, some of which playing key roles in disease susceptibility [cit] . accurate identification and genotyping of cnv is important for population genetic and disease studies and can lead to improved understanding of disease mechanisms and discovery of drug targets [cit] . to profile cnv, earlier studies relied on array-based technologies such as array comparative genome hybridization or single-nucleotide polymorphism (snp) genotyping arrays, while in recent years, next generation sequencing (ngs) technologies have allowed for high resolution cnv profiling [cit] . with the drop in sequencing cost, many large-scale genetic studies have adopted whole exome sequencing (wes) and/or whole genome original paper sequencing (wgs) to profile genetic variation in large cohorts. often, these cohorts were previously studied using array-based technologies. for example, alzheimer's disease sequencing project (adsp) is an ongoing study that has 578 samples with both wgs and snp-array data and 10 913 individuals with both wes and snp-array data; alzheimer's disease genetics consortium (adgc) is another ongoing study that has 3084 samples with both wes and snp-array data."
"because the objective of this work was to develop a distributed trust system for improving network reliability and scalability, we focus on how to construct a trusted network, rather than how trust should be calculated. furthermore, to reduce vulnerability to repeated attacks by the same attacker, whenever a node detects an adversary in its vicinity, the attacker information is distributed through the network. a variety of detection schemes and trust models will work in the proposed scheme, but the dcfm [cit] was adopted as a representative scheme that is applicable to the solution presented in this paper. all nodes in the network follow the dcfm rules to detect malicious neighbors. as discussed above, the dcfm algorithm detects adversaries by analyzing topology information obtained from neighboring nodes. if there are any contradictions in the received information, the sender node is identified as a malicious node and receives a large penalty from the checking nodes. as soon as a particular node is identified as an adversary, that node is effectively removed from the network by sharing its information across the network via blockchain technology."
"as shown in table 5, the highest resource utilization is in slices used as logic, which is a result of optimization of mib sizes. slices used as registers are almost exclusively used to provide storage in the mibs, as well as a minor component going to pipelining and state registers. ram/fifo unit utilization is very small and is used entirely as bram to store the program, data, and sbox tables. dsp48e1 slices provide the ability to use high speed multipliers while at the same time freeing slice logic to be used for other operations. the design uses only three dsp units in the multiplicative fus. table 3 results for the six candidate solutions on the pareto front for the synchronous processor design. 5 4 5 2 2 2 2 2 3 0 1 3 5 5 1 6 6 2 3 4 5 1 4 6 6 0 0 candidate 3 3 5 3 0 0 2 2 2 6 4 4 0 3 3 4 5 5 2 0 6 2 3 6 5 5 1 1 candidate 4 0 3 2 1 1 2 2 2 2 0 2 3 2 2 0 3 3 1 2 0 3 1 2 3 3 1 0 candidate 5 0 3 2 1 1 2 2 2 2 0 2 3 2 2 0 3 3 1 0 0 2 2 2 3 3 1 0 candidate 6 1 4 2 2 2 4 4 4 5 1 6 2 2 2 5 3 3 3 6 6 6 2 4 3 3 3 0 table 5 resource utilization results obtained for xc6vlx240t package ff1156 speed grade -1using xilinx ise design suite 14.5."
"a blockchain-based decentralized trust management scheme was also introduced for vehicular networks [cit] . based on the high mobility and dynamic nature of vehicles, vehicular networks require a trustful environment, similar to manets. in this paper, the authors have introduced blockchain technology combined with pow and proof-of-stake (pos) to regulate the benefits of both mechanisms. vehicles performed a rating process for other vehicles, while roadside units (rsus) verified and maintained the tvs in the blockchain network. because blockchaining was conducted by rsus, this scheme could be considered as a decentralized system for vehicular networks. however, for completely distributed manets without centralization, the application of a more decentralized and lightweight blockchain for trust management is still an important research topic."
"that is, solution a is better for at least one objective, while being at least the same for all other objectives. a solution which is not dominated by any other solution is said to be a pareto optimal solution. a pareto solution, which represents one possible compromise solution, is a point where one cannot improve one objective without degrading at least one other objective. one seeks to obtain the set of pareto solutions (or the pareto front) for the problem of crypto processor design."
"to evaluate the accuracy of icnv, and also to serve as illustration, we analyze two sets of samples from the adsp. for detailed data processing procedure, see supplementary methods. the first set of samples comprises of 38 unrelated individuals with snp array and wes data. the second set of samples comprises of 75 related individuals with snp array and wgs data. the family structure in the second dataset allows the assessment of detection accuracy by mendelian concordance and the benchmarking of detection methods."
"as discussed in principle 1, for realizing fully protective and collaborative detection, at least three nodes must work together so that even if node n 1 is being manipulated by an attacker and n 2 is taking a break from detection duty, node n 3, which is connected to both nodes, can still detect the attacker. however, for complete security, new nodes should not be included in the collaboration process. a node should only perform detection in this situation if it has been contributing to the network for a specific amount of time and earned a reasonably high tv."
the design of embedded processors -such as the considered security processor -needs to comprehensively address different performance aspects [cit] . some of these performance measures are major quantifiable properties of the system that are typically used as optimization goals. these are termed the primary objectives of the design. other performance aspects are domainspecific features that are not easily quantifiable but cause the designer to prefer one of otherwise equal designs. these are termed the secondary objectives of the design.
"the nodes with the highest tvs are the candidates to become validators in the network. to determine which node will be the block creator, we adopt the bully election [cit] strategy, which is one of the common election algorithms used in distributed environments. it is called bully election because the nodes with the highest identification numbers make other nodes accept them as coordinators. a node that intends to become the leader contacts any other nodes with higher priorities. if there is a reply from any of those nodes, it gives up on becoming the coordinator. otherwise, it becomes the coordinator in the network. when the highest priority node directly claims the coordinator role, the communication overhead is the lowest, meaning this is the best scenario. therefore, this algorithm is appropriate for a manet environment."
"each run of the optimization algorithm starts with a random population and uses a population size of 100 individuals and runs for 200 generations. it is also to be noted that efficient hardware design dictates the placement of some related fus within the same region. these constraints are added to the optimization process. in particular, fu for logic functions (2, 12, and 13) should be in the same region. similarly, memory-related fus (15, 16, 23, and 24), combine fus (3 and 4), and extract fus (5, 6, and 7) are placed in the same regions. these constraints have been enforced by the program in all individuals of the generated populations."
"in general, the cardinality of the obtained pareto front gave a wide range of design choices. for example, in the case of synchronous design the final population of 100 solutions contained 75 distinct pareto non-dominated solutions. however, the following method is used to identify the major candidate solutions considering each of the objective functions."
"to test icnv, we first compare cnv detection accuracy using icnv on two platforms versus simply performing intersection or union of the two platforms. results suggest that icnv achieves higher sensitivity and robustness. we further assess the impact of adding array data to sequencing data in cnv detection by an in silico spike-in study and find that sensitivity increases when adding snp array to wes, but there is negligible improvement when adding snp array to wgs. we also consider the case where wgs is the sole platform used and compared icnv to other commonly used cnv calling methods on a wgs dataset with pedigree information, where we find higher mendelian concordance, indicative of higher accuracy, for icnv detections. figure 1 shows an overview of icnv analysis pipeline. input data depend on experiment design: when both snp array and ngs data are available, the input includes (i) snp log r ratio (lrr) and (ii) baf, which quantify, respectively, relative probe intensity and allele proportion, and (iii) sequencing mapped reads (bam file) [cit] . this pipeline simplifies when data from only one platform is available ( supplementary fig. s1 ). for sequencing data, icnv also receives target positions (bed file) for read depth background normalization. in wes, the targets are exons, while for wgs, icnv automatically bins the genome and treats each bin as a target (the default bin size is 1 kb). icnv first performs cross-sample bias correction for sequencing data using codex and computes a poisson log-likelihood ratio (plr) for each target [cit] . heterozygous snps are detected and bafs are computed within target regions using samtools [cit] ). integrated cnv detection is then conducted through a hidden markov model (hmm) that treats the array intensity, array baf, sequencing plr and sequencing baf as observed emissions from a hidden copy number state. the hmm segments the genome of each sample into regions of homogeneous copy number and outputs an integrated z-score for each position that summarizes the evidence for an abnormal copy number at that position. finally, integer-valued copy numbers are estimated in regions of high absolute z-score, utilizing information from all platforms."
"hmm parameters are fit by the baum-welch (em) algorithm, which maximizes the likelihood of observed data. the viterbi algorithm is applied to infer the most likely path given the best-fit parameters. details are in supplementary method."
"we conduct another in silico spike-in study to study the power gains from adding array data to wgs. the result shows that wgs has comparable power to the integrated method, performing only slightly worse (fig. 5b) . however, for the detection of small cnvs in snp-dense regions, snp-array does add valuable information. precision, reflecting specificity, of joint detection is similar to the intersection method, and sensitivity is similar to the union method (supplementary fig. s4c) . thus, icnv achieves the best of both worlds: the high precision of a stringent intersection procedure and the high sensitivity of a relaxed union procedure."
"a rithmetic coding is among the most popular entropy coding techniques employed nowadays. the codeword generated by the arithmetic coder is a number within an interval arithmetic that represents the coded symbols. briefly described, the coder begins by segmenting the interval of real numbers in as many subintervals as there are symbols in the alphabet. the size of the subintervals is commonly selected according to the probabilities of the symbols, more precisely, as with representing the alphabet of symbols and being the cumulative mass function of . the first symbol of the message is coded by selecting its corresponding subinterval. then, this procedure is repeated within the selected subintervals for the following symbols. the transmission of any number within the range of the final subinterval (i.e., the codeword), guarantees that the reverse procedure decodes the original message losslessly."
"for the purpose of fpga implementation and testing -presented in the next section -it is decided to give priority to delay, and hence candidate 5 has beens selected."
step 1: identify the set of points yielding minimum area or minimum delay or minimum energy when applying the nsga-ii and using the instruction sets for aes.
"when, the interval is reduced according to (2) in this case, four more additions (or three in the algorithm below) than those necessary in (1) are required. the execution of (2) can be minimized by performing a conditional exchange between symbols 0 and 1 when so that the most probable symbol is always coded as 0. the interval is exhausted when . then, (which represents the codeword) is dispatched and the registers are reset to and . in general, is not known during coding, so it is estimated considering the distribution of the last symbols coded. the estimation of is commonly carried out by context-adaptive mechanisms and probability models. our coder uses a variablesize sliding window [cit] that utilizes between and symbols except at the beginning of coding. this technique has been devised to minimize computational costs without affecting coding efficiency. more precisely, it has low memory requirements since it does not hold the symbols coded, reduces the times that some variables are updated, and computes the probability estimate only once every symbols coded. fig. 1 illustrates the variable-size sliding window employed. the thick horizontal line represents the symbols coded. the probability estimate, denoted by, is updated every symbols according to (3) with being the number of symbols within the variable-size window and being the number of zeroes coded within the window, this is, during the last symbols. denotes a bit shift to the left. the operation is employed to make sure that even when . the division in (3) is carried out in the integer domain. as depicted in fig. 1, when the window is reduced to symbols and the number of zeroes within the window is updated according to (4) with being the number of zeroes coded during the first symbols of the window."
"we utilized pedigree data that are available for the 75 individuals with wgs to assess the improvement of icnv over existing methods for cnv detection on wgs alone. true germline cnvs are more likely to be shared between related individuals than between unrelated individuals, whereas false positives are not expected to have enriched sharing between related. based on this fact, we can compute the cnv sharing frequency f at each position l between related individuals, defined as"
"for a value of i 1, collaborating nodes should determine the task assignment order in the same manner as a peer-to-peer network. in this context, the network may be vulnerable to malicious attacks based on misunderstandings between nodes. task assignment should be performed based on a global value that is unique for every node. therefore, we set the node address to a value suitable for determining the task assignment order. the node with the smallest address number is responsible for the first interval and the node with the greatest address number is responsible for the final interval."
"is an important factor that must be considered. each node should have trust values (tvs) for its relay nodes, even when they exist at multi-hop distance, so it can select the optimal nodes for forwarding data. additionally, the data accessed by all nodes should be consistent for each node. 3. tamper-proofing: based on some of the aforementioned design goals, such as decentralization and availability, all available information will be accessed by all nodes in the network. even with reliable support from the proposed scheme, critical information should not be tampered with by malicious nodes in the network. in other words, the system should be resilient to a small set of collaborative adversaries. 4. lightweight performance: based on strong security goals, it is important that any trust solution provide reasonable performance, even with the complex computations required for authentication. however, for manets with limited resources, it is desirable to use a straightforward and effective solution, rather than a strongly secure, but complicated mechanism. 5. efficiency of security model in manets: to the best of our knowledge, most trust solutions require repetitive processes to be performed based on the dynamic nature of manets. moreover, a network is vulnerable to repeated attacks from the same attacker when there is no collaboration between individual nodes. therefore, a trust solution with greater efficiency must be developed for manets."
"although various attacks can occur in any security layer of a manet, in this study, we decided to focus on evaluating the trust of a node based on routing issues, which occur in the network layer and represent a critical research issue in dynamic manet environments. olsr [cit] was selected as a representative routing protocol for implementing blockchain operations since it is one of the proactive protocol which maintains the neighbor information. the proposed system is a trust management system using a policy-based and reputation-based trust management framework. therefore, a policy-based mechanism that can generate binary results according to whether or not a node is trusted is defined such that trust evaluation (reputation-based evaluation) can be performed based on binary results. we implemented denial contradictions with a fictitious node mechanism (dcfm) [cit] as a representative detection mechanism for identifying malicious attackers and trusted nodes as this is one of the effective schemes that was introduced recently."
"the new interval for a node is based on the number of nodes mentioned in the hello messages of its neighbors. according to the first principle, the multiplicative factor f will start from a minimum value of two, which is the minimal count of required collaborators in addition to the target node itself. after multiplying the value of f by the initial interval i 0, the resulting value i 1 will be utilized as the new detection interval. therefore, it can be concluded that the greater the minimum number of neighbors, the longer the detection interval for nodes."
"this work introduces two arithmetic coders employing fixed-length codewords that are devised for image/video coding systems. the first coder aims at low computational complexity. it utilizes one integer interval that is reset when exhausted. the second coder aims at high coding efficiency. it utilizes two integer intervals in which the symbols are selectively coded depending on their probabilities. a novel variable-size sliding window mechanism that estimates the probabilities of the symbols is included in both coders. the main difference between the proposed method and previous fixed-length arithmetic coders [cit] (not including the mq or the m coder) is the use of a binary alphabet, adaptive mechanisms for probability estimation, low-complexity instructions for the interval division, and a selective interval coding technique to enhance efficiency. [cit] and by the m coder of hevc-both in terms of coding efficiency and computational throughput. [cit] codec to illustrate their performance and ease of integration."
"an alternative approach used in most recent works involving dse is to use the concepts of dominance and pareto solutions, which originate from the area of economics and game theory. in this approach, a set of solutions representing the possible range of trade-offs is obtained. in the following, we give a definition of pareto solutions [cit] ."
"suppl. fig. 5 shows the minimum delay obtained on the pareto front in each of the considered cases, compared with that obtained with synchronous operation. optimization results show that if asynchronous design uses zero link delay (as in the synchronous case), minimum delay for both asynchronous design with slow ir and synchronous design is obtained by arranging the units into 4 ers. the minimum delay for asynchronous design is slightly lower than the synchronous case. by speeding up the ir however, asynchronous design benefits from regions that can run at higher clocks so that when ir clock period is reduced to 30% of its original value, delay becomes 56% of the execution delay in the synchronous case. however, as link delay increases, its overhead causes the delay of the asynchronous design to increase by a large factor. thus, with two clock periods link delay and original ir speed, execution delay increases to 311% of the delay in the synchronous case. speeding up the ir improves the asynchronous processor delay, but it remains higher than the synchronous processor."
"furthermore, with the goal of making the proposed system more efficient, we introduce a cooperative mechanism for our security solution. although cooperative networking has been considered previously for manets [cit], in most of the security mechanisms available for proactive routing protocols, nodes perform individual detection processes. in dcfm, detection occurs in every hello interval. however, in our solution, the detection interval is decreased according to the number of neighbors in the node vicinity. by taking advantage of the synergistic effects of nearby neighboring nodes, the detection interval can be increased. nodes that fulfill the following principles can perform collaborative detection instead of working individually. (4) and (5) for the calculation of a new detection interval)."
"the rest of the paper is organized as follows. section 2 presents a brief overview of recent work regarding the three main categories of security processors. section 3 provides an overview of the design of the data-flow crypto processor, while section 4 presents the design objective functions and performance metrics. then section 5 presents the proposed optimization algorithm, and section 6 gives the optimization results and analyzes their significance. in section 7, the implementation of the optimized processor on fpga is presented, and finally section 8 concludes the paper."
"historically, security processors were designed as customized gpps. these are based on using standard processors (whether cisc or risc) and adding special functional units to its arithmetic and logic unit (alu) that cater for cryptographic operations, such as ''bit shifting\", ''bit rotation\", ''modular addition\", ''modular multiplication\", ''substitution\", and the like. since new instructions are introduced to deploy these additional functions, customized gpp's are also called instruction set extension (ise) processors. while they offer the highest flexibility -because of their reliance on easily modifiable software instructions -they require modifications in the existing processor hardware which comes at the expense of increased chip area, increased cost, and reduced throughput. to further enhance the flexibility of customized gpp, reconfigurable designs have been proposed [cit] and arrays of gpps have been deployed [cit] ."
"where dip, dup and del are short for diploid, duplication and deletion, respectively. the score is positive if the hidden state has higher conditional probability of being a duplication than a deletion, and negative otherwise. this quality score integrates information from intensity/coverage and bafs across multiple platforms, allowing for easy visualization and straightforward quality assessment."
"the latency of operations will be affected by all the design choices mentioned in the previous section. for example, placing a frequently used fu in certain region may force this unit to operate at a clock frequency less than the maximum possible value. this in turn delays the execution of instructions and causes the block encryption process to take a longer time."
"secondary objectives that are typically used to compare designs include utilization of computation and communication resources, i/o and memory specific metrics, and testability of design. all these secondary objectives can be used to evaluate different designs of the security processor, but a major objective will be the immunity to side channel attacks which target the hardware implementation of a cryptosystem."
"the original work shows how legal proof standards (which play an essential role in legal reasoning) can be modeled using adfs. 28 adfs have also been used to reconstruct the much-cited carneades system. 8, 35 the reconstruction not only puts carneades on a safe formal ground, but it also allows the somewhat unrealistic restriction of the original system to acyclic argumentation scenarios to be relaxed. this shows the potential of adfs as systems for generalizing not just abstract frameworks, but also more logic-based approaches."
"we presented a complete framework for continuous prediction of human emotions based on features characterizing head movements, face appearance and voice in a dynamic manner by using log-magnitude fourier spectra. we introduced a new correlation-based measure for feature selection and evaluated its efficiency and robustness in the case of possibly time-delayed labels. we proposed a fast regression framework based on a supervised clustering followed by a nadaraya-watson kernel regression that appears to outperform, for the aimed task, support vector regression. our fusion method is based on simple local linear regressions and significantly improves our results. because of the high power of generalization of our method, we directly learned our fusion parameters using our regressors outputs on the training set. in order to improve the fusion for methods that are more prone to over-fitting, we would have to learn these parameters in cross-validation. our system has been designed for the audio/visual emotion challenge (avec'12) which uses pearson's correlation as evaluation measure. therefore, every step of our method has been built and optimized to maximize this measure. an accurate system for everyday interactions would need to be efficient in terms of correlation but also in terms of root-mean-square error (rmse). some modifications on our system would be needed to increase its performance regarding this measure. the se-maine database on which our system has been learned and tested contains videos of natural interactions but recorded in a very constraint environment. a perspective for adapting these kinds of human emotion prediction systems to real conditions, as for assistance robotics, would be to learn the system on \"in the wild\" data"
"a generative adversarial network (gan) consists of a generator network, g, whose goal is to learn a distribution matching a true data distribution, and a discriminator network d, which tries to distinguish real training data from synthetic data. g and d compete in a two-player minimax game with the following formulation:"
"within a set of features, the range of values can be highly different from one feature to another. in order to give the same prior to each feature, we need to normalize them. a global standardization on the whole database would be a solution but we chose to standardize each feature by subject in order to reduce the inter-subject variability. this method should be efficient under the hypothesis that the amount of data for each subject is sufficiently representative of the whole emotion space."
"to create designs in a manner that considers the timevarying shape of the robot during deployment, we simultaneously compute both the design of the robot and timesequences of control inputs that will feasibly guide the robot to the user-specified goal sites. the latter requires solving a motion planning problem from the robot's start location to the goal sites while avoiding anatomical obstacles. motion planning for complex robots such as concentric tube robots is pspace-hard and computationally intractable to completely solve [cit], so like most practical motion planners, we use a sampling-based approach. we build on a previously developed motion planner for concentric tube robots [cit], which searches for feasible trajectories in the robot's configuration space, and augment it to also search for designs in the robot's design space. by properly blending motion planning and design, we retain one of the key features of sampling based motion planners: probabilistic completeness [cit] . extending probabilistic completeness to design of concentric tube robots, we guarantee that if a taskfeasible design exists, as computation time is allowed to increase, the probability of our algorithm finding a taskfeasible design for performing a given task approaches 100%."
"for the sets of visual cues, we first extract temporal signals describing the evolution of facial shape and appearance movements before calculating multiscale dynamic features on these signals. the feature extraction process is described in fig. 2 ."
"in this subsection, we evaluate the effect of the standardization of the features that we performed by subject in order to reduce the inter-subject variability. we compare the results we obtained (presented in the previous table) to those achieved with a global standardization on the whole training set ( table 2) ."
"the second set of features we used is based on global face appearance. first, we warp the faces into a mean model using the point locations obtained with the face tracker. this way, the global appearance will be less sensitive to shape variations and head movements, already encoded in the first set. then, we select the most important modes of appearance variations using pca. we obtain a set of temporal signals by projecting the warped images on the principal modes."
"under design d dom and using a sequential insertion strategy ending at q seq, the robot failed to navigate the passage as shown in fig. 3 (top row) . we also ran a motion planner [cit] to see if there existed some other motion plan which, under design d dom, could still navigate the passage. the planner failed to find such a plan, even when allowed 10 hours of computation time. a design deemed task-feasible under the telescoping dominant stiffness assumption is unlikely to fully avoid the obstacle wall for real-world concentric tube robots."
we develped a task-oriented design method for concentric tube robots which combines a search of the robot's design space and sampling-based motion planning of the robot's configuration space in order to find a task-feasible design for performing a given task without colliding with anatomical obstacles. our method relaxes assumptions made in prior work in order to consider a broader class of concentric tube robots and generalizes probabilistic completeness to design space.
"the third set is based on local face appearance. first, we extract local patches of possibly interesting areas regarding deformations related to facial expressions. we extract an area around the mouth in order to capture smiles, areas around the eyes to capture the gaze direction, around the eyebrows to capture their movements, and areas where the most common expressionrelated lines can appear (periorbital lines, glabellar lines, nasolabial folds and smile lines). we chose to avoid the worry lines area because of the high probability it has to be occulted by hairs. then, we use pca as for the global warped images to compute temporal signals corresponding to the evolution of the local appearance of the face during time."
"the last set of features we used is the audio feature set given to the participants of the avec'12 challenge. it contains the most commonly used audio features for the aimed task of predicting emotions from speech (energy, spectral and voice-related features)."
"using each of the four feature sets, we make separate predictions for the four dimensions, leading to a total of 16 signals. the method used for each prediction is described below."
"including a new type of relation requires a careful adaptation of the existing semantics. the combination of attack and support leads to indirect attacks, originally referred to as complex attacks. 10 for instance, there is a supported attack from a 1 to b if there's a sequence of support links from a 1 to a n and an attack from a n to b. there's a secondary attack from b to a n if there's a sequence of support links from a 1 to a n and an attack from b to a 1 . a mediated attack from b to a 1 takes place if there's a sequence of support links a 1 to a n and an attack from b to a n . for the generalization of dung-style semantics to bafs, these indirect, complex notions of attack then need to be taken into account adequately. there are various ways of doing this, and what's adequate depends on the specific interpretation of the support."
"they formulate an objective function that codifies a design's fitness for performing a particular task, and then optimize this function using a global pattern search. these approaches make the significant assumption of telescoping dominant stiffness, which limits their applicability to designs that satisfy this assumption. with this assumption, a design can be quickly evaluated by considering only its final configuration. the assumption ensures the shape of the device up to the tip does not change when the tip is advanced using a sequential tube insertion strategy: the tubes are first rotated to their final orientations and then deployed such that inner tubes do not protrude from outer tubes until the outer tubes have reached their final insertion lengths [cit] . to consider a broader class of concentric tube robots for which this assumption does not hold, motion planning will be required to consider the device's time-varying shape changes during task performance."
"however, our view is that the general idea underlying the abstract, instantiation-based approach to argumentation is still valid. the modularity of this method provides the flexibility needed in response to changes in the modeling languages. we believe that the mentioned results suggest that dung's afs might not be the best target systems for the instantiation. indeed, their expressive abilities are limited due to the fact that we have only a binary conflict at hand. this can make modeling, for example, collective 9 or supportive 10 relations unpleasant if not problematic."
"this way, we consider the correlation between the feature and the label, but also between the feature and different delayed versions of the label weighted by an estimation of the delay probability. as before, with different separate video sequences, we need to calculate the mean of this measure for the different sequences to obtain a correlation score between the i th feature and the label. to simplify notations, we refer to this score as ρ(fi(t), y(t)). this measure is more robust than a simple correlation measure in the case of possibly time-delayed label (see section 5.3). by selecting features maximizing ρ(fi(t), y(t)), we select a relevant set of features."
"where p data (x) is the true data distribution, p z (z) is a distribution to draw samples. the generator network, g, transforms a noise variable z into g(z), which is a sample from distribution p z, and ideally distribution p z can converge to distribution p data under mild conditions (e.g. g and d have enough capacity) [cit] . the meaning of (1) is that the generator, g, tries to fool out the discriminator, d, while the discriminator wants to maximize the differentiation power between the true and generated samples."
"we experimentally demonstrated the effectiveness of our concentric tube robot design method by considering two tasks: (1) navigating through a twisted half-torus, and (2) reaching two specified points of interest in the bronchial anatomy, where in both cases we wish to avoid collisions between the robot and the surrounding environment."
"in general, the more expressive the modeling languages become, the more involved the instantiation step grows. consequently, the generated afs might turn out to be far from natural. for instance, recent formal systems such as aspic+ 7 and carneades 8 provide various useful syntactical features, in particular, a separation between strict and defeasible rules, different types of premises and proof standards, preferential information, and the like. this makes the modeling language expressive yet results in a rather complicated instantiation, and it's not always easy to see whether the instantiation performs correctly."
"in human-computer intelligent interaction systems, a current challenge is to give the computer the ability to interact naturally with the user with some kind of emotional intelligence. interactive systems should be able to perceive pain, stress or inattention and to adapt and respond to these affective states, or, in other words, to interact with humans vocally and visually in a natural way. an essential step towards this goal is the acquisition, interpretation and integration of human affective state within the human-machine communication system. to recognize affective states, humancentered interfaces should interpret various social cues from both audio and video modalities, mainly linguistic messages, prosody, body language, eye contact and facial expressions."
"we present in this paragraph a feature selection method adapted to a possibly time-delayed label. the kernel regression proposed in this paper uses a similarity measure based on distances between samples. using all the features (including the ones that are not useful for our prediction) would corrupt the regression by adding an important noise. we need to identify the most relevant ones and then reduce the number of features that will be used in our distance calculation. in order to only select the features that are correlated to the label knowing the delay probability distribution (eq. 2), we introduce a time-persistent-correlation-based measure, defined as follows:"
"various alternative definitions of af semantics have been defined. 5 most prominent are the labelingbased approaches, 14 which, instead of generating sets of accepted arguments, provide us with three-valued (true, false, undecided) interpretations. however, for our purposes, the notions we've introduced are sufficient. moreover, we do not aim here to discuss computational properties such as complexity or expressiveness; in-depth analysis of these topics can be found in relevant sources. [cit] an additional overview of systems for abstract argumentation is available in further works. 18"
"in this simple environment we can analytically derive a design d dom which would allow the robot to navigate the passage under the assumption of telescoping dominant stiffness: both tubes would have radii of curvature κ i equal to that of the torus from which the environment was generated and the curved section lengths would each have to be at least as long as half the environment's center-line length. a collision-free configuration q seq which would reach the goal can also be analytically defined: have the outer tube's plane of curvature coincide with that of the first portion of the torus, and the inner tube's plane of curvature coincide with that of the rotated portion. the insertion length of the outer tube would be the length of the first part of the environment and the insertion length of the inner tube would be the length of the entire passage."
"arguments against it (further away, bigger workload, and thus less time for your family). in the end, you'll weigh the pro and con arguments against each other and reach your decision."
"in fig. 3, we represent the four different delay probability distributions that have been learned on the training database for the first feature set. by looking at those distributions' maxima, we identify an averaged delay between 3 and 4 seconds for valence and arousal, and between 5 and 6 seconds for expectancy and power. the differences between those delays could be explained by the higher complexity of human evaluation for expectancy and power."
"our regression method, which consists of a clustering and a kernel regression, is particularly fast to learn and compute and is therefore suited for real-time implementation. we compare our method to the commonly used support vector regression combined with the kernel defined in eq. 5. as for our method, the hyperparameters are optimized in subject-independent cross-validation. the obtained results are presented on table 4 . the mean score after fusion has increased by 11% by using our method. however, we can see that the fusion is less efficient with svr than with our regression method. it can be explained by the fact that the fusion coefficients have been estimated using the svr predictions on the training set. a likely explanation could be that svr are prone to over-fitting. a solution to this issue would be to learn the fusion coefficients in cross-validation. it is thus not relevant to compare the results after fusion. it is more reliable to compare our regression method to the svr feature set by feature set. we obtain, in this case, an averaged gain of 5%."
"as described in alg. 1, our method begins by adding a random initial design d 0 to its set of design samplesd. at each method iteration, we perform one of the following two functions, where the probability of invoking the former is user-specified weight p design :"
"in future work, we will extend the scope of our design method to other types of robots. our method is easily generalizable because its formulation makes almost no assumptions about the robot's kinematics. specifically concerning concentric tube robots, we plan to consider cases in which the robot is allowed to make contact with some tissues subject to constraints on forces. we also will evaluate our computed designs on concentric tube robots operating in tissue phantoms."
the use of z is to learn to match the distribution learned from real data. it is generally represented by a random vector and used as the input of gans [cit] . we follow this step in gan1.
"the notion of evidential support was first developed in evidential argumentation systems (eass). 26 this approach builds on a generalization of dung afs, in which sets of arguments rather than single arguments might be needed to attack another argument. 9 in addition, the frameworks introduce a distinction between prima facie and ordinary arguments. the former can be accepted without further requirements. the support of the latter must be rooted in the prima facie arguments to be considered valid. otherwise, it can't be accepted, and it isn't even considered a valid attacker. thus the resulting semantics are stronger versions of the dung semantics that impose a type of grounding on the arguments. a survey on various types of support in argumentation is available in further work. 27 research on bipolar argumentation frameworks has certainly demonstrated that going beyond the attack relation is interesting and useful. on the other hand, it's apparent from the literature that there's no such thing as a single interpretation of support. we can easily think of further interpretations, not among the three we briefly discussed. for instance, an argument may strengthen another one without guaranteeing its acceptance as required for deductive support. we can also think of situations in which different notions need to be combined in flexible ways. finally, in all bipolar frameworks so far, there's the hidden assumption that conflict is stronger than support. this means that no matter the support that an argument receives, it still must be defended from incoming attacks. it thus appears useful to have frameworks that-rather than being built on a fixed interpretation of supportmake it possible to specify exactly, for each argument, in what way support and attack interact. this is precisely the functionality provided by abstract dialectical frameworks."
"to train classifiers, we consider two cases: (1) case1: training data and test data are mismatched; (2) case2: training data and test data are matched. for case1, the training data is cifar10 [cit], a 10-class benchmark image dataset, widely used in image classification evaluation [cit] . the cifar10 image styles, such as resolution and center cropping, are different from cub-200's. case1 aims to evaluate the robustness of our synthesized images. for case2, the bird images in cifar10 are replaced with the images from cub-200, to evaluate the quality of the synthesized images generated using our model in comparison with images from cub-200."
"our method requires as input a specification of the environment geometry (i.e. anatomical obstacles) and the location of the goal region. the environment geometry is implicitly defined by a user-defined function collision free(q 1, q 2, d), which returns false if under design d the path linearly interpolated from configuration q 1 to configuration q 2 collides with an obstacle. we define the goal regions with a function is in goal(q, d) which returns true if the end effector of a robot of design d at configuration q lies within the goal region. the user must also specify the number of tubes and the allowable ranges for each of the design parameters defined in sec. iii."
"in explore prior design, we gain more information about the free configuration spaces of the set of designsd we have already considered. first, we uniformly sample a design d from the set of previously considered designsd. next, we expand the motion planning graph of d. this would be identical to rrt expansion except that there may be edges in g d which are \"unchecked\" due to the lazy heuristic. so, we begin by adding a new configuration q new (generated by extend q ) to g d without collision-checking the new edge. then, we ensure that there exists a collision-free path from q 0 to q new by collision-checking any unchecked edges between q 0 and q new in g d . if the path is found to be collision-free, q new remains in the tree. if not, we delete the entire subtree of configurations rooted at the first in-collision configuration found when checking the path from q 0 . in this way we combine configuration space exploration with validation of the information we imported from neighboring designs' motion planning graphs."
"this is a harmonic series whose limit approaches ∞. hence, the configuration space of a design created at iteration i of our method is fully explored, proving probabilistic completeness of our design method for concentric tube robots."
"phan minh dung introduced what are currently the most prominent abstract systems for argumentation. 4 they are equipped with various types of semantics used for their evaluation. in a nutshell, dung's argumentation frameworks (afs) are directed graphs, with the vertices being the abstract arguments and the directed edges corresponding to attacks between them. conflicts are resolved using appropriate semantics. the different semantics reflect the different intuitions about what can be considered reasonable, thus providing a suite of calculi of opposition. they produce acceptable subsets of the arguments, called extensions, corresponding to various positions that someone might take in light of the available arguments. crucial here are the concepts of conflict-freeness and admissibility. based on them more advanced semantics have been defined, ranging from dung's original stable, preferred, and grounded semantics to the more recent semi-stable, ideal and cf2-semantics. 5 recent studies have shown, however, that within the argumentation process the construction of proper afs can cause much more concern than expected. 2, 6 attention must be given to avoid the risk of violating some natural rationality postulates in the overall instantiation-based argumentation process. indeed, generating the right argumentation structures is the crucial step here for yielding reasonable-and consistent-output."
"recently, a study of three types of support-deductive, necessary, and evidential-has been conducted. 10 the former two types are quite strong notions and, as shown in the analysis, dual to each other. if a deductively supports b, then b must be accepted whenever a is accepted. necessary support between a and b means that accepting a is a necessary (but not necessarily sufficient) precondition for accepting b."
"designing these robots to perform specific tasks is made challenging by their complex kinematics; unlike most manipulators, concentric tube robots cannot be separated into kinematically independent links because rotating or extending any tube affects the shape of the entire robot. simplified kinematic models which assume telescoping dominant stiffness (meaning that each tube is assumed to be infinitely stiff compared to all within it, and all tubes are assumed to be infinitely torsionally stiff) have been used to mitigate modeling complexity for motion planning and design [cit] . while these models' computational simplicity allow for a good starting point, more advanced models are required to accurately describe these robots' kinematics [cit], as fig. 2 . two overlaid time frames of a concentric tube robot. as the inner tube is extended past the outer tube, the inner tube interacts with the outer tube's curvature, causing the entire robot's shape to change. this illustrates that the tubes cannot necessarily be treated as kinematically independent during the design and control of these robots."
"in this section, we present some experiments we carried out to evaluate the different key points of our method. in order to be robust in generalization, we chose to optimize the hyperparameters in subject-independent cross-validation (each training partition does not contain the tested subject)."
"currently, two research directions aim to address such problems and bridge the gap between the modeling languages and afs. the first, meta-argumentation, 11, 12 lets us stay within dung's well-established setting. however, it comes at the cost of auxiliary arguments, which are required to represent relations other than attack. the second research direction focuses on extending afs by equipping them with more expressive concepts to model the aforementioned situations, such as preferences or support relations. compared to the meta-argumentation approach, the main challenge of the new frameworks is to correspondingly generalize the semantical concepts. they must not only fit the extended frameworks, but also remain intuitive and relatively compatible with the original structures. 13 the primary objective of this article is to give an overview of the currently available generalizations of dung's framework. in the interest of space, we'll keep our discussion on a rather informal level but try to raise awareness of the difficulties that arise in this vivid and interesting branch of research in the argumentation community."
"we present in this paragraph a clustering step with supervised weighted-distance learning. the feature selection step presented in the previous paragraph gives a correlation score between the label and each selected feature using eq. 3. we use these scores as the weights of a diagonally-weighted distance dw, defined as follows:"
"we demonstrate our method's effectiveness for concentric tube robot design by (1) showing that we can successfully design a robot for scenarios where prior concentric tube robot design techniques would fail, and (2) applying our new method to a medically motivated simulated scenario involving navigation through bronchial anatomy to reach several points of clinical interest in the lung."
"trevor bench-capon's value-based argumentation frameworks (vafs) are based on similar ideas. 23 however, vafs assume that arguments promote specific values, and the preferences are among these values rather than between the arguments themselves. again, the evaluation of a vaf is based on the generation of an af. here, attacks are disregarded whenever the attacked argument promotes a more preferred value than the attacker. vafs were further generalized to include different audiences that might disagree about the preferences among values. a comparison of pafs and vafs is available in further works (see the \"further reading in argumentation frameworks\" sidebar)."
"we calculate p (τ ) for τ varying in a range [[0, t ]] where t is the largest expected delay that we fixed at 20 seconds to obtain an estimate of the delay probability distribution in this range. eq. 2 requires continuous functions. in our case, the data contain different video sequences. we thus estimate the delay probability as the mean of the delay probabilities estimated for the different sequences. to simplify notations, we refer to this estimate as p (τ )."
"in our future work, we will test our approach on some smaller datasets in different conditions, and further test robust more methods for image segmentation. in addition, we will also consider perceptual testing and subjective evaluations."
"we apply our presented method to design a 3-tube concentric tube robot that can access two sites in distinct bronchi without damaging (e.g. colliding with or piercing) the walls of the bronchial tubes. irregularity of the bronchial tubes' shapes combined with very narrow passageways make this a difficult problem. a simulation of one of the resulting designs is pictured in fig. 4 with two configurations reaching the specified points with the robot's end-effector. averaged over 50 runs, our method took 174 seconds to compute designs which can reach these two points without colliding with the surrounding environment. since pre-operative ct scans are typically obtained at least a day before an actual biopsy, this is a clinically acceptable computation time."
"a s for future work, it seems fruitful to continue the recent research on adf semantics 13, 30 with a particular focus on the limits of expressiveness. another interesting, so far unexplored, aspect is to switch the language of the acceptance conditions. so far, these conditions describe relations between arguments in terms of standard propositional logic. moving to temporal, modal, or even nonmonotonic logic for the interpretation of the acceptance conditions offers new exciting research perspectives in the area of formal argumentation."
"shown in fig. 2 . model accuracy has a significant impact on the quality of motion plans and designs, as discussed in sec. vi. due to the importance of high quality motion plans and designs in minimally invasive surgery, the design process introduced in this paper uses one of the most accurate kinematic models available."
"here, a 2 supports a 3, which attacks a 4, creating a supported attack from a 2 to a 4 . another supported attack on a 4 comes from a 1, a supporter of a 2 . there's a secondary attack from a 3 to a 5 and a mediated one from a 6 to a 4 ."
"although our full method is asymptotically slower per iteration than the simplified method, it adds far more nodes to the designs' motion planning graphs than the simplified method and therefore comes to solutions in fewer iterations. this results in a significant speed-up: averaged over 40 executions in the twisted half torus environment described in sec. vi-a, the full method computes task-feasible designs approximately 94% faster than the simplified method, and 20% faster than a method with the same p design -based exploration but without design coherence."
"we perform a k-means clustering algorithm to reduce the uncertainty of the label by grouping samples that are close in the sense of the learned distance dw. we calculate the label of each group as the mean of the labels of the group. in order to initialize the algorithm, we sort out the samples by label values and gather them in k groups of the same size. we calculate the initialization seeds as the means of the features of each group's samples. this initialization is done to ease the repeatability of the clustering and because we expect to gather samples with neighboring labels after the clustering algorithm by using the learned distance dw. this step leads to the identification of a set of representative samples."
"to simplify analysis, we consider a weaker form of the method which does not use design coherence and performs one generate new design step and one explore prior design step at each iteration. these simplifications do not modify the algorithm's asymptotic behavior."
"to accelerate the algorithm, we leverage design coherence, the observation that collision-free configuration spaces of robots of similar designs are similar. our design method can be seen as an \"rrt of rrts,\" where the former rrt is in design space and the latter are in configuration space. we explore design space using an rrt that utilizes configuration space information from nearby design samples to accelerate configuration space exploration at new design samples."
"in set of graphs into g dnew . instead of collision-checking all of the edges under the new design, we take a lazy approach: we tag them all as \"unchecked\" edges in d new using the hash structure is checked. we only collision check an unchecked edge if it is expanded upon in explore prior design, thus speeding exploration while reducing unnecessary checks."
"as evaluation procedure, we first present the results of the full system (with feature normalization by subject, our timepersistent-correlation measure and our regression framework). then, we evaluate the contribution of each key point by replacing it by a more commonly used process (global normalization, pearson's correlation and support vector regression):"
"at the input layer of gan2, we do not directly use vector z but instead apply a dropout function to x by masking some of its values randomly. after using dropout the input dimension is still same as x's, less than the dimension of concatenating z into x. we can thus simplify system structure and reduce computing cost."
"in our first experiment, we consider a 2-tube concentric tube robot maneuvering through a simple tubular environment as shown in fig. 3 . the environment consists of a halftorus for which one half is rotated by 45 degrees such that the environment is non-planar. the robot starts at the center of the proximal end of the environment and the goal is to reach a point at the distal end of the environment."
"in this paper we investigate task-oriented design of concentric tube robots, which are minimally invasive medical devices capable of following curved paths through body cavities. these robots may enable physicians to perform new surgical tasks requiring greater dexterity than possible with current instruments, including skull-base surgery [cit], neurosurgery [cit], operation on a fetus in the womb [cit], and lung procedures [cit] . in fig. 1 we show by simulation how, if designed correctly, a single concentric tube robot can access multiple surgical sites in the bronchial tubes while avoiding damage to sensitive structures in the lung."
our design approach also requires a motion planner that can plan sequences of tube rotations and insertions that will maneuver the robot from a start location to a target while avoiding obstacles. prior motion planning methods have used improved kinematic models as they have been developed: from optimization-based motion planning using a simplified kinematic model [cit] to a sampling-based method using the fully torsionally compliant kinematic model [cit] . the planner from the latter paper is incorporated into our design method in order to provide probabilistic completeness guarantees and solution accuracy.
"in decision-making scenarios, we're often faced with pro and con arguments, and our decisions are based on preferences among these arguments. thus, it's natural to apply preference-handling techniques in argumentation. in fact, this was done even before abstract argumentation as such had emerged, 19 where an argument's strength is measured in terms of the specificity of the underlying information. 20 including the preference information in argumentation frameworks not only lets us model the problem more accurately, it also reduces the number of extensions we can obtain."
"in each of 15 blocks of fig. 2, we show 30 synthetic images generated in different conditions. the images shown in the first row contain only bird shapes and are generated using gan1. as comparisons, in the other two rows, we also show the synthetic images generated using gan1 trained in the other two different conditions. the first different condition (imd1) is that we use 600 raw color images instead of their object contours. the second different condition (imd2) is that we use the same 600 raw color images, but black their backgrounds and keep only the bird region of each image. imd2 uses additional bird shape information in comparison with imd1. the columns, from left to right in the figure, show an evolution procedure of generated images with increasing epochs. as gan1 is initialized randomly, the images in the blocks at the leftmost column show random noise. when running more epochs, the generated images in the first row show that clearer birds' shapes are synthesized. however, the images generated in condition of imd1 shown in the third row are only some colored squares, and the images generated in condition of imd2 shown in the second row are some fuzzy and irregular colored images. it means that the use of gan1 on raw color image fails to generate reasonable images. even if imd2 takes the bird shape information into account, the generated images in the second row do not show clearly recognizable bird either. as aforementioned in section 1, this phenomenon is probably caused by the case that a small-sized dataset has a data sparsity problem when simultaneously processing a large number of visual features, which has a poor impact on optimizing a gans model."
"to tackle the challenging problem mentioned above, we develop a divide-and-conquer approach by separately processing shape and the other two visual features: color and texture. in our approach, we train two gan models (gan1 and gan2). gan1 is for shape generation, and gan2 is for color image generation using color, texture and the shape information generated by gan1. our approach is based on two factors. the first factor is that shape is the most important element to convey the identity of an object among the three visual features in content-based image processing [cit] . when learning new words, humans tend to assign the same name to similarly shaped items rather than to items with similar color, texture, or size [cit] . to our knowledge, most previous studies used only gans to tackle visual features as a whole. however, if we do not have a large number of image instances to train gans, the number of samples of all visual features combinations may be small and lead to data sparsity. so, as a second factor, separately processing different visual information may be useful to reduce the adverse effect of data sparsity when training a gans model on a small-sized dataset."
we learned our system on the concatenation of the training and the development sets to compute our predictions on the test set. we compare in table 5 our results to those given in the baseline paper [cit] . we can notice that the results obtained on the test set are quite similar to those obtained on the development set. this highlights the high generalization power of the proposed framework. it can be explained by the small number of representative samples for the kernel regression (60 in our system) which limits the flexibility of the model and allows the system to only capture important trends in the data.
"the normalization by subject has increased the mean score by 9.8%. the effect on valence is more important than on the other dimensions, which can be explained because the selected features for valence predictions are more sensitive to human morphological variations. most of the features selected for the three other dimensions are high-frequency subbands energies extracted from the temporal signals, which are more robust to morphological variations than the signals' mean values that seem to be useful to predict valence."
"based on the idea [cit] that shape information plays a more important role than color and texture for general object identification. we have presented an adversarial approach in order to strengthen the contribution of shape information. we have demonstrated that the use of our proposed approach can generate better bird images than a typical gans model using and without using shape information on a small-sized dataset, and our classification accuracy results also show that the quality of generate images are comparable to real ones."
"as a comparison, from top to bottom row of fig. 3, we also show the examples of tsgan-img, gan-img and real-img, respectly. the quality of the images (tsgan-img) generated using our approach is much better than those fig. 3 . the three rows show the synthetic bird images of tsgan-img and gan-img, and the real images of real-img. generated using gans [cit] . in comparison with the real images, the quality of images of tsgan-img is close to the real images although a bit more details appear in the real ones."
"biopsy is required for a definitive diagnosis of lung cancer. however, many sites within the lung currently cannot be safely accessed for biopsy without highly invasive procedures [cit], which makes early definitive diagnosis and treatment of lung cancer impossible for many patients who cannot tolerate highly invasive procedures. a properly designed concentric tube robot may have the dexterity to safely reach sites deep within the lung that currently available medical instruments cannot."
"a robot's effectiveness at performing a task depends largely on its design. a robot's design can be seen as a set of parameters that are fixed throughout the robot's use. given the tasks we wish the robot to perform, we can design the robot to most ably perform the tasks while ensuring that the robot does not damage itself or its environment."
"the proposed fusion method, which is based on a simple linear combination of the inputs learned via local linear regressions, is particularly fast and well-suited for a real-time system. to evaluate the efficiency of this fusion method and the contribution of each feature set, we present the results we obtained by learning on the training set and testing on the development set in table 1 . we can see that visual features give better results than audio features. local appearance-based features give a better valence prediction than the other sets. the fusion system significantly improves arousal and power predictions giving a mean score increased by 11.7%. we can notice that when the four predictions (using each set of features) are accurate, the fusion is more successful. on the contrary, the prediction scores of valence and expectancy are lower and the fusion does not improve the system performance."
"the semantics of adfs in the original paper 28 were later generalized and further developed in various directions, including both extension 13 and labeling-based 30, 31 ones. in the first approach, a conflict-free extension is a set of arguments having their acceptance condition satisfied. admissibility generalizes the original intuition from dung by making sure that the extension has the power to discard undesired arguments. in the labelingbased approach, the definition of adf semantics is based on the notion of a model. a (two-valued) interpretation v-a mapping from arguments to the truth values t and f-is a model of an adf whenever v maps exactly those statements to t whose acceptance conditions are satisfied under v. the definition of grounded, complete, preferred, and stable semantics is then derived from an analysis in terms of three-valued interpretations, where an additional truth value u (undefined) is used. a key notion is the following consensus operator: for an adf d and a three-valued interpretation v, the operator γ d returns the (threevalued) interpretation γ d (v), which assigns to each statement s the consensus truth value for its acceptance formula ϕ s, where the consensus takes into account all possible two-valued interpretations w that extend the input valuation v. the relevant semantical notions are derived from this operator. for instance, the grounded model of d is defined as the least fixpoint of γ d . further technical details 30, 31 and a comparison of the extension and labeling-based approaches 13 are available in relevant sources. however, we want to emphasize that the adf semantics are actually proper generalizations of the original dung semantics in the sense that they treat adfs corresponding to afs in exactly the same way as defined by dung."
"we define the design of a concentric tube robot as a set of parameters that are selected before its use and cannot be changed once it begins to perform a task. specifically, we define a design d of an n-tube concentric tube robot as a vector of 3n real-valued parameters where we associate with each tube the following three parameters:"
"optimization approaches have been applied to the design of other types of robots. a sampling-based approach was applied to optimizing the base location of a robot manipulator [cit], but this approach requires an initial collision-free plan. merlet used interval analysis to find robot designs which fulfill a set of requirements [cit] . methods such as synergy [cit] and darwin2k [cit] rely on genetic algorithms to perform multiple simulations to optimize the structure of a robot for various metrics. this methodology has been applied to the design of manipulators [cit] and surgical robots [cit] . however, these approaches provide no guarantees on performance. other methods aim to optimize structural properties of robots for specific problem domains, including geometric methods for planar manipulators [cit], and grid-based methods for manipulators [cit] . in contrast to these methods, our sampling-based approach is applicable to the complex kinematics of concentric tube robots, accounts for obstacles in the workspace, and provides probabilistic completeness guarantees."
"(1) fig. 1 . a divide-and-conquer framework uses two gans to simulate the procedure of human drawing picture. gan1 is to generate object contours, and gan2 is to paint the black-and-white image generated by gan1."
"given that this kind of argument-based reasoning and decision making is ubiquitous, it isn't surprising that argumentation itself has emerged as a scientific field. it studies how to model arguments and their relationships, with the ultimate goal of solving conflicts in the presence of diverging opinions. thus, argumentation has also been referred to as reasoning tested by doubt (guillermo r. simari credits david zarefsky with this definition). argumentation has become a major focus of research in artificial intelligence (ai) over the last two decades. it is strongly connected and highly beneficial to various other ai subfields, in particular knowledge representation, nonmonotonic reasoning, and multi-agent systems. it has been successfully applied to legal reasoning, which uses argumentation principles to formulate legal cases as arguments. moreover, it has proved valuable in decision support for resolving conflicts between different opinions and in the context of dialogues and persuasion. finally, argumentation techniques can be found in expert systems in areas ranging from medicine to egovernment. due to space restrictions, we had to be selective with our references; more details can be found in the extended version of this article. 1 within argumentation, we can distinguish two major lines of research: logic-based and abstract approaches. the former takes into account the logical structure of arguments and defines notions such as attack, undercut, and defensibility in terms of logical properties of the chosen argument structures. in contrast, abstract approaches-the topic of this article-consider arguments as atomic items, focusing entirely on the relations between them. consequently, the arguments and the relevant a ssume you have to make an important decision, such as whether to accept a job you've been offered. the conclusion you have to reach will hardly be a simple deductive one. most likely, you'll construct arguments in favor of the new job (such as a better salary or better prospects for the future) and relations between them are assumed to have already been constructed, usually from a given background knowledge base. the obtained argumentation system is then evaluated on the abstract level, yielding possibly alternative sets of (abstract) arguments that may be collectively accepted. finally, these results are interpreted in terms of the original knowledge base. this three-step creation, evaluation, and interpretation process is known as the argumentation process or instantiationbased argumentation. 2, 3 this method provides a high degree of modularity, as solving a given problem is kept on an abstract level, detached from a particular representation in the modeling language used in the knowledge base. this article gives an overview of the most popular argumentation systems for this abstract level."
"the proposed framework, presented in fig. 1, is based on audio-visual dynamic information detailed in section 2. as visual cues, we propose a set of features based on facial shape deformations, and two sets respectively based on global and local face appearance. for each visual cue, we obtain a set of temporal signals and encode their dynamic using log-magnitude fourier spectra. audio information is added using the provided audio features. regarding the prediction, we propose a method based on independent systems for each set of features and for each dimension (section 3). for each system, a new correlation-based feature selection is performed using a delay probability estimator. this process is particularly well-adapted to unsure and possibly time-delayed labels. the prediction is then done by a nonparametric regression using representative samples selected via a k-means clustering process. we finally linearly combine the 16 outputs during a fusion process to take into account dependencies between each modality and each affective dimension (section 4). section 5 is dedicated to evaluation and analysis. finally, conclusion and future work are presented in section 7."
"under the telescoping dominant stiffness assumption, a concentric tube robot of design d dom should be able to reach the goal while avoiding collisions by following a sequential insertion strategy ending in configuration q seq . to best approximate a real-world concentric tube robot which may exhibit the telescoping dominant stiffness property, we simulated a robot with both the thickest outer tube and the thinnest inner tube found in prior literature [cit], with the inner tube having an inner radius large enough to still allow for passing tools through. we chose the inner tube's inner and outer radii to be 0.824 mm and 0.924 mm respectively (for a thickness of 0.1 mm), and the outer tube's inner and outer radii to be 0.925 mm and 1.1175 mm respectively (for dominant stiffness with sequential insertion our method with sequential insertion our method with motion planning fig. 3 . three mechanically accurate simulations of the insertion of a 2-tube concentric tube robot through a red twisted torus environment with varying designs and insertion strategies. the robot's outer tube is pictured in yellow and the inner tube in light blue. the top row shows a robot under design d dom which collides with the environment because it was designed and inserted under the assumption of telescoping dominant stiffness, which does not hold under a mechanically accurate kinematic model of concentric tube robots. when motion planning was applied to this design, a collision-free path through the tube under design d dom could not be found (not pictured). the middle row shows a robot design d * computed by our design method, but it is inserted with a sequential insertion strategy which does not require motion planning; when simulated under realistic robot kinematics, the insertion collides with the environment and fails. the bottom row shows a robot of our computed design d * successfully navigating the environment without collisions because it combines an accurate kinematic model of concentric tube robots with motion planning to enable collision-free performance of the task. a thickness of 0.25 mm). in order to realistically simulate the robot's behavior, we use a highly accurate, mechanics-based kinematic model of the robot [cit] ."
"the rest of the paper is organized as follows. we introduce the overview of gan and related work in section 2. we present the details of our proposed framework in section 3. the used data set and related evaluation metric are given in section 4. we analyze the obtained results in section 5, and finally conclude in section 6."
"in his seminal paper, dung showed that it's possible to analyze acceptability of arguments in an abstract way, independently of where the arguments come from and how they're generated. 4 moreover, he aimed to represent different types of nonmonotonic approaches in a uniform setting. to this end, he introduced his abstract argumentation frameworks (afs), a surprisingly simple concept. an af is thus nothing but a directed graph with a specific intuitive interpretation of nodes and links. because there are no restrictions on the attack relation, cycles, self-attackers, and so on, are allowed. arguments don't have any particular structure and the precise conditions for their acceptance are defined by the semantics. in what follows, we'll present several such semantics, originally defined by dung."
"for evaluating the efficiency of the new proposed correlationbased measure, we compare our results to those we obtain by selecting the features with a standard pearson's correlation measure which does not take the delay into account. the results are presented in table 3 . the use of the proposed time-persistent-correlation-based measure has increased the mean score by 6%, which can be explained by the improved robustness of the proposed measure to possibly time-delayed labels."
"to evaluate classification performance, we test three kinds of bird images. tsgan-img denotes that the images are generated using our approach. gan-img denotes that the images are generated using gans [cit], whose training images contain only the bird and backgrounds in images are blacked. in table 1, we show classification accuracy values obtained using classifiers on the three test datasets, tsgan-img, gan-img and real-img, respectively. in condition of case2, the classification accuracy on tsgan-img reaches 95.90%, which is much better than gan-img and is slightly less than that obtained on the cub-200 bird images. this case probably means that the images of tsgan-img contain most distinct bird features and have a comparable quality to the real images. the classification results, obtained in condition of case1, are relatively low. this is maybe caused by a factor that to train the classifier we only select 600 image instances of each class, same as the number of instances to train gans. this training number is relatively small for classification evaluation on cifar10 when testing mismatched data. even if the classification condition is poor, we still find the classification accuracy obtained on tsgan-img in condition of case1 is even better than those obtained on real-img. this is maybe caused by the irrelevant features learned from complex background of real images. they finally interfere the classifier. the low classification accuracy on gan-img is caused by poor image quality."
"we associate a distinct motion planning graph with each design sample because the collision-free configuration space of a robot is dependent on its design. for a design d, the sampling-based motion planning graph"
"in this section, we present the four different sets of features we used. we propose three multiscale dynamic feature sets based on video; the fourth one is based on audio."
"to accelerate the algorithm, we leverage design coherence, the observation that collision-free configuration spaces of robots of similar designs are similar. this coherence implies that information gained about the free configuration space of a robot under one design can be partially re-used for a similar robot design."
"we applied four different design methodologies to the problem of designing a robot to navigate this environment: 1) dominant stiffness with sequential insertion: we analytically derive a design d dom and configuration q seq for which q seq is collision free and reaches the goal under the assumption of telescoping dominant stiffness using the insertion strategy described in sec. ii. 2) dominant stiffness with motion planning: given d dom and q seq from above, we execute a motion planner that considers an accurate mechanics-based model of the robot kinematics in order to compute a plan to reach the goal. 3) our method with sequential insertion: we use our proposed method which computes a design d * that can navigate the environment without the assumption of telescoping dominant stiffness, but perform the task using the sequential insertion strategy. 4) our method with motion planning: we use our proposed method to compute a design d * and a motion plan that can navigate the environment."
"dung's frameworks consider only a single relationship among arguments, namely attack. in various contexts it's natural to go beyond conflict. in particular, the ability to model various notions of support appears useful."
we also simulated a sequential insertion of the robot under design d * that ended in the final configuration of the collision-free path yielded by our method. this failed to navigate the passage as shown in fig. 3 (middle row) . this demonstrates that motion planning is indeed vital to the design process of concentric tube robots under realistic kinematics.
"for the simplified version of the design method described in sec. v-a, the dominating asymptotic factors are the nearest neighbor searches. when implemented efficiently, these calls are logarithmic in complexity to the number of elements searched over [cit] . at iteration i, our method will have created at most i samples, so the complexity of iteration i of the simplified method is o(log i). we note that our integrated design and motion planning method is no worse asymptotically than rrt for motion planning alone. the heuristics in our full method add computational complexity: copying configuration samples from one motion planning graph to another of a similar design requires, in the worst case, o(i) time at iteration i. therefore, the complexity of iteration i of our full method is o(i)."
"adfs also provide a new handle on the treatment of preferences, 32, 33 values, and audiences. 34 as shown in the original work, 28 preferences on links between statements can directly be used in the definition of acceptance conditions. in the further developed work, 30 a treatment of preferences among arguments in the style of pafs has been introduced. a prioritized adf (padf) consists of a prioritized set of arguments, a set of support links, and a set of attack links. a padf is then compiled to a standard adf. the approach is shown to be a proper generalization of pafs. similar generalizations of vafs are straightforward. the same paper 30 also proposes a new approach to argumentation with dynamic preferences which, rather than being given in advance, are a matter of debate themselves. in a nutshell, dynamic preferences are handled as follows. we first guess a (stable, preferred, or grounded) extension m. we assume some nodes in m carry preference information. we extract this information and check whether m can be reconstructed under this preference information, thus verifying that the preferences represented in the model itself were taken into account adequately to construct the model."
"for each of these three sets, we calculate the log-magnitude fourier spectra of the associated temporal signals in order to include dynamic information. we also calculate the mean, the standard deviation, the global energy, and the first and second-order spectral moments. we chose to compute these features every second for different sizes of windows (from one to four seconds). this multiscale extraction gives information about short-term and longer-term dynamics."
"as output of this system, we obtain temporal signals: some of them correspond to the external parameters and give information on the head position, and the others characterize deformations related to facial expressions."
"concentric tube robots are composed of nested, pre-curved tubes, usually shaped with a straight section followed by a constant-curvature section. as each of the robot's component tubes is independently rotated or extended, the entire device can change shape and trace intricate paths through open air or l. g. torres tissues. these devices are highly customizable by selection of component tubes with varying properties. the pre-curvatures and lengths of the tubes have a significant impact on the set of clinical targets reachable by the device, so proper selection of design parameters for a patient's anatomy is critical to the success of a surgical procedure. our goal is to computationally optimize the design of a concentric tube robot on a patient-and surgery-specific basis to enable a single robot inserted into the patient to reach multiple clinically relevant sites. prior to the procedure, we assume that a volumetric image (e.g. ct scan or mri) is available, from which we can extract the geometry of the anatomical environment through which the robot will navigate, including free space and anatomical obstacles such as nerves, vessels, sensitive organs, and bones."
"we consider that the fusion of feature maps with too small resolution has basically no improvement in detection accuracy, but will slow down detection speed. and ssd generally uses low-level feature map to detect small targets. therefore, only the feature maps of the five scales of dense block (1), dense block (2), conv7, conv8_2, and conv9_2 are selected for feature fusion (as shown in fig. 1 ). there are two main methods to fuse different feature maps together: element-wise summation and concatenation. element-wise summation requires that the feature maps be the same size and must be converted to the same channels, thus limiting the flexibility of fusing feature maps [cit], concatenation can achieve better results than element-wise summation. therefore, we use concatenation to perform feature map fusion."
"according to the literature, significant positive relationships could be found between it adoption and government support [cit] . because of their size and lack of resources, smes generally depend more on external resources and support than other companies [cit] . according to fink [cit], government support for facilitating information transfers to smes is incrementally increasing. government initiatives and policies could directly and/or indirectly stimulate the development of it infrastructure and information provision to energize faster technology diffusion [cit] . nevertheless, the literature suggests that governmental assistance is generally not advantageous. a study by dutta and evrard [cit] on small businesses in six different european countries indicates that although governments have tried to assist smes in adopting it through increasing public spending on technology projects, however, there are adoption barriers built into governmental agency mechanisms designed to help these businesses. these adoption barriers are attributable to the gap between what is really required by smes and what is provided by the government [cit] for analyzing the computerization experience of 40 small businesses which have computerized through government incentive programs, with another 40 small businesses which have computerized without government assistance. [cit] showed that participation in a government computerization program has not resulted in more effectual is. however, this program has encouraged small businesses which suffer a lack of financial resources and technical expertise to computerize their operations. from a similar perspective, fink [cit] found that government grants do not appear to be a significant factor supporting it adoption within australian smes."
"in summary, it could be inferred that ceos of smes are not the only users of it who contribute to the success of the implemented it. as the valuable assets of firms, employees also have a drastic influence over adoption and successful implementation of new it. therefore, development of these resources seems to be necessary for the success of the business [cit] ."
"premkumar and roberts [cit] suggested that increasing users' awareness of the benefits of information telecommunication technologies will also positively influence the process of adopting these technologies, while this awareness could be amplified through improved education and training. correspondingly, a study by kleintop and blau [cit] investigating the impact of end user training on electronic mail system implementation has demonstrated that end users practicing with new it systems before implementation will result in higher level of it system acceptance. in addition, their research suggested that the increment in amount of training among end users before it implementation might lead to higher levels of perceived ease of using it, as well as perception of is usefulness. moreover, it was suggested that positive change or improvement of business functionality through new systems (e.g., new it applications) may not be believed by some employees [cit] . in such circumstances, it was suggested that employing new staff instead of training current employees might be more effective [cit] ."
"the element-wise summation and concatenation fusion methods increase the model performance by 2.3% map and 3.2% map, respectively (rows 6, 7, and 8), and concatenation is better than element-wise summation with a margin of 0.9 points. therefore, we chose concatenation as the feature fusion method of df-ssd."
"we start the proof from an assumption that, the effect of number one in the logarithm terms can be ignored, the transmission power p t in (9) and (10) therefore can be separated from integrals. thereafter, by using gauss, gauss-hermite, and gauss-laguerre quadratures on all of the capacity expressions, we can reduce the computational complexity and process do simplification works, and then we can conclude the proof. it should be noted that, more accurate estimation can be obtained by increasing the quadrature orders."
"the df-ssd we proposed is an improved ssd object detection algorithm based on densenet, feature fusion, and residual prediction module. the way of training from scratch enables us to use less training data to achieve more competitive performance compared to other pre-training models. our df-ssd can achieve advanced performance on three common datasets with real-time processing speed and more compact models. moreover, df-ssd shows good detection effects for small objects and objects with specific relationships."
"in addition, organizational culture is another significant determinant of is/it implementation in organizations [cit] . a number of organizational culture definitions can be found within the body of literature. for smes, culture can be regarded as the way of doing and sharing things for individuals through compliance with the firm's beliefs, values, and attributes [cit], or, it can be defined as indigenous characteristics of organization including level of openness to change and characteristics of human resources [cit] suggested that characterizing organizational culture is necessitated since the culture and its various impacts are the key to success of it projects (such as enterprise resource planning (erp)) that are an integral part of significant organizational change. these authors argued that many erp failures can be attributed to paying inadequate attention to the culture of the organization while it is imperative to notice that in most of the time, 'desired' and the \"actual\" organizational culture are different [cit] . in light of organizational readiness to change, [cit] suggested that organizational cultures having a more supportive climate and flexible structures might be more advantageous to successful deployment of new technologies in organizations than less flexible and mechanistic cultures. in addition, constructs of organizational culture, including perceived norms, values and attitudes, predominant in organizations, might affect the behavior of employees toward ict in those organizations [cit] . regarding these findings, [cit] asserted that employees perceiving the culture of their organization as open system are more inclined to have positive attitudes toward organizational change, and subsequently will perceive more readiness for changes before the deployment of new technology in the organization. these authors demonstrated that employees who perceive strong human relations values in their area have shown a greater readiness for change prior to the deployment of a new computing system."
"by adding the residual prediction module, the model performance increase from 76.6% map to 78.9% map, which is an improvement of 2.3%. after adding coco datasets for training, the module performance is further improved."
"in most organizations, employees are regarded as significant assets, and along with the role of owner/manager, seriously affect the firm's survival and success [cit] . these assets, as the users of it within smes, are another precious resource for firms [cit] which need to be developed to contribute to the success of business [cit] ."
"to further verify our analysis results in fig. 4, in this section, we experimentally evaluate the ee improvement of noncooperative and cooperative traffic forwarding scenarios by smartphone test-bed which follows from real communication standards. in this work, we use wi-fi and bluetooth interfaces to transfer local data and share control information among mts, respectively, and lte link is created to communicate between the proxy and bs."
"furthermore, family involvement and intervention in firm management could have significant impacts upon it adoption [cit] . it has been largely confirmed that in family businesses, the tenure of senior managers who are family members is much higher than those in non-family organizations [cit] . family firm managers are less inclined to have higher educational levels than those in non-family firms [cit] . family firms have also been characterized by having smaller management teams while larger management team, can result in more effective business management [cit] . smith [cit] provided evidence that the size of a firm is a dominant determinant of managerial differences, since these differences between micro, small and medium enterprises have been found to be more significant than those between family and non-family businesses. the above mentioned findings, together with specific characteristics of family businesses (such as lack of professionalization, more informal organizational structures, and reliance on informal internal control systems) suggest that the objective of it adoption as well as the implementation process might be highly different in family smes [cit] . in addition, the involvement and intervention of family members in day-to-day activities and management of family business may bring about organizational issues since, in most small businesses, family members are hired to fill vital positions [cit] . compared with the hiring of external staff that are better fitted for positions in the business, family members' non-qualification often results in management problems such as ineffective it usage [cit] . a study by de lema and duréndez [cit] on managerial behavior of family smes found that when businesses are managed by family members, lack of attention to personnel training and management qualifications and a commitment to family well-being might result in inefficiency in the decision making process. likewise, an empirical study by lybaert [cit] found that less significant family ownership and intervention in strategic management tends to produce higher levels of information use in smes."
"characteristics of it users, including knowledge of it, training, attitudes and intention toward it, and participation and involvement in adoption process could affect is/it acceptance, or its adoption process as well [cit] . lack of training and skills of it in organizations will result in a limited use of it and lack of success in reaping benefits from computer hardware and software in organizations. this situation will further lead to a lack of is/it adoption success in smes, given that the successful adoption of it needs the sharing of knowledge, training, and higher levels of skills by those employees who use it [cit] ."
"the total power consumption p coop con for cooperative transmission forwarding is a function of the power consumption of the clients p clients con and proxy p proxy con . therefore, we have the following expression:"
"the process of it adoption within smes also depends on characteristics of marketed is/it itself which consist of a cluster of factors including type, process compatibility, user friendliness and popularity of implemented is/it, quality of software available in market and the costs of it [cit] . for adoption of enterprise application software, easy-to-understand and relatively long-experienced enterprise applications are more effective in smes as compared to hard-to-understand and brand-new applications [cit] . given that information systems and technologies are considered as the major enablers of superior business performance, quality of is/it products available in the market (e.g., attribute of the selected product, its reliability and usefulness) could be an important determinant when it comes to deciding on the adoption is/it products among smes [cit] ."
"the associate editor coordinating the review of this manuscript and approving it for publication was lefei zhang . been widely applied in many visual tasks, including object detection."
"it has critically become an indispensable tool for the daily operations of organizations. smes now invest significant amounts of financial resources in it to strengthen their competitive positions [cit] . due to the large-scale application of it among smes, they have been exposed to several associated risks within the adoption and development of it solutions [cit] . prior literature on it adoption in smes shows that approximately most failures and most dissatisfaction resulted in one or more of the following reasons [cit] 49, 58, 78, 127, 155, 156] :"
"because of the rapid and explosive development of wireless applications [cit], recently the mobile terminals (mts) are very popular and important to modern people for various kinds of network communication demands in cellular systems, for example, high-definition video streaming, online gaming for multiple users, and instant text messaging, etc. as the mt applications are growing dramatically, the traffic loading of network and the power consumption of each mt the associate editor coordinating the review of this manuscript and approving it for publication was guan gui."
"business size definable by turnover and/or number of employees is one of the most important determinants of it adoption [cit] . the importance of firm size is partly due to its role as the source of the firm's capabilities [cit] . another rationale, however, is the fact that firm's resources, including financial and human capital, might be an approximation of firm size [cit] . thong and yap's [cit] survey points out that business size is the most important discriminator between adopters and non-adopters of it within singaporean small businesses. likewise, an investigation by premkumar and roberts [cit] of rural small businesses revealed that even within the small business category, firm size is the most important determinant to the adoption of it. this finding is reinforced by a study of premkumar [cit] on it adoption within 207 smes who indicated that larger firms in the small business group have a higher inclination to adopt communication technologies than smaller ones."
"f a c il it a t e / a s s is t / r e g u l a t e to come up with more an organized and efficient deployment and application of it, smes must precisely realize their need for it and the proportionate advantages of it for their business. smes ought to judge costs and benefits associated with utilizing it. in this regard and in addition to taking advantage of external consultants, ceos should actively participate in the it benefit/risk evaluation process to assess whether the benefits of it for their business overweight its risks. in general, is/it is regarded as a crucial resource required for better communication and integrating business functions [cit] . it is believed that it promises many benefits for smes, ranging from modest advantages, such as reduced communication and administration costs, to transformative advantages such as quick response retailing [cit] . however, the appearance of new inter-organizational systems such as electronic customer relationship management (e-crm) and erp has made it more sophisticated to identify and evaluate the benefits of it adoption which necessitates more effective benefit monitoring. in particular, smes managers should realize that it is not only generic it, per se, that directly affects relative firm performance, rather, higher-order process capabilities act as the mediators between it resources and firm performance which transform the value of it resources controlled by smes to business performance. in other words, a firm's it resources can augment critical organizational capabilities (such as green management, integrated supply chain processes, and coordinated manufacturing), which can result in improved company performance."
"in this paper, feature fusion and reuse between different layers are enhanced by replacing feature extraction structure and redesigning the front-end prediction network, which includes multi-scale fusion module and residual prediction module."
"the first aspect is in a scenes that contains small or dense objects, as shown in fig. 8 (a) . ssd algorithm does not work well on small targets, but df-ssd shows obvious improvement. on the one hand, compared with large objects, the position information of small targets is more likely to be lost during the detection process. on the other hand, the recognition of small targets depends more on their surroundings. since ssd only detects smaller objects from shallow layers such as conv4_3, whose receptive field is too small to observe the object's context information, which results in bad detection performance of the ssd on smaller objects."
"in smes, the it adoption process is directly affected by top management where all decisions from daily functions to future investments are made by them [cit] . smes mainly have simple and highly centralized structures with the ceos in which, in most cases, the owner and chief manager are the same person [cit] . a number of studies have revealed that in smes, the role of ceos (top management or owner/manager) is central to the enterprise, since their decision influences all firms' activities, both currently and in the future [cit] . this also refers to it adoption decision from planning stage to the implementation, maintaining, and system upgrade stages [cit] ."
"the aim of this research is to achieve a better understanding of it adoption in smes through explicitly and understandably exploring and identifying factors influencing it adoption processes within smes in both developed and developing countries. this will be undertaken by reviewing existing literature with a high concentration of certain sme-related issues. the proposed conceptual framework demonstrates the determinants of the it adoption process in smes through a review of prior literature, including concepts, methodologies, theories, empirical research and case studies related to it adoption among smes, and by combining existing perspectives. the research investigates and reveals a number of internal and external issues pressuring and persuading smes to adopt it solutions. likewise, barriers to it adoption in smes will be addressed by reviewing and classifying it adoption factors. in addition, by using the proposed conceptual model of effective it adoption, the authors propose a systematic it adoption strategy for smes to succeed in it institutionalization at different adoption stages."
"in addition, we also developed a novel smartphone application on android operation system and installed this application on a google smartphone nexus 5x to process the cooperative operations among the mts. for giving an intuitive understanding, in fig. 6, an example of the application interface is shown. for the smartphone application used on the mts, it can realize cooperative communications among mts. besides, it can also implement the operations that, one mt serves as proxy while other mts can do communication via the mt which serves as proxy. the detail information about the database is listed in table 3. besides, this information is also shown in the main interface of our self-developed smartphone application (please see fig. 6 )."
"the technological characteristics of it products available in the market, including their compatibility and security, are also significant determinants of it adoption in organizations [cit] . compatibility is an important technological characteristic perceived by individuals, which was suggested by diffusion of innovation theory as a driver of the decision to adopt a new system [cit] . it compatibility can be defined as the extent (or ease) with which it is integrated with the existing technological infrastructure, culture, values, and preferred work practices of an organization [cit] . several prior studies on it adoption within smes found that it adoption and usage is significantly affected by the compatibility of relative products [cit] . it is imperative that ceos of smes consider the most appropriate application in their businesses when deciding whether or not to implement new it [cit] . deficient it investment decisions (regarding the compatibility and security issues of it products) can impose a significant impact on organizational profitability [cit] . it can participate in enhancing sme performance. nevertheless, with no effectual it adoption and development strategy in the right place, the anticipated and demanded performance enhancement may not materialize. therefore, with its counter-productiveness, it might be considered as an asset sinkhole [cit] . in this regard, it was found that the high cost of it tools and expensive software in addition to ict security concerns are the major risks of ict adoption perceived by malaysian and australian smes [cit] . with regard to the above-mentioned findings, it could be inferred that technological characteristics of marketed it products has become one of the common concern of smes when it comes to adopting it."
"ssd adopts multi-scale feature layers for object classification and location. the low-level feature maps have the characteristics of high resolution and small receptive field, which can well represent the detail information such as the texture and edge of the image. this is beneficial to object positioning, but its weak global semantic features are not conducive to object classification. on the contrary, the high-level feature maps can provide rich semantic information, which is beneficial to object classification, but the low resolution of the high-level feature maps is not good for the object location task [cit] ."
"feature fusion of different scales is a very effective strategy to realize feature complementarity between feature layers of ssd, which is also confirmed by feature-fused ssd [cit] and fssd [cit] . sharpemask [cit] and fpn [cit] fuse feature layers of different scales through top-down structures to generate new feature layers for classification and position regression."
"after adding the residual prediction module, the model detection performance of df-ssd is improved from 76.6% map to 78.9% map, which verifies the validity of the residual prediction module. our overall model improved by 3.1% (78.9% vs. 75.8%) compared to ssd300 [cit], and by 1.2% and 0.3% compared to dsod300 [cit] (77.7% map) and dssd321 [cit] (78.6% map), respectively. the feature fusion module of dssd321 only uses deconvolution to integrate the semantic information of high-level feature maps into low-level feature maps, which proves that the bi-directional fusion method of df-ssd is more effective."
"prior research on is/it within smes have revealed a number of organizational characteristics as potential determinants of the adoption process which include sme strategies, business size, type of industry, information intensity, organizational culture and technological maturity [cit] . strategically, it tools are employed within smes in order to achieve pre-determined business strategy. therefore, smes' investments in it are strongly affected by their strategic context, such as cost reduction versus value added strategies [cit] . according to nguyen [cit], many businesses adopt new it merely to keep up with other smes which have implemented these technologies. under such circumstances, lack of definition or strategy of the purposes of it adoption will lead to project failure."
"similarly, and based on the upper echelon theory, prior literature suggests that ceo's demographic characteristics and personality traits of openness and extraversion are the significant determinants of it usage behavior and performance within businesses [cit] ."
"model fine-tuning limits the structural design space of the object detection networks. this is critical for deploying deep neural networks models into esource-limited internetof-things scenario. however, our model df-ssd only uses about only 1/2 parameters to ssd300, 1/9 to faster r-cnn, and 1/4 to r-fcn, which shows great application potential in low-end devices such as embedded electronic products and mobile phones. all in all, the way of training from scratch is very interesting and deserves our in-depth study and discussion."
"a number of prior studies have attempted to gain a clear understanding of numerous pitfalls and challenges associated with it adoption awaiting smes, as well as evaluate those factors affecting the successful deployment of it. the authors categorized influencing factors into two main groups: internal and external. internal factors include top management, a firm's resources, end users and organizational characteristics. external factors comprise characteristics of it products, external and competitive pressure, external it consultants and vendors, and government. the authors believe that the categorization of it adoption issues and sme-related factors through a developed, integrated framework and suggested model of effective it adoption process can help organizations, managers and it consultants to achieve clearer understanding of it adoption influencing factors, and also add further knowledge to the literature. although the authors inclusively discussed various distinguished influencing factors affecting it adoption decisions, acceptance, satisfaction and usage, the authors did not categorize the reviewed influencing factors in terms of different adoption concepts. this issue can be addressed by future research. this paper might not cover all aspects of the it adoption process in the literature. likewise, due to the unique characteristics of each organization and its specific conditions of technological innovation diffusion, it is not claimed that this framework is applicable for all firms or is able to deal with all of their issues. for this reason, these findings require empirical testing to determine their relevance and conformity in the practical setting. in addition, a more comprehensive study of it adoption within smes for investigating sme-related influencing factors simultaneously with other aspects (drivers, enablers and inhibitors) of it adoption seems to be necessary."
"under the effect of a frequency selective simo channel, for non-cooperative transmission scenario, if the transmission and its l sub-channels are also affected by the shadowing and spatially i.i.d. rayleigh fading, then its ergodic capacity is given by (9), as shown at the bottom of this page, where"
"become very important issues for modern mobile networks. in order to lower this kind of transmission loading and improve the performance of power saving for each mt in an effective and efficient way, forwarding transmission by utilizing cooperative wireless methods among mts, which is also called user cooperation or cooperative communications, is widely considered and investigated as a promising approach to solve these issues [cit] . the cooperative traffic forwarding can achieve higher spectrum utility and energy efficiency (ee) [cit] by using the device-to-device (d2d) transmission methods. in current wireless technology standards like long term evolution (lte) direct and wi-fi volume 7, 2019 this work is licensed under a creative commons attribution 3.0 license. for more information, see http://creativecommons.org/licenses/by/3.0/ direct [cit], d2d communications has been realizable in practice cases [cit], and have motivated industry standardization efforts as well as a lot of academic researches [cit] . in order to implement user cooperative traffic forwarding, a series of processes, for example, link sensing and configuration, especially, choice of a proper proxy, which is also called aggregator or head-mt [cit], are needed. for this topic, many studies about how to select a proper proxy in wireless sensor networks, for example, low energy adaptive clustering hierarchy, well known as the leach technology [cit] and so on [cit], can be found in the literatures. in recent years, although several proxy selection methods have already been proposed and this topic have been widely researched, the existing transmission schemes are less related to the issue about how to use spectrum resource effectively. besides, these schemes cannot be implemented directly on cellular systems because they are not designed for being applied on higher data rate transmission applications, and cannot well take the advantage of the channel state due to that base stations (bss) are hard to consistently control the nodes. henceforth, user cooperative communication technique adopted in cellular systems has been very attractive to academic and industrial groups."
"based on suggested categorization and reviewed influencing factors, and to come up with more systematic guidelines for effective it adoption by smes, the authors put forward a conceptual model (figure 2 ) that is believed to assist successful it institutionalization in this context. this model has been conceptualized based on extant perspectives and theories, and uses the technological innovation literature as a reference discipline. as suggested by prior literature, initial it adoption, it implementation and post-implementation of it are three different stages in the technology innovation cycle [cit] . initial it adoption refers to a stage in which decisions are made about whether to adopt a new is/it. if the decision is to go ahead with adoption, the it implementation stage involves implementing the it infrastructures (including hardware and software) in the organization. once the it has been implemented successfully, the it post-implementation stage is concerned with how much organizational learning takes place within the business so as to facilitate further it adoption [cit] . accordingly, our suggested model of the it adoption process addresses all the three different stages and definitions of it adoption in providing guidelines for successful it adoption in smes. it is believed that the presented categorization of it adoption issues and factors through the developed conceptual framework and conceptual model of effective it adoption process can help governments, organizations, managers and it consultants to achieve a clearer understanding of the it adoption process. it may also increase the knowledge and literature bases by providing a clearer understanding of the reasons and methods that smes adopt it, and establish the determinants that contribute to the success of the it adoption process in these businesses."
"in the aforementioned bandwidth allocation strategy, since ν is a proportion coefficient ranged from 0 to 1, based on the classical shannon's information theory, two extreme cases of 0 and 1 will result in that transmit power at clients and proxy goes to infinity. therefore, we can infer that, there exists an optimal ν value which can minimize the consumption power of the whole system, and this optimal ν value indirectly varies with the selected proxy and uplink csi of mts. this fact therefore enables system simplification and characteristic improvement by the machine learning approaches. in brief, we can use the position and demand information of each mt, to train neural networks of predicting proxy and bandwidth proportion coefficient ν. we leave the details of this machine learning approach described in sec. vii, and an illustrations of the considered non-cooperative and cooperative transmission scenarios, the corresponding bandwidth allocation strategy are shown in fig. 1 ."
"recently, with the rapid development of cnn, a large number of highly efficient models have emerged in academic circles, such as vggnet [cit], googlenet [cit], and resnet. however, with the deepening of the number of network layers, the pretransmit signals and gradient signals in the training process of the network may gradually disappear after passing through many layers [cit] . many excellent papers have focused on solving this problem. for example, resnets and highway networks [cit] all propose a skip-layer technique to enable the high-speed flow of signals between the input and output layers."
"and we denote l as the channel length which is depending on the receive filter, the transmit filter, and the delay spread power profiles. it should be noted that the channel attenuation model becomes frequency flat model when the conditions of"
"as the majority of smes have limited financial resources [cit], deficient it investment decisions can impose momentous impacts on organizational profitability or even survival, while investment in newly presented technology often involve high risk and might imperil an smes' survival [cit] . however, it is imperative for managers to know that pioneers in adopting new information technologies reap most benefits. for example, in the banking industry, pioneer banks that adopted new technology or developed new applications for existing technologies are the organizations that achieved the most benefits from their risky investment [cit] . consistent with the resource-based view of the firm suggesting the complementarities of firm resources in value creation [cit], and due to wide availability of generic it in the market, simple it alone cannot be a source of competitive advantage [cit] . the adoption of state-of-the-art it applications ahead of competitors will make it resources 'firm specific' and imperfectly mobile across firms, providing the adopting firm with additional business value not achievable by late users [cit] ."
"as an object detection algorithm based on deep learning, single shot multibox detector (ssd) [cit] has high performance in both detection accuracy and detection speed. [cit] to solve the problem of insufficient detection accuracy of yolo [cit] series algorithm in object positioning. its main idea is to sample densely and evenly at different locations of the image. ssd draws on the concept of anchor in faster r-cnn [cit] . at the time of sampling, the object bounding box is predicted by priori boxes of different scales and aspect ratios, then the feature is extracted by cnn and then classified and regressed directly. the whole process only needs one step. ssd adopts the feature layer group of the pyramid structure for object detection. the high-level feature maps with large receptive field are used to predict large targets, and the low-level feature maps with small receptive field are used to predict small targets. however, the feature layers of different scales are independent, lacking the complementarity of features between feature layers, which makes ssd less effective in both general object detection and small object detection tasks. the idea to solve this problem is to fuse the high-level semantic information with the low-level detail information. therefore, in order to enhance the representation ability of low-level feature maps, we map high-level feature maps rich in semantic information in ssd network structure to low-level networks by deconvolution. in addition, atrous convolution is used to make the low-level feature maps have high enough resolution to obtain the location information of objects. experimental results show that our detector improves the small objects detection capability of ssd. df-ssd has advanced detection effects for small objects and objects with specific relationships."
"in this 21st century, a worldwide system of commerce is evolving, in much the same way as national markets evolved from local and regional networks. the modern economic environment which is dominated by globalization, hyper-competition and the knowledge and information revolution has revolutionized the way business is conducted [cit] . this new technological epoch is apparent through intensified investment in computer-processing and data preparation appliances in the manufacturing and service industries and telecommunications infrastructure, and also to its widespread usage in government agencies, educational organizations, and, more recently, in the household. as a result of this technological progress, the implementation and application of it is a significant driving force behind many socioeconomic changes [cit] . as the utilization and commercialization of it becomes more widespread throughout the world, the adoption of novel it can generate new business opportunities and various benefits. nowadays, both large organizations and small and medium-sized enterprises (smes) are seeking ways to reinforce their competitive position and improve their productivity [cit] . accordingly, there is an increasing consciousness of the necessity to derive profit through investment in it within smes. it tools can significantly assist smes by supplying the required infrastructure, which is necessary for providing appropriate types of information at the right time. it can also provide smes with competitiveness through integration between supply chain partners and inter-organizational functions, as well as by providing critical information [cit] . however, prior it literature has shown that only a small number of studies focused on the adoption and use of it in smes [cit] . moreover, it has been found that in spite of the exponential growth of it within smes, the rate of it adoption by these businesses has remained relatively low [cit], and large organizations have noticeably profited more than smes in both it-enabled improved sale and costs savings [cit] . in looking for reasons for such differences in it adoption in smes, unique characteristics of these businesses can be highlighted. smes generally have limited access to market information and suffer from globalization constraint [cit] . in addition, management techniques such as financial analysis, forecasting and project management are rarely used by smes [cit] . a tendency to employ generalists rather than specialists, reliance on short-term planning, informal and dynamic strategies and decision-making process, plus an unwillingness to develop and the use of standard operating procedures are other distinctive characteristics of smes [cit] . however, it is the restricted resources controlled by smes, commonly referred to as resource poverty [cit], that is the major differentiator between smes and large organizations. compared with large organizations, smes are relatively weaker at various levels (i.e., organizational, managerial, technological, individual and environmental). therefore, it adoption and usage in smes is at a disadvantage [cit] ."
"based on the above-mentioned viewpoints and studies, the authors conclude that regarding the lack of it knowledge and internal it/is experience and skills, smes could fill this knowledge gap the through the use of external assistance, such as engaging external experts and the use of vendor assistance. the authors suggest that because of the unique characteristics of sme resources and financial poverty, smes should cautiously consider the available financial resources for hiring external consultants since they generally entail considerable expense. moreover, it should be considered that external experts' recommendations and suggestions might not be always a practical fit for sme requirements if strategies and objectives of the businesses are not sufficiently understood. as a result, a clear objective and definition of new it implementation within smes seems to be necessary [cit] ."
"the ceos of smes are required to be supportive regarding the implementation of it in their businesses and should actively participate in the implementation process. this means that they should be willing and committed to provide the necessary resources and authority/power for it adoption. this support can involve providing training campaigns, allocation of adequate resources such as financial resources and encouraging staff to use is in their daily activities. likewise, user it knowledge is a significant facilitator of implementation success within smes. smes have to ponder the fact that due to the general lack of is expertise in these businesses and the difficulty of recruiting is professionals, user is knowledge can facilitate more successful is implementation through a decreasing degree of uncertainty entangled with is implementation, increasing satisfaction with the implemented is and enhancing the effectiveness of contributions to the different phases of is adoption. therefore, the level of is knowledge and skills of users of is in smes should be improved. similarly, these businesses should provide themselves with potential knowledge resources from networking and also benefit from it when it comes to implementing it, given that smes' employees generally lack it skills (e.g., it knowledge and computing skills). in smes, networking can be defined as the amount of interaction between organizations, counterparts, suppliers, customers, and vendors so that they could be either personal network or business network [cit] . hence, networks are crucial ways for acquiring access to external knowledge required for successful implementation of it [cit] ."
"for the implementation stage, and as suggested in literature, external assistance is imperative for successful it implementation in smes, as these businesses generally suffer from a lack of it knowledge, skills and training resources. as a result, external consultants and vendors are the main sources of external it knowledge and skills in smes. accordingly, higher levels of external consultants and vendor effectiveness and support will increase it effectiveness in smes [cit] . nevertheless, regarding the fact that smes typically do not have sufficient financial resources to afford the costs associated with hiring external experts and it training campaign expenses, as well as the fact that some smes do not trust in using external expertise and consultants, the role of government support and initiatives to help and encourage the adoption of it is much more significant in the context of smes. likewise, it is very important that governments precisely consider what is demanded to support it adoption in smes in order to eliminate the gap between the support provided by government and the needs of smes. in spite of some reports of disadvantageous and ineffective assistance provided by governments, a number of studies have demonstrated that it adoption in smes has been significantly improved through supportive policies and initiatives provided by both developed and developing governments, especially in recent years. thus, governments should provide comprehensive policies and support to encourage small and medium enterprises to develop and use it. policies and support should be periodically re-evaluated to suit the dynamic characteristics of smes, it tools, dynamism within the global economy and market conditions. on the other hand, smes are very susceptible to security issues, such as a sense of insecurity and vulnerability about performing transactions through the internet, as well as the risk of information loss and digital theft when putting information online. therefore, it could be suggested that through the passing of cyber laws by governments to regulate and secure online transaction activities, and also by providing appropriate anti-virus and/or firewall/security protocols for smes by vendors and service providers to reduce or prevent the attacks of hackers, viruses and spyware, the perceived risk of it adoption by these businesses, should be alleviated."
"in fig. 8, we show some detection examples of the ssd and df-ssd model on coco datasets. compared to ssd, our df-ssd model improves mainly in two aspects."
"from our analysis results in sec. v, it can be known that, there exists an optimal value of ν which can minimize system power consumption. in order to find this optimal value ν op, bs needs to collect the uploaded csi and traffic demand of each mt, and perform a series of calculation tasks, which causes some delay and thus reduces communication efficiency. because if we want to resolve this kind of tasks which are complex and computationally resource consuming, the past experience and available data are needed rather than using approaches based on explicit rules and instructions. therefore, the machine learning approaches can be adopted in this study to reduce system complexity and improve transmission performances."
"another influencing factor attributable to the top management characteristics is ceo innovativeness, both in general and it-specific terms [cit] . personal innovativeness in it (piit) has been revealed to be a reliable predictor of users' attitude about the simplicity of use and effectiveness of new technologies [cit] . piit refers to \"the willingness of an individual to try out any new information technology\" [cit] . agarwal and prasad [cit] discuss that piit is a major determinant of it acceptance by moderating in perceived usefulness (pu), compatibility and perceived ease of use (peou). here, it should be considered that in most of it acceptance models, such as technology acceptance model (tam) [cit], decomposed theory of planned behavior (dtpb) (firstly introduced by taylor and todd [cit] ), unified theory of acceptance and use of technology (utaut) [cit], as well as in the majority of models of users' satisfaction, including end user satisfaction (eus) model [cit], model of small business user it satisfaction [cit] and wixom and todd [cit] integrated model of user satisfaction and technology acceptance with it, pu and peou, are two key constructs of user behavioral intention and subsequently it usage behavior. thatcher and perrewe [cit] demonstrated that highly innovative individuals with higher levels of piit are more likely to look for stimulating experiences, as well as having more confidence in their competence to use it. on the other hand, individuals possessing lower levels of piit are more likely to present general computer anxiety, and they might have less tolerance for risk."
"the capacity used in cooperative forwarding scenario is derived with the fact that the channel with best condition is selected and used. when a mt with best channel condition is selected as proxy, the ergodic capacity of the simo channel for this mt can be written as"
the network structure of df-ssd can be divided into two parts: the backbone network densenet-s-32-1 (a variant of densenet) for feature extraction and the front-end network (including feature fusion module and residual prediction module) for object detection on multi-scale feature maps. the structure of the entire model is shown in fig. 1 .
"first of all, because of the physical constraints applied on the proxy, for example, maximum transmit power, we cannot arbitrarily increase the total communication demand. this condition can be verified by that, the maximum number of the mts working on the cooperative transmission scenario is limited with fixed communication demands, or decreases with the increase of the communication demand. secondly, the results in fig. 4 are evaluated with consideration of the worst channel attenuation case, in other words, all mts are affected by the c.c. shadowing and hence the large-scale fading is approximately identical. in real cases, however, shadowing on each mt may be quite different because of the surrounding obstacles. as a result, the benefits of user cooperation aided transmission forwarding should be able to further improved. however, the mt with the best largescale fading gain may be frequently selected as a proxy, and hence the energy consumed for traffic forwarding should be larger than other mts. therefore, several issues about fairness should be considered more carefully in related works."
"in fig. 5 the smartphone deployment are shown with different reference signal receive power (rsrp) following lte standard. the rsrp information adopted in our experiments is used to imply that the distance between the service providing bs and experiment field, where small value of rsrp implies a large distance. in current indoor experiments, we used prepaid pack with 1 gb nanosim card (au 4g lte compatible) which can provide mobile virtual network operator service via kddi 1 lte network to connect with lte bs. there are totally three types of traffic forwarding scenarios adopted in the experiment, i.e., (a) non-cooperative case with two mts, (b) cooperative case with two mts, (c) same as (b) with four mts, which are demonstrated in this experiment."
"currently, object detection methods based on deep learning are mainly divided into two categories: one is based on region proposal, also known as two-stage algorithms."
"in fig. 4, some comparisons of power consumption for non-cooperative and cooperative scenarios are shown. the results are shown according to the varying number of mts with different traffic demands for each mt. the power consumption of proxy is also shown in fig. 4 as a benchmark. it should be noted that, the power consumption in cooperative case is evaluated by estimating an optimal value of ν."
"we use the composite function of the original densenet, which consists of three consecutive operations: batch normalization (bn), followed by a rectified linear nnit (relu) and a convolution (conv)."
"in addition, sme managers need to assess available it products and services that are compatible with their need and should find out existing obtainable external aid and incentives supplied by government-related agencies, advisors, vendors, and their counterparts, namely the external sources that might assist them in adopting it. smes need to consider what predictable effects could be imposed by adopting it on their business situations, customers, suppliers, competitive positions as well as their competitors. thus, smes have to consider these drivers, barriers and issues that might affect the successful adoption of it solutions. it is imperative that smes precisely evaluate their capability to reap benefits from it adoption and not to underestimate them. they should know that it is competent to act as a strategic tool to assist them in competing with their larger counterparts in the globalized market. however, it should be considered that deficient it investment decisions and imprecise it adoption strategies may imperil business survival. at the initial adoption stage, vendors should follow marketing strategies that enable smes to easily afford their products and services. likewise, they should cooperate with smes to jointly improve the compatibility of it applications with specific the characteristics of smes that are active in different industries."
"in addition, in the case of the power consumption of the proxy p proxy con, the result of proxy selection has been included and therefore it can be evaluated directly. however, in the case of the power consumption of the clients p clients con, it is a rv which is varying according to the result of the proxy selection. as a result, it is difficult to evaluate the value of the power consumption p clients con . to resolve this problem with reasonable computational complexity, an average inter-mt distanced to approximate the power consumption of clients p clients con is adopted with the assumption that shadowing between the clients and proxy follows the same distribution. then we can obtain the following result:"
"to verify the theoretical results derived in sec. iii, some simulations, which employ the time-varying frequency selective fading channel model addressed in sec. ii, are conducted in this section. the simulation parameters are summarized and listed in table 2."
"where f s (s) is the joint distribution of u correlated shadowing rvs, and s is defined as a vector consists of u shadowing rvs and is written as"
"it is straightforward to know that, the total power consumption p cell con of u mts in cellular system without user cooperation is the sum of consumed power per mt with simo transmission in cellular system. therefore, we have:"
"by replacing the backbone network vggnet with densenet-s-32-1, df-ssd improve detection accuracy by 3.8% compared with ssd300s † [cit] (73.4% vs. 69.6%), while the detection speed decrease by half. ssd300s †is a model that is trained from scratch using vggnet. it can be seen that densenet has strong feature reuse and extraction ability."
"it is worth noting that by adding feature fusion modules using concatenation, df-ssd is 3.2% higher than df-ssd without feature fusion modules (76.6% vs. 73.4%). the effectiveness of the feature fusion method propose in this paper is proved. compare with fssd300s † [cit] (72.7% map) with vggnet as the backbone network, the detection accuracy of df-ssd is 3.9% higher. fssd300s † refer to the idea of fpn and adopt concatenation for lightweight fusion of multi-scale feature maps, while df-ssd is based on densenet-s-32-1 network and integrate more context information, so the detection speed decrease."
"in this study, two types of ergodic capacities which are used in non-cooperative and cooperative transmission scenarios are considered, respectively. from the results in shannon's information theory, if the channel h are perfectly known to the bs for a large t l, the ergodic capacity of a simo channel without use of cooperative transmission can be expressed as"
"after adding coco datasets for training, the model performance of df-ssd is further improved to 81.4% map. slightly better than ssd300 * [cit] (81.2% map). ssd300 * is the latest ssd results with the new expansion data augmentation trick, which are already better than many other stateof-the-art detectors. df-ssd decreased by 0.3% compared to dsod300 * [cit] (81.7% map), due to the growth rate k adopted by dsod300 * is 48, and we set the growth rate of df-ssd to 32 for consideration of computing costs and to avoid the network becoming too wide. [cit] test set is shown in fig. 7 . it can be seen intuitively that df-ssd is slightly better than dssd321 in both detection accuracy and speed. compared with ssd300, the detection accuracy is improved while the speed is greatly decreased. the detection speed of df-ssd is faster than that of r-fcn, which has the highest detection accuracy."
"df-ssd achieves 76.5% map, which is better than ssd300 * (75.8% map). after adding coco datasets for training, our model performance is improved to 79.0% map, 1.4% higher than r-fcn (77.6% map). df-ssd shows a large improvement in testing tasks with specific backgrounds and small targets. for example, airplane (89.5% map), boat (65.8% map), chair (62.4% map), person (85.7% map), etc. this shows that df-ssd improves the weakness of ssd for small target detection to some extent, and achieves better performance for classes with specific context semantic relationship."
"most of our training strategies follow ssd, including scale and aspect ratios for default boxes, loss functions, data augmentation (e.g., randomly sample a patch and flip horizontal), and so on. it is worth noting that the latest ssd result, ssd300 * [cit], includes a random expansion data augmentation strategy that has proven very useful for detecting small targets, and we also adopt this strategy in df-ssd framework. we adopt the hard negative mining technique of ssd so that the ratio between positive and negative samples is at most 3:1, which lead to faster optimization and more stable training. other parameter settings such as learning rate and mini-batch size will be specified in the experiment section."
"organizational change is another significant influencing factor over it adoption. business growth forces smes to adopt novel and more effective technological solutions [cit] . the use of icts in small firms is the result of many internal factors such as business expansion, downsizing or relocation, and finding and capturing new markets which bring about change in organizations. owner/managers may regard it or icts as an essential tool to help managing changes [cit] . this view is supported by drew [cit], suggesting that industry changes and trends and opportunities for growth are some of the major driving forces pushing smes toward the adoption of it. since in smes the concept of business growth requires and is associated with deployment of a total quality system and professionalization processes as well, it adoption might be regarded as a rational response to these alterations from managers [cit] ."
"from the results shown above, it can be seen that, the power consumption can be significantly reduced by using user cooperation aided forwarding technique over frequency selective fading channel, and the proportion of power consumption reduction is getting larger with the increase of total communication demand. however, there are also some points are needed to be noted."
"in summary, the main contributions of this paper are listed as follows: 1) we propose an improved ssd object detection algorithm df-ssd. feature extraction network densenet-s-32-1 is designed to replace the original backbone network vgg-16 of ssd. we propose a novel feature fusion module to inject more context information into the pyramid feature layers. a residual prediction module is added for each feature layer used for detection. 2) we train our models from scratch. using less training data, our detector df-ssd achieves competitive or even better performance on common datasets (pascal voc and ms coco) than other state-ofthe-art pre-training models. 3) df-ssd has advanced detection effects for small objects and objects with specific relationships. it requires only 1/2 parameters to ssd and 1/9 parameters to faster rcnn, which shows great application potential for resource-limited scenarios."
"in the original design of densenet, each transition layer contains an average pooling operation to down-sample the feature maps. the number of dense blocks is fixed (4 dense blocks in all densenet architectures) if one wants to maintain the same scale of outputs. the only way to increase network depth is to add layers to each of the dense blocks of the original densenet [cit] . therefore, we also use the transition w/o pooling layer to eliminate the layer limitation of each of the dense blocks of the df-ssd network structure, which makes the network deeper and has stronger feature extraction capability, and the final feature map resolution is not reduced."
"on the other hand, employee (as the users of it) satisfaction with it is another dimension of it adoption success in smes [cit] . contrary to technological acceptance literature focusing on individual behavior and beliefs, system and information characteristics have been regarded as core concepts in user satisfaction literature [cit] . adam [cit] argue that end-user information satisfaction is strongly affected by perceived benefit and expectations characteristics, user background and involvement, and organizational support and encouragement, as well as by subcomponents of these three factors. on the other hand, palvia [cit], and palvia and palvia [cit] found that the unique characteristics of smes such as specific computing environment, mandatory environment, resource constraints, and generalist employees (employees and managers of smes as the users of it are inclined to act as specialists in various aspects of is, while not being very well qualified or expert in different it roles) are specific attributes of user satisfaction with it in smes. in this regard, palvia [cit] formulated the sbusit model to evaluate it impact over smes, based on it user satisfaction measures. subsequently, this model was developed by palvia and palvia [cit] through adding business related factors and owner/manager characteristics as two other determinants of users' satisfaction with it in smes."
"where p t denotes transmission power, r represents the rayleigh fading matrix which has the same form as h with a replacement of h(l, t) to r(l, t). the r(l, t) can be defined as"
"it is should be noted that, because only operator has the permissions to adjust the parameters of bs including working frequency and bandwidth, the proposed bandwidth allocation strategy cannot be directly adopted in the current experiments. however, considering that the bandwidth setting of the test-bed can be seen as a simplified version of the proposal with a fixed ν where νb is allocated to lte link and (1 − ν)b is allocated to wi-fi and bluetooth links, regardless the optimization issue, our experimental results still can be applied on general cases."
"specifically, with machine learning approaches, bs can further utilize these previous information, i.e., proxy and ν op obtained by the existing algorithms, to train or update anns which are used to predict the selection of proxy and ν op without the existing algorithms. once stable anns are established at bs side, the networks thereafter can be distributed to each mt by operator updates for instance. then mts use these trained or updated networks to select proxy and calculate ν op (surely, error should be allowed). finally, it is not necessary to upload mt's information to the bs and mts can select a proxy (may not be accurate) and use the predicted ν op to perform the cooperative traffic forwarding. that can result in reduction of system complexity and improvement of communication efficiency."
"the other is the one-stage algorithms based on regression. the object detection algorithm based on region proposal divides the detection problem into two stages. first, region proposals are extracted from the input images according to the region selection algorithm (such as selective search [cit], edge box [cit], etc.). second, region proposals are classified and position adjusted to output the target detection results. although this kind of algorithm has high accuracy, its detection speed is slow, which is difficult to meet the real-time requirements of some scenes. typical representatives include r-cnn [cit], faster r-cnn, and r-fcn [cit] . however, the regression-based object detection algorithm has more advantages in terms of speed. for a given input image, it needs to be processed only once, and the target border and category of this position can be regressed in multiple positions of the image. typical representatives are yolo and ssd. yolo mainly classifies and locates objects of different scales on the single scale feature layer, which makes the burden of the feature layer too large. ssd extracts default boxes on multiple feature layers of different scales to complete detection and location of targets of different sizes. it can achieve 74.3% [cit] datasets with a processing speed of 46 fps. both the map indicator and the detection speed have higher performance. volume 8, 2020"
"we train our models from scratch on these common datasets with 0.9 momentum, 0.0005 weight decay, and initial learning rate 0.1. the learning rate decay policy is slightly different for each dataset, and we will describe details later. the batch size we used is 128, which beyond the gpu memory capacity. therefore, we refer to the training trick of dsod implemented on caffe platform to overcome gpu memory constraints by accumulating gradients over two training iterations. all conv-layers are initialized with the ''xavier'' method [cit] . model detection performance is mainly evaluated with mean average precision (map). other indicators such as frame per second (fps) and parameters will also help us further evaluate model performance."
"desire for growth is another characteristic of ceos that deserves our attention as an important influencing factor over adoption of it. lybaert [cit] discusses that firm's size is positively related to the decision to accumulate additional information, and growth of a firm is coupled with the gathering of additional information. they found that an sme's owner/manager, who makes most of the critical decisions and allows the firm to grow, uses more information when he/she possesses a greater desire for growth. moreover, familiarity with administration is other important ceo-related determinant which influences the use of information and is within smes. lybaert [cit] showed that in comparison with ceos not possessing knowledge of administration, ceos with high familiarity with administration will use more information and subsequently it."
"since the ergodic capacity is not affected by the time variation for ergodic fading channels, the theorem can be proved by considering the slow time-varying scenario. the rayleigh fading matrix r in (6) can be viewed as a block toeplitz matrix with the following expression r :"
"it is well known that, because single carrier used communication has low peak-to-average power ratio and thus each mt can get great benefit in terms of the reduced cost on the power amplifier and the transmit power efficiency, such kind of communication technique has been adopted and applied as an appropriate access scheme in uplink cellular systems. by using the user cooperation aided forwarding approach, when compared with non-cooperative schemes and in order to deliver the total communication demands which are collected from other mts, the proxy tends to be allocated more bandwidth or spectrum resource and may undergo frequency selective fading. as a result, proxy's transmission performance is worse than we expected. in our survey, there is no previous work discussing about this issue, which is very important and may affect the system performance significantly."
"despite sbusit and its developed version addressing user satisfaction in small businesses in connection with their particular characteristics, this model has excluded user involvement as a determinant of user satisfaction in small business. user involvement in is/it development has been largely considered as a significant mechanism leading to successful implementation of a new system [cit], specifically in smes [cit] . in general, a firm initiating a change to a new information and computer system may causes concern over job security, causing employees to worry about the adoption outcomes, such as the threat of losing jobs [cit] . as previously mentioned, some employees may not believe that it will result in positive progress or improvement of organization functionality [cit] . thus, there should be assurance by owner/managers that employees are aware and have an understanding of the effects of changes to a new computer system on the organization [cit] . moreover, managers should keep employees aware that new is/it will enable them to make the best use of resources that can help them be more productive [cit] . in such circumstances, involving employees as a part of new projects and systems will make them believe that as members of a family, team or organization, they are very important to and responsible for the new projects' success in the organization. hence, involving employees in the adoption process will result in higher levels of success [cit] suggest that this user involvement should be initiated from the commencement of it project feasibility studies, continue throughout the design phase and must be maintained during the deployment and testing stages."
"our multi-scale fusion module takes full advantage of the relationship between different feature layers. on the one hand, atrous convolution is used to fuse the low-level feature maps and the high-level feature maps. this can significantly improve the receptive field range of the classification network, which is beneficial to the model to learn more global information. on the other hand, the semantic information of the high-level feature maps is integrated into the low-level feature maps by using deconvolution, which is conducive to the small target detection of the low-level feature maps and enhances the semantic representation ability of the model. this multi-scale fusion module enables the front-end prediction network to take into account the different scales of objects and enhance the generalization ability of the model."
"smes have generally been distinguished by and are suffering from their restricted access to particular resources compared with large organizations [cit] . the literature on it adoption suggest that, due to the smes' unique characteristics, their financial resources, technical and managerial resources, information resources accessibility, internal and external expertise, market accessibility and in-house it knowledge and experience can hinder or simplify the adoption of it in smes, and positively or negatively affect this process as well [cit] ."
"in smes, culture is highly affected by owner/manager attitude, perceptions and characteristics [cit] . thus, it is imperative for managers to know that employees' usage of ict might be affected by supervisors' (managers at different levels) behavior toward work and it [cit] . moreover it conflict with organizational culture can result in user resistance to it adoption [cit] . as stated previously, openness to change is an important characteristic of organizational culture [cit] . thus, and given that it deployment will often bring about significant change in smes, some researchers suggested that smes possessing an adaptable and flexible organizational culture with higher levels of openness to change will be more inclined and prepared to accept it-related changes that might result in it project success [cit] . from the other point of view, it was suggested that the examination of sme culture should also be addressed through studying organizational learning and the learning organization pattern [cit], since interrelationship between sme culture and learning processes might result in the enhancement of a firm's competitive capacity [cit] suggested that knowledge required for the adoption of complex it projects which is vastly distributed needs to be integrated within the organization through a process of networking and knowledge sharing, while the effectiveness of this process is rooted in the culture of smes. moreover, the adoption process requires the integration of internal and external knowledge within the firm."
"as the frequency selective channel attenuations are considered, it is necessary to analyzed the channel capacity based on a block of t output symbols at receiver (or bs in this case). the simo channel with isi can be expressed as"
"finally, our df-ssd model is evaluated on the ms coco datasets. ms coco detection task contains a total of 80 categories, and the data distribution includes 80k training images, 40k validation images and 20k testing images (test-dev set). [cit] evaluation server. compared with pascal voc, there are more small targets in coco datasets, more objects in a single image, and most objects are not center-distributed, which is more consistent with the daily environment. therefore, coco detection task is more difficult. for the first 60k iterations, we set the initial learning rate to 0.1 and divide it by 10 after each 40k iterations. the training is completed when the number of iterations reaches 300k. [cit] are shown in table 5."
"several factors, including management's perception of and attitude on it, support and commitment, it knowledge and experiences, innovativeness, perceived behavioral control over it, desire for growth and familiarity with administration directly affect the process of it adoption in smes [cit] . accordingly, the characteristics of ceos should be taken into consideration in the investigation of strategic activities, such as the adoption of innovation, including it as a new technology [cit] . small businesses that have adopted it are more likely to have ceos who possess better positive attitudes in it adoption [cit] . this view is reinforced by caldeira and ward [cit] who found that the positive attitude of top management has resulted in relative success of is/it adoption in smes, especially in manufacturing industry. in addition, it is argued that a greater intention to adopt it solutions is directly attributable to the more positive attitude of small and minority business owners in it adoption [cit] . consequently, if the ceo perceives that benefits of it adoption outweigh its risks, then the business is more likely to adopt it [cit] . likewise, when management has been highly willing to implement it application, smes do not perceive management priority on it as a major barrier in adopting is applications [cit] . in this regard, a positive attitude by top management in using it (as the users of it in smes) will result in it acceptance and subsequently success in smes [cit] ."
"one of the design principles of dsod is deep supervision, whose fundamental idea is to provide an integrated objective function, that is, the loss function should provide supervisory signals to both the output layer and the non-output layer. both resnet [cit] and densenet [cit] have skip-connection, which satisfy this requirement. we design the feature extraction network densenet-s-32-1 based on the idea of densenet. on the one hand, it satisfies the condition of depth monitoring. on the other hand, it greatly reduces the number of model parameters and improves the feature extraction capability of the backbone network."
"the atrous convolution operation is equivalent to inserting spaces between the convolution kernel elements, where a new super parameter dilation is introduced, and the value of (dilation − 1) is the number of spaces inserted. assuming that the original convolution kernel size is k, then the new kernel size n after inserting (dilation − 1) spaces is:"
"although the input image size of ssd300 * is smaller than that of faster r-cnn and ion [cit], the detection performance of ssd300 * is better. after a series of improvements to the original ssd model, df-ssd achieves 29.5/50.7%, superior to ssd300 * and dsod300, and close to r-fcn. we observe that our [0.5:0.95] result is 0.3% higher than r-fcn, but our result (50.7% map) with 0.5 iou is lower than r-fcn (51.5% map). this indicates that the object position prediction of df-ssd is more accurate than that of r-fcn under larger overlap settings."
"on the other hand, and comparing with large organization, it has been acknowledged that smes are suffering from a lack of in-house it expertise which might negatively affect the process of it adoption [cit] . as a result, smes are facing significant risks and problems with their computerization regarding their inadequate knowledge of is/it implementation [cit] . cragg and zinatelli [cit] conducted a longitudinal study over an eight-year period of is sophistication and evolution in eighteen small firms and demonstrated that evolution and sophistication of is within small firms will be drastically inhibited when small enterprises suffer a lack of internal expertise. this view is supported through a study by caldeira and ward [cit] who revealed that internal expertise consisting of employees, supervisors, or those from top management teams are powerful determinants of it adoption and success. in addition, knowledge of it is another key resource influencing it adoption in smes. development of internal is/it knowledge and skills is one of the most important bases required for providing superior levels of is/it adoption and satisfaction in smes [cit] . in general, the lack of it knowledge in smes can be regarded as a barrier to it adoption since ceos of smes might be bewildered by swift development of it tools and a countless variety of choices [cit] ."
"according to the discussion above, in this study, we propose a simply machine learnable bandwidth allocation strategy for user cooperative traffic forwarding, and evaluate the power consumption of the systems by both theoretical and experimental methods. in order to fit the present study to the real transmission environment closely, in this study, we employ a generalized channel attenuation model in which frequency selective fading channel and effect of spatially correlated shadowing are considered. besides, in order to analyze the user cooperation aided forwarding scenario, we also introduce a recent lte smartphone based power consumption model, which can make the results more convincing. finally, a smartphone test-bed based experiment is also conducted to conclude our study."
"on the other hand, time-permanency is another characteristic of it tools which might affect the it adoption process in smes. salmeron and bueno [cit] stressed the positive role of a time-permanency factor in isomorphic application of it tools is manifested within smes: in particular between those belonging to the same industry. a possible explanation for this isomorphism is that most sme managers are interested in old, safe and vastly applied it solutions which have a lower risk of failure."
"for instance, in fig. 9, we show an illustration of machine learning aided proxy selection and ν op prediction with the use of artificial neural networks 2 (anns) for the proposed cooperative traffic forwarding. by averaging the effects of small-scale fading, traditionally, each mt uploads its position and traffic demand to the bs, and the bs uses existing algorithms to select proxy and calculate ν op with the help of these uploaded information. then the results (proxy and ν op ) are fed backed to each mt and bs can performs follow-up operations."
"from table 2 (rows 2 and 3), we can see that compared with the case without transition w/o pooling layer (64.3% map), the case with this design layer (67.2% map) brings about a performance improvement of 2.9% map, so as to verify the effectiveness of this layer. that is, increasing the number of dense blocks without decreasing the resolution of the final feature maps."
"the second aspect is some classes with different contexts. compared to ssd, df-ssd can capture scene contexts. in fig. 8 (b), we can see that the results of classes with specific relationships can be improved: baseball player and baseball bat, skateboard and jumping people, men in suit and tie, and football and football player, etc. in fig. 8, we can observe all the objects that benefit from the feature fusion module of df-ssd detector."
"certainly, there are still other aspects of problems which can be solved by machine learning approaches, and in the current study we provide a design framework which can introduce the machine learning related approaches to solve the problems. more related studies for cooperative communications are planned in our future works."
"within this study and as suggested in figure 1, influencing factors are categorized into two major clusters of factors and their subcategories: internal and external factors. in addition, a brief review and categorizations of factors influencing it adoption in smes have been offered in table 1 : factors that are merely sme-related. internal factors are defined as factors within the technological context and organizational context of smes. technological context describes the internal technologies relevant to the firm. organizational context refers to descriptive measures regarding the organization, such as firm size and scope, managerial structure and internal resources. external factors, however, refer to the factors within the environmental context that describe the arena in which a firm conducts its business: its industry, competitors and dealings with government [cit] ."
"in fig. 2, the numerical results of two types of capacities evaluated by expressions (9) and (10) are shown. the results of arbitrary simo channel are evaluated by (9), and the results of simo channel using the best channel condition are obtained by (10) . in both cases the results are evaluated with different correlations for shadowing. from the simulation results, it can be seen that, the theoretical expressions derived in sec. iii match the numerical simulation results quite well, which validates the effectiveness of the theoretical results and convinces that the theoretical expressions can be used in the evaluation of power consumption. however, because of the large computational cost for numerical simulations, since the theoretical results can well approximate the numerical simulation results, we show the theoretical results only without numerical simulation results in the following figures to save the computational resources."
"in term of is success, thong [cit] demonstrated that user involvement in is implementation is one of the most important factors for successful is implementation and user information satisfaction. if end users could be encouraged to become involved with is/it implementation through having time off from routine responsibilities, several advantages would be achieved. these advantages include a better fit of it to users' expectations, ease in using it applications due to achieved it knowledge and learning experience during the design phase, a strong sensation of ownership and decreased resistance to change [cit] . accordingly, these factors could increase the probability of successful it implementation as well [cit] ."
"in the above subsection, two types of ergodic capacities which are corresponding to, (a) non-cooperative transmission scenario, (b) cooperative transmission scenario with the use of the best channel selection under the channel effects of frequency selective channels, are derived. because it is difficult to calculate transmit power p t from these complicated mathematical expressions of capacity, here we try to approximate the expressions of transmit power for practical use."
"we train our models from scratch. our df-ssd detector is only trained with 16,551 [cit], but achieves competitive or even better performance than those models trained with 1.2 million + 16,551 images. we also acknowledge that given a modest assumption of unlimited training data and computing power, deep neural networks should perform very well. but as datasets get larger, training deep neural networks will become more expensive. moreover, most of the pre-trained models may have huge domain differences from the pre-trained mode domain to the target domain. it is very difficult to apply the pre-trained model on imagenet to medical images, multi-spectral images and other fields."
"in this review, it will cover a wide range of information processing and computer applications in organizations, since various definitions of it have been employed widely by different researchers. therefore, it will cover information system (is), information and communication technology (ict) and the internet, as well as and their infrastructure, including computer hardware and software, and those technologies that process or transmit information to enhance the effectiveness of individuals and organizations. a variety of perspectives on factors that affect the it adoption process is available in a huge body of literature. the review of previous research has identified a number of influencing factors. most of these perspectives and studies have concentrated on influencing factors such as top management, organizational behavior and characteristics, firms' resources, government, customers, supplier and external it consultant and vendors. from a macro-perspective, and based on the review of the existing literature on it adoption in smes, an integrated framework has been developed and used to classify various issues and factors related to the process of is/it adoption within smes (figure 1 ). this framework merely comprises different aspects of internal and external it adoption factors (drivers, influencing factors and barriers) and does not categorize adoption factors based on being drivers or barriers of it adoption in smes. likewise, the authors will address different concepts of it adoption (also considered as different it adoption phases) suggested by prior literature, such as decisions to accept and use innovation [cit], the full use of innovation as intended by the designer [cit], implementation success [cit], extent of usage [cit] and the effectiveness and success of adopted it based on acceptance of, or satisfaction with, it [cit] ."
"where η rx and η tx are binary variables representing whether the mt is receiving or transmitting. the constants p on, p rx, and p tx are the consumption power of the cellular network, the receiver, and the transmitter, respectively. p rxbb, p txbb, p rxrf, and p txrf are liner functions of rate and power which are listed in the table 1 [cit] . in the following, we evaluate consumed power of the mts for different scenarios."
"for many firms, pressure to keep up with the competition, providing a means to enhance survival and/or growth, managing change, promoting services to customers and staying competitive and/or enhancing innovation abilities have forced them to adopt it [cit] . prior literature suggests that as small businesses are susceptible to customer pressure, these firms adopted it as a result of demand from customers to develop the efficiency of their inter-organizational dealings [cit] . hence, it has become an indispensable strategy for firms to have these technologies [cit], while others suggested that the main driving forces to move toward it tools adoption in smes are internal factors., including industry changes and trends, maintaining current market, finding new markets, opportunities for growth and the necessity to keep up with competition [cit] ."
"it should be noted that, the sum of transmit power and channel gain in dbm is the average receive power and it can be mathematically calculated by"
"owing to the importance of external assistance to smes, these business are facing difficulties as it vendors often restrict their marketing to larger organizations and generally do not understand smes' unique needs [cit] . consequently, if powerful technology suppliers develop their marketing strategies and become more aware of issues, including quality, training provision and maintenance regarding smes' needs, this will encourage smes to implement it to improve their performance [cit] . in general, the duties undertaken by external expertise comprise is project management, encouraging employees to accept new systems and overcome their fear of new technology, fulfilling information requirements analysis of business needs, is user training and recommending suitable computer hardware and software [cit] . these external consultants act as intermediaries to compensate for the absence of it knowledge in smes and diminish the is knowledge barrier to successful and effective is/it implementation [cit] ."
"in this study, we proposed a simple and novel machine learnable bandwidth allocation strategy for user cooperation aided wireless communication systems, and evaluated the power consumption of the systems using theoretical and experimental methods in cellular network systems under the environment with frequency selective fading channels and spatially correlated shadowing. with the adoption of the proposed bandwidth allocation strategy, first of all we derived some theoretical expressions to evaluate the transmission power for non-cooperative and cooperative transmission scenarios, and then we mathematically analyzed the power consumption with the help of a recent lte power model for smartphones, and finally evaluated the concluded results by our smartphone test-bed. from the numerical and experimental results, it can be seen that, the benefits of cooperative forwarding are significant. however, according to our analysis, it is difficult to be fully achieved in real environment because of some limitations, for example, maximum transmit power on mt. furthermore, allocation of bandwidth resource dominated the transmission performances of user cooperation aided transmission forwarding and is strongly affected by, e.g., position and traffic demand of each mt. machine learning approaches aided cooperative traffic forwarding can be used to reduce the considered system complexity and improve transmission efficiency, which is also discussed in this study and left as one of the future topics."
"the m0bni microarray lab in university of michigan has provided a gputools [cit] package for r which constitutes a handful of common statistical algorithms that are used in the biomedical research community. another work in this area is the rgpu package which enables parallel evaluation of linear algebra expressions, as well as access to some of the function provided in cuda sdk [cit] . the magma [cit] package provides an interface to the hybrid matrix algebra on gpu and multicore architectures implementation."
"the framework presented in the paper spans across different domains to harness their capabilities in an attempt to provide a scalable system for using data mining applications for knowledge discovery. this section gives a detailed description of the different components used in the framework. the front end of the framework is the widely used r-statistical tool. mpi is used for communication between the cluster nodes and parallel i/o is achieved using mpi-io and parallelnetcdf [cit] interface. the computation intensive tasks are handled by multi-core cpus and multithreaded gpus [cit] . figure 1 shows the different components of the system. figure 2 shows the dataflow in our framework. the framework is launched on all the nodes in a master-slave configuration. the application is written as an r script. the script calls the highperformance i/o interface to read the data from platform independent netcdf [cit] file in parallel. once the data is read, the script invokes highperformance kernels, through an efficient r-c interface, to analyze the data. the mpi communication enables the nodes to interact with each other. the results, which are communicated to the r environment, can take advantage of the rich analysis and/or visualization tools available in r. the programming model consists of - (1) a programming infrastructure and (2) a library of high performance kernels. the former provides tools and methodologies for developing scalable applications while the latter provides a collection of commonly used data mining kernels to accelerate application development."
"besides the above mentioned features, the proposed framework outlines a new optimization technique aimed for gpu architectures. this technique involves interleaving kernels from different applications to improve their throughput. the optimization relies on the domain specific knowledge that it is not always known apripori, what is the best algorithm to mine raw data for useful information. in such situations the data is explored using multiple algorithms. since all the algorithms work on the same dataset, they can run in close coordination to improve the overall performance. overall, the major contributions of the paper are as follows: 1) a scalable framework for writing high performance applications on heterogeneous platforms. 2) a high performance library of commonly used kernels for data exploration. 3) an interface to parallel i/o functionality 4) various optimizations to increase the throughput of applications"
"besides the above mentioned kernels, our framework provides a number of optimizations which can help increase the speedup of the applications. we present a couple of optimizations here. notice that these optimizations are currently specific to the hardware but as new hardware devices are introduced, new optimizations for that particular hardware are easy to integrate in the current system. 1) hybrid implementation: hybrid implementation refers to harnessing the capabilities of both gpus and cpus simultaneously. consider a situation when we have a gpu and a multi-core cpu in the system. it would be desirable to distribute the tasks between the gpu and the cpu cores. since the computational power of gpu is significantly higher than that of the cpus, the data need to be distributed such that the work remains balanced. our framework provides the functionality to run an application in the hybrid mode. in this mode, the data will be distributed among the nodes and the corresponding cpu or gpu kernels will be launched. figure 5 shows how a hybrid kernel call gets broken down into architecture specific kernel calls using the framework."
"the programming infrastructure provides a software platform which presents different methods for managing the various components with a view of scalable implementation and a high performance scripting interface for an easy-to-use front end to write various applications. 1) software platform: as mentioned above, we have four major components in our framework: the front end (or the scripting interface), the back end (managed by c/c++/cuda), communication (which is mpi) and the i/o. broadly defined, there are two different implementation methods for gluing these components to have a scalable platform while keeping it flexible enough to incorporate different kernels. these methods are described in the following. the first implementation, which is referred to as c-level parallelism (c-lp), is shown by dotted arrows in figure 3 . in this, the mpi communication is not visible in the r environment. each of the nodes running r call the corresponding c interface functions and all the mpi calls are handled at the c level. the other way of implementation, called r-level parallelism (r-lp), in which the mpi communication is visible at the r environment, is shown by solid arrows in figure 3 . the r nodes call the c interfaced kernels and the communication among the nodes is handled in r. notice that the c kernels are serial as opposed to mpi-enabled kernels in c-lp."
"3) communication + i/o: in our framework the communication among the nodes of a cluster is handled by mpi though the rmpi package. however, besides sharing the data during computations, large amounts of data need to be accessed from storage devices. absence of a parallel i/o interface will severely affect any performance gain achieved using multi-core multi-threaded kernels. we, therefore, enhace the capabilities of rmpi package to provide mpi-io interface to r for parallel read/write capability. further, parallelnetcdf is built on top of mpi-io. we have implemented a parallel-netcdf interface for r which figure 4 : kernels for large data using cuda streams provides the capability of reading/writing netcdf file format to all the nodes in the r-cluster. table i gives a list of interface functions for parallel read/write."
"to provide parallel-netcdf functionality pnetcdf library version 1.2 is used. the gpu kernels are compiled using the cuda compiler driver, nvcc, release 2.0. the entire software framework is compiled using gcc version 4.4.2. figure 7 shows the performance of clustering algorithms like k-means and fuzzy k-means for 20 clusters and different sizes of the input data set ranging from 10k to 1 million records. we notice that as the data size is increased, there is an initial improvement in the speedup, which eventually saturates at around 40 for k-means and 70 for fuzzy k-means. the performance difference between the two algorithms occurs because of the computationally intensive membership calculation resulting in higher speedups for larger workloads. due to limited space we cannot provide performance charts for other applications. however, for basic statistical kernels we obtain a speedup of up to 30x and for pca the speedups achieved are of the order of 35x when compared to a single cpu implementation. figure 8 shows the performance results for larger datasets when the entire dataset does not fit into the gpu device memory. data is divided into smaller tiles of size 768k records and the number of cuda streams used is 4. we notice that the speedup in this case is somewhat smaller as compared to k-means in figure 7 . this can be attributed to two reasons. first, since the data transfer time (between cpu and gpu) is not negligible, copying data for all iterations incurs extra overhead lowering the performance gains. second, since the kernels take less time compared to the memory transfer, this overhead cannot be eliminated even using cuda streams. hence, the speedup saturates around 25 compared to a figure 7 ."
"the rmpi [cit] package provides an interface from r to mpi. the snow [cit] package runs on top of rmpi (or directly via sockets), allowing the programmer to express the parallel disposition of work more conveniently. rdsm package [cit] gives the r programmer a shared memory view, but the objects are not physically shared. instead, they are stored in a server, and accessed through network sockets, thus enabling a threads-like view. parallel-r [cit] and pr [cit] enable the statistical analysis routines available in r to be deployed on high performance architecture."
"we have implemented the kernels both for cpu and gpu. cpu kernels are used for hybrid-execution on a heterogeneous cluster comprising of gpus and cpus. for cpu, some kernels are already available in r. the implementation is done keeping in view that the kernels can easily scale in a cluster environment. for the gpu implementations, the input data is first shipped to the gpu device memory and then kernels are launched which process the input data in the device memory. the results are subsequently shipped back to the host (cpu) memory. table i shows a list of kernels and their corresponding interface functions for r."
"2) high-performance r: the programming infrastructure of our framework includes a highperformance scripting language capable of being used in a distributed computing environment. this scripting language interface is based on the widely used statistical tool r. however, since r is not good for heavy lifting, an interface to high level languages like c/c++/fortran, known for their computational capabilities, is provided. furthermore, since all the accelerator/coprocessors have an interface to high-level languages an efficient r-c interface is necessary for true high performance scripting capabilities. r serves as a frontend interface to the user. compiled c functions can be invoked in the r environment using .c or .call interface functions. with .c, the r objects are copied to c data structure before being passed to the c code, and copied again to a r list object when the compiled code returns. on the contrary .call does not copy arguments. since data mining algorithms process huge amounts of data, copying of arguments can severely hamper the performance of applications. our framework uses .call function to provide the c interface to r. also, we havent noticed any degradation in the execution of the c functions using .call interface. besides lower amount of copied data, other advantages of using .call function include:"
"knowledge driven decisions are a key to success in today's world. business corporations, financial institutions, government departments, research and development organizations collect huge amounts of data with a view to gain a deeper insight in their respective fields. social networks such as facebook and micro-blogging website twitter generate enormous amounts of data which can provide useful information about the latest trends in the society. sifting through such vast collection of data and discovering unknown patterns is not a trivial task, especially when the data sizes are of the order of exabytes and petabytes. data mining presents a pool of automated analysis techniques which can discover hidden knowledge and predict new trends and behaviors."
we evaluate the performance of the scalability infrastructure provided by our framework by scaling the above applications on a homogeneous and heterogeneous cluster of machines. our heterogeneous environment consists of gpus and cpus. figure 10 shows the execution times for different ratios of data distribution between the cpus and gpus for k-means clustering algorithm. the hybrid middleware invokes cpu kernels and gpu kernels optimized for large datasets for distance computation and cluster update. we vary the data distribution ratio from 20 to 34 and notice that we achieve the best performance around the ratio of 29. this heterogeneous implementation results in a performance gain of around 9% compared to gpu-only implementation. figure 11 shows the scalability of k-means algorithm on a homogeneous cluster of gpus using our framework. the framework achieves 7.2x speedup when the number of gpus increases from 1 to 8.
"2) kernel optimization for gpus: the above mentioned kernels work well when the input data fits entirely into the gpu device memory. however, since data mining deals with huge amounts of data, typically, the entire data will not fit into the gpu device memory. transferring data to and from the gpu device will result in significant performance degradation. this requires out-of-core implementation using cuda streams. the framework, however, uses the multi-threaded kernels (mentioned above) and schedules them to overlap with host/device data transfers. this limits the need of developing new out-of-core kernels. figure 4 shows an example where the input dataset is divided into smaller tiles and assigned to two different streams. each data transfer on stream 1 is overlapped with a kernel execution on stream 2 resulting in reduced overhead."
"in this paper we have presented a scalable framework for developing and using data mining algorithms. our framework spans across different technologies to harness their capabilities. data mining techniques require heavy computations and deal with huge amounts of data. to provide a scalable environment we have provided an efficient interface to handle compute-intensive tasks as well as a parallel i/o interface for optimized read/write of the data. we have further described optimizations which can be easily implemented using the framework. we introduce the concept of multiple kernel optimizations, which runs kernels from different applications for each data transfer. we also present a middleware for heterogeneous computations enabling both gpus and cpus work together. in future, other architectures and more data mining applications can also be easily integrated. our framework provides the flexibility of integrating newer kernels and optimizations easily in the framework. it provides a library of (highly optimized) high performance kernels which are commonly used in data mining algorithms. the results show that we can achieve significant speedup with our optimizations. we also present, through case studies, how the framework can be used to write scalable applications."
"in this section, we evaluate the performance of the applications developed using the framework as well as various proposed optimizations. on the hardware side, we have a cluster of 4 nodes. the host cpu on each node is an intel quad core 2.4 ghz processor with 4 gb of main memory. the co-processor on each node is nvidias geforce 8800gt graphics processing unit with 112 processing cores and 512 mb of device memory. on the gpu, the grid of thread blocks can have a maximum of 65535 blocks on each dimension, with a maximum of 512 threads per block. each multiprocessor has 16 kb of shared memory and can run 8 thread blocks concurrently. each node has two of these gpus. the software setup includes r version 2.8.0. mpich2 version 1.2.1 is used for providing the mpi communication."
"previous section has given a detailed description of the different components that make our framework. in this section, we present how applications can be written using all these components. we divide this section into three subsections discussing about the implementation of algorithms using the kernels, the optimizations offered by the framework, and how to scale the applications to a cluster of nodes. notice that application development is done on the front end in r script and the kernels and i/o functions are called only when necessary."
"exploring hidden patterns and trends require a collection of data mining techniques. tools such as clementine [cit] and weka [cit] provide a rich collection of algorithms. however, they lack the capability to utilize the benefits of co-processors and do not have scalable i/o capabilities. this limits their usability as a high performance data analytics tool. this paper describes a scalable framework for developing parallel applications on a heterogeneous computational backbone. it incorporates a library of compute-intensive kernels and explores performance optimization techniques to increase the throughput of applications. in our framework, an application is written as a script which is composed of modules (e.g., commonly used kernels). the framework provides a middleware which deploys these modules on a cluster of heterogeneous hardware platforms. further, processing huge amounts of data requires reading and writing to storage devices, like disk drives, ssds etc. i/o presents a significant bottleneck in the overall performance of data mining applications as a poor read/write interface can hinder any benefit obtained from parallel architectures. to alleviate this problem, our framework incorporates a parallel i/o interface. thus, the framework discussed in this paper provides parallelism both for i/o and computations while still being simple and flexible."
"using the framework, we have developed different data mining algorithms. due to limited space we give only a brief description of three of them: k-means [cit], fuzzy k-means [cit] and pca [cit] . k-means is a widely used clustering algorithm which attempts to find k partitions of the input dataset by minimizing the squared error within each partition. the kmeans algorithm can be implemented using the distance computation, cluster update and histogram kernels as mentioned in table i . a variation of k-means algorithm called bisection kmeans can also be implemented similarly. fuzzy k-means is a superset of k-means algorithm with the distinction that it allows each record in the data set to have a degree of membership to each partition. principal component analysis (pca) aims at finding the principal components which are representative of the input dataset and can be implemented using eigenvalue and eigenvector kernels."
"analyzing large quantities of data requires computational resources. recent times have seen the emergence of many high performance architectures like gpgpus, cell, multi-cores, fpgas, etc., each presenting its own unique benefits. the paradigm of homogenous computing, where all the nodes have the same architecture, is transforming itself to heterogeneous computing, where each task is allocated to the architecture that suits its properties best. since data mining kernels are characterized as being computationally intensive, the new generation of architectures can provide a significant boost to their performance. furthermore, storing and retrieving large quantities of data adds to the complexity of data mining applications."
"in figure 9, we show the performance gain achieved by interleaving different kernels. as an example, we consider k-means with different number of clusters (which is a parameter for kmeans) as different applications. the results show the speedups obtained for 2 to 7 applications relative to a single application for data sizes ranging from 4 million to 20 million records. we notice that as more kernels are interleaved for the same amount of data transferred to the device memory, the speedup increases. the speedup increases from 1.6x for two applications and saturates around 2x as the number of applications are increased beyond 4. it should be noted that the speedup of 2x w.r.t. a single application amounts to an overall performance gain of 50x when compared against a single threaded cpu implementation. the saturation is attributed to the fact that memory copy time is completely hidden by the kernel execution time and any further addition of kernels will not result in a performance gain."
the paper is organized as follows. section ii presents the related work. section iii presents the implementation overview of our framework. section iv describes how applications can be written for the framework. section v presents a discussion of the results. we conclude the paper in section vi with directions to future work.
"the second component of our frameworks programming model is the library of optimized and table i : kernels for large data using cuda streams high performance kernels which can be embedded in r scripts. the library provides a collection of commonly used data mining kernels implemented for different architectures. apart from this, intranode and inter-node optimizations are also included. to keep the development process simple and flexible, we follow the r-lp approach (refer to figure 3 ). decoupling the high performance kernels from applications gives us the opportunity to develop new applications. furthermore, kernels implemented on different architectures enable us to explore the design space to achieve the best performance."
"both the implementations have pros and cons. r-lp has a higher overhead in sharing data among the nodes as the data needs to come to the r environment and then communicated to other nodes before finally filtering down to the c environment, as opposed to c-lp where the data can be shared at the same level i.e., across the c environment as shown in figure 3 . secondly, c applications/kernels which are already written using mpi paradigm can be directly interfaced to the r environment with no or little modifications. on the contrary, r-lp requires application to be written as r script. the limitation faced by c-lp is that it requires all the development to be done within rmpi package, i.e., all the code needs to be compiled with the rmpi code. the reason lays in the fact that mpi initialization can be done only once for whole system which makes it impossible for packages not compiled within rmi to use mpi function calls. r-lp is more flexible in this regard as high performance library packages can be developed independent of the rmpi package. both these approaches have been followed in the framework for different components. as discussed in later sections, parallel i/o interface is built upon c-lp, while the kernel libraries and application development follow the r-lp methodology."
"2) multiple kernel optimization: this optimization is specific to the cuda implementation. we notice that data mining kernels process huge amounts of data and it is not always possible to fit the entire data in the gpu device memory. as mentioned in section iii-b2, this will require the usage of cuda streams to lower the overhead caused by copying data from host memory to the device memory. we further notice in our experiments that kernel execution time is smaller figure 6 : concept of interleaved kernel optimization than the time it takes to copy smaller tiles of data to the gpu device memory. this presents us a unique opportunity to leverage the time difference. in practical situations, a number of different data mining algorithms are used on a given dataset. we propose to run kernels from different applications on the dataset while it is in the device memory so as to reduce the overhead of memory copy as much as possible. figure 6 shows the idea behind this optimization. as an example, three different applications app1, app2, and app3 are shown in the figure. for each memory transfer call put on a cuda stream, one kernel call from each of the applications (1.a, 2.a, and 3.a) is allocated on that particular stream as shown. this can be viewed as a single kernel whose execution time is close to the combined execution time of the same kernels running separately. the kernel execution and host-device memory copy times can be used to predict the number of applications which can be interleaved in the above fashion."
where the solution for j p (1) is obtained using ridgeregression 2 . notice that the above optimization problem is formulated in p. as our experiments have shown working in this subspace is necessary for achieving good performance. see also section 4.
"to find a locally optimal solution to the above problem, we iterate the following procedure: given a current estimate of p and c at iteration k, we perform a first-order taylor approximation in a similar fashion to the lucas-kanade algorithm [cit] . then, an update for p and c can be found by solving the following optimization problem arg min"
"in this section, we evaluate the performance of po-cr for the problem of face alignment in-the-wild. to this end, we conducted a large number of experiments on the most popular facial databases, namely lfpw [cit], helen [cit], afw [cit] and ibug [cit] . we compare the performance of po-cr with that of a variant of our method as well as with that of two publicly available systems."
"we note that there are many examples of computer vision problems including bundle adjustment [cit], parameterized model fitting [cit] and detection/tracking [cit], in which the resulting optimization problems have structure; for example, the underlying normal equations might exhibit a sparse block or circulant structure [cit] . within the proposed formulation, our results show that this structure must be exploited during learning to produce accurate and robust solutions during testing."
"in this work, we propose regression to learn a sequence of averaged jacobians from data, and from them descent directions. in (d), the learned averaged jacobian with respect to the 3rd shape parameter for the first level of the cascade is shown. notably, po-cr learns averaged jacobians from which facial appearance variation is projected-out."
"in particular, for notational clarity let us first make the dependency of variables on iteration k explicit. then, the key idea in po-cr is to compute from a set of training examples an averaged jacobian j(k) from which the facial appearance variation is projected-out. the averaged projected-out jacobian, denoted as j p (k), is then used to compute an averaged projected-out hessian and descent directions. in detail, our learning strategy for po-cr is as follows:"
"against aams. the proposed project-out formulation is reminiscent of the well-known project-out inverse compositional (po-ic) algorithm used in aam fitting [cit] . both algorithms work in a subspace orthogonal to the appearance variation and have the same computational complexity per iteration (o(nn ) ). however, po-ic precomputes and employs an image jacobian from the mean appearance a 0 which remains fixed in all iterations. in contrast, po-cr proposes eq. (9) and regression to precompute a sequence of averaged jacobians from data, one per iteration. po-ic is an approximate algorithm for solving the problem of eq. (4) [cit] . in contrast, po-cr uses eqs. (7) and (8) as a basis for regression, i.e. the exact method for solving the problem of eq. (4)."
"publicly available systems. we compared the performance of po-cr with that of two publicly available systems: sdm [cit] and chehra [cit] . sdm was trained on internal cmu data that are not publicly available, and chehra on the whole lfpw, helen, afw and ibug data sets including data that is not publicly available. we note that the training data for chehra included the test sets of lfpw, helen, afw and ibug on which we report performance below, and hence chehra has an inherent advantage over all other methods."
"during testing, and in a similar fashion to cascaded regression techniques, given a current estimate of the shape parameters at iteration k, p(k), we extract image features i(s(p(k))) and then compute an update for the shape parameters from"
"learning in po-cr is based on eqs. (7) and (8) . in particular, as we may observe from eq. (8), at each iteration calculating ∆p requires (a) computing the image jacobian, (b) projecting-out the facial appearance variation from it and (c) computing the hessian and its inverse. based on this procedure, we propose to adopt a similar idea for our learning strategy in po-cr."
"regression is a standard tool for approaching various computer vision problems like human and head pose estimation [cit], deformable model fitting [cit], object localization and tracking [cit], and face and behaviour analysis [cit] to name a few. typically, regression-based methods wish to learn a function that maps object appearance to the desired target output variables. being discriminative in nature and by capitalizing on the very large annotated data sets that are now readily available, they have been (a) (b) (c) (d) figure 1 . project-out cascaded regression vs gauss-newton optimization. in prior work in face alignment, given the current estimate of the landmarks' location (a)-(b), image specific jacobians are calculated to be used in analytic gradient descent. in (c), the image jacobian with respect to the 3rd shape parameter is shown."
"against sdm. both po-cr and sdm learn a sequence of regression matrices (one per iteration) and during fitting the update of the shape parameters is computed in a very similar fashion. both methods have similar computational complexity. however, sdm uses non-parametric shape and appearance models as opposed to the parametric ones employed by po-cr. more importantly, learning in po-cr and sdm is very different. sdm learns directly a mapping from image features to problem parameters. in contrast, po-cr learns a set of averaged jacobian and hessian matrices from data, and from them descent directions in a subspace orthogonal to the appearance variation."
"we proposed project-out cascaded regression, a cascaded regression approach derived from a gauss-newton solution to a non-linear least squares problem that has structure. the learning strategy in po-cr capitalizes on this structure to compute averaged jacobians from which the facial appearance variation is projected-out and then employs the projected-out jacobians to compute descent directions. the fitting process in po-cr is similar to that of other cascaded regression techniques and hence our method maintains a high degree of computational efficiency. we conducted a large number of experiments on the most popular facial databases, namely lfpw, helen, afw and ibug that show that our system outperforms state-of-the-art systems sometimes by a large margin. figure 3 . application of project-out cascaded regression to the alignment of the ibug data set. for each image, the black bounding box shows the face detection initialization. our algorithm is able to produce highly accurate fittings for images with very large shape and appearance variation even with challenging initializations. the first 70 images of the ibug data set are shown."
"shown to produce state-of-the-art performance for many of the aforementioned tasks. at the same time, regressionbased methods enjoy a high degree of computational efficiency in both training and testing. in this work, the focus is on regression-based fitting of facial deformable models to unconstrained images, also known as face alignment inthe-wild. arguably, for this problem, regression-based approaches have recently emerged as the state-of-the-art. a plethora of regression methods have been employed to tackle the above mentioned problems including linear and ridge [cit], support vector [cit], boosted [cit], gaussian process [cit], and more recently, deep neural nets [cit] . a recent notable approach that is of particular interest in this work is the so-called cascaded pose regression (cpr) [cit] . cpr is an iterative (cascaded) regression method that is related to boosting with the main difference being that it uses pose-indexed features i.e. features that are sampled from the image based on the current pose estimate. this idea has been shown to produce excellent results on a variety of tasks and, owing to its efficiency and accuracy, it has been recently extensively explored by a number of authors for the problem of face alignment [cit] ."
"as we may observe at each iteration one needs to solve for both ∆p and ∆c. fortunately, there is an alternative way that by-passes the computation for the optimal ∆c at each iteration and guarantees an exact update for ∆p by taking into account the problem structure. this structure can be readily seen by writing"
"which as we may observe is a function of ∆p. then, we plug in the solution back to eq. (4) [cit] . by doing so, we end up with the following optimization problem"
"the proposed project-out cascaded regression (po-cr) uses generative models of facial shape and appearance fitted via cascaded regression in a subspace orthogonal to the learned appearance variation. in the following sections, we describe (a) the facial shape and appearance models employed by po-cr (section 3.1), (b) the optimization problem which provides the basis for learning in po-cr (section 3.2), (c) the learning and fitting process in po-cr (section 3.3), and finally (d) the differences between po-cr and related prior work (section 3.4)."
"the main reason enora and nsga-ii behave differently is as follows. nsga-ii never selects the individual dominated by the other in the binary tournament, while, in enora, the individual dominated by the other may be the winner of the tournament. figure 1 shows this behavior graphically. for example, if individuals b and c are selected for a binary tournament with nsga-ii, individual b beats c because b dominates c. conversely, individual c beats b with enora because individual c has a better rank in his slot than individual b. in this way, enora allows the individuals in each slot to evolve towards the pareto front encouraging diversity. even though in enora the individuals of each slot may not be the best of the total individuals, this approach generates a better hypervolume than that of nsga-ii throughout the evolution process."
"where vs is the volume of the search space. computing hvr requires reference points that identify the maximum and minimum values for each objective. for rbc optimization, as proposed in this work, the following minimum (f d lower, n r lower ) and maximum (f d upper, n r upper ) points, for each objective, are set in the multi-objective optimization models in equations (4)- (6):"
"and the fewer rules the models have and the fewer conditions and attributes the rules have, the easier it will be for a human to understand the logic behind each classification. in fact, rbcs are so natural in some applications that they are used to interpret other classification models such as decision trees (dt) [cit] . rbcs constitute the basis of more complex classification systems based on fuzzy logic [cit] such as logitboost or adaboost [cit] ."
"where k is the number of instances of the dataset d, and t d (γ, i) is the result of the classification of the instance i in d with the classifier γ, that is:"
"from the statistical tests (whose results are shown in the appendixes a and b) we conclude that among the six variants of the proposed optimization model there are no statistical significative differences, which suggests that the advantages of our method do not depend directly on a specific evolutionary algorithm or on the specific performance measure that is used to drive the evolutions. significant statistical differences between our method and very simple classical methods such as oner were expectable. significant statistical differences between our method and a well-consolidated one such as part have not been found, but the price to be paid for using part in order to have similar results to ours is a very high number of rules (15 vs. 2 in one case and 47 vs. 7 in the other case)."
"finally, zeror (zero rules [cit] ) is a classifier learner that does not create any rules and uses no attributes. zeror simply creates the class classification table by selecting the most frequent value. such a classifier is obviously the simplest possible one, and its capabilities are limited to the prediction of the majority class. in the literature, it is not used for practical classifications tasks, but as a generic reference to measure the performance of other classifiers."
"we would like to highlight that both the breast cancer dataset and the monk's problem 2 dataset are difficult to approximate with interpretable classifiers and that none of the analyzed classifiers obtains high accuracy rates using the cross-validation technique. even powerful black-box classifiers, such as random forest and logistic, obtain success rates below 70% in 10-fold cross-validation for these datasets. however, enora obtains a better balance (trade-off) between precision and interpretability than the rest of the classifiers. for the rest of the analyzed datasets, the accuracy obtained using enora is substantially higher. for example, for the tic-tac-toe-endgame dataset, enora obtains a 98.3299% success percentage with only two rules in cross-validation, while part obtains 94.2589% with 49 rules, and jrip obtains 97.8079% with nine rules. with respect to the results obtained in the datasets car, kr-vs-kp and nursery, we want to comment that better success percentage can be obtained if the maximum number of evaluations is increased. however, better success percentages imply a greater number of rules, which is to the detriment of the interpretability of the models."
"the interpretability of classification systems refers to the ability they have to explain their behavior in a way that is easily understandable by a user [cit] . in other words, a model is considered interpretable when a human is able to understand the logic behind its prediction. in this way, interpretable classification models allow external validation by an expert. additionally, there are certain disciplines such as medicine, where it is essential to provide information about decision making for ethical and human reasons. likewise, when a public institution asks an authority for permission to investigate an alleged offender, or when the ceo of a certain company wants to take a difficult decision which can seriously change the direction of the company, some kind of explanations to justify these decisions may be required. in these situations, using transparent (also called grey-box) models is recommended. while there is a general consensus on how the performance of a classification system is measured (popular metrics include accuracy, area under the roc curve, and root mean square error), there is no universally accepted metric to measure the interpretability of the models. nor is there an ideal balance between the interpretability and performance of classification systems but this depends on the specific application domain. however, the rule of thumb says that the simpler a classification system is, the easier it is to interpret. rule-based classifiers (rbc) [cit] are among the most popular interpretable models, and some authors define the degree of interpretability of an rbc as the number of its rules or as the number of axioms that the rules have. these metrics tend to reward models with fewer rules as simple as possible [cit] . in general, rbcs are classification learning systems that achieve a high level of interpretability because they are based on a human-like logic. rules follow a very simple schema:"
"whereĉ γ i is the predicted value of the ith instance for the classifier γ, and c i d is the corresponding output value in the database d. accuracy, area under the roc curve, and root mean square error are all well-accepted measures used to evaluate the performance of a classifier. therefore, it is natural to use such measures as fitting functions. in this way, we can establish which one behaves better in the optimization phase, and we can compare the results with those in the literature."
"whereĉ γ i is the predicted value of the ith instance in γ, and c i d is the corresponding true value in d. our second optimization model is based on the area under the roc curve:"
"the results of our tests allow for several considerations. the first interesting observation is that nsga-ii identifies fewer solutions than enora on the pareto front, which implies less diversity and therefore a worse hypervolume ratio, as shown in figures 3 and 4 . this is not surprising: in several other occasions [cit], it has been shown that enora maintains a higher diversity in the population than other well-known evolutionary algorithms, with generally positive influence on the final results. comparing the results in full training mode against the results in cross-validation or in splitting mode makes it evident that our solution produces classification models that are more resilient to over-fitting. for example, the classifier learned by part with monk's problem 2 presents a 94.01% accuracy in full training mode that drops to 73.51% in splitting mode. a similar, although with a more contained drop in accuracy, is shown by the classifier learned with breast cancer dataset; at the same time, the classifier learned by enora driven by accuracy shows only a 5.57% drop in one case, and even an improvement in the other case (see tables 5, 6, 9, and 10). this phenomenon is easily explained by looking at the number of rules: the more rules in a classifier, the higher the risk of over-fitting; part produces very accurate classifiers, but at the price of adding many rules, which not only affects the interpretability of the model but also its resilience to over-fitting. full training results seem to indicate that when the optimization model is driven by rmse the classifiers are more accurate; nevertheless, they are also more prone to over-fitting, indicating that, on average, the optimization models driven by the accuracy are preferable."
"our initial motivation was to design a classifier learning system that produces interpretable, yet accurate, classifiers: since interpretability is a direct function of the number of rules, we conclude that such an objective has been achieved. as an aside, observe that our approach allows the user to decide, beforehand, a maximum number of rules; this can also be done in part and jrip, but only indirectly. finally, the idea underlying our approach is that multiple classifiers are explored at the same time in the same execution, and this allows us to choose the best compromise between the performance and the interpretability of a classifier a posteriori."
"in this paper, we have proposed a novel technique for categorical classifier learning. our proposal is based on defining the problem of learning a classifier as a multi-objective optimization problem, and solving it by suitably adapting an evolutionary algorithm to this task; our two objectives are minimizing the number of rules (for a better interpretability of the classifier) and maximizing a metric of performance. depending on the particular metric that is chosen, (slightly) different optimization models arise. we have tested our proposal, in a first instance, on two different publicly available datasets, breast cancer (in which each instance represents a patient that has suffered from breast cancer and is described by nine attributes, and the class to be predicted represents the fact that the patient has suffered a recurring event) and monk's problem 2 (which is an artificial, well-known dataset in which the class to be predicted represents a logical function), using two different evolutionary algorithms, namely enora and nsga-ii, and three different choices as a performance metric, i.e., accuracy, the area under the roc curve, and the root mean square error. additionally, we have shown the results of the evaluation in 10-fold cross-validation of the publicly available tic-tac-toe-endgame, car, kr-vs-kp and nursery datasets."
"to compare the performance of enora and nsga-ii as metaheuristics in this particular optimization task, we use the hypervolume metric [cit] . the hypervolume measures, simultaneously, the diversity and the optimality of the non-dominated solutions. the main advantage of using hypervolume against other standard measures, such as the error ratio, the generational distance, the maximum pareto-optimal front error, the spread, the maximum spread, or the chi-square-like deviation, is that it can be computed without an optimal population, which is not always known [cit] . the hypervolume is defined as the volume of the search space dominated by a population p, and is formulated as:"
"supervised learning is the branch of machine learning (ml) [cit] focused on modeling the behavior of systems that can be found in the environment. supervised models are created from a set of past records, each one of which, usually, consists of an input vector labeled with an output. a supervised model is an algorithm that simulates the function that maps inputs with outputs [cit] . the best models are those that predict the output of new inputs in the most accurate way. thanks to modern computing capabilities, and to the digitization of ever-increasing quantities of data, nowadays, supervised learning techniques play a leading role in many applications. [cit] s; in those days, researchers were focused on both precision and interpretability, and the systems to be modeled were relatively simple. years later, when it became necessary to model more difficult behaviors, the researchers focused on developing more and more precise models, leaving aside the interpretability. artificial neural networks (ann) [cit], and, more recently, deep learning neural networks (dlnn) [cit], as well as support vector machines (svm) [cit], and instance-based learning (ibl) [cit] are archetypical examples of this approach. a dlnn, for example, is a large mesh of ordered nodes arranged in a hierarchical manner and composed of a huge number of variables. dlnns are capable of modeling very complex behaviors, but it is extremely difficult to understand the logic behind their predictions, and similar considerations can be drawn for svns and ibls, although the underlying principles are different. these models are known as black-box methods. while there are applications in which knowing the ratio behind a prediction is not necessarily relevant, (e.g., predicting a currency's future value, whether or not a user clicks on an advert or the amount of rain in a certain area), there are other situations where the interpretability of a model plays a key role."
"to ensure the reproducibility of the experiments, we have used publicly available datasets. in particular, we have designed two sets of experiments, one using the breast cancer [cit] dataset, and the other using the monk's problem 2 [cit] dataset."
"to perform an initial comparison between the performance of the classifiers obtained with the proposed method and the ones obtained with classical methods (part, jrip, oner and zeror), we have executed again the six models in full training mode."
"in this section, we propose a general schema for an rbc specifically designed for categorical data. then, we propose and describe a multi-objective optimization solution to obtain optimal categorical rbcs."
"the term optimization [cit] refers to the selection of the best element, with regard to some criteria, from a set of alternative elements. mathematical programming [cit] deals with the theory, algorithms, methods and techniques to represent and solve optimization problems. in this paper, we are interested in a class of mathematical programming problems called multi-objective constrained optimization problems [cit], which can be formally defined, for l objectives and m constraints, as follows:"
"our approach investigates the conflict between accuracy and interpretability as a multi-objective optimization problem. we define a solution as a set of rules (that is, a classifier), and establish two objectives to be maximized: interpretability and accuracy. we decided to solve this problem by applying multi-objective evolutionary algorithms (moea) [cit] as meta-heuristics, and, in particular, two known algorithms: nsga-ii [cit] and enora [cit] . they are both state-of-the-art evolutionary algorithms which have been applied, and compared, on several occasions [cit] . nsga-ii is very well-known and has the advantage of being available in many implementations, while enora generally has a higher performance. in the current literature, moeas are mainly used for learning rbcs based on fuzzy logic [18, [cit] . however, fuzzy rbcs are designed for numerical data, from which fuzzy sets are constructed and represented by linguistic labels. in this paper, on the contrary, we are interested in rbcs for categorical data, for which a novel approach is necessary. this paper is organized as follows. in section 2, we introduce multi-objective constrained optimization, the evolutionary algorithms enora and nsga-ii, and the well-known rule-based classifier learning systems part, jrip, oner and zeror. in section 3, we describe the structure of an rbc for categorical data, and we propose the use of multi-objective optimization for the task of learning a classifier. in section 4, we show the result of our experiments, performed on the well-known publicly accessible datasets breast cancer, monk's problem 2, tic-tac-toe-endgame, car, kr-vs-kp and nursery. the experiments allow a comparison among the performance of the classifiers learned by our technique against those of classifiers learned by part, jrip, oner and zeror, as well as a comparison between enora and nsga-ii for the purposes of this task. in section 5, the results are analyzed and discussed, before concluding in section 6. appendices a and b show the tables of the statistical tests results. appendix c shows the symbols and the nomenclature used in the paper."
"breast cancer encompasses 286 instances. each instance corresponds to a patient who suffered from breast cancer and uses nine attributes to describe each patient. the class to be predicted is binary and represents whether the patient has suffered a recurring cancer event. in this dataset, 85 instances are positive and 201 are negative. table 2 summarizes the attributes of the dataset. among all instances, nine present some missing values; in the pre-processing phase, these have been replaced by the mode of the corresponding attribute."
"oner (one rule) is a very simple, while reasonably accurate, classifier based on a frequency table. first, oner generates a set of rules for each attribute of the dataset, and, then, it selects only one rule from that set-the one with the lowest error rate [cit] . the set of rules is created using a frequency table constructed for each predictor of the class, and numerical classes are converted into categorical values."
"since the optimization model encompasses two objectives, each individual must be evaluated with two fitness functions, which correspond to the objective functions f d (γ) and n r(γ) of the problem (equation (3)). the selection of the best individuals is done using the pareto concept in a binary tournament."
"where f i (x) (usually called objectives) and g j (x) are arbitrary functions. optimization problems can be naturally separated into two categories: those with discrete variables, which we call combinatorial, and those with continuous variables. in combinatorial problems, we are looking for objects from a finite, or countably infinite, set x, where objects are typically integers, sets, permutations, or graphs."
"is the proportion of negative instances classified as negative by γ in d, and t is the discrimination threshold. finally, our third constrained optimization model is based on the root mean square error (rmse):"
p ← n best individuals from r according to the rank-crowding function in population r 20: t ← t + 1 21: end while 22: return non-dominated individuals from p algorithm 2 binary tournament selection.
where acc d (γ) is the proportion of correctly classified instances (both true positives and true negatives) among the total number of instances [cit] obtained with the classifier γ for the dataset d. acc d (γ) is defined as:
"as a future work, we envisage that our methodology can benefit from an embedded future selection mechanism. in fact, all attributes are (ideally) used in every rule of a classifier learned by our optimization model. by simply relaxing such a constraint, and by suitably re-defining the first objective in the optimization model (e.g., by minimizing the sum of the lengths of all rules, or similar measures), the resulting classifiers will naturally present rules that use more features as well as rules that use less (clearly, the implementation must be adapted to obtain an initial population in which the classifiers have rules of different lengths as well as mutation operators that allow a rule to grow or to shrink). although this approach does not follow the classical definition of feature selection mechanisms (in which a subset of features is selected that reduces the dataset over which a classifier is learned), it is natural to imagine that it may produce even more accurate classifiers, and more interpretable at the same time."
"once the set of optimal solutions is available, the most satisfactory one can be chosen by applying a preference criterion. when all the functions f i are linear, then the problem is a linear programming problem [cit], which is the classical mathematical programming problem and for which extremely efficient algorithms to obtain the optimal solution exist (e.g., the simplex method [cit] ). when any of the functions f i is non-linear then we have a non-linear programming problem [cit] . a non-linear programming problem in which the objectives are arbitrary functions is, in general, intractable. in principle, any search algorithm can be used to solve combinatorial optimization problems, although it is not guaranteed that they will find an optimal solution. metaheuristics methods such as evolutionary algorithms [cit] are typically used to find approximate solutions for complex multi-objective optimization problems, including feature selection and fuzzy classification."
"both enora and nsga-ii have been implemented with two crossover operators, rule crossover (algorithm 9) and rule incremental crossover (algorithm 10), and two mutation operators: rule incremental mutation (algorithm 11) and integer mutation (algorithm 12). rule crossover randomly exchanges two rules selected from the parents, and rule incremental crossover adds to each parent a rule randomly selected from the other parent if its number of rules is less than the maximum number of rules. on the other hand, rule incremental mutation adds a new rule to the individual if the number of rules of the individual is less than the maximum number of rules, while integer mutation carries out a uniform mutation of a random antecedent belonging to a randomly selected rule."
"we have conducted different experiments with different optimization models to calculate the overall performance of our proposed technique and to see the effect of optimizing different objectives for the same problem. first, we have designed a multi-objective constrained optimization model based on the accuracy:"
"to avoid the premature convergence and improve the performance of the algorithm, we introduce a diversity control method into qpso. the population diversity of the dcsqpso is denoted as and is measured by average euclidean distance from the particle's personal best position to the mean best position, namely (10) where m is the number of the population, d is the dimension of the problem, and is the length of longest the diagonal in the search space."
"but unlike to uresem and riget [cit], in dcsqpso, only low bound is set for to prevent the diversity from constantly decreasing. the procedure of the algorithm is as follows. after initialization, the algorithm is running in convergence mode."
"from the comparison of dcsqpso with other algorithms, dcsqpso achieved better results than other algorithms averagely. it shows the diversity-controlled qpso with local search strategy were effective for most test functions. for function f 1, f 2, f 4, f 5, f 7, dcsqpso has the best performance among all of the tested algorithms. in some case, these functions obtain minimum objective function values zeros. for function f 3, qpso has the minimum value when the swarm size is 40 and dimension is 10, the swarm is 80 and dimension is 10 and 20, while dcsqpso outperforms for the rest functions. for function f 6, spso has the best results in three cases. for function f 8, qpso-rs algorithm has the minimal value in some cases. fig. 1 shows the convergence process of the four algorithms on the eight benchmark functions with dimension 30 and swarm size 40 averaged on 50 trail runs. it is shown that, although dcsqpso converge more slowly than the spso and qpso during the early stage of search, it may catch up with sqpso and qpso at later stage and could be generated better solutions at the end of search."
"particle swarm optimization (pso) is a kind of stochastic optimization algorithms proposed by kennedy and eberhart [cit] that can be easily implemented and is computationally inexpensive. the core of pso is based on an analogy of the social behavior of flocks of birds when they search for food. pso has been proved to be an efficient approach for many continuous global optimization problems. however, as demonstrated by f. van den bergh [cit], pso is not a global convergence guaranteed algorithm because the particle is restricted to a finite sampling space for each of the iterations. this restriction weakens the global search ability of the algorithm and may lead to premature convergence in many cases."
"the efficacy of the cca approach has been widely proven, and its superiority to psda in terms of speed, accuracy, and computational load has been shown [cit] . for this reason, several cca variations have been proposed over the years [11-13, 15, 21-26] ."
"in the pso with m individuals, each individual is treated as volume-less particle in the d-dimensional space, with the current position vector and velocity vector of particle i at the n th iteration represented as and . the particle moves according to the following equations:"
"where is called mean best position defined by the average of the position of all particles, i.e. (7) thus the position of the particle updates according to the following equation:"
"ten healthy volunteers (aged 22 to 26, 4 males and 6 females) participated in the study. all of them had normal, or corrected to, normal vision. during the experiment, the participants sat on a comfortable chair, with their arms relaxed and their head still, approximately 60 cm distant from the pc monitor."
"particles are located at different regions. therefore, if the current particle falls into local minima, particles in other regions may pull the trapped particle forward."
"a brain-computer interface (bci) is a system enabling direct communication between the brain and the outside, as it directly translates the recorded neural activity into a control signal for an external device (e.g., a computer, a machine, or a speller) [cit] . among noninvasive systems, electroencephalography-(eeg-) based bcis are the most widespread [cit], and they can rely on four possible electrophysiological sources: slow cortical potentials (scps), event-related desynchronization/synchronization (erd/ers), event-related potentials (as p300), or steady-state visually evoked potentials (ssveps) [cit] . among these, ssvep-based bcis are appealing for their high accuracies and information transfer rate (itr), thanks to the high signal-to-noise ratio of ssveps even without user training [cit] . for this reason, ssvep-based bcis have been raising increasing attention over the years [cit] ."
"using monte carlo method, we can measure the j th component of position of particle i at the (n+1) iteration by (5) where is a sequence of random numbers uniformly distributed within . the value of is determined by:"
"we suggest that all the above-introduced suppositions are likely to be true in real eeg signals. supposing indeed that the ssvep response is generated in a limited area of the occipital cortex, it will undergo different delays to reach the different locations of electrodes, due to a delay in spatial transmission. however, we suggest that the second supposition also is reasonable in real eeg. given indeed the origin of ssvep in the occipital cortex, the signal has to pass through multiple tissue layers (fluids, bone, and skin) before reaching each eeg location. this is likely to produce phase distortion between different frequency components, besides the well-known spatial blurring effect."
"on the course of evolution, if the diversity measure of the swarm drops to below the low bound, the mean best position is reinitialized."
"ssveps are periodic brain signals elicited over the occipital cortex by visual stimulations with frequencies higher than 6 hz [cit] . in case different flickering objects (leds, symbols, and squares) are simultaneously presented, an analysis of the ssvep spectral content permits to reconstruct which stimulus the user is focusing on."
"recently, a new variant of pso called quantum-behaved particle swarm optimization (qpso) [cit], which is inspired by quantum mechanics and particle swarm optimization model. qpso has only the position vector without velocity, so it is simpler than standard particle swarm optimization algorithm. furthermore, several benchmark test functions show that qpso performs better than standard particle swarm optimization algorithm. although the qpso algorithm is a promising algorithm for the optimization problems, like other evolutionary algorithm, qpso also confronts the problem of premature convergence, and decrease the diversity in the latter period of the search. therefore a lot of revised qpso the authors are with school of information science technology, hainan normal university, haikou 571158, hainan, china (e-mail: haixia_long@163.com, 595615374@qq.com, 605515770@qq.com)."
"for each particle, its neighborhood may cover better solutions. to improve the ability of exploitation, a local neighborhood search strategy is proposed. during searching the neighborhood of a particle, a trial particle is generated as follows [cit] : (11) where is the position vector of the particle, is the previous best particle of, and are the position vectors of two random particles in the k-neighborhood radius of and are three uniform random numbers within (0,1), and . the random numbers and are the same for all and they are generated a new in each generation."
"in this paper, diversity-controlled qpso with local search (dcsqpso) is introduced. this strategy is to prevent the diversity declining of particle swarm declining in the search of later stage."
the random numbers and are the same for all and they are generated a new in each generation. the gns strategy is helpful to solve multimodal problems.
"the above-described interpretation fits the experimental results well; indeed the accuracy significantly increased when switching from one to three canonical correlations. we consequently suggest that the consideration of more than one canonical correlation permits to encompass a more complete information on the investigated frequency, and this finally translates in an increased accuracy, revealed in almost every subject and run. from the third set of canonical variables on, we hypothesize that the amount of information described by each correlation depends on each user's individual characteristics, for example, the amount of delay across different harmonics and electrodes, as well as the differential amplitude of the ssvep response between different harmonics of the same stimulation frequency. according to this hypothesis, from the fourth canonical correlation on, there would not be a group effect anymore, and this would explain why the accuracy increments in the experimental data are not significant anymore."
"as a final comment, we believe that, beyond making a comparison of our methods to literature, the main aim and contribution of this work were giving a systematic study of the effect of two simple, modular, and computationally light variations of the standard cca algorithm. these proposed variations might be intended as modular \"algorithm bricks\" and might be flexibly translated to the design of cca-based algorithm that is even different from ours in order to increase the overall accuracy."
"a. diversity-controlled mechanism qpso is a promising optimization problem solver that outperforms pso in many real application areas. the introduced exponential distribution of positions makes qpso global convergent. but it suffers from premature convergence. at the beginning of the search, the diversity of the population is relatively high after initialization. with the development of evolution, the convergence of the particle makes the diversity been declining, which is enhancing the local search ability (exploitation) but weakening the global search ability (exploration) of the algorithm. at early or middle stage of the evolution the decline of the diversity is necessary for the particle swarm to search effectively. however, after middle or at later stage, the particles may converge into such a small region that the diversity of the swarm is very low and further search is difficult. at that time, if the particle with global best position is at local optima or sub-optima, premature convergence occurs."
"in this work, we evaluated the impact of two simple and modular variations of the cca algorithm in a 4-class ssvep recognition setup. the two variations involved (i) the number of considered canonical correlations and (ii) the inclusion of a narrow-band prefiltering step around the employed stimulation frequencies and related harmonics by means of sincwindowing technique. our results indicate that even simple consideration of more than one canonical correlation can significantly improve accuracy, without any increment of computational load. notably, there were significant increases in accuracy when switching from one to three canonical correlations, while the increments were not significant from the fourth canonical correlation on. an additional narrowband prefiltering permitted to gain up to 7-8% of accuracy on average, with peaks of 25-30%, with respect to classical cca. a further advantage of sinc-windowing implementation is that it permits the enhancement of multiple frequency components in one single step, by simply modulating the composition of the sinc-function. given the modular nature of the proposed variations and the significant increments in accuracy, regardless of whether the variations were used separately or, even more, in combination, together with the minimal computational costs, we believe that they could easily represent valid integrations to be included in future ccabased designs."
"this paper presents an enhanced qpso algorithm called dcsqpso to solve complex optimization problems. the proposed approach explores diversity enhancing mechanism and local search strategies. to verify the performance of dcspso, different types of benchmark functions are tested in the experiments."
"as a second variation, we evaluated the influence of such type of prefiltering with using a sinc-windowing implementation. the technique of sinc-windowing consists in the convolution of the analyzed signal with an adequately modulated sinc function. as it is known, the inverse fourier transform of an ideal rectangular band-pass filter centered in 0 and with bandwidth is"
"although each introduced variation produced significant increments of classification accuracy, all of them tended to increase the complexity of the algorithm. they indeed either required additional user training, to incorporate information from individual eeg data [11-13, 15, 21, 23], or increased computational load by multiplying the number of ccas to assess each stimulation frequency [cit] . however, we believe that even taking simple procedures and low computational costs may be relevant, especially to favor the spread of low-cost and high-portability devices. in addition, it would be desirable that variations are as general or scalable as possible to facilitate the translation of results to different setups."
"the eeg was recorded from 8 electrodes (po7, po8, po3, po4, o1, o2, poz, and oz), positioned according to the international 10-20 system. the signals were acquired using a brainbox eeg-1166 amplifier, with a 256 hz sampling frequency and a 50 hz notch filter on."
"given these premises, this work presents two simple and modular variations based on the classical cca method. the variations regard (i) the number of correlations considered for classification and (ii) the preprocessing of the signals. we show that both modifications can significantly improve classification accuracy but still leaving the whole procedure training-free and with no (variation (i)) or minimal (variation (ii)) impact on the number of steps required for each ssvep identification."
"the parameter  in eq. (6) and (8) is called contraction-expansion (ce) coefficient, which can be adjusted to balance the local search and the global search of the algorithm during the optimization process."
"to test the performance of the dcsqpso, eight widely known benchmark functions listed in table i . dcsqpso are tested for comparison with standard pso (spso), qpso, qpso-rs [cit] . functions f 1 -f s are unimodal and functions f 4 -f s are multimodal. these functions are all minimization problems with minimum objective function values zeros. the fitness value is set as function value and the neighborhood of a particle is the whole population."
"cca is a multivariate statistical method able to reveal the underlying correlation between two sets of data [cit] . for ssvep recognition, cca is performed several times between the considered eeg segment and a set of sine-cosine 2 computational intelligence and neuroscience reference signals modeling the pure ssvep responses to each stimulation frequency [cit] . the frequency response showing highest correlation with the analyzed eeg portion is finally recognized as the observed one."
"when varying the length of the eeg portions used to recognize the ssveps, the behavior of the proposed variations on classification accuracy tended to be confirmed, with the only exception of the 0.5 s window length (figure 2 ). while the consideration of more than one canonical correlation always outperformed the use of the largest one only, the positive impact of sinc-windowing emerged only for window lengths greater than 0.5-1 s."
"in every generation, if the particle's current position is better than its parent then replaced with current particle position; otherwise, we keep parent particle unchanged. after this operation, the local search strategy is conducted with probability. if the probability is satisfied, two particle and are generated. then, the fittest particle among, and is selected as the new ."
"besides the local neighborhood search, a global neighborhood search (gns) strategy is proposed to enhance the ability of exploration. when search the neighborhood of a particle another trial particle is generated as follows [cit] : (12) where is the global best particle, and are the position vectors of two random particles chosen for the entire swarm, and are three uniform random numbers within (0, 1), and"
"where f is the frequency and −1 is the inverse fourier transform. thus, the filtering around the stimulation frequencies and harm harmonics can be accomplished by means of a convolution with the following function:"
"our study also relates to cross-lingual studies in other applications [cit] . these approaches adopted embedding projection based method to achieve cross-lingual transfer and achieved prosing results. however, since the lexical mapping in these methods is usually deterministic and irrespective of contexts, they might not directly fit with cross-lingual ed, where the cross-lingual transfer should be context-dependent."
"the main disadvantage of multivariate machine learning models is that local inference with respect to the brain neuroanatomy is complex: although linear models generate weights for each voxel, the model predictions are based on the whole pattern and therefore one cannot threshold the weights to make regional statistical inferences as in univariate analysis. in the present work, we developed a methodology based on a labelled anatomical template (e.g. aal [cit] or brodmann) to display a regionally smoothed version of the model weights and compute a ranking of the regions according to their contribution to the predictive model. furthermore, we defined a \"ranking distance\", which allows the quantitative comparison of patterns in terms of their localization."
"while ∆t is the time resolution and equal to 1 f s, f s is the sampling rate. the hough transform of wvd maps the point (t, f ) of the tf plane line onto the point (ρ, θ) in the parameter plane. as a consequence, a peak is formed at the point (ρ, θ) of the parameter plane. once the peak value exceeds a predetermined threshold, it can be determined that there is a chirp interference."
"in total, 29 subjects participated in the study: 14 patients (7 males; mean age: 65.1 ± 9.5 years) diagnosed with ipd with different degrees of severity of gait disturbances and 15 controls matched for age (63.8 6 ± 8.1 years) and gender (7 males). written informed consents for this research protocol approved by the local ethics committee were obtained from all participants."
"we conduct two groups of experiments to investigate the ability of our model in 1) performing cross-lingual transfer concerning different language directions, and 2) handling the annotationpoor scenario."
"we conduct ablation study to explore the effects of our different model components. we limit our study in the extremely annotation-poor scenario, that is, we assume there is no training data in the target language (chinese)."
"wvd produces an energy distribution concentrated along a straight line. as a result, the problem of detecting the chirp interference can be turned into an issue of detecting the tf plane line and performed by hough transform, which has been widely used for detecting chirp signal combined with the tf distribution [cit] . the combination of wvd and hough transform can be defined as follows"
"the roc curves have been used to assess the performance of the notch filter to mitigate chirp interference by monte carlo simulations. the results depict that the proposed method can effectively detect and identify the chirp interferences with crossed frequency and provide the same root mean square errors (rmse) of the parameter estimation for chirp one and the improved initial frequency estimation for chirp two compared with the hough transform of wvd when jnr equals or surpasses 4 db. furthermore, the rspwvd method itself can provide better performance in reducing cross-term interference and needs less computational requirements compared with the wvd method."
"to illustrate our motivation, consider an english example e1 and its parallel chinese translation c1 in figure 1 . as shown, e1 and c1 have rather different word orders ( figure 1a ), but they share a similar syntactic structure which captures enough generality for identifying event triggers ( figure 1b ). this observation motivates us to explore the syntactic similarity to achieve multilingual co-training. to achieve this goal, we propose a shared syntactic order event detector based on graph convolutional networks (gcns) [cit], which can provide each word a contextual feature based on its immediate neighbors in the syntactic graph irrespective of its original position in the sentence. this decoder allows us to train a model on multilingual resources effectively, which circumvents the word order different problem."
"health care professionals have the responsibility to look after the interests of their patients, the public, the profession and the organization in that order. as healthcare systems respect and advocate this hierarchy of accountability, patient safety and quality care becomes part of the system. organizational learning in health care should focus on understanding how it was done, perfecting how it is done and exploring how it can be done to optimize patient care without compromising patient safety."
"we used a subset of the data by randomly selecting 54 subjects that were acquired in centre 1, and 54 subjects acquired in centre 2. please note that there is no significant difference between the two populations in terms of age."
"the case study also poses several future directions for this work. for example, one is how to address the one-to-many mapping between different languages. in the above example, the correct chinese translation of \"every year\" should be one single word \"每年\", not the combination of two words \"每个(every)\" and \"年(year)\". this calls for more advanced lexical mapping methods."
"the performance of the developed method has been assessed by experiments under conditions where the real bds b1i signals corrupted by the simulated chirp interferences are collected by the gnss software receiver. the actual performance of experiments has been shown by the quantitative metric rmse of the parameter estimation. in addition, the effect of the sweep period on the estimation of the initial frequency and chirp rate has been analyzed."
"specifically, for each token w i, our model computes a graph convolution feature vector based on its immediate neighbors in the syntactic graph. figure 3 illustrates the process of extracting the feature for \"fired\"."
"for the digital signal processing methods, they include spatial domain methods [cit], spatial-temporal domain methods [cit], time domain methods [cit], frequency domain methods [cit], and time-frequency (tf) domain methods [cit] . time domain techniques as well as frequency domain techniques cannot completely describe the nature of time-varying signals."
"the present work investigated pattern localization, as well as quantitative pattern comparison using two datasets 5 . the preliminary results show that the developed methodology seems promising, although more work is needed."
"we therefore built four regression models comprising each 54 subjects: centre1, centre2, mix1 (first half of the 54 subjects from each centre: 27 + 27) and mix2 (second half of the 54 subjects from each centre). the age regression led to significant results (1000 permutations), with correlations (mse) of 0.45, 0.63, 0.43 and 0.65 (21. 16, 20.77, 26.90, 16.19), respectively for each model."
"where y denotes the (chinese) word embedding of y; r y (w x i ) indicates the mean cosine similarity between w x i and its k neighbors in y, which is defined as:"
this paper presents a joint method based on the hough transform of rspwvd to detect time-varying interferences with crossed frequency for gnss receivers with a single antenna. the analytical expression of initial frequency estimation and chirp rate estimation is presented and the double threshold detection is proposed as well.
"where f (ρ, θ) is the instantaneous frequency of the chirp interference in polar coordinates. equation (18) as well as equation (19) depicts that the line of the tf plane becomes a peak at point (ρ, θ) of the parameter plane by hough transform. conversely, a peak in the parameter plane represents a line in the tf plane and can be used to estimate the parameter of the line. as a result, this can be used to detect and identify chirp interferences. the joint method based on the hough transform of rspwvd is displayed as figure 3 and is based on the following steps."
"in the case of a single satellite and a single chirp interference, the input signal x if (t) in equation (1) enters the adc without considering the quantization effect and can be rewritten as"
"peter senge, introduced the term -learning organization‖ a scenario in which people are continuously learning together for the best possible outcomes from the organization [cit] . a successful learning organization in senge's system's theory has the capacity to change and manage change where individuals in an organization adopt system thinking, attain personal mastery, share mental models, have a shared vision and learn in teams [cit] nonaka and takeuchi proposed a model of organizational learning as the process of knowledge management where they discuss the knowledge spiral in which tacit knowledge of an individual becomes explicit knowledge through a process of socialization and externalization which is in turn disseminated through the organization which promotes the organization to learn [cit] ."
"as english and chinese usually have different word orders, the transferred result t might be seen as a corrupted sentences from chinese, which could introduce noise for multilingual co-training. we tackle this problem by proposing a graph convolutional neural networks (gcns) [cit] based syntactic order event detector, which provides each word with a feature vector based on its immediate neighbors in the syntactic graph irrespective of its position in the sentence. this allows our model to train with the translated data t and the other labeled data in chinese indiscriminately."
where thex if (t) is the hilbert transform of x if (t); the use of the analytic signal x a (t) can avoid the presence of cross-terms which could be generated by the interaction between positive and negative frequency components [cit] .
"as shown in fig. 2 (b), the proposed active inductor consists of a high-frequency inductor l f, a low-frequency inductor l 1, and a controlled-current source connected in parallel with l 1 . the controlled-current source i aux is equal to the undesirable low-frequency current harmonics i lh of l 1 with out of phase and high-frequency harmonics i hh . the voltage across the controlled-current source is equal to the undesirable lowfrequency voltage harmonics of v aux . therefore, the auxiliary circuit used to implement the controlled-current source processes the partial voltage of v ab and the partial current of i ab only, as indicated in fig. 2(b) ."
"in our method, for w i, we take j chinese words which have the smallest cslss as its translation candidates. we denote by t (w i ) the set of translation candidates for w i, where t"
organizational learning can be viewed as a modified context specific learning by multiple teams and team members to translate knowledge to action and to evaluate those actions to create shared knowledge within an organization/institution. individual and team learning complement organizational learning but do not produce organizational learning because these learnings often occurs in professional or team silos without knowledge sharing with other groups within an organization. organizational learning can occur either as a result of organizational change or as a precursor to organizational change and has been explained by change or system's theories and knowledge management theories respectively.
"informal network leaders are the community builders who weave organizations together. with little or no formal position or authority, they turn this weakness into strength by demonstrating commitment when they act from personal conviction [cit] . these informal leaders can be bedside nurses, social workers in an inpatient ward, senior residents, attending staff physicians or any other healthcare professional with a passion to make a change to improve patient care. they have to be able to convince and create a team to address the cause, assign responsibilities and accountability, make the changes and evaluate outcomes to foster organizational learning [cit] . it is important that senior management ensures that line managers do not stifle these informal leaders by their rigid adherence to what they consider unbreakable rules and priority projects."
"in this paper, we propose a new cross-lingual approach for event detection, which demonstrates a minimal dependency on parallel resources. specifically, we propose a context-dependent lexical mapping method to obtain content-dependent translation, and we devise a shared syntactic order event detector to explore the syntactic similarity for multilingual co-training. experiments demonstrate the effectiveness of our method."
"wvd produces an energy distribution concentrated along a straight line. as a result, the problem of detecting the chirp interference can be turned into an issue of detecting the tf plane line and performed by hough transform, which has been widely used for detecting chirp signal combined with the tf distribution [cit] . the combination of wvd and hough transform can be defined as follows"
"this paper adopts the double threshold detection method named primary threshold and secondary threshold [cit] . the gnss signals are buried in thermal noise assumed to be zero mean, independent and identically distributed (iid). when interference is absent, the complex random variable x a (t) in equation (8) is the zero mean and iid. its magnitude spectrum φ(k, l) in the tf plane can be written [cit]"
"from the above analysis, not only wvd but also rspwvd can be combined with hough transform. however, wvd suffers from severe cross-term interference. as a result, it will affect the result of the hough transform. therefore, the hough transform of rspwvd is proposed [cit] . the general expression of the hough transform of the time-frequency transform can be written as"
"frequency of the auxiliary circuit, which is not discussed here for the sake of simplicity. with the conventional inductor solution, the required inductance l p is"
t j (5) where b sweep represents the sweep bandwidth. the mth jamming power to the lth gnss signal power ratio (jsr) is written as follows
"where d indicates the word embedding dimension. we take c i as the contextual representation of w i . learning selective attention. for each token w i, after obtaining its translation candidates list t (w i ) and contextual representation c i, we impose a selective attention mechanism to automatically weigh each candidate. specifically, the weight of the jth candidate t (w i ) j is computed as:"
"local unofficial leaders are also called opinion leaders or educational influentials in medical education literature. a systematic review of 18 studies involving close to 300 hospitals and over 300 primary care practices showed that the use of -opinion leaders alone or in combination with other interventions may successfully promote evidence-based practice; effectiveness of the interventions varied both within and between studies‖ [cit] . informal leaders and opinion leaders are important to promote, sustain and evaluate organizational learning within each group of professionals or teams. for example, to sustain a hand hygiene guideline implementation aimed at reducing spread of infection in a hospital, informal local leadership will be needed at each and every department, division, unit, ward, clinical and nonclinical area, to promote, monitor and evaluate the initiative. the success of the initiative depends on it being a system wide practice so that hand hygiene is observed even in a non-patient care area such as the hospital library. if a librarian touched a door knob on her way to work and does not practice hand hygiene in the library, she can spread the infection through other clinical staff who frequent the library."
"with dr(f 1, f 2 ), the ranking distance between the models f 1 and f 2 and n, the number of elements in the ranking, which corresponds to the number of regions in the atlas in the present case (i.e. 117 labelled regions). the values of dr range from 0 (exactly the same rankings) to 1 (exactly opposite rankings)."
"all subjects then underwent an fmri session comprising three tasks: mental imagery of standing on the path (stand), walking at a comfortable pace along the path (comf) and walking briskly along the path (brisk). the comf and brisk conditions were self-paced, subjects indicating when they had completed each trial by a key press, while each trial of the stand condition was constrained by the duration of the previous comf trial."
"when it comes to the time-varying interference, the classical time-frequency analysis based on wvd and rspwvd provides superior performance. however, wvd suffers from cross-term interference seriously when the analytic signals have two or more components [cit] . the rspwvd can reduce the cross-term interference, but it cannot deal with the signals with crossed or overlapped frequency by the peak detection method [cit] . an example of rspwvd with two chirps whose frequencies are crossed is shown as figure 1 . figure 1a depicts that the peaks of two chirps are clear except the overlapped frequency part. in figure 1b, it is obvious that the outline of two chirps in the overlapped frequency is blurred so that it is difficult to distinguish which signal the frequency of the overlapped part belongs to. therefore, the proposed method based on hough transform of rspwvd is introduced [cit] ."
this paper presents a joint method based on the hough transform of rspwvd to detect time-varying interferences with crossed frequency for gnss receivers with a single antenna. the analytical expression of initial frequency estimation and chirp rate estimation is presented and the double threshold detection is proposed as well.
"author contributions: q.l. contributed to design the algorithms, performed the experiment, analyzed the results of the experiments and wrote this paper. h.q. provided valuable feedback and advice during the manuscript modification."
"event detection (ed) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. traditionally, the study of ed has focused on monolingual training. the proposed models can be divided into feature-based methods which employ fine-grained features [cit], and deep learning-based methods which employ neural networks to automatically learn features for the task [cit] b; [cit] . usually, their performance is limited by the amount of labeled data in a specific language."
"where α is the ripple current ratio limit of i ab (i.e., the dc-link current in the case study). the apparent power ratio between the auxiliary circuit of the proposed active inductor in fig. 2(b) and the one of existing solutions in fig. 2(a) is"
"at present, gnss receivers with a single antenna are threatened by a serious jamming environment where many gnss receiver failures occurred [cit], limiting the gnss applications. as a result, the techniques used to detect and mitigate interference effects have become an increasingly important issue and can be divided into the automatic gain control (agc) method [cit], digital signal processing methods [cit] and receiver methods [cit] from gnss signal processing chains."
"finally, for each token w i, we perform a contextaware selective attention mechanism to weigh each translation candidate in t (w i ) and get the best-suited translation for it."
"organizational learning is linked to organizational changes and its management of the change process [cit] . organizational change and restructuring due to economic and political reasons is becoming more common in many countries. in a recent survey, 88% of the 263 american hospitals that were surveyed, were engaged in restructuring changes to improve cost efficiencies, quality, or both [cit] . this study showed that a -stronger orientation to learning and innovation had a statistically significant and positive relationship with the ability to sustain the restructuring changes and that a control orientation decreased organizational consensus on the perception of improved costs, but did increase perceptions of organizational ability to sustain the effort‖. some of the factors identified as control in the study included -the importance to follow existing rules, avoiding risk, and using budgets to motivate employees‖. the authors caution that the usual habit of health care organizations to become more control oriented during times of change is not beneficial as it may create ineffective change [cit] ."
"based on [cit], we built the comf versus stand comparison for the control and patient groups separately. the different conditions were also combined to compare the two groups (see table i ). the pattern leading to the only significant discrimination between groups was then localized."
"from the results, we observe that 1) even though both of cl trans (1 cand.) and embedding proj are content-independent mapping methods, the former outperforms the latter by a margin (+3.2% on f1). this implies that the embeddingprojection method might suffer from the misalignment in the shared embedding space, and enforcing a word-to-word alignment (as in cl trans (1 cand.)) could alleviate this problem to some ex- tent. 2) retrieving more translation candidates could consistently improve recall. but when too many candidates (e.g., 5) are added, the precision drops, which harms the overall f1 measure."
"health care organizations have to be dynamic to accommodate changes to their system and service delivery models due to changing demands brought on by several factors but not limited to globalization, migration trends, travel and changing socio-cultural landscape of urban and rural areas. for example the patient populations in several urban centers have changed requiring an increased use of translators to service the multicultural group. translators need to communicate in a culturally sensitive manner and ascertain the breadth and variety of potential disease exposures endemic to the patient's home country. healthcare staffs, through the interpreters, need to accurately convey the nuances of these interactions and find the right terms."
", where i l 1 -pp is the peak-to-peak value of the ripple current of l 1 . therefore, the value of β determines the processed current in the auxiliary circuit. there is a tradeoff between its apparent power and the inductance value l 1 . in practice, there are high-frequency current harmonics in i aux due to the nonideal operation of the auxiliary circuit. the high-frequency current level depends on the values of c 1, l 2, and the switching fig. 5 . relation between ripple current ratio β of l 1, apparent power ratio γ s, and energy storage ratio γ e with 17% current ripple ratio α."
"for each token w i in s, context-dependent lexical mapping aims to search for its best-suited word translation according to its contextual representation. this process involves: 1) learning multilingual alignment, 2) retrieving translation candidates, and 3) ranking translation words via a selective attention mechanism."
"organizational change in health care does not happen in isolation; routine work is expected to continue during the change process. iterative design methodology requires repeated testing and refinements which can affect performance [cit] . to confound matters, disruptive innovations such as, robotic surgery, telemedicine, self monitoring and self management of chronic conditions such as diabetes, the use of ultrasound by non radiologists, reading glasses in retail stores, medical tourism and retail medical clinics are changing how health care is accessed by the public and delivered by the system. many inpatient procedures are delivered in outpatient settings now and the need for hospital care is decreasing while home care needs are increasing. health care organizations have to learn and change with these new innovations to survive the market changes [cit] ."
"fmri data preprocessing and univariate analysis were performed using spm8 2 . functional images were realigned and co-registered to the structural image before normalisation using dartel [cit] . finally, smoothing was applied using a 8mm fwhm gaussian kernel. a general linear model then summarized the time series from each subject by modelling each condition by a boxcar function convoluted with a canonical haemodynamic response function. in the end, three images per subject were considered for further analysis: the parametric maps of stand, comf and brisk representing the bold signal activity associated with each condition."
where v lh is the rms value of low-frequency voltage harmonic of interest. ω lh is the angular frequency of the low-frequency harmonic. β is the ripple current ratio of
where if b is the rf front-end bandwidth. the analytical expression of the received signal in equation (1) avoid the presence of cross-terms which could be generated by the interaction between positive and negative frequency components [cit] .
"for the last few years, machine learning models have been applied to neuroimaging data [cit], allowing to make predictions about a variable of interest based on the pattern of activation or anatomy over a set of voxels. in addition, they might lead to an increased sensitivity to detect the presence of a particular mental representation compared to univariate methods such as the general linear model (glm, [cit] ). application of these methods made it possible to decode the category of an object [cit] seen by the subject . they also allowed classification of patients and healthy controls [cit] and could therefore be used as a diagnostic tool due to their ability to predict the class of an unseen sample."
"we also compared our model with several variants including 4) cl trans self., which replaces the gcns with a self-attention network, and 5) cl trans gcn self, which combines gcns with a self-attention network. we train these models on the same translated english data. table 3 shows the results."
the specification of the case study is shown in table i . the diagram of the three-phase diode-bridge rectifier with a twoterminal active inductor is shown in fig. 4 . the high-frequency current harmonics and high-frequency filter l f are not considered in the following discussions. the inductance of l 1 is
"similar to a principal component analysis, the labelled regions can then be ranked according to the percentage of the total normalized weights that they explain. this results in a sorted list of regions, that can then be compared (to a certain extent) with univariate results."
"teams within a health care organization are not generally consistent in scale or membership. they can be homogenous and heterogeneous teams. heterogeneity exits within what may be called a homogenous team such as a team of surgeons, where it may not be possible to replace one member with another, as training level, specialty and sub-specialties may vary. these teams can be visible teams that provide frontline patient care such as physicians and nurses or ancillary care teams such as phlebotomists, laboratory technicians, radiographers and radiologists or frontline nonclinical teams such as security, registration and information clerks, patient service aids, housekeeping, or invisible teams such as the biomedical engineering, information technology, kitchen staff or the plant operation team. these interconnections and interdependence are not apparent or appreciated in the large scope of daily activities. for example if the tube system that transport specimens and medications or the information technology (it) systems that manages all the computers, telephone and paging systems in the hospital breakdown, there is a disruption and loss of communication, compromising standard care in a timely manner by frontline workers. as such the smooth functioning of a health care organization is dependent on multiple diverse teams learning and working towards a shared goal. there are several reports on improvements in the quality of healthcare and patient safety when inter-professional and team based learning have been implemented [cit] ."
"author contributions: q.l. contributed to design the algorithms, performed the experiment, analyzed the results of the experiments and wrote this paper. h.q. provided valuable feedback and advice during the manuscript modification. figure 7 depicts that the roc curve in the two chirp scenario presents a worse acquisition performance than that in the one chirp scenario. however, when the notch filter is adopted, the roc curves present an improved acquisition performance. the results depicts that the proposed method can effectively detect the one or two chirp interferences, especially for those with crossed frequency without a priori knowledge."
"these six senior residents (postgraduate trainee physicians) were the informal leaders who evaluated their practice of performing exchange transfusions basing their decisions on the results of salicylate binding sites, a test of insignificant value, which resulted in a bad outcome for a baby. they were able to persuade their colleagues to follow their unofficial protocol to prevent another tragic outcome, until quality assurance studies found the test unreliable."
"compared to previous univariate results. the ranking of the regions might lead to an easier interpretation of the weights, as well as a way of checking that the model correctly identified the variable of interest to perform the classification/regression. the developed methodology could be applied to any map: the results of sparse models in the voxel space, of multi-kernel machines, of accuracy map from the searchlight approach [cit] or using the \"source\" pattern instead of the weight map [cit] ."
"when it comes to the time-varying interference, the classical time-frequency analysis based on wvd and rspwvd provides superior performance. however, wvd suffers from cross-term interference seriously when the analytic signals have two or more components [cit] . the rspwvd can reduce the cross-term interference, but it cannot deal with the signals with crossed or overlapped frequency by the peak detection method [cit] . an example of rspwvd with two chirps whose frequencies are crossed is shown as figure 1 . figure 1a depicts that the peaks of two chirps are clear except the overlapped frequency part. in figure 1b, it is obvious that the outline of two chirps in the overlapped frequency is blurred so that it is difficult to distinguish which signal the frequency of the overlapped part belongs to. therefore, the proposed method based on hough transform of rspwvd is introduced [cit] . where j t is the sweep period, begin f is the initial frequency, and k is the frequency change rate, also named chirp rate, defined as follows"
"there is often a lag time between knowledge creation and knowledge translation in health care. the guideline movement and knowledge translation aspect of research in health care is intended to provide patients with the best evidence-based care in a timely manner. what is perceived as best patient care may change temporally and challenging the status quo and exploring alternate ways should be encouraged to provide the best possible patient care. in this example, it took almost a decade for basic research plus knowledge translation to intersect to drive a change in peptic ulcer disease treatment from cutting the vagus nerve to treating with antibiotics [cit] ."
"from the results, 1) cl trans mlp, cl trans cnn, and cl trans hbrid behave poorly, as expected. the reason might be that these models usually employ order-sensitive structures (e.g., cnns) for ed, which would suffer the word order inconsistency problem when trained on the translated data. 2) cl trans self. yields relatively good performance. the reason might be that self-attention network could provide each word with a feature vector based on all the words of a sentence, which is also irrespective of the words' positions in a sentence. this could address the word order difference to some extent. 3) our syntactic order event detector yields the best performance. while, we do not observe salient advantages by combining gcns with a self-attention network (by comparing cl trans gcn with cl trans gcn self)."
"when the doppler shift f d and the code delay τ are correctly recovered, the detection probability that the statistical variable s(τ, f d ) surpasses a fixed threshold β in the gnss acquisition stage can be defined: in order to assess the impact of chirp interference on the detection probability, it is supposed that the gnss signal and the chirp interference are known. the integration results of gnss signals in equation (24) can be written [cit] :"
"organizational learning also encompasses old knowledge and institutional memory that is held within organizations of how things were done and what the consequences were. the level of expertise and experience of nursing staff in a hospital, determines the infrastructure, functioning and learning as an organization [cit] . healthcare organizations have institutional memory which has traditionally been held by nurses and passed on verbally. some of the practical knowledge from the nursing pool was compiled into institutional policies and procedures which were updated, modified or changed as needed. nursing shortages seem to be a global phenomenon and shortage can lead to overwork, burn out and high turnover [cit] . burn out and high turnovers can negatively impact organizational learning as tacit knowledge transfer and institutional memory are compromised. as such, organizational learning has to be actively encouraged to share tacit knowledge and maintain institutional memory [cit] ."
"from the linear models leading to a significant classification (or regression), weight maps were built. the weights were then locally averaged based on labelled atlases of regions (here using the aal): for each region, we computed its \"normalized weight\" as the average of the weights (in absolute value) of the voxels comprised in that region."
"with v representing the index of a voxel in the weight image, w v its weight and m roi, the number of voxels in region roi."
"in order to assess the performance between the hough transform of wvd and rspwvd, the root mean square errors (rmse) of the rate estimation and initial frequency estimation for chirp interference are used and displayed as a function of jnr in figure 6 . the setting parameters of the experiments are the same as in figure 4 . figure 6a shows that the rmse of the chirp one rate estimated by the hough transform of wvd is the same as that estimated by the hough transform of rspwvd when jnr ranges from 0 db to 12 db. similarly, the rmse of the chirp two rate estimated by the hough transform of wvd is the same as that estimated by the hough transform of rspwvd when jnr ranges from 0 db to 12 db. in figure 6a, it is obvious that the hough transform of rspwvd as well as that of wvd provides the same rmse of the rate estimation for each chirp when jnr ranges from 0 db to 12 db. properties; the combination of rspwvd and hough transform can effectively reduce the effect of cross-term interferences. in order to assess the performance between the hough transform of wvd and rspwvd, the root mean square errors (rmse) of the rate estimation and initial frequency estimation for chirp interference are used and displayed as a function of jnr in figure 6 . the setting parameters of the experiments are the same as in figure 4 . figure 6a shows that the rmse of the chirp one rate estimated by the hough transform of wvd is the same as that estimated by the hough transform of rspwvd when jnr ranges from 0 db to 12 db. similarly, the rmse of the chirp two rate estimated by the hough transform of wvd is the same as that estimated by the hough transform of rspwvd when jnr ranges from 0 db to 12 db. in figure 6a, it is obvious that the hough transform of rspwvd as well as that of wvd provides the same rmse of the rate estimation for each chirp when jnr ranges from 0 db to 12 db. in figure 6b, the rmse of the initial frequency for chirp one estimated by the hough transform of rspwvd is close to when jnr is equal to 0 db and 2 db. however, when jnr equals or surpasses 4 db, the hough transform of rspwvd and wvd provides the same rmse of the initial frequency for chirp one. on the other hand, the rmse of the initial frequency for chirp two estimated by the hough transform of wvd is close to when jnr ranges from 0 db to 12 db. figure 6b depicts that the hough transform of rspwvd offers a poorer rmse of the initial frequency estimation for chirp one compared with the hough transform of wvd when jnr is below 4 db and provides the same rmse of the initial frequency estimation as that of wvd for chirp one when jnr equals or surpasses 4 db. for chirp two, the proposed method provides a better rmse of the initial frequency estimation compared with the hough transform of wvd."
"finally, the chirp signal is a continuous wave at any fixed time and can be mitigated by a notch filter based on the second-order direct form structure with two parameters named α and β which are determined by the power of chirp interference and the instantaneous frequency estimated by proposed method, respectively. the receiver operating characteristic (roc) curves are shown in figure 7 by monte carlo simulations to analyze the performance of the notch filter to mitigate the chirp interference under the condition where the bds b1i signals are simulated with chirp interferences. the jnr is set to 12 db and the c/n 0 is set to 43 db-hz. the integration time is set to 1 ms. the instantaneous frequency of one chirp ranges from 30 mhz to 50 mhz and the instantaneous frequency of the other ranges from 50 mhz to 30m hz. the sweep period is 2.56 us."
"using permutations, we were able to associate a p-value to the obtained distance, thereby providing significance of dr. more specifically, the labels were permuted before training, enabling the building of \"permuted\" weights maps, and hence \"permuted\" rankings. the null hypothesis here is that the labels do not contain any information about the location of the variable of interest, such that the ranking obtained from the model using the permuted labels is random. if the ranking distance between two patterns is significantly small (or large), those two patterns are then significantly (dis-) similar in terms of their localization."
"next, we retrieve translation candidates for each token w i in s. specifically, we first project w i into the aligned embedding space (i.e., by applying w on x i ), and then we explore its neighborhood to find the nearest chinese words as its translation candidates. in order to measure the distance"
"at times, systems changes can be the result of public efforts; for example the acknowledgement of the harmful effects of sleep deprivation on physicians' performance and shortening of duty hours in residency training in north america were due to a distraught father's relentless campaign following the death of his daughter from a drug prescription error [cit] ."
"the rankings obtained from different patterns could be quantitatively compared using a measure of distance, inspired from the field of web search [cit] . it is computed as:"
"this letter proposes a two-terminal active inductor concept with a minimum apparent power for the auxiliary circuit. it enables a plug-and-play solution without any additional efforts compared to a conventional passive inductor. a current control strategy is applied based on internal feedback signals from the auxiliary circuit only. the key component parameters, energy storage, and the apparent power of the auxiliary active circuit are analyzed. in the case study, the implemented active inductor achieves almost an equivalent impedance curve to a 28 mh passive inductor at the frequencies of interest, while with 27.5% rated inductive energy storage and 21% apparent power compared to the existing solutions shown in fig. 2(a) ."
"then, the signal x if (n) is multiplied by two orthogonal sinusoids as well as a local signal replica and is integrated; the results are as follows [cit] (24) where"
"innovation in health care can often come from frontline workers and other informal leaders. sister ward a frontline worker observed that sunshine decreased neonatal jaundice. she shared this observation with doctors r. h. dobbs the consultant paediatrician and cremer, the paediatric registrar, during a ward round [cit] . this observation and other incidents where blood samples kept in the sunlight had reduced levels of bilirubin (the cause of neonatal jaundice) led to collaboration with the biochemist mr. perryman and his team, where they scientifically studied the effects of sunlight on serum bilirubin and developed the phototherapy lamps, which in turn revolutionized medical care of neonatal jaundice world-wide [cit] ."
"before fmri, the subjects were asked to walk comfortably and then briskly on a 25m path. after gait evaluation, they were trained to mentally rehearse themselves walking on the path."
"a second example from bellevue hospital (new york city) illustrates a similar learning opportunity. bellvue had strict protocols for treating infants with exchange transfusions if the total serum bilirubin levels (tsb) were above 25 and basing exchange transfusions on a test called salicylate displacement tests at lower levels, illustrates the role of informal leaders in system changes [cit] ."
where θ denotes all the parameters in our model; w e ranges over each token in the translated examples and w c enumerate each token in the original chinese training set; l w e and l wc denote the ground-truth event types of w e and w c respectively. we adopt adam rules [cit] to update our model's parameters and add dropout layers to prevent over-fitting.
"2. separate the data of rspwvd x (t, f ) into two sets and map the set above the primary threshold onto the hough transform."
"this study focuses on cross-lingual ed, which aims to transfer knowledge from a source language with abundant labeled data to a target language with insufficient training data. figure 2 visualizes the overall architecture of our model, which consists of three main components: (1) monolingual embedding layer, which transforms each token into a continuous vector representation. (2) context-dependent lexical mapping, which maps each word in the source language to its best-suited translation in the target language, by examining its contextual representation and imposing a selective attention over different translation candidates. (3) shared syntactic order event detector, which employs a graph convolutional networks (gcns) to explore syntactic similarity of resources of different languages, in order to achieve multilingual co-training."
"the agc acting as an adaptive variable gain amplifier adjusts the input signal level to the analog-to-digital converter (adc) input range [cit] . therefore, the agc gain variation can be utilized to detect interference, especially continuous wave interference (cwi) and pulsed interference (pi). however, there are not enough effective quantized bits for adc to realize the gain variation range of agc for a common gnss receiver."
"exploring the syntactic order event detector. we compare our syntactic order event detector (cl trans gcn) with several event detectors, including 1) cl trans mlp, which employs a feed-forward network as event detector; 2) cl trans cnn, which uses cnns as the event detector; 3) cl trans hbrid, which use a hybrid network [cit] for event detection."
"the delivery of quality patient care and patient safety is dependent on the healthcare system in which care is provided. for example, administrative staff such as registration clerks are crucial for timely registration before patient care is initiated. patient service aids or housekeeping staff are responsible for cleaning patient rooms after discharge or transfer or the operation room after surgery, to get the rooms ready for the next patient. stores and transport system staff are crucial for logistics of equipment, dressings, linen and several other items. as such, any improvements in efficiency have to include participation and learning among clinical, nonclinical and administrative staff who work within a system. a study examining a systems approach where multiple, multilevel clinical and administrative staff were involved in redesigning processes and roles has been shown to increase organizational capacity and efficiency in hospitals [cit] . identifying and including all diverse groups involved in the delivery of services in reducing waste and improving efficiency of the system using the -lean methodology‖ been shown to increase efficiency and safety in healthcare institutions [cit] ."
"we give a case study on the cross-lingual transfer process of a real example in ace: \"36,000 people died every year from the flu\". table 4 and figure 6 gives the chinese translation candidates and the learned attention weights respectively. from the results, the best-suited translations indeed often correspond to larger attention weights, which implies the validity of our approach. in the above example, the chinese words \"从(from)\" and \"流感(flu)\" do not correspond to the nearest neighbor of \"from\" and \"flu\", but our contentdependent lexical mapping method enable us to successfully obtain them as the translations."
"another approach to organizational learning associates the organizations' ability to actively create and use knowledge with outcomes [cit] . knowledge management is an important aspect of learning in health care organizations; as new discoveries and knowledge become available professionals in their respective fields will have to acquire, transmit, retain and use that knowledge. [cit] 's surgery was the standard treatment for severe peptic ulcer disease. organizational policies and procedures were in place to ensure patient safety for these surgeries. surgeons practiced and perfected their skills to reduce operative complication and the rest of the team followed policies and procedures to ensure patient safety. the discovery that helicobacter pylori infection is the cause of peptic ulcer disease by two australian physicians changed the management for peptic ulcer disease from surgery to antibiotic therapy [cit] . as such, peptic ulcer diseases became the responsibility of general practitioners and gastroenterologists with surgical teams being consulted only on rare occasions."
"from the gnss receiver, the position accuracy and effective carrier to noise density ratio are commonly adopted to assess the impact of interference on gnss receivers [cit] . however, both of them rely on a particular gnss receiver performance. if the jamming power surpasses the spread spectrum gain of gnss causing the gnss receiver not to work, the corresponding algorithm will fail."
"the importance of organizational learning in health care systems is to provide the framework for complex interconnected dynamic systems where all operational units have to learn and execute their assigned functions to collectively improve safe patient care. policies and procedures are developed in healthcare organizations to reduce errors and improve patient safety. regulated health professional are expected to engage in continuing education to maintain and update knowledge and skills to provide safe patient healthcare as continuing education of health care professionals has shown to be related to improved patient outcomes [cit] . conversely, there is no explicit mandate to engage in continuing education for the support or administrative staff in healthcare institutions although many organizations provide and expect ongoing professional development to improve efficiency at an individual level or local level. organizational learning forms the backdrop to weaving these diverse groups and mandates into a cohesive platform to advance patient care."
event detection (ed) is a subtask defined in the overall event extraction (ee) evaluation in automatic content extraction (ace) 2005 program. we first introduce some ace terminologies to facilitate the understanding of ed task.
"exploring lexical mapping method. to explore our lexical mapping method, we compare the performance of several variant systems retrieving a different number of candidates (ranging from 1 to 5) and the embedding-projection method (embedding proj). note the system retrieving only one candidate actually takes the nearest chinese neighbor as the word translation. the lexical mapping in it is still context-independent. table 2 summarizes the results."
"healthcare organizations are composed of health care professionals from multiple disciplines forming several interconnected care teams that strive to provide safe and consistent care [cit] . the care teams have to coordinate and communicate amongst their team members and with other teams to function in a cohesive manner to execute the highly coordinated and high risk activity that is called patient care. health care organizations have to be able to modify their activities based on sudden changes in the condition of their patients or sudden demands due to public health disasters such as pandemics (severe acute respiratory syndrome, [cit], without compromising patient safety or quality of care. new knowledge creation, technology advances and other market changes can add new and unexpected demands in health care delivery. health care organizations have to maintain stability following institutional protocols but have to assess their performance and evaluate protocols to create and incorporate new knowledge."
"φ(k) is the average mean of φ(k, l), p f alse is the false alarm rate. from equation (15), the primary threshold can be determined by a predefined p f alse . then, the data in x a (t k, f l ) are separated to a new set which is higher than the primary threshold and the hough transform is applied to these data. the secondary threshold in the hough domain is used to detect the target peak and is based on the primary threshold. the p f alse is determined by both the primary threshold and secondary threshold, written as"
"selective attention figure 2 : the overview architecture of our model. the figure illustrates the process of performing cross-lingual transfer for an english sentence \"a man died when a tank fired on the hotel\" into chinese and using the shared syntactic order event detector to predict the event type for the word \"fired\"."
"in comparison to monolingual ed models. we first compare our cross-lingual approach with the existing monolingual ed models, including cnn [cit] which employs convolutional neural networks for the task and hbrid [cit] which combines cnn with recurrent neural networks (rnn) for ed. figure 5 presents the experimental results, where the number of the available chinese documents ranges from 0 to 50. from the results, our approach demonstrates a definite advantage over monolingual ed models in the annotation-poor scenario. particularly, when there is no chinese training document available (i.e., in the unsupervised cross-lingual transfer scenario), our model achieves a performance of 27.0% on f1 in ace, and 28.7% on f1 in tac kbp, while supervised ed methods completely fail. additionally, our approach can consistently outperform the embedding-projection method."
"where x a (t) is an analytical signal and x * a (t) is a complex conjugate of x a (t), f 0 is the initial frequency, k is the chirp rate. both k and f 0 can be deduced from figure 2 by a geometric relationship, the results can be written"
"-a health system consists of all organizations, people and actions whose primary intent is to promote, restore or maintain health‖ according to the world health organization who [cit] . health care systems vary by country and are financed by varying compositions of public and private sector funding. health care organizations such as hospitals and academic health centers (university affiliated teaching hospitals) form a major component of a healthcare system, regardless of the country or funding arrangements."
"a qualitative study examined the process of organizational learning in primary health care innovation in two canadian provinces studying how health care managers, physicians and other health professionals work and share knowledge. the authors conducted 170 interviews over 3 years and examined documents associated with each of the sites involved in primary health care innovation projects [cit] ."
"the authors found that organizations that were able to choose and adapt existing knowledge, experiences and processes (bricolage) to solve the problem at hand; implement supportive mechanism and create the -right kind of space‖ for learning; strengthen learning through experimenting; manage the rivalry between medical professionalism and management; and balance power differences by the local leaders were successful in facilitating both organizational learning and the spread new ideas on providing primary health care services [cit] . the authors caution that -organizational learning takes time; patience and persistence are essential. setbacks and challenges are inevitable, and finding ways to cope with them is an important factor in overall success‖ [cit] ."
"to estimate our method, we have conducted extensive experiments on two standard datasets, using english, chinese, and spanish as experimental languages respectively. the experimental results demonstrate that: 1) our model can perform cross-lingual transfer between different language pairs. especially, the improvement in chinese ed is large, with an absolute 3.8% in f1 over the monolingual approaches. 2) our model is robust in the extreme annotation-poor scenario where a language has very limited training data, which demonstrates a definite advantage over previous monolingual models. additionally, compared with mt-based methods, our model achieves competitive results but requires much less parallel resource. this paper is organized as follows: section 2 briefly introduces the task description and terminologies used in ed; section 3 elaborates details of our approach; section 4 reports on our experimental results and other analysis; section 5 reviews related work; section 6 concludes the paper and illustrates future work."
"where n(t) is the additive white gaussian noise term with two-sided power spectral density n 0 /2 and zero mean, j if (t) is the jamming signal. n s is the number of visible satellites, s if,l (t) is the signal transmitted by the lth satellite, which can be defined"
"learning from failures does not happen routinely in health care organizations and is hindered by pervasive barriers such as a lack of psychological safety and a culture of blame [cit] . leadership that accepts that failures are bound to happen, proactively develops context-specific strategies to prevent them and promote a culture of safety for admitting and reporting errors, facilitates organizational learning and consequently patient safety [cit] ."
"here, we investigated the differences in patterns between the two different centres. if the two centres were leading to similar patterns generated from the regression based on the subjects' age, the ranking distance between the two patterns should be (significantly) small. on the other hand, using a mix of data from the two centres in each regression model (half from centre 1, half from centre 2) should lead to a decrease in the ranking distance between patterns since the variability due to the centre is distributed equivalently in the two patterns."
"in this paper, we propose a new simple but effective method for cross-lingual ed, which can overcome the data scarcity problem in annotationpoor languages by jointly training with resources in other languages. compared with previous methods, our approach demonstrates a minimal dependency on parallel resources, which may fit with language pairs that do not have large bitexts."
adverse critical events such as privacy breaches or medical errors are some of the most unfortunate yet powerful reasons for health care organizations to reevaluate their practices and engage in organizational learning. [cit] . the system changes that occurred at dana-faber cancer institute following this critical incident and the ongoing organizational learning to improve safe care were reflected in a 10-year report which shows the evolution of systems thinking and safety as a system property in a healthcare organization [cit] .
"senior management and formal leaders such as -chief executive officers, presidents, and chiefs of staff‖, sets the tone for institutional learning but middle management (line managers), such as divisional chiefs and unit leaders play a more important role in encouraging and supporting practical experiments and ensuring psychological safety to acknowledge and rectify individual and system issues [cit] ."
"however, figure 4 shows that the common frequency (overlapped frequency) which belongs to chirp one as well as chirp two cannot be identified by the peak detection method. as a result, it is difficult to estimate instantaneous frequency in overlapped parts for each chirp. in order to deal with this problem, the experiment with the condition where the hough transform is combined with wvd and rspwvd is conducted; the experimental setup is as in figure 4 and the results are shown in figure 5 . however, figure 4 shows that the common frequency (overlapped frequency) which belongs to chirp one as well as chirp two cannot be identified by the peak detection method. as a result, it is difficult to estimate instantaneous frequency in overlapped parts for each chirp. in order to deal with this problem, the experiment with the condition where the hough transform is combined with wvd and rspwvd is conducted; the experimental setup is as in figure 4 and the results are shown in figure 5 . figure 5a shows the hough transform of wvd and there are three peaks in the hough domain. the peak on the right is the pseudo peak. it is a cross-term, which is integrated by hough transform in the tf plane and formed into a peak. what is shown in figure 5b is similar to that in figure 5a . although the rspwvd can reduce cross-term interferences, the power of chirps is as strong as that of noise. as a result, the rspwvd reallocates the noise energy which is integrated by the hough transform in the tf plane and is formed into a strong peak. figure 5c depicts two strong peaks and a weak pseudo peak, while in figure 5d there are only two strong peaks without a pseudo peak. in figure 5e and figure 5f, there are two peaks without a pseudo peak. figure 5 depicts that although the wvd suffers from cross-term interferences, the hough transform can help to reduce the effect of the cross-term interferences when the chirp signals are strong. in addition, the rspwvd can eliminate the cross-term interference at the expense of its good localization and concentration the peak on the right is the pseudo peak. it is a cross-term, which is integrated by hough transform in the tf plane and formed into a peak. what is shown in figure 5b is similar to that in figure 5a . although the rspwvd can reduce cross-term interferences, the power of chirps is as strong as that of noise. as a result, the rspwvd reallocates the noise energy which is integrated by the hough transform in the tf plane and is formed into a strong peak. figure 5c depicts two strong peaks and a weak pseudo peak, while in figure 5d there are only two strong peaks without a pseudo peak. in figure 5e,f, there are two peaks without a pseudo peak. figure 5 depicts that although the wvd suffers from cross-term interferences, the hough transform can help to reduce the effect of the cross-term interferences when the chirp signals are strong. in addition, the rspwvd can eliminate the cross-term interference at the expense of its good localization and concentration properties; the combination of rspwvd and hough transform can effectively reduce the effect of cross-term interferences."
"we next investigate the annotation-poor scenario, where the source language is set as english and the target language is set as chinese to compare with previous works. in this scenario, we assume that only a few of annotated documents are available in"
"similarly, if a change to improve ambulance wait times in emergency rooms (er) is mandated, the whole organization has to learn and change its practices to make this happen. for example discharge delays can affect schedules for next day surgeries as bed availability for post operative care is compromised, which in turn results in either delaying or cancelling the surgery. the ripple effect of in-patient discharge delays also impacts er bed availability as acute care beds are blocked by admitted patients. er services are delayed and wait times are increased. increased er wait times and er bed scarcity places a strain on emergency medical services (ems) who need to re-route ambulances to other hospitals. as er wait times increase, government funding may be withheld as in canada or hospital may be fined as in the united kingdom (uk) which impacts the financial health of the institution. the chain reaction of seemingly minor delays caused by late discharges affects the various internal and external arms of the health care system. targeted quality improvement initiatives such as discharge planning schedules are prime candidates for organizational learning."
"\"you're australian, and you were working against a pretty strong north american medical group. did that play into it?\" barry marshall: -yes, and no. if i'd discovered the initial findings in the united states, i might have just discounted them. there's a very structured and very conventional gastroenterology program in the united states. if your head's just full of that conventional learning (50% of which is incorrect), it's very difficult to get a new concept in‖ [cit] ."
"in ace, 1) an event mention refers to a phrase/sentence within which an event is described. 2) event trigger refers to a specific word in an event mention which is considered the most representative of the event. each event trigger has a certain type corresponding to the event mention."
"currently, as our approach is predicated on the availability of syntax trees of training examples, it might not fit with languages which lack syntactic parsers. in the future, we plan to investigate more language-independent patterns in crosslingual transfer to circumvent this dependency."
"the signals received from an antenna of a gnss receiver pass through the radio frequency (rf) front-end and are down-converted to intermediate frequency (if) . before the adc, they can be written as"
"the roc curves have been used to assess the performance of the notch filter to mitigate chirp interference by monte carlo simulations. the results depict that the proposed method can effectively detect and identify the chirp interferences with crossed frequency and provide the same root mean square errors (rmse) of the parameter estimation for chirp one and the improved initial frequency estimation for chirp two compared with the hough transform of wvd when jnr equals or surpasses 4 db. furthermore, the rspwvd method itself can provide better performance in reducing cross-term interference and needs less computational requirements compared with the wvd method."
"to test the performance of the introduced method, an experiment under the condition where the bds b1i signals are corrupted by two simulation chirps is carried out for several scenarios. the main hardware parameters of the down converter and adc are recorded in table 1 . first, the jnr is set to 0 db, 6 db and 12 db. the instantaneous frequency of one chirp interference is from 50 mhz to 30 mhz, and the instantaneous frequency of the other chirp interference is from 30 mhz to 50 mhz. the sweep period is 2.56 us and the sample length is 512. the hough transforms combined by wvd and rspwvd are shown as figure 4 . although the power of chirps is 12 db above that of noise, the wvd of chirps in figure 4e still suffers from cross-term interference, while in figure 4f, the outline of rspwvd is very clear and its energy distribution hardly suffers from cross-term interference. figure 4a shows that the wvd of two chirps suffers from cross-term interferences seriously due to the interaction of different chirp signal components; the outline of two chirps are not clear. figure 4b depicts the rspwvd of two chirps, the outline of which are clear. the cross-terms are less than those in figure 4a . figure 4c presents the wvd of chirps with a higher jnr and its energy distribution is clear and suffers from less cross-term interferences compared with figure 4a . figure 4d represents a clear rspwvd which suffers from fewer cross-term interferences compared with that in figure 4c . although the power of chirps is 12 db above that of noise, the wvd of chirps in figure 4e still suffers from cross-term interference, while in figure 4f, the outline of rspwvd is very clear and its energy distribution hardly suffers from cross-term interference."
"event detection (ed) is a crucial natural language processing problem that aims to identify event triggers in texts [cit] . for example, in the sentence: \"a man died when a tank fired on the hotel\", ed requires a system to identify two event triggers, died and fired, along with their types die and attack."
"where tfh x ( f 0, k) is the tf distribution of the analytical signal x a (t). from equation (10), the chirp interference in the tf plane is written as polar coordinates as well as cartesian coordinates. therefore, equation (11) can be written as"
"the performance of the developed method has been assessed by experiments under conditions where the real bds b1i signals corrupted by the simulated chirp interferences are collected by the gnss software receiver. the actual performance of experiments has been shown by the quantitative metric rmse of the parameter estimation. in addition, the effect of the sweep period on the estimation of the initial frequency and chirp rate has been analyzed."
"our model incorporates a logistic regression classifier to predict w i 's event type. specifically, we compute a prediction vector for w i by taking f i as the input:"
"the underlying idea of label propagation is that the connected nodes in the graph tend to share the same sentiment labels. in bilingual word graph label propagation, the words tend to share same sentiment labels if they are connected by synonym relations or word alignment and tend to belong to different sentiment labels if connected by antonym relations."
"the boxplots of figures 8, 9 and 10 show the results of the simulated 2k-point asymmetric algorithm. the upper-side boxplots are for a two user-eight radios each (k is 8) case. the number of channel varies from 11 to 101. the number of common channels (w) is one in figure 8, three in figure 9 and 10 in figure 10 . the common channels are selected randomly. the non-common channels are also selected randomly, without repetition. the bottom-side boxplots are for a four user-eight radios each (k is 8) case. the number of channel also varies from 11 to 101. the number of common channels (w) is also one, three or 10. the band inside each boxplot corresponds to the median. the ettr, according to theorem 6, is shown. the simulation results match well the theoretical performance, in particular for the three and ten common channel cases. figure 11 plots the simulation average ttr for the k-point symmetric algorithm with multiple users, from two to 64. each user has 16 radios. the number of channels varies from 11 to 101. the number of common channels (w) is 10. for multiple user and multiple radio cases, the simulation results are better than what is predicted by the theory. indeed, there are opportunities for rendezvous across pairs that are not captured by theorem 6. for the multiple user case, we hypothesize function"
"bsl lf: the words in existing chinese sentiment lexicons are used as features. a total of 836 positive words and 1,254 negative words are collected from hownet. 9 we use libsvm 10 and perform 10-fold cross-validation on the ntcir polarity sentences. the accuracies over n number of features are plotted in figure 7 . our approach achieves a very promising improvement, although the features and the sentences that need to be classified are selected from different corpora. this suggests that the generated sentiment lexicon is adaptive and qualitative enough for sentiment classification."
"traditional group cell analysis has been widely used in microbiology and biotechnology. it studies information based on the averages of large cell populations. however, the information obtained by this method is often incomplete and misleading. it can report only the average values for the population. it is not capable of determining the characteristics or contributions of individual cells [cit] . also, group cell analysis requires many cell samples and a great deal of time. recently, single cell analysis has received significant attention and it will in future contribute to more detailed information about the individual cell [cit] ."
"we first validated if this fbp could indeed faithfully report aba levels in planta. to do this, we assayed the bioluminescent signal produced three days after infiltration into n. benthamiana leaves that were treated with increasing concentrations of aba. as expected, we observed increasing bioluminescence signal with increasing doses of aba ( figure 4b )."
"based on the second method mentioned in section 4.1, we succeed in replacing the whole solution inside the microfluidic channel after the fabrication of the microgear. 100% pegda (200) was injected into the pdms microfluidic channel. the microgear was fabricated by illuminating the uv for 0.2 s via the 100x objective lens on the pdms surface and then transported to the glass surface, as presented in section 3.1."
"for the k-point algorithm, figure 4 plots the ettr for 10 to 100 channels and two, four, eight and 16 radios. here is why the k-point algorithm worst case performance is better than the 2k-point algorithm worst case performance. in the 2k-point case, given two pairs of coupled radios, one pair from each user, the two distances can be both odd and require crossovers in both cases before rendezvous is achieved. in the k-point case, because m is assumed to be odd, given two pairs of coupled radios, if the distance is odd for one pair (crossover required), it is even for the other pair (no crossover required)."
"we present a generic approach to automatically learning sentiment lexicons for the target language with the available sentiment lexicon in english, and we formalize the cross-lingual sentiment learning task on a bilingual word graph."
"the on-chip laser manipulation and fabrication systems are shown in figure 2 . the laser (ytterbium fibre laser, wavelength 1064 nm) for the optical tweezers was built into an inverted microscope (ix-71, olympus). the x-y stage and the height of the objective lens (z-axis) were controlled for manipulating the objects. several objective lenses were used, including the oil immersion objective uplfln 100xoi2 (olympus, for laser manipulation and on-chip fabrication) and the air objective 40x (olympus, for on-chip fabrication). the laser was scanned on the x-y coordinate by controlling the angle of the galvanometer mirror (lsa-10a-30, harmonic drive systems). uv was illuminated by the mercury lamp (ush-1030l, ushio) and the exposure time was controlled by the shutter (bsh-rix, sigmakoki). the microfluidic device was set on the stage. the observation was performed by the ccd camera (xc-555, sony)."
"in this article, we studied the task of cross-lingual sentiment lexicon learning. we built a bilingual word graph with the words in two languages and connected them with the inter-language and intra-language relations. we proposed a bilingual word graph label propagation approach to transduce the sentiment information from english sentiment words to the words in the target language. the synonym and antonym relations among the words in the same languages are leveraged to build the intra-language relations. word alignment derived from a large parallel corpus is used to build the inter-language relations. experiments on chinese sentiment lexicon learning demonstrate the effectiveness of the proposed approach. there are three main conclusions from this work. first, the bilingual word graph is suitable for sentiment information transfer and the proposed approach can iteratively improve the precision of the generated sentiment lexicon. second, building the inter-language relations with the large parallel corpus can significantly improve the coverage. third, by incorporating the antonym relations into the bilingual word graph, the blp approach can achieve an improvement in precision. in the future, we will explore the opportunity of expanding or generating the sentiment lexicons for multiple languages by bootstrapping."
"the transient expression assays for s. lycopersicum and a. thaliana were carried out using the agrobest protocol 15 . briefly, seeds were sterilized and germinated in 0.5x ms and once cotyledons emerged, the seedlings were co-cultured with the appropriate agrobacterium strain that had been transformed with the t-dna encoding an fbp, in a mixture of ms and ab-mes. for all tomato agrobest treatments fbp_6 was used (table 1) . for the arabidopsis agrobest fbp_12 and fbp_11 (table 1) were used. these strains were primed for agrobest the previous day through overnight culture in ab-mes salts according to the agrobest protocol. visualization of the bioluminescence signal was performed after two days of coculture."
". d e and d t are the degree matrices of the synonym intra-language relations w e and w t, respectively. we then define the distance function to indicate that if two words are connected by the antonym relation they tend to belong to different sentiment labels. the distance function can be defined as"
"as shown in figure 2, a mask made of pet (polyethylene terephthalate) was set in front of the shutter. the patterns on the mask were able to be shaped arbitrarily. uv was illuminated through the mask and the objective lens into the pegda (molecular weight 200, polysciences) inside the channel of the pdms (silpot 184 w/c, dow corning toray) microfluidic devices. the uv was patterned by the masks. the photocrosslinkable resin was polymerized and the arbitrarily shaped microstructures were fabricated inside the solution."
sop: [cit] present a method to predict the semantic orientation of unlabeled words based on the mean hitting time to the two sets of sentiment seed words.
"we present two rendezvous algorithms, with a bidirectional behavior, for users that have 2k radios each, where k is a non-null positive integer. the algorithms differ in the way they are initialized. in the first algorithm, 2k-point algorithm, each user picks 2k random starting channels. in the second algorithm, k-point algorithm, each user chooses k random starting channels. let m be the number of available channels (assumed to be an odd number without loss of generality). the performance of our algorithms is determined by the time-to rendezvous (ttr) measure. we study first the two-user symmetric case, which means that the two users share exactly the same channel set. for the two-user symmetric case, the first algorithm achieves worst case performance in m − 1 time slots. the second algorithm achieves worst case performance in (m − 1)/2 time slots, but at the expense of an additional constraint, i.e., users must start running the algorithm at the same time slot. their expected performance is almost the same, time slots, asymptotically in k. next we study, the more general multiple-user asymmetric case, which means that users may hold different channels sets, but with at least one channel in common. equations modeling the worst case performance and expected performance are developed for all cases."
"after the fabrication of the movable and immovable microstructures designed as shown in fig 16, a surrounding wall similar as that mentioned in section 4.2 was fabricated on the glass surface, with the microstructures inside. however, there are small gaps of about 7 μm in the wall. the cells are able to go inside the wall, while the microstructures cannot escape from the wall through these gaps during the change of the solution."
"show the moving and rotational manipulations, respectively. all the manipulation experiments were conducted using a laser power of 1 w. the moving speed inside the ypd was about three times as large as that inside the pegda, and the rotational speed was about 2.5 times as large. the results demonstrate the improved laser manipulation inside the ypd culture medium."
"rule: for the intra-language relation, this approach assumes that the synonyms of a positive (negative) word are always positive (negative), and the antonyms of a positive (negative) word are always negative (positive). for the inter-language relation, we regard the chinese word aligned to positive (negative) english words as positive (negative). if a word connects to both positive and negative english words, we regard it as objective. based on this heuristic, we generate two sets of sentiment words."
"the analysis in theorem 2 implicitly assumes that both users start running the algorithm at the same time slot. this assumption simplifies the analysis. it is not required. although, in the following algorithm the same assumption is present and required. the reward is a significant improvement in the worst case performance, (m − 1)/2 steps, for the algorithm 2, versus m − 1 steps for algorithm 1."
"abstract in this paper, we present the fabrication and assembly of microstructures inside a microfluidic device based on a photocrosslinkable resin and optical tweezers. we also report a method of solution replacement inside the microfluidic channel in order to improve the manipulation performance and apply the assembled microstructures for single cell cultivation. by the illumination of patterned ultraviolet (uv) through a microscope, microstructures of arbitrary shape were fabricated by the photocrosslinkable resin inside a microfluidic channel. based on the microfluidic channel with both glass and polydimethylsiloxane (pdms) surfaces, immovable and movable microstructures were fabricated and manipulated. the microstructures were fabricated at the desired places and manipulated by the optical tweezers. a rotational microstructure including a microgear and a rotation axis was assembled and rotated in demonstrating this technique. the improved laser manipulation of microstructures was achieved based on the on-chip solution replacement method. the manipulation speed of the microstructures increased when the viscosity of the solvent decreased. the movement efficiency of the fabricated microstructures inside the lower viscosity solvent was evaluated and compared with those microstructures inside the former high viscosity solvent. a novel cell cage was fabricated and the cultivation of a single yeast cell (w303) was demonstrated in the cell cage, inside the microfluidic device."
"where r is the distance between the laser trapping point and the rotation centre, and l is the distance between the viscous resistance acting point and the rotation centre. the above theoretical analysis is a simplified model for the qualitative analysis, which contributes to the following experiments for improving the laser manipulation by adjusting the fluid viscosity η. the microgear with six teeth shown in figure 9 was used in the experiment. the laser trapping field was formed into an arbitrary pattern by the high-speed scanning of a single laser with a galvanometer mirror [cit] . therefore, multiple points were able to be trapped. here, a single laser was formed into three points and the fabricated microgear was rotated as shown in figure 9 . the rotation diameter was 15.8 μm. the laser trapping points were at the tip of the teeth and the rotational manipulation was performed by trapping these teeth. the experiment was done inside 100% pegda (molecular weight 200, with a 6% polymeric initiator, polysciences/shin-nakamura chemical co., ltd). by releasing the negative pressure of the outlet after the pegda injection, all the experiments were done inside the solution at a flow-rate of zero. the experimental results show that the rotation speed is almost proportional to the laser power. the relationship between the rotation speed and laser power is shown in section 4.3, in figure 15 (the line represents the gear inside 100% pegda 200). the rotation speed was about 5 rpm when the laser power was 2 w. this rotation speed is low, and the reason for this is the high viscous resistance. if the resistance can be reduced, it is possible to achieve a motion with a higher speed in the microfluidic device, which is important for single cell handling and analysis."
"theorem 6. let n be the number of users, an integer greater than one. using the 2k-point-asymmetric algorithm (algorithm 3), all users rendezvous in at most log 2 n · (m − w + 1) · (m − 1) steps and with a log 2 n · proof. the multiplier log 2 n comes from the fact that in each round the number of entities to rendezvous is divided by two."
"opportunistic spectrum access is a cognitive radio approach. it works on the assumption that certain radio bands are allocated to a primary service (e.g., television) and a secondary service (e.g., computer networks). there are primary users and secondary users. a secondary user cannot cause harmful interference to the transmissions of a primary user. opportunistic spectrum usage is a medium access model for secondary users. primary users may access the wireless medium anytime. secondary users must always monitor activities of primary users. they can only use residual air time. secondary users must relinquish channels to primary users when the latter become active. spectrum utilization can be improved by opportunistically transmitting in spectrum holes. an important question is: where are the spectrum holes? there are two approaches for finding them: database and sensing. in the database approach [cit], secondary users query a database to find channels that are available for their operation. this approach requires a databaseserver infrastructure and a communication protocol between the secondary users and servers. in the sensing approach, secondary users observe the spectrum. they uncover unoccupied channels."
"in planta substrate-independent bioluminescence promises to be a powerful reporter technology, as it enables long-term characterization of gene expression on a macro scale in a cost-effective manner. the approach also does not suffer from the issues associated with current bioluminescent reporters, such as the challenges of substrate application and non-uniform substrate penetration. to validate that the pathway could be used to study spatio-temporal patterns of gene expression, we built a version of the pathway in which the expression of luz was driven by the promoter of the odorant1 (odo1) gene from petunia 16, 17, 18 ( figure 3b ). this gene has been previously characterized with a firefly luciferasebased reporter. its expression was only observed in the flowers of petunia and was shown to be diurnal, peaking in the evening at the transition of day to night 16 . we used agrobacterium infiltration to deliver two versions of fbp to petunia flower petals: one with luz expressed from the odo1 promoter (podo1) and the other with luz expressed from the constitutive 35s promoter. we then performed time course imaging of detached flowers. we observed an increase in luminescence in the evening at the transition between day and night in the flowers treated with the podo1:luz, a trend not seen in the 35s:luz control ( figure 3a )."
"to solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. specifically, we model this task with a bilingual word graph, which is composed of two intra-language subgraphs and an interlanguage subgraph. the intra-language subgraphs are used to model the semantic relations among the words in the same languages. when building them, we incorporate both synonym and antonym word relations in a novel manner, represented by positive and negative sign weights in the subgraphs, respectively. these two intra-language subgraphs are then connected by the inter-language subgraph. we propose bilingual word graph label propagation (blp), which simultaneously takes the inter-language relations and the intra-language relations into account in an iterative way. moreover, we leverage the word alignment information derived from a parallel corpus to build the inter-language relations. we connect two words from different languages that are aligned to each other in a parallel sentence pair. taking advantage of a large parallel corpus, this approach significantly improves the coverage of the generated sentiment lexicon. the experimental results on chinese sentiment lexicon learning show the effectiveness of the proposed approach in terms of both precision and recall. we further evaluate the impact of the learned sentiment lexicon on sentence-level sentiment classification. when using words in the learned sentiment lexicon as features for sentiment classification of the target language, the sentiment classification can achieve a high performance."
"when we imaged n. benthamiana leaves three days after delivery of the fbp via agrobacterium infiltration, we see significant bioluminescence over background at the site of infiltration ( figure 1b,c,d), demonstrating the pathway can produce bioluminescence using natively synthesized caffeic acid. upon comparing luminescence from leaves that had the complete pathway versus controls in which enzymes from the pathway were missing, we only observe a bioluminescence signal with the complete pathway ( figure 1d, supplement figure 1 ). this indicates that there are no native enzymes expressed at functional levels in n. benthamiana leaves which could complement the functions of the fbp enzymes. we also observe no bioluminescence from confluent cultures of agrobacterium that carry the pathway (supplementary figure 1), implying the observed bioluminescent signal is not from expression of this pathway in agrobacterium."
". we conclude that rendezvous is accomplished within at most m−1 2 steps. we look at the expected number of time slots. as a function of the initial distance d j in edges, let h j be a random variable measuring the number of steps until rendezvous. the previous discussion shows that the following identity holds true:"
"for the time lapse data collected in figures 3a,c and in figure 4c, experiments were performed using the nightowl lb 983 in vivo imaging system 16 . in all cases agrobacterium infiltration of the fbp into the desired tissue was performed as described previously. for the petunia time lapse imaging data displayed in figure 3a and c, the petals of flowers were infiltrated and then the flowers were excised from the plants either one or two days after infiltrations and were mounted in the imaging platform with their pedicles immersed in a solution of 5% sucrose 16 . images were then captured every hour with a tenminute exposure. long day light conditions were implemented in the times between images. for the data displayed in figure 4c, leaves of n. benthamiana were infiltrated and then excised from the plant after three days and placed in petri dishes. for the watered conditions a moistened filter paper was added to the petri dish, whereas for the drought conditions the filter paper was left dry. images were then captured every hour with a ten-minute exposure and the timepoint with the peak prab18:luz signal was reported."
"in the sequel, we present two rendezvous algorithms for two users equipped with 2k radios each, where k is a positive integer. the algorithms differ in the way they are initialized. in the first algorithm, 2k random starting channels are chosen. in the second algorithm, each user chooses k random starting"
"we leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. we build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results."
"for example, one single english word typically has four spanish or italian word forms (two each for gender and for number) and many russian word forms (due to gender, number, and case distinctions) [cit] . usually, this approach requires an additional process to disambiguate the sentiment polarities of all the morphological variants."
"where η is the viscosity of the solution, a is a constant that depends on the dimensions of the object, and (dv/dy) is the derivative of the fluid speed in the direction perpendicular to the object. the moment for rotating the microgear is calculated by:"
"the viscosity of pegda (molecular weight 200, with 6% polymeric initiator, polysciences /shin-nakamura chemical co., ltd) is 24 mpa･s/25℃. the viscosity of ethanol (99.5 vol%, amakasu chemical industries) is 1.09 mpa･s/25℃. the reason why we choose ethanol is that pegda (200) does not dissolve inside water but does dissolve in ethanol. by mixing 100% pegda with 99.5% ethanol, which has a much lower viscosity, the viscosity decreases. for this mixed solution, a lower concentration of pegda means a lower viscosity, which means a lower viscous resistance."
"the wild-type yeast cell (w303) was used for demonstrating single cell cultivation [cit] . inside the microfluidic channel, the solution was replaced by the ypd culture medium mixed with yeast cells. after immobilizing the cells inside the cages -by assembling the movable part with the immovable part -the ypd culture medium was continuously supplied from the inlet of the microfluidic channel in conducting the long-term cell culture. after 12 hours cultivation, divisions of the yeast cells were observed. besides this, cell cages of different sizes were fabricated based on different masks. more cells were able to be cultured inside the bigger cell cage. as shown in figure 19, the smaller cage (a1, b1) (diameter 15 μm) may not have enough space for four cells, while the bigger cage (a2, b2) (diameter 20 μm) has. four cells were obtained with the bigger cell cage. based on these results, long-term (12 hours) single cell cultivation and cell samples divided from one cell were demonstrated."
"based on this method, a series of microgears were fabricated inside different concentrations of pegda. the concentrations were based on volume. as shown in figure 10, the shape of the microgear becomes more transparent as the concentration of pegda decreases. the reason for this is that the shape of the microstructure is determined by the amount of photocrosslinkable material inside the solvent. for the laser trapping force, the trap efficiency of the microstructure is determined by its shape and refractive index. based on equation (1), the shape of the microstructure influences the refraction index, which in turn influences the laser trapping force. in terms of the theories above, while the viscous resistance decreases, the laser trapping force also decreases. based on equation (3), it is barely apparent that the manipulation performance (the rotation moment) can be improved. the experiment results of the microgear fabricated and rotated inside different concentrations of pegda are shown in figure 11 . they demonstrate that during a decrease in the concentration, the rotational speed of the microgear cannot increase invariably."
"to be able to network together, secondary users meet and agree on one common channel. in the sequel, it is assumed that the secondary users are synchronous. time is divided in slots of equal length. a rendezvous occurs within one time slot. there are two conditions for a successful rendezvous: a successful protocol handshake and being on the same channel during a time slot. these two conditions can be considered separately. they can be modeled individually and independently. the probability of a successful rendezvous is the product of the probability of a successful protocol handshake and probability of being on the same channel during a time slot. the focus of this paper is on the latter aspect. we address the problem of finding a com-mon channel by secondary users, on which they can network. the problem of finding and selecting a common channel can be approached using either a central controller, a dedicated common control channel or a distributed blind rendezvous technique. a blind rendezvous technique may use channel scanning. each secondary user scans a set of channels looking for a rendezvous with a peer. participating users may all have a common channel set, in the symmetric case, or a different, but non disjoint channel set, in the asymmetric case. the goal is to make the secondary users rendezvous on a common channel in a minimum number of time slots."
"we represent the words in english and in the target language as the nodes of the bilingual word graph. we use the synonym and antonym relations of the words in the same language to build w and w in the intra-language graph, respectively. in the rest of this section, we will focus on how to build the inter-language relation."
"there are four cases to consider. in all cases, the number of steps is not greater than m − 1. we conclude that rendezvous is accomplished within at most either m − 1 steps or m time slots."
"the factor 1/m 2k counts the number of possible functions pairs, i.e., f 1 and f 2, where each function is from a domain of cardinal k to a co-domain of cardinal m. in the summation, the first two multiplicands count the number of subsets of cardinal i, chosen form a set of cardinal m, times the number of injective functions from a domain of cardinal k to a co-domain of cardinal i. the last two multiplicands count the number of subsets of cardinal j, chosen form a set of cardinal m − i, times the number of injective functions from a domain of cardinal k to a co-domain of cardinal j. the summation counts the total number of surjective functions pair f 1 and f 2 with empty co-domain intersection. therefore, we conclude that the probability of a successful rendezvous is"
"each boxplot describes the statistical dispersion of the data. for each value of number of channels, the ranked data is divided into four equal groups. each group, comprises a quarter of the data. they are delimited by three values called quartiles. the box bottom indicates the first quartile. the boxed horizontal bar corresponds to the second quartile, i.e., the median. the box top indicates the third quartile. the lowest bar corresponds to the lowest datum still within 1.5 of the interquartile range (i.e., difference between the second and first quartiles) down of the first quartile. the highest bar corresponds to the highest datum still within 1.5 the interquartile range (i.e., difference between the third and second quartiles) up of the third quartile. crosses correspond to extremities, i.e., outliars. the ettr of the k-point model discussed in section 4 predicts results slightly better than the simulation results."
in this report we sought to reconstitute the fbp in planta and to test if natively produced caffeic acid could be used to generate bioluminescence. we also tested whether the fbp functioned across plant species and if it could be used to study the spatiotemporal patterns of gene expression. in this work we describe a toolkit of resources to easily generate fbp-based reporters to study gene expression in planta. we also demonstrate how this toolkit could be deployed to generate programable autoluminescence patterns in planta and to build plant-based biosensors with luminescent outputs that do not require external substrate addition.
"in order to demonstrate the value of the solution replacement inside the microchannel, we evaluated the rotation speed of the microgears shown in figure 12 . laser tweezers trapped three teeth of the microgear with three trapping points. by releasing the negative pressure of the outlet, all the experiments were done inside the solution at a flow-rate of zero. figure 14 shows the different experiment images between the microgears inside 100% pegda (200) and 99.5% ethanol at the same laser power of 1 w. the rotational speed inside 100% pegda (200) was about 2.5 rpm while inside 99.5% ethanol it was about 32 rpm."
"inter-language relation with word alignment is used to build the graph. that means w e, w e, w t, and w t are defined as zero matrixes."
"the problem addressed specifically in this paper is enabling communications for a group of secondary users by making rendezvous on an available channel. we assume that each secondary user scans the set of channels, attempting to make rendezvous with other secondary users. time is divided into equal length intervals called time slots. during one time slot, each user is tuned into one or several channels, simultaneously. two or several users make rendezvous when they are all tuned into a common and same channel during a time slot. most of the research works conducted so far on this problem assume a single radio per user. we assume that each user has several radios that are concurrently used to achieve rendezvous with other users. with the current software-defined radio technology, multi-radio operation is perfectly doable."
"with programmatically controlled long exposures and a tripod stabilizer, the dslr camera provides the best of both worlds: high resolution and detail for macroscopic plant subjects under multiple light conditions, and sufficient sensitivity to capture even low levels of bioluminescence signal. we do observe higher levels of thermodynamic noise than using cooled ccd camera setups, but this is easily filtered away using opensource software such as imagej 22 . an example for comparison is given in supplementary figure 3, where the same periwinkle petal infiltration was imaged under the ccd camera (supplementary figure 3a) and then the dslr (supplementary figure 3b) . the dslr captured high resolution, true color images under both the light and dark conditions, while showing similar light sensitivity to the reporter."
video figure 1. time lapse imaging was performed with our raspberrypi controlled dslr imaging platform to visualize aba induced fbp expression in plants exposed to drought stress. we infiltrated one leaf each on two n. benthamiana plants with fbps that had either a 35s (+control) or prab18 driven luz.
"the learned chinese sentiment word lists are also evaluated with precision at k. as shown in figure 5, we find that the alignment-based approach outperforms the dictionary-based and mt-based approaches. the reason that contributes to this is that we can build more inter-language relations based on word alignment, compared with the translation entries from the dictionary and the translation pairs from google translator. for example, the english word move is often translated to (shift) and (affect, touch) by dictionaries or mt engines. from the parallel sentences, besides these word translation pairs, the word move can be also aligned to (plain sailing bon voyage) that is commonly used in chinese greeting texts. this translation entry is hard to find in dictionaries or by mt engines. the words are aligned between the two parallel sentences. sometimes the word move may be forced to be aligned to in the parallel sentences good luck and best wishes on your career move and . thus, when building the inter-language relations with word alignment, our approach is likely to learn more sentiment word candidates. it is also the reason why the dictionary-based and mt-based approaches learn fewer sentiment words than our approach, as indicated in table 4 . according to our statistic, on average a chinese word is connected to 2.3 and 2.1 english words if we build the inter-language relations with the dictionary and google translator, respectively. by building the inter-language relation with word alignment, our approach connects a chinese word to 16.21 english words an average, which greatly increases the coverage of the learned sentiment lexicon."
"before applying the movable parts of the cages for yeast cells, the evaluation of the laser manipulation of the movable parts was conducted, inside both pegda (200) and the ypd culture medium (5% ypd broth, becton dickinson and company). figure 17 (a)(c) and (b)(d)"
"the rotational microstructure, including a rotation axis and a microgear, was assembled as an example to demonstrate the assembly technique. a movable microstructure -a microgear -was fabricated at the pdms area, and then relocated to the glass area by laser manipulation. secondly, an immovable microstructurea rotation axis -was fabricated in the hole of the microgear. the microgear and the rotation axis were fabricated by illuminating the uv for 0.2 s via the 100x oil immersion objective lens (olympus). the fabricated rotation axis was fixed and the microgear was able to rotate on it. the designs of these microgears are shown in figure 7 . the assembled rotational structures are shown in figure 8 . based on this assembly method, various functional structures for single cell analysis are able to be constructed, such as actuators, cell sorters and cell cages."
"for quantification of luminescence signal, imagej 22 was used to box areas of the same size in images of infiltrated leaves or 96 well plates containing hole punches from infiltrated leaves and then the average signal intensity was recorded and plotted in python using the seaborn 28 plotting package. all p values reported were calculated using the t-test function in the scipy package. all the raw data and data analysis code were made available on github (https://github.com/craftykraken/p4)."
"secondly, a square wall was fabricated and fixed on the glass surface with a movable microgear inside. next, 99.5% ethanol was introduced through the inlet of the channel. with the negative pressure applied at the outlet, the pegda solvent inside the microchannel was continuously replaced by the ethanol solvent from the inlet. the side length of the wall was about 500 μm. as the top surface of the channel was pdms, the square wall had an open top, which granted space for replacing the solvent inside the wall. the thickness of the space was about 2 μm. by calculating the volume of pegda and ethanol, we indicated that the solution inside the microchannel changed to 99.5% ethanol after a certain time. figure 13 shows the observation results of the microgears through an optical microscope inside 100% pegda (200) and 99.5% ethanol solvent. these two images are obviously different because of the total replacement of the solution."
"the inter-language relation is built upon the translation entries from ldc 7 and universal dictionary (ud). 8 from these dictionaries (both english-chinese and chinese-english dictionaries), we collect 41,034 translation entries between the english and chinese words. if the english word x i can be translated to the chinese word x j in ud dictionary, w a (i, j) and w a (j, i) are set to 1."
"based on our transient expression assays, the fbp can function in a broad range of plant species. with the recent development of rapid morphogen mediated plant transgenesis protocols 24, we believe fbpbased biosensors and reporters will be easy to extend across a range of plants for long term visualization of gene expression. this extensibility to a broad range of plants makes this approach to autoluminescence superior to the existing transplastomic approach 11, as plastid transformation is challenging and largely limited to tobacco and a few related solanaceae 25 ."
"as commonly used semi-supervised approaches, label propagation [cit] and its variants [cit] have been applied to many applications, such as part-of-speech tagging [cit], image annotation [cit], protein function prediction [cit], and so forth."
"the power of bioluminescence as a tool to study dynamic signals in biology, such as aba accumulation during drought, is challenging to access in lower resource settings due to high costs of substrate and instrumentation. the substrate independence of the fbp makes it more economical than other bioluminescent systems. however, the high cost and small chamber size of commercially available luminescence imaging setups make running multiple experiments in parallel cost prohibitive. the cost of such commercial systems can exceed $100,000usd. to address this challenge, we leveraged a wealth of open-source software and off-the-shelf hardware components to design and build a relatively low cost, modular platform for bioluminescence imaging. our platform uses a raspberry pi single board computer as a controller for a dslr camera, outfitted with a macro lens and mounted in either a dark room or pop-up plant growth tent. the code used for controlling customized still and time-course imaging with this platform, as well as the image processing pipeline, is available on github ( table 2 ). the entire cost comes to less than $2,500, the bulk of which is for the camera and lens; substituting an entry-level dslr and kit lens would bring the cost below $2,000."
"two methods for reducing the viscosity of the solution are presented. the first one is to adjust the concentration of the pegda by mixing it with other low viscosity liquids, such as ethanol, and then fabricating the microstructures with this mixed solution. the second one is to replace the whole solution inside the channel with a low viscosity liquid after the fabrication of the microstructures. the biggest distinction between them is that the first method is performed before fabricating the microstructure while the second one is performed afterwards. this section describes the first method."
"besides the substrate costs, the high cost of commercially available bioluminescence imaging systems restrict access to luminescence-based reporters. we demonstrate how commercially available photography equipment and open source software can be used to set up relatively low-cost, modular and programmable luminescence imaging platforms. this coupled with the substrate independence of the fbp will broaden access to these tools to lower resource settings as well as enable scalable application of these reporters. we hope the work presented here will serve as a first step for the creation of novel bioluminescence-based tools in plants for basic science discovery and synthetic biology enabled applications."
"we have addressed the problem of secondary users with multiple radios making rendezvous on any of the available channels. we have introduced two bidirectional algorithms, the 2k-point and k-point procedures. we have addressed the two-user case, with a common channel set (the symmetric condition) or with different but overlapping channel sets (the asymmetric condition). the results have been generalized to an arbitrary number of secondary users. equations have been developed to model the worst case and expected performance. a performance model has also been developed empirically using simulation data. homogeneous secondary users have been assumed, i.e., they all have the same number of radios. this assumption is present for the purposes of the analysis. the algorithms work also in the heterogeneous case, where secondary users might have different numbers of radios."
"blp-ac: word alignment and the chinese wordnet synsets are used to build the intra-chinese relation w t, but the intra-english relation w e and w e are set to zero matrixes."
", initialize µ and ρ 1∼4 output: f t for x t and f e for x e 1. initialize y e with the english sentiment seeds 2. set y t as zero 3. calculate s e, s e, s t, s t, and s a, then calculate m e and m t 4. loop 5. f"
"for all static ccd camera-based luciferase imaging eight minute exposures of plant tissue were taken in using a uvp bioimaging systems epichemi3 darkroom. for transgenic plants a twelve minute exposure was used. paired bright field images were also taken using the same camera. the brightness and contrast of the long exposure images were adjusted using imagej 22 to optemize signal visibility and then overlaid as a false colored image on the bright field image, where warmer colors correspond to higher signals."
"as mentioned in section 2.2, movable and immovable microstructures are able to be fabricated on different surfaces, such as glass and pdms. a unique microfluidic device that has both glass and pdms surfaces on the bottom of the channel was fabricated, as shown in figure 5 . the photo resist (az 5214-e, clariant (japan) k.k.) was used as the sacrificial layer. the patterned az layer on the glass was made by the photolithography method. the pdms layer was spin-coated on the top. after the pdms was solidified, the az layer was removed and only the pdms on the glass remained. the precise pdms coating area was constructed. the side view of the device is shown in figure 6 . the microstructures fabricated on the pdms surface are movable and those fabricated on the glass are fixed. therefore, a novel on-chip assembly method for microstructures was developed."
"from this result, the rotation speed of the microgear inside 99.5% ethanol is approximately 11 times as large as that inside 100% pegda (200). this improvement of manipulation is obvious, considering with the change in viscosity. the manipulation speed increased because the viscous resistance for the movement was weaker inside the lower viscosity solvent. the laser manipulation performance for the microgear was improved. however, there are other factors influencing the improvement of the manipulation, such as velocity and shape [cit] . we will study this more in future works. furthermore, this solution replacement method is important in applying the system for cells. it provides an approach of on-chip changing the pegda to water or other low-viscosity solutions which are also suitable for cell living and culture. the fabricated microstructures can be used for manipulating cells via the laser manipulation system."
one plant was allowed to desiccate while the other was kept watered. bioluminescence in the leaves as then imaged over three days. the treatments of the leaves are labeled with white legends.
we are interested in calculating the expected number of steps until rendezvous by at least one radio pair. rendezvous is accomplished when the first radio pair (one from each user) meets at a channel of the ring. therefore the expected number of steps until rendezvous does not exceed the following quantity
"the procedure for replacing the solution is shown in figure 12 . firstly, we adjusted the uv expose position where the microgear was in the centre and then exposed the uv through the mask with the square pattern."
"the results presented here demonstrate how the fbp can convert a common plant metabolite, caffeic acid, into a luciferin and turn it over to produce robust luminescence in planta, without the addition of any additional substrate. we go on to show how the fbp can be deployed as a useful addition to the bioluminescent reporter toolbox in plants. as it emits in the green spectrum 13, it should be spectrally separable from firefly luciferase, allowing it to be applied in a complementary manner to the reporters that already exist. our observation that the incorporation of the recycling pathway prolongs the autoluminescent signal implies further tuning this pathway might be an avenue to enable long term imaging without substrate depletion in stable lines. while this may explain why we observe stronger autoluminescence in infiltrated tissue than in the stable transgenic line, it might also be due to the overexpression of the enzymes due to the delivery of multiple t-dnas. thus, further optimizing the expression of the enzymes in the fbp might be another avenue to increase the auto-luminescent signal. this reengineering can be easily implemented in the future with our fbp toolkit, thanks to the easy promoter and terminator swapping enabled by using the moclo system 14 ."
"the data we present on using the prab18 driven luz to track drought stress via aba signaling serves as a demonstration for how the fbp could be used to build biosensors for internal or external signals by driving genes in the pathway with synthetic promoters. in the future, stable lines generated with this construct could be deployed to track soil moisture levels at field scales. these biosensors would have an output more easily visualized than fluorescent reporters, have a high signal to noise ratio, and be cost effective due to their substrate independence. thanks to the ease of promoter swapping in our fbp toolkit, this approach could be easily extended to build biosensors for a range of different phytohormones, environmental cues like light, or synthetic signaling systems for basic science and translational applications."
"we presented the fabrication and assembly of microstructures inside a microfluidic device based on a photocrosslinkable resin and optical tweezers. we also reported a solution replacement method inside a microfluidic channel to improve the manipulation performance and apply the assembled microstructures for single cell cultivation. microstructures of arbitrary shapes were fabricated by photocrosslinkable resin inside a microfluidic channel. by replacing the solution components, the manipulation speed of the rotational microstructure increased when the viscosity of the solvent decreased. the rotation speed of the microgear inside 99.5% ethanol was approximately 11 times greater than that inside 100% pegda. we demonstrated the availability of the microstructures for manipulation at a higher speed inside the microfluidic device. a novel cell cage was fabricated and the long-term cultivation of a single yeast cell (w303) was demonstrated in the cage, inside the microfluidic device. such on-chip fabrication, assembly and solution replacement will contribute to the creation of microfluidic devices for single cell analysis with many more functions."
"where p ′ is the transpose of the matrix p. the graph laplacians s e and s t of the synonym intra-language relations are (i − d (6) and (7), we can obtain the optimal solutions"
"the work on cross-lingual sentiment lexicon learning is still at an early stage and can be categorized into two types, according to how they bridge the words in two languages. [cit] generate sentiment lexicon for romanian by directly translating the english sentiment words into romanian through bilingual english-romanian dictionaries. when confronting multiword translations, they translate the multiwords word by word. then the validated translations must occur at least three times on the web. [cit] learns sentiment words based on english wordnet and wordnets in the target languages (e.g., hindi and arabic). crosslingual dictionaries are used to connect the words in two languages and the polarity of a given word is determined by the average hitting time from the word to the english sentiment word set. these approaches connect words in two languages based on crosslingual dictionaries. the main concern of these approaches is the effect of morphological inflection (i.e., a word may be mapped to multiple words in cross-lingual dictionaries)."
"where d e and d t are the degree matrices of the absolute value of the antonym intralanguage relations w e and w t, respectively. intuitively, for the inter-language smoothness and the synonym intra-language smoothness, the nearer the words connect with each other, the better performance will be achieved, whereas for the antonym intralanguage distance, the farther the better. the objective functions can be defined as"
"blp: this approach is based on the bilingual word graph. it incorporates the interlanguage relation w a, the synonym intra-language relations w e and w t, and the antonym intra-language relations w e and w t . in these approaches, µ is set to 0.1 [cit] . the precision of these approaches are shown in figure 3 . the figure shows that the approaches based on the bilingual word graph significantly outperform the one based on the monolingual word graph. the bilingual word graph can bring in more word relations and accelerate the sentiment propagation. besides, in the bilingual word graph, the english sentiment seed words can continually provide accurate sentiment information. thus we observe the increase in the approaches based on the bilingual word graph in term of both precision and recall (table 2) . meanwhile, we find that adding the antonym relation in the bilingual word graph slightly enhances precision in top-ranked words and similar findings are observed in our later experiments. it appears that the antonym relations depict word relations in a more accurate way and can refine the word sentiment scores more precisely. however, the synonym relation and word alignment relation dominate, whereas the antonym relation accounts for only a small percentage of the graph. it is hard for the antonym relation to introduce new relations into the graph and thus it cannot help to further improve recall."
"the various fbp encoding t-dna constructs that were characterized in this work were built using a twostep process. first base plasmids containing an expression cassette was built for each of the five enzymes in the fbp. these base plasmids were designed to have promoters for either constitutive or tissue/time-period specific expression of the fbp using a mix of promoters that were either from the moclo toolkit 14 or amplified from genomic or plasmid dna with primers that added the appropriate bsai sites. the atrab18 and phodo1 promoters were amplified from published plasmids that were obtained from either addgene or via requests from the authors. terminators were chosen from the moclo kit to ensure high expression 14 . dna sequences encoding the enzymes were designed to be codon optimized for n. benthamiana. additionally, all the common type-iis restriction sites were removed via codon swaps. these sequences were synthesized by twist biosciences. all these parts were assembled into the base vectors with a bsai-based goldengate assembly reaction. the base vector backbones were designed to contain the appropriate aari sites to be assembled together into either four or five expression cassettes containing t-dnas through an aari-based goldengate assembly reaction 26 . all vectors listed in table 1 are available via addgene or upon request."
"our experiments thus far implied that we could create auto-luminescent plants by genomically integrating expression cassettes for the three enzymes in the biosynthesis pathway, namely npga, h3h, and hisps, the recycling pathway, cph, and the fungal luciferase, luz. we used the fbp characterized in figure 1 to generate a stable transgenic line of n. benthamiana. when we characterized the luminescence on rooting medium ( figure 1f ) and in soil ( figure 1g ), we observed auto-luminescence across the plant. we see stronger signals in the root tips and shoot apical meristem, consistent with a higher density of cells in these tissues. similar patterns of expression are observed in root tips of arabidopsis thaliana with constitutively expressed firefly luciferase 8 . it is therefore possible to generate plants with genomically integrated fpbs to create stable auto-luminescence."
"blp-a: as the baseline of this set of experiments, it does not build the intralanguage relations with either english or chinese wordnet synsets. only the table 4 influence on recall of the inter-language relation."
"in this article, we propose to leverage a large bilingual parallel corpus, which is readily available in the mt research community, to build the bilingual word graph. the parallel corpus consists of a large number of parallel sentence pairs from two different languages that have been used as the foundation of the state-of-the-art statistical mt engines. like the example shown in figure 2, the two sentences in english and chinese are parallel sentences, which express the same meaning in different languages. we can easily derive the word alignment from the sentence pairs, automatically using a stateof-the-art toolkit, like giza++ 4 or berkeleyaligner. 5 in this example, the chinese word (happy) is linked to the english word happy and we say that these two words are aligned. similarly the english words best and wishes are both aligned to (wish). the word alignment information encodes the rich association information between the words from the two languages. we are therefore motivated to leverage the parallel corpus and word alignment to build the bilingual word graph for cross-lingual sentiment lexicon learning. we take the words from both languages in the bilingual parallel corpus as the nodes in the bilingual word graph, and build the inter-language relations by connecting the two words that are aligned together in a sentence pair from a parallel corpus. there are several advantages of using a parallel corpus to build the inter-language subgraph. first, large parallel corpora are extensively used for training statistical mt engines and can be easily reused in our task. the parallel sentence pairs are usually automatically collected and mined from the web. as a result, they contain the different and practical variations of words and phrases embedded in sentiment expressions. second, the parallel corpus can be dynamically changed when necessary because it is relatively easy to collect from the web. consequentially, the novel sentiment information inferred from the parallel corpus can easily update the existing sentiment lexicons. these advantages can greatly improve the coverage of the generated sentiment lexicon, as demonstrated later in our experiments."
"the demand for wireless continues to grow. wireless traffic is increasing. devices, such as smart phones and tablets, are numerous and bandwidth hungry. the numbers of wireless users, devices and applications are all booming as occupants of some of the segments of the radio spectrum. radio spectrum is a limited natural resource. lack of available radio spectrum is an issue with respect to the introduction of new applications. indeed, the radio spectrum from nine khz to 275 ghz has been entirely allocated to various services. in theory, there is no room for new services and accommodating growth. this is dubbed the spectrum crunch problem. nevertheless, not all allocated bandwidth is currently being intensively used. for instance, a limited number of frequencies allocated to television, space exploration and defense are occupied every-time, everywhere. measurement experiments observed a remarkably low usage of the radio spectrum. for example, [cit] have concluded that over 80% of the allocated spectrum is unused. the cognitive radio paradigm aims at improving the radio spectrum usage efficiency and support of the expected growth of wireless traffic."
"3 [cit] . they manually produce two high-level gold-standard sentiment lexicons for two languages (e.g., english and spanish) and then translate them into the third language (e.g., italian) via google translator. they believe that those words in the third language that appear in both translation lists are likely to be sentiment words. these approaches connect the words in two languages based on mt engines. the main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by mt engines [cit] . the shortcoming of these mt-based approaches inevitably leads to low coverage."
"hormone signals coordinate diverse aspects of plant metabolism and development by carrying information from cell to cell across tissues and organ systems in the plant. firefly luciferase-based reporters have been an invaluable tool to study how hormone fluxes can trigger developmental changes in the plant 8 . however, the challenges of substrate cost, application and penetration make using them across the entire plant body or over long periods problematic. the substrate independence of the fbp makes it an attractive alternative to overcome these challenges. to demonstrate how this pathway could be engineered to build a hormone biosensor we built a version of the pathway with the atrab18 promoter (prab18) driving expression of luz ( figure 4a ). this promoter has been previously characterized as having strong and selective increase in expression in response to the plant hormone abscisic acid (aba) 20 ."
"additionally, thanks to its water-soluble luciferin substrate, firefly luciferase can be used to visualize live changes in gene expression in plants in a non-invasive manner, theoretically enabling whole plant time lapse imaging 8 . however, current bioluminescent reporters do present some significant challenges which has limited their broad application for macro scale visualization of gene expression. one major challenge is uniform delivery and penetration of the luciferin substrate, especially in adult plant tissues. some luciferins like coelenterazine, the substrate for renilla luciferase, are non-water soluble and so cannot be used for imaging without cell-lysis 9 . the luciferin for firefly luciferase, d-luciferin, is water soluble and can be topically applied to whole plants or delivered through watering 10 . however, uniform substrate delivery is challenging to achieve, which makes it difficult to disambiguate whether a lack of bioluminescent signal is due to a lack of expression of the luciferase or delivery of the substrate."
"three cognitive radio paradigms have been identified [cit], namely, underlay, overlay and interweave. they refer to the model of spectrum usage by secondary users with respect to primary users. in the underlay model, secondary users are allowed to transmit until interference created to primary users remains below a threshold [cit] . in the overlay model, because of their transmission technique, secondary user transmissions have no impact on the performance of primary users. in the interweave model, secondary users detect non-occupied spectrum segments and use them to communicate. the work presented in this paper falls into the interweave category."
"we next tested if this system could respond to endogenous aba signals. the prab18 driven fbp was delivered to the leaves of n. benthamiana via agrobacterium infiltration. one set of leaves was allowed to desiccate, while the other set was kept moist. the bioluminescence of the leaves was then imaged over time. we expected that as the leaves dry out, they would begin to produce aba 21, leading to an increase in luminescence. we observed a significant increase in luminescence in desiccating leaves, up to levels of an fbp with a 35s:luz ( figure 4c ). we did not see an increase in signal in leaves that were kept moist ( figure 4c,d). these results demonstrate the utility of the fungal luciferase as a tool to enable substrate independent visualization of hormone fluxes in planta. they also serve as a proof of concept for how this pathway could be deployed in stable plant lines as a biosensor for soil moisture levels in the future."
"in this section, we handle the asymmetric and multiple user cases. under the asymmetric model, the secondary users have different sets of available channels. if the sets are overlapping, which we assume, they can eventually make rendezvous. the goal is to make the secondary users rendezvous on a common channel in a minimum amount of time."
"in general, the work on sentiment lexicon learning focuses mainly on english and can be categorized as co-occurrence-based approaches [cit] and semantic-based approaches [cit] . the co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. the statistical information is mainly derived from certain corpora. [cit] assumes that the conjunction words can convey the polarity relation of the two words they connect. for example, the conjunction word and tends to link two words with the same polarity, whereas the conjunction word but is likely to link two words with opposite polarities. their approach only considers adjectives, not nouns or verbs, and it is unable to extract adjectives that are not conjoined by conjunctions. [cit] define several pattern templates and extract sentiment words by two bootstrapping approaches. [cit] calculate the pointwise mutual information (pmi) of a given word with positive and negative sets of sentiment words. the sentiment polarity of the word is determined by average pmi values of the positive and negative sets. to obtain pmi, they provide queries (consisting of the given word and the sentiment word) to the search engine. the number of hits and the position (if the given word is near the sentiment word) are used to estimate the association of the given word to the sentiment word. [cit] research sentiment word learning on customer reviews and they assume that the sentiment words tend to be correlated with product features. the frequent nouns and noun phrases are treated as product features. then they extract the adjective words as sentiment words from those sentences that contain one or more product features. this approach may work on a product review corpus, where one product feature may frequently appear. but for other corpora, like news articles, this approach may not be effective. [cit] combine sentiment lexicon learning and opinion target extraction. a double propagation approach is proposed to learn sentiment words and to extract opinion targets simultaneously, based on eight manually defined rules."
"we build a bilingual word graph by using synonym and antonym word relations and propose a bilingual word graph label propagation approach, which effectively leverages the inter-language relations and both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning."
"simulations have been conducted in the omnet++ environment [cit] . the boxplots of figures 6 and 7 show the performance of the simulated kpoint algorithm. along the x-axis, the number of channels (m) varies from 41 to 201. the y-axis corresponds to the ttr. the analytic ettrs for the rps [cit], k-point and randomized algorithms are also plotted. the goal is to compare the simulation results with the ettr of the models discussed in sections 2, 3 and 4."
"in contrast to all-in-one tabletop chamber systems, our setup with a dark room or pop-up tent 23 permits both much larger plant subjects and flexible positioning of the camera relative to the subject. for example, we took images of flower petal infiltrations on intact rose bushes a meter in size. these features make our platform a compelling alternative to commercially available systems at a fraction of the cost. we used this platform to capture high-resolution images of various plant subjects ( figure 2b, figure 3d,e), and to perform time-course imaging for an fbp under an atrab18 drought-inducible promoter ( figure 4d ). these results illustrate how our platform, in conjunction with the fbp, enable low cost bioluminescence reporting for dynamic biological phenomena."
"on-chip fabrication and assembly comprise a feasible way to conduct cell immobilization and cultivation. we also report on the on-chip solution replacement method inside the microfluidic channel. the solution replacement reduced the viscosity of the solvent and improved the manipulation performance of the microstructures. higher speed laser manipulation was obtained and demonstrated by rotating a microgear. the solution replacement also provided the environment control method for the single cell cultivation inside the microfluidic channel. a novel cell cage was assembled with both movable and immovable microstructures and the cultivation of a single yeast cell (w303) was conducted in the cell cage, inside the microfluidic device. the entire system developed shows strong potential for application in single cell analysis."
we thank prof. toshifumi inada at tohoku university for providing us with the yeast wild-type strain w303. this work was partially supported by mext kakenhi (hyper bio assembler for 3d cellular systems (bioassembler)) and nagoya university global coe programme (coe for education and research of micro-nano mechatronics).
"intuitively, there are two ways to connect the words in two languages. one is to insert links to the words if there exist entry mappings between the words in bilingual dictionaries (e.g., the english-chinese dictionary). this method is simple and straightforward, but it suffers from two limitations. (1) dictionaries are static during a certain period, whereas the sentiment lexicon evolves over time. (2) the entries in dictionaries tend to be the expressions of formal and written languages, but people prefer using colloquial language in expressing their sentiments or opinions on-line. these limitations lead to the low coverage of the links from english to the target language. an alternative way is to use an mt engine as a black box to build the inter-language relation. one can send each word in english to a publicly available mt engine and get the translations in the target language. edges can then be inserted into the graph between the english words and their corresponding translations. this approach suffers from the problem of low coverage as well because mt engines tend to use a small set of vocabularies [cit] ."
"while n. benthamiana is an excellent model to prototype the fbp, we hypothesized that it should, in principle, work in any plant capable of producing caffeic acid, as long as the appropriate promoters and terminators were used to ensure expression of the pathway enzymes. to test this hypothesis, we performed transient agrobacterium-based delivery of the fbp into the model plant a. thaliana and the crop plant solanum lycopersicum (tomato) using the agrobest protocol 15 . we observed robust autoluminescence signal over background in the cotyledons of treated seedlings (figure 2a,b). we also performed agrobacterium infiltrations of the leaves of the ornamental plant, dahlia, and were able to observe auto-luminescence ( figure 2c ). finally, we explored if the fbp was functional in petals, as luminescence of this tissue has applications for horticulture as well as engineering plant pollinator interactions. agrobacterium infiltrations with the fbp of petals from catharanthus roseus (periwinkle), petunia axillaris, and three different varieties of rosa rubiginosa (rose) resulted auto-luminescence in all flowers with a range of intensities ( figure 2d,e, figure 3a ). we also observed that the signal dropped off within a day of the flowers being detached from the plant body, whereas it persisted when the flowers remained connected to the plant body. this might mean some of the pathway substrates or cofactors might be trafficked from source tissues; however, more tests are required to confirm this hypothesis. these results demonstrate that the fbp can function across a wide range of plants."
"in section 2, we review related work. we study a randomized algorithm in section 3. the bidirectional algorithm exploiting several radios per user is described in sections 4 and 5. we compare the theoretical performance of our algorithms with related algorithms in section 6. simulation results are presented in section 7. we conclude with section 8."
"bioluminescence is one of nature's more spectacular tricks. it is used by a diverse set of organisms to achieve a broad range of goals such as attracting mates, scaring off predators and recruiting other creatures to spread spores 1, 2, 3, 4, 5 . the mechanism of light emission is broadly conserved: an enzymatic oxidation reaction by a luciferase enzyme turns a luciferin substrate into a high energy intermediate which decays to produce light 1 . there have been several different examples of luciferin substrateluciferase enzyme pairs described to date 6, 7 . researchers have leveraged some, such as the firefly and renilla luciferase enzymes to build reporters to study gene expression in plants and other eukaryotic systems 8, 9 . these reporters have a high signal to noise ratio because there is effectively no background signal produced by plants."
bsl df: the chinese word unigrams and bigrams are extracted from the ntcir data set as features. we rank the features according to their frequencies and gradually increase the value of n for the top-n classification features.
"visualizing multiple diurnal oscillations in flowers is technically challenging, as the auto-luminescence signal only lasts approximately one day after the flower is separated from the plant body, a necessity for mounting it in the imaging platform. to overcome this limitation, we infiltrated two sets of flowers 24 hours apart and then harvested tissue and began time course imaging on the same day. in this way we were able to quantify luminescence signal from the podo1:luz over a two-day period post infiltration. we observed the expected luminescence signal peak at the transition between day and night both one and two days after infiltration ( figure 3c ), with a diminishing signal over time, which can be explained by the gradual loss of t-dna expression over time 19 combined with gradual senescence of the detached flower. these results demonstrate that the fbp can be used in a similar fashion to firefly luciferase to study spatio-temporal patterns of gene expression without necessitating substrate addition. these experiments also serve as a proof-of-concept that, by using an appropriate set of promoters to drive the pathway genes, an fbp could be programmed to generate specific spatio-temporal patterns of autoluminescence in stable transgenic lines for synthetic biology applications."
blp-woa (bilingual word graph without antonym): this approach is based on the bilingual word graph. it only involves the inter-language relation w a and the synonym intra-language relations w e and w t . w e and w t are set to be zero.
"then solution inside the microfluidic channel was changed from 100% pegda (200) to 99.5% ethanol, first, and then water, and finally the culture medium of ypd liquid mixed with the cells. the viscosity of the pure water was 0.9 mpa･s/25℃. the microstructures inside the different solutions are shown in figure 18 and figure 19 ."
"we now look at the expected number of time slots. let h j be a random variable measuring the number of steps until rendezvous, as a function of the initial distance d j in edges. the previous discussion shows that the following identity is valid:"
"a sentiment lexicon is regarded as the most valuable resource for sentiment analysis [cit], and lays the groundwork of much sentiment analysis research, for example, sentiment classification [cit] and opinion summarization [cit] . to avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. the existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus [cit] or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., wordnet) [cit] . however, current work mainly focuses on english sentiment lexicon generation or expansion, while sentiment lexicon learning for other languages has not been well studied."
"in this article, we address the issue of cross-lingual sentiment lexicon learning, which aims to generate sentiment lexicons for a non-english language (hereafter referred to as \"the target language\") with the help of the available english sentiment lexicons. the underlying motivation of this task is to leverage the existing english sentiment lexicons and substantial linguistic resources to label the sentiment polarities of the words in the target language. to this end, we need an approach to transferring the sentiment information from english words to the words in the target language. the few existing approaches first build word relations between english and the target language. then, based on the word relation and english sentiment seed words, they determine the sentiment polarities of the words in the target language. in these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. two approaches are often used to connect the words in different languages in the literature. one is based on translation entries in cross-lingual dictionaries [cit] . the other relies on a machine translation (mt) engine as a black box to translate the sentiment words in english to the target language [cit] ). [cit] tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language."
"referring to the scenario depicted in figures 2 and 3, let us assume that channel four is not available to user 1, all others are. there are no changes for user 2. in these conditions, rendezvous on channel four fails. the users repeatedly crossover and never make rendezvous. this cyclic behavior needs to be broken. in this example, the initial distance is even. an analogous behavior is obtained when the initial distance is odd."
"word alignment and the english wordnet synsets are used to build the intra-english relation w e, but the intra-chinese relation w t and w t are set to zero matrixes."
"the on-chip solution replacement does not only realize the higher speed manipulation of microstructures, but also provides a novel technique to change and control the cell culture environment inside the microfluidic devices. there are many potential applications, such as long-term single cell cultivation, in applying this solution replacement method. single cell cultivation is very useful for obtaining cell samples with identical characteristics. it is important and significant for single cell analysis. for conducting single cell cultivation, a novel cell cage is presented, as shown in figure 16 . it contains both movable and immovable parts. the size of the cage is about 20 μm which is suitable for the yeast cells. based on the assembly and fabrication method for movable and immovable microstructures -as mentioned in section 3it is possible to manipulate the cells by movable microstructures and immobilize the cells by immovable microstructures. the assembled cages shown in this figure are able to immobilize cells for longer than 24 hours during which the culture medium has been changing inside the microfluidic channel. by changing the solution from pegda to water and a culture medium, it is possible to conduct long-term cultivation for an immobilized cell inside the cage."
"when all channels are available, with the 2k-point algorithm (algorithm 1) we know that rendezvous does not require more than m − 1 steps. let c i,2j−1 and c i,2j be the start channels of radio 0 of user 1 and radio 1 of user 2. after the completion of m − 1 steps, let the new start channel of radio 0 be (c i,2j−1 + m − 1) mod m. let the new start channel of radio 1 be c i,2j + 1 mod m. for the figures 2 and 3 scenario, it means that after visiting channel four, radio 1 jumps to channel two. after visiting channel four, radio 0 does channel four again. after one step, they crossover again. after three more steps, they finally meet on channel three. if the channel happens to be also not available to both, then this procedure is repeatedly applied until rendezvous is made."
"we tried to reduce the viscosity of the solution inside the microfluidic channel. based on equation (2), a lower viscosity means a lower resistance for the movement of microstructures. therefore, the improvement of the laser manipulation's performance would be expected with any decrease in the viscosity of the solution inside the microfluidic channel."
petal infiltrations of c. roseus flowers with an fbp imaged three days post infiltration with either our dlsr-based imaging system (a) or a ccd based imaging system (b).
"in order to apply a microstructure to a microfluidic device, it is important to understand the manipulation performance of the microstructure. in this section, the microgear is evaluated by measuring the rotation speeds under different laser powers."
"where f is the laser trapping force, q is the laser trapping efficiency, n1 is the refraction index of an object for the solvent, p is the laser power and c is the velocity of light. while the laser power decreases gradually, the laser trapping force decreases. as the fluid resistance acts on the microstructure, the laser trapping points with a constant rotation speed will depart from the trapped points of the microgear. just before the moment of departure, the laser power was recorded and it was considered as the appropriate power level for the rotation speed used. based on this result, we demonstrated the relationship between the rotation speed of microgear and the correct laser power. the fluid resistance, which acts on the object in the direction of movement, is related with the viscosity. the viscosity -also called 'dynamic viscosity' -represents the internal friction coefficient, which determines the resistance for the object moving inside the liquid [cit] . the viscous resistance is expressed as the following equation:"
"the semantic-based approaches determine the sentiment polarity of a given word according to the word semantic relation, like the synonyms of sentiment seed words. the word semantic relation is usually obtained from dictionaries, for example, wordnet."
"both experimental results (inside pegda or ethanol) for the rotation speeds were almost proportional to the laser power, as shown in figure 15 . the approximation polynomial of the microgear inside 100% pegda (200) is expressed by the following equation:"
"we assume that each user has two or more radios that can be used simultaneously to achieve rendezvous. from a quantitative point of view, fair performance comparisons are possible with algorithms also making this assumption. to the best of our knowledge not much work has been published in that direction. [cit] have conducted research in that direction, the role-based parallel sequence (rps) algorithm. at the end, we have four comparable algorithms: rps, randomized, 2k-point and k-point. the ettr is an evaluation metric applicable to all. let p be the smallest primer number greater than or equal to m. let k be the number of radios. assuming, a homogeneous cognitive radio network, i.e., all nodes have the same number of radios, for the rps algorithm the ettr (also the maximum ttr) is for the rps algorithm, figure 5 plots the ettr for 10 to 100 channels and two, four, eight and 16 radios. the ettr for the randomized and k-point algorithms are shown in figures 1 and 4 . note that k-point is slightly better than 2k-point (see theorems 2 and 3). they are both better than rps. the randomized algorithm does better than all, when the number of radios is greater than four."
"with respect to the equation of theorem 6, note the presence of term 2k-point figure 9 : boxplots depicting the performance results obtained with the omnet++ simulations of the 2k-point-asymmetric algorithm. upper-side boxplots are for the two userthree channels in common case (k is 8). bottom-side boxplots are for the four user-three channels in common case (k is 8). curves depict the analytic ettr for the 2k-pointasymmetric algorithm. log 2 log 2 (n) in the denominator. figure 12 shows, for the 64 user case, the simulation average ttr, ettr according to theorem 6 and ettr according to equation (15) . the number of common channels (w) is 10. we ran the kolmogorov-smirnov goodness-of-fit statistical test [cit] on the sample data and equation (15) . the statistical test yields to acceptance of the null hypothesis that the distribution follows equation (15), at the 5% level of significance."
the passengers taking a flexible flight accept arriving at one of several airports in a great metropolitan area without caring about the specific destination airport.
"in this research our algorithm generates the optimized route for a flexible flight while another team works on the runway scheduling based on mctma. the airport/runway balancer is designed particularly for the metroplex destination to facilitate mctma with flexible flights scheduling. the idea of their algorithm is to consider all the runways resource from all the airports in the same metroplex together. since the regular destination-fixed flights already take most of the runway time slots, the airport/runway balancer checks the available time slots (holes) of all the runways for flexible flights. once there is a slot available in any one or more runways of a metroplex airport, the approaching flexible flight will be scheduled to the closest runway. the airport/runway balancer can access mctma through the interfaces shown in fig. 9 . fig. 9 also shows the six steps of a complete metroplex runway scheduling."
"in the sections that follow, we introduce the atc21s project, which is the context of our study, and review the literature on ict literacy, culminating in a section on the inception of the atc21s ict literacy effort. we then give a very brief description of the bear assessment system mentioned above, as that is used as the organizing principle for the next four sections: the construct map, the items design, the outcome space, and the measurement model. this is followed by a description of the key results from an empirical study of the assessments using a sample of students from four countries: australia, finland, singapore and the united states. the paper finishes with a discussion of conclusions and next steps."
"the construct maps. levels in the atc21s learning in digital communities learning progression framework, as described below, were developed using the bas approach described in the previous section, and draw substantively from the literature noted above. the levels in each strand follow a similar valuing, starting first with awareness and basic use of tools, and then building to more complex applications. evaluative and judgmental skills improve. leadership and the ability to manage and create new approaches emerge as the more foundational skills are mastered."
"the atc21s methodology group has described that, in order to achieve a working hypothesis of such a complex domain, one a priori approach by which known information is incorporated is to describe \"dimensions of progression,\" or theoretical maps of intended constructs, in terms of depth, breadth and how the skills change as they mature for students . for this, the atc21s project set up an expert panel of ict experts who turned to the research literature to inform expanded definitions of digital literacy."
"experimental results show that the processing time can be reduced by increasing patch size while reducing the search area. from fig. 10, an intuitive explanation is that when we reduce the search area, more candidate patches overlap with the object region. in this scenario, the bigger the patches are, the more overlap there will be. as a result, the amount of required computations becomes smaller. thus, as we increase the patch size and reduce the search area, the processing time decreases. equation (9) shows that for bigger search factor α, the term (2α + 1)"
"extract initial bo objects, and set confidence m fig. 5 algorithm workflow of the exemplar-based object removal algorithm. please note that one opencl block might be mapped to one or multiple opencl kernels depending on the computation and memory data access patterns. (red green blue alpha) format, we use efficient vector data structures such as cl uchar4 to take advantage of built-in vector functions of opencl."
"two of the three scenarios were selected for empirical study with students. the two, the science/math arctic trek collaboration contest and the webspiration shared literature analysis task, were identified by participating countries as the most desirable to study at this time. the science/math and language arts activities were more aligned with the school systems in the countries, which rarely used anything like cross-country chat tools in the classroom, but sometimes did employ math simulations and online scientific documents as well as graphical and drawing tools for student use. in particular, the third task (the second language chat) was described by participating countries, teachers, and schools as a forward-looking, intriguing scenario, but farther away on the adoption curve for school-based technology."
"the profiling and experiments are performed on a development platform consisting of a qualcomm snapdragon 8974 chipset [cit], which supports opencl embedded profile for both cpu and gpu. the details of the experimental setup are listed in table 1 ."
"first, ict literacy was seen as measurement of a discrete set of knowledge about computers and their use, coalescing into the concept of ict literacy in the early years of the field. second, ict literacy was seen as measuring a broad set of skills that have links to many traditional and non-traditional school subjects, incorporating the move to technology integration in education. third, ict literacy was seen as measuring a set of progress variables, which are essential tools for the design of curriculum and assessments in terms of competencies. the \"competencies\" view depicts the need to understand initial ict knowledge likely to emerge followed by a developing picture of mastery. in this perspective, students are acknowledged as not being one-size-fits-all in ict literacy but moving toward increasing competency in their virtual skills, knowledge, competency, awareness, and use. fourth, the measurement is seen as needing to reflect the \"network\" perspective on ict-the critical need for building the power of virtual skills through proficiency with networks of people, information, tools, and resources. [cit] . this paper goes beyond that introduction and presents an initial analysis of the results from a pilot test looking across the multiple dimensions of the construct."
"for a sample of 103 students in our first field test (i.e., those for whom we were able to match their login ids for two scenarios), assessment results from the two scenarios within the overall ict literacy domain were analyzed using a three separate consecutive unidimensional item response (rcml) models."
"in the second simulation, the set contains 35 images (examples labelled as 'original' in fig. 4a-c), which aims to simulate imaging of general scenes and some examples for a 6% sampling ratio are shown in fig. 4a-c . figure 4d illustrates the comparison results for this set. the 'russian dolls' result still shows the same characteristic at the sampling ratios of 50%, 25%, 12%, 6% and 3%, where the relative errors exhibit local minima. however, in this simulation, the performance of the 'russian dolls' approach is diminished due to the absence of a uniform dark background, as this maximises the effect of a perfect reconstruction. the evolutionary compressive sensing outperforms the other two approaches, however this is in the limit of optimal a priori information (we know all values of s i ) and any real application does not have this luxury. conventional compressive sensing performs better only when the sampling ratio is small and still requires computationally intensive reconstruction."
"by contrast, a few students showed behaviors across settings that would propel them into the upper category of discriminating consumer proficiencies. these students were showing leadership ability with their ict collaboration skills. they could not only question but also judge credibility of sources. they often were able to integrate information into larger views of coherent knowledge, construct searches suited to personal circumstances, and then filter, manage, and organize information much more effectively than other students. because they had a tendency to seek expert knowledge and select optimal tools, their collaboration skills and their teams benefitted. groups in which they engaged might take leaps forward based on such participants. a notion of how to combine teams so that they represented a range of support and incorporated peer learning would probably be helpful to support the wide range of skills."
"the aim of this work is to first introduce the concepts of metroplex and flexible flight and to illustrate the dependency between the airports in the same metroplex. secondly, we present the jetway plus network model and the metroplex routing algorithm which considers the sector congestion as the constraint. the routing is followed by a mctma scheduler solving the runway assignment problem. to our best knowledge, this is the first work that studies the metroplex routing under the sector congestion constraint. in addition, the algorithm implementation is explicitly shown step by step and the simulation for one flexible flight is visualized by using facet interfaces [cit] ."
"some groups have begun to establish standards of practice in learning analytics for such 21st century complex data analysis methodologies [cit] . in this paper, we will present an example that helps establish the coherent evidentiary argument for the learning analytics involved through a framework called a \"learning progression.\" this framework connects the results to (a) the data and the learning analytics? questions being asked, and (b) to the techniques for the analytics employed. other researchers have begun to describe the need for such frameworks when learning analytics goes beyond data analysis alone and is to be used for predictive analytics, actionable intelligence, and decision-making [cit] . in learning analytics, the need to establish a coherent evidentiary argument to support claims about learners can be approached either a priori (in advance of the analysis) or a posteriori (following the analysis). the a priori approach is essentially a theoretical approach, based on a strong theory or prior empirical information (or both), and thus might be considered a confirmatory learning analytic technique."
"to introduce sector congestion constraint, the edges that intersect those congested sectors will be assigned with an extra huge weight. the congested sectors or the congested areas in a sector are depicted as polygons in this paper in order to easily check the intersections with the edges in jetway plus network. therefore the resulted shortest path represents the shortest weather conflict free route from origin to destination."
"with rapid growth of air traffic, the airports in a metropolitan area can not be considered as separated entities, but rather as subsystems of a larger, interdependent system. we call such a system of airports a metroplex. \"concept of operations for the next generation air transportation system\" [cit] from the joint planning and development office (jpdo) defines a metroplex as a group of two or more adjacent airports whose arrival and departure operations are highly interdependent."
"observation formats included numerous innovative types [cit] . the data set included process data from activities in the tasks, collaboration data from chat logs and other activities, explanations and argumentation from students, and answers to technology-enhanced selected response queries of various types. some examples are illustrated in the upcoming figures, when each task is introduced."
"the dependencies and impacts of the three major airports jfk, lga and ewr in nynj metroplex were analysed by delaurentis and ayyalasomayajula from purdue university together with their collaboration partner george mason university [cit] sponsored by nasa. the dependency metric is developed to formulate policies and strategies for metroplex operations."
"just as schools have offered a more level playing field for the learning of traditional knowledge areas such as math and reading, so too direct intervention may be needed to bring all students to career and college readiness skills in their digital undertakings. other 21 st century skills should be similarly assessed. given the importance to the future opportunities that students will have depending on the types of skills and abilities they can exhibit in these areas, the contributions they will need to make through the use of such skills cannot be underestimated [cit] . again, more examples that show the possibility of more fine-grain interpretation at the individual level through such learning analytic approaches will be discussed in an upcoming paper."
"to be more accurate, the sector congestion can be described in different statuses. a multi-level rating of 0, 1, 2 is used to describe \"no congestion, light congestion, heavy congestion\", whose number of levels can be adjusted when it is applied in practice. the only difference between multi-level rating and on-off rating is that w βe will need to be normalized similarly as w αe for multi-level rating. one problem for multi-level rating is that sometimes an edge may intersect several areas with different congestion ratings as fig. 10 . in that case, the largest congestion rating on this edge will be picked as w βe . in the case of fig. 10, the direct route will be assigned 2 (heavy congestion) as its penalty weight."
"as a response to such transitions, the assessment and teaching of twenty-first century skills project (atc21s) [cit] . a goal was to employ new analytical approaches to the assessment of learning such that the challenges above could be better addressed. we will utilize an example of work from this project to illustrate the points we are making in this paper."
"sometimes data sets can be pre-existing, or extant data sets, as described above. examples of preexisting data include downloads from twitter feeds, click streams in user data, or other online collections generated as a result of processes with various purposes [cit] . at other times, data sets are collected at least in part directly for the purpose of applying learning analytics to the results. data collection can include, for instance, an adaptive recommender where ratings on prior experiences are solicited for the purposes of prediction of respondent interest in future experiences [cit], or evidentiary data collection for educational or professional development, to address personalized or grouped components to support the learner in educational assessment [cit] ."
"in either case, the same cycle is present but with different points of entry and a different flow to the interacting elements. in terms of data types for which learning analytics by the lak/solar definition are likely to be useful, in most cases complex data should be involved. if not, other simpler techniques might be better employed [cit] . complex data can take the form of large data sets (big data), multi-faceted data sets, or other elements in the data that encode more complex patterns or hard-to-measure constructs not readily identifiable without complex analytic techniques [cit] ."
"by defining the search factor α this way, we can easily adjust the search area. moreover, this method allows the search area to grow along four directions with an equal chance, so as to increase the possibility of finding a better patch. since there are no useful pixels in the object area for patch matching, only the candidate patches not in the object region will be compared with the object patch. so the actual size of the search area (sa) can be expressed as:"
experimental results in fig. 14 demonstrate the performance improvement by utilizing the local memory to enable the data sharing between work items inside the algorithm 2 parallel data loading from global memory to local memory in findbestpatch kernel function. same work group. we observe on average a 30% reduction in processing time after using the local memory.
"in single-pixel imaging, the measured intensity s i, associated with each measured pattern p i, is directly proportional to the overlap between the pixelated scene i o and the pattern p i and a reconstructed image i r can be obtained using the knowledge of s i and p i 3, 4 . if the patterns form an orthonormal basis, then an n pixelated scene can be fully sampled after performing n pattern projections and measurements, and the reconstructed image i r can be obtained using"
"besides the observations above, although the relative error per pixel is an overall criterion assessing how similar the reconstructed image is to the original one, it can be inconsistent with visual impression. we also note that the conventional compressive sensing performed in this work is only a representative method within a broad field. we emphasise that these results hold for the specific case of moderate resolution, which is chosen to be compatible with video rate image acquisition. we have confirmed that in this moderate resolution regime traditional compressed sensing does not have the impact seen for high resolution applications while still requiring long reconstruction times. our 'russian dolls' technique provides similar rmse results to the other methods, we believe that this method can be useful due to its speed and lack of reliance on a priori information. ultimately, the specific imaging application will inform which technique has the best performance and we believe that this 'russian dolls' ordering can be useful for low-resolution real-time imaging of moving scenes."
"however, that said, learning analytics as described in the lak/solar definition constitute a type of educational assessment. as such, meaningful interpretation means having an evidentiary framework designed to connect results clearly and on an empirical basis back to the goals and objectives of the analysis in order to make clear evidentiary claims about the learner [cit] . it also means being able to understand the uncertainty or range and degree of error likely to be present in the results."
"to accurately capture the real flight paths that air traffic follow, a jet route-based waypoint network is built. firstly, jet route and waypoint information are obtained from facet. using the navigationinterface in facet, the waypoint identifiers (index) and locations (longitude and latitude) are obtained for all waypoints. also using the navigationinterface, the sequence of waypoints that defines a jet route is obtained for all jet routes. to convert a jet route description in terms of waypoint locations to waypoint indices, the corresponding index is identified for each waypoint on a jet route using the waypoint information from facet."
"another level in this network leads to a scenario in which airports in a metroplex can be considered as \"modules\" which are connected to other metroplex modules and airports (γ-level in fig. 1 ). the airports within each module are dependent on each other operationally and possibly economically. at the same time they are connected to the other parts of nas by all means of airline services. the \"external\" interactions between a metroplex module and the other parts of the nas are different from those dependencies \"internal\" to the metroplex module. in order to understand this difference, metrics are developed to characterize and quantify the dependencies between metroplex airports."
"we use fig. 10 to help us analyze the overall computation complexity. first of all, we assume the search factor α is defined as in the previous section. we also define the search area as sa as in (7)."
"the congestion status is not necessarily published in shape of sector. to be more precise, the congestion can be reported as a smaller and more accurate area within a sector, which makes it look like the weather forecast in fig. 12 ."
"the expert panel then challenged itself to define, for each of the competencies within the learning progression, increasing levels of sophistication of student competence, that is, to describe what having \"more\" and \"less\" of the competency would look like, for age 11-, 13-and 15-year-old students. in other words, as one expert noted, \"when someone gets better at it, what are they getting better at?\" this might also be thought of as describing how the field of education will document the range of competencies that we might expect as students develop in sophistication."
"third, developing and sustaining social capital through networks (scn) involves using, developing, moderating, leading and brokering the connections within and between individuals and social groups in order to marshal collaborative action, build communities, maintain an awareness of opportunities and integrate diverse perspectives at community, societal and global levels."
"these results show that even in this low resolution regime conventional compressed sensing still performs well for low sampling ratios (i.e. high compression), though with a penalty of long reconstruction times. the russian dolls performance for low sampling ratios seems to be excellent, and indeed it achieves the lowest rmse of all methods. by contrast ecs can contain finer details and does not incur long reconstruction time penalties, however, practical implementations require a priori knowledge of the scene which in practice comes from the previous frames, resulting in errors in scenes with motion."
"in this work, we proposed an optimized order of the hadamard basis for use in compressive single-pixel imaging applications. the russian dolls ordering utilizes the sparsity of natural scenes, similar to transform coding 30, 31 . our numerical simulations demonstrate that this 'russian dolls' order of the hadamard basis can yield a similar image quality compared to conventional or evolutionary compressive sensing but with minimal computational resource, and is not limited to binary images 32 . in the case of a properly chosen sampling ratio and imaging a single object on a uniform background, this 'russian dolls' approach outperforms the other methods with regards to snr and image reconstruction, but suffers from reduced detail. furthermore, without a computational overhead, the 'russian dolls' method reconstructs images significantly faster than conventional compressed sensing. therefore, this method can be utilised to improve real-time performance in single-pixel video applications, particularly where a priori estimate of the scene is unavailable or unreliable."
"in addition to time reduction, reducing the search area can also reduce the possible false matching. as a comparison metric, ssd can roughly represent the similarity of two patches, but it cannot accurately reflect the structural and color information embedded in the patches. therefore, the patches with the highest distance scores (ssd in this algorithm) may not be the best patches to fill in the hole and to generate visually plausible results due to the limitation of ssd metric, especially for complex scenes with different color information and structures. the algorithm sometimes can lead to false matching, in which the reported \"best\" patch fig. 9 the effect of reducing the search area. the search area factor α is defined as in fig. 8 ."
"the object removal algorithm is an iterative algorithm, in which one object patch is processed in each iteration. we need roughly wh/p 2 iterations to finish the whole process. that said, if we increase patch size p, fewer iterations are needed to fill the object area which may lead to shorter processing time. meanwhile, the complexity of the ssd computation (o(p 2 )) becomes higher for the patch matching, which tends to increase the processing time. therefore, it is not straightforward to determine the impact of increasing patch size p to the overall complexity and performance."
"the atc21s panel of experts identified a set of distinctive ict literacy goals for students. earlier frameworks had included individual consumer skills, often on a web 1.0 model of data repositories that could be accessed over the internet by students. a variety of producer skills, in which students needed to manipulate digital assets in new ways-due to the emergence of web 2.0 technologies-were seen as emerging trends. lastly, they found that, as described in documents from the national initiative for social participation (nisp) (e.g., [cit], the field of ict literacy was on the cusp of recognizing the importance of digital networks to education, which would require from the students both \"social capital\" skills and the ability to draw on the \"intellectual capital\" of groups and teams. included in intellectual capital are the web 3.0 skills of \"semantics,\" or meaning-making through technology, including complex tools such as analytics, the effective use and evaluation of ratings, crowd-sourcing, peer evaluation, tagging, and the ability to judge the credibility and viability of sources."
"in light of these sets of clarifications, we suggest a revision to the lak/solar definition, which we propose as \"learning analytics definition, lak/solar.v2\": \"learning analytics is the measurement, collection, analysis, interpretation, and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs, by means of a coherent evidentiary argument... complexity should be introduced in the data and the analysis only when necessary to the development of the evidentiary argument.\" complex data as described here can take the form of large data sets (big data), multi-faceted data sets, and/or other the data elements that encode patterns or hard-to-measure constructs not readily identifiable without advanced analytic techniques."
"the rest of this paper is organized as follows. the second section introduces the concepts of metroplex and flexible flights. also the dependency metric of the metroplex airports is illustrated in detail. the third section shows how the network model is established for running our metroplex routing algorithm. in the fourth section, the metroplex routing algorithm under the sector congestion constraint is presented and the runway scheduler following the routing algorithm is also simply introduced. the routing algorithm simulation is performed in section v and section vi concludes the paper."
"each scenario was designed to address more than one strand of the construct, but there were different emphases in how the strands area were represented among the scenarios. where possible, we took advantage of existing web-based tools for instructional development. these are each briefly described below."
"principle 4: there is evidence of quality in terms of reliability and validity studies and evidence of fairness, through the data analytics; the building block is an algorithm specified as a measurement model that provides for a visual representation of the students and the items on the same graph (called a \"wright map\"), and a number of other data analytic tools that are helpful for testing the quality of the measurement. [cit] how these principles become embedded in the process and the product of the assessment development, and the nature of these building blocks, is exemplified in the account below."
"in order to take advantage of the boost c++ graph library (bgl) [cit], we transform the obtained waypoints and the jet routes data into the bgl format. the major contribution of the boost graph library (bgl) is a generic interface that allows access to a graph's structure, but hides the details of the implementation. this is an \"open\" interface in the sense that any graph library that implements this interface will be interoperable with the bgl generic algorithms and with other algorithms that also use this interface. the bgl provides some general purpose graph classes that conform to this interface."
"once the best matching patch is found, we copy the pixel values of ψq into ψ p . the aforementioned search and copy process is repeated until the whole target region ω is filled up. more details of the algorithm can be found in reference [cit] ."
"in this paper, we take the exemplar-based image inpainting algorithm for object removal as a case study to explore the capability of the mobile gpgpu to accelerate computer vision algorithms using opencl. the remainder of this paper is organized as follows. section 2 introduces the architecture of the modern mobile socs and the opencl programming model for the mobile gpgpu. section 3 briefly explains the exemplar-based inpainting algorithm for object removal. we analyze the algorithm workflow and propose a method to map the algorithm between mobile cpu and gpu in section 4. to adapt the complex algorithm to the limited hardware resources on mobile processors, we further study implementation trade-offs and optimization strategies to reduce the processing time in section 5. section 6 shows experimental results on a practical mobile device, which indicates that our optimized gpu implementation shows significant speedup, enabling fast interactive object removal applications in a practical mobile device. section 7 concludes the paper."
"the idea of learning through digital networks and the use of digital media, in the context of schooling, has been based on the conceptualization of student use of information and communication technology, or \"ict\" literacy. to wit, information technologies are seen as resources for creating, collecting, storing, and using knowledge as well as for communication and collaboration ."
"mcclain and clarke [cit] developed the metric for metroplex clustering analysis and clarke has been working on building a theoretical framework for quantifying the interactions between the airports in a metroplex. [cit] and the two papers took the first look at the status of each airport in terms of the markets served, seat capacity, delays and other features. his recent work [cit] with schaar has applied metroplex concept to improve the domestic airline services."
"in conclusion, this paper suggests a modified definition for learning analytics. the definition underscores the need for meaningful interpretation of learning results and also incorporates key ways that learning analytics are employed with complex data and algorithms."
"from the input flight plans which need to be optimized, firstly we obtain the sequence of waypoints of the flight route and insert the waypoints into the hashed jetway network. the related edges are also added in. secondly for each flight we obtain its origin-destination pair because our routing algorithm needs them to generate the optimal reroute."
"to demonstrate the best patch matching behavior of the object removal algorithm, we employ several different test images to cover different scenarios, which are shown in the first row of fig. 7 . the characteristics of these test images are summarized in table 3 . we chose images with different background scenes and textures, with variable image sizes and object sizes, and with different object shapes, so that by performing experiments on these images, we can better understand the implementation trade-offs related to the performance improvement."
"a commonly used definition of learning analytics that we will draw on here was proposed by the first international conference on learning analytics and knowledge [cit] ) and adopted by the society for learning analytics research [cit] : \"learning analytics is the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs.\" while this definition is helpful, two additional aspects are important to consider: the interpretation of results, and the choice of appropriate data types and algorithms."
"no single entry point to the cycle is best among the choices in every situation. the choice can be determined by the intended purposes of interpretation, and the current state of claims that can be made in a given context. in any particular situation, one relevant question to ask is does the analysis begin with an interpretive framework a priori, as in the theory component of the cycle below, or is interpretation made a posteriori, as when even the initial interpretive framework is derived from data because little is yet known?"
"not all of the planned automated scoring and data analysis for the items in the two piloted scenarios was available for application to this data set, as the total numbers of cases was too small for the empirically-based scoring to be successfully calibrated. this calibration? will be completed at a later point, when larger data sets are available. each of the two scenarios were presented in three forms, for 11-, 13-and 15 year-olds, respectively, with a subset of common items across the three forms."
"for this initial data collection, we chose to use the random coefficients multinomial logit model [cit], which allows polytomous data (as is appropriate for the items in the tasks), and provides the wright maps mentioned above (and are shown in figures 7, 8 and 10 below) . due to small sample size, we ended up with only two scored items in the scn strand and therefore excluded this strand from the analyses. as a result, we report our findings from three separate unidimensional analyses with 27 items measuring the cin strand, nine items measuring the pin strand, and 22 items measuring the icn strand. the conquest 3.0 estimation software [cit] was employed throughout the analyses, which uses rcml as its basis for statistical modeling. [cit] for the relevant equations and statistical estimation procedures for the analysis."
"the definitions of information and communication literacy range from simple-basic digital knowledge and skills-to complex, a wide range of tools and competencies that atc21s describes as \"tools for working\" in a digital age [cit] . these generalized tools include accessing and using technologies to evaluate and utilize information, analyzing media, creating information products, and they include understanding ethical and legal issues involved in being a participant in the knowledge economy."
the four strands are seen as interacting together in the activity of learning in networks. they are conceptualized as parallel developments that are interconnected and make up that part of ict literacy that is concerned with learning in networks.
"measuring intricate constructs through digital analytics, as displayed here, can help us to appreciate the new ways that students will be required to think and work compared to previous decades. this paper describes a domain modeling process for assessing ict literacy through the bear assessment system, along with examples of task development, and results from implementation of a first field test in four countries."
team situational awareness in process optimize assembly of distributed contribution to products extending advanced models (e.g. business models) producing attractive digital products using multiple technologies / tools choosing among technological options for producing digital products the items design. the berkeley evaluation and assessment research (bear) center at uc berkeley developed three scenarios in which to place tasks and questions that could be used as items to indicate where a student might be placed along each of the four strands.
"each direct route between two nodes is considered as two directional edges in a jetway network graph and each of the two is recorded in a c++ struct which contains the starting point and the ending point. all the edges are also stored in an indexed c++ map. at the beginning of the network construction, the great circle distance of each edge is calculated and recorded as the non-negative weight in each edge struct. by using boost::add vertex and boost::add edge functions from bgl, a jetway network graph is built based on the waypoints and direct routes as nodes and edges respectively. additionally the weight of each edge is set into boost::weight map(pmweigth). finally the dijkstra algorithm boost::dijkstra shortest paths will take care of the optimizing job and output the distance and predecessor of each node after the input of origin airport is set. in other words, the dijkstra algorithm from bgl provides the results of all the shortest paths from the origin airport to every other airport except those are disconnected from the origin."
"one such orthonormal basis is derived from the hadamard matrix; a square matrix with elements ±1 whose rows (or columns) are orthogonal to one another 26, 27 . each pattern is formed by reshaping a row (or a column) of the hadamard matrix into a two-dimensional square array. the lowest-order hadamard matrix is of order two:"
"the solid-black lines represent flights to and from nynj to the remaining parts of nas. the arrows between the three airports are the \"internal\" interactions within this metroplex which bring dependencies on multiple dimensions (table 1) . for example, connecting passengers flying into lga for their international flight from jfk require ground transportation, which is an operational dependency. from the perspective of the port authority of new york and new jersey (panynj), the ownership of all three nynj airports provides better economic synergies than the single ownership. however, many other metroplexes have multiple owners e.g., northern california metroplex, which create different types of dependencies. furthermore, policies and regulations at one airport will affect other airports in the same metroplex. for instance, all airports in a metroplex must agree to participate in the flexible arrivals policy and flexible flight operating. in summary, there are various interactions among many dimensions of dependencies as shown in table 1 ."
"secondly, because the patch size is p, any candidate patch within (p −1) pixels range surrounding the object area ω will partially overlap with the object area. we define this overlapping area as oa, whose area can be calculated as:"
"fourth, developing and sustaining intellectual capital through networks (icn) involves understanding how tools, media, and social networks operate, and using appropriate techniques through these resources to build collective intelligence and integrate new insights into personal understandings. an initial idea of the relationship amongst these four strands of the learning progression is given in figure 2 . the four columns are the four strands, each with defined levels within them (exemplified in the next paragraph), and the arrows show expected tracks of development among these constructs. an example of an activity that is hypothesized to map into the icn2 level (\"functional builder\") of the intellectual capital strand is demonstrated in the chat log in figure 3 . in this example, students are creatively using tags to make their communication clearer. specifically, the team members have devised a strategy using different fonts and/or colors to distinguish what each team member is typing. in table 1 the levels of the second of these four strands have been described as a hypothesized construct map showing an ordering of skills or competencies involved in each. at the lowest levels are the competencies that one would expect to see exhibited by a novice or beginner. at the top of the table are the competencies that one would expect to see exhibited by an experienced person -someone who would be considered very highly literate in ict. the construct map is hierarchical in the sense that a person who would normally exhibit competencies at a higher level would also be expected to be able exhibit the competencies at lower levels of the hierarchy. the maps are also probabilistic in the sense that they represent different probabilities that a given competence would be expected to be exhibited in a particular context rather than certainties that the competence would always be exhibited. [cit] ."
"imaging is one of the most ubiquitous and useful techniques for gathering information. imaging is conventionally performed using cameras based on detector arrays and though a very mature technology, these have their limitations. recently there has been a push towards imaging with only a single detector 1, 2 and this so-called 'single-pixel imaging', also closely related to classical ghost imaging 3, 4 . rather than capturing a two-dimensional (2d) image with a pixelated array, these techniques use an alternative strategy to retrieve spatial information by recording only the total light intensities in each component of a spatial sampling basis. these intensities corresponding to each of the basis components are measured on a single-pixel detector sequentially in time, and together with knowledge of the sampling basis, an image can then be reconstructed. though detector array technology has superior performance in the visible region of the spectrum, single-pixel imaging is particularly well-suited to non-conventional imaging, such as multi-wavelength imaging 5, depth mapping [cit], 3d profiling 10, 11 . the most mature method of single-pixel imaging is the raster scanning approach 12, 13, where the object is scanned one image pixel at a time. entering the new century, single-pixel imaging utilized pseudo-thermal random speckle patterns to sample a scene 14, 15 . advances in computational ghost imaging led to the use of a spatial light modulator (slm) to generate the random patterns 3, 4 . however, the non-orthogonality of random patterns often means that more than n measurements are required for a high quality reconstruction of an n pixel image 16 . improvements can be made by sampling a scene with patterns forming an orthogonal basis set, allowing, in principle, a perfect reconstruction of an n pixel image with n measurements 17, 18 . the single frame acquisition time of single-pixel imaging is typically longer than that of a conventional camera due to the need for sequential measurements. acquisition time can be shortened by reducing the number of measurements, however, this potentially leads to loss of information. compressed sensing can be used to produce higher quality image reconstructions from fewer than n measurements by exploiting the sparsity in the spatial frequencies present in natural scenes. this 'conventional compressive sensing' 1, 2, 19, is usually performed by minimizing a certain measure of the sparsity. it is widely understood that the number of measurements required to form a 'good' reconstruction is related to the sparsity of the image 2, 20 :"
"the view of learning analytics that this paper is based on starts with the observation that the current practices of schooling are somewhat outmoded in the new global working environment. today, people work both individually and in groups to share complementary skills and accomplish shared goals-this practice contrasts to schools and assessments where students take tests individually."
"the paper begins with a description of some expansions to a common definition of learning analytics, then includes a review of the literature on ict literacy, including the specific development that led to the atc21s effort. this is followed by a description of the development of a \"learning progression\" for this project, as well as the logic behind the instrument construction and data analytics, along with examples of each. the paper goes on to describe the data that was collected, and reports on the learning analytics approach that was used. the paper concludes with a discussion of the next steps for this effort."
"effectively; the building block is the outcome space, or the set of categories of student responses that make sense to teachers. these categories of student responses become the core of quantitative measures for conclusions through the data analytics."
"wireless communication and digital imaging technologies. on one hand, as mobile processors have become more and more powerful and versatile during the past several years, we are witnessing a rapid growth in the demand for the computer vision applications running on mobile devices, such as image editing, augmented reality, object recognition and so on [cit] . on the other hand, with the recent advances in the fields of computer vision and augmented reality, the emerging algorithms have become more complex and computationally-intensive. therefore, the long processing time due to the high computational complexity prevents these computer vision algorithms from being practically used in mobile applications."
from the high dependency metric value in table 2 and the low dependency metric value in table 3 . we can see there are more dependencies among airports in a metroplex than non-metroplex airports. the shared resources of the metroplex airports link them together as a system. therefore the concept of the metroplex can be considered as a super destination airport for flexible flights.
"knowledge is applied across disciplinary boundaries in the process of solving real-world problems, but in schools, the school subjects are divided by disciplinary boundaries. furthermore, at work, problem solving is often complex and ill-structured, in contrast to the simplified problem-types featured in much of school education, and especially school standardized testing. finally, in the work setting, people have access to enormous resources of information and technological tools, where the challenge is to strategically craft a solution process, which contrasts strongly with the traditional \"closed book\" analytics of what learners know and can do [cit] ."
"finally, it would be helpful if the lak/solar definition made reference to algorithms, or characteristics of algorithms, that might be to useful to apply for aggregating and parsing of patterns, since this is an important consideration in the use of learning analytics [cit] . while it is important to keep the definition general to be inclusive of many useful algorithms that might arise, as a general class the approach typically needs to involve algorithms to automatically process the data, assuming the purposes of interpretation and the complexity of data require algorithmic approaches to the accumulation and parsing of patterns in the data. algorithms can be statistical in nature, applied as inferential statistical tests or to yield inferential indices as part of the processing, which can help with assessing quality of results [cit] . numerous algorithms in the form of measurement models that take a statistical form for learning outcomes have been created and applied. these are well established in the psychometrics research literature and some of the advanced models as well as basic models can be appropriate to apply in learning analytics to complex 21 st century skill settings [cit] . algorithms can also process patterns in more descriptive ways, yielding machine-readable results such as categorization or subsetting of respondents [cit] . note that since machine processing is required, however, the data sets at some point have to include machine-readable data. this may be text-based or graphical in nature, or in some other innovative format, depending on the processing requirements of the algorithm and platform, or the data sets may be numeric [cit] . the desired data characteristics may already be present for a given data set in any particular case, or may require preprocessing. this could include types of scoring, ordering, subsetting, or other types of aggregation. for this, reliable data collection, warehousing and prep can be a problem, so a variety of \"clean-up\" procedures may be needed. an important stage in learning analytics is reducing construct irrelevant variance including noise, user errors, or out-of-scope entry of data, which should be clarified and validated before conclusions can be drawn [cit] ."
"in practice the \"decision boundary\" consists of a series of metering fix points [cit] for the ease of operation. in our algorithm, the \"decision boundary\" is considered as a polygon around the metroplex as shown in fig. 3, where the blue polygon is the decision boundary of nynj metroplex."
"the trajectory datasets provided by nasa contain trajectory information such as flight plan, longitude, latitude, altitude and speed, for every flight in the nas. using the facet apis, the flight plans are extracted from the dataset to be input to our program. in this research, facet is used to compute the flight trajectories by given the optimized flight plans. facet contains an aircraft performance model database with 66 different aircraft types, along with an equivalent list that maps over 500 aircraft types to these 66 models. facet also contains various datasets used to model the nas, such as center/sector boundaries, sector capacities and jet route system/waypoints information. facet has two routing options [cit], the direct routing (dr) option and the flight plan routing (fpr) option. the dr option computes a great circle route directly from origin to destination. the fpr option computes a route that connects waypoint to waypoint, from origin to destination. segments connecting the waypoints are great circle routes. the default simulation time interval, which was used in this research, is 1 minute."
"to address this problem, researchers have been exploring general-purpose computing using graphics processing units (gpgpus) as accelerators to speed up the image processing and computer vision algorithms thanks to the heterogeneous architecture of the modern mobile processors [cit] . on desktop computers or supercomputers, numerous programming models have been extensively studied and utilized to facilitate the parallel gpgpu programming, such as the compute unified device architecture (cuda) [cit] and the open computing language (opencl) [cit] . as a comparison, due to the lack of parallel programming models in the mobile domain, the opengl es (embedded system) programming model was commonly used to harness the computing power of the mobile gpu [cit] . however, the inherent limitations of the opengl es lead to poor flexibility and scalability, as well as limited parallel performance, due to the fact that the opengl es was original designed for 3d graphics rendering. recently, emerging programming models such as the opencl embedded profile [cit] and the renderscript [cit] have been supported by the state-of-the-art mobile processors, making the mobile gpgpu feasible for real-world mobile devices for the first time [cit] ."
"once the coherence of the four building blocks is in place and results are obtained, interpretation of the learning analytics can take place. this interpretation is done through the maps yielded. for example, the distribution of students over the proposed construct for the consumer strand can be seen in figure 7 . most students fell into the conscious consumer portion of the construct, as shown by the histogram composed of the \"x\" character on the left portion of the display. this location on the map indicated that the students tended to approach the ict learning in social network activities with the ability to select appropriate tools and strategies (strategic competence), construct targeted searches, compile information systematically, and at least know that credibility of online information is an issue, though perhaps not how to address this. some students, represented by the lower x's in this band, represented some but not full proficiency in these areas while others, as they placed in locations up the histogram, could be expected to employ these behaviors effectively more often and more fully."
"results from computing coarse grain dependency metrics [cit] ) are shown in table 2 . data in table 2 indicate that there was no significant difference between nynj and nocal. the minor differences can be due to the difference in their total traffic or the definition of the metrics. this indicates that similar factors cause dependencies at these two metroplexes. it is found that the metric values vary with the choice table 2) . 2002 data is not available to estimate the traffic per runway at oak and sjc, causing nocals dependency metrics for this year to significantly differ from the other years. atlanta international (atl) and denver international (den) airports are analysed to understand how dependencies differ between metroplex and non-metroplex airports. in particular, atl and den had operations comparable in volume to jfk and sfo (nynj and nocal respectively), many airlines operate at each of them, and both have problems of delays and congestion. they are good candidates for this comparative study. [cit] (table 3 ). [cit], causing \"decoupling\" of runway operations at atl. [cit], our metrics could not detect any significant change in den's dependencies."
"in the first simulation, we reconstruct a set of 35 images (three examples of which are labelled as 'original' in fig. 3a-c), in which each image contains an object on a black background. the reconstructed images at a sampling ratio of 6% are also shown and labelled correspondingly in fig. 3a-c. figure 3d shows the rmse of the reconstruction image as a function of sampling ratio, where the rmse is the average derived from all 35 reconstructed images. as expected all three approaches show a similar trend in that the reconstruction quality is improved as the number of patterns increases. for sampling ratios below 20% ecs and conventional compressed sensing perform very similarly, while the 'russian dolls' ordering result is characterized by some optimal points at 50%, 25%, 12%, 6% and 3% sampling ratio, which is coincident with our prediction that we can form an optimised reconstruction with lower resolution using a sub-set of patterns. for moderate sampling ratios from 20-50% all methods perform very similarly. for sampling ratios above 50% ecs outperforming the other methods. at sampling ratio 6%, the average reconstruction times of all 35 images are 0.103 s, 0.111 s and 12.573 s for 'russian dolls', ecs and conventional compressive sensing respectively, where ecs is 10% slower due to the need to rearrange patterns 25, and conventional compressive sensing is slower due to the increased computational overhead."
"in this paper, we take the exemplar-based inpainting algorithm for object removal as a case study to show the methodology of using the mobile gpu as a coprocessor to accelerate computer vision algorithms. the object removal algorithm involves raw image pixel manipulation, iterative image processing technique, sum of squared difference (ssd) computation and so on, which are typical operations for many computer vision algorithms. therefore, the case study of the object removal implementation can represent a class of computer vision algorithms, such as image stitching, object recognition, motion estimation, texture analysis and synthesis, and so on. therefore, by studying and evaluating the performance of the exemplar-based object removal algorithm on mobile devices with cpu-gpu partitioning, the feasibility and advantages of using the mobile gpu as a co-processor can be demonstrated. furthermore, the optimization techniques proposed in this paper can possibly be applied to other computer vision algorithms with similar operation patterns or algorithm workflows."
"in detail, for each flight plan we first extract the sequence of the waypoints, then the facet interface is used to translate these waypoints into five-digit integers. for each waypoint, we obtain its latitude-longitude pair and search it in the existed jetway network hash table. if this waypoint exists, continue to process the next waypoint of the current flight plan; if this waypoint is new, insert it to the hash table and create a new hash key pair by rounding the absolute latitude-longitude pair of this waypoint. after all of the flight plans are processed and all the airports are inserted, the jetway plus network is formulated."
"where the items are polytomous, the labeling is a little more complex, for example, in figure 7, note that item 5 is represented by two labels: 5.1 and 5.2. the former is used to indicate the threshold between category 0 and categories 1 and 2 (combined); the latter is used to represent the threshold between categories 0 and 1 (combined) and category 2. the interpretation of the probability is equivalent to that for a dichotomous item: that is, when a student's x matches the 5.1 location, the probability of that student succeeding at levels 1 or 2 on that item is expected to be 0.50; and similarly, when a student's x matches the 5.2 location, the probability of that student succeeding at only level 2 on that item is expected to be 0.50."
"for the producer strand of the construct, the group of students surveyed tended to be more evenly split between the emerging and functional producer traits. this positioning? indicates that though living in the era of web 3.0, when students may be immersed in a maker culture, some students have considerably more skills toward producing digitally and collaboratively online than do other students. about half the students could produce simple representations from templates or start an online identity, but were challenged to go much beyond this to participate in an effective collaboration, problem solve, or produce a product. by contrast, the other approximately half of students were able to go far beyond this. they showed the ability to organize communication effectively in social networks, use collaborative networked tools to develop creative and expressive artifacts, plan for shared solutions, and also demonstrate? an awareness of safety and security online in the process of their production activities."
"the metroplex and flexible flights concept can bring many benefits to both passengers and traffic controllers with existed airports and ground traffic infrastructures. developing and analysing the concept of flexible operations at a metroplex and determining whether the alternate, flexible flight plan have significant potential operational and economic value. some of the potential benefits are: a) enhancing metroplex airport throughput without extensive infrastructure investment; b) increasing robustness to disruptions; c) providing maximal flexibility to passengers."
"in which r(p) is the confidence term indicating the amount of reliable information surrounding the pixel p, and d(p) is the data term representing the strength of texture and structural information along the edge of the object region δω in each iteration. r(p) and d(p) are defined as follows:"
"studying these internal dependencies inside a module is critical for proper implementation of our flexible operation concept on metroplexes. therefore the dependency metrics and related analyses have been developed and the major results are presented as the following. the premise for quantifying dependencies is to observe how dependencies vary from one metroplex to another. fig. 2 is a simplified view of nynj metroplex with its three major airports jfk, lga and ewr."
"the approach in this paper is based on a weighted shortest path search. a network model is required in order to design an efficient and effective metroplex routing algorithm. the waypoint-based network is exploited as the data structure to generate the optimal route for each flexible flight under sector congestion constraint. the initial network built from waypoints provided by facet is named as jetway network. another model called jetway plus network is formulated by inserting additional waypoints and all airports information, which is used as the final data structure for running our algorithm."
"the outcome space. each item that was developed was targeted at one or more of the four strands, and the expected range of levels that would be represented in the item responses were also noted. where the responses are selected from a fixed set (as in a multiple-choice item), the set can be planned ahead of time, but for open-ended tasks and activities, the resulting work product is more complex. examining the student work product is a something that needs to be empirically investigated. the tabulation is shown in figure 6 . as can be seen, the first three levels were reasonably well-covered, but level 4, which we seldom expect to see for students in this population, had only one instance. figure 6 . the number of data points from each scenario, and their planned allocation to the levels from each strand."
"the economy of leading countries is now based more on the manufacture and delivery of information products and services than on the manufacture of material goods. even many aspects of the manufacturing of material goods are strongly dependent on innovative uses of technologies. the start of the 21st century also has witnessed significant social trends in which people access, use, and create information and knowledge very differently than they did in previous decades, again due in many ways to the ubiquitous availability of ict. [cit] it is to be expected that this widespread change will significantly affect the personal and working lives of many people, and hence should have equally large effects on the school systems that educate people for their later lives and careers. these effects may include the nature of and even the names of the subjects that are taught in schools, how those new subjects (and the traditional subjects) are taught and learned, and how education is organized. thus, the atc21s project was initiated to develop new assessments in the area of 21 st century skills, based on the idea that new assessments could lead the way to these new subjects. the project determined to develop sample assessments in a few specific domains of the 21 st century skills, to serve as examples leading to further developments in learning analytics in those areas."
the fpr option would be the more suitable option for modelling current air traffic patterns since most of today's flights do not fly a direct route from origin to destination. the dr option would be more suitable for studying free flight operations. the fpr option is used in this research.
"to better optimize the opencl-based implementation, we first measure the timing performance of the opencl kernels. table 2 shows a breakdown of processing time when running the program on a single core of the cpu on our test device. the opencl kernel function used to find the best matching patch with the current patch (denoted as findbestpatch) occupies most of the processing time (97%), so optimizing the findbestpatch kernel becomes the key to improving performance."
"this paper first introduces the concept of metroplex and flexible flights and then the analysis shows that the airports within the same metroplex have dependency on each other, which makes the metroplex concept a potential solution for improving the throughput of the metroplex airports and increasing the flexibility for passengers. moreover, the routing algorithm for flexible flights is developed under sector congestion constraint based on weighted shortest path algorithm. the routing algorithm is applied on a data structure called jetway plus network which is built from waypoints and jetways. during the implementation process, the facet from nasa ames and its interfaces facilitate the data acquisition for establishing the jetway plus network and the algorithm simulation. the algorithm performance turns out to be efficient and effective for the metroplex routing and after the flexible flight arrives the decision boundary of the metroplex, the mctma scheduler will assign a runway to it. the routing and scheduling together constitute a complete flexible flight operation."
"further large-scale data collection and analysis of results also helps to validate, or refine and reconfigure meaningful frameworks. an important consideration in emerging areas such as these is not only what student performances are currently, but what teachers, schools and states would like them to be. validated working definitions help school systems plan instructional interventions and assessments, and work with teachers for professional development."
"second language chat. this scenario was developed as a peer-based second language learning environment through which students interact in learning. developing proficiency in a second language (as well as in the mother tongue) requires ample opportunities to read, write, listen and speak. this assessment scenario asks students to set up a technology/network-based chat room, invite participants, and facilitate a chat in two languages. it also involves evaluating the chat and working with virtual rating systems and online tools such as spreadsheets. worldwide, \"conversation partner\" language programs such as this have sprung up in recent years. they bring together students wishing to practice a language with native speakers, often in far-flung parts of the world. the cultural and language exchanges that result demonstrate how schools can dissolve the physical boundaries of walls and classrooms. collaborative activities also tap rich new learning spaces through the communication networks of ict literacy. this task shows how collaborative activities can also provide ample assessment opportunities in digital literacy. [cit] ."
"to form the network, we record the connection (edge) between two consecutive waypoints (vertices) for each jet route. intersection points on the jet route system are automatically considered as waypoints when recording the connection along a jet route because an intersection point will be part of the waypoint sequence for every jet route sharing that intersection point. figure 4 illustrates how edges are obtained from a jet route, along with intersection points. additional edges were created to connect airports to the jet route system. collectively, the edges and vertices define the graph. the edges are not directed because a jet route can support traffic in both directions (in different altitudes)."
"the bear assessment system (bas) consists of employing data analytics structured around four basic principles of assessment, and four accompanying building blocks that are tools to help develop the assessment. the principles and building blocks are as follows:"
"since the congestion's change is not very rapid in reality, the sector congestion status can be forecasted and published hourly by faa or other professional agencies. the congestion status report imported into the jetway plus network model is used to update the edge weights and the metroplex routing in next one hour is calculated based on these new weights."
"this initial field test has demonstrated through the learning analytics approach that, for this data set, students in the 11-15-year age group show widely differing knowledge and skills in these areas. some students are only just beginning to take initial steps toward digital competence while other students show levels of mastery that would likely challenge skilled adult samples, such as collaborating easily to create professional-level commentaries or reports. doubtless the wide range is due at least in part to the absence of formal teaching and opportunities to learn these skills in many schools, which results in an increasing gap between the haves and the have-nots. it looks as though this is particularly true in the domain of ict literacy, which we take to be a gatekeeper for other skills and achievements in the future."
"the final weight w e consists of the effects from w αe and w βe, and a coefficient γ is introduced to adjust the strengthen of the penalty weight in eqn. 1. the larger γ places more emphasis on sector congestion avoidance in the algorithm."
"the unit of analysis itself can also generate data complexity, for instance, when data are collected for individuals but the individuals are collected into groups, programs, institutions, or other collective units, for which data are also collected."
principle 1: assessment should be based on a developmental perspective of student learning; the building block is a construct map of a progress variable that visualizes how students develop and how we think about their possible changes in response to items. data analytics in this case are structured around the theoretical conception of the construct map.
"in addition to nynj, northern california (nocal) metroplex is analysed to understand how dependencies differ from one metroplex to another. nocal contains san francisco international (sfo), mineta san jos international (sjc) and oakland international (oak) airports."
"one way of checking if the assumptions and requirements of the confirmatory approach has been met within each of the three consecutive models is to examine the weighted mean square fit statistic estimated for each item within each model. item fit can be seen as a measure of the discrepancy between the observed item characteristic curve and the theoretical item characteristic curve [cit] . conquest estimates the residual-based weighted fit statistics, also called infit; ideally, infit values are expected to be close to 1.0. values less than 1.0 imply that the observed variance is less than the expected variance while values more than 1.0 imply that the observed variance is more than the expected variance. a common convention of 3/4 (0.75) and 4/3 (1.33) is used as acceptable lower and upper bounds [cit] . after deleting items that fit poorly, we found that all of the remaining 58 items for the three consecutive models fell within this range. table 2 shows the variances and correlations of eap scores obtained from the consecutive approach. the closest estimated relation is between the cin and pin dimensions, at a correlation of .69. figure 7 shows one of the \"wright maps\" [cit] obtained from the unidimensional model for consumer in social networks. items are vertically ordered with respect to their difficulties and persons (cases) are vertically ordered with respect to their abilities. each \"x\" on the left-hand side represents a small number of students, and the items are shown on the right-hand side using their item numbers. the locations are interpreted as follows, for dichotomous items:"
an extension to the lak/solar definition we propose here is the specification that complex analytic techniques are needed to resolve the multifaceted or complex patterns. the same argument can be made as above for data sets. complexity should be introduced in the analysis for a coherent evidentiary argument only when necessary. so the usual parsimonious definition should be applied when models or other algorithms are used to fit learner data and resolve patterns.
"first, functioning as a consumer in networks (cin) involves obtaining, managing and utilizing information and knowledge from shared digital resources and experts in order to benefit private and professional lives. it involves questions such as:"
"flight plans are documents filed by pilots or a flight dispatcher with the local civil aviation authority (e.g. faa in the usa) prior to departure. they generally include basic information such as departure and arrival points, the sequence of waypoints of the flight route, estimated time en route, alternate airports in case of bad weather, type of flight (whether instrument flight rules or visual flight rules), pilot's name and number of people on board. the standard faa flight plan form is shown in fig. 7 ."
"the choice between an exploratory or confirmatory approach is an option, with the choice depending on how much prior theory and/or empirics are available. put together, these exploratory and confirmatory stages can be seen as a cycle in the evidence chain, as shown in figure 1 . the figure depicts a simple example of a learning analytics interpretive cycle, where entry points can be either confirmatory-entering at the \"theory or conceptualization\" node-or exploratory-entering at \"analysis and results\" node-for extant data or at the \"empirical data\" node when observations will be designed and collected (see below for a discussion of extant and collected data)."
"for this project, learning analytics were applied in an a priori approach, entering at the theory/conceptualization node of the learning analytics interpretive cycle discussed in the prior section, since considerable information about the domain is already available. information about the domain is based on prior empirical results and theory briefly summarized in this section."
"the atc21s panel of ict experts looked into a wide range of topics in laying out the background for their work in developing the learning progression, including in the areas of augmented social cognition [cit], applied cognition [cit], team cognition [cit], social participation [cit], cognitive models of humaninformation interaction [cit], technological support for work group collaboration [cit], theories of measurement for modeling individual and collective cognition in social systems [cit], topics in semantic representation [cit], and social information foraging models [cit] )."
"we can draw similar conclusions from the timing results for other test images shown in table 6, 7 and 8. to demonstrate the effectiveness of our proposed optimization schemes, the speedup gained from the proposed optimization strategies are concluded in table 9 . we observe speedups from 8.44x to 28.3x with all our proposed optimizations applied to the heterogeneous opencl implementations."
"the maps are also useful in investigating whether there is a good coverage of abilities by items. ideally, if permitted a sufficient number of items for each strand, we would hope to see the range of item difficulties approximately match the range of person abilities. this would mean that there are items approximately matching every level of the person ability. this can be seen to be true for the consumer in social networks strand. figure 7 also shows \"banding\" for the wright map for the consumer in social networks-that is, we have carried out a judgmental exercise to approximately locate where students in each level would be located. we note that, after misfit analysis, only level 3 of item 7 (i.e., 7.3) remains in the item pool to represent the highest level, the discriminating consumer. in contrast, the lower two levels, conscious consumer and emerging consumer, are well-represented. we can see that students in this sample have shown a range of abilities on this strand that spans all three of the hypothesized levels, from emerging to discriminating consumer, although, as we might have expected, there are relatively more in the lower levels than in the higher levels (an observation that will be repeated in the other strands)."
"in this section, we analyze the workflow of the object removal algorithms and describe the algorithm partitioning between the cpu and gpu to fully utilize the resources of the mobile soc chipset."
"the a posteriori approach can be considered generative, or in other words, exploratory, and in many cases will need to be confirmed by a subsequent data collection and analysis. the exploratory approach is sometimes called by the name \"data mining\" [cit] . exploratory approaches can be useful when the research goal is to learn more about the patterns in the data sets in a context where little is yet understood, or where new patterns may become evident that were not previously suspected."
the only metroplex routing research we can found is [cit] which describes a mixed integer linear programming formulation and solves the routing and scheduling problems at the same time for the flights whose destinations are metroplexes. the authors also formed the problem to maintain its computational feasibility and analysed the resulted routes.
"fig . 5 shows a workflow diagram of the exemplar-based inpainting algorithm for object removal. the algorithm can be partitioned into three stages: initialization stage, iterative computation stage, and the finalization stage. the blocks with the slashed lines are core functions inside the iterative stage and represent most of the computational workload. we can map the core functions into opencl kernels to exploit the 2-dimensional pixellevel and block-level parallelisms in the algorithms. the cpu handles the opencl context initialization, memory objects management, and kernel launching. by analyzing the algorithm, we partition the core functions into eight opencl kernels based on the properties of the computations, as is shown in table 2 . in each opencl kernel, the fact that no dependency exists among image blocks allows us to naturally partition the tasks into work groups. to represent color pixel values in rgba"
"the skills can be seen as building mastery, in a theoretical sense, but more information from student work is needed to help validate frameworks. examples of student work can be collected in cognitive laboratories as was done here or in other venues. such frameworks become especially useful if ranges of performances can be collected that successfully populate the strands and levels of expected performances. thus a working definition of the competencies is strengthened through exemplification."
"while these techniques and approaches are illustrated through one example project, they potentially are broadly applicable to many learning domains. the takeaway lesson may be that effectively educating students for 21 st century skills may also require offering them 21 st century supports for learning-including improved approaches to examining patterns and evidence of learning in complex settings."
note that the name of each waypoint facet provides is a five-digit integer beginning with a 1 or 2 while the standard flight plan contains a sequence of the waypoints which are described by a three-letter combination. the facet has a method called getwaypointname inside navigationinterface to translate one expression to the other.
"for advanced multiple-level sector congestion, the sector congestion impact is taken into account by assigning the edges with additional weights of different congestion statuses. more congested is a sector, the heavier weight it is on each intersecting edge. the congestion weight will be summed up with the great circle distance weight for each edge and then the shortest path algorithm is applied in this new weighted jetway plus network. the detail of the weighting scheme will be discussed in the following."
"facet is a simulation and analysis tool designed by nasa to evaluate future atm concepts and methods. facet can be used through a graphical user interface (gui) or by using the application programming interfaces (apis) in the java programming language. some of facet's functionalities used in this research include playing back recorded trajectories, simulation results and reading, writing and processing various nas datasets such as sector capacities, aircraft performance data, jet route information, etc. the facet apis are also used extensively in this research because it allows for greater functionalities compared to the gui."
"principle 2: there must be a match between what is taught and what is assessed; the building block is the items design, which describes the most important features of the format of the items-the central issue, though, is how the items design results in responses that can be analytically related back to the levels of the construct map."
"use adobe pdf printer, high quality with a high correlation score may have very distinctive textural or color information compared to the object patch. under such circumstances, the artificial effects introduced by the false matching will degrade the quality of the result images significantly. fortunately, spatial locality can be observed in most of the natural images, therefore, the visually plausible matching patches (in terms of color and texture similarity) tend to reside in the surrounding area of the candidate patch with high chances. by reducing the search area to a certain degree, we can reduce the possibility of false matching and therefore generate visually plausible result images."
"to take just one example, the research in augmented social cognition field [cit] describes the emergence of the ability of a group of people to remember, think and reason together. this field investigates how people augment their speed and capacity to acquire, produce, communicate and use knowledge, and to advance collective and individual intelligence in socially mediated environments. this is seen as very relevant to the ict field, as it is expected that augmented digital and virtual settings will be increasingly common learning environments for students in the 21st century."
"first, in this paper we make the point that, for learning analytics, the meaningful interpretation of the data analysis is critical to consider, not simply reporting the results [cit] . interpretation is not directly included in the lak/solar definition of \"collection, analysis and reporting.\" this weakness in the definition can lead to the assumption that once results are composed and reported, their meaning for learners and learning outcomes is self-evident."
"the four strands. for the atc21s project effort, ultimately the focus of ict literacy was on learning in networks, which was seen as being made up of four strands of a learning progression:"
"so the message here, should it be verified by larger data sets, is a strong word of caution to programs, schools or instructors. educators may presume every k-12 student today is a digital native. it is true that the digital divide may have eased in many locations and that students in a number of primary and secondary classrooms today may have grown up surrounded by many new technologies. however, the matter of access is not the sole determining factor in student development of 21 st century skills and abilities [cit] . figure 9 below) and item 3 require students to create a graph representing data. in item 1, we asked students to summarize the data in a table. we hypothesized that all of these three items would end up in the upper band of the wright map and our hypothesis was confirmed. the two most difficult items in this band, as we hypothesized, are items 4 and 9. in both of these items we asked students to upload files to the webpage. items in the emerging producer band require students to post an artifact and/or perform basic production tasks. in items 5, 6 and 7, for instance, we asked students to copy/paste the readily available content. in item 8, we asked students to connect at least one node in the concept map. all of these items ended up in the lower part of the wright map, as we hypothesized before the test. we do not display a banded wright map for the developer of social capital strand, as there are only two items remaining in this strand, both of which are aimed at measuring at third level (proficient connector) of the construct. finally, we do not display and discuss the wright map results for the participator in the intellectual capital strand, as it will appear in a future paper."
"to operate a flexible fight from departure to landing includes two parts. the first part is to route the flexible flight towards its destination metroplex similarly like to route a regular flight to its destination airport. in this part we just need to treat a metroplex as a super airport. the second part is when the flexible flight arrives the decision boundary of the metroplex, it will be scheduled to some certain runway at one of the metroplex airports. however, until the runway scheduling decision is made, neither the operator nor the atc system will know the exact destination airport. this paper mainly deals with the first part which is the metroplex routing algorithm while the second part scheduling will be simply introduced."
"a more fine-grained interpretation at the level of each student can also take place using these maps. this is needed for clear decision-making about current learning outcomes and future needs if intervention will take place at the student level. for this purpose, a student at any location on the map can be selected. based on their map location, inferences can be made about what they have mastered in this part of the complex terrain of 21 st century skills. such examples at the individual level will be discussed in an upcoming paper."
"the measurement model: algorithm used for learning analytics. in this approach to data analytics, a statistical analytic technique called a \"measurement model\" serves as the algorithm to compile the results and make inferences about learners. other fields such as computer science that come to learning analytics from a different historical basis often use different vocabulary to describe such algorithms. for instance, the rasch model often used in educational assessment from a computer science perspective would be considered an algorithm employing a multilayer feed-forward network [cit] ) with g as the rasch function (a semi-linear or sigmoidal curve-fitting function), in which weights (item discrimination) are constrained to one for all inputs, and the item parameters estimated are only the thresholds on each item node (item difficulty). the 2pl irt model, by contrast, is an algorithm employing a multilayer feed-forward network with g as the 2pl function (also a sigmoidal curve-fitting function), in which both weights (item discrimination) and thresholds on each item node (item difficulty) are estimated. the 3pl model is an algorithm employing a multilayer feed-forward network with g as the 3pl function (sigmoidal), in which weights (item discrimination), thresholds on each item node (item difficulty), and a lower asymptote (guessing parameter) are estimated."
"however, a few students remained in the emerging consumer terrain. here, students could perform basic searches for information and knew that some digital tools existed to do more. but they were often routinely unable to employ digital tools to perform more than the most basic tasks. some students were challenged even with the notion of clicking a link to navigate to another page, where the information they sought could be found. others exhibited only the concept of linear reading, where they would attempt to read through each web page they reached from top to bottom. without the concept of scanning or searching for information within a page, they were unable to progress very far with the activities, or contribute effectively to collaborations. also, students in the emerging terrain often exhibited no notion of questioning sources or considering the credibility of information. these students would need substantial support with their digital literacy skills to take part in virtual or distributed collaborative learning."
"airborne traffic congestion, airports in close proximity, and limited infrastructure resource lower the efficiency in those busy metroplex airspace. separation rules and environmental problems also bring down efficiencies. resource optimization of all the runways and airports and associated separation rules within the same metroplex will improve the overall throughput and also potentially reduce noise and emissions."
"a flexible flight is initially routed towards a metroplex instead of a pre-determined destination airport. unlike the regular flight having a fixed destination airport, when the flight arrives the decision boundary of the destination metroplex, it will receive the information of which runway of which airport to land in."
"according to the requirement of c++ bgl, we have each waypoint provided by facet in a c++ struct and put all these waypoint structs in an indexed c++ map. each struct contains the name, latitude, longitude information of the waypoint. the waypoints serve as the nodes in a jetway network graph."
"a metroplex is a region with several close airports which share traffic resources such as airspace, ground transportation. more rigorously, a metroplex consists of several close airports with consequential dependencies. each metroplex subsists in a system-of-systems which includes the airports, the flights and ground traffic between them, the airline companies, air traffic control (atc) services etc. fig. 1 illustrates the types and layers of networks in national airspace system (nas) and how they interact with the metroplex operations [cit] . studying a composite network combining the service networks of airline companies will provide insights into nas-wide traffic between the various airports as nodes in this network (δ-level in fig. 1 )."
"for metroplex routing and scheduling, there are very few literature works existing. however, many weather avoidance reroute generating algorithms have already been developed and they can be borrowed to solve the routing problem with sector congestion constraint by treating congested sectors as severe weather areas. [cit] adopted a grid network model to generate a reroute for a general aviation aircraft in free flight. unfortunately, this algorithm can not be extended for commercial flights with air traffic control."
"after we have the jetway network, we can optimize the routes for input flight plans of flexible flights. since the waypoints offered by facet may not be complete, the new waypoints mentioned in the input flight plans need to be imported to the jetway network. at the same time, all the airports are imported by treating them as waypoints too. the additional edges are obtained from facet by the historic flight track files. together all the additional inserted elements with the jetway network formulate the jetway plus network, which is the final data structure to apply our routing algorithm."
"object removal is one of the most important image editing functions. as is shown in fig. 4, the key idea of object removal is to fill in the hole that is left behind after removing an unwanted object, to generate a visually plausible result image. the exemplar-based inpainting algorithm for object removal can preserve both structural and textural information by replicating patches in a best-first order, which can generate good image quality for object removal applications [cit] . in the meanwhile, this algorithm can achieve computational efficiency thanks to the block-based sampling processing, which is especially attractive for a parallel implementation."
"(a) when a student's x matches an item location, the probability of that student succeeding on that item is expected to be 0.50; (b) when a student's x is above the item, then the probability is above 0.5 (and vice-versa); and (c) these probabilities are governed by a logistic distribution [cit] for a discussion of this)."
"the idea of mara-p is to compute the optimal rate for each link (according to mara's mathematical model), considering the size of the packet to be transmitted. this can be done either online (whenever a packet has to be transmitted through a link, the best rate is computed) or offline (the best rate for each possible size of computed periodically and stored in a table for later consults). both options are computationally expensive: the first in terms of time, and the second in terms of space."
"for the past few years, mesh networks have experienced a huge increase in popularity, mostly due to their potentially low cost of deployment and maintenance. many companies already provide wmn solutions [cit], although their costs are, usually, still elevated. nevertheless, there are low-cost commercial solutions targeted specifically at end-users [cit] . moreover, there are also consolidated open solutions often applied to digital inclusion projects [cit] ."
"in summary, mara-rp's implementation works as follows. slsp defines the packet size classes it will use and informs them to pprs by writing to the file /proc/pprs classes (see fig. 2 ). it then computes the routing tables and informs them to the linux kernel using system calls. after that, iptables rules are created to mark the packets before the routing decisions, according to their size. also, routing rules are created by slsp, associating the marks with their respective routing tables. these rules are informed to pprs through the file /proc/pprs. after these configurations, the operating system takes charge of doing the correct forwarding."
"in this topology, the destination for the tcp flow was always node 0. in the actual network, this node works as a gateway for the internet. the source of the flow was varied from nodes 1 to 9. for each pair of nodes, the experiment was repeated six times. the 95% confidence interval is plotted in all graphs. fig. 4 shows throughput results for the indoor topology for three different sources: nodes 1, 6, and 9. as expected, as the geographic distance between source and sink nodes increases, the throughput decreases. for the closest sources, mara's performance is good, but very close to the other proposals. indeed, when the destination is node 1, 10 out of the 13 combinations had quite comparable performances. when the destination is node 6, the number of combinations with similar performances drops to four. for sources farther than node 6, mara's performance decreases much slower than the others'. when the source is node 9, this difference becomes evident, with mara achieving four times over the throughput of any of the other proposals. throughout all our topologies, this tendency was verified: mara's performance increases (with respect to the other proposals) with the increase of the geographic distance between source and sink nodes. as we select farther source nodes, we increase the complexity of rate adaptation and routing metric problems since more alternative paths and heterogeneous links are available. with diversity, the advantages of mara are accentuated. fig. 5 shows the average packet loss rate and end-to-end delay of the tcp flow between nodes 0 and 9. mara's higher throughput is explained by its low delay [(third best in this experiment, as shown in fig. 5(a) ] and very low packet loss rate [the best in the experiment, as shown in fig. 5(b) ] although other combinations presented a slightly lower average delay (hop count/samplerate and etx/samplerate), they achieved this by sacrificing the packet loss rate. therefore, mara could keep a good balance between these two performance metrics."
2) gradient calculation: the partial derivatives of the objective function with respect to the robot pose x r are required for solving the optimisation problem described by (7) . these gradients represented in (8) can be expanded with the help of (6) as (9) .
"recent evolution of mobile devices such as smart-phones and tablets has facilitated access to multi-media contents anytime and anywhere but such devices result in an explosive data traffic increase. [cit] that these traffic demands will be grown up to 24.3 exabytes per month and the mobile video streaming traffic will occupy almost 72% of the entire data traffic [cit] . interestingly, numerous popular contents are asynchronously but repeatedly requested by many users and thus substantial amounts of data traffic have been redundantly generated over networks [cit] . motivated by this, caching or pre-fetching some popular video contents at the network edge such as mobile hand-held devices or small cells (termed as local caching) has been considered as a promising technique to alleviate the network traffic load. as the cacheenabled edge node plays a similar role as a local proxy server with a small cache memory size, the local wireless caching has the advantages of i) reducing the burden of the backhaul by avoiding the repeated transmission of the same contents from the core network to end-users and ii) reducing latency by shortening the communication distance."
"the inferred control unit is an fsm working in both transmitter (tx) and receiver (rx) modes. its overall structure is given in fig. 4, where dash lines denote parallel states. in tx mode, the controller first waits for a start signal from the upper layers indicating that a data frame is available. in this mode, the controller consists of two major states called super-states. first is the idle super-state corresponding to the inactive state of the transmitter. after detecting a start from the mac layer for example, it transitions from the idle to the framing super-state where data coding is sketched. generally speaking, dataflow transmitters are feed-forward architectures hence less complex to implement as compared to their associated receiver. the framing super-state is declined into three parallel sub-states namely, the coding state, the insert state and finally the bl-reconf state. in the coding, typical mapping operations together with filtering operations are performed. the output samples are fed to the dacs (digital to analog converter) prior to carrier frequency modulation. in parallel to the coding state, an insert state is active. this state manages the insertion of specific data in the frame, both at the time and spectral levels. considering the standards using an ofdm modulation, they require to inject pilot symbols from time to time within the spectrum for coherent detection. moreover, a data frame often includes constant fields (e.g. preamble) that remain the same in all the transmitted frames. for the purpose of reducing the overall computation, such fields are one-time computed and inserted (at run-time) at the sample level in the frame before dacs. the bl-reconf state handles the block-level (fine-grained) reconfiguration of the transmitter. as aforementioned, modern standards require certain blocks to be adaptive (acm) i.e. changing their properties on-the-fly. one approach consists in hard-coding all the configuration of the block once, then using software controlled switch to select the desired configuration at run-time. a second approach is to reconfigure the block when a given configuration is desired. it is a suitable approach which fits the best to the paradigm of sdr and requires indeed partial reconfiguration capabilities."
where f i denotes the i-th field. each field f i is characterized by its duration t i its constant or variable nature state of its transported data p ayload:
"an alternative, adopted by mara-p, is to take the offline approach, but computing and storing the rates only for a small set of packet sizes. mara-p defines classes of packet sizes (for instance, from 1 to 300 b) and computes the best rate for the highest value of the interval using (1). mara-p considers this rate to be also the best rate for every other size within the same class. whenever a packet has to be transmitted through a link, mara-p finds the packet size class to which the packet belongs and consults the rate table."
"a moc is an abstract specification of how the computation is managed. radio link applications can be modeled using the synchronous data flow (sdf) moc [cit] . this moc represents a program as a directed graph in which each node, communicating via channels, consumes or produces a predetermined and fixed number of data (tokens) per invocation. an example of sdf signal flow graph is given in fig. 1, where fb1, fb2, fb3 and fb4 denote functional blocks (fbs) communicating through channels. an sdf implementation requires a static scheduling and sdf was employed in various embedded systems design tools such as the development environments derived from the ptolemy project [cit] . a typical sdf-based dataflow architecture is given in fig. 2 where the fbs depict the computing cores. fig. 2 can be interpreted as an implementation of the graph in fig. 1 . at the inputs and outputs of the fbs, fifos are interfaced to control the streaming flow. an important element that is making the waveform structure consistent is the control logic, which purpose is to orchestrate the data routing and computing in graph. indeed, it provides the system with specific signals that are used to change the state of the system. this generic overview of the dataflow architecture clearly shows that, for efficiency purposes, enhancements can be performed at three distinct levels, namely the fbs, the communication infrastructure and finally the control unit. in this work, the fbs and the communication infrastructure are synthesized using hls tools/languages as it will later be explained. an emphasis is given on the controller required to schedule the resulting waveform running on an fpga device in context of multirate sdr."
"in this section, in order to investigate how channel selection diversity affects the optimal caching solution, we first consider a noise-limited network; when the number of active users is much smaller than the number of caching helpers, the impact of interference is negligible compared to the noise power and the typical user can be served without resource sharing with other users."
"where dt is the distance transform of the occupancy grid map m and x o is the template generated using the laser scan s in (5) with the potential robot pose x r y r φ r . therefore we propose to use an optimisation algorithm to solve this problem in order to obtain the optimum pose x r that will yield minimum chamfer distance,"
"the data-path is described by using a set of heterogeneous fbs that are sourced from hls-based libraries. the instantiation of each block requires specifying both the inputs and the outputs data rates using the rates defined at the beginning of the specification. the idea is to provide a rich description for each block in order to optimally synthesize the corresponding rtl. the hls tool intended for each block is specified at that time by using the annotations #catapultc or #vivadohls. indeed, an important development work has been carried out to implement multiple fbs that are gathered in libraries. so far, those blocks are synthesizable either by catapultc from calypto or by vivado-hls from xilinx. an ongoing work aims at including native rtl fbs, by using the annotation #rtl. furthermore, real-time constraints for each block are taken into account. these constraints are the latency, the throughput or the clocking requirements of the block that are further used to implement the control unit of the waveform. the provided dsl-compiler generates, for each fb, a .tcl script which is used as an interface with the intended hls tools to synthesize the rtl solution. the compiler is also charged of the consistent gluing of the generated rtl solutions by using rtl wrappers. it also performs some type checking and many other verification in order to ensure consistency in the final waveform."
now we show that the objective function in p3 is concave and optimization problem p3 is also the constrained convex optimization problem. if we define g i (p i ) as
"success of content delivery in wireless cache network depends mainly on two factors: i) channel selection diversity gain and ii) network interference. for given realization of nodes in a network, these two factors dynamically vary according to what and how the nodes cache at their limited cache memory. specifically, if the more nodes store the same contents, they offer the shorter geometric communication distance as well as the better small-scale fading channel for the specific content request, which can be termed as channel selection diversity gain. on the contrary, if the nodes cache all contents uniformly, they can cope with all content requests but channel selection diversity gain cannot help being small. moreover, according to content placement, the serving node for each content request dynamically changes, so the network interference from other nodes also dynamically varies. thus, it might be required to properly control the channel selection diversity gain and network interference for each content."
"it is important to note that, in contrast to ekf or particle filter based localisation algorithms, the statistics related to the robot pose estimate at the previous time step is not exploited during the proposed optimisation algorithm. if odometry is available, it is possible to compute an estimate for the robot pose using the incremental motion from the robot pose at the previous time step and easily fuse the two results as they are independent of each other."
the implementation of mara in a real environment was developed using one of the wireless mesh networks of the remote project [cit] . this network is composed of linksys wrt54g commercial routers using a customized firmware based on the openwrt linux distribution [cit] .
"the snr algorithm is often considered to be optimal. the idea is to choose the highest possible rate in the transmitter such that the snr in the receiver is high enough to decode the frame with an error probability lower than a given threshold. this idea, however, is not feasible in practice because it depends on future information. nevertheless, the snr algorithm is usually employed on simulation-based evaluations as a baseline. there are also other proposals that use simplified versions of the snr algorithm, such as receiver-based auto rate (rbar) [cit] ."
"in recent years, there have been growing interests in wireless local caching. the related research has focused mainly on i) femto-caching with cache-enabled small cells or access points (called as caching helpers) [cit], ii) deviceto-device (d2d) caching with mobile terminals [cit], and iii) heterogeneous cache-enabled networks [cit] . for these local caching networks, varieties of content placements (or caching placements) were developed [cit] and for given fixed content placement, the performance of cache-enabled systems with different transmission or cache utilization techniques was investigated [cit] . specifically, content placement to minimize average downloading delay [cit] or average ber [cit] was proposed for fixed network topology. in a stochastic geometric framework, various content placements were also proposed either to minimize the average delay [cit] and average caching failure probability [cit] or to maximize total hit probability [cit], offloading probability [cit] . however, these caching solutions were developed in limited environments; they discarded wireless fading channels and interactions among multiple users, such as interference and loads at caching helpers."
"the notation r i,k and h i,k represent the distance and the channel fading coefficient from the typical user to the caching helper with the k-th smallest reciprocal channel power gain among the caching helpers storing content i, respectively. note that the caching helper with the largest instantaneous channel power gain is equivalent to that with the smallest reciprocal of the channel power gain (i.e., ξ i,1 ). assuming gaussian signaling and time/frequency resource sharing among the users associated with the same caching helper, the mutual information between the typical user requesting content i and its serving caching helper is"
"in this paper, we presented mara, a joint approach to the problems of automatic rate adaptation and routing metrics. although these two problems have been frequently considered separately, we argue that they are closely related and dependent. moreover, we propose a method of per conversion among different transmission rates of a given link. this method allows mara to obtain accurate statistics about each link while maintaining low overhead. in addition to proposing mara, we also present two possible variations, mara-p and mara-rp, which take into consideration the size of the packet to be transmitted in the rate and route selection decisions."
"one issue of using olsr as a routing protocol when evaluating routing metrics is the use of multipoint relays (mpr). the mprs of a node are defined as a set of direct neighbors through which can reach every 2-hop neighbor (in at most 2 hops). olsr uses the concept of mpr to reduce the overhead associated with the diffusion of control messages, increasing the scalability of the protocol. however, as a side-effect of its mpr usage, olsr may not find the optimal network paths when using metrics other than hop count. the problem happens because the topology graph generated by olsr contains only links between nodes and its mprs. if the used metric is hop count, using only these links is enough for finding the optimal paths. however, when using a different metric, this property does not hold. therefore, allowing olsr to use its mpr mechanism would not be fair for evaluating routing metrics because links with good quality that might be used in one or more paths may be discarded before the shortest path selection. with this in mind, we disabled the usage of mprs by olsr during our simulations."
"the motivation behind conducting both real and simulated experiments is to take advantage of the qualities of both methods. simulations are completely reproducible and very flexible and make it possible to extract information about every network event. on the other hand, a real environment provides all the complexity of wireless communication systems, and one can trust its behaviors are not a result of simulation artifacts. with this in mind, we opted for including in our simulations a topology modeled after the real testbed."
"in this section, a design flow that was defined to enable rapid prototyping of fpga-based sdr physical layers is presented. in addition, a practical example of a radio protocol specification is depicted as proof of concept. this flow relies on a dsl which enables to specify a data frame, its attributes and a data-path. a dsl-compiler is provided to both infer the control unit of the physical layer and operate the connection between the instantiated blocks. the instantiated fbs are compiled down to rtl with hls tools that are integrated to the flow."
"∂dt ∂xo i and ∂dt ∂yo i in (9) can be obtained by looking up the gradients of the distance transform with respect to global x, y coordinates. as mentioned previously distance transform and its derivatives can be precomputed using the grid map and stored to make the gradient computation computationally algorithm 1 details the steps of solving the localisation problem."
this demonstrates how c-log performs in a real dynamic environment. the fig. 9 shows the plot of the pose estimates from the algorithm and the plot of the laser on those poses. the trails of moving people on the map can be clearly seen on the figure.
"in the previous section, the cache-based channel selection diversity gain for each content has been highlighted and the optimal caching probabilities to balance them were derived without consideration of interference. in this section, in the presence of network interference, we derive near-optimal content placement and analyze the effects of network interference"
"to validate our simulation results, we conducted a series of experiments in a real testbed using the implementation described in section iv. these experiments took place in a topology depicted in fig. 3 (after which the indoor topology used during the simulations was modeled). the real experiments used a methodology very similar to the one used in the simulations. the only difference was that an icmp flow (using the ping tool) was added to each experiment to provide a measurement of the round-trip time (in replacement of the one-way delay of the main flow, which is difficult to measure in practice). another difference between the practical and simulated experiments is in terms of compared proposals. in the practical experiments, we could only compare mara to combinations of the metrics etx, ml, and hop count with the rate adaptation algorithm arf because we did not have access to the source code of the wireless interface driver of our routers. therefore, it was not possible to implement other rate adaptation proposals. fig. 11 shows the round-trip time (rtt) and the throughput between nodes 0 and 9. as in the simulations, mara performed better than the other proposals. it achieved roughly twice the throughput of the second best proposal. in terms of rtt, mara performed almost three times better. by repeating this experiment varying the sink node (from 1 to 9), we noticed the same trend observed in the simulations. as the distance from source to sink increases, the relative performance of mara (with respect to the other proposals) also increases. fig. 12 shows the average rtt for the audio flow between nodes 0 and 9. differently from what happened in the simulations, mara-p and mara-rp had different results. mara-p was slightly better than the original mara, while mara-rp had the worst result. considering the confidence intervals, nevertheless, it is not possible to state that mara-p was definitively superior. for other pairs of nodes (sources and sinks), the results of mara and mara-p were also very close (which is consistent with the simulations). besides presenting the worst result, it is interesting to notice that mara-rp also had the highest variability. although in the simulations mara-p and mara-rp were equivalent, the processing overhead of mara-rp is much higher (which is not considered by ns-2 simulations). hence, the processing overhead affects both the average performance and its variation."
where f i is the content requesting probability and ρ i is the target bit rate of content i [bits/s/hz] to successfully support the real-time video streaming service of content i without playback delay.
"the proposed method for converting a link's per presents an issue. the function that relates snr and per (for a given rate and a given frame size) has an asymptotic behavior for both low and high values of snr. as snr increases, per approaches 0. conversely, as snr decreases, the value of per approaches 1. the extreme cases (per equals 0 or 1), however, are never reached because, independently of how high (or low) the snr is, there is always a chance of failure (or success)."
"the solution found to this issue was the creation of a loadable kernel module (lkm) for the linux kernel. this lkm, called per-packet rate selection (pprs), maintains a table of transmission rates and monitors the packet transmission process within the kernel. whenever a packet is about to be sent to the network interface, pprs consults the table looking for a match for the current packet's parameters (e.g., next hop and packet size). pprs then requests the network interface to change its transmission rate to the one matching the packet. fig. 2 gives an overview of the complete implementation. when pprs is loaded, it creates two files in the /proc directory. on linux, files in this directory work as a communication channel between the user space and the kernel space. specifically for pprs, these two files allow the definition of packet size classes and rules for choosing transmission rates. these definitions are done by simply writing to /proc files (this is done by slsp). besides creating the /proc files, pprs also intercepts packets in their regular transmission path within the wireless interface and puts itself as an intermediary. using this strategy, pprs is able to process packets exactly before they are transmitted by the network interface and to configure its transmission rate. once the correct rate is configured on the interface, pprs puts the packet back on the normal transmission path in order to complete the process, as shown in the lower part of fig. 2 ."
"it is important to notice that these two problems, automatic rate adaptation and routing metrics assignment, are strongly related. the characteristics of a wireless link, as evaluated by routing metrics, are dependent on the chosen transmission rate [cit] . for instance, if the routing metric evaluates packet error rates, it is important to know what is the current transmission rate, when the rate will be modified, and what impact this change will have on the link quality. however, despite this strong dependency, historically these two problems have been studied separately, leading to suboptimal solutions [cit] . this paper presents a new mechanism based on a joint approach for solving these two important problems in wmns: automatic rate adaptation and routing metric assignment. the idea is to use a cooperative cross-layer method, called metric-aware rate adaptation (mara). with this method, rate adaptation decisions are based on statistics provided by the routing metric. conversely, aware of the chosen rate, the metric may provide better estimates for link costs."
"the problem with this proposal is that the per must be inferred at each available transmission rate using probes. thus, the overhead associated with this solution is considerably higher than with the original ett metric."
"there is yet another class of rate adaptation algorithms targeted at differentiating frame losses caused by collisions from the ones caused by channel degradation. on dense environments under heavy traffic loads, nodes may experience an increasing number of frame collisions, which may cause automatic rate adaptation algorithms to misinterpret losses as a sign of channel quality degradation. among the proposals on this class, one can cite the snoopy rate adaptation (sra) [cit], the robust rate adaptation algorithm (rraa) [cit], and the collision-aware rate adaptation (cara) [cit] ."
"lemma 1: the cdf of the smallest reciprocal of the channel power gain, ξ i,1, in a nakagami-m d fading channel is given by"
"the growth of the platforms complexity exhibits the limitations of the current programming languages. furthermore, these platforms evolve rapidly while the application codes are still written and maintained manually. a mainstream approach to handle such platform evolution is the modeldriven engineering (mde) [cit] . it comprises both domainspecific languages (dsl), which formalize the application structure behavior and requirements in a declarative way, and transformation engines and generators to generate multiple artifacts such as source code. the mde approach ensures a \"correct-by-construction\" development of the final product."
"besides modeling nodes' positions within the topology, we also conducted an adjustment phase to find the values for the parameters of the propagation model (the shadowing model, in this case) that best represent the actual network performance. the ns-2 implementation of this model uses three parameters: a loss exponent, a standard deviation, and a reference distance."
"according to (1), in order to compute the metric, it is necessary to know the value of etx for in every available transmission rate. since the original formulation of etx relies on broadcast probes, this value is only computed at one rate (the basic rate used in broadcast transmissions, according to the ieee 802.11 standard [cit] ). therefore, mara needs a different approach for computing etx."
"since a weighted sum of convex functions is also convex function, problem p1 is a constrained convex optimization problem and thus a unique optimal solution exists. the lagrangian function of problem p1 is"
"as mentioned before dt matrix as well as the coefficients for cubic spline interpolation for this matrix would be precalculated. the fig. 2a shows the contour map of the chamfer distance at the estimated robot pose (x r, y r is varied in the vicinity of the true pose, which is at (1.1m, 1.1m ). meanwhile, fig. 3 shows the effect of φ r on the chamfer distance. it is clear that the chamfer distance behaves well in the vicinity of the global minimum, which corresponds to the true pose of the robot. the basin of attraction in the xy plane appear to be around ±1m for this particular example. fig. 2a also reveals that there are two local minima about 3m south of the global minimum. fig. 2b shows the variation of the chamfer distance in a region with a long corridor. as expected, the contours correspond to a narrow valley, indicating that there is less information available for localising the robot in direction along the corridor."
"in addition to the main mechanism, two variations are also evaluated. the first one, named mara-p, considers the impact of packet 1 size on rate adaptation choices. similarly, the second variation, called mara-rp, considers the impact of packet size on both rate adaptation and routing metric choices."
"where (c) is due to independence of the channel; (d) comes from the probability generating functional (pgfl) of ppp; (e) is from the mogment generating function (mgf) of the nakagami-m i distribution. substituting (60) into (56), we obtain"
"for assigning the link cost, mara-p proceeds exactly as the original proposal of mara. in other words, it computes the expression of (1) considering the size of the probe packet."
"an adaptive fast fourier transform (fft) block, as the main block of the ieee 802.11a phy, specification is also given in fig. 5 . it corresponds to a set of fft blocks varying by the size of the fft. partial reconfiguration technique is used to configure the desired fft at run-time. in order to comply with the requirements of partial reconfiguration, i.e. non varying i/o set between two configurations, we have made the fft input and output non vector type. the samples are then reordered inside the fft for a given fft size. fig. 6 shows a design space exploration for a 256-point fft, performed with catapultc in order to obtain different solutions of the same design. the solutions were synthesized for a virtex 6 fpga platform by using the xilinx ise tool suit and the results are collected after place and route. it is important to mention that as the fpgas are getting larger, i.e. including more resources; it is now possible to program complex waveforms into a single one. this exploration shows indeed an increasing number of slices depending on the data rate. hence suitable solution can be chosen for the final design."
"another feature that is making this flow specific to sdr is the fact enabling the reconfiguration of fb at run-time. indeed, all the fbs that are subject to run-time reconfiguration are annotated with the key word adaptive. in practice, they are defined into partial reconfiguration region on the fpga and interfaced with larger memory resources in order to store the incoming data stream while operating the reconfiguration. once the system enters the bl-reconf state, all the blocks that are not impacted by the reconfiguration are held active by the controller while the rest of the graph is being stalled. as soon as the reconfiguration terminates the data coding or decoding is performed normally."
"results from experiments with c-log algorithm presented in algorithm 1 together with dataset 1 to localise a robot are presented in this section. at the beginning, an approximate initial robot pose was provided. in subsequent iterations, the optimum pose estimate given by the optimisation algorithm was updated with encoder odometry measurements that were available in the dataset. this updated value was then used as the initial guess to the robot pose for the next iteration. comparison between the pose estimates obtained after gmapping was completed and the poses obtained with c-log are shown in fig. 7 ."
"even though the distance from any point to the nearest object is a continuous value, distance transformation as described above quantises these distances into pixels. furthermore, the derivatives of a distance transform function are not continuous at points which belong to the map or to the cut locus [cit] . as the intention is to use the distance transform to calculate the chamfer distance and use it within an optimisation algorithm, an interpolation algorithm based on a cubic spline approximation is used to compute the distance transform and its derivatives at any given location in the map. all future references to dt in this paper refers to the interpolated version of the dt matrix."
"future work includes a deeper analysis (using analytical methods) of mara-p and mara-rp to determine whether the two proposals are indeed equivalent (as suggested by our simulation results). this analysis may also help to understand the little benefit obtained with these variations, with respect to the original mara proposal. we also intend to improve the cost model of mara by taking into consideration propagation effects, intraflow interference, and interflow interference."
"in dataset 1, a robot takes three loops on a floor with multiple rooms. map of the environment was generated with gmapping algorithm [cit] using the information gathered during loop 3. the experiments were carried out using the remainder of the data to ensure the integrity of the evaluation. dataset 2 does not have multiple independent runs. therefore, every other laser scan was used to create the map, and rest was used for evaluating the c-log algorithm. the dataset 3 has multiple runs in the environment and was collected using a turtlebot™ with a hokuyo® 30m laser range finder. the first run that was used to create the map was done with the environment free of any dynamic objects while the data used for the experiment was collected about eight months after the first run when there were people moving about."
"for each content i, we denote a set of the reciprocals of the channel power gains from φ i to the typical user in ascending order as"
"1) behaviour of the chamfer distance in the vicinity of the true robot pose: to further explore the properties of the chamfer distance lets consider the following example. it is assumed that the robot operates in a 2-dimensional planar environment and the occupancy grid map of the environment is available. robot position with respect to the map is (x r, y r ) and the orientation is φ r . the robot has a 2d laser range scanner which reports range (r) and bearing (θ) data with respect to the robot coordinate frame."
"when the typical user receives content i from the caching helper with the smallest reciprocal of the instantaneous channel power among the caching helpers storing content i, the other caching helpers interfere with the typical user because they are assumed to serve other users. then, the received signal-to-interference ratio (sir) at the typical user is represented as"
"in practice, the hop count metric does not perform well because the quality of a wireless link depends on a number of factors, such as length and interference sources. therefore, different wireless links tend to present different levels of quality."
"other proposals, known as quality-aware metrics, improve performance by dynamically evaluating characteristics of links. for instance, the expected transmission count (etx) metric [cit] tries to estimate the number of layer-2 transmissions necessary to successfully transmit a packet between two nodes. the etx metric works by periodically broadcasting control packets to infer a per on every link. since the transmission of a packet in the link layer usually can be modeled using a geometric distribution (the sender repeats the transmission until it succeeds), the expected number of link-layer retransmissions is the reciprocal of the per."
"theorem 1: when the typical user receives content i from the caching helper with the largest instantaneous channel power gain, the average success probability for content delivery p s in a nakagami-m d fading channel is obtained as"
"the expression used by mara to define the cost of a link, presented in (1), is parametrized by the size of the packet. the proposal presented so far supposes this value is always a constant, namely the probe packet size. however, the size of data packets transmitted by each node varies according to the network traffic. therefore, the following questions arise."
"one can notice that the structure of a frame gathers exploitable information (duration, source, data properties) that can be leveraged to achieve better control performance. the duration of each field helps generating the read and write clock signals during the appropriate slot of time. the oversampling or down sampling mechanisms imply some rate changes that are tracked across the graph. to manage the block-level communication, fifos are interfaced between two consecutive blocks. in our approach, these fifos are inferred from the specifications. they are implemented by hls tools by using the fpga memory resources. in addition, each block within the graph is meant to perform a given action on a specific set of data. once this action terminates, the blocks may no longer be required then disabled. for instance, some functional blocks may address only synchronization and some others address only data decoding. it is then convenient to control the activation and deactivation of each fb."
"the cost computed using (1) is associated with a transmission rate, which is the best possible rate in the proposed model (i.e., the rate that minimizes the link transmission delay, considering the average number of transmissions). as such, the rate adaptation component of mara selects as unicast transmission rate of link ."
"is a set of the caching helpers which do not cache content i and ξ i is a set of the reciprocals of the channel power gains from φ i . note that the interfering signal power dynamically changes according to content placement of caching helpers since it is a function of ξ i,1 and φ c i . therefore, optimal caching probabilities are expected to be obtained by optimally controlling channel selection diversity and network interference for given content popularity and cache memory size."
"as with the routing metrics, there is vast literature on rate adaptation algorithms. perhaps the simplest and most widely adopted [cit] mechanism is the auto rate fallback (arf) [cit] . this algorithm keeps track of sequences of frame transmission successes and failures. if arf detects that the number of consecutive failures has reached a given threshold (by default, 2), the current rate is decreased. conversely, if the number of consecutive successes transpasses its given threshold (by default, 10), arf increases the transmission rate."
"the result of this approach is a set of topology graphs, i.e., if the number of defined packet size classes is, then there are different topology graphs, one for each class of packet sizes. applying a shortest-path algorithm to each graph results in routing tables. whenever a node has to decide how to forward a packet, it first decides to which size class it belongs and then consults the appropriate routing table."
"given that the objective function in (7) is twice differentiable, this unconstrained non-linear optimisation can be solved using a variety of gradient based techniques. in the experiments presented in section iv matlab implementation of the trust-region algorithm was used."
"in terms of routing metrics [cit], the very first proposal is the hop count. this metric considers the best path to be the one with lowest number of hops. this approach, however, does not take into account the differences between wireless links since it considers all network links to be equally good."
"the mara mechanism has two major components: the routing metric, which evaluates and assigns costs for network links, and the rate adaptation, which chooses the most suitable transmission rate for each link. these two components share information and make coordinated decisions."
"thus, each block is affected a time slot to process a field when this field is traversing the graph. they are then activated depending on the ongoing field. to achieve this, the controller decides a starting moment for each block in the graph. this starting time is computed by considering both the graph structure and the properties (latency and throughput) of each block composing it. indeed, each state is associated to a distinct data-path and once the system enters a state, the processing starts with a specific block that is tagged as a reference block. the activation moment of the remaining blocks in the graph is then estimated based on the latency and the throughput of the blocks preceding them. blocking reads and blocking writes are also implemented since the blocks are interfaced with fifos. this algorithm has been integrated into an sdr phys design flow. the flow is implemented as a dsl which addresses rapid prototyping of the phy. it relies on the principle of the mde and its details are discussed in the next section."
"in rx mode, the fsm is composed of three super-states. an idle state, as in tx mode, denotes the inactive state of the receiver. in this state, the receiver monitors the environment seeking for an incoming signal. once a signal is detected through an rssi (received signal strength indicator) for instance, the receiver transitions from the idle state to the pre-sof state. the pre-sof state consists essentially of synchronization tasks as imposed by most of the standards. once the system enters the pre-sof state, a set of synchronization elements has to be detected and computed within a certain delay. if not, the system returns in the idle state. these synchronization elements detection and computation are associated to an event that is called start-of-frame (sof). an sof detection makes the system transition from pre-sof to post-sof where a coherent data decoding is sketched. the post-sof state is declined into three parallel substates namely, the decoding state where most of the signal processing is required, the sync-track state in which the system keep on tracking synchronization elements and finally the bl-reconf state to handle the run-time block-level reconfiguration as in tx mode."
"in the case of template matching, the highest computational complexity lies on the creation of the distance transform map which should be created for every input image. recent implementation advances to the algorithm of creating the dt includes multi-cpu and gpu based implementations [cit] which enable faster execution and hence make it possible to use chamfer distance in people recognition and tracking on surveillance footage in real-time."
"to cope with low-cost requirements, wmns usually employ off-the-shelf hardware based on the ieee 802.11 standard. these devices are capable of operating at multiple transmission rates, varying from 1 to 54 mb/s. however, selecting the most suitable transmission rate is not trivial since there is a tradeoff between link capacity and transmission rate. typically, packet error rate (per) increases with the transmission rate, considering the same signal-to-noise ratio (snr) conditions, because lower rates tend to use more robust modulations and code rates. furthermore, wmn nodes must implement a dynamic route discovery mechanism. one of the core elements of this mechanism is the routing metric. even though there are a number of metrics with very coherent formulations, they all face the obstacle of obtaining consistent and reliable information about the quality of wireless links [cit] ."
"1) is the optimum transmission rate dependent on the size of the packet we wish to transmit? 2) does the optimum route depend upon the size of the packet we wish to transmit? in order to answer these questions, in this paper we propose two variations of mara: mara-p and mara-rp. in the first variation, mara-p, we consider the size of packets in order to choose the best rate, whereas in the second, mara-rp, the packet size is also taken into consideration for choosing the best route. notice again that packets with different sizes present different per."
"1) artificially reduce the rate at which laser scans are used. 2) ignore odometry observations. 3) ignore odometry observations except orientation estimate. this simulates a scenario where robot is not equipped with encoders but a solid-sate gyroscope is available for orientation estimates. 4) corrupt laser range finder readings to simulate dynamic objects in the environment. in this scenario, a percentage of the range readings were replaced with a random number from a uniform distribution in [0, actual range]. the table i shows the mean squared errors (mse) of the post estimates under different scenarios in comparison with the results presented in fig. 6 . experiments were stopped when the algorithm failed to converge to the actual pose at some point in the trajectory. in summary, when odometry is available, c-log performs well even when every tenth laser scan is used. without odometry, it is possible to localise with every fourth scan while every fifth scan is adequate if an orientation estimate is available. furthermore, c-log was able to tolerate significant corruption of the range observation resulting in accurate pose estimates with 25% of scans containing false data. it fails to converge only when 60% of the scans contain corrupted data. note that pose errors particularly in x and y are quite large at this error level. it was observed that even beyond these limits, the algorithm would still perform well over the majority of the robot trajectory, but would fail to accurately localise in some places."
our solution is quite similar while relying on a dsl. it is also specific to fpga-based sdr. an algorithm is proposed to infer a control unit from a dsl specification as discussed in the next section.
"as discussed in section iii-b both the frame and the datapath are used to infer the control unit and the dsl provides an environment to describe a rich specification of these two aspects of a waveform. an important feature that was added in the dsl is to clearly mention, for each fb, a list of the fields that it is intended to process and their respective source (i.e. another fb) by using the key words processing and from respectively. these information make it possible to track each field within the graph hence activating the fb when required. the dsl-compiler parses these information and generates the controller."
"the metric component of mara evaluates routes according to the expected end-to-end delay. to do so, it assigns each link a cost given by the following expression: (1) where represents the th available transmission rate, is the etx of the link using rate, and is the size of the probe packet used to infer etx. the physical meaning of is the total transmission delay of the link, considering all expected retransmissions."
"in this paper, a novel software over-layer to specify a phy and implement it over fpga platforms is proposed. indeed, the nascent radio protocols are expected to meet a certain data throughput and the fpgas include specialized resources such as dsp slices, block-ram (bram) and clocking resources, which enable achieving such data throughput requirements. the proposed design flow relies upon high level synthesis (hls) tools [cit] that provide a mean to compile an abstracted description (c/c++) of an application down to its rtl description. to enable the specification of a multi-rate dataflow application running on fpga, the main contributions describes in the paper are:"
"using this precomputed table, it is possible to infer the snr of a link from the packet error rate computed with probe packets at the basic rate (as done by the etx metric). the transmission rate and the frame size (probe size) are known from the transmission of the probe packets. the per can be approximated by the value measured using the probes. therefore, the table can be consulted in order to find a value for the snr. conversely, using this snr estimate and choosing a target rate for a new table consult returns an estimate for the link's per."
"to validate our simulation results, we present an implementation of mara and its variations in a real wireless mesh network composed of 10 nodes. with this implementation, we reproduced our simulations, obtaining results very similar to those of our simulations. again, mara was considerably superior to other proposals (two times better in terms of throughput, and three times better in terms of delay, considering the two most distant nodes). one difference between simulations and practical results was the performance of mara-rp. while in the simulations, mara-rp was totally equivalent to mara-p, during the practical experiments, mara-rp's processing overhead resulted in low performance and high variability."
"t . when the robot makes an observation, the range scanner captures n number of range (r i ) and bearing (θ i ) scans. the template s that needs to be aligned with the map is given by (5) ."
"2) (2) where denotes the error function. table i shows the values of and found for each combination of transmission rate and frame size of the original table. fig. 1 compares the original data and the obtained curves for four different rates, considering 1500-b packets. even tough this paper considers the transmission rates available for the ieee 802.11b/g standard, we argue that the method described here can be applied"
"assuming that f b j processes the field f i of duration t i at an input rate of f in and output rate of f out, the block requires being enable a number to time equals to:"
"although these metrics take different approaches quantifying the quality of wireless links, they are all based on the same per estimation method. this process retrieves statistical data from periodical broadcast probes, which, according to the 802.11 standard, are always sent at a robust rate. however, since nodes usually apply a rate adaptation algorithm, data packets are typically sent at higher rates in which packet error rates are possibly higher. therefore, these metrics apply their models to possibly inaccurate statistics, leading to suboptimal performance."
"theorem 2: when the typical user receives the requesting content from the caching helper with the smallest reciprocal of instantaneous channel power gain, the average success probability of content delivery is bounded below by"
localisation using wireless sensor networks rely heavily on optimisation based methods. [cit] explains how different techniques are applied to this unique problem and how optimisation based methods can solve this localisation problem.
situations where the optimisation algorithm fails to converge was dealt with by processing the next laser scan with the current best estimate of the robot pose during the experiments reported. using the current best estimate of the robot pose and its uncertainty in order to take appropriate action in such situations is also an avenue for further work.
"in the context of fpga-based sdr, hls significantly helps with rapid prototyping. as aforementioned, it shortens the design time and improves the overall productivity. thus, given the requirements in terms of flexibility of sdr and the capabilities of hls, integrating hls within an sdr-phy design flow can yield the development process. a preceding study [cit] consisted in showing the gain for an hls-based design flow. to this end, a software-based over-layer is proposed to abstract the sdr-phy designing process and anticipate on sdr requirements. in our design flow, we will consider fbs being specified in hll and synthesized by hls tools."
"data framing is imposed by most of the modern radio protocols and often specified accurately by the wireless standards [cit] . a frame is composed of fields, conveying various type of information. this structure enables a consistent data coding and decoding while ensuring the inter-operability of software and hardware solutions released in the market. by considering a unique data frame structure, the designers are given the freedom to implement the solutions to process the frame. the resulting datapath and control are deeply associated to structure of the data frame."
"another challenge faced when implementing mara was the mara-rp variation. this variation generates multiple routing tables, one for each packet size class. fortunately, such a feature is available in the advanced routing implementation of the linux kernel [cit] . to use multiple routing tables, it is necessary to create routing rules that associate packet characteristics to each table. among the possible characteristics is the logical mark of the packet (a field internal to the kernel of the local node), which can be created through the iptables tool [cit] ."
"besides comparing mara to other proposals in the literature, we also performed simulations to compare mara and its variations. to do so, we substituted the tcp flow with three constant bit rate (cbr) flows, modeling audio, video, and a background traffic. the choice for these three traffic models is due to their different characteristics in terms of packet sizes. the audio traffic is configured to use small packets (120 b at a rate of 48 kb/s), while the video traffic uses medium-size packets (900 b sent every 40 ms) and the background has the largest packets (1400 b sent every 22.4 ms). hence, we could observe how mara-p and mara-rp react in the presence of packets of different size classes. fig. 9 shows end-to-end delay and packet loss rate results for the audio flow, obtained in the random topology. the graph shows that mara-p and mara-rp both obtained lower end-to-end delay at the cost of a higher packet loss rate. this higher packet loss rate is expected since mara-p and mara-rp tend to choose higher transmission rates to smaller packets. although in this set of simulations, the variations have shown a slight performance increase (in terms of delay), the general trend, considering all simulation topologies, was of a tie. throughout our simulations, mara-p and mara-rp could not be consistently better than the original mara proposal. another interesting result is that, in all simulations, mara-p and mara-rp were totally equivalent. the two variations always took the same decisions. fig. 10 gives an insight to why the variations did not improve the performance of mara. it shows the end-to-end delay of each packet of the audio flow in the indoor topology (between nodes 0 and 3), considering the executions of mara and mara-p (as stated in the previous paragraph, mara-p and mara-rp yielded identical results). to provide a fair comparison, only the packets successfully received by both proposals were plotted. as the curves show, the end-to-end delay varies with time for both proposals since the propagation model used (shadowing) is probabilistic. when links' condition improves, mara-p achieves delays similar to mara. at some points, as expected, the delays with mara-p are even lower. however, when the propagation condition becomes worse, mara-p's delay increases much more than mara's. this happens because mara tends to be more conservative than mara-p in terms of rate choice for small packets. therefore, mara's choices result in a lower number of layer-2 retransmissions when the condition of the links gets worse."
"in typical particle filter implementations [cit], the mismatch between the map and a range observation is computed using the difference between actual range measurement and the expected range to a corresponding point on the map. ray tracing from a particle placed at a potential robot pose is used to determine correspondences. given its iterative nature, ray tracing is not a suitable strategy for defining d(s, m ) in (1), if the latter is to be solved using a numerical process."
"the datapath, on both the transmitter and receiver side, is a set of interconnected fbs as illustrated in figure 2 . they are characterized by their latency (l), throughput (t p ), input and output data rates (f in and f out ). for each block, the inputs and outputs rate are known and they can be managed using enable signals. moreover, each block is activated or deactivated on a per-field basis since computation happens to be specific to a given field. thus, let f b j be the j-th fb within the dataflow graph:"
"the distance transform matrix, dt for any point x in the map m, can be pre-computed and stored in a matrix, taking a significant computational burden away from the algorithm. for a given template the distance transform can be computed in two passes over the image [cit] and using which the cost function (2) can be evaluated in linear time o(n) via (4) [cit] ."
it's also worthwhile to mention that implicit function theorem can be exploited to analytically compute the influence of the noise in the measurements on the estimated robot posê x r [cit] .
"we studied probabilistic content placement to desirably control cache-based channel selection diversity and network interference in a wireless caching helper network, with specific considerations of path loss, small-scale channel fading, network interference according to random network topology based on stochastic geometry, and arbitrary cache memory size. in a noise-limited case, we derived the optimal caching probabilities for each content in closed form in terms of the average success probability of content delivery and proposed a bisection based search algorithm to efficiently reach the optimal solution. in an interference-limited case, we derived a lower bound on the average success probability of content delivery. then, we found the near-optimal caching probabilities in closed form in rayleigh fading channels, which maximize the lower bound. our numerical results verified that the proposed content placement is superior to the conventional caching strategies because the proposed scheme efficiently controls the channel selection diversity gain and the interference reduction. we also numerically analyzed the effects of various system parameters, such as caching helper density, user density, nakagami m fading parameter, memory size, target bit rate, and user density, on the content placement."
"v. conclusion sdr is an outstanding concept that requires to be further developed. as the fpga technology evolves, it turns out to be a good candidate for sdr solutions. the mde approach is a promising alternative toward the optimization of the development process of an embedded system in general. our contribution is essentially to bring together both sdr and mde concepts into an sdr-phy design flow, with an emphasis on control aspects. the flow is featured with the nascent hls tools and implements several phy artifacts from the specifications. this combination enables to achieve good design performance, thanks to hls tools, and to significantly shorten the required development time."
"since mara is a mechanism that combines both routing metrics and automatic rate adaption principles, there are three lines of related work: routing metrics, rate adaptation algorithms, and some recent joint approaches. in the next three sections, we summarize some proposals in each of these lines."
"where n ins i is a random load of the tagged caching helper when an arbitrary user receives content i from the caching helper with the largest instantaneous channel power gain. to characterize (37), we require both the probability mass function (pmf) of the load at the tagged caching helper and the sir distribution when multiple contents are cached at each helper and the association is based on the instantaneous channel power gains. however, unfortunately, the exact statistics of the required information are unavailable because they are complicatedly determined by many interacting factors, such as multiple cached contents, locations of caching helpers and users, content request of users, instantaneous channel fading gains, etc. thus, the optimal caching probabilities to maximize (37) have to be found by numerical searches of which complexity is prohibitively high for a huge number of contents. in this context, we propose near-optimal content placement to obtain some useful insights in interferencelimited scenarios. to this end, we first approximate (37) with the average load of the tagged caching helper [cit] as"
"in each of the states, a set of dataflow computation is intended. in the context of fpga, the data-path is once mapped to dedicated resources and ready to operate as soon as the fpga is powered on. one of the roles of the controller is to distribute the clock signal to activate or deactivate the fbs when required. we leverage the properties of the data frame together with the intrinsic structure of the data-path (dataflow) to build the control unit. first, a data frame f is as a collection of fields i.e.:"
"is the set of caching helpers which do not cache content i in their cache memory. the small-scale channel fading terms of the desired link and the interfering links follow the independent nakagami-m distributions with parameters m d and m i, respectively."
"here n is the number of points in u . the distance transform is an implicit shape representation whose pixel value indicates the minimum distance from that point to the closest object. once a distance map dt of an occupancy grid map m is generated using an unsigned distance function via (3), which specifies the euclidean distance from each pixel to the nearest edge pixel in v, it is clear that computing the chamfer distance of a scan s with respect to the map m is very straightforward."
"require: dt, compute the best guess to the pose estimate at the next time step: update x* using odometry information or use x* if odometry is not available. end loop fig. 4 shows the manner in which the optimisation algorithm converges to the solution from an initial guess which is about 1.2m away from the true location, with an angle offset of π 8 . as seen, the initial guess is within the basin of attraction. there are 33 iterations in this instance as the initial location is far away from the actual pose."
"the computer vision literature is abound with distance measures between image contours, for example chamfer distance [cit], hausdorff distance [cit], that do not require defining explicit corresponding point pairings. chamfer distance gives the average mismatch between m and s for all points, rather than the worst mismatch between m and s as in hausdorff distance. in this work it is proposed to use the chamfer distance to compute d(s, m )."
"a total of five topologies representing different kinds of scenarios (with different characteristics) were used during the simulations. because of space restrictions, though, only the most relevant results are presented here."
a typical fpga-based development process of a dataflow application is twofold. the first part consists in specifying the system in an abstracted way using high-level languages (hll) such as c/c++ or matlab. this representation highlights the sub-elements of the system and their interactions. it often considers streaming data as floating-point data types that are refined to fixed points when it comes to implementation. the second part consists in a hardware description of the system. hardware description languages (hdl) such as verilog or vhdl are used to implement the specification previously written in hll. the implementation gives a functional description of the system and takes account of the computing and communication resources available. a mainstream trend is to merge these two designing steps into a single one. the hls bridges the gap between the specification and the implementation. it takes as an input a function specification written in a hll and generates its rtl description for a specific target platform (e.g. fpga). the existing hls tools emphasize on the functional aspects of the application and they propose several design optimization techniques to fulfill the performance requirements without shortcomings.
"to avoid such an overhead increase, mara adopts a different approach, shown in algorithm 1. this approach is based on a process of conversion of the links success probabilities. this conversion happens in two steps."
"our simulations show that mara is consistently superior to several combinations of routing metrics and automatic rate adaptation algorithms of the literature in various topologies. the simulation results also show that mara can maintain a good balance between end-to-end delay and packet loss rate, and that the performance of mara, relative to the other proposals, increases with the distance between source and sink nodes. at the most extreme example, mara was eight times better than the second best proposal. with respect to the variations, in our simulations they showed little or no benefit. in most cases, mara-p and mara-rp were not considerably superior to mara. more interestingly, perhaps, is the fact that mara-p and mara-rp were completely equivalent in all our simulations."
maps generated from gmapping was processed to remove isolated pixels of size less than 4. a simple gating function (10) was used to eliminate the obvious outliers from the laser range finder data. the gate admits only the values that are smaller than a maximum error. this is the only tuning parameter required for this algorithm and clearly it is relatively easy to establish.
"where ∆x, ∆y and ∆φ are the maximum expected error in the initial guess. in the experiments 0.15m were used for ∆x and ∆y while ∆φ was set to 0.05rad."
"the dataflow model of computation (moc) [cit] has encountered a lot of success in the real-time signal processing domain. it describes an application as a set of functional blocks that run concurrently and communicate through fifo channels. this view of the moc represents the processing unit of the application, which is usually associated to a control unit. this associated control unit can be modeled as a finite state machine (fsm), which provides a behavioral description of the application. in most of the sdr frameworks, the functional description of the blocks are written using highlevel languages such as c/c++ and the application runs over digital signal processors (dsp) based platforms. blocks can be also written at the register level (rtl) level by using a hardware description language (hdl) and run over field programmable gate array (fpga), however this description differs from the initial idea of sdr where the blocks are software defined."
"since the ppp of φ i is transformed by the displacement and mapping theorems, ξ i is also a ppp [cit] . therefore, the cdf of ξ i,1 is obtained as"
"where when the typical user receives content i from the caching helper with the smallest reciprocal of the channel power gain (i.e., the largest channel power gain), the cumulative distribution function (cdf) of the smallest reciprocal of the channel power gain (i.e., ξ i,1 ) is derived in lemma 1."
"experimental results presented in section iv demonstrate that the optimisation based technique proposed in this paper provides a competitive solution to the problem of robot localisation within an occupancy grid. even with a matlab implementation, each iteration of the optimisation process took only 20 msec. the algorithm converged within 14 iterations on average, making it near real-time for the intel data set where laser range finder operates at about 5 hz."
"the evaluation methodology consists of comparing mara to different combinations of routing metrics and rate adaptation algorithms from the literature. each experiment consists of a 300-s tcp flow between a specific pair of nodes, and the performance metrics considered were throughput, end-to-end delay, and end-to-end packet loss."
"software defined radio (sdr) is a concept that has come to maturity [cit] . according to the wireless innovation forum [cit], an sdr is a radio in which some of the physical layer (phy) functions are software defined. indeed, these radios are mainly characterized by their ability to support different waveforms. different attempts to standardized sdr have led to two major works, namely the software communication architecture (sca) [cit] and the space telecommunication radio system (strs) [cit] . these two frameworks provide the building environment for sdr, however a need for tools to design the phy is still pointed out. the gnuradio [cit] and the sdrphy [cit] projects are examples of software frameworks. they both use a block-based model of the phy applications, namely a dataflow program, to implement such phys."
"the received power at a distance is defined as (3) where is the reference distance, is the path-loss exponent, and is a normal random variable with zero mean and the standard deviation specified in the model. the value (the average received power at the reference distance) is computed using the free-space model. table ii summarizes the parameters used in this paper."
"1) data association: in the context of localisation, data association is the process of matching uncertain measurements to known feature points in the map. as the chamfer distance represents the entire laser scan (cluster of laser readings), its value at any given robot pose x r represents a measure of the placement of the whole scan, not just any individual laser reading. therefore, data association does not need to be explicitly considered when the objective function for robot localisation is evaluated."
"localisation of a robot relative to a given map of the environment based on information gathered from sensors mounted on the robot has been a well-studied problem. when the map can be represented using geometric primitives such as points or line segments, extended kalman filter (ekf) based algorithms are capable of efficiently estimating the robot pose within the map by fusing information gathered from robot odometry and observations to geometric beacons in the environment [cit] . if the environment map is available in the form of an occupancy grid, particle filter [cit] has become the most popular technique for robot localisation when a range-bearing sensor such as a laser ranger finder or a rgb-d sensor like microsoft®kinect is available."
"on a running platform such as fpga on which each function is mapped to dedicated resources, an fsm is a good candidate to implement a control unit. its main task is to handle consistently the traversal of data through the graph. thus, depending on the current state and the conditions on the inputs and outputs channels, the controller activates or deactivates a set of fbs. this approach is only valid when the properties of the graph remain unchanged. the fsm in this case appears to be relatively simple to implement. in the context of sdr, some part of the graph could be reconfigured at run-time. for instance, most of modern standards [cit] implement adaptive coding and modulation (acm) technique in order to enhance the bandwidth efficiency. acm consists in adapting the modulation and coding schemes according to the environment. thus, each part of the data frame may be modulated or coded differently. on the receiver side, data should be decoded with the appropriate demodulation algorithm so that the decoding remains consistent. modern waveforms require a controller that can switch between configurations at run-time."
"t of a mobile robot with respect to an a-priori map m using a collection of data points s obtained from a sensor such as a laser range finder. this is essentially an exercise in finding the best alignment between the map m and the scan s. if some function d(s, m ) that quantifies the mismatch between m and s at some robot pose x r exists, then the robot localisation problem can be stated as:"
"the rate selection algorithm of mara-rp works exactly as the one used by mara-p. the difference of this variation lies on the cost assignment process. while mara and mara-p use the probe packet size as a constant value for (1), mara-rp computes the expression for every packet size class (using the highest value of the interval as the packet size, just as mara-p does for rate selection)."
"ireless mesh networks (wmns) [cit] are composed of mesh routers and client nodes. the mesh routers compose a mesh of wireless links that is used for multihop communication by client nodes. each mesh router may act as an access point, serving as an internet gateway for client nodes, or simply as a part of the backbone, forwarding packets from other routers."
"two different implementations of mara and its variations were developed in this paper for evaluation purposes. the first implementation was in the form of a module for the ns-2 simulator [cit], whereas the second was a practical implementation based on the openwrt linux distribution [cit] . in both cases, the following packet size classes were defined for mara-p and mara-rp: [cit] . this section gives details on both implementations, discussing some of the challenges found during the development."
"to evaluate the performance of mara and its variations, comparative experiments have been conducted in both simulated and real environments. in these experiments, mara has been compared to every combination of a number of rate adaptation algorithms and routing metrics of the literature. the results show that mara is consistently superior to these other proposals in terms of throughput and end-to-end delay, especially in scenarios with large number of hops."
"therefore, for given finite memory size and content popularity, the average success probability of content delivery can be maximized by controlling the channel selection diversity gains for each content. this can be achieved by optimally determining caching probabilities in random content placement. consequently, the corresponding optimization problem can be formulated as"
"it is important to note that in contrast to its vision applications the algorithm proposed in this paper uses a single fixed query image which is the occupancy grid map against multiple templates which are laser scans. therefore, the distance transform and its derivatives with respect to the image coordinates can be precomputed and stored together with the interpolation coefficients."
"in this section, we present a performance evaluation of mara and its variations. this evaluation was conducted on both simulated and real environments, using the implementations discussed in section iv."
"since mara manipulates both rate adaptation and routing information, its implementation has been separated in two modules: a routing module and a rate selection module. the routing module is responsible for collecting information about links and assigning them costs. this module also selects the optimal transmission rates for each link and informs them to the rate selection module. in turn, the rate selection module stores the optimal rates for each link using a table and analyzes each packet before transmission to configure the wireless interface to the proper rate."
"the most straightforward solution is to simply manipulate the transmission rate for broadcast frames, so that mara can send the etx probes at every available rate. this way, mara would have statistical data in order to compute etx in all rates. however, considering, for instance, the ieee 802.11b/g mixed mode (widely used in commercial devices), there are 12 available rates. hence, this strategy would considerably increase network overhead."
"since mara also has a rate selection component, the original ns-2 implementation of the ieee 802.11 standard was not applicable to our evaluation because it does not allow dynamic rate adaptation nor does it consider the effect of different rates in per. to cope with such needs, in this paper we adopted the dei-802.11-mr module [cit] instead. the dei-802.11-mr implementation not only allows dynamic rate adaptation, but also includes better models for wireless medium phenomena such as self-interference and capture effect."
"the metric component of mara can be implemented over any routing protocol based on link state or distance vector. although there are many protocols optimized for multihop wireless networks, in this paper we chose to develop a simple protocol, called simple link state protocol (slsp), which just implements the basic functionalities of the link-state algorithm. this protocol was written in c and implements the metrics hop count, ml, and etx besides mara and its variations. the goal of the rate selection module is to create a communication interface between the routing protocol and the wireless interface in order to provide the necessary functionalities for mara. this module is needed due to the configuration inflexibility of the drivers for the wireless network interfaces available in the market. for instance, on most interfaces, it is not possible to configure different transmission rates for different neighbors. specifically, that is the case with the routers used in our testbed. there is not a mechanism on the interface driver that allows slsp to inform the transmission rate for each neighbor. in fact, the driver only allows the specification of one transmission rate."
"the route selections performed by the ett and ml metrics in this topology are very interesting. in both cases, the metrics opted for paths along the borders of the grid instead of following the diagonal (which is geometrically shorter). one hypothesis to explain these choices is that both ett and ml are influenced by \"border effects\" in this topology. the ml metric has a great tendency to choose only very good links. if there is an extremely long path composed only of links with perfect delivery probability, an ml will choose it over a much shorter one with at least one \"imperfect\" link. in this topology, border nodes suffer interference from fewer neighbors when compared to the inner nodes. therefore, the use of an active probing metric may cause the estimated delivery probability of inner links to be worse than the estimated delivery probability of nodes closer to the borders. given the sensitivity of ml to these parameters, the result is the preference for paths along the borders. the issue with ett is somewhat different since this metric accounts for the number of transmissions necessary to deliver a packet. therefore, the number of hops should have a greater weight in the route choices, as it had for mara, etx, and hop count. however, the version of ett evaluated on these experiments magnifies these border effects since it uses much more probes than the other proposals. this hypothesis is coherent with the throughput results shown in fig. 7(a) . although longer paths induce higher intraflow interference, the use of links with better quality and that are less prone to interference from probe packets yielded comparable results between ett and ml. this hypothesis, however, still needs more analysis to be proved or refuted."
"an external dsl has been defined to model the phys meant for sdr applications running on fpga devices. the dsl provides the primitives to describe on one side the data frame and on the other side the data-path by considering the data frame attributes. a frame is considered as a collection of fields that are destined to be processed independently by the fbs of the processing unit. a typical specification with the dsl starts by including fbs hls-based libraries. after that, a set of data rates are defined in order to model the multi-rate constraints within the graph. these definitions are used to impose reading and writing rates to the different blocks. then, each field of the data frame is described independently by highlighting some of their features like duration or eventually data redundancies for optimization purposes. constant fields are stamped with the key word #fieldc. they are once computed, in contrast to variable fields that are specified with the key word #fieldv and modulated through the data-path. in rx mode, since the system requires a detection of an sof to switch from the pre-sof to the post-sof state in order to further process the data, one of the fields is designated as an sof. this field is typically a preamble that is appended for synchronization purposes."
"a c or a c++ implementation and further experimentation to confirm the real-time performance of the proposed algorithm is planned for the immediate future. one of the main advantages observed is that the algorithm does not require tuning parameters, except for a relatively large gate for filtering outliers from laser range data. this is due to the fact that the models of process and observation uncertainty are not used within the optimisation algorithm. these models are only required when the robot pose uncertainty is computed and if odometry observations are to be fused to the estimate provided by the optimisation algorithm. further work to examine whether the observation gate can be replaced with a robust kernel in order to adequately deal with outliers is also planned."
"to avoid this issue, mara uses probe packets in four different transmission rates: 1, 18, 36, and 54 mb/s. these rates were chosen because, according to the data on the conversion table, their curves intersect on useful intervals. the points plotted in fig. 1 show the data available on the conversion table for these four transmission rates. for instance, the lowest snr value that results in a per value of 0 for the 1-mb/s transmission rate is associated to a per lower than 1 for the 18-mb/s transmission rate, as shown in fig. 1 . when mara has to compute the metric for a link, it first chooses one of the four probabilities using algorithm 2. in the code, is an array, containing the link error rates estimated at each of the four transmission rates. this simple algorithm chooses as the most appropriated statistics for the current link the one associated with the higher probe rate, such that the per is lower than 1. therefore, extreme values (0 and 1) are avoided, improving the precision of the snr estimate."
"is the average load of the tagged caching helper when the user requests content i to the caching helper with the largest instantaneous channel power gain. the validity of approximation (38) is demonstrated in fig. 4, where red star and blue circle represent the monte-carlo simulation (37) and its approximation (38), respectively. this figure verifies that the approximation (38) is quite tight to (37) for arbitrary p 1 . moreover, a lowerbound of (38) is obtained in the following theorem."
"a frame is generally considered at distinct levels, namely the bit level, the symbol level, and optionally the sample level. it is mainly characterized by its duration, its source (e.g. an fb) and is composed out of fields. the nature of the data conveyed by each field can be either specific information (e.g. modulation schemes or coding rate), synchronization information (e.g. preamble) or useful data (e.g. data payload). an ofdm frame is given as an example in fig. 3 where each field is characterized by its duration and the type of transported data."
"there are works in the literature that suggest using a variation of the ett metric based on unicast probes [cit] . in other words, each node has to send one control packet per neighbor to collect statistical data for computing per. despite yielding more precise statistics, this approach has serious scalability implications, especially on dense networks."
"1) the average snr of the link is estimated using the information provided by probe packets. 2) the average snr is used to estimate the link success probability in every rate, which is later used to compute etx for each rate. both steps require the knowledge of a function that relates snr and the success probability of a link. while defining a closed expression for such a function is not trivial, previous works have collected data through experiments and simulations for all transmission rates used in the ieee 802.11 b/g mixed mode [cit] . these data can be used to build a table relating four physical quantities: snr, transmission rate, frame size, and per. such a table is used, for instance, by the dei802.11-mr [cit] module, an enhanced implementation of the ieee 802.11 standard for the ns-2 simulator [cit] ."
"to provide a higher degree of scalability to the method, the periodical probes are sent in broadcast. this guarantees that the overhead does not increase with the number of neighbors. moreover, instead of sending the probes of each rate all at once, mara sends only one probe per period. in other words, in the first period, mara sends the probe at 1 mb/s. then, in the second period, it uses 18 mb/s and so forth. with this policy, the overhead is even lower than the one caused by the original formulation of the etx metric since probes at 18, 36, and 54 mb/s use the wireless medium for less time."
"since the pathloss dominates the small-scale fading effects according to lemma 1,n ins i is approximated as the load of the tagged caching helper with the largest channel power gain averaged over fading (i.e., the load based on the association with long-term channel power gains),n ins i ≈n i . moreover, the received sir with the association based on instantaneous channel power gains is larger than that with the association based on long-term channel power gains. accordingly, (38) can be further bounded below as"
the idea to define a new language instead of using an existing language aims first at making the most of the expressiveness given by a specialized language. a dsl can be either internal or external [cit] . an internal dsl is a language that is nested within a host language. the host language is generally a well-known general purpose language (gpp) such as c++ or java. external dsls in opposition enable to explicit any domain by defining a new syntax and an appropriate grammar. they require a lot more efforts to be implemented but they remain much less constrained in comparison to internal dsls.
"nevertheless, in practice, per can reach values so close to 0 or 1 that it is not possible to estimate it with the necessary precision. due to limitations in available memory and bandwidth, it is not feasible for routing protocols to use a very large number of probes for computing per. most likely, the number of probes considered is in the order of hundreds of packets. in this case, if a link has a very low per, for example, chances are no probes will be lost during the window considered by the routing protocol, and per will be estimated as 0. therefore, for practical purposes, the function that relates snr and per is not injective because, for the extreme values of per, there may be many associated snr values. in other words, it is impossible to properly evaluate the snr of a link if its estimated per is equal to 0 (or 1)."
"this example further illustrates the fact that the control requirements are more complex in the context of sdr. changing some parts of the waveform (i.e. the dataflow) at run-time is quite complex. this complexity in terms of processing and control in modern phys led the designers to re-think the overall designing process. the hls partly addressed these issues by enabling to go straight from specifications to the implementation. nonetheless, issues relative to control remain while considering hls."
where: n -the number of considered speakers. the block diagram illustrating the methodology of the snr i and snr w calculation is presented in fig. 4 . it shows the processing chain for a single microphone (analogous processing can be applied for all microphones in the array).
"the results of speech recognition accuracy on the modality speech corpus indicate the analogous behavior of both: the state-of-the-art asr and the self-developed asr system when noisy speech is introduced. speech accompanied with babble noise is one of the most difficult cases for asr systems to operate with, as it causes the occurrence of insertion and substitution errors in the recognized speech, leading to an increase of wer which is visible from achieved results. limiting the list of possible word outputs (rs-constr.) results in a significant decrease of wer when the isolated commands are recognized. furthermore, the wer results for en-gb are slightly better (i. e. lower) what can be explained by the fact of having native british english speakers in the modality corpus. no wer results for audio video speech recognition in case of realsense engine can be provided for comparison with our method as it is a closed solution accepting only audio stream as an input."
the file naming convention is presented in table 2 . for example: speaker24 s5 strl.mkv is a file containing the fifth session of sequence recording in noisy conditions of the speaker no. 24 by the left stereo camera.
"finally, statistics of co-occurrence matrix (gcm-inner and gcm-outer) of lips pixels in 4 directions are used. the co-occurrence matrix c is defined as in (13):"
"in this paper we go one step further and additionally allow the channel to vary in an unknown and arbitrary manner from symbol to symbol. this is the concept of arbitrarily varying wiretap channels (avwc), which provides a suitable and robust model for secure communication under channel uncertainty, especially for such scenarios where the legitimate users are confronted with unknown varying interference induced by other coexisting transmitters."
up to now we have ensured that there is a random codẽ c ran with polynomial many elements for the avwc w with passive wiretapper with the desired properties. the next step is to make this code suitable for the case with an active wiretapper as well.
"where: e s -energy of the speech signal, e n -energy of the noise. in order to accurately determine the snr indicator according to the formula (1)), several steps were performed. first of all, during the preparation of the database, every type of disturbing noise was recorded separately. at the beginning of the noise pattern, a synchronizing signal (1 [khz] sine of 1 [s] long) was added. the same synchronizing signal was played while making recordings of the speech signals in disturbed (noisy) conditions. owing to this step, two kind of signals were obtained: disturbing noise only (e n ) and speech in noise (e s +e n ). both of those recordings include at the beginning the same synchronizing signal. after obtaining synchronization of the recordings, it was possible to calculate the energy of speech signal (e s ). a digital signal processing algorithm was designed for this purpose. the snr calculations were performed in the frequency domain, for each fft frame (index i in e i,n (f ) and e i,s+n (f ), denotes the i-th fft frame of the considered signal). the applied algorithm can calculate instantaneous snr (snr i (i)) based on formula (2):"
"for every speaker 144 files were generated (9 audio files, 2 video files, 1 label file per 12 recording sessions), which were named according to the following principle:"
"where: j -number of the word spoken by k-th speaker, k -number of considered speaker. in the same way, it is also possible to calculate the average value of the snr indicator for a given speaker (snr s ), using formula (6):"
"here we assume that all parties, i.e., the legitimate users and the wiretapper, have access to a common randomness. this assumption can be motivated by the fact that this is realized over a public channel which is open to the wiretapper."
"where: h is the number of correctly recognized words, d is the number of deletions, s is the number of substitutions and i is the number of insertions. the evaluation of implemented avsr system was performed on recordings contained in the modality corpus in leave-one-out (speaker independent) cross validation manner as shown in table 4 bringing results described in the following subsection of the article. values in table 4 follow the terminology used in the modality corpus as presented in table 2 with more details. the test speaker was excluded from hmm models trainings and then the procedure was repeated until all speakers were tested. in case of testing in the presence of noise the c4 audio recordings were used."
"the task is now to establish a reliable communication between the transmitter and the legitimate receiver in the presence of unknown varying channel conditions and, at the same time, keeping the confidential information secret from the wiretapper. this is formalized as follows."
"the corpus has been made publicly available through the servers of the gdansk university of technology. the database is accessible at the web address: http://www.modality-corpus. org. the access is free, but the user is obliged to accept the license agreement. the web service overview page is presented in fig. 7 . the website is divided into four subpages:"
"a new multimodal english speech corpus was developed and presented. owing to its open access it can be used for researching and developing audio-visual speech recognition systems. the modality corpus enables researchers to test their avsr algorithms using the video material recorded with a better quality then it was hitherto offered by available corpora. it also offers an opportunity to test the robustness of recognition algorithms, thanks to the inclusion of recordings made in challenging conditions (presence of acoustic noise, non-native speakers employment). the authors expect that the avsr research progress will benefit from sharing the corpus among the research community free of charge. it is visible from preliminary experiments that in the environment where speech is accompanied by acoustic noise, the addition of visual parameters results in an improvement of automatic speech recognition accuracy and in lowering of wer. further data analysis of provided audio and visual parameters in the modality corpus may lead to creating of avsr systems that will be more robust to noise than those representing state-of-the-art, commercially available asr engines, based on the acoustic modality, only. the future work related to a corpus extension is planned. the modality corpus would benefit from extending it with additional recordings made with the same setup, containing some of the language material present in older, but popular corpora (e.g. vidtimit, grid). hence, results achieved with the new, high-quality corpus might be compared with previous avsr research achievements. the corpus could also be improved by adding some variations to the video modality (e.g. head poses, changing lighting conditions). moreover, the comparison between results of the proposed classification methodology and several state-of-the-art classifiers is envisioned."
"to facilitate the testing of audio-visual speech recognition algorithms, hand-made label files were created, to serve as ground truth data. this approach revealed also some additional advantages, especially that numerous minor mishaps have occurred during the recordings, including speakers misreading and repeating words or losing their composure (especially while reading random word sequences), instructions being passed to the speaker (e.g. please repeat) and pauses being made due to hardware functioning problems. the supplied label files include the position of every correctly-pronounced word from the set, formatted according to the htk label format. this addition prevented from having to repeat the recording sessions after every mistake occurrence. since the actual mistakes have not been removed from recorded material, it can be used to assess the effectiveness of disordered speech recognition algorithms [cit] ."
"where: i and j are the image intensity values, (p, q) are the coordinates, n and m define the size of the image i and ( x, y) is the offset. for feature extraction, the region of interest is placed either on the outer lip contour (gcm-outer) or on the inner lip contour (gcm-inner)."
"wireless communication systems are inherently vulnerable for eavesdropping due to the open nature of the wireless medium. the physical properties of the wireless channel make the communication accessible to external wiretappers but, on the other hand, also offer possibilities to establish security by other approaches than cryptographic techniques."
"furthermore, the luminance histogram parameters are provided. both histogram-inner and histogram-outer parameters represent the 32-element luminance histogram in which the bins are evenly distributed over the luminance variation range. histogram-inner denotes the analysis bounding rectangle placed on the inner lips region, whereas histogram-outer represents the outer lips region. moreover, the vertical lips profile is extracted in following manner: the rectangular bounding box encloses the outer lip region or the inner lip region, then it is scaled using linear interpolation to 16-pixel height and the for each row of the rectangle the mean value of r channel from rgb color space is calculated, resulting in verticalprofile-outer or vertical-profileinner parameters."
"where: j -number of the word spoken by k-th speaker, k -number of considered speaker, nnumber of fft frames for j-th word and k-th speaker (word boundaries were derived from the data contained in the label file -see next section for details)."
"based on the acoustic energy of speech (expressed in [db] ) and snr w calculated for every spoken word, the distribution of levels of the given indicator was calculated (for all speakers) as is presented in fig. 6 . we can observe that the disturbing noise causes a shift of the snr histogram by 18.8 db towards lower values. [cit], 1993; [cit] ) ."
"where: (12) j and k are the actual analyzed pixel coordinates, and n is equal to 32. similarly, the dct-outer is calculated, besides the outer lips contour instead of the inner contour is enclosed by the bounding rectangle."
"where: w represents the recognized word, w i is the i-th word in a training data, o av represents the sequence (vector) of combined both acoustic and visual features that can be for the evaluation purposes the word error rate (wer) metric is introduced [cit] ). according to wer definition, the recognition results are evaluated in terms of deletion, substitution and insertion errors, hence wer is calculated as in formula (20):"
"as the raw video files consumed an extensive volume of space (about 13 gb of data for a minute-long recording of a single video stream) a need for a compression arose. beforehand, an additional processing was needed in order to perform demosaicing of the original bayer pattern images, which was performed using a self-developed tool for this purpose. the compression was done in ffmpeg using h.264 codec. the results were saved to '.mkv' container format with the size of almost 18 times smaller than the original files size. some sample images are presented in fig. 2 . additionally, the h.265 codec was used in order to reduce the amount of data needed to be downloaded by the corpus users. therefore, the material is available in two versions: one encoded using h.264 and another one using h.265 codec. authors decided to use two codecs as the second one is currently still less popular and its full implementation is still under development. however, as the h.265 codec is more future-oriented, thus the user is given a choice per coding type, entailing the file size. the depth data from time-of-flight camera is recorded in raw format and the sample images are presented in fig. 3 ."
"as high-quality audio can be provided with relatively low costs, thus the main focus during the development of a avsr corpus should be put on the visual data. both: high resolution of video image and high framerate are needed in order to capture lip movement in space and time, accurately. the size of the speaker population depends on the declared purpose of the corpus -those focused on speech recognition, generally require employment of a smaller number of speakers than the ones intended for the use in speaker verification systems. the purpose of the corpus also affects the language material -continuous speech is favorable when testing speech recognition algorithms, while speaker verification can be done with separated words. ideally, a corpus should contain both above types of speech. the following paragraphs discuss historic and modern audio-visual corpora in terms of: speaker population, language material, quality, and some other additional features. the described corpora contain english language material unless stated otherwise."
"remark 2: if the wiretapper has no access to the common randomness, the legitimate users can immediately use this resource to create a secret key corresponding to the size of the common randomness and therewith keep the confidential information completely secret from the wiretapper."
"the multimodal database presented in this paper aims to address above mentioned problems. it is distributed free of charge to any interested researcher. it is focused on high recording quality, ease of use and versatility. all videos were recorded in 1080p hd format, with 100 frames per second. to extend the number of potential fields of use of the dataset, several additional modalities were introduced. consequently, researchers intending to incorporate facial depth information in their experiments can do that owing to the second camera applied to form a stereo pair with the first one or by utilizing the recordings from the time-of-flight camera. investigating the influence of reverberation and noise on recognition results is also possible, because additional noise sources and a set of 8 microphones capturing sound at different distances from the speaker were used. moreover, snr (signal-to-noise ratio) values were calculated and made accessible for every uttered word (a detailed description of this functionality is to be found in section 3.4)."
"cuave (clemson university audio visual experiments), [cit] was focused on availability of the database (as it was the first corpus fitting on only one dvd disc) and realistic recording conditions. it was designed to enhance research in audio-visual speech recognition immune to speaker movement and capable of distinguishing multiple speakers simultaneously. the database consists of two sections, containing individual speakers and speaker pairs. the first part contains recordings of 36 speakers, uttering isolated or connected numeral sequences while remaining stationary or moving (side-to-side, back-and-forth, head tilting). the second part of the database included 20 pairs of speakers for testing multispeaker solutions. the two speakers are always visible in the shot. scenarios include speakers uttering numeral sequences one after another, and then simultaneously. the recording environment was controlled, including uniform illumination and green background. the major setback of this database is its limited dictionary."
"rapid developments in communication systems make information available almost everywhere. along with this, the security of sensitive information from unauthorized access becomes an important task and a common approach is the use of cryptographic techniques to keep information secret. such techniques have a wide variety of use and are based on the assumption of insufficient computational capabilities of nonlegitimate receivers. due to increasing computational power, improved algorithms, and recent advances in number theory, these techniques are becoming more and more insecure."
"in order to validate data, gathered in the developed corpus, the experiments in speech recognition were performed. a comparison of a commercially available state-of-the-art asr engine with a self-developed asr engine preceded planned experiments. the selfdeveloped asr was implemented utilizing htk toolkit based on hidden markov models (hmm). it makes possible adding visual features besides acoustic ones. the mel-frequency cepstral coefficients (mfcc) were employed in the acoustic speech recognition mode. they were complemented by vision-based parameters calculated owing to self-developed parametrization methods of the visemes in the avsr mode. the conducted experiments consisted of solely audio-based or combined audio-visual speech recognition attempts as described in the following subsections."
further visual parameters provided in the modality corpus are named dct-inner and dct-outer. dct-inner parameters denote 64 dct coefficients calculated from the bounding rectangle placed on the inner lips contour which is linearly interpolated to the region of 32x32 pixels. the dct transform is computed from the luminance channel l of luv color space as in (11):
"iv2, [cit], is focused on face recognition. it's a comprehensive multimodal database, including stereo frontal and profile camera images, iris images from an infrared camera, and 3d laser scanner face data, that can be used to model speakers' faces accurately. the speech data includes 15 french sentences taken from around 300 participating speakers. many visual variations (head pose, illumination conditions, facial expressions) are included in the video recordings, but unfortunately, due to the focus on face recognition, they were recorded separately and they do not contain any speech utterances. the speech material was captured in optimal conditions only (frontal view, well-illuminated background, neutral facial expression)."
"the audio files use the waveform audio file format (.wav), containing a single pcm audio stream sampled at 44.1 ksa/s with 16-bit resolution. the video files utilize the matroska multimedia container format (.mkv) in which a video stream in 1080p resolution, captured at 100 fps was used after being compressed employing both the h.264 and h.265 codecs (using high 4:4:4 profile). the .lab files are text files containing the information on word positions in audio files, following the htk label format. each line of the .lab file contains the actual label preceded by start and end time indices (in 100 ns units) e.g.:"
"where: i -number of the fft frame, e i,s -energy of the speech signal for i-th fft frame, e i,n -energy of the noise for i-th fft frame. based on energy components e i,s and e i,n, the sum of energy of the speech signal e w,s and the sum of energy of the noise e w,n for a given word can be calculated using formulas (3) and (4):"
"if the wiretapper is unaware of the actual channel realization or is not able to influence the channel conditions of the legitimate users, the wiretapper is called passive. but avwcs also serve as a model for scenarios with a more powerful wiretapper which can maliciously influence the channel conditions. a wiretapper, which can control the actual state sequence of the legitimate users, is called an active wiretapper."
"the proposed algorithm is based on simultaneous processing of two signals recorded during independent sessions. during the first session, only the acoustic noise was recorded. a recording of the speech signal disturbed by the noise was acquired during the second fig. 4 block diagram illustrating the snr calculation methodology session. after a manual synchronization of the signals, the energy of the signal e n and noise e s in the frequency domain can be calculated. the window length for the instantaneous snr calculation was the same as the fft frame and was equal to 4096 samples. the sampling rate for the acoustical signals was equal to 44100 sa/s. moreover, the calculation of the snr value can be performed for the determined frequency range. in our corpus we provide two versions of snr data. the first one represents the results of snr calculation limited to the frequency range from 300 hz (f l -lower frequency limit) up to 3500 hz (f u -upper frequency limit) which corresponds to the traditional telephone bandwidth, whereas the second version was calculated for the full frequency range of human hearing (20 hz -20 khz). both versions are available in modality downloadable assets. based on the timestamps contained in the label file, it is possible to determine the snr value for every spoken sentence according to formula (5) and average snr value for considered speaker according on the basis of formula (6). these calculations were performed for all speakers and for all microphones in the array. in fig. 5 the graphical presentation of the snr i and snr w . calculation results for selected speakers were depicted. moreover, the energy of the speech and noise expressed in db were also shown."
"in this context the concept of information theoretic, or physical layer, security is becoming more and more attractive, since it solely uses the physical properties of the wireless channel to establish security. so, regardless of what the wiretapper does with the received signal, the confidential information cannot be reproduced with high probability. information theoretic security was initiated by wyner, who introduced the wiretap channel [cit] . this is the simplest scenario involving security with one legitimate transmitter-receiver pair and one wiretapper to be kept ignorant. recently, there is growing interest in information theoretic security; for instance see [cit] ."
"in this paper we analyzed the active secrecy capacity c active s,ran (w) of the avwc w with active wiretapper. we tried to characterize the relation between the active and passive secrecy capacities c active s,ran (w) and c s,ran (w) of the avwc w with active and passive wiretapper, respectively, in a similar fashion as for the deterministic code and random code capacities c det (w) and c ran (w) of an ordinary avc w."
"the database wapusk20, [cit], is more principally focused on audio-visual speech recognition applications. it is based on the grid database, adopting the same format of uttered sentences. to create wapusk20, 20 speakers uttered 100 grid-type sentences each of them recorded using four channels of audio and a dedicated stereoscopic camera. incorporating 3d video data may help to increase the accuracy of liptracking and robustness of avsr systems. the recordings were made under typical office room conditions."
"in table 3 the list of visual features described above is shown accompanied with each vector size. all described parameters are provided with the modality corpus as .csv format files, thus they can be used for further research."
"in our research multiple feature extraction methods were utilized. in the acoustic layer of the avsr system the standard mfcc features were used. the features were extracted from consecutive 10 ms -long fragments of the input signal. the hamming window was applied to the analysis frame corresponding to the speech duration of 25 ms. the preemphasis with the coefficient equal to 0.97 was used at the acoustical signal preprocessing stage. for mfcc calculation, 13 triangular bandpass filters were used. the coefficients are calculated using the formula known in the literature [cit] ) [cit] as is presented in (8):"
"the audio material was collected from an array of 8 b&k measurement microphones placed in different distances from the speaker. first 4 microphones were located 50 cm from the speaker, next 2 pairs at 100 and 150 cm, respectively. an additional, low-quality audio source was a microphone located in a laptop placed in front of the speaker, at the lap level. the audio data was recorded using 16-bit samples at 44.1 ksa/s sampling rate with pcm encoding. the setup was completed by four loudspeakers placed in the corners of the room, serving as noise sources. the layout of the setup is shown in fig. 1 . to ensure a synchronous capture of audio and video streams, fast, robust disk drives were utilized, whereas the camera-microphone setup was connected to the national instruments pxi platform supplied with necessary expansion cards and a 4 tb storage array. the registration process was controlled through a custom-built labview-based application. the pc also ran a self-developed teleprompter application. the laptop computer and teleprompter did not obstruct the microphones and cameras in any way. the position of the loudspeakers, all microphones and cameras were permanently fixed during all recording sessions. the sound pressure level of the presented disturbing noise emission was also kept the same for all speakers."
"another attempt in expanding the xm2vts corpus is dxm2vts (meaning \"damascened\" xm2vts), [cit] . similar to valid, it attempts to address the limitations of xm2vts stemming from invariable background and illumination. instead of re-recording the original xm2vts sequences in different real-life environments, the authors used image segmentation procedures to separate the background of the original videos, recorded in studio conditions, in order to replace it with an arbitrary complex background. additional transformations can be made to simulate real noise, e.g. blur due to zooming or rotation. the database is offered as a set of video backgrounds (offices, outdoors, malls) together with xm2vts speaker mask, which can be used to generate the dxm2vts database."
"the available datasets suitable for avsr research are relatively scarce, compared to the number of corpora containing audio material only. this results from the fact that the field of avsr is still a developing relatively young research discipline. another cause may be the multitude of requirements needed to be fulfilled in order to build a sizable audio-visual corpus, namely: a fully synchronized audio-visual stream, a large disk space, and a reliable method of data distribution [cit] ."
"where b g denotes a vector of coefficients (aam-texture parameters). the shape parameters indicate information concerning lip arrangement, whereas texture parameters determine, for instance, tongue or teeth visibility. in the modality corpus the aam-combined parameters are provided, which regard both the shape and texture information of the modeled object. those parameters are obtained by concatenating aam-texture and aam-shape parameters and then performing pca in order to remove correlation between both representations. the detailed description of the aam algorithm implementation can be found in related publications [cit] ."
"the material collected in the corpus uses a considerable amount of disk space (2.1 tb for h.264 codec, 350gb for h.265 codec). to give users the freedom to choose only the fig. 7 homepage of modality-corpus.org recordings they need, the files of every recording session were placed in separate .zip files. the corpus was structured according to the speakers' language skills. group a (16 speakers) consists of recordings of native-speakers. recordings of non-natives (polish nationals) were placed in group b. the group of non-natives included 5 english language students and 14 faculty students and staff members."
"another challenge in wireless networks is the question of sufficient channel state information at the users. due to the nature of the wireless channel, in practical systems there is always uncertainty in channel state information. since this has a huge impact on the performance of wireless systems, the analysis of information theoretic security for different models of channel uncertainty is an important research field and, thus, indispensable for bringing this concept into practice."
"additional innovative features of the modality corpus include: supplying wordaccurate snr values to enable assessments of the influence of noise on recognition accuracy. the audio was recorded by a microphone array of 8 microphones in total, placed at three different distances to the speaker and, additionally, by a mobile device. a feature only rarely found in existing corpora, is that the whole database is supplied with htkcompatible labels created manually for every utterance. hence, the authors presume that these assets make the corpus useful for scientific community."
"the remainder of the paper is organized as follows: section 2 provides a review of currently available audio-visual corpora. our methods related to the corpus registration, including used language material, hardware setup and data processing steps are covered in section 3, whereas section 4 contains a description of the structure of the published database, together with the explanation of the procedure of gaining an access to it. hitherto conceived use-cases of the database are also presented. example speech recognition results achieved using our database, together with procedures and methods employed in experiments are discussed in section 5. the paper concludes with some general remarks and observations in section 6."
"to enable a precise calculation of snr for every sentence spoken by the speaker, reference noise-only recording sessions were performed before any speaker session. for synchronization purposes, every noise pattern was preceded by an anchor signal in a form of 1s long 1 khz sine."
"as a visual counterpart to the widely-known timit speech corpus [cit] ), [cit] . it is composed of audio and video recordings of 43 speakers (19 female and 24 male), reciting timit speech material (10 sentences per person). the recordings of speech were supplemented by a silent head rotation sequence, where each speaker moved their head to the left and to the right. the rotation sequence can be used to extract the facial profile or 3d information. the corpus was recorded during 3 sessions, with average time-gap of one week between sessions. this allowed for admitting changes in speakers' voice, make-up, clothing and mood, reflecting the variables that should be considered with regards to the development of avsr or speaker verification systems. additional variables are: the camera zoom factor and acoustic noise presence, caused by the office-like environment of the recording setup."
"the authors performed numerous experiments employing the multimodal corpus presented in this paper. the main goal of the research was to examine the role of the additional visual modality in the automatic speech recognition in difficult acoustic conditions, where the recognized speech was accompanied by babble, factory or street noise. the best performance of the above outlined avsr system in noisy environment was achieved using aam-shape features of the length equal to 10 coefficients combined with 39 mfcc acoustic features. the mean wer results for clean speech and for speech distorted by babble noise, tested for all speakers represented in the corpus are presented in table 5 . results indicate that babble noise dramatically worsens the accuracy of speech recognition (wer increases from 21 % to 51 %) and by addition of visual features to feature vector the accuracy of the proposed system increases by 5 percentage points (wer decreases by 5 %)."
"conditions (3) and (4) show that an active wiretapper has different strategies. one the one hand, it can try to maximize the information leaked to him by choosing the state sequence such that (4) is maximized. another strategy is to disturb the communication of the legitimate users by choosing the state sequence such that the probability of decoding error is maximized. of course, any combination is also valid."
"if common randomness is available, the legitimate users can use this resource to coordinate their choice of encoder and decoder. this leads to the following definition."
"where: e s,s -the total energy of the speech signal for given speaker, e s,n -the total energy of the noise for given speaker. finally, it is possible to calculate the average snr indicator (snr av g ) for all considered speakers and for given acoustic conditions using formula (7):"
"our previous work on a multimodal corpus resulted in a database containing recordings of 5 speakers [cit] . the recorded modalities included: stereovision and audio, together with thermovision and depth cameras. [cit], reflecting the frequentation of speech sounds in standard southern british. the resulting corpus could be used for research concerning vowel recognition."
"proposition 1 ( [cit] ): under the assumption of a best channel to the wiretapper, for the common randomness assisted secrecy capacity c s,ran (w) of the avwc w with passive wiretapper it holds that"
"but common randomness has also an impact on the behavior and the abilities of potential wiretappers. a passive wiretapper is unaware of the actual channel conditions or at least does not exploit the knowledge about the common randomness to influence the channel conditions. on the other hand, an active wiretapper can advantageously use this knowledge to influence and control the actual state sequence of the legitimate users. this is further analyzed in the following."
"the multi modal verification for teleservices and security applications corpus (m2vts) [cit], included additional recordings of head rotations in four directions -left to right, up and down (yaw, pitch), and an intentionally degraded recording material, but when compared to david-bt, it is limited by small sample size and by the used language material, because it consists of recordings of 37 speakers uttering only numerals (from 0 to 9) recorded in five sessions."
"the file labeling was an extremely time-consuming process. the speech material was labeled at the word level. initial preparations were made using the hslab tool, supplied with htk speech recognition toolkit. however, after encountering numerous bugs and nuisances, it was decided to switch to a self-developed labeling application. additional functionalities, such as easy label modification and autosave, facilitated the labeling process. still, every hour of recording required about eleven hours of careful labeling work."
the average snr value for clean conditions in the frequency range from 300 hz up to 3500 hz was equal to 36.0 db. for noisy conditions the average snr was equal to 17.2 db. calculation results of the average speech level for clean conditions and for noisy conditions were respectively: 66.0 db and 71.7 db. it means that during the recording in noisy conditions acoustic energy emitted by the speakers was 3.7 times greater than during clean conditions. information on snr values described in this section (calculated for every audio file in the corpus) are included in a text files supplementing the corpus and are available for download from the modality corpus homepage.
"in table 6 a comparison between speech recognition accuracy of the self-developed avsr system and of the state-of-the-art, commercial asr system incorporated into the intel realsense technology is presented. both speech recognition systems were tested on the modality corpus. the experimental speech material consisted of isolated commands separated by a short pause of 400ms. the realsense engine has vast possibilities of customization -it can operate in an unconstrained mode with built-in large dictionary and language model (denoted as rs-unconstr. in table 6 ), as well as with limited word list and a self-prepared grammar. in the performed experiment, denoted as rs-constr., the word list was limited to 184 speech commands recorded in the modality corpus. furthermore, the acoustic model is switchable to serve different languages. in the experiment two languages were tested, namely: british english (en-gb) and american english (en-us)."
"the discussed existing corpora differ in language material, recording conditions and intended purpose. some are focused on face recognition (e.g. iv2) while others are more suitable for audio-visual speech recognition (e.g. wapusk20, bl, unmc-vier). the latter kind can be additionally sub-divided according to the type of utterances to be recognized. some, especially early created databases, are suited for recognition of isolated words (e.g. tulips1, m2vts), while others are focused on continuous speech recognition (e.g. xm2vts, vidtimit, bl)."
"but even if common randomness cannot be used as a secret key, it is an important resource to establish reliable communication over arbitrarily varying channels. it has been shown for ordinary avcs [cit] that for symmetrizable channels the random code capacity is positive while the deterministic code capacity is zero. thus, in these scenarios common randomness is a necessary and important resource for reliable communication."
"for an avc w it is shown that its deterministic capacity c det (w) displays a dichotomy behavior: it either equals its random capacity c ran (w) or else is zero [cit] . the main techniques to characterize this behavior are the random code reduction, elimination of randomness, and symmetrizability."
"v. active wiretappers an active wiretapper exploits its knowledge about the common randomness to maliciously influence the channel conditions of the legitimate users. accordingly, the state sequence depends now on the outcome of the random experiment."
"based on the sum of energy of noise and speech signal, the snr for every recorded word (snr w ) can be determined, according to formula (5):"
"home subpage is an introductory page containing a short summary of the offered corpus. license explains the conditions under which the usage of the corpus is allowed. additional information concerning the corpus can be found on the about subpage. the list of available files is located on the explore corpus subpage. the access to the file list is granted only after accepting the license agreement. the subpage provides users with information on every speaker contained in the corpus, including gender, age & photo linked to a list of files corresponding to speaker's recordings."
"the corpus includes recordings of 35 speakers. the gender composition is 26 male and 9 female speakers. the corpus is divided between native and non-native english speakers. the group of participants includes 14 students and staff members of the multimedia systems department of gdańsk university of technology, 5 students of the institute of english and american studies at university of gdańsk, and 16 native english speakers. nine native participants originated from the uk, 3 from ireland and 4 from the u.s., whereas speakers' ages ranged from 14 to 60 (average age: 34 years). about half of the participants were 20-30 years old."
the signal-to-noise ratio is the one of the main indicators used while assessing the effectiveness of algorithms for automatic speech recognition in noisy conditions. the snr indicator is defined as the relation of signal power to noise power as expressed in the general form by (1):
"the aim of the more recent work of the authors of this paper was to create an expanded corpus, with potential applications to audio-visual speech recognition field. the language material was tailored in order to simulate a voice control scenario, employing commands typical for mobile devices (laptops, smartphones), thus it includes 231 words (182 unique). the material consists of numbers, names of months and days and a set of verbs and nouns mostly related to controlling computer devices. in order to allow for assessing the recognition of both isolated commands and continuous speech, they were presented to speakers as a list containing a series of consecutive words, and sequences. the set of 42 sequences included every word in the language material. approximately half of them formed proper command-like sentences (e.g. go to documents select all print), while the remainder was formed into random word sequences (e.g. stop shut down sleep right march). every speaker participated in 12 recording sessions. they were divided equally between isolated words and continuous speech. half of the sessions were recorded in quiet (clean) conditions, but in order to enable studying the influence of intrusive signals on recognition scores, the remainder contained three kinds of noise (traffic, babble and factory noise) introduced acoustically through 4 loudspeakers placed in the recording room. to confirm the synchronization of modalities, every recording session included a hand-clap (visible and audible in all streams) occurring at the beginning and at the end of the session."
"triphone-based left-right hidden markov models (hmm) with 5 hidden states were used in the process of speech recognition. the model training was performed with the use of the htk toolkit. the unigram language model was employed, meaning that every word symbol is equally likely to occur. in fig. 9 the general lay-out of the speech recognition setup is presented. when both audio and visual features are used, they are concatenated into one fused vector of features (i.e. early integration) and then used for the model training and speech decoding tasks. the same hmm structure is used for audio and audio-visual speech recognition, however, for audio asr the 39 mfcc features were provided to train the hmm models, whereas in case of audio-visual asr the 39 mfcc and 10 aam-shape features were used. the 22 parameter aam-shape vector was truncated to the first 10 parameters. the aam parameters in the provided vector are sorted from highest to lowest variance. the word recognition process is based on the bayesian theory, thus it requires the calculation of the maximum a posteriori probability (map) [cit] and adapted to the problem of audio-visual speech recognition as in (19):"
"where: b s is corresponding to vector of coefficients (aam-shape parameters). for texture parameters the feature extraction process is similar, namely the lip region texture g may be approximated as the sum of mean textureḡ and the linear combination of the eigenvectors of the texture φ g revealing the highest variation (10):"
"autofocus is of great importance in high-throughput microscopy of microbial cells. it allows unattended imaging over large length and time scales. in this work, we presented a convolutional neural network (cnn) approach for inferring the focal position of microbial cells under bright-field microscopy and for using the cnn output to reliably and accurately control a microscope in real-time to maintain focus. we have developed this method to assist in collecting high-throughput, live-cell imaging of yeast growing in a microfluidic device. our method enables us to maintain focus on the cell monolayer during long-duration imaging and while rapidly scanning the stage to image the entire device, which may have height imperfections along its length."
"using the neural network to automatically maintain focus during live-cell imaging. we connected our best cnn model to the focal control of our microscope through a software interface. the cnn processes the live microscopy images at a rate of 20 hz to estimate the current z-position position of the sample. the controller then automatically adjusts the objective in real-time to maintain the estimate within ±1 μm of the z-position locked-in by the user. figure 6 shows the result of a 24 hour growth experiment. here, we purposely allowed the temperature of the environmental chamber to vary by 3 °c to induce expansion effects on the stage (see fig. s4 ). the stage position varies by 40 μm during the 24 hour period, but the sample is maintained in precisely the desired focal range."
"for the second test, we presented the human annotators and the computer with 100 individual images that were randomly drawn from the testing data set. we recorded their individual estimates of the z-position of the image. human annotators relied heavily on the reference z-stacks for this test. figure 5c shows the results of determining the z-position of an individual image without access to nearby context. this was a very difficult test for the human annotators and they generally had a large degree of error. here, the cnn performance exceeded that of all of the human annotators, being both more accurate and more precise. interestingly, the cnn was more precise than the original human annotator who was one of the test subjects. if we assume that the annotator had a similar level of imprecision when annotating the training images, it appears that these errors average out during the training phase. therefore, our trained cnn model was able to achieve accurate and consistent position inference that outperformed manual focusing."
"(1) the iqa profile is peaked at a z-position of zero so it cannot distinguish whether the image is above or below the focal plane. we assumed in our analysis that some outside metric was available to correctly make this determination with 100% accuracy and always used the correct half of the iqa profile in our error calculation. (2) the iqa peak value varies for different z-stacks so a global normalization constant cannot be used. in our analysis, we calculated the maximum iqa score for each testing z-stack before calculating the error. a real-world application of iqa would therefore require collecting a new z-stack of the imaging region whenever the field of view experienced substantial changes. however, even with these two advantages for iqa in our comparison our cnn model still outperforms iqa in predicting the z-position of an image."
"during sequence-based characterization of biological communities, the supervised taxonomic classification of sequences is an important goal. numerous sequence classification software programs accomplish this by measuring sequence similarity and modelling relationships between sequence similarity and taxonomic affiliation. such classifiers often rely upon strong, and frequently violated, assumptions concerning database curation."
"approaches to identifying and extracting marker sequence data from disparate sources exist but are typically designed for specific markers or taxa [cit], with the exception of anacapa and the metaxa2 [cit] ) . additionally, in silico pcr techniques exist for analyzing marker sequences [cit], though these pipelines are intended to improve metabarcoding primer universality as opposed to aiding database curation."
"we collected 431 z-stacks of yeast cells (see fig. 1b ) from five independent growth experiments for use in training and testing the neural network (see methods). each z-stack spanned ~30 μm of focal distance using 31 images collected ~1 μm apart in the z dimension. we manually curated each z-stack and recorded the z-slice that was considered in focus, usually near image 15. we randomly partitioned the z-stacks into training (80%) and testing (20%) data sets."
"finally, while our cnn model only infers the best single focal position for an entire image, by using deconvolution techniques 39, 40 the focal position of all individual cells within the image could also be obtained. then, all cells within the field of view could be focused on and imaged individually or complex spatially varying features could be tracked and followed in the z dimension. such a method would assist in automatic high-throughput imaging of multiple cell layers within complex biofilms or tissues."
"the prediction errors of these trained neural networks are shown in fig. 4a . the width of the error distribution drops at ~100 z-stacks. similarly, the mean squared error (fig. 4b) also drops sharply at ~100 z-stacks and then continues to slowly decrease as more z-stacks are used. therefore, while the cnn trained with the full training data set outperforms the networks trained with smaller data sets, it is possible to train a network to high accuracy with ~100 z-stacks. the collection and annotation of unique z-stacks represents a significant fraction of the total effort to build an autofocus cnn model. collecting data incrementally until the model's accuracy converges, rather than collecting a large data set up front, appears to be a reasonable strategy."
"to compare the ability of the iqa and cnn methods to infer the z-position of an image, we calculated the iqa profile for all of the z-stacks in our training set. we normalized each profile individually against its maximum and assigned the z-position of the peak to be zero. we then averaged all of these profiles to obtain a global iqa versus z-position profile. we used this profile to infer the z-position of each image in our testing data set. we calculated the error as the difference between the inferred and actual z-position, relative to the iqa estimated focal plane for that z-stack. figure s3b shows the iqa method has significantly greater error than our cnn method when inferring the z-position of an image."
"we evaluated the trained neural networks using the testing data set. we used the cnn to infer the z-position of each image and compared that with the annotated position. supplementary fig. s7 shows the probability of a particular z-position inference given a labeled z-position (bin size 12 μm). both neural networks were able to reliably infer the z-position of an image. the probability distribution at any labeled position is wider than for the yeast cnn due to the significantly increased depth of field of the 10x objective. despite making no changes to the hyperparameters, our cnn method was able to properly learn to infer the focus of a new cell type and imaging configuration."
"neural network implementation. the neural network was implemented using the tensorflow framework 41 . in tensorflow, computation, shared state, and operations that mutate that state are represented by dataflow graphs. specifically, functional operators, such as matrix multiplication and convolution, as well as mutable state and operations that update the state, are represented as nodes, while multi-dimensional arrays (tensors) that are input to or output from nodes are represented as edges. all training runs were executed on 4 gb tesla k20m gpus (nvidia)."
"the first fully connected layer maps the feature output from the second convolution block to 1024 classification neurons. it is followed by relu activation and a dropout layer to minimize overfitting 34 . the dropout probability was set to 0.5. the last fully connected layer maps the output of neurons from the previous layer to a variable number of neurons that predict the discretized z-position of the image, using one neuron for each possible z-bin. finally, a softmax layer gives the z-position probabilities. for reference, a model using 4x downsampling and 41 z-bins has 6.8 million parameters."
"moreover, focus is well-controlled regardless of large changes that occur in the field of view. over the course of the 24 hour growth experiment the number of cells in the frame increases from fifteen to many hundreds (see supplementary video s8). additionally, because we did not affix the cells to the coverslip in this experiment, cells and cell clusters move around and occasionally drift into and out of frame. focus is maintained within the set range regardless of these variations. the cnn is robust to these changes because it does not predict focus using any specific features but globally over the entire image."
"comparison of neural network and human focusing performance. to independently evaluate the performance of cnns, we compared the accuracy of our autofocus cnn with four human annotators (h1-h4). we used for the computer (cnn) the best cnn model from the previous sections (4x downsampling, batch size 50, bin size 1, trained with 345 z-stacks). the evaluation was conducted using two separate tests: (1) identify the best in-focus image given a full z-stack, and (2) determine the numerical z-position of an individual image without using the z-stack for context. for training, the human annotators were given a ~10 minute lesson on the appearance of in-focus images as assigned by the original annotator. additionally, each was given five example z-stacks with annotations showing the z-position relative to the focal plane to use as a reference during the tests."
"we also tested the effect of varying batch size on the models. for the models trained with downsampling of 10x and 4x, the mean squared error increases with decreasing batch size, which also can be seen from the larger distributions in fig. 2d . with larger batch sizes the gradient is calculated from more training data, which helps improve training performance in general. even though models trained with larger batch size give better results, the training time increases significantly with batch size, as shown in fig. 2f . it took more than 100 hours to train our models with a batch size of 50. in addition, for a specific batch size the training time also increases with reduced downsampling."
"for these issues, we developed the metacurator toolkit. the main tool, iterrazor, identifies and extracts target marker reference sequences from any available source, including whole genome assemblies for example. after extracting the references for a given marker, derepbytaxonomy is used to dereplicate reference sequences using a taxonomy-aware approach. additional python tools and a unix shell script facilitate the formatting of taxonomic lineages for hierarchical curation and the removal of taxonomic lineage artifacts commonly found within the ncbi nucleotide database [cit] ."
"for the first test, we provided both computer and human annotators with 100 z-stacks and asked them to estimate which z-plane was the most in focus. we recorded the difference between the estimated focal position and that determined by the original annotator. as shown in fig. 5a, the cnn performed well in determining the correct in-focus image given an entire z-stack. even though this was the easier of the two tests for the human annotators, since they could scan up and down in the z-stack to compare images, the cnn performance equaled the best human annotator and exceeded the other three. moreover, in some cases, the human annotators showed a bias in determining the focal plane, while the computer showed minimum bias in the test. to control for potential bias by the human annotators due to differences in interpreting what in-focus cells look like, we later tested the human annotators again on the same 100 z-stacks as the first test but in a random order. we calculated the difference in predictions between the two experiments for each human to study the estimation variance regardless of the original z-position annotation. these results are shown in fig. 5b . the overall error and the variance of the human annotators went down relative to the first experiment. however, both were still greater than for the cnn model."
"in this paper we demonstrate a deep learning approach to use convolutional neural networks as an autofocus method for bright-field and phase-contrast microscopy. a benefit of the proposed method is that the z-position of an image can be determined solely from the information in the image itself, without the necessity of using other z-plane images. thus, real-time control of focus can be realized, which allows high-throughput data acquisition from time-lapse and fast-tracking microscopy. using the proposed method we achieved accurate control of the focal position in bright-field microscopy of yeast cells, and the method was shown to be robust against noise, optical artifacts, and features other than cells."
"comparison to other software-based focus inference methods. we wanted to compare the performance of our model to other software focus-inference methods. we implemented three focus-scoring algorithms that summarize various features of an image (e.g. the density of edges) to pick out the most \"in-focus\" image [cit] . the details of the methods are given in supplementary text s1. we calculated the score for each method for all z-slices in our testing data set. figure s2 shows the distribution of scores by z-position. of the three methods tested, only image quality assessment (iqa) was able to capture a relationship between image features and z-position. we also tested the ability of each method to uniquely identify the focal plane. again, only iqa was able to reliably identify the focal plane in our z-stacks by finding the z-slice with the maximum detail, as shown in fig. s3a ."
"using mafft [cit], the iterrazor algorithm produces a seed multiple sequence alignment (msa) of approximately 5 to 10 reference sequences trimmed to the exact marker of interest and provided by the user. this msa is used as input for the hmmbuild algorithm of hmmer3 [cit], producing an initial hmm profile of the marker. after preparing the hmm profile for searching using hmmpress, the algorithm proceeds into a multi-round, multi-iteration search. heterogeneous input query sequences (e.g. whole chloroplast genomes or sanger sequences) from diverse taxa, are searched against this profile using nhmmscan. tabular hmm search output is parsed for profile matches which meet the userspecified quality standards. sequences exhibiting a sufficient match to the hmm profile are trimmed to the matching region and aligned individually to the existing msa of marker sequences using hmmalign. an updated hmm profile is then calculated with the new alignment using hmmbuild. following each iteration, sequences from which the marker was extracted are removed from the query sequences to decrease extraction time. with this approach, the corpus of references can increase with each iteration and the hmm profile becomes more representative of the marker and taxonomic group of interest as sequences from novel taxa are incorporated. to avoid the addition of false positive hmm matches, users can adjust the hmm profile match evalue threshold using the '--hmmevalue' argument, set to a default of 0.005."
"the effect of downsampling on inference accuracy. next, we began to study the effect on prediction accuracy of the various hyperparameters of the cnn model. to investigate the effect of both feature size and resolution on cnn accuracy, we evaluated three different image downsampling factors: 2x, 4x, and 10x (see fig. 2a-c) . in the full resolution image, a large yeast cell with a diameter of 6 μm would be ~100 pixels across. for 2x downsampling this corresponds to a diameter of 50/10/2 pixels in the various convolution layers, 25/5/1 for 4x downsampling, and 10/2/0.4 for 10x downsampling. we trained a new cnn model using data downsampled with each of these three values."
"the user can define the number of extraction rounds, iterations per round and the percent profile match coverage of the hmm profile required for each round. by default, iterrazor conducts four rounds of extraction, requiring profile matches to cover 100, 95, 90 and 65 percent of the hmm profile per round, with 20, 10, 5 and 5 search iterations per round. if a search iteration fails to yield new extracted sequences, iterrazor will break out of the iteration loop and proceed to the subsequent search round. after the final iteration of the final round, an output file containing all extracted sequences is written and the number of sequences extracted in each iteration is printed to standard output. additionally, a temporary directory containing the intermediary files produced during extraction is deleted unless '--savetemp true' is declared. default iterrazor settings work well for high-conservation markers. low-conservation, lengthvariable markers sometimes require more stringent hmm search e-value and percent profile match coverage thresholds."
"in comparison with other autofocus approaches, our method does not require physical calibration nor the acquisition of z-stacks during imaging, and is robust to noise, optical artifacts, and features other than cells (e.g. structures in the microfluidic device). even though the presented method provides some advantages over other forms of autofocus control, it is system specific in terms of microscope objective, imaging mode, and cell type. it is unlikely that a trained cnn model would work well if any of these variables were changed. nevertheless, given that a relatively small data set of 100 z-stacks is needed and an adequate cnn can be trained in a few days on a gpu, construction of system specific autofocus models is quite practical. additionally, it should be possible, given a large and varied data set and a deeper neural network, to train a single model to recognize focal position of many different systems. likewise, we have only tested our method on microbial systems with relatively homogeneous cells. a deeper or more sophisticated neural network may be required to infer the focal position of more complex cell types, such as neurons. our method therefore provides a novel conceptual and practical framework for automated focus control in high-throughput imaging of biological samples, and we anticipate that this framework is generally applicable in the field of bright-field and phase-contrast microscopy where unattended and real-time control of focal position is desired."
cell growth and microscopy. the yeast gfp clone collection ste20/yhl007c fusion strain (courtesy of dr. john kim) was used in this study. yeast cells were initially cultured overnight in low-fluorescence synthetic defined media without histidine (sd-h) at 30 °c. cells were then loaded into a custom soft-lithography microfluidic device with a ~6 μm tall growth chamber containing 10 μm diameter pillars on a 50 μm grid to prevent chamber collapse. the device was fabricated according to standard protocols. yeast cells were continuously supplied with fresh sd-h media through the entire imaging process.
"in addition to implementing the metacurator tools separately, users can execute the entire pipeline using the metacurator command. this parental code runs taxonomy reformatting, cleaning and mid-point correction; iterrazor marker sequence extraction; and derepbytaxonomy dereplication while allowing the same options flexibility as implementing each sub-component individually."
"biologists now routinely use live-cell imaging to monitor the dynamics of the cell's state, to track real-time biochemical processes in vivo, and to read out cellular phenotypes from time-lapse microscopy images [cit] . from these experiments, it has become apparent that cell populations inevitably display significant cell-to-cell variability in cellular state, as quantified by protein and rna expression levels [cit] . of particular interest, populations are often observed to contain cells exhibiting rare phenotypes, i.e., small subpopulations with a distinct state [cit] . studying the dynamics of these small populations is indispensable for understanding the probabilistic principles behind how cells make transitions to these rare, but stable, phenotypic states [cit] . the challenge in live-cell imaging is to record large populations such that rare phenotypes are sufficiently sampled for statistical analysis, perhaps requiring 10 5 -10 6 total cells per experiment. with advanced optical instruments and computer-controlled stages, time-lapse microscopy is a viable method for capturing high-throughput data regarding cellular phenotypes 15, 16 . additionally, data-intensive computing methodologies are now available that can analyze the large volumes of imaging data that are generated 17 . one challenge to bringing high-throughput microscopy into every biology lab is its performance on less sophisticated equipment. keeping the cells of interest in focus during time-lapse experiments and/or continuous movement is difficult 18 . due to thermal drift, uneven slide surfaces, diverse cell sizes, and cell motion in 3d space, there is not a single focal plane that will accommodate the entire experiment 19, 20 . in order to maintain image sharpness throughout the experiment, focus need to be corrected before each image is acquired. manual control of focal position is not practical for experiments running for hours or when specimens are in continuous motion. instead, automated focusing is needed to establish the best focus in the absence of human intervention 21 . autofocus methods can be broadly classified into two categories: active and passive autofocus. active autofocus uses knowledge of the physical characteristics of the system to obtain defocus information, and then correct the defocus accordingly 22 . typically, electromagnetic waves, such as a laser or infrared light, are applied to maintain the distance between object of interest and lens 20 . this approach is able to provide real-time correction, however, what it actually measures is the distance between the reflective surface and lens, rather than the specimen itself 19 . therefore, any variations in thickness of the sample or coverslip can introduce error in determining . moreover, active autofocus methods require calibration before focal length can be determined, which makes it impractical for many image acquisition processes 23 . in contrast, passive autofocus is based on image analysis, and thus requires little knowledge of the specific imaging system 22, 24 . in passive autofocus, a predefined focal reference is first determined, typically by taking a series of images at multiple z-positions on both sides of best focus of the sample. the images are then processed and high frequency contents or edge information is extracted from the z-stacks to establish the focal position 20 . since passive autofocus is based on the information of the sample being imaged, it is generally more reliable than active autofocus 21 . nevertheless, passive autofocus suffers from limitations that are intrinsically hard to overcome. low image intensity or low contrast may prevent it from being accurately analyzed 20 . in addition, because multiple images at different z-positions need to be acquired and processed in order to determine the relative focus, it takes longer to refocus the sample. therefore, passive autofocus is not ideal for tracking changing objects or for high-throughput imaging acquisitions."
"as a long-standing topic over the years, autofocus in high-throughput microscopy has been approached by a number of methods, most of which provide satisfying solutions under well-defined circumstances 25 . still, these methods are designed for a single type of imaging mode, and thus not applicable to other imaging systems. in recent years, deep learning has become a promising approach for inference across various fields, such as speech recognition, visual object recognition, object detection, drug discovery, and genomics 26 . it allows computational models with multiple processing layers to learn features of data by extracting information from multiple levels 26 . specifically, deep convolutional neural networks (cnns) are especially helpful in visual object recognition and object detection, for instance, annotation of cellular cryo-electron tomograms, classification of cancer tissues using hyperspectral imaging, cell segmentation in fluorescence microscopy images, and autofocusing in digital holographic microscopy [cit] . cnns are essentially neural networks that employ the convolution operation as one of its layers. typically, a cnn consists of a feature extraction layer that extracts features from input data and a classification layer that classifies the feature map, the weights of which are determined through a training process 26 . the feature extraction layer consists of pairs of convolution and pooling layers in tandem, where the convolution layer performs convolution operation on input data, and the pooling layer reduces the dimension of the input data. the output of cnns is generated from the classification layer, which in most cases employs an ordinary multi-class classification neural network."
"inference accuracy depends upon the z-bin size. a key advance in our study is the classification of images into z-position bins. thus, the number and spacing of the bins may have a large effect on the accuracy of our method. to investigate whether and how bin size affects the z-position prediction of our cnn, we tested various bin sizes ranging from 1 μm to 5 μm. all of the bins spanned the range −20 μm to +20 μm, covering the entire focal distance of our z-stacks. thus, the output of the final classification layer ranged from 9-41 categories. we assumed that the inferred z-position was the center of the predicted z-bin. figure 3a shows the probability of a particular focal position inference given a label, for the highest resolution model with a bin size of 1 μm (41 categories). near the center of the focal plane, the correct bin has the highest probability in all cases with a lower amount of probability appearing in the ±1 bins. near the edges of the z-stack the probability becomes more spread out as these images become more difficult to distinguish. the model has the highest accuracy when the cells are near the focal plane. figure 3b shows the probability map for the lowest resolution model with a bin size of 5 μm (9 categories). here, the model is able to accurately infer the single correct bin, except when the actual z-position approaches the boundary between two bins. but the localization error is overall greater because the width of the bins is greater. figure 3c,d show the inference error by bin size. the width of the error distribution and the mean squared error both increase monotonically with bin size. if the z-bin resolution dropped below the information content of the images we would expect there to be a minimum in the mean squared error. however, for our z-stack resolution even a one-to-one mapping between z-slices and bins produces good accuracy. it appears that there are sufficient differences between z-slices separated by a single step to accurately differentiate them. with higher resolution z-stacks further improvement in the positional inference could likely be achieved by the model."
"using metacurator, we constructed databases for five metabarcoding markers (table 1), including the low-conservation plant trnh and its2 markers. the primer sets used to designate these barcode regions are detailed in supplemental table s1 . relative to previous studies, between 14.3 to 50.7 percent more genera were retained during data curation. further, while 1.8 to 34.1 percent of genera were lost during database dereplication in these previous studies, no taxa were lost during metacurator dereplication."
"the use of poorly curated reference databases and taxonomically naïve sequence dereplication methods are important methodological shortcomings for metabarcoding. during composition-based classification, extraneous sequence regions adjacent to the marker of interest bias the k-mer distributions used by many tools, such as sintax, the rdp naïve bayesian classifier and vsearch [cit] . for classifiers which estimate taxonomic boundaries by analyzing the distance distributions of the top n aligned sequences [cit], retention of sequence duplicates can bias characterization of the distance distribution between query sequences and top hit reference taxon matches, especially as the number of top hit taxon replicates approaches n. however, taxonomically naïve removal of sequence duplicates without ensuring that the two sequences are from the same taxon represents the removal of important information from the database regarding the lack of genetic variation between two taxa."
"provided with a fasta file of reference sequences and a text file containing associated taxonomic lineages, derepbytaxonomy utilizes vsearch-based alignment [cit] to find exact match semi-global alignments, test if the associated lineages belong to the same taxon and, if so, remove the shorter of the two duplicates. to accomplish this, derepbytaxonomy iteratively searches the fasta sequence corpus against itself requiring 100 percent identity matches and 100 percent query coverage while calculating alignments using an infinite internal gap penalty and a null external gap penalty. by default, derepbytaxonomy performs 10 search rounds during dereplication. since vsearch uses k-mer composition to prioritize a limited number of alignment tests (set by '--maxaccepts' and '--maxrejects'), multiple dereplication rounds ensure sequences will be exhaustively searched for duplicates. if no duplicates are found for a search round, the tool breaks from the loop and subsequent rounds are not performed."
"the effect of training data set size on cnn performance. in the previous sections, we used our complete training data set consisting of 345 z-stacks to train the cnn. however, it would be useful to know the minimum number of z-stacks needed to achieve good focal position inference. thus, we trained additional cnns using only a subset of the training data in order to investigate how the size of training data set affects the accuracy of trained cnns. we randomly selected 50, 100, 150, 200, 250 and 300 z-stacks from the complete training data set to train the new models."
"the algorithm iteratively does these five steps until the stopping criterion is met. as for all other meta-heuristics, the stopping criterion can either be static, i.e. a fixed number of iterations or dynamic, i.e. a fixed number of non-improving iterations. the implementation of these five steps is described below."
"we set these parameters according to many experiments. table 1 shows the best, worst and average values over the ten runs for each problem. also it shows the number of vehicles needed for best values and standard deviation from best-known solutions. table 2 shows the comparison of our algorithm with published results. the first column describes the various instances and their related size, whereas the second specifies well-known published best results obtained using simulated annealing and tabu search in five references. the following column refer to the best result of our method for these instances. new best results found with our approach are depicted in bold. the results show that our approach could find the new solutions for five instances. although minimization of the vehicles is not the aim of our algorithm, we presented the number of vehicles which was needed for finding the best result of previous algorithms in column v1 and for the best result of our algorithm in column v2. table 3 shows the results obtained for the second problem instances and presents the comparison of best results of previous methods and our algorithm. we depicted the best result in bold. it also includes the worst and average results of the new proposed algorithm. the results show that our approach could find the new solutions for six instances. the results show our algorithm can find better solutions for most of the problem instances in comparison with the other metaheuristics proposed previously to solve the vrp."
"in the constructive phase of the aco algorithm decisions are based on both heuristic information and the pheromone values. we add a new parameter to this decision in our approach which is called weight. at the end of each iteration, i.e. once all ants have gone through solution construction and local search, the pheromone update procedure is applied to these pheromone values. fig. 1 . title of the figure, left justified"
x y z u iso */u eq s1 0.64031 (5) (9) 0.0342 (7) 0.0033 (7) −0.0008 (6) 0.0068 (6) c1 0.0432 (10) 0.0354 (10) 0.0275 (8) −0.0049 (8) 0.0057 (7) −0.0020 (7) c2 0.0434 (10) 0.0292 (9) 0.0266 (8) −0.0018 (7) 0.0039 (7) 0.0020 (7) c3 0.0514 (12) 0.0369 (11) 0.0414 (10) 0.0027 (9) 0.0038 (9) 0.0085 (8) 0.0374 (10) 0.0438 (11) 0.0315 (9) −0.0038 (8) 0.0050 (7) −0.0025 (7) c11 0.0355 (9) 0.0319 (9) 0.0316 (8) 0.0036 (7) 0.0017 (7) 0.0007 (7) c12 0.0337 (8) 0.0270 (8) 0.0270 (7) 0.0004 (7) 0.0014 (6) 0.0022 (6) c13 0.0502 (12) 0.0512 (12) 0.0292 (9) −0.0080 (9) −0.0011 (8) 0.0006 (8) c14 0.0569 (14) 0.0693 (16) 0.0444 (11) −0.0056 (11) −0.0116 (10) 0.0096 (10) c15 0.0405 (10) 0.0392 (11) 0.0344 (9) −0.0067 (8) 0.0020 (7) 0.0036 (7) c16 0.0358 (9) 0.0340 (10) 0.0341 (9) 0.0019 (7) 0.0031 (7) 0.0052 (7) c17 0.0341 (9) 0.0301 (9) 0.0262 (7) 0.0000 (7) 0.0039 (6) 0.0039 (6) c18 0.0403 (10) 0.0376 (10) 0.0319 (8) 0.0018 (8) 0.0028 (7) 0.0079 (7)
"step 1. decomposing the problem with minimum spanning tree at this step we build the mst of the problem graph. an mst can be computed in polynomial time by some wellknown algorithms, e.g. the algorithms of kruskal [cit] or prim [cit] . clearly, we want to find a decomposition that leads to sub-problems with geographically close tours in order to be able to then solve these regional sub-problems efficiently. this should help us to improve both the routing on these tours but also the clustering. the problem of finding an mst is quite easy and several efficient exact algorithms have been proposed for its resolution. in this paper we will use the well known algorithm of prim. starting from an empty tree, i.e. no selected arcs, it repeatedly includes the shortest arc connecting a node that is already part of the growing tree with a node that is not yet part of this tree, if it doesn't construct a tour for connected nodes. the algorithm ends once n−1 arcs have been included. the time complexity of an efficient implementation of this algorithm is o(n2), where n is the number of nodes. we modify this method by assigning a weight wij to each arc (i,j) in the problem graph and using this weights instead of length of arcs for constructing the mst. in other words, in each step, the method selects the arc with maximum weight instead of minimum distance that doesn't construct a tour. the initial weight values are the reverse of distances between nodes, that means."
"a computational experiment has been conducted to compare the performance of the proposed algorithm with some of the best techniques designed for vrp. we executed the algorithm on some of the well-known problem instances from one dataset. then, additional simulations were conducted over another dataset and results reported accordingly. the algorithm were coded in c and, using a 2.0 ghz cpu. it was first applied to the 7 vehicle routing problems proposed by christofides and can be downloaded from the or-library (http://mistic.heigvd.ch/taillard/problemes.dir/vrp.dir/taillard.dir), and which have been widely used as benchmarks. the first 5 problems (i.e. c1 to c5) have customers that are randomly distributed with the depot in an approximately central location. in the last two problems (i.e. c11 and c12), the customers are grouped into clusters. then we applied the algorithm to the twelve instances from the second dataset which proposed by taillard and can be downloaded from this url: http://people.brunel.ac.uk/~mastjjb/jeb/orlib/files/. the results for those two datasets are reported for ten independent runs, and in each run the algorithm was executed for 200 iterations. also we used 10 ants in each iteration for building the solutions. the common parameters for these instances include 0.8"
"the proposed algorithm exploits two important problem characteristics by hybridizing aco with an exact algorithm for clustering the problem. more precisely, a solution of the problem asks for two decisions, namely the clustering of nodes to find heuristically the nearest nodes and the 'design' for each cluster."
"due to its constructive nature of aco and because at each construction step an ant chooses exactly one of possibly several ways of extending the current partial solution, aco can be regarded as a tree search method [cit] . this view of aco as a tree search procedure allows us to put aco into relation with classical minimum spanning tree. our strategy for solving vrp is based on the fact that the vrp is a generalization of the tsp. thus, besides the routing aspect already existing in the tsp one has to find an assignment (or clustering) of customers to vehicles. once this assignment is done, the problem reduces to independently solving a vrp. each ant works only on its current cluster which is a branch of the minimum spanning tree. the intuition is that nodes that are near to each other will probably belong to the same branch of the minimum spanning tree of the graph and thus will probably belong to the same route in vrp. the idea of this paper is to hybridize the solution construction mechanism of aco with a modified version of mst, which results in a new approach to solve vrp. in this approach the extension of partial solutions is usually done by using a probabilistic greedy policy with respect to a weighting function that gives weights to the arcs of the problem graph and update the weights to reinforce the edges contained in the best."
"where, and correspond to the pheromone values, heuristic information and weight on edge (i,j) respectively. in the above equations, is the feasible neighborhood, i.e. the set of nodes which ant k can choose to move to them from node i. the neighborhood consists of all the nodes in the current branch of the mst which hasn't already been visited. if the selected node has a capacity less than or equal to the remaining capacity of the current vehicle, the ant moves to this node, adds it to the current route and subtracts the request of this node from the capacity of the current vehicle, otherwise moving to this node violates the capacity constraint of the vehicle, so in this situation, the ant returns to the depot (it starts a new vehicle with full capacity), and then moves to the selected node. when all the nodes of the current branch, were selected before, the ant considers the nearest neighbor branch as the current branch. in other words, the nearest branch to node i is assigned to the current branch and neighborhood is updated. all pheromone parameters have values in the interval. we set all initial pheromone values to."
"where is the trail persistence and ne is the number of elitists. using this scheme two kinds of trails are laid. first, the best solution found during the process is updated as if ne ants had traversed it. the amount of pheromone laid by the elitists is, where l* is the objective value of the best solution found so far. second, the ne − 1 best ants of the iteration are allowed to lay pheromone on the arcs they traversed. the quantity laid by these ants depends on their rank r as well as their solution quality lr, such that the rth best ant lays. arcs belonging to neither of those solutions just lose pheromone at the rate, which constitutes the trail evaporation."
"the molecular structure is shown in fig. 1 . the bond lengths and angles are within normal ranges and comparable to related structures [cit],b; [cit],b ). an intramolecular o1-h1o1···n1 hydrogen bond (table 1) in the crystal packing (fig. 2), the molecules are linked into three dimensional network via intermolecular c14-h14a···o2 i, c22-h22a···f1 ii and c25-h25a···s1 iii (table 1) hydrogen bonds."
"a mixture of 3,5-bis[(e)-(2-fluorophenyl) methylidene]-tetrahydro-4(1h)-pyridinone (1 mmol), acenaphthenequinone (1 mmol), and thiazolidine-2-carboxylic acid (1 mmol) were dissolved in methanol (5 ml) and refluxed for 1 h. after completion of the reaction as evident from tlc, the mixture was poured into water (50 ml). the precipitated solid was filtered and washed with water (200 ml) and recrystallized from ethyl acetate to give the title compound as colourless crystals. supplementary materials sup-2 . e68, [cit]"
"at the end of each iteration, weight values of the arcs which belong to the best solution are being updated to increase the probability of selecting these arcs in future solutions. the weight update is in the following way"
"step 4. update of the pheromone information after all ants have constructed their solutions and all ants' solutions have been taken to a local optimum, the pheromone trails are updated on the basis of the local optimal solutions. according to the rank based scheme the pheromone update is as follows [cit] :"
"according to recent researches, hybrid algorithms are very promising so we used this idea in our approach. in this paper we presented a new algorithm which hybridized the aco with an exact algorithm means mst to improve both the performance of the algorithm and the quality of solutions for the vrp. the intuition is that nodes which are near to each other will probably belong to the same branch of the minimum spanning tree of the graph and thus will probably belong to the same route in vrp. in the proposed algorithm, in each aco iteration, we first apply a modified implementation of prim's algorithm to the graph of the problem to obtain a feasible minimum spanning tree (mst). given a clustering of client nodes, the solution is to find a route in these clusters by using aco with a modified version of transition rule of the ants. at the end of each iteration, aco tries to improve the quality of solutions by using a local search algorithm, and update the associated weights of the graph arcs. future work will be conducted to improve the proposed algorithm. alternative formulas will be examined to enhance the method of weight update and pseudorandom decision of ants for selecting the next node in solution construction. additional improvements might lie on the combination of our approach with the other metaheuristics like genetic algorithm."
"after filtering out unlikely words, we dummified the learning space by creating a boolean variable for each word. stemming [cit] was considered but it doesn't improved the performances of the proposed approach. thus, given a phrase, we set to one all boolean variables corresponding to the words composing the text, and to zero otherwise. after processing the entire training set, we obtain a description of the text of each offer as a function of dummy variables. on average we extracted five words per message and used all of these in the first learning phase. moreover, the tf-idf normalization schema have been considered as text representation. specifically, tf-idf was employed for locally weighted regression (lwr) method in which an opportune metrics can be defined (see sect. 4 for more details)."
"those methods have been applied for palmprint identification system. the system is tested using database of 1000 palmprint images, are generated from 5 samples from each of the 200 persons randomly selected. the first three images from each user were used for training and the rest were used for testing, so the total number of testing and training image are 400 and 600 respectively. the performance of the identification system is obtained by matching each of testing palmprint images with all of the training palmprint images in the database. a matching is noted as a correct matching if the two palmprint images are from the same palm and as incorrect if otherwise. this paper used far (false acceptance rate), frr (false rejection rate), system accuracy, genuine and imposter score distribution graphics, and receiver operation curve (roc) as indicators of the performance system."
"finally, we define base size as the number of offers that have never been sent to customers and are present at the beginning of the first learning phase (in a sense, the base size is the overcapacity measured just before the first learning phase takes place). if the overcapacity rate is greater than zero, the number of offers that have never been subject to learning increases over time (i.e., the overcapacity grows monotonically). this leads also to a growth of the base size just before a first learning phase is initiated."
"from the analysis of table 3 we conclude that the regression cascade improves the predictive ability of our system, estimating more accurately the ctr of each offer. the results seem also to suggest that this method can better capture the properties of each kind of representation (visual, text), and take into account of the peculiarity of each regression method."
"from table 2 we can see that combining visual and text features provides better predictive accuracy, suggesting that both sets of variables indeed capture different aspects of the offers likeability (the svr model using the visual and text features together outperforms the results obtained by when using each kind of feature separately). hence, the textons based features seem to be able to capture the visual content whereas, text features add semantic information related the category of the offer. despite these results, we note that svr did not provide the best results for the text features (fig. 8) . the best method to exploit text features is the lwr model. in addition, as discussed in the previous section, our findings seem to suggest that we should use text features before the visual component in order to capture the category of each mms offer (e.g., music, sport, wallpaper, etc.). then, we could add the visual features to discriminate between different visual content and improve predictive ability. this analysis induced us to use a greedy combination of features and regression methods. the final regression approach selected involved the lwr and the svr models in cascade to take first advantage of the text features and then the visual features."
"among all words extracted from the collection of messages we removed the least significant words and the stop words. the significance of a word is function of its entropy computed over the distribution of that word across all messages considered. that is, words that appear on a large percentage of messages tend to be less discriminant (thus, less significant) than words appearing on fewer messages. in addition, we removed also all words appearing in a very limited number of messages (less than five). this is justified by the low probability of finding the same word in the learning set, thus, it becomes very unlikely that we can use that word in the generalization phase."
"taking into account the results of the previous sections, we have combined the different static features using the considered regression methods. we first combined the visual and text features using svr model. this regression method obtained good performances on the two types of static features (see fig. 8 ). we report the rmse(w) results for the svr model in table 2 ."
"we first estimate which of the offers available for learning are likely to be good (in terms of estimated ctr) by taking advantage of their static (non-behavioral) visual and text features, and of the past ctr of similar (in terms of visual and text content) offers. as depicted in fig. 3, at the end of the first phase all new items will be labeled with the estimated ctr."
"as well known, humans can process texture quickly and in parallel over the visual field [cit] : this makes texture a good candidate as holistic cue. the main process [cit] we used in this paper to encode textures cues starts by building a vocabulary of distinctive patterns, usually called visual words, able to identify properties and structures of different textures present in the offer's images. using the built vocabulary each offer's image will be represented as an histogram of visual words."
"svr methods learn a nonlinear function in a kernelinduced features space. the parameters tuning and the choice of the best kernel for the features space, remain the main disadvantage of this approach. in our experiments we have used the epsilon-svr algorithm [cit] within the libsvm library [cit], and a radial basis function as kernel."
"segmentation of palmprint roi is one of important factor for recognition performance. this paper proposed new technique to extract the roi is called two steps in moment central method. the steps of the method can be explained as follow: a. the gray level hand image is threshold to obtain the binary hand image. the threshold value is computed automatically using the otsu method. to avoid the white pixels (not pixel object) outside of the hand object is used median filter. b. each of the acquired hand images needs to be aligned in a preferred direction so as to capture the same features for matching. the moment orientation method is applied to the binary image to estimate the orientation of the hand. in the method, the angle of rotation ( θ ) is the difference between normal axis and major axis of ellipse that can be computed as follows."
"where ctr m represents the real ctr of the mms m [ x, and price m is the corresponding price. we can also compute the overall system performances when we select randomly the offers to send to the population for testing (performance rand ) as in the traditional learning case. note that performance rand represents the performance lower bound."
"customers' evaluation of the offer greatly depends upon their interpretation of the visual content. thus, we use visual features to model how appeal an image offer can be for a user."
"we tested different regression models on our dataset: regression tree (rt) [cit], lwr [cit] and support vector regression (svr) [cit] . we selected these regression methods based upon some properties of the involved features:"
in this work we propose to use a holistic representation based on textons that is more suitable to capture properties and structures of different textures inside the visual content of each mms.
"the broadening of the direct marketing process and the creation of new channels, like the one in our empirical application, has brought new learning and knowledge acquisition challenges to the industry. first, the number of new offers in need to be tested in most of the new applications grows faster than the available opportunities for learning. second, many of the offers have very short lifespans, posing additional pressure to produce fast learning. under these circumstances the learning space may not be enough to learn on all available items."
we propose a two-phase methodology to improve learning on new offers under time and learning space constraints. in fig. 3 the overall schema of our approach is reported.
"lwr is a memory-based method that performs a regression around a point of interest using only training data that are neighbors to that point. the basic assumption of lwr is that the neighborhood of each point in the dataset share similar values for the dependent variable. an important parameter in lwr is the metric to be used to compare similarity of features points. we note that, in using lwr, we have chosen a metric based on bhattacharyya coefficient [cit] to compare textons distribution whereas the l2 metric for colour based representation. in addition, we employed the cosine similarity (dot product) and the term frequency inverse document frequency (tfidf) methodology [cit] on text features. the lwr algorithm was implemented by using [cit] as reference."
"a holistic representation to capture the visual content of the scene as whole entity is a good deliverable in our application domain. more specifically, we have to choose a holistic representation able to capture the structure present in the mms image."
"as shown in fig. 8, we also find that text features perform better than visual features across all regression methods. one possible reason for this result is that the text might be working as a proxy for the offer's category (i.e., telling us whether the offers are connected to music, sports, phone wallpaper, etc.). in this specific domain application, we do not know a priori the offer's category (we do not have the classification of the offers into categories). hence, we associate the power of text variables in predicting ctr for each mms to the ability of the mms short text in providing a category semantic. in contrast, the visual representation, though still showing predictive power, does not perform as well as the text. we conclude then that the visual component does not provide as clear clues as the text when predicting an offer's category or its likeability. indeed, many of the offers from different categories are very similar in terms of visual content (fig. 9 )."
"as previously described, we obtain the static features for each mms through automatic computer vision and text mining techniques (see sect. 3). based on these features we create feature vectors for each mms:"
"to build the visual vocabulary each image in the training set is processed with a bank of filters. all pixel responses are then clustered pointing out a centroid for each cluster. these centroids, called textons, represent the visual vocabulary. each image pixels is then associated to the closest textons taking into account its filter bank responses. hence, each image becomes a ''bag'' of textons (or visual words). a normalized histogram on textons is used as descriptor for the holistic representation."
"the remainder of the paper is organized as follows: sect. 2 introduces the dataset we have used to test our approach, the simulated mms direct marketing system, and the main structure of our two-phase learning strategy. section 3 describes the process we have employed to extract visual and text features from the mms offers. in sect. 4 we provide details on the regression tools, and on the cascade learning strategy that uses the extracted features. section 5 reports the experiments and discusses the results in the mms application domain. we conclude in sect. 6 with avenues for further research."
"future work would require the evaluation of online techniques for the learning phase strategy (e.g. online boosting [cit] ), and could take into account also other active learning strategies based on uncertain sampling. the discriminant power of other visual features should be evaluated. in addition, customers' behavior and the offer's price could be exploited as features and be used jointly with visual and text features to predict performance. finally, the combination of different types of features able to capture text and visual contents should be tested also in other direct marketing domains."
"hence, the rapid growth in the number of offers to be tested, the limited number of learning occasions, and the reduced time available for learning, make the traditional pre-testing not suitable in this context. this is because of significant time and learning space constraints: there are not enough opportunities for learning or, by the time enough learning could be gathered, the offers have already expired."
"to test the proposed approach, we use a real dataset of commercial multimedia messages sent to mobile users in europe over a period of 15 months. there were more than one million users who opted-in for the service. there were also more than 70,000 possible direct marketing offers to advertise, but only a subset of 8,600 items were labeled with a ctr due to the limit of our real system in sending more than twenty items per day. each commercial offer is composed by a small picture, a short description, and a price. the database we use contains the rgb thumbnail image associated with each offer. all images are encoded by jpeg standard with a high quality setting (i.e., no blocking is evident). the typical resolution size is 200 9 200 (see fig. 1 ) or 200 9 116 (see fig. 2 ). our database includes also the short text description associated with each offer. this text message contains on average twelve words and briefly describes the commercial offer. let's note that differently than [cit] the text does not describe the image content. in particular the overall dataset contains several kinds of pictorial contents (e.g. face, people, building, cartoons, etc.) that is not directly related to the category and text of the offers. for each offer we also know the exact price charged to users who click on the offer and the ctr of the offer across all mobile phone users."
"palmcode matching aimed to obtain similarity degree between two palmcode. the similarity degree is noted as score. this paper used normalized hamming distance. the distance of two palmcode p and q can be computed as follows: d is between 0 and 1. the score will be closed to 0 if two palmcodes come from the similar palmprint, otherwise, the score will be far from 0. one of palmcode can be translated in both vertical and horizontal direction with one or more pixels to minimize the impact of imperfectness at image preprocessing or acquisition phase. the minimum score from the translated palmcode matching process is selected as score of matching. threshold value (t) is used to decide the tested palmcode is genuine or impostor. if the score less than or equal with t then the tested palmcode is noted as authorized, otherwise the palmcode is noted as not authorized."
"the best offers are the only ones sent to the wider population of potential customers, reducing waste (by avoiding sending irrelevant and potentially annoying messages to too many customers) and allowing higher performance and profitability. this specific phase may be accomplished by using response models approaches [cit] ."
"from the mobile operator point of view this is in general very cost-effective: operators can easily reach millions of potential buyers at little cost, making the profit potential of these advertising-related services very high. in addition, given the mobile phone market saturation and fierce competition [cit], value added services (vas) and direct marketing offers are currently a significant revenue source for many mobile service providers. because these services are now central to profitability, mobile phone operators are becoming increasingly creative in generating and proposing new services and offers to their customers. the result is a rapidly growing set of possible services available."
"the current section introduces the dataset and the basic concept used to simulate a real mms direct marketing system. moreover, the structure of the two-phase learning approach will be presented."
"one significant advantage of mobile operators for this learning task is that the current infrastructure keeps detailed logs of all messages delivered and the response of each user. we can then track all messages and offers sent to customers (including their visual and text characteristics), and of the corresponding performance (e.g., whether the customer opened a message, viewed a page, bought a video, or clicked on a link). the information contained in these logs can then be used by an automated targeting system to aid message selection and customer targeting."
"studies in scene perception and visual search [cit] emphasize the fact that humans are able to recognize complex visual scenes at a single glance, despite the number of objects with different colors, shadows and textures that may be contained in the scenes. moreover, recent works of computer vision research community [10, [cit] have efficiently exploited holistic cues 1 of an image to solve the problem of rapid and automatic scene classification bypassing the recognition of the objects inside the scene."
"the rmse(w) results useful for comparison are reported in fig. 7 . as depicted in fig. 7, f textons features outperform f colour features by using lwr or svr. as expected the best results are obtained by using f textons features and svr. in fact svr is able to properly work on the globally visual representation f textons . the f colour features heve reported better results by using rt because this regression method discriminate by looking at one single component feature at each level of the three. so rt is not able to treat properly the textons based visual representation because it encode the mms visual content as a whole entity by a global distribution. anyway, svr outperform rt independently by the involved features. these evidence allow us to assert that f textons features and svr are better suited for our application."
this work is driven by the need of improving and accelerating the learning process in direct marketing applications under severe time and space constraints. we propose to optimize learning by taking advantage of the different features extracted from an offer's multimedia content (which include visual features).
"in sum, one important result from our experiments is that text mining on an offers' short text might provide a good proxy for an offer's category, and as result, helps predict its ctr. this result suggests that combining the two sets of static features, visual and text-based, we are likely to obtain prediction improvements (as both sets of variables seem to capture different aspects of the message). this result further suggests that a greedy-like combination of regression methods could potentially provide additional improvements in ctr prediction. what we propose is to use a combination of regression methods known as a regression cascade. in this regression cascade, we first use text to infer the offers' category (using the best model for text features given by our previous analysis), and then we combine this result with the information that can be provided by visual features. we use visual features to find the most similar offers in terms of fig. 9 two images extracted from our dataset. the images are similar in terms of visual content, but they come from two different category fig. 10 the lift results confirm that our approach outperform the traditional learning also using just visual features alone (e.g. textons based)"
"as shown in fig. 10 our approach outperform the traditional learning also when visual features are used alone. the performances are better than traditional approach already at low arrival rate values and increase in a more constrained time and space domain. the improvement is more than 16% considering an arrival rate equal to 30 offers per day 2 . this means that in order to achieve better results, information on visual content can be effectively used in direct multimedia marketing application."
"from fig. 11 we can see that the direct marketing system outperforms the traditional learning approach in more than 35% when the arrival rate is 30 items per day (a value comparable to the one in the real system) that is comparable to the mean arrival rate value observed in our real system. in addition, we can clearly see that the performances of the system increases as the system constraints become more severe (more than 30 items arrival per day) reaching 45% of improvement when compared to the traditional testing system. it is interesting to note also that the proposed approach outperform the traditional learning approach also when using only visual features. in such case, the proposed a cascade of regression methods achieve best results approach outperforms the traditional testing in more than 16% when the arrival rate is 30 offers per day."
"we apply the proposed two-phase approach to select the best offers to be sent for further testing (second learning fig. 5 the cascade of regression method involved in the first phase of our learning approach phase, based on traditional learning), and determine the performance in the simulated system."
"one of the challenges of direct marketing applications, like the one in our empirical domain, relates to the need of acquiring knowledge regarding the performance of all possible offers. such knowledge is to be used in the development of better targeting and segmentation policies, which have always been at the core of the success of direct marketing activities [cit] ."
"then, in the second phase, we constrain learning to the offers with the highest performance potential until we reach the system's learning capacity. the performance potential of each offer is defined as the product between the estimated ctr and the price. this measure represents the expected revenue for each new delivered item. learning in this phase takes place as in traditional direct market applications using the pre-testing approach discussed previously, that is, new offers are sent to the learning panel and the behavioral feedback (i.e., response) is registered and used for final offer selection. in other words, we apply the traditional learning only to a subset of the newly arrived items. such subset is chosen taking into account the performance potential of each item computed by using its static features. thus, we limit the traditional learning to the most promising items for which we have enough time to learn on."
"the importance of today's direct marketing industry is reflected by its significant economic value [cit] . in recent years, as a result of technological advances in computing and communications, new channels for customer contact have become available, leading to the broadening of the direct marketing process. beyond the traditional channels, which included traditional mail, catalog, and telephone contact, companies can now use multimedia channels as varied as email, mobile phone messaging, customized websites, addressable broadcasting, and direct-response tv and radio. it is this increase in the number of channels that has allowed, at least in part, the steady growth of direct marketing activities in recent years."
an important test we have done was devoted to understand if the two phase approach outperform the traditional just employing visual features. the lift related the proposed approach by using svr and visual features are reported in fig. 10 .
"where n represent number of pixel object. furthermore, the grayscale and the binary image are rotated about ( θ ) degree. c. bounding box operation is applied to the rotated binary image to get the smallest rectangle which contains the binary hand image. the original hand image, binarized image, and the bounded image shown in figure 2"
"traditionally, direct marketing companies have relied heavily on pre-testing to acquire knowledge and select the best offers [cit] . the pre-testing process is simple and widely used across the industry. first, the set of offers under consideration is sent to a limited sample of potential customers. then, depending on the sample response, companies compare the performance of each offer and select the best offer for each segment of the population. performance measures will vary depending on the specific application (catalogue, email, mobile messaging, etc.) and can include the number of items bought, the revenue per order, the click-through-rate (ctr), and the number of calls generated."
"these simple visual features, coupled with text features, revealed to be quite powerful for the learning phase, especially when compared to the performances obtained using only text characteristics [cit] ."
"to determine the static features of the short descriptive message of each offer, we collected the set of most common words among all messages in the training set (i.e., among all messages for which performance is assumed known)."
"as described in sect. 3, in the type of applications we are considering, it is very likely that the arrival rate is greater than the sending rate. we call overcapacity rate the difference (arrival rate-sending rate). in addition, we call overcapacity the total number of unlearned messages. hence, overcapacity measures the number of items on which we cannot learn, under the given learning space constraints. since overcapacity grows monotonically each day (the rate of growth is given by the overcapacity rate), with a positive growth rate (common in the new direct marketing applications discussed previously) the learning task becomes more difficult over time."
"considering the result obtained with the regression methods mentioned above (see sect. 5 below) and taking into account the properties of the mms feature representation (e.g., that textons provide a global representation, that text is represented by binary variables for each relevant word, etc.) we also tested a combination (also called cascade) of lwr and svr methods as depicted in fig. 5 ."
"as we can see from fig. 8, for the textons-based features we obtain the best results using the svr model. we can conclude that the svr is able to exploit the global visual representation of f textons, whereas other regression methods, like the rt, are not able to treat properly the textons-based visual representation. these other methods rely on the information from individual components of the we can make similar considerations for text-based features. rt is less powerful than the other two regression methods because it is not able to look at the word-related dummies jointly. however, to capture the semantic of an offer, it is better to take into account different words together than to look at each word singularly. lwr works well because the cosine similarity metric employed works well on the tf-idf representation during the experiments."
the normalization process is needed to reduce the possible imperfections in the palmprint image due to non-uniform illumination. the normalization method employed in this research as follow:
"the domain of our empirical application is the targeting of multimedia messages (mms) sent to mobile phone users, one of the new direct marketing channels. in this context a single multimedia message is sent everyday to the mobile phones of millions of customers. each message contains a commercial offer and each offer advertises a specific product or service that can be purchased directly from the mobile phone with few clicks (e.g., a ringtone, a song, or a video)."
"given the speed of offer production, even with daily contact of customers (e.g., through messages sent to mobile phone users daily), the number of offers to be tested grows at a faster pace than the rate at which the system is able to learn while at the same time keeping enough potential customers for optimized delivery. finally, offer expiration is now often close to its release date. in some cases offers need to be sent in a matter of days or even hours after they have been made available for selection and learning (e.g., news related services, holiday related offers). this further limits the opportunities and time for learning."
"we tested for the number of dummy variables to use in representing the mms text. we report on the experimental results based on the best number of dummy variables for each one of the methods applied. specifically, through the filtering process discussed in sect. 2, we obtain the best results using an average of about 1,150 dummy variables in svr and rt, and an average of 881 dummy variables in lwr."
"the rmse(w) and the lift measures were computed for each simulation run. we then computed the average rmse and lift across all data subsets and across all runs. below, we will report on such averages."
"in this case, the lift provides us a measure of how the proposed approach performs (in terms of revenue) relative to the traditional pre-testing (in which we do not pre-screen items for subsequent testing). in addition to the lift measure, and to better assess the performances of the proposed learning approach in estimating the ctr fig. 6 the labeled dataset was subdivided in three subset to perform the testing phase. each subset was used to simulate the mms direct marketing system. the results relative to each subset were collected and used to analyze the performances of the proposed two phase learning approach of each offer, we also compute the root mean squared error (rmse) on the ctr predictions from the test data w:"
"another contribution of this work is a new two-phase learning strategy in which a cascade of regressions methods is used to improve on classical learning methods. the suggested learning approach first learns through regression based methods which mms offers available for learning should be subject to further testing, and which offers should wait for a better moment or simply be discarded. this first phase takes advantage of the static (non-behavioral) features extracted from image and text and from previous ctr information. in a subsequent phase, we propose to learn only on pre-screened mms offers up to the limit of the system's learning capacity. this secondphase learning will take place just like in traditional direct marketing learning systems: the offers are sent to a sample of potential customers and the behavioral feedback registered and used for final offer selection. in our empirical application we show that the proposed approach can improve substantially the learning and optimization process, whenever direct marketing companies are operating under time and learning space constraints."
in fig. 4 we report some images from the test dataset and the closest training images in terms of textons distribution. the similarity between test and training images was computed by using a metric based on bhattacharyya coefficient [cit] on textons based representation discussed above. note that there is a semantic consistence in contents between the test images and the corresponding closest training images.
"a successful approach to improve the performance of direct marketing systems on new multimedia channels, will require the contribution of wide range of disciplines and technologies including computer vision, data mining, statistics and marketing. in this paper, we presented an application domain where we learning needs to take place under severe time and space constraints. in particular, we propose a two-phase learning approach for a direct marketing application that relies on mms for offer delivery. in our approach, we propose to exploit the visual and text features of each mms offer through a cascade of regression algorithms to estimate the potential (in terms of expected revenue) of each offer. we demonstrated that this approach leads to a considerable improvement in overall performance. researchers and businesses can also use the presented schema in other domains in which visual and text information is available (e.g. offers sent by email, interactive television, etc). moreover, the results obtained during experimental phase have shown good performances also when visual features are used alone. this means that when visual information (e.g. images) are present, exploit information on visual content can make more efficient and effective the learning phase."
rmse is a frequently used measure of the the distance between the predicted values of a dependent variable and the true variable values for a given prediction method. note also that in eq. 3 the ctrs values were normalized to the range [cit] .
the first learning phase (see sect. 3) uses visual and text features and the related known ctr of the commercial offers previously sent. through automatic computer vision and text mining techniques we extract a set of static features that characterize the content of each mms offer. the extracted features on past offers and the related effective ctr are used to train a regression-based method (see sect. 4) in order to estimate the ctr for new items that will be used in the second learning phase.
"we randomly split the dataset described in sect. 1 into three partially overlapping subsets of 4,500 mms offers each (fig. 6) . we sorted the mms based on their arrival date and time and used the first 80% of the mms offers within each subset as training data. we retained the remaining 20% as our test data."
"as stated previously, for those messages not yet tested in the population, we use the predicted ctr (obtained during a first phase of learning) to sort the offers and decide which ones to subject to further testing (i.e., to send to a second phase of learning). hence, we only perform additional tests to the most promising offers (up to system capacity). we assess each offer based on performance potential (see sect."
"by using the rmse(w) defined in previous section, we compare the two visual representations. specifically, we are interested in understanding which representation achieve the best results in predicting the ctr of mms offers. moreover, we wish to understand which regression method (rt, lwr, svr) gives the best result on visual representation and eventually why."
"for example, in the context of mobile messaging the number of alternative offers to be tested is extremely high and grows at a fast pace. it is not unusual to have more than 50,000 possible products or services to advertise at any moment, and the content catalogues can grow by twenty to thirty new items a day, a growth rate that is not likely to be reduced. in addition, even though there are millions of customers to contact, the number of opportunities for contact is small. receiving too many commercial messages a day increases the likelihood that a customer will cancel a service or switch operator due to annoyance. hence, operators do not want to fill customers' inboxes with too many messages. this means that one single person can only be exposed to no more than a very small fraction of all possible products and services. hence, the limited size of mobile phone screens and the risk of alienating and overwhelming users if too many messages are sent daily, impose significant limits on the number of learning opportunities."
"the pre-testing process has worked well for the many direct marketing applications characterized by a low cost of contact and a large customer base (e.g., traditional mail)."
"we believe the cascade of regression methods we propose is better suited to capture the properties of each static feature (visual and text) and takes into account the specific strengths of each regression method, as we will show below in our experimental results."
"all of palm images are captured using sony dsc p72 digital camera with resolution of 640x480 pixels. each person was requested to put his/her left hand palm down on with a black background (five samples for each person). there are some pegs on the board to control the hand oriented, translation, and stretching as shown in figure 1 ."
"the main contribution of this research is to demonstrate that one can make the learning phase more efficient and effective, with respect to the traditional learning phase, by taking advantage of the information on visual content together with the textual information present in each offer. the proposed strategy is assessed by simulation of an mms direct marketing system using a real mms dataset, and it outperforms traditional learning methods. moreover, results demonstrate that the proposed learning strategy outperform the traditional learning also when the visual information are used alone."
"in this section, we describe in detail the experimental setup of our simulated mms direct marketing system. next we describe the results of our experiments using the proposed learning approach, and we then discuss our main findings."
"because we can extract different types of features from our multimedia data (textons based and text based), we first conduct a performance assessment when using each one of these features in isolation, with each one of the regression methods (rt, lwr, and svr). to determine the most appropriate regression method for each type of feature when considered in isolation, we compare the rmse(w) of the each method-feature combination. this comparison will also allow us to select the best regression method we should use to extract knowledge from each feature (which we can then use when we combine features together in the regression cascade). figure 8 reports the rmse(w) results obtained using visual and text features when used in isolation when using the alternative regression models."
the genuine and imposter scores are obtained from the matching scores of same person and different person respectively. there were not zero scores that produced from the matching process that means all palmprint samples that are used in this experiment are different.
"given the number of people in the customer base, and the reduced number of opportunities to contact them, we are able to learn only on a small number of new mms offers each day. we define the sending rate to be the number of daily (learning) trials. this corresponds to the maximum number of mms offers we can send to the learning panel each day (the learning panel being a set of customers we use for learning on new offers). we define arrival rate as the average number of new mms items added to the offer catalogue each day. we note that, before a final decision is made on which mms items to send to the optimization portion of the customer base, we should learn on the revenue potential of each one of these new items."
"3). we determine this potential by multiplying the estimated ctr of each mms with its price (i.e., the revenue for the provider from each user click). hence, once the learning phase is complete, we can compute the overall expected performance of any regression method (rm) taking into account the set of the chosen mms offers as follows:"
"results conduced by using the two-phase approach are discussed in sect. 5. we note that improving the learning procedure, by including the initial filtering (first phase learning), could potentially allow us to use a smaller sample of customers in the traditional learning phase and release customers for the more profitable optimized content delivery."
the rt method derives a set of if-then logical (split) conditions. this method does not assume any linear relationship between dependent and independent variables. one major drawback of rt models is that a threshold value determined in each stage of the tree construction does not correspond to the optimal boundary of bipartition of input subspace in that stage. to test the rt performances the wagon library [cit] has been used in our experiments.
"we tried two alternative formulations for the cascade of regressions. in the first one, we model the text features using the lwr. we then combined the ctr predicted using the lwr with the visual features, used a svr model. the second alternative cascade tested included also text features directly in the svr model in addition to the predicted ctr from the lwr output and the visual features (this cascade model is presented in fig. 5 ). we added text features again because the svr and lwr models provided comparable result on these features, though they exploit the data in a very different manner (svr looks at feature point in the global space whereas lwr looks locally around the features point). we then exploit again the text features directly in the svr to take advantage of the different types of information contained in the text. we report the rmse(w) results of these cascade regressions in table 3 ."
"visual content, and again use the method most adequate for the visual features (as indicated by our initial experiments). we will describe in more detail the cascade of regression methods in next section."
future works should be devoted in building a new visual representation by exploiting text first to have some prior about the offers' category [cit] .
real and imaginary part of normalized gabor filters are convoluted to the normalized palmprint images and produces real and imaginary characteristic features with equal size to the original palmprint size.
"as a result, it is today essential for the mobile service providers to quickly understand the customer's needs and interests and select the right services to promote, at the right time, and in the right way. a good optimization and targeting system, that learns quickly and efficiently and that selects the right message/product to be sent to each customer so that revenues are maximized, could provide an additional profitability boost."
"to better understand the performances of the overall system using the cascade approach, we computed the lift for alternative model formulations. in fig. 11, we present these lift results. it is clear from the figure that using the proposed approach (irrespective of the final regression formulation) improves significantly the overall performance of the system producing significantly higher revenue. this is because we selected the best offers during the first learning phase to be subject to further learning, whereas we discarded the weakest options did not wasted the scarce testing opportunities with offers that a priori seemed too weak."
"in sect. 5 we will discuss experiments to discover which features (f color, f textons, f text ) leads to maximum performances. moreover, we will point out that using only visual features our proposed methods outperform the traditional learning approach (see sect. 4)."
"the remaining sections are organized as follows. in section 2, the signal model of the forwardlooking fda-sar system is presented. in section 3, a range ambiguity resolution approach is proposed. section 4 gives a method to distinguish left-right ambiguity. section 5 discusses the selection of system parameters. in section 6, some simulations are presented to verify the effectiveness of the proposed method. in the end, conclusions are drawn in section 7."
"to fix the concepts and also motivate our methodological development, consider the following example. table 1 gives mrna expression data from a female subject exhibiting triple negative breast cancer, or tnbc. (tnbc is often more difficult to treat than other breast cancers and thus demands a poorer prognosis; it also disproportionately affects minorities."
"fda [cit] . it has attracted growing attention in recent years. the most important difference between fda and the traditional phased-array is that the former employs a small frequency increment, while the latter has the same frequency between each transmit array. differing from the traditional phased-array and multiple-input multiple-output (mimo) antennas, which provide only angle-dependent transmit beampatterns, fda is capable of forming angle-range-dependent transmit beampatterns. consequently, the echoes of different range regions can be completely separated in the frequency domain. the characteristics of angle-range-dependent"
"to define gene clusters within a pathway, we devised a two-stage algorithm: (i) estimate inter-gene correlation via independent source(s), then (ii) conduct unsupervised clustering of those genes using the external correlations to define gene-gene similarity."
"the allocator web interface provides several interactive and dynamic views to explore and edit the results generated by the allocatorsd processing pipeline (or by camera). in the following bold face names mark tools and views as they are available in the allocator user interface, of which some are displayed in figure 3 : the molecule list view provides a central table for each chromatogram, which displays all detected pseudo spectra and some relevant information, like the putative mass m of the original molecule and a list of kegg [cit] compounds that have the same molecular mass m, as well as links to its pseudo spectrum view. the pseudo spectrum view consists of a table, listing all adducts and losses that were assigned to it, and an interactive pseudo spectrum plot that displays these peaks, their isotopes, and (if available) 13 c isotopologues. on demand, the extracted ion currents of all the contributing masses can be loaded directly into the view. using context menus, peaks can be edited or removed. other correlating peaks can be loaded into the view, and eventually added to the pseudo spectrum. a detailed list of kegg compounds with the mass m is integrated. additionally, a spectrum-aware mass decomposition is integrated, that optionally restricts resulting sum formulae using a variety of intelligent filters (see the section spectrum-aware mass decomposition below). we define 'spectrum-aware' methods as logics that do not only base on the mass of a pseudo spectrum's putative molecule, but additionally consider the available fragmentation pattern to generate more precise results."
"in a short simulation study, the method exhibited satisfactory false-positive error control and robustness against certain modeling assumptions, including the form of inter-cluster correlation. the method also powerfully detected deps when the number of clusters and fold-change of differentially expressed genes (degs) was large. in addition, it is worth mentioning that our cluster-based approach does not require estimation of inter-gene correlations for the single n-of-1 subject being studied, as we show in an example with a tnbc patient."
"here, we consider a far-field point target at p(x 0, r 0 ), as shown in figure 2 . the echo received by the mth element can be expressed as"
"while the broad n-of-1 strategy possesses obvious impact for advancing personalized medicine, in certain settings, the approach can exhibit greatly expanded capabilities. consider, e.g., a molecular n-of-1 analysis where data on a gene's messenger rna (mrna) expression are sampled from a patient's diseased organ or tissue. rather than study expression levels across the patient's entire transcriptome en masse, we previously proposed a framework, called n-of-1-pathways, that applies the n-of-1 strategy to pertinent gene pathways, i.e. to preassigned collections of gene sets within which the genes are assumed to have associated mechanisms or functions. [cit] gene set (pathway) membership is typically defined using curated knowledgebases; here, we employ the gene ontology (go) 9 knowledgebase and its corresponding go-biological processes (go-bp) gene sets."
"all automatically annotated 13 c-labeled peaks and thereby inferred numbers of carbon atoms were consistent with the annotation of neutral losses, which was initially performed only on the basis of m/z differences. intensity ratios for all 12 c monoisotopic peaks to their fully +, the adduct of two fully 13 clabeled glutamic acid molecules. this peak was manually added to the pseudo spectrum using the context menu (see pseudo spectrum in figure 4a ). all available information taken together enabled a reliable identification of glutamate, although no distiction between the l-and d-enantiomer was possible."
"metabolomics is the systematic analysis of the set of metabolites that are synthesized by an organism -also known as the metabolome [cit] . the analysis involves different steps to get from the wet-lab experiment to an evidence or assumption of biological significance. one of the workhorses for the measurement of small molecules in biological samples is liquid chromatography coupled to mass spectrometry (lc-ms), using electrospray ionization (esi). but it is not the data acquisition that is posing the greatest challenge to metabolomics: [cit], asking for the greatest bottleneck of metabolomics, 35% of the respondents named the identification of metabolites the biggest challenge, 22% thought that assigning biological significances is most important, and 14% decided that data processing/reduction is the crucial bottleneck [cit] ."
"11 ) the table presents matched normal-tumor pairs of mrna expression outcomes from a sample of the patient's healthy breast tissue (left column) and of her cancerous breast tissue (middle column) in a specific gene pathway, renal system process involved in regulation of systemic arterial blood pressure (go:0003071), made up of 15 individual genes. the data were taken from a large collection of rna-seq data sets obtained from the cancer genome atlas (tcga; see acknowledgments). the collection yielded 20,051 mrna counts, as mapped to hugo gene symbols 12 for both normal and tumor samples derived from this patient. following standard practice, a stabilizing transformation was applied by adding 1 to each normalized count and then taking a base-2 logarithm."
"where c is the velocity of light; d tn and d rm denote the range of the nth transmit element and the mth receive element relative to the reference element, respectively; v is the velocity of the platform. we assume that the scene satisfies the far-field and narrowband assumptions. then, τ mn (t a ) can be expressed as"
"in the allocator workflow (see figure 1 ), the first job to execute applies the centwave [cit] lc-ms feature detection method of the xcms [cit] software (version 1.26.1) for r. this three-step procedure starts with the creation of m/z-slices, socalled extracted ion base peak chromatograms (eibpc). each of these is further processed using a matched filter, which is equivalent to a second derivative gaussian function. using the zero crossing points of the resulting filtered chromatogram as integration borders, peaks with a sufficiently high signal-tonoise ratio are integrated in the unfiltered chromatogram. generated peak tables and the r object are stored to serve as input for the next step in the workflow: spectra deconvolution. now, two options are available: either the new allocatorsd algorithm for spectra deconvolution, which will be described in detail in the next section, or the camera tool for ''compound extraction and annotation'' [cit] . camera groups peaks based on retention time and peak correlation. within these peak groups, isotopic peaks are identified and associated with their respective monoisotopic peak. differences between the m/z-values of all possible pairs of monoisotopic peaks are calculated and matched against a list with differences of common adducts and neutral losses, as well as possible combinations of these."
"amongst the metabolites with the most prominent peaks in both strains we identified several dipeptides. the calculated monoisotopic masses all matched those of at least two different peptides, containing a glutamyl residue at the n-or the c-terminal end. on the basis of the calculated mass alone it was not possible to distinguish between the isobaric compounds, but positional information could be inferred from the generated pseudo spectra. these included peaks for the respective y 1 99-fragment of the peptide (figure 5), showing that all dipeptides had an n-terminal glutamyl residue (see figure 5 and figures s2, s4-s6) . the automatic annotation of the y 1 99-fragments was possible through the unique ability of allocatorsd to deal with 13 c-labeling experiments. these uncommon fragments are not included in the list of small neutral losses, but could be annotated in the 6 th step of the allocatorsd pipeline (see figure 2 and corresponding section). so far we were able to identify the dipeptides as glutamylmethionine, glutamyl-valine, glutamyl-(iso)leucine and glutamyl-glutamine. in case of glutamyl-glutamine the y 1 99-fragment was not assigned by allocatorsd. the tool find correlating peaks was used with a lowered correlation coefficient threshold of 0.75. the peaks (m/z 147 and m/z 152) representing the expected y 1 99-fragment and its fully labeled 13 c isotopologue were present and added to the pseudo spectrum (see figure s2 ). checking the extracted ion chromatograms (eics, see figure s3 ), a different peak shape for these m/z values and a slightly higher retention time compared to the other peaks of the pseudo spectrum was observed. additionally, the intensity of the fully 13 clabeled peak compared to the 12 c monoisotopic peak was higher than for all the other peak pairs. all these differences could be referred to the coelution of free glutamine, which was checked by the analysis of l-glutamine standard."
"if questions exist regarding the t-test's reliance on normal (gaussian) distribution sampling, appeal can be made instead to its well-established nonparametric analog, the wilcoxon signed-rank test for two-sample matched-pairs data."
"indeed, an unanswered problem when testing for deps with single-patient n-of-1 data is how to incorporate non-zero inter-gene correlation(s) from a single n-of-1 sample of g gene expressions. herein, we propose use of external information to identify clusters of correlated genes within a pathway, and then manipulate that information to build a correlation-adjusted, cluster-based test statistic for assessing differential n-of-1 expression in that pathway. (mention of external information naturally leads one to consider some form of hierarchical bayesian model. while we do not dismiss this possibility, it is not our goal in the methods we present below to employ a bayesian approach. our attention focuses instead on developing frequentist strategies for the dep testing scenario.) we begin in section 2 with a description of our model and clustering strategy, along with a suggested algorithm for its implementation. section 3 follows with a short simulation study of the method's operating characteristics, with focus on its performance in the presence of inter-gene correlation. section 4 illustrates the approach by returning to the tnbc data from section 1.2, while section 5 ends with an overview and discussion. note that all calculations we present below are performed in the r statistical programming environment. 18"
"where f m (ρ, θ) represents the energy of the pixel (ρ, θ) (ρ, θ) received by the mth channel, ρ is the slant range between the pixel and the reference element, and θ is the azimuth angle of the pixel. since two targets, which are symmetrical about the y axis (as figure 4 shows), have the same doppler frequency magnitude, the two symmetrical pixels accumulate the same energy."
"it can be seen from equation (16) that the transmit steering vector is a function of range and angle, a(r, ψ) can be expressed as"
"where d m and d n denote the distance between the receive elements and transmit elements, respectively. we assume that the scene satisfies the far-field and narrowband assumptions. then, ( ) here, i d is the range between the ith epc and the reference epc. it can be written as"
"pre-processing algorithms that are offered by allocator can be started either for a single chromatogram or for all the chromatograms of an experiment at once. users can set parameters for these algorithms through the web interface. these pre-processing ''jobs'' are submitted to the compute cluster of the center for biotechnology of bielefeld university (cebitec), hosted by the bioinformatics resource facility (brf). whenever the java software has to call programs running in the r environment [cit] (version 2.13.2), this is realized through the runiversal package [cit] for r."
"as a referee has noted, selection of the core clustering algorithm in section 2.2 is a fundamental linchpin for creating our external gene clusters and implementing the test statistic in equation (7). while we recommend the use of ap clustering, many other algorithms could be applied in its place 44 and whether any of these would provide enhanced (or inferior) performance over ap is an open question. indeed, one would expect results from any reasonable alternative clustering procedure to be at least roughly comparable to those from ap if the clustering pattern is sufficiently strong."
"in the traditional phased array radar, it is assumed that the same carrier frequency is radiated by each array element. differing from a traditional phased array, the carrier frequency of fda radiated from each element is identical, with a frequency offset ∆ f, as shown in figure 1 ."
"in this paper, we introduce an approach to imaging without ambiguity for forward-looking fda-sar. the range ambiguity can be solved by applying a range compensation function and transmit beamformers. then, the range ambiguous echoes can be separated. next, back projection (bp) is applied to imaging. finally, the imaging results of all channels are processed by receive beamforming. as a result, unambiguous imaging for the forward-looking fda-sar can be achieved."
"the imaging of the targets by the proposed method for the forward-looking fda s ented in figures 7 and 8 . the trajectory of the targets after range compression is given in ue to left/right ambiguity, the trajectories of target 1 and target 2 are overlapped. the traje rget 4 and target 5 are also overlapped. the echoes of the first range region after extractin range ambiguous echoes by compensation and transmit beamforming are shown in figur be seen that the range ambiguous energy of target 4, target 5 and target 6 has bee pressed by applying transmit beamforming. the echoes of the second range region are pre igure 7c. we can observe that the energy of target 1, target 2 and target 3 is also well suppr the imaging of the targets in the first and the second range regions, which are focused shown in figure 8a,b, respectively. it is apparent that the proposed range ambiguity reso roach has effectively resolved the range ambiguous echoes into two unambiguous parts. in t can be seen that the energy of the targets in the first range region is well focused, while second range region is suppressed. in addition, the energy of target 1, target 2, and targ ributed on both the left and right sides of the scene because of left-right ambiguity. in figu n be observed that the energy from the second range region is focused by bp. additional rgy of target 4, target 5, and target 6 is distributed on both the left and right sides of the sce lying receive beamformers to the images after bp, we can resolve the left-right ambiguous i two unambiguous parts. figure 8c,d show the imaging of the targets from the first range and right sides of the scene, respectively. compared with figure 8a, it can be observed th rgy from target 1 is well focused, as shown in figure 8c, by applying the receive beamf igned for the left side of the scene. we can observe that the targets on the right side of the the first range region are well focused in figure 8d by applying the receive beamf the imaging of the targets by the proposed method for the forward-looking fda sar is presented in figures 7 and 8 . the trajectory of the targets after range compression is given in figure 7a . due to left/right ambiguity, the trajectories of target 1 and target 2 are overlapped. the trajectories of target 4 and target 5 are also overlapped. the echoes of the first range region after extracting from the range ambiguous echoes by compensation and transmit beamforming are shown in figure 7b . it can be seen that the range ambiguous energy of target 4, target 5 and target 6 has been well suppressed by applying transmit beamforming. the echoes of the second range region are presented in figure 7c . we can observe that the energy of target 1, target 2 and target 3 is also well suppressed. the imaging of the targets by the proposed method for the forward-looking fda sar is presented in figures 7 and 8 . the trajectory of the targets after range compression is given in figure 7a . due to left/right ambiguity, the trajectories of target 1 and target 2 are overlapped. the trajectories of target 4 and target 5 are also overlapped. the echoes of the first range region after extracting from the range ambiguous echoes by compensation and transmit beamforming are shown in figure 7b . it can be seen that the range ambiguous energy of target 4, target 5 and target 6 has been well suppressed by applying transmit beamforming. the echoes of the second range region are presented in figure 7c . we can observe that the energy of target 1, target 2 and target 3 is also well suppressed."
"cluster algorithm selection is clearly a non-trivial component of our larger strategy and a need exists for moreextensive study of it and of the larger clustered-t methodology. we are exploring all the various issues discussed here and in section 5.1 above, and we hope to report on them in future manuscripts."
"one major aspect of lc-ms based metabolomics that is only recently stepping into the focus of cheminformatics is metabolite quantitation via isotopic labeling. the use of stable isotopic labeling (sil) has become an important and popular approach in the field of metabolomics. many strategies using sil were developed, enabling more accurate metabolite identification and quantitation in complex biological samples [cit] . the numerous advantages of this common approach have been reviewed recently [cit] . common to most sil experiments is the mixing of naturally labeled (unlabeled) samples with samples that are enriched with stable isotopes and the analysis of these mixed samples by gc-or lc-ms. either one group of samples from one experimental condition is unlabeled and another set of samples from a second experimental condition is labeled, or both groups of samples are unlabeled and a labeled internal standard is added to each sample. in any case, this allows calculating abundance ratios of metabolites in the two samples, while matrix effects can be neglected [cit] . additionally, the distance between the signals of the unlabeled and fully labeled isotopologue peaks provides substantial benefits for metabolite identification as it can be used to infer the correct number of atoms of the respective element in the analyte. this facilitates a more precise calculation of sum formulae. the software tool mzmatch-iso [cit] offers the necessary preprocessing for that and consequently allows to associate 13 c peaks to their respective 12 c counterparts, thus providing the basis to generate ratios. mzmatch-iso however lacks support to identify the adducts and losses of complex lc-esi-ms spectra. this holds also true for the commercially available isotopic ratio outlier analysis (iroa) software (nextgen metabolomics, michigan, usa) [cit] an algorithm and program (metextract) was published that associates monoisotopic unlabeled and monoisotopic labeled peaks of the same metabolites [cit] . it uses the mass difference between the two peaks and the charge that is inferred from the isotopic pattern to calculate the corresponding number of atoms of the labeling element (e.g. carbon). furthermore, it assembles peaks of extracted predefined adduct-, fragment-and polymer ions into peak groups. only recently, the xcms package was extended for the analysis of isotopically labeled compounds by the introduction of x 13 cms [cit] . x 13 cms associates e.g. (u-) 13 c-labeled peak groups to their corresponding unlabeled peak groups in another measurement. this is taken as a basis for differential analyses."
"where 0 f denotes the carrier frequency of the reference antenna. usually, δf is far smaller than 0 f and the system bandwidth b ."
"where i denotes the index of the targets in a range bin, l is the index of range bins, p denotes the index of range region, and k is the index of the pulse number. the instant slant range between the ith target and the reference element can be expressed as"
"the processing flow chart can be summarized as in figure 5 . after matched filtering, the signal is processed by range compensation and transmit beamforming to resolve the range ambiguous echoes into several unambiguous parts. then, by performing bp on the echoes without range ambiguity received by each channel, we will have the image with left and right ambiguity. next, the echoes after performing bp are processed by two receive beamformers, respectively. finally, we can obtain the unambiguous images of the two sides."
"aiming to assign a component to each valid pseudo spectrum, an easy to use annotation functionality has been set up. pseudo spectra can be annotated either by filling a simple manual annotation form, or favorably by confirming a kegg compound with a single click. hits from the massbank database can be copied into the manual annotation form with a single click, too."
"the current landscape of metabolomics software provides solutions for each step of the entire processing from lc-ms raw data, to signal processing, to metabolite identification and relative quantitation. nevertheless, it misses one that (a) uses the full potential of 13 c-stable isotopic labeling for metabolite and fragment annotation, (b) is optimized for mass isotopomer ratio analysis, (c) provides users with an interactive interface not only to explore but also to modify the results of automatic processing, and finally (d) addresses the strong and well advanced evolution of research projects towards cross-group collaborations [cit] . to fill these gaps we developed the allocator system, presented in this manuscript. allocator is a novel web-platform particularly for the comprehensive analysis of metabolomics lc-esi-ms (labeling) experiments and is streamlined for mass isotopomer ratio analysis. it covers all aspects (a) -(d), as shown in our application example."
"the complete procedure shall be demonstrated by the identification of glutamic acid, the most prominent metabolite and initial substrate for arginine biosynthesis in c. glutamicum. using the search toolbar, the peak list was filtered to solely display metabolites with annotations containing the term ''glutamate''. the list included a pseudo spectrum (m147.052t287.92) with six unlabeled peaks of a putative metabolite with a calculated neutral monoisotopic mass of 147.052 da and a retention time of 288 seconds as depicted in figure 4 . the neutral mass matched 10 entries listed in the kegg database with a mass deviation of 0.01 da (see table s5 )."
"since it mimics the f-ratio from an analysis of variance, large values of equation (4) suggest higher pertinence for the given cluster assignment, or translated here, for the input value of the corresponding q. thus, we maximize equation (4) to help select an operable value for q. with all these components in place, our correlation-based clustering algorithm takes the following steps:"
"where c n (r) is the transmit steering vector for the nth transmit element. it can be seen from equation (24) that the second term of each range region is different. therefore, we can separate the ambiguous echoes by utilizing the second term. the first term can be compensated by the following compensation function."
"the aim in our previous works [cit] was to improve and enhance mechanism-anchored, single-subject, gene expression analysis. in practice, the tactic assumes that the mrna expressions are collected across all pathways of interest from a single patient under two different paired conditions-e.g. baseline and case, unaffected tissue vs. tumor tissue, before and after treatment, etc. the goal is then to quantify and test for differential pathway expression for that patient using the data derived under the two paired transcriptomes from those different conditions. the framework represents a substantial opportunity for a clinically actionable and economically efficient approach to personalized medicine. 10"
"conditional on the external cluster assignments, we calculated our ap-based clustered-t statistic from equation (7) and found its pointwise, two-sided p-value, corresponding to a tðm à 1þ reference distribution, for each of the scored pathways (including the four pathways exhibiting no inter-gene correlation). pathways could be classified as either differentially expressed (dep) or non-dysregulated, depending on whether the null hypothesis was rejected or not, respectively. one goal of the n-of-1-pathways strategy is to use such information to isolate possible pathways that affect or associate with the patient's disease outcome, i.e. here with tnbc. for example, a non-dysregulated pathway that is a target of a therapeutic drug could be indicative of a patient's poor response to therapy."
"to achieve optimal range ambiguity resolution performance, the system parameter design for the forward-looking fda-sar is discussed in this section. in order to analyze the suppression effectiveness of the echo from the undesired range region, a function is defined by"
"for patient tcga-a7-a0ce, 601 of her scored pathways gave a pointwise, unadjusted p-value at or below 5%. to correct for multiplicity, we further applied a benjamini-hochberg 15 false-discovery adjustment. this resulted in 80 deps with a false discovery rate (fdr) less than 15%. (among these was included the go:0003071 pathway in table 1.) table 3 displays the 10 top-identified dysregulated pathways, ordered by their original, cluster-based p-values. supplemental table s4 lists all these 80 deps."
"where ϕ n (t) is the unit energy transmit waveform of the nth antenna. additionally, it is assumed that waveforms are orthogonal, which can be written as"
"besides this major aspect, there are further differences between allocatorsd and camera. firstly, the two tools use different lists of predefined adducts and neutral losses: the number of adducts and neutral losses prearranged in camera is higher, and combinations of these can be detected, too. the respective list in allocatorsd is shorter, but customizable through the web interface. secondly, allocatorsd offers an additional level of control by introducing the concept of seed adducts, which have to be present in every pseudo spectrum. for example it can be specified that every pseudo spectrum must contain at least one peak annotated as either the pseudomolecular ion [m+h] + or [m+na] + . both here described characteristics of allocatorsd develop their potential most, if there is some empirical knowledge about the occurrence of certain adducts and neutral losses. the most frequent ion species should be used as seeds, those which are never observed can be excluded from the list."
"we have introduced a method for scoring and testing differentially expressed pathways (deps) from a singlesubject, matched-pair collection of gene expression data under the n-of-1-pathways paradigm. 6 by focusing on gene sets/pathways, this n-of-1 strategy provides a powerful tool for development of subject-specific precision medicine. unfortunately, the limited amount of data within the n-of-1, single-subject setting makes construction of suitable statistical inferences difficult. in particular, we illustrated that the presence of inter-gene correlation within a pathway undermines the ability of standard paired-testing methods to control false-positive error. we argue instead for incorporation of external information into the significance test procedure. toward this end, we propose a novel correlation-based clustering algorithm that employs external gene expression data to aggregate positively co-expressed genes into clusters within pathways. to our knowledge, this is the first single-subject gene set testing procedure that accounts for inter-gene correlation."
"the identification of bottlenecks by the detection of accumulating pathway intermediates in large libraries of strains is an integral part of modern metabolic engineering strategies and biotechnology [cit] . to demonstrate functionalities for export of data and relative quantitation of metabolites, it was obvious to compare the relative abundances of metabolites of the arginine biosynthesis pathway, since c. glutamicum atcc 21831 is an l-arginine producing strain. here it was possible to identify the substrates l-glutamate and l-glutamine, the intermediates n-lacetylglutamate, l-citrulline and n-l-argininosuccinate, as well as the endproduct l-arginine (see figures s1, s7, s8, s9, s10). for each confirmed metabolite, peak intensities and areas were automatically normalized to internal standard and biomass, and exported to an xls document. relative quantitation between sample groups and statistics were performed in a spreadsheet (see table s6 ). metabolites mentioned in the following were quantified using the peak areas of the respective [m+h] + ions, and all their abundances were significantly different between strains. the significance was determined by student's t-test and multiple testing errors were corrected using the method of benjamini and hochberg [cit] ."
"where r re f is the slant range between the target and the reference element, and θ(t a ) and ϕ(t a ) denote the instant azimuth and pitch angle, respectively, as shown in figure 2 ."
"in this paper, an approach to range ambiguity and left-right ambiguity resolution for hsv forward-looking fda-sar is proposed. utilizing the proposed algorithm, high-resolution and wide-swath (hrws) imaging can be achieved. compared with the conventional forward-looking sar imaging algorithm, the algorithm presented in this paper can distinguish the range ambiguous echoes by fda. fda introduces the wave-path difference among the elements, leading to the range-angle-dependent property of the transmit steering vector. therefore, fda is capable of separating the range ambiguous echoes in the spatial frequency domain.by performing range compensation and transmit beamforming on the echoes after matched filtering, the range ambiguous echoes can be resolved into several parts without range ambiguity. afterwards, the bp algorithm is applied to the received data of each channel, allowing the images with left-right ambiguity to be achieved. by exploiting the receive degree-of-freedom in space, left-right ambiguous echoes can be resolved into two parts. subsequently, we can obtain the hrws imaging. evidently, the simulation results have verified the effectiveness of the proposed range ambiguity and left-right ambiguity resolution approach for the forward-looking fda-sar system. for future work, we will explore the waveform design issues for the fda-sar system."
"data can be exported in several ways and file formats. for each chromatogram, peak lists as well as molecule lists can be exported. peak lists are basically in the same data format, as generated by the camera functions xsannotate and getpeaklist, but extended by one column for the association information of c 'abundances' are given to reflect relative quantities. molecule lists can be downloaded as a single file for the entire experiment. here, 'abundances' are intensity, area or baseline corrected area as determined by xcms, divided by the samples biomass or optical density. all files can be downloaded as comma separated files, tab separated files or microsoft excel sheets."
"our motivating goal is to identify deps within a single n-of-1 subject while recognizing that the genes' expressions within a pathway are likely correlated. we restrict attention in this section to a single target pathway, studied under two paired conditions-generically listed as ''case'' and ''baseline''-as is typical in n-of-1-pathways applications. (consideration of more than two conditions is certainly possible, but we have not seen any instances of such in practice.)"
"applying the obtained receive weight w 1 and w 2 to the received signal after bp, we can separate the signal from the pixels (ρ, θ) and (ρ, −θ), which can be expressed as"
"more generally, while both algorithms yielded roughly similar patterns in the cluster sizes here, the potential exists for them to produce different cluster solutions with a given historical data source. a particular pathway therein may possess an affinity pattern which engenders several essentially-optimal clustering solutions and (7) when applied to tnbc data from section 4. dot color indicates p-value overlap status: (i) dark gray dots for significant p-value overlaps (both below 5% cutoff), (ii) white dots for insignificant p-value overlaps (both above 5% cutoff), (iii) light gray dots for p-value discords with its match (high informatic similarity), and (iv) black dots for p-value discords with no its match. see text for details. assignments for m. different clustering algorithms might settle on different choices for these cluster solutions, which in turn could lead to very similar or very different test outcomes under our clustered-t strategy. when the pathway's cluster pattern is strong and a single optimal solution stands out, any differences should be minimal. in cases where the pattern is more ambivalent, however, choosing a different cluster algorithm may affect the nature of the final outcome."
"allocator is an integrative data analysis system, so users can solve as many tasks in one system (with one interface) as possible, in order to generate datasets that can be used for statistical analyses. it covers the entire workflow of data annotation, beginning from uploading raw chromatogram data, to peak detection, to spectra deconvolution, to compound identification, and finally to data exploration and annotation (see figure 1) . the core feature is the new processing pipeline for spectra deconvolution 'allocatorsd'. it is optionally capable of dealing with data derived from 13 c-labeling experiments, and the use of this information to detect even large uncommon losses. allocatorsd will be described in very detail in this manuscript. the results of the pipeline can then be used to identify each small molecule via different (semi-) automated or manual ways. all generated data can be explored and curated with interactive and dynamic visualizations. the compound identification methods, data exploration and manual annotation features can also be applied to results achieved with the camera tool [cit], which is integrated into the allocator web platform as an alternative approach. to ensure long-term use of manual metabolite annotation efforts, allocator provides the possibility to generate and make use of user-and protocol-specific reference databases."
"to determine rough estimates of both parameters for each g th gene, we returned to the tcga brca database and retrieved rna-seq read counts on normal breast tissue samples from the same 110 independent patients. for each gene in the specified pathways from table 2, we found corresponding estimates g and g via the method of moments. (if g indicated under-dispersion compared to a poisson random variable, we conservatively deemed the gene's read count to be poisson distributed with mean g .) these values defined the nb distribution for baseline responses of gene g in our simulations."
"in this section, the imaging results of the forward-looking fda-sar are given to validate the proposed imaging algorithm. in the simulation, the fda-sar system parameters and the position information of targets are listed in tables 1 and 2, respectively. the center of region 1 is taken as the scene center. figure 6 shows the filter response of the transmit beamformer designed for first region. as can be seen, the range ambiguous echoes can be separated in the spatial frequency. [cit], 20, 1169 figure 6 . filter response of the transmit beamformer designed for the first region."
"while the method presents ample potential, there are, of course, some caveats. one potential drawback is the availability of external, context-relevant, gene expression data. in the cancer literature, there are a growing number of openly available patient data sets and cell lines that could be recruited for use. other biomedical contexts may prove less accommodating, however. furthermore, we implicitly assume that co-expression of genes within the same tissue is stable across the external database, but further study is warranted to test the validity of this supposition. depending on the estimated, external, inter-gene correlations and the chosen gene set ontology, some pathways may exhibit resistance to accurate and effective clustering. this could result in small cluster numbers or large, positive, inter-cluster correlation, and potentially lower power under our cluster-based strategy."
"where ξ is the complex amplitude of the target echo; t r and t a denote the fast time and the slow time, respectively; f d is the doppler frequency; w a (t a ) denotes the azimuth envelope; η mn0 denotes the instant when the target is closest to the equivalent phase center (epc); τ mn (t a ) is the time delay of the signal transmitted by the nth transmit element and received by the mth receive element, which can be expressed as"
"since two symmetrical targets have the same magnitude of doppler frequency, there is the problem of left and right ambiguity for the forward-looking fda-sar system. in this section, an approach to distinguishing left-right ambiguity is proposed. suppose the range ambiguity echoes have been separated by the method given in section 3."
"14 applied over many different, preselected pathways, corrections for multiple false discoveries to these simple per-pathway inferences may also be included. 15 additional complexities occur with matched-pair rna-seq n-of-1 data, however. it is important to recognize that expressions across genes within a given pathway are likely to be both heterogeneous and, more critically, correlated. when testing for differences between the two conditions, inter-gene heterogeneity may not detrimentally affect the null distribution of the test statistic; however, experience with cohort-based gene set testing has shown that inter-gene correlation is non-trivial and should be accounted for in order to maintain the nominal operating characteristics of any inferential procedure. 16, 17 essentially, both the naı¨ve t-test and the signed-rank test assume independence among the pathway's gene expressions and as we will see below, in the presence of non-zero inter-gene correlation, they generally fail to contain the test's false-positive error rate at nominal levels."
"in this paper, an approach to range ambiguity and left-right ambiguity resolution for hsv forward-looking fda-sar is proposed. utilizing the proposed algorithm, high-resolution and wideswath (hrws) imaging can be achieved. compared with the conventional forward-looking sar"
"modern precision medicine increasingly relies on molecular data analytics, where standard approaches accumulate large amounts of molecular data from multiple patients. 1, 2 within this context, an experiential paradigm that has garnered recent attention is development of interpretable single-subject signals to truly focus on the individual patient. 3, 4 a novel approach to generating single-subject information is known as the n-of-1 trial, 5 where the individual patient is the sole source/unit of observation and any statistical descriptions and inferences are intended to relate only to that patient."
"previously, c-glutamyl-l-glutamine, c-glutamyl-l-valine, c-glutamyl-l-leucine and c-glutamyl-l-glutamate have been isolated from c. glutamicum fermentation broths, but the physiological role of these metabolites stayed elusive [cit] . although the presence of + and absence of [m+h-h2o] + ions in the spectra of the before mentioned peptides were an indication for c-linkages [cit], it was not possible to readily distinguish between dipeptides with a-or c-linkages. this is the first report on the synthesis of (c-)glutamyl-methionine by c. glutamicum, but amongst other c-glutamyl dipeptides it was detected earlier for example in samples of synecococcus sp. pcc 7002 by an untargeted metabolomics approach [cit] . it will be interesting to investigate their functional role in prokaryotic organisms, but further interpretation exceeds the scope of this article."
"of interest here is whether or not the paired observations in this pathway exhibit significant differential expression between normal and tumor tissues in this single patient. it is of further interest to also repeat the calculations across the larger collection of all pathways under study. (a total of 3411 pathways are available with this patient's data; see section 4.1. as noted above, we define the pathways using the go 9 knowledgebase via go-bp gene sets.) by isolating significant, differentially expressed pathways (deps) for this patient, the n-of-1-pathways approach offers a glimpse into the individual, dysregulated, cellular mechanisms between her tumor and normal breast tissue control."
"with the above limitations in mind, one might consider a number of possible extensions and variations to our cluster-based approach. we propose what is, in effect, a self-contained gene set test, 41 meaning that only genes in the pathway impact the score. a test that employs inter-gene correlations across the entire transcriptome could present a competitive alternative approach when many pathways fail to cluster under a chosen ontology. indeed, correlation-based clustering is an active area of research across many disciplines. 42 and, as noted above, future nof-1 data may present more than two conditions beyond our case-vs.-baseline pairing. the statistical details for identifying deps under multiple conditions would make for an interesting extension of our cluster-based approach."
allocator web platform for lc-esi-ms data analysis tool allows filtering these by a retention time window and a minimum intensity. any orphan peak can be selected as a basis to generate a new pseudo spectrum.
"a subset of peaks that are associated to a large pseudo spectrum can sometimes be added to an additional pseudo spectrum for another putative mass. this tends to happen when multiple consecutive small neutral losses occur. this shall be demonstrated again using the pseudo spectrum of glutamate (147.0532 da, respectively. the putative neutral monoisotopic mass of this second pseudo spectrum (129.04 da) matched for example 4-oxoproline in the kegg database. as both pseudo spectra are formally correct when regarded separately and peak correlations can be very good for different coeluting compounds, this ambiguity cannot be solved reliably without manual revision. thus, it is one of the main goals of the manual editing process to eliminate multiple annotations of such peaks. for this purpose we used the allocator function claim peaks, which in this case deleted the mentioned peaks from all pseudo spectra except that of glutamate. this is an important advantage over editing annotations in a spread sheet, because it ensures data integrity and the concise visualizations help keeping the overview."
"our goal is not to present a comprehensive investigation for further medical treatment(s) with this particular patient, rather, simply to exhibit how our cluster-based statistics are calculated in practice and to suggest possible avenues for their further use. to start, gene expressions on all available genes for the patient were normalized to account for library size (via transcripts per million) and then transformed via log 2 ð y g þ 1þ for each g th gene's expression count y g in both the baseline (normal tissue) and case (cancerous tissue) groups. as above, we defined our pathways from a standard gene set knowledgebase, chosen here to be the go-bp ontology. 9 recall that example data from one such pathway, go:0003071 (renal system process involved in regulation of systemic arterial blood pressure), were presented in table 1 . to make the process manageable and results more interpretable, we retained the pathways that had no fewer than 15 and no more than 500 genes annotated and provided a measured rna-seq data set. this produced 3411 different go-bp pathways."
"mass decomposition for 147.052 da was performed with all available filters activated and finally resulted in only the single formula c 5 h 9 no 4 . this is indeed the sum formula of glutamate, but also of all other nine metabolites listed in table s5 . a pseudo fragment spectrum was queried against the massbank database. the best retrieved hit was a spectrum of glutamic acid (glutamic acid; lc-esi-qtof; ms2; ce:15 ev; [m+h]+; massbank: pb000462) with a score value of above 0.98. in fact, the list of fragment peaks in the pseudo spectrum was identical to that of the ms/ms spectrum of glutamic acid."
"to examine this aspect, we considered an alternative clustering algorithm from one of the more important classes of affinity-based clustering strategies, spectral clustering, see von luxburg 45 for an instructive introduction. we applied von luxburg 's recommended settings to the spectral clustering (sc) implementation from the kknn package 46 in r. then, we simply replaced our calls to the apcluster package, and associated support coding, with calls to the kknn specclust function in our r code. we also chose to employ eigengap heuristics specific for the sc approach to select the number of clusters, m, in place of the pseudo-f statistic applied with ap's q input preference parameter. these heuristics find m as that number of clusters with a maximal difference (or 'gap') between adjoining-ordered eigenvalues of the data's graph laplacian, see von luxburg 45 for complete details. lastly, we restricted the algorithm to require, as above, n j ! 4 8j."
"lastly, we studied the effects of inter-gene dependence by manipulating the inter-gene correlations. this was quantified by a within-pathway correlation matrix r consisting of all pairwise correlations r ih between genes g i and g h in the given pathway. we used the externally determined correlations calculated from the tcga database (summarized in table 2 ) and constructed a g â g r matrix for the pathway's g genes using the corresponding r ih values. for each pathway, we considered three forms for the correlation structure:"
"correlated rna-seq gene expressions were then simulated under our negative binomial model using the correlations in r. the negative binomial read counts were simulated via r's rnbinom pseudo-random variate generator. we employed copulas 32, 33 in order to 'tie' together the marginal negative binomial distributions and form a multivariate construction. we found that a few simulation replicates could result in poor performance in the implementation 33 if generated serially, so we conducted all the simulations at once. we set the number of replicate, simulated, n-of-1 [cit], and first generated an entire non-deg simulated data collection of size g â 2000. next, an entire deg simulated data collection of size g â 2000 was generated with every marginal gene mean multiplied by the current fold change . the two data collections were merged based on the selected deg genes (see above) to form the complete simulated data set for the particular configuration with the appropriate proportion of degs, . this ensured that the desired correlation structures were properly represented."
"in general, these results suggest that the ap-based clustered-t statistic in equation (7) exhibits good falsepositive error control and reasonable power, at least under the settings chosen for these simulations."
"another tool accessible from the molecule list view allows browsing 'orphan peaks', i.e. peaks that have not been associated to any pseudo spectrum yet. the figure 3 . screenshots from the allocator web platform user interface; bottom left: the dashboard that servers as a starting point after log-in; top: the list of molecules or pseudo spectra that were detected in a certain chromatogram; bottom right: a pseudo spectrum view that provides a table of all adducts and fragments, as well as a spectral view of all contributing peaks. doi:10.1371/journal.pone.0113909.g003"
"to study the effect of clustering-algorithm choice in practice, we returned to the tnbc data from section 4 and applied spectral clustering as described above in place of ap to define the gene clusters. after application of the clustered-t statistic from equation (7), this produced an alterative ordered list of p-values and corresponding pathways, the results from which indicated reasonably strong similarities between the two clustering algorithms. for example, among the 3351 pathways that produced p-values under both methods-see below-the topidentified dysregulated pathway using ap clustering, positive regulation of cell adhesion (go:0045785), was also top ranked under sc. at a broader level, 2870 of the 3351 pathways indicated qualitative agreement at the traditional 5% level: 361 give both p ap and p sc below 0.05, while 2509 show p ap and p sc above 0.05."
"in the first stage, we identify biological context-relevant mrna expression data from some external knowledgebase(s). many different possibilities exist and users can choose any external database that suits their needs. some caution may be necessary; however, many sources provide a wide selection of gene-pair correlations, derived from multiple inputs. when these are collected to define inter-gene correlations for the g genes within a specific pathway, the consequent correlation matrix need not be positive definite, as the original observations likely did not come from a single, coherent sample. this can lead to computational (and interpretational) problems. to avoid this concern, we recommend use where possible of knowledgebases that contain multiple gene-expression samples from independent individuals. we give an example of such in section 4.2."
"where f 0 denotes the carrier frequency of the reference antenna. usually, ∆ f is far smaller than f 0 and the system bandwidth b."
"it is also natural when considering incorporation of external information to consider some form of bayesian methodology. 43 the nature and form of any such model would be highly complex, and it is unclear how much external information or subjective input would be required to implement any eventual construct. still, the richness of the bayesian paradigm offers many avenues for study and it may provide a useful arena for development with n-of-1, precision-medicine inferences."
where e 1 is the first column of the two-dimensional identity matrix; e 2 is the second column of the two-dimensional identity matrix; w 1 and w 2 can be given by
"applying the obtained receive weight 1 w and 2 w to the received signal after bp, we can separate the signal from the pixels (, ) ρ θ and (, ) ρ θ −, which can be expressed as"
"all chromatograms were uploaded to allocator and organized in a single experiment. peak detection was performed using xcms and resulted in the detection of approximately 1,400-1,500 peaks for each chromatogram (for xcms parameter settings see table s2 ). subsequently, the allocatorsd algorithm was started to associate isotopologues and to generate pseudo spectra based on xcms peak tables (for allocatorsd parameter settings see tables s3 and s4) . the molecule list view was then used for manual revision of peak annotations. at first pseudo spectra with a high number of peaks and those containing 13 clabeled peaks were reviewed. in addition, substrates and intermediates of the larginine biosynthesis pathway were specifically searched for."
"hypersonic vehicles (hsv) have attracted much attention in recent years because of their military value. hsv, with a velocity of more than five times the speed of sound, generally fly in near space, which means an altitude between 20 km and 100 km [cit] . hsv radar (hsv-r) has been a research hotspot in the field of hsv. compared with traditional airborne radar, hsv-r has higher velocity and altitude, leading to range ambiguity and doppler ambiguity. additionally, its forward-looking mode also causes left-right ambiguity. the key to achieving high quality imaging lies in resolving ambiguity. on the one hand, doppler ambiguity can be avoided by selecting an appropriate pulse repetition frequency (prf). on the other hand, the range ambiguous echoes can be distinguished by introducing frequency diverse array (fda)."
"applying the transmit weight vector w p to the echo after compensation x l (k), we have the received signal of the pth range region as"
"with allocatorsd and camera two different tools for spectra deconvolution are provided. both use the xcms results as input and generate output that can be explored and processed manually using the visualizations and user interface of the allocator web platform (see section: data exploration). however, differing output results may be delivered for the same dataset. the most important difference between the two offered methods is the ability of allocatorsd to properly process peaks deriving from the addition of u-13 c-labeled internal standard. clearly, allocatorsd is the recommended deconvolution method for data containing this kind of information."
"the peak side lobe ratio (pslr) of targets and the integral side lobe ratio (islr) are listed in table 3 . the resolution of the images are listed in table 4 . it is obvious that target 1, target 2, target 3, target 4, target 5, and target 6 are all well focused. the computational time of the algorithm is o(n r n m (2 + n θ m)))(nr is the number of range bins, n m is the number of pulses, n θ is the number of azimuth bins, while m is the receive elements number). table 3 . imaging performance of targets. note: pslr, peak side lobe ratio; islr, integral side lobe ratio. figure 9a,b show the amplitude of the 150th range gate and the amplitude of the 164th azimuth gate, respectively. target 1 is in the 150th gate and the 164th azimuth gate of the first range region. thus, the actual energy of target 1 should be in the first range region. however, the energy of target 1 is distributed in the first and the second range regions due to range ambiguity. the blue line represents the amplitude of target 1 without range ambiguity. the red line and black line denote the amplitude of the first range region and the second range region after performing range ambiguity resolution, respectively. as can be seen from figure 9, there is no obvious difference between the red line and the blue line. furthermore, the peak of the black line is lower than that of the red line and the blue line, and the gap is greater than 50 db. this shows that the range ambiguity resolution proposed in this paper can effectively suppress the range ambiguous echoes."
"in order to extract the unambiguous echo of each range region from ambiguous echoes, we need to construct a series of weight vectors that can enhance the signal of the desired range region and suppress the ambiguous signal from other range regions. the constraint condition in designing this weight vector can be expressed as"
"in order to safe the manual annotation effort and to transfer it to all the other chromatograms in the experiment, the curated pseudo spectra with confirmed metabolite annotations were stored in a reference list using the tool create reference spectra. this reference list was later used to automatically detect, assemble and annotate similar pseudo spectra in all the other chromatograms of this experiment using the function apply reference list."
"the receive steering vector of fda is the same as that of the traditional multi-channel sar system. the difference is that the transmit steering vector of fda is range-dependent, which can be utilized to separate the range ambiguous echoes."
"the identification of truly novel compounds is not possible by massspectrometry alone, but requires complementary analytical techniques such as nmr. metabolite identification in the context of mass-spectrometry based metabolomics rather means assigning possible known molecular entities to all detected peaks or peaks of interest. using electrospray ionization, peaks can be observed representing so called pseudo-molecular ions. here, intact analytes build adducts with small inorganic ionic species. determining m/z-values with high accuracy allows the determination of a reasonable number of possible sum formulae for each adduct by mass decomposition. mass spectra created by lc-esi-ms pose unique challenges on interpretation. during analysis different adducts and fragments of the original metabolite are formed and thus can be found as mass signals. for a proper identification and quantitation of the original metabolite, these signals have to be associated and annotated. in case two given molecules m 1 and m 2 could not be separated by retention time in the chromatographic step they have to be separated in artificial spectra that contain only those peaks which originate from the same analyte m 1 or m 2, also referred to as pseudo spectra. such a pseudo spectrum might for example comprise the peaks of the hydrogen ion adduct and the ion fragments created through the losses of water or ammonia ( 4 ] + ). this is highly dependent on many technical parameters, for example mobile phase composition and ion optic settings."
"+ adduct features a 13 c peak in distance of 1561.003355 da, only sum formulae with fifteen carbon atoms will be presented. as a result, the list of mass decompositions will only contain sum formulae that pass all of the activated filters."
"to achieve optimal range ambiguity resolution performance, the system parameter design for the forward-looking fda-sar is discussed in this section. in order to analyze the suppression effectiveness of the echo from the undesired range region, a function is defined by"
"+ of the same m as the [m+h] + . 7 th step: in the last step of the procedure, it is checked whether the peaks that were assigned to a pseudo spectrum (all adducts and fragments) correlate well enough: if a peak's correlation to the primary peak of the pseudo spectrum is worse than a user-defined correlation threshold, it is removed from this pseudo spectrum."
"further study of patient tcga-a7-a0ce's, 80 deps may offer insights into disease pathogenesis, progression, and possible therapeutic targets. for example, the dysregulated molecular pathways identified in in table 3 are known to be associated with breast cancer progression, including cell adhesion, 35 signal transduction, 36 nf-b, 37 and toll-like receptors. 38, 39 the latter (toll-like receptor) has particularly promising immunotherapy potential. 40"
"where w 1 and w 2 represent the weight that can enhance the echo from the left and the right, respectively. here, r is the covariance matrix of the signal and can be written as"
"one of the most powerful data analysis packages for untargeted metabolomic profiling is xcms [cit], providing means for peak detection, retention-time alignment, data annotation, and statistics. to solve the before mentioned problem of mass spectral deconvolution, xcms interacts with the camera tool [cit] that assembles pseudo spectra of peaks with high retention time correlation and identifies isotopes, common adducts and losses. for the annotation of fragment peaks, the tool requires all potential losses to be predefined and thus does not cover more compound specific (uncommon) losses. however, identification of fragment peaks can provide structural information that might help to distinguish between isobaric compounds. xcms and camera are available through the web platform xcmsonline [cit], which allows conducting and exploring the fully automated processing, but does not provide any possibility to easily curate these results. another freely available framework for lc-ms data processing, visualization and analysis is mzmine 2 [cit] . this software also offers automated peak identification, including the detection of common adducts and matching of calculated neutral masses to chemical databases. most importantly, peaks representing fragments of analytes in the full scan ms data are detected by matching peaks to multistage ms spectra generated in the same run. however, automatic identification of fragments for one-dimensional ms data is not supported. with met-cofea another platform software was published recently [cit], combining novel mass trace based extracted-ion chromatogram (eic) extraction, continuous wavelet transform (cwt)-based peak detection, and compound-associated peak clustering and peak annotation algorithms."
"correct metabolite identification in lc-esi-ms datasets heavily relies on expert knowledge and cannot be done automatically per se. due to this, metabolite identification is a major bottleneck in untargeted metabolomics experiments. in addition, stable isotope labeling was reported to greatly facilitate this process."
"this part presents an approach to range ambiguity resolution. for hsv-r, we can avoid doppler ambiguity by choosing a higher prf. it is assumed that the echo is free of doppler ambiguity by selecting an appropriate prf."
"the external clustering proceeded in a manageable fashion. employing r without parallelization, the algorithm processed all pathways in slightly under 30 min on a macbook pro carrying a 2.5 ghz intel core i7 processor and 16 gb of 1600 mhz ddr3 ram. note that the external clustering need to be completed only once for a given ontology and biological context/target tissue."
"this part presents an approach to range ambiguity resolution. for hsv-r, we can avoid doppler ambiguity by choosing a higher prf. it is assumed that the echo is free of doppler ambiguity by selecting an appropriate prf."
"all data uploaded to allocator is organized into experiments. the creator of the experiment may easily grant and revoke access to other users, but at the same time stays owner of the submitted data. in contrast to typical web services, all raw data uploaded to allocator will be stored until single chromatograms or the entire experiment are deleted by one of the authorized users. downloading of raw data from the platform is not supported. as the web platform is designed in a stateless way, urls from the web browser address bar can be bookmarked. this can be used for example to inspect a spectrum later or at another working station, as well as to point colleagues towards a specific chromatogram or spectrum. customized lists of adducts and neutral losses are also protected by permission management and can be shared with other users. in the same way user generated reference spectra can be shared with other users or applied to subsequent experiments. all these features greatly support in-depth analyses of data, distributed collaboration on data, and knowledge transfer between experiments."
"the system contributes to the metabolomics software landscape by extending the bioinformatics coverage of analytical technologies. by supporting lc-esi-ms data and especially 13 c sil it complements the community of metabolomics online platforms, until now constituted by platforms like meltdb 2.0 [cit], xcmsonline [cit], and metaboanalyst [cit] ."
"by performing receive beamforming for all pixels, we can obtain the image without left-right ambiguity. the processing flow chart can be summarized as in figure 5 . after matched filtering, the signal is processed by range compensation and transmit beamforming to resolve the range ambiguous echoes into several unambiguous parts. then, by performing bp on the echoes without range ambiguity received by each channel, we will have the image with left and right ambiguity. next, the echoes after performing bp are processed by two receive beamformers, respectively. finally, we can obtain the unambiguous images of the two sides. to resolve the left and right ambiguity, we need to design two filters, so that one can enhance the echo signal of the left area and the other is able to enhance the echo of the right area. under this constraint, the optimal beamforming problem in the sense of minimum output power for the system can be expressed as"
"the imaging of the targets in the first and the second range regions, which are focused by bp, are shown in figure 8a,b, respectively. it is apparent that the proposed range ambiguity resolution approach has effectively resolved the range ambiguous echoes into two unambiguous parts. in figure 8a, it can be seen that the energy of the targets in the first range region is well focused, while that in the second range region is suppressed. in addition, the energy of target 1, target 2, and target 3 is distributed on both the left and right sides of the scene because of left-right ambiguity. in figure 8b, it can be observed that the energy from the second range region is focused by bp. additionally, the energy of target 4, target 5, and target 6 is distributed on both the left and right sides of the scene. by applying receive beamformers to the images after bp, we can resolve the left-right ambiguous images into two unambiguous parts. figure 8c,d show the imaging of the targets from the first range on the left and right sides of the scene, respectively. compared with figure 8a, it can be observed that the energy from target 1 is well focused, as shown in figure 8c, by applying the receive beamformer designed for the left side of the scene. we can observe that the targets on the right side of the scene from the first range region are well focused in figure 8d by applying the receive beamformer designed for the right side of the scene. figure 8e,f show the imaging of the targets from the second range on the left and right sides by applying receive beamforming, respectively. from figure 8c -f, it can be concluded that the proposed left-right ambiguity resolution approach has effectively extracted the targets in the desired side of the scene from left and right ambiguous echoes. the peak side lobe ratio (pslr) of targets and the integral side lobe ratio (islr) are listed in table 3 . the resolution of the images are listed in table 4 . it is obvious that target 1, target 2, target 3, target 4, target 5, and target 6 are all well focused. the computational time of the algorithm is ( (2 ))) the imaging of the targets in the first and the second range regions, which are focused by bp, are shown in figure 8a,b, respectively. it is apparent that the proposed range ambiguity resolution approach has effectively resolved the range ambiguous echoes into two unambiguous parts. in figure 8a, it can be seen that the energy of the targets in the first range region is well focused, while that in the second range region is suppressed. in addition, the energy of target 1, target 2, and target 3 is distributed on both the left and right sides of the scene because of left-right ambiguity. in figure 8b, it can be observed that the energy from the second range region is focused by bp. additionally, the energy of target 4, target 5, and target 6 is distributed on both the left and right sides of the scene. by applying receive beamformers to the images after bp, we can resolve the left-right ambiguous images into two unambiguous parts. figure 8c,d show the imaging of the targets from the first range on the left and right sides of the scene, respectively. compared with figure 8a, it can be observed that the energy from target 1 is well focused, as shown in figure 8c, by applying the receive beamformer designed for the left side of the scene. we can observe that the targets on the right side of the scene from the first range region are well focused in figure 8d by applying the receive beamformer designed for the right side of the scene. figure 8e,f show the imaging of the targets from the second range on the left and right sides by applying receive beamforming, respectively. from figure 8c -f, it can be concluded that the proposed left-right ambiguity resolution approach has effectively extracted the targets in the desired side of the scene from left and right ambiguous echoes."
"introducing allocator we provide now a powerful web platform for the semiautomatic annotation of peaks in lc-ms chromatograms and an interface that supports manual improvement of metabolite annotation with interactive tables and visualizations. at the core of this platform we implemented the allocatorsd pipeline for the automatic assembly of pseudo spectra. as a major improvement compared to previously existing software, this new algorithm is capable of dealing with 13 c-labeling experiments, enabling not only relative quantitation (mass isotopomer ratio analysis), but also automatic annotation of fragments resulting from large neutral losses. for the subsequent manual revision and correction of automatic annotation results, the user benefits from the integration of the platform with public metabolite and mass spectral databases (kegg [cit], chemspider [cit], massbank [cit] ) and new powerful tools, as for example the spectrum-aware mass decomposition. the possibility to create, share and query user-defined reference lists is an important feature that ensures transferability of once made annotation efforts to other chromatograms and experiments."
"collecting all this together produced four categorizations: (i) those where both p ap and p sc dropped below 0.05 (positive outcome ''overlap,'' with 361 pathways), (ii) those where both p ap and p sc exceeded 0.05 (negative overlap, with 2509 pathways), (iii) those where only one p-value dropped below 0.05 but the pathway exhibited information-theoretic similarity (''its match,'' with 396 pathways), and (iv) those where only one p-value dropped below 0.05 and there was no its match (''no match,'' with 85 pathways). clearly, the categorical similarity here is strong, with only 85/3351 (2.5%) pathways showing no form of overlap or match. moving to a different significance level changes the counts, but not the mismatch pattern. table 4 summarizes the results at the popular 10%, 5%, and 1% levels. figure 3 visualizes the relationship at the 5% level by plotting the 3351 paired p-values, as à logfpg, from both the algorithms. the figure distinguishes the four overlap groups via the plotting character: (i) dark gray dots for positive overlaps, (ii) white dots for negative overlaps, (iii) light gray dots for its matches, and (iv) black dots for no match. the pattern shows a clear progression along the 45 line of pure agreement, with a spread that widens somewhat but then stabilizes as à logfpg grows. as expected, the two overlap groups lie in diagonal quadrants along the 45 line to the lower left and upper right of the plot. notably, the black-dot ''no match'' group intermixes with the light-gray its-match group in the other quadrants of the display, with no otherwiseremarkable pattern. on balance we see that the two clustering algorithms exhibit reasonable outcome similarities when applied to this patient's data, but that their p-values can nonetheless vary somewhat."
"the allocator web platform comprises methods and tools for the semiautomated analysis of lc-esi-ms experiments, from the import of chromatographic raw data, to the export of lists of annotated and quantified compounds. users can create experiments and upload chromatograms (cdf-, mzxml-, mzdata -files) of both, positive-and negative-mode measurements. the web interface then guides the user through the customizable pre-processing steps, and finally displays the results in interactive and dynamic visualizations for data exploration and manual annotation. the general concept of all features is to achieve transparency of the data, i.e. to provide researchers with all information to after raw data upload, the first step is peak detection. second, either the new allocatorsd algorithm or camera can be applied for spectra deconvolution. next, tools are provided to identify and annotate detected compounds. then, data exploration is facilitated through dynamic and interactive visualizations that directly allow to confirm or to modify results of the automated processing steps. in the final step, results can be exported for external use. support decisions in peak annotations, rather than to plot irrevocable results of black box algorithms."
"from the molecule list view it is possible to create a reference list of all the confirmed or manually annotated compounds, which can then be used in another chromatogram to automatically annotate similar pseudo spectra. the similarity is measured via the dot-product for which a score threshold can be defined. in the pseudo spectrum view, the respective single spectrum can be added to (or matched against) a reference list. additionally, pseudo fragment spectra can be matched against the massbank ms 2 database [cit] ) and all its fragments, but exclude any further adducts and 13 c peaks."
"a mass decomposition for the putative mass m of any found pseudo spectrum is accessible from the pseudo spectrum view. on demand, this view suggests sum formulae that fit to m, along with a link to chemspider [cit] . as the number of theoretical sum formulae increases vastly with the size of m and with the accepted mass error e m, it is crucial to highly reduce the number of results, without discarding any true positives. therefore, a set of filters has been implemented. five of the seven golden rules [cit] can be activated (filter by element number, element probability, element ratio, senior rule, lewis rule), which check sum formulae for chemical plausibility -some of them by chemical rules, others heuristically. we also introduce a new filter that discards all sum formulae with less than 3.3 oxygen atoms per phosphorous atom, as such molecules are rarely (or never) found in the kegg compounds database. additionally, two spectrum-aware filters are available: the first considers neutral losses, for example a neutral loss of c 6 h 12 o 6 in the pseudo spectrum requires all sum formulae to contain at least six carbon, twelve hydrogen and six oxygen atoms. the second new filter that has been implemented considers the 13"
"in our application example we have demonstrated the applicability of the allocator web platform on complex biological samples and used it to annotate and relatively quantify intermediates of the l-arginine biosynthesis in two strains of c. glutamicum. analyzing the data specifically with regard to arginine biosynthesis, the last step of the pathway was identified as a bottleneck in larginine production with strain atcc 21831. in an untargeted manner, we have identified (c-)glutamyl-methionine as a previously unknown metabolite of c. glutamicum. by providing tools for widely automated identification, quantitation and exploration of lc-esi-ms data, allocator is well suited for the processing of lc-esi-ms datasets in the fields of systems biology and biotechnology. figure s1 . pseudo spectrum of l-glutamate. green:"
"where r n (t a ) is the slant range between the nth transmit element and the target, and r m (t a ) is the slant range between the mth element and the target. they can be expressed as"
"for the more practical case with non-zero inter-gene correlations, figure 1 (right two columns) shows that our cluster-based approach using equation (7) exhibits reasonable false-positive error performance. under the cluster-correlated (''block'') structure for which it is designed, its empirical error rates are essentially at or slightly below the nominal rate for all pathway sizes. interestingly, under the ''all'' correlation structure, our cluster-based test also shows good performance, either maintaining the nominal false-positive rate or becoming slightly conservative. we view this as a form of robustness to misspecified clustering, at least of the form we impose in our ''all'' correlation setting. the feature might be explained by the propensity of positively correlated genes to cluster together in the clustering algorithm (section 2.2), which would tend to drive inter-cluster correlations to be negative (depending on the assumed correlation distribution) rather than close to zero. this could then lead to over-estimation of the standard errors and produce slight drops in sensitivity in the test statistic."
"in the traditional phased array radar, it is assumed that the same carrier frequency is radiated by each array element. differing from a traditional phased array, the carrier frequency of fda radiated from each element is identical, with a frequency offset, as shown in figure 1 . the carrier frequency at the nth transmit antenna can be written as"
"step 3 results in a set of pseudo spectra that are generated for a set of masses m. these pseudo spectra consist of peaks that have been annotated as adducts or fragments for each m in this step, as well as their isotopologues as detected in step 2. c peak has to be n6 1.003355 da larger than the 12 c peak, where n is a natural positive number. n is further restricted to a range of possible carbon atom occurrences according to mass decomposition. according to this, the associated 13"
"to illustrate use of our cluster-based methodology, we return to the breast cancer example from section 1.2. recall therein that a female patient exhibiting triple negative breast cancer (tnbc) provided matched samples from both her healthy breast tissue and her cancerous breast tissue. (the complete data for this patient-identifiable only as ''tcga-a7-a0ce''-were quarried from the cancer genome atlas/tgca.)"
"our experience indicates that without some form of external information on the within-pathway correlation(s), it is difficult to develop a test of differential pathway expression for a single, n-of-1 sample. incorporating external gene expression data in testing procedures for pathway dysregulation in a single sample is rare, but not unexplored. 19 to address the issue, we appeal to the growing amount of transcriptomic information being uploaded to modern data storehouses/knowledgebases, and in particular aim to extract from these sources relevant biological-context input to aid in the test's construction. our approach has a bioinformatic flavor, and it attempts to turn the problem on its head: rather than allow the inter-gene correlations to stymie dep assessment, we use the external correlation information to aggregate non-negatively correlated genes (assumed strictly co-expressed, not anti-expressed) into correlated clusters within a pathway. we propose the following strategy: (a) identify an existing knowledgebase from which all g 2 inter-gene correlations can be determined/ calculated for the g genes within our target gene set (pathway), (b) apply an existing clustering algorithm to group the g genes into clusters based on the external inter-gene correlation data, then (c) incorporate the derived cluster information into a test for differential expression. the latter effort is facilitated by a useful cluster-adjusted t-test given by williams. 20 the following sub-sections provide details for each step."
"(i) cluster-correlated data that imposes the ap cluster assignments from section 2.2, such that pairs of genes within each cluster receive their corresponding correlation of r ih from the full set of pairwise correlations, and genes between clusters (within a pathway) receive a correlation of zero. this is intended to reproduce the clustered correlation structure assumed by our testing methodology. we refer to this as ''block'' correlation, since r takes on a block-diagonal form. (ii) cluster-correlated data where gene pairings receive their corresponding correlations of r ih irrespective of their imposed, within-pathway, cluster assignments. this is intended to study our method's robustness when the cluster assignments are not correctly specified. we refer to this as the ''all'' correlation case. (iii) an ''independence'' assumption among all genes, such that r equals the g â g identity matrix i. this studies the operating characteristics of our method under the supposition of no within-pathway correlation. note: g is the total number of genes for which the tcga brca data have measurements for each pathway; m is the number of clusters in each particular pathway determined by the correlation clustering algorithm (see text). the remaining columns contain the five-number summary and mean for the inter-gene correlations, r, observed in tcga brca normal tissue rna-seq data on an available sample of 110 patients."
"our work considers 4 types of mri sequences that are usually included in the basic protocol used in the assessment of cerebral tu-978-1-4799-5751-4/14/$31.00 ©2014 ieee fig. 1 . for each clinical case, the physicians generate one or more pairs (sequence, indicator) by manually labeling the roi of each indicator (when it is present according to the radiologist criterion) in that sequence where the indicator is more noticeable or is particularly well-defined. since our system is able to handle missing data, it is not required that all pairs (sequence, indicator) are available to make a decision about a clinical case. in fact, in practice, just a few subset of the potential pairs is available for each clinical case (in average, 5.1 in our dataset), and some of them are not even present in the whole dataset."
"once our clinical-based parameterization has been assessed, we aim to demonstrate how the proposed fusion scheme handles the problem of missing data (absent clinical tests) and models the uncertainty due to the scarcity of training data and the confidence on each expert."
"to this purpose, we have evaluated the performance of the global system at classifying clinical cases for a different number of available diagnostic tests (experts). in particular, given the whole dataset, we randomly pick n % of the diagnostic tests for each case (ensuring at least we have 1 diagnostic test per case) and run our system. in fig. 4 we show the results of this experiment, comparing the performance of our bayesian fusion scheme to other well known fusion schemes that can also handle missing data, namely: mean fusion, max fusion and max-voting fusion."
"second, we have proposed a bayesian fusion model that successfully handles the problem of missing data, takes into account the uncertainty of each expert, and provide useful information to the radiologists. that is especially important because we consider a realistic scenario where although an extended set of potential diagnostic tests could be performed, just a small set of them is actually available for each clinical case."
"in these days, social media have a huge impact on our life in many dimensions: socialization, business, politics, etc. it is the most popular digital communication tool to spread ideas and information. the present number of active social media accounts is around 2 billion out of all 7.2 billion people in the world [cit] . this novel source of data is very attractive for researchers and decision makers. in our research, we focused on twitter as a social media text source for two reasons. firstly, twitter is one of the most popular social media applications and it is the fastest growing one. referring to recent statistics, there are 316 million monthly active users who generate 500 million twitter messages (tweets) per day [cit] . twitter had 95% growth in active users and 35% [cit] . these are significant numbers, which means that we will gain a lot of benefit if we can extract the essence from this data type. secondly, the data structure of twitter and their support api are convenient for researchers to operate with. tweets are text files limited to 140 characters presented in json file format [cit] and a twitter search api can be used to retrieve tweets with a rate limited to 180 queries per 15-minute window [cit] and a seven-day search back [cit] ."
"in this section, we introduce the features that describe the appearance of the roi in each pair (sequence,indicator). in general, we consider two kinds of features: a) a set of classical general-purpose filter banks, e.g. laplacian of gaussian filters [cit], gabor filters [cit], glcm statistics( homogeneity, contrast, energy and correlation ) [cit], lbp/var histograms [cit], fractal dimension and lacunarity [cit], entropy, range and standard deviation filters; and b) a set of clinicalbased features, containing specifically tailored features that model perceptions and intuitions of radiologists and other clinical experts."
"magnetic resonance imaging (mri) is the diagnostic tool that currently offers the most sensitive non-invasive way for detection and diagnosis of brain neoplasms. according to the world health organization (who), brain tumors characterization is based on both their histological grade of malignancy (i-iv), and their type (primary or secondary) [cit] . while the classification of the tumor grade is often simple for an experienced radiologist, the differentiation between more aggressive primary tumors (grade iv) and metastases could be more challenging [cit] . in addition, this accurate diagnosis is essential because the management and prognosis of these type of tumors are very different [cit] ."
"at present, there are many weighting schemes for text mining: boolean weighting, term frequency (tf) weighting, tf-idf weighting, tfc weighting, ltc weighting, and entropy weighting [cit] . tf-idf is the most widely used technique to extract keywords from documents. it is composed of 2 steps: term frequency (tf) and inverse document frequency (idf). tf is computed from the number of times a word appears in a document, divided by the total number of words in that document. it can be defined as a counting function in eq. (1) [cit] ."
"the rest of this paper consists of two main parts: first, the main techniques that were used are discussed. second, our approach for effective social media text classification is explained."
"as can be seen in figure 3 and table 1, the use of clinical-based features produces a notable increment of the auc with respect to both the general-purpose and the reference sets for the same number of features selected. furthermore, the proportion of clinical-based features over the total number of features is very significant in almost every case."
"our dataset contains 97 clinical cases, 85 of them corresponding to the most frequent type of primary tumors (gliomas) and 12 to metastases. as previously mentioned, the average number of experts per patient is 5.1. the number of training data available for each expert model varies according to the sequences and indicators that the physicians decided to label for each clinical case. furthermore, some experts have been discarded since there are not enough metastases to train and evaluate the corresponding models. in particular, in our experiments we only consider those experts for which we have at least 10 primary tumors and 5 metastases, what leads to a set of 8 experts in total. we have conducted our experiments following a well-defined protocol. for each individual expert, we have designed the corresponding model using a 5-fold cross validation, which has been used to perform both the feature selection process and the selection of the parameters of the svm (c and γ, the width of our rbf kernel)."
"this paper is organized as follows. related research works are discussed in section 2. in section 3, we explain our approach and the main techniques that were used. the experimental results are shown in section 4. finally, in section 5, we draw the overall conclusions from the experiment, including a brief discussion of future work regarding the effectiveness of social media text classification."
"tf t, d is actually the total number of term t appearing in document d and fr x, t is a simple function defined as eq. (2):"
"for the initial state, we need to retrieve news articles from an online news source consisting of well-formed text. a web crawler, also known as a robot or a spider [cit], is the main module to get access to the data source. because most websites today are implemented with hypertext markup language (html), extensible markup language (xml) and cascading style sheet (css), the structure of the targeted website must be verified. then, the uniform resource locator (url), the news category and the news article part, which are needed as main parameters, have to be specified. afterwards, these parameters are applied through the xml path (xpath) query technique to retrieve the demanded data, i.e. the online news articles. the rapidminer software application [cit] was used as the main web crawler module."
"term frequency (tf) is one of the weighting techniques that can be used to identify the importance weight of words related to their corpus. however, we cannot directly merge tf values of words that appear in more than one corpus. each corpus contains a different total number of words with different weights. for example, the word \"aaa\" in corpus 1 has a tf value of 0.5, while the total number of words in corpus 1 is 100 words. the word \"aaa\" in corpus 2 has a tf value of 0.4, while the total number of words in corpus 2 is 1,000 words. we cannot find the tf value of the word \"aaa\" by adding their 2 tf values directly because we are considering two different corpus weights. the way to solve this problem is using word vector normalization (tf normalization) (5), (6) and tf-merging (7)."
"besides the final selected class, the previously described model allows us to obtain some further information of great interest, such as the uncertainty of its decision or the individual opinion of every expert. these kind of additional information may be useful to assist the physician who has to make the decision and even to contribute to the education process of inexperienced physicians. in particular, we propose to compute the following additional measures for each clinical case."
"computer-aided diagnosis systems (cad) have been developed that help physicians to make more accurate decisions. typically, these systems use a region of interest (roi) segmentation method, followed by a feature extraction/selection procedure that feeds machine learning algorithms carrying out the lesion classification concerning the description of the roi, most of these systems make use of traditional general-purpose texture-based image features: (laplacian of gaussian filters [cit], gabor filters [cit], glcm statistics [cit], lbp/var histograms [cit], etc.). these features model the texture of the roi by means of filter banks at several scales and orientations, and have been successfully used in many computer vision tasks [cit] . it has not been until the last years, when some recent approaches have introduced some specific clinical-based descriptors [cit] . these type of descriptors are directly derived from the intuitions of the radiologists, and therefore model their expert knowledge about the problem. however, they were still very few in comparison to the general-purpose features, and sometimes they lacked of strong connections with the pathological information that radiology experts use in their diagnostics."
"to overcome the aforementioned limitations, in this paper we present a system for brain tumor classification with two important contributions: 1) we propose a rich set of clinical-based features that model properties of various pathological indicators used by physicians in their diagnosis. as we will show in the experimental section, this new set of features improves the traditional system performance working with general-purpose features. and 2) we propose a novel bayesian fusion scheme that integrates the outputs of independent classifiers working on particular mri sequences and pathological indicators. bayesian fusion successfully handles the absence of information (missing clinical tests), and becomes a natural framework to model the uncertainty around the predictions what, as we will show in the paper, provides valuable information to physicians for both decision making and staff training purposes."
"in fig. 2 two examples of this learning process are shown, each one for a different expert. specifically, the distributions of each expert output given each class (primary tumor and metastasis) are shown along with the a priori distributions. as can be observed, when enough data is available the distributions clearly move apart from the a priori distributions. otherwise, they would stay close to the a priori distributions expressing a high level of uncertainty. moreover, the actual overlap between the distributions for primary tumor or metastasis for each expert clearly indicates its discrimination ability. in this particular case, expert 1 will likely discriminate much better than expert 3."
"we propose a semi-supervised learning technique with the utilization of a wellformed text source, as shown in figure 5 . this is the first step. an online news source is used as the main source to collect data from, which gives access to a well-formed document with appropriate grammar that is properly categorized by the publishers. the online news article was retrieved from the dailynews website, http://www.dailynews.co.th/ [cit], published by a popular newspaper in thailand. a total of 13,085 news articles were collected, as shown in table 1 . the news categories that will become class labels can be extracted automatically. the news articles can be extracted related to their category, after executing the following preprocessing steps: removing html tags, removing stop words, word stemming. then we used thai word segmentation and the tf-idf weighting technique to extract a bag of keywords from each news category. afterwards, we generated the initial wam (i-wam) from the set of extracted keywords. the top six of the terms with the highest tf-idf score were selected as the keywords for each category to search twitter to enhance the model. then, we used the keyword set from i-wam to collect related tweets through a twitter search api, as shown in figure 6 . the api allows collecting related social media text, where the search index has a searchback limit of 7 days. after collecting a heap of tweets, around twenty thousand, they were saved in text file format. subsequently, the same process as described before was used to extract keywords by using thai word segmentation and the tf-idf technique. additional terms were selected according to their tf-idf value. the result was a new set of keywords indicating specific categories that are potentially used in social media. in the implementation of m-wam, the term frequency merging (tf merging) technique is used, which is generated by updating i-wam. the tf of existing words in i-wam is recomputed with additional counting. the newly found words with their tf values are added into the table. as shown in figure 7, the m-wam process is repeated, iterating the procedure until a result is achieved in which precision, recall, f-measure, and accuracy are in steady state at nearly 100%. finally, m-wam is modified to fit social media text. this m-wam will be an effective model containing terms that can represent a text category and reflect social developments. as can be seen in figure 8, the evaluation of social media text classification is conducted manually. the training data set is used for building the model while the testing data set is searched from twitter randomly and used for evaluating the model. the retrieved tweets are evaluated by human judging. the testing data set evaluates all models, from i-wam to m-wam (n). finally, accuracy rate, precision, recall, and f-measure value are determined."
"the keywords that were extracted from the online news source showed a significant result, especially the keywords from the entertainment category (\"gubgib\"/ \"กุ  บกิ ๊ บ\": the name of a popular actress in thailand), it category (\"windows 10\"/ \"วิ นโดวส 10\"), and sports category (\"karate\"/ \"คาราเต \"). however, their tf values can identify their text categories when we consider the word vector cosine similarity. then, these keyword terms were used to search twitter through the twitter search api. around twenty thousand tweets were collected as our data source from to extract a new set of the twitter keywords. the m-wam1 was generated from these new specific keywords and merged with the existing keywords from i-wam (tf merging operation), well-formed text source keywords. the newly found words in the m-wam1 showed a significant result. for example, in table 3, \"refugee\"/\"ผู ้อพยพ\" (sample keyword from i-wam) led to finding a new keyword, \"tier3\"/ \"เที ยร์ 3\", which scoped down the word vectors for the foreign category. other category keywords also generated promising results."
"then, due to the limited size of the individual expert datasets, we have evaluated our classifiers using the leave-one-out (loo) approach. the advantage of loo is twofold: 1) it obtains more stable results since it maximizes the number of training samples for every test sample; and 2) it obtains results for every sample in the dataset, what allows us to train the fusion model over the whole set of clinical cases. finally, we have used the area under the curve (auc) to assess the performance of our approach."
"the main challenge is to analyze social media text. since tweets are short text messages, they look like colloquial text compared to written documents. the data stream contains a large amount of noisy and unstructured information, informal language, slang and missing words. this makes text classification in order to distinguish categories before extracting useful information very difficult. in our experiment we therefore applied a technique to extract keywords using term frequency-inverse document frequency (tf-idf) and word article matrix (wam) to expand the set of keywords reflecting the nature of the texts retrieved from twitter. we collected data and created word vectors from an online news source consisting of well-formed text, which was already categorized beforehand by publishers to extract keywords and classify information from twitter. semi-supervised machine learning can solve selfassigned classes labeling for topic classification problems. the computer can automatically extract categories from the news website for use as proper classlabels with a sense of human familiarity, such as economic, entertainment, foreign, information technology (it), politics, regional, sports, etc. finally, we get a productive set of keywords from the official site and twitter, which can be representative for text categories. new words, abbreviations and argot that never appear in well-formed documents are extracted from the tweet messages, which can then be used as the main keywords that reflect interesting topics in society at a certain moment in time."
"the remainder of this paper is organized as follows: section 2 introduces our model for brain tumor classification. section 3 focuses on the design of a novel set of clinical-based features and justifies each descriptor from the experts' knowledge. our bayesian fusion scheme is later discussed in section 4. section 5 assesses the performance of both the parameterization and global system, whereas section 5 draws our conclusions and states further work."
"wam is a significant data structure [cit] in the generic engine for transpose association (geta). it creates a large matrix of weighted relationships between documents and keywords in which the rows are indexed by names of documents (articles) and the columns are indexed by keywords from the documents. the keywords in the documents are counted to fill in the table as shown in figure 4 (a). the initial wam (i-wam) is generated by using the normalized tf value of each word. the i-wam with normalized tf values is shown in figure 4 the set of documents in a corpus is viewed as a set of vectors in a vector space. each term will have its own axis. using the cosine similarity technique [cit] we can find out the similarity between any two documents (8). lastly, we calculate the cosine similarity values and get the result of the example query as shown in figure 4(d) . as the weight of the word \"intelligence\" in the information technology (it) category is high, 0.95, the result of the operation shows that the query is more likely to be for a document about it, which produced the highest cosine similarity score at 0.768."
"after retrieving online news data by using the web crawler module and extracting a set of keywords, we selected the words with the highest tf-idf score and generated the initial-wam (i-wam), as shown in table 2 . we added a row to show the idf value of each keyword to identify their importance weight. the words \"financial budget\"/ \"งบการเงิ น\", \"gubgib\"/ \"กุ  บกิ ๊ บ\", \"refugee\"/ \"ผู  อพยพ\", \"windows 10\"/ \"วิ นโดวส 10\", \"politician\"/ \"นั กการเมื อง\", \"artificial rain\"/ \"ฝนหลวง\", \"karate\"/ \"คาราเต \" are examples of keywords with their tf value in each category (economic, entertainment, foreign, it, politics, regional and sports) respectively."
"-compactness and annularity: these are features designed to describe the whole enhancement and necrosis area. metastasis tends to have circular necrosis areas surrounded by annular enhancement, while primaries are more decentralized and snaky."
"in this paper, we have proposed a complete system for brain tumor classification from mri-based images. first, we have introduced some clinical-based features that intend to model radiologists' intuitions and, as shown in the experiments, notably improve the results, either increasing the classification performance or reducing the total number of features required for classification."
"the experiment in this study was conducted on social media text written in the thai language. word segmentation is a crucial factor in text mining. however, the thai language is written without spaces between words. therefore, a word segmentation module was used, applying the maximal matching algorithm to determine the word boundaries [cit] . the recent word list in the dictionary was updated before the research was conducted. consequently, the segmentation result was acceptable for determining the essential words for further processing in keyword identification."
"since they are a central point in the scope of this paper, we next provide a description of the clinical-based features, focusing on how they are derived from the experts' intuitions. hence, given the roi of an indicator in a sequence, we compute: -size relations: computed as areas, perimeters and their relations to those of other indicators. size of primary tumors tends to be bigger when compared to other associated pathological evidences ( edema, enhancement, etc )."
"-edge intensity: this feature describes the transition between the boundary of the lesion and the healthy tissue. primary tumors are mainly composed by neurons which are visually similar to the rest of the gray matter, but metastases are formed by cells from other organs, so the intensity gradient along the border tends to be larger."
"another common limitation of current cad systems for brain tumor type characterization is their lack of flexibility since they require exactly the same fixed set of clinical tests in all the cases (in most approaches, tumor area in t1 and t2). therefore, in case an additional information is available, it is filtered out by the system. in contrast, if any of the images belonging to the fixed input set is not available (it has been not possible for the radiologists to make the associated clinical test), traditional cad systems cannot make decisions about the case. however, in practice, many other mri sequences (fluid attenuation inversion recovery or flair, t1 after intravenous contrast agent administration) and some potential pathological indicators (enhancement, necrosis, edema, neovascularization, bleeding, etc.) are studied by physicians, and are frequently used to make their diagnosis."
idf is defined as the logarithm of the number of all documents in a collection divided by the number of documents in which the observed term appears in eq.
"the growth and information power of social media text are remarkable. keywords collected from social media can be a prediction tool of social developments. a holistic decision support system can be developed according to interesting topics collected from the dynamic social media environment, which is a factor of concern today and will be in the future. social media text classification using term frequency-inverse document frequency (tf-idf) weighting and word article matrix (wam) is very effective. text from social media can be categorized with a sense of human familiarity by utilizing online news categories that have already been indicated by the publishers. good results can be expected from the proper modified wam (m-wam) for social media text classification after updating it for 3 times, the suitable iteration number of m-wam modifications. this modified wam can be a suitable model for social media text classification and the set of keyword terms can be representative of interesting social topics during the time of monitoring. however, a good result also depends on the performance of the thai word segmentation module. alternative thai word segmentation programs, such as name entity recognition (ner), can generate proper word boundaries for conducting other processes, so keywords can be generated more accurately and the model's accuracy will be improved significantly. deep learning could also be a good choice for conducting experiments related to natural language processing and text mining."
"as can be easily noticed, the proposed bayesian fusion model consistently outperforms the rest of the approaches. it is worth noting how the performance of our fusion scheme is specially good when the number of diagnostic tests is very low. from our point of view, the rationale behind this fact is that for these values of n, the number of training samples for each expert is very low, what produces an important increment of the uncertainty of the individual svm-based classifiers. since our approach is the only one modeling this uncertainty using gaussian distributions and conjugate priors, the confidence on the individual expert decisions is taken into account."
"while the load-proportional speed adjustment scheme offers these benefits, it also has a drawback in the case of small output power. in that case, the system responds slower relative to external condition changes, such as a sudden load current increase. to address this problem, the frequency control loop in each converter has a dedicated fast voltage drop detector that monitors converter output voltage and triggers a drop detection signal when it goes below certain threshold. the drop detector requires periodic reset to detect output voltage changes and maintain a certain threshold level. hence, each converter contains two drop detectors for uninterrupted overlapping operation and detection: one detector always remains on while the other is being reset. by focusing only on triggering upon a fast single drop event without considering stability or continuous operation, the detector's response time can be boosted hundreds times faster (simulation) than the main feedback path, rendering the control loop fast enough for sudden current load changes. once the detection signal is triggered, the clock frequency is set to its maximum, quickly restoring the output voltage to safe levels. afterwards, feedback through the main path slowly lowers the clock frequency to support any sustained increase in load current. drop detector bias current is also adjusted to be load proportional."
"visits to the website were conducted in three groups of two, where the first two visits allowed participants to familiarize themselves with the interface, the third and fourth visits stabilized behavioral patterns and served as baseline performance, and the last two visits were used to evaluate the effects of tdcs. each visit comprised two phases, before and after choosing the song. this moment was marked by a click on the \"buy\" buttons, which was found next to the title of each item. the time between the beginning of the visit and the decision was computed using timing of clicks and url changes in the tobii data files. as it coincides with the final decision-making process, analysis of eeg and eye-tracking data was performed in the 30-second period preceding the purchase decision marked by pressing the \"buy\" buttons."
"when it is applied over the dlpfc, bilateral tdcs (where one electrode is placed over the left dlpfc and the other over the right dlpfc) was found to modulate behavioral output of cognitive functions such as risk taking [cit], craving [cit], decision making [cit], emotion processing [cit], emotional regulation [cit], mental flexibility [cit], language comprehension [cit], learning [cit], verbal performance [cit], attention [cit] and working memory [cit] ."
"while post-hoc tests comparing the three post-stimulation groups showed increased decision time for the left cathodal/right anodal group compared to the placebo group, this effect disappeared when comparing percentage of change between the pre-and post-stimulation visits, suggesting that non-significant but slight pre-existing differences before stimulation may have been exacerbated following stimulation."
"another limitation of our results is the sensitivity of psychometric scales to manipulations in tdcs. tdcs effects are usually of a small magnitude [cit] and a 7-point scale was probably not sensitive enough to provide robust effects. furthermore, a recent meta-analysis showed that changes elicited by tdcs protocols usually affect reaction times in healthy participants and accuracy in clinical populations [cit] . in the present study, significant changes were observed in decision time, which shows that the stimulation protocol had an effect on website use, which was probably too small to be detected by the webqual questionnaire. testing participants with very low proficiency in website use could provide an opportunity to assess this hypothesis."
"for the purpose of this paper, we use the acl anthology network [cit] as our underlying corpus, but note that it can be easily exchanged. the application is not limited to this particular data source. the aan is based on papers in acl 10 anthology -a digital archive of conference and journal papers about natural language processing and computational linguistics -and provides citation and collaboration information. [cit], the aan in- we further enriched the data with more detailed author information by heuristic wikidata and google scholar entity page look-ups that match an author's name. note that not every author has a wikidata or google scholar entry, and some authors have multiple entries. in total we count approximately 9k authors with a matching google scholar entity, and 14k authors with matching wikidata entities, of which 1.5k authors can be linked to exactly one wikidata entity. our heuristic does not distinguish between wikidata entities and shows them all, whereas only the first google scholar entity is selected."
"at this point, the 1:3 dickson upconverter turns on and generates the higher voltages, 2.4v and 3.3v. the 2.4v is then used to power an internal 1.2v voltage reference and an ldo to generate a clean reference voltage v ref for more accurate regulation of the 1.2v supply voltage. after v ref becomes available, feedforward control acts to set the binary conversion ratio by directly computing the desired conversion ratio using an 8b adc. after the adc has measured the battery voltage, the conversion ratio is calculated in digital logic to be the measured ratio v ref / v bat plus an offset value. this allows for an optimal voltage drop to balance conduction and switching losses, maximizing efficiency. after the system is fully turned on, the binary converter ratio is periodically adjusted while supplying output voltages, allowing for self-adaptation in the face of slow input voltage drift arising from battery discharge or temperature changes, both of which frequently occur in wireless iot systems. in addition to conversion ratio adjustment, dc-dc converters in iot systems should be able to self-adapt to widely varying output load conditions to maintain good efficiency. fig. 8.5.3 shows the frequency control loop of the binary converter, consisting of a main feedback path and a fast voltage drop detection path. for initial startup, the main path compares the first stage output v1 with the divided input voltage, maintaining a proper amount of voltage drop δ through the first stage for optimum conversion efficiency. after the system is fully turned on, the binary converter output v1p2 is compared to v ref to be the same level as v ref ."
"the main contributions of this tool are to provide different expert search methods, detailed expert profiles and evidence features, which support a user's decision making process. the application's main page is shown in figure 1a, which contains a simple search field for query input and a list of the retrieved ranked list of experts. the ranked expert list consists of condensed expert profiles showing the name, an image, a description, statistics and keywords representing expertise areas. the result is obtained by the particular method that is selected beforehand from a range of different expert finding methods."
"we presented the lt expertfinder, a user-friendly tool for expert search, expert profiling, and most of all it enables the qualitative comparison of different ranking approaches and provides evidence for the ranking process. we implemented several ranking methods that can be easily extended with more methods. also, it provides detailed expert profiles, which are linked to wikidata and google scholar. additionally, an explorable graph view is provided, which helps for further analysis of the results. this combination of features in a single tool is, to the best of our knowledge, still unexplored and helpful for the community for further development and evaluation of expert finding methods. for the future, we plan to expand our corpus using automatic crawling methods of scientific papers, which are analyzed and indexed on a daily basis. [cit] with the help of their pdf extraction tool ocr++ [cit], which we also intend to use. further, we plan to utilize the lt expertfinder to develop methods for finding emerging experts in a field. we release the lt expertfinder as freely available, open source application, under a permissive license. 13, 14, 15, 16 a short demonstration video is also available 17 ."
"this study is the first, to our knowledge, to use tdcs to investigate the role of a specific brain structure in the interaction between a user and an it artifact. [cit], who found that dlpfc activity was associated with peou, this area was chosen as a target for tdcs to determine how modulating its excitability would affect the user in terms of technology acceptance and interaction behavior with a website. the present results do not support the hypothesis of a modulating effect of tdcs on peou, but provide partial support for the second hypothesis regarding the effects of tdcs on behavioral and neurophysiological aspects of the interaction with the website."
"the lt expertfinder features detailed expert profiles and various expert finding methods in an extendible framework. moreover, it provides userfriendly evaluation methods: a view of the querydependent citation graph, an evaluation view combining different results and provenances in form of relevant documents and related statistics."
"given this ability to maintain a constant output voltage level, the binary converter offers near-optimum efficiency across load conditions as the conversion ratio is already configured for a proper voltage drop amount for optimum efficiency (via the feedforward ratio control path). the 1:3 dickson upconverter and 2:1 downconverter also have similar frequency control loops for their own oscillators, allowing each of the 3 converters to independently adapt to different load currents at their corresponding output voltages."
"that are relevant to the query and written by the candidate expert including their document relevance score calculated by the respective method, and several query dependent statistics such as hindex, number of citations, etc."
"pu and peou were measured using the webqual scale, assessing the following website dimensions: 1) information quality, 2) interaction, 3) trust, 4) response time, 5) ease of understanding, 6) intuitive operations, 7) visual appeal, 8) innovativeness, and 9) emotional appeal. the dimensions 1 to 4 are used to estimate pu, and the dimensions 5 and 6 are part of the peou construct. the dimensions 7 to 9 are linked to the entertainment factor of the website."
"neurophysiological indicators of cognitive effort. a decrease in beta power at the f4 electrode was found in both active stimulation groups compared to sham, which suggests that tdcs had a significant effect on brain activity, which could be related to the behavioral changes found after the two active stimulation conditions, irrespective of polarity. the present data are in line with a previous study showing increased beta power following tdcs over the dlpfc. however, these effects were reported to occur during, but not after, stimulation [cit] . furthermore, it has been shown that bilateral stimulation of the dlpfc decreases frequencies above 15hz (which include the beta band) [cit] . the beta band has mostly been associated with cognitive effort, working memory and visual attention [cit] which could underlie some of the behavioral effects reported here. taken together, the present data suggest that dlpfc activity may underlie the decision-making processes and cognitive effort associated with a goal-oriented interaction with a is artefact. additional studies are needed to disentangle the contribution of cognitive functions associated with the dlpfc to interaction with a website."
"(b) evidence view: this view is opened from the evaluation view (cf. figure 2a) . it shows the author, which is linked to the author profile, several query dependent statistics and, and relevant documents (which open via a click)."
"the primary objective of this proof-of-concept study was to determine whether tdcs can be used to assess the contribution of specific brain areas to constructs used in research fields such as information systems and human-computer interaction. to this end, tdcs was applied over bilateral dlpfc to verify the existence of a causal relationship between dlpfc activity and peou of a commercial web site. [cit] fmri study, a bilateral stimulation protocol has the advantage of stimulating both dlpfc simultaneously with different polarities. this approach may thus maximize potential effects by interfering with both targets. by targeting and modulating dlpfc activity, we sought to determine whether this area plays a significant role in human interactions with user interfaces and whether tdcs can modify their subjective perception."
"the power management system chip was fabricated in 0.18μm cmos and a die photo is shown in fig. 8 .5.7. the system stably delivers 1.2v, 3.3v, and 0.6v output voltages from an input voltage ranging from 0.9v to 4v (fig. 8.5.4) ."
"thus, systems, such as the arnetminer 5 tool [cit], aim at modeling entire academic social networks by automatically extracting researcher profiles from the web. moreover, arnetminer models topical aspects of papers, au-thors, and publication venues. the careermap 6 [cit], which is now a component of arnetminer, visualizes a scholar's career trajectory, which is extracted from arnetminer's publication database. the cl scholar 7 system [cit] mines textual and network information for knowledge graph construction and question answering using natural language or keywords. csseer 8 [cit] ) is a keyphrasebased recommendation system for expert finding based on the citeseerx 9 digital library and wikipedia. it extracts keyphrases from the title and abstract of documents in citeseerx and utilizes this information to infer the author's expertise. the expert2bólè system [cit] features generic expert finding as well as bólè search, which aims at identifying the top supervisors in a given field. the authors argue that generic expert finding methods are insufficient for finding specific experts for different purposes. in their application, bólè search, it is for example more important to find persons who are able to judge and nuture other experts than to assess their own expertise. hence, generic expert finding methods cannot be applied to this problem."
"we further improve rp by introducing additional edges and edge weights. we include document citations, co-authorship relations and various weighting schemes for every edge type. document citations are weighted by recency since we argue that a random user will most likely decide to read the most recent paper. co-authorship relations are weighted by the number of total co-authorships, i.e. all outgoing edges to other author nodes. authorships are weighted by a combination of the local and and the global h-index, where the local h-index refers to the h-index that is computed on the current result set of documents, and the global h-index refers to the to corpus wide h-index. as the h-index represents both the number of publications as well as the number of citations per publication, it is a suitable choice for determining the query-independent relevance of an author. finally, the expert ranking is obtained by an infinite random walk through the weighted expertise graph. the main difference to rp is, that this method's infinite random walk is applied with respect to the calculated weightings of the expertise graph."
"electroencephalogram (eeg), eye-tracking data, and mouse/keyboard actions were simultaneously recorded on two different computers (one for eeg and the other for eye-tracking and user-generated actions) and were synchronized using a noldus synchbox (wageningen, netherlands), with a final delay shorter than 10 milliseconds between datasets. during tdcs stimulation, all recordings were stopped because of electrical current interference and participants were told to relax during stimulation. data collection was resumed after stimulation and participants completed the last two visits and questionnaires. a short debriefing procedure took place after the experiment."
"human expertise is one of the noteworthy resources in the world. however, true experts may be rare and their expertise difficult to quantify due to multiple continuously changing factors. the goal of expert finding is to rank people regarding their knowledge about a certain topic, which is a challenging, yet rewarding task with many real-world applications. just to name a few, some applications might be: companies may require highly trained specialists whose consultancies can be requested for specific problems [cit], conference organizers may need to assign submissions to reviewers which best match their expertise [cit], recruiters may search for talented employees and emerging experts in a particular field [cit] ."
"transcranial direct current stimulation (tdcs; [cit] ), a non-invasive brain stimulation (nibs) technique, offers the possibility of establishing a causal relationship between brain and behavior. tdcs works on the premise that a weak constant current applied to the surface of the head through two surface electrodes of different polarity can modulate the resting membrane potential of the neurons it reaches transcranially. in turn, sustained modulation of membrane potentials can increase or reduce cortical excitability in surface brain areas, resulting in modified behavioral output. in the classical way of performing tdcs stimulation on a human participant, a secure and controlled 8 volts current source is equipped with two wires and electrodes. the electrodes are fitted to saline soaked sponges, generally ranging from 4 to 6 square inches. from that point, the current flows from the anode to the cathode through the different tissues in between. a low amperage, usually below 2 ma, is applied and continuously monitored by the tdcs device to insure constant and safe stimulation (see [cit] for detailed explanations of the method."
"factors that can explain this response variability can be described in terms of stimulation parameters and individual differences [cit] . as a result, a different stimulation protocol might have yielded significant results on peou, but since parameter space of tdcs is not well understood, it is difficult to make specific recommendations for such a protocol. nonetheless, many behavioral indicators were significantly affected by the stimulation protocol in the present study, which suggest that tdcs has the potential to influence processes involved in a complex task such as interacting with a website in an online shopping context. the eeg results are of particular relevance in this context, showing a physiological effect of the stimulation on a neurophysiological marker. this result substantiates the notion that the stimulation protocol affected brain activity, but that this effect was either not sufficient for our hypothesis to be supported or unrelated to our initial hypothesis."
"shared tasks, such as the enterprise track of trec [cit] resulted in many automatic methods for predefined topics. those systems can be grouped into four major categories: a) generative models [cit], b) voting models [cit], c) graph-based models [cit], and d) discriminative models [cit] highlights the importance of expert profiling mainly because the results should provide more information than simply a ranked list of person names. [cit] emphasizes the importance of the social network, i.e. colleagues and collaborators contribute greatly to the value of an expert."
"based on the a priori hypothesis of a direct link between dlpfc and peou, as evidenced by fmri [cit], it is first hypothesized that bilateral tdcs over dlpfc will affect peou of a commercial website, and that it should not affect pu given the non-overlapping neural correlates that were previously identified. given the reported effects of dlpfc tdcs on a wide variety of cognitive domains [cit] and on eeg activity [cit], it is also hypothesized that behaviors reflecting other cognitive and affective processes will be affected by stimulation. such behaviors would be those directly involved in the completion of the goal of the task, namely time before purchase, fixation on the buy buttons and eeg activity in the seconds prior to the choice. directional hypothesis on such processes are hard to make given the sometimes contradicting or inconclusive results of meta-analysis of tdcs effects over dlpfc [cit] . given the absence of specific research combining decision-making, tdcs over the dlpfc and eeg measures, analysis of the eeg data is exploratory."
"the decrease in the response time sub-scale suggests that participants find the website to be less responsive and fast as the study goes on. one of the possible explanations could be related to attentional understanding of time perception. to have an accurate perception of time at this scale, which requires attention and involves various loops between the basal ganglia, thalamus and related cortical structures, such that when attention is on other stimuli or processes, time can seem to go by faster [cit] . when participants are still learning to interact with the interface, there are less resources to attribute to these neural signals, hence a perception of faster response of the website is to be expected."
"in this paper, we address this issue and present the lt expertfinder web application, which is equipped with several query-based expert finding methods and can be easily extended. a detailed expert profile helps users to eventually select one expert in favor of another. methodological evidence, in form of relevant documents and various statistics, as well as a view of the query-dependent citation graph, is provided. finally, we added an evaluation component that allows the qualitative comparison between different rankings. to the best of our knowledge, this is the first tool that provides evidences and a direct comparison to multiple expert rankings. we apply our system to the acl anthology network 4 in order to find experts on various computational linguistics topics."
"all participants were right handed and were between 18 and 35 years old. further information on the samples can be found in table 1 . upon contact by the research team, potential participants received a copy of the consent form, which they signed before the start of the experimental session each participant received a 30$ amazon gift certificate as compensation for participation and could also keep the products they had bought during the experiment (which ensured that participants took the task seriously since they went through a complete and real purchase procedure). this project was approved by the ethics research board (cerfas) of the university of montréal (13-115-ceres-d). participants had to conform to the following exclusion criteria: 1) psychiatric or neurological disorder history, 2) history of head trauma resulting in loss of consciousness, 3) presence of a cardiac pacemaker, 4) presence of a piece of metal on the skull, 5) presence of tinnitus, 6) fainting history, 7) epileptic history, and 8) substance abuse."
"this study was a proof-of-concept of the use of tdcs in neurois. whereas no effect of dlpfc stimulation on peou ratings was observed, specific effects of stimulation were found on behavioral and neurophysiological measures during website interactions. taken together, the present results suggest that tdcs may provide valuable insight into the cognitive mechanisms that underlie interaction with it artifacts such as websites. this can only be achieved, however, if the neurophysiological effects of tdcs are better understood. indeed, widespread use of tdcs in applied fields such as neurois crucially depends on the establishment of stimulation parameter guidelines (intensity, duration, electrode size and placement, etc.) based on empirical evidence. furthermore, before tdcs over areas such as the dlpfc can be used to assess the neuronal underpinnings of complex is constructs (such as pu and peou, among others), its effects on cognitive functions such as decision-making, behavioral inhibition and working memory must be disentangled."
"the profile view (cf. figure 1b) can be accessed by clicking on an expert's name (anywhere in the application). it shows publications as well as collaborators, various statistics like citations, citations over time, h-index and i10-index, and more. keywords are extracted for each document in the corpus using a keyword extractor tool [cit] 11, which provides results as a ranked list of keywords 12 . in order to provide keywords for each author, the keywords of each document that an author has written are aggregated and ranked by document frequency. lastly, the profile view shows information such as awards, educational degrees, employers (current and previous) as extracted from wikidata."
"this issue has been discussed in a theoretical framework which categorizes experimental designs in applied neuroscience as able to state that a region is either associated with a concept, necessary or sufficient to explain this concept [cit] . in that regard, previous evidence [cit] associated peou and the dlpfc, whereas there are ways to determine whether the dlpfc is necessary or sufficient to explain peou. [cit] localization findings before addressing the more complex question of identifying the cognitive underpinnings of tam."
"a tobii x60 (danderyd, sweden) system was used to monitor eye movements and pupil dilatation, the threshold to detect a fixation was set at 200ms. all url that had \"buy\" buttons were grouped depending on the number of songs presented in a given page (which was between 1 and 10 buttons per webpage) and areas of interest (aoi) were drawn as a 80 pixel wide vertical column with a length depending on the number of songs presented on each webpage."
"the entire control loop operates with a divided converter clock to maintain dynamic power consumption that is proportional to the sc converter switching loss. this ensures that efficiency loss due to the control loop is always a small predictable value regardless of load current level. other digital blocks are also clocked by this divided converter clock (fig. 8.5.3, bottom right) . this helps reduce their power consumption relative to the system's output power level, but also maintains control loop stability since the operating speed of the various blocks are all scaled by the same factor -hence, blocks can communicate with each other at similar relative response speed, including voltage output."
"interacting with a website in an online shopping context is a complex goal directed task which involves a multitude of cognitive function, some of which were brought forward by this experiment. this stresses the idea that specific cognitive functions may be vulnerable to tdcs and result in behavioral changes during complex operations such as interactions with a transactional website. the two main cognitive functions that could potentially be involved in that process are decision making and cognitive effort. behavioral indicators of decision making. the main result arguing in favor of an involvement of decision making processes in the interaction with a website is that eyetracking data (visual fixation) showed significantly reduced time spent on the buy buttons following left anodal/right cathodal stimulation, which was not the case for left cathodal / right anodal and sham stimulation. the buy buttons are visited when the decision-making process is in its later stages, so decreases in time spent by visual fixation following stimulation can indicate habituation in this part of the decision-making process for the sham and left cathodal /right anodal groups, since relevance of the information and content of this aoi stayed constant during the experiment. the absence of habituation in the left anodal /right cathodal group may indicate an impact of tdcs stimulation of the dlpfc on the efficacy of decision-making, which may be an underlying cognitive process behind peou. for example, an interaction with a website that is not designed to promote efficient decision making could make the website more difficult to use, hence affecting peou. furthermore, left cathodal / right anodal stimulation of the dlpfc has been shown to promote conservative decision-making in healthy subjects [cit], which is consistent with the present findings. it has also been shown to alter other aspects of decision making such as lying behavior [cit], decision time [cit] and confidence in decisions [cit] ."
"as mentioned earlier, there are conflicting results with regards to the efficacy of tdcs in modulating cognitive functions [cit] . of primary concern is recent data suggesting that tdcs is associated with very high levels of inter-subject variability. for example, some studies estimate that 45% of participants do not respond in the expected way to tdcs [cit], which would be a facilitation of neuronal activity beneath the anode and the opposite beneath the cathode."
"the website used for the experiment was zik.ca, a major online music store in canada at the time of data collection. the experiment was conducted without contact with this company and results of the study will not directly serve, nor are they motivated or funded by, private company interests. three is experts evaluated the website using the webqual measurement scale [cit] to provide a general baseline outside of the tdcs experimental setting. it also allowed to make sure there would be no floor or ceiling effects in the ratings. descriptive results for the present sample and experts are shown in table 2 and a potential desirability bias can be observed between the experts and participants."
"the experiment consisted of six visits to an online music store, during which participants had to choose and purchase one song of their liking in each visit, using a prepaid credit card. each of the 6 visits lasted a maximum of 6 minutes to achieve comparable experimental length between visits and participants."
"repeated measures factorial anovas were conducted on the main variables (peou and pu) and on all nine subscales of the webqual, time between the start of the visit and \"buy\" decision, and average fixation duration of the \"buy\" buttons. visit was the within subject factor and stimulation group was as a between subject factor. when necessary, simple effects tests and dependent, or independent, sample t-tests were conducted with bonferonni corrections. the same type of anova was also conducted on eeg data. differences in spectral power for the four main frequency bands (delta, theta, alpha and beta) were compared for each visit and group at the two stimulation sites (f3 and f4) ."
"the tool also supports several other methods. it includes basic ranking methods based on simple statistics like h-index and citation count. these methods basically find all authors that dealt with the query topic and then rank the authors by their global or local h-index or citation count. in addition to that, the tool includes pagerank [cit], which ranks authors based on their incoming citations and co-authorships. lastly, the tool contains a ranking method based on relevance scores obtained from a document retrieval on the query topic. simply put, this method utilizes the sum of the relevance scores of an author's documents to create an expert ranking."
"as illustrated in fig 1, two sets of two visits were made to the website before the tdcs stimulation session. because online and offline tdcs have been shown to have a significant impact on learning and working memory [cit], tdcs was applied when the tested behavior had completed its learning phase. to achieve this, the first two visits allowed participants to familiarize with the website, the third and fourth visits allowed a pre-stimulation measure and the last two visits were done after the tdcs protocol ended. after each visit, participants completed the webqual questionnaire to assess peou and pu [cit] . this questionnaire also includes items assessing the entertainment factor of the website."
"early expert finding systems relied on manually crafted, and manually queried, databases. maintaining these databases is obviously a time consuming and complex task on the administrative and user side. early automatic expert finding systems usually focused on specific domains like email [cit] or software documentation [cit] . one of the first approaches that automatically extracts expertise from any kind of document was the p@ [cit] ."
"the graph view, which is shown in figure 3 visualizes the query-based citation network for a particular method. documents and authors are rendered as nodes whereas citations, authorship relations and co-authorship relations are represented as edges. thus, it allows a quick peek into the data and an even better understanding of the results."
"a magstim dc stimulator (magstim, whitland, uk) was used to deliver tdcs for 15 minutes at 1.5ma. the 5 by 7 cm sponge electrodes were placed on the left and right dlpfc, corresponding to the f3 and f4 sites of the international 10-20 eeg system as identified by the eeg headset. the current slowly increased in the first 30 seconds and slowly decreased during the last 30 seconds to minimize the tingling sensation sometimes associated with the beginning and end of stimulation. in the sham condition, the current ramp up was performed at the beginning of stimulation and was automatically turned off for the remaining stimulation period. after completion of the fourth visit, the sponge electrodes were quickly slid under the inactive eeg headset guided by the position of the f3 and f4 electrodes and removed once the stimulation protocol was over. the position and impedance of the eeg headset was checked before resuming data collection for the fifth and sixth visits."
"one of the major contributions of our tool is to provide a user-friendly comparison method. the evaluation view (cf. figure 2a) executes the major expert finding methods and presents columnar results. with this view, it is easy to identify differences as well as to qualitatively compare the results. clicking on an expert's name in this component will open the evidence view (cf. figure 2b) for further investigation. it shows the documents figure 3 : graph view: authors are rendered as blue nodes, documents are rendered as green nodes, the highlighted node is rendered in red. the size of a node reflects its relevance. the graph is initially filtered by relevance to reduce cognitive overload and can be expanded or reduced for particular nodes."
"although [cit] may partly explain conflicting results. [cit] study compared interfaces that scored both very high and very low in terms of peou and pu. this may have elicited different processes than averagerange websites such as the one used in this study. it is therefore possible that tdcs could modulate behavior when users are interacting with a web interface that is hard to use. indeed, as state dependency theories of nibs suggest [cit], the efficacy of stimulation depends on the nature of the task and the neuronal activity in participant's brain both before and during the task. [cit] found that effects of tdcs were only visible in a high workload task whereas there were no significant effect in an easier task."
"in recent years, an increasing number of studies have been conducted in information systems (is) research using neuroscience tools and theories. this emergent field of research, known as neurois, aims to refine and better understand the cognitive and affective mechanisms underlying interactions with it artifacts [cit] . in one of the first neurois studies, dimoka and collaborators [cit] investigated the neural correlates of the technology acceptance model (tam) [cit] . tam aims to predict use and appreciation of technology using two main constructs, perceived ease of use (peou) and perceived usefulness (pu), which are usually measured using psychometric scales such as the webqual instrument [cit] s and was initially used to evaluate the implementation of information systems in the workplace. it has since become a leading model in is research and has been applied to contexts and domains as wide as software adoption in the workplace [cit], a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 user experience research [cit] and marketing [cit] . using functional magnetic resonance imaging (fmri) in 6 healthy adults, dimoka and colleagues found that peou of a commercial website was associated with activity in the dorsolateral prefrontal cortex (dlpfc) whereas assessing pu recruited the caudate nucleus, anterior cingulate cortex and insula [cit] ."
"the fcp/indi project shows how a community effort can result in the availability of large, freely-available datasets that can be used to enable novel scientific discoveries. the success of fcp/indi also suggests that the neuroimaging community has a greater appreciation for the benefits of data sharing than it did when the fmridc first began 10 years ago. at the same time, it is important to highlight that by focusing on resting state fmri, the fcp/indi project sidesteps many of the difficult metadata problems that are present for task fmri (in particular, the need to represent task paradigms and behavioral data in a systematic way)."
"the proposed detection method are evaluated on the ntu rgb+d dataset [cit], which is defined as the present biggest widely obtainable 3d action recognition dataset. the dataset includes more than 56k action videos and contents 4 million frames. in this paper, seven different human health-related actions of video sequences were selected from ntu rgb+d dataset (falling, nausea, headache, neck pain, sneezing, staggering, and stomachache) presented as frames sequences for training using tensorfolw object detection models and testing using android's camera. the training dataset was created by manually tagging the human action in the video frames using labelimg software [cit] because it is needed to have a ground truth of what exactly the object is. in other words, it is important to draw a bounding box around the person with his action as shown in fig.2, so the system knows that this \"action\" inside the box is the actual human action. each person with his action was tagged as a name of the human activities related to the type of action (a health-related action). labelimg saves the annotations as xml-files in pascal voc format is prepared for creating tfrcords ( tensor flow record format). each dataset requires a label map connected with it, which represents a mapping from string class names to integer class ids. label maps should always start from id1. in this work, there are seven ids related to seven human health-related actions. [cit] x1080 sized video frame (and the corresponding annotation files) were later resized to improve the model training efficiency. once all the frames were labelled, the next step was to split the dataset into a train and test the dataset."
"in addition to these efforts at sharing raw data, another set of efforts has focused on sharing of highly processed data, namely the activation coordinates reported in papers. these include brainmap (http://www.brainmap.org) [cit], sumsdb (http://sumsdb.wustl.edu/sums/), and neurosynth (http://www.neurosynth.org) [cit], each of which provides tools to perform coordinate-based metaanalyses. this approach has been very powerful, but at the same time is clearly limited by the coarseness of the data at every level; [cit] . these results suggest that in addition to the sharing of raw data, there is likely utility in the sharing of processed data (e.g., statistical images)."
"ten years ago, the field of neuroimaging was a virtual tower of babel, with a number of incompatible image data formats used across different software packages and scanner platforms. this made early efforts at data sharing very difficult. in recent years, the field has gravitated toward two standard formats for data storage which have addressed this problem to some degree. most mri scanners now save the raw mri data to the dicom format. however, dicom is not convenient for everyday analysis due to the fact that it requires a large number of small files. in addition, the dicom standard varies between implementations across different scanners. within the neuroimaging community, a standard known as nifti (http://nifti.nimh.nih.gov/nifti-1/) has been widely adopted in the field and is now supported by every major fmri analysis package. the nifti format provides support for 3d and 4d images and supports rich metadata including orientation information, which can help alleviate problems with left-right orientation that were common in early days of fmri. although the nifti format is a step forward, differences in its implementation remain between software packages, such that problems can still arise when using data processed across multiple packages."
"another common concern about data sharing for researchers relates to intellectual property. we believe that sharing of data with highly restrictive terms and conditions would defeat the purpose of an open data sharing repository, and we trust that the community will largely be responsible in their use of the data and attribution of its provenance. for this reason, data shared by the project will be released by default under the public domain dedication and license developed by the open data commons (odc). this license states that users can download the data and use them for any purpose they wish, with no requirement for permission, citation, or coauthorship. we will encourage users to follow the odc attribution/share-alike community norms, which request that users give credit to the originator of the data and share any resulting products in a similar manner. we realize that some investigators may wish to share high-value datasets but may not be comfortable with public domain dedication; in this case we will consider more restrictive licensing on a case-bycase basis. in cases where investigators wish to stage a dataset for release on a specific date (e.g., to coincide with the publication of a paper), we will allow investigators to specify an embargo period for submitted datasets (generally not to exceed 6 months), which will provide sufficient curation time for the dataset to be ready for release on the intended date. individuals sharing data should reasonably expect to receive credit for having gone to the effort of data sharing. on the openfmri web site, credit is given via a link to the publication on the associated data page as well as a list of investigators involved in collecting the data. in addition, the inclusion of the openfmri database within the neuroscience information framework (nif: [cit] ) allows links to the dataset to be added automatically to the pubmed listing for each associated paper, using the nif link-out broker [cit] . with the advent of venues for data publication including nature scientific data and gigascience, it is also possible for contributors to publish a separate paper that describes the dataset [cit] b) ."
"to evaluate the model during the training and after it, each time the training produces a new checkpoint, the evaluation tool will perform predictions using the video frames available in a given directory."
"the open sharing of fmri data has the potential to revolutionize cognitive neuroscience in much the same way [cit] . first, doing so would allow investigators to search for similar patterns of activity in multiple datasets, and thus to identify relations between cognitive tasks that result in these similarities. this could help address the common problem of reverse inference, wherein patterns of activation are informally used to infer putative mental function. the sharing of fmri data would allow researchers to more formally assess the specificity of observed brain activity with various cognitive tasks, thereby permitting probabilistic inferences about the role of various brain regions or networks in mental function. second, by allowing researchers to decompose the mental processes involved in each study and then test for associations between these processes and brain activity, large databases would support more direct identification of relations between mental processes and brain networks, rather than relying on associations with activation on single tasks [cit] . third, by making published datasets available to a wide range of researchers, open sharing would encourage the re-analysis of existing data with new analysis methods (e.g., [cit] ) . doing so would not only obviate the need for additional data collection in some cases, but would also allow more direct comparison between previous and new analysis methods. in addition to these judicious effects on scientific knowledge, the availability of a large database of published datasets would also have a powerful impact on education and training, as new trainees and individuals at institutions without imaging resources would have access to extensive datasets. finally, there is an ethical argument to be made that sharing of data is essential in order to fully respect the contributions of the human subjects who participate in research studies [cit] )."
"here, we describe a new resource for the open dissemination of functional neuroimaging data, called the openfmri project (accessible online at http://www.openfmri.org). the goal of the project is to support the free and open distribution of both raw and processed neuroimaging datasets, focused primarily on whole-brain datasets from task-based fmri studies. the project aims to use what was learned in previous data sharing efforts and take advantage of subsequent improvements in computing and information technology as well as changes in the social landscape that have made open data sharing more viable."
"(2) headache action. www.ijacsa.thesai.org tables 2 and 3 summarize all the parameters (map, total loss, classification loss, localization loss) of the training and evaluation processes in addition to the number of the steps with the time that they consumed to achieve these requirements. table 4 labels the mean average precision (total map) for all the actions classes of the both per-trained models. the total map that belongs to the ssd-mobilenet model is somewhat higher than the total map of the faster-r-cnn-resnet model."
"the action detection from the phone's camera consumes roughly 25 seconds for all 50 video frames. in the headache action, there are two misdetections, however, when the detection using the android camera, these two problems have been solved. the action detection for all frames come to be all properly visible and the bounding detection boxes have become in the correct action location. www.ijacsa.thesai.org"
"a final challenge arises from the need to specify the psychological constructs that are meant to be indexed by each experimental comparison in a dataset. this is a much more difficult undertaking than describing the task and imaging metadata because atlas task database, and relations between these tasks and mental processes can then be specified by researchers in the community."
"the ssd applies smooth l1-norm [cit] to determine the location loss. regarding classification process, the ssd performs object classification. therefore, for every predicted bounding box, collections of n categories predictions are calculated for each likely category in the dataset. furthermore, feature maps are a description of the interest features of the image at various scales, hence working multibox on multiple feature maps rises the likelihood of any object whether large or small which to be eventually detected, localized and properly classified."
"another challenging issue surrounds the representation of behavioral data, which are essential to the modeling of fmri data for many studies (e.g., for modeling of accuracy or reaction times). because there is no general framework for the representation of behavioral data, we have developed a simple protocol for behavioral data storage for the openfmri project. this is a trial-based scheme in which any number of variables can be specified, including independent variables (such as condition names or stimulus identities) and dependent variables (such as response time or accuracy). an additional key file describes the meaning and possible values for each variable. if additional variables need to be represented in a way that is not trial-based (e.g., eye position measured at every timepoint), these data can be specified in additional files. in this way, we allow maximal flexibility with minimal need for reformatting (since the data for most studies will already be stored in a trial-based manner within a spreadsheet). as the project progresses, we plan to develop a more formal representation of the behavioral data (e.g., using xml)."
"objects detection utilizing a mobile camera has many functions like video surveillance, object stability, and collision revocation.most techniques have been used to detect tracking objects from a non-stable platform. whereas these techniques require the movement parameters of the camera to be recognized, that is frequently not easily obtainable or are incapable of object detection if the tracking object's size is small. in this paper, we aim to present a new detection application for video images of human health-related actions using android phone's camera. the software is based on deep learning system running on tensorflow's object detection api using android platform. a robust tool gives it straight to construct, train, and use object detection models. in most of the cases, training a complete convolutional network from scratch www.ijacsa.thesai.org is time-consuming and needs massive datasets. accordingly, this problem can be resolved by utilizing the power of transfer learning with a pre-trained model [cit] using the tensorflow api."
"in conclusion, the health-related actions can be detected with high-speed detection and its great accuracy. it can figure out the correct action required to deal with the appropriate situation using the smartphone camera. a new detector model was built for seven different human health-related video actions using two techniques of tensorflow object detection api, which are tesnsorflow object detection notebook and tensoflow in android, using the phone's camera. in addition, the hhra detection model was trained and evaluated using ntu rgb+d dataset based on two pre-trained models (faster-r-cnn-resnet and ssd-mobilenet). according to the results, in the best-detectable category, the map total achieved 93.8% for the faster r-cnn-resnet and 95.8% for ssd-mobilenet. in addition, the lowest error was calculated through both losses of classification and localization and the results were satisfactory. furthermore, the detection speed and the highperformance efficiency have been improved by the use of the smartphone. in the future, we plan to the opportunity to train using google cloud to decrease the training and evaluation time. moreover, new methods can be developed to detect the further human actions classes."
"we have implemented an automated processing stream for the data in the openfmri database; the processing steps are listed in table 1, and the code for these analyses is freely available via the openfmri web site. because of the use of a precise organizational scheme and metadata format, it is possible to completely automate every step of data processing, including the generation of fsl design files for each level of analysis. eventually, the results from each intermediate processing step will be made available in the future through the openfmri web site along with the raw data. because of the computationally intensive nature of such processing on a large dataset, analysis is performed using the high-performance lonestar cluster at the texas advanced computing center. all code used to implement this processing stream is available at http://github.com/poldrack/ openfmri. the specific processing stream was selected based on its current use in the first author's laboratory, but represents a fairly standard processing stream in the field. after conversion to nifti format and organization via the standard data scheme, the bold data are motion-corrected using mcflirt (fsl) and the brain is extracted using bet (fsl). the event onsets for each experimental condition are represented using the 3-column (onset time, length, and weighting) format from fsl. using custom code, we automatically generate the fsl design files from these onset files, with extracted motion parameters and their temporal derivatives included as nuisance regressors. first-level statistical modeling is performed using feat (fsl) and contrasts are automatically generated for each experimental condition compared to baseline, in addition to any other potential contrasts of interest. for studies with multiple runs per task, second-level modeling is performed using a fixed-effects model. third-level modeling is performed using flame (fsl), implementing a mixed-model that treats subjects as a random effect."
"the openfmri website storage and processing mechanisms have been chosen to provide an extensible software platform. datasets are stored in an xnat server [cit], and processing streams access the datasets through xnat's built in rest api. in our initial model, the lonestar cluster at tacc accesses data from xnat, performs its processing operation, and then writes the processed data back into xnat. using xnat's web services, we can expose that read/write api to other applications on a case by case basis. this will allow qualified users to apply their own analysis methods to the openfmri database and then expose the results via the database. this platform model will give end users a variety of choices in how their data are processed, while providing automated documentation and quality control."
"generally, the elderly, children and the people with special needs are considered to be in need of care and supervision of their behavior all the time. safety is a priority especially when their caregivers are not available to prevent accidents like falling, passing out or nausea due to an existing medical conditions or unforeseen dangers to prevent such accidents as in falling, nausea this led us to build an application to detect the abnormal actions through the android platform and in the future, it can be manipulated to send a alerting message to the observer to provide the necessary assistance as soon as possible. the object detection is a common concept for computer vision methods for definition and labeling objects. human action detection is one of the top massively studied topics that can be used for surveillance camera. most of the techniques include substantial limitations when it comes to the specific form of computational resources, the dependence of the motion of the objects, disability to differentiate one object from another, the absence of proper data analysis of the measured trained data, and a major interest is over the speed of the movement and illumination. therefore, drafting, applying and recognizing new methods of detection that manage the present limitations, are much needed. the techniques of object detection can be applied both to still images or video images. the objective of human action detection is to detect each appearance of a specific action in a video and to localize every detection identified together in space and time. recently, the video action recognition performance has improved based on deep learning (dl) approaches. human action detection task is challenging compared to the human action recognition because of the change in size of the human and in addition to the spatio-temporal location. generally, deep learning has been connected with data centers and large clusters of great-powered gpu machines. nonetheless, it can be highly expensive and time-consuming to transfer all of the data in the device and deliver it across a network connection. implementation on a mobile makes it possible to deliver real interacting applications using a method that is not possible when you should wait for a network round trip. the smartphone is being added to video surveillance systems almost everywhere at any time. in other words, the video surveillance system for mobile has spread and expanded significantly in recent times so you can monitor your home or business when you are away. smartphone devices with a camera are a massive part of our daily lives. however, there is a growing interest in the interaction between the users and their devices. the interaction between the user, the phone, and real-world objects [cit] represents the many variations of smart device applications."
"in this work, the pre-trained with one of the models (ssd_mobilenet or faster r-cnn-resnet ) was fine-tuned for ntu rgb+d dataset using manually labeled video frames of hhra saved this tagged data as an xml file and adapted this xml file to a csv file. next, the csv file is converted to tfrecord file by satisfying the similar specifications as shown in fig.1 . the entire training process is addressed by a configuration file recognized as the \"pipeline\". the pipeline is split into several essential structures that are responsible for determining the model, the training and evaluation process parameters, and both the training and evaluation dataset inputs. actually, the tensorflow advises that the training should apply one of their own and already trained models as an outset point. the idea behind this is that training a completely new model from scratch might need an excessive amount of time. therefore, the tensorflow gives various configuration files, which only needs a number of changes that correspond to a new training environment. the results of the training and the evaluation stages can be observed by applying tensorflow's visualization platform, tensorboard [cit] . this tool can observe various metrics such as the training time, total loss, number of steps and much more. the tensorboard also runs while the model is being trained, making this an excellent tool to confirm that the training is going in the right direction. the given checkpoint file for the models is applied as a beginning point for the fine-tuning process. www.ijacsa.thesai.org"
"quality control is performed using the fmriqa package (https://github.com/poldrack/fmriqa) for raw data, and the fsl-qa cortical surface generation and parcellation (highres) freesurfer package (https://github.com/poldrack/fsl-qa) for analyzed data. qa results for the raw data are included in the base download. reports are also generated that allow manual inspection of defacing, spatial registration of structural and functional images to standard space, and statistical analyses; these reports are examined and validated by the openfmri staff before the data are made publicly available. when data are uploaded to the openfmri database, they are processed by the curators through the level of group analysis, in an attempt to replicate the results of the original analysis (e.g., in a published paper associated with the dataset). given the multiplicity of different analysis streams and likelihood of different results between streams [cit], it is expected that the results will sometimes fail to exactly match those of the original analysis. in such a case, we first contact the investigators to ensure that the task has been properly modeled in our analysis (e.g., that there are no mistakes in the event timing files). if the modeling is confirmed to be correct, then the authors will be given a chance to withdraw their submission, or to have the data shared despite this mismatch in results."
"in this paper, human health related action videos have been detected by using the implemented tensorflow object detection api technique. the two new pre-trained (faster r-cnn-resnet and ssd-mobilenet) models have been applied for training human actions dataset. the average precision (ap) is the average of class predictions estimated over several thresholds. the detection accuracy (map) at 0.5iou is a high value with different num_steps. this is due to the fact that the network deals with the video images, therefore it takes a long time to train the entire samples of each frame for every action and depend on the type of the model's architecture. the parameter num_steps determines how many training steps they will run before stopping. this number certainly depends on the size of the dataset along with how long the user wants to train the model. localization loss represents the predicted bounding box which matches with the ground-truth bounding box. however, the loss value is decreased and the intersection over union is high, which means that the detection of the action is in the correct location. the classification loss shows the effectiveness of human action class which is classified and matched with the previous trained class. when the classification loss values decrease and are near zero that means that the classification accuracy is outstanding and the performance of the detector becomes more high-level. the android detection results were compared to the detection process tensorflow object detection notebook technique. these two distinct processes were used to examine which one does a better job in measuring the detection speed and how accurate the detection is. in the end, utilizing the android smartphone's camera revealed that the seven types of human health-related actions were precisely detected with high accuracy and reasonable detection speed rate."
"in the light of this, the model detector has the power to detect a big percentage of objects in an image; however, it as well produces a large number of false positives. while the model detector by a big threshold for detection only generates a small false positive, but, it likewise quits a greater percentage of objects, which remain undetected. the best equivalence between these two depends on the applicability."
"the usual fmri dataset comprises a set of functional images (usually 4-8 scanning runs lasting 6-10 min each) along with structural brain images and other associated measurements (such as physiological and behavioral data). the functional data typically consist of 4-dimensional data sets (3 spatial dimensions x time); depending upon the number and length of scanning runs, spatial resolution, and the number of slices acquired, the raw functional data for a single subject in an fmri study can range in size from 50 mb to more than 1 gb, and most studies have at least 15 subjects. datasets of this size require substantial resources for storage and processing, although improvements in computing technology have made it feasible to store and process such datasets on commodity hardware. in addition, cloud-based resources make the sharing and analysis of very large datasets possible without purchasing any physical hardware."
"high-resolution anatomical images are first brain-extracted using freesurfer. the anatomical image is aligned to the mni152 template using a combination of boundary-based registration and linear registration with fnirt (fsl). the functional images are aligned to the high resolution image and the warps are combined to provide a transformation of the functional data into the mni152 space, which is applied to the results of the statistical analysis at the higher levels. cortical surface generation and automated anatomical parcellation are performed using freesurfer."
"in the evaluation stage, the map measures the trained model percentage of correct predictions for all seven actions labels. the iou is particular to object detection models and a case for intersection-over-union. this measurement represents the overlap between the bounding box generated by hhra model and the ground truth-bounding box, described as a percentage. the map graph is averaging the percentage of correct bounding boxes and labels of the hhra model resumed with \"correct\" in this case relating to bounding boxes that had 50% or more overlap with their corresponding ground truth boxes."
"in addition to being hosted directly by the openfmri.org web site, the shared dataset is also available via the incf dataspace (http://www.incf.org/resources/data-space), which is a federated data sharing environment based on the irods data management system."
"in this work has used the developed software tool for the android camera; it focuses on two recent tensorflow object detection api models: ssd_mobilenet framework, and the faster r-cnn-resent framework. the algorithm proposed for hhra detection model is important to understand how efficient the framework performs. www.ijacsa.thesai.org iv. tensorflow object detection api tensorflow object detection api's package is a process to resolve object detection problems. this is a technique of detecting real-time objects in an image. in agreement with the documentation and the paper [cit] that shows the library, what makes it exclusive is that it is capable to trading accuracy for speed and memory application (vice-versa). therefore, you can modify the model to satisfy your requirements and your platform, like a smartphone. the tensorflow object detection api library contains multiple out-of-the-boxes object detection structures like ssd (single shot detector), faster r-cnn (faster region-based convolutional neural network), and r-fcn (region-based fully convolutional networks)."
it is important to mention that which the there is an opposite correlation between precision and recall and which these metrics are dependent on the model score threshold [cit] .
"in this paper, the proposed model aim to detect human health-related actions from videos of n frames by using android camera. the dataset splits into training and testing dataset. first step is to label the dataset by drawing a bounding box (ground truth) around each video frame of human healthrelated action. then, save them as xml file. the xml file converts to a csv file and thereafter, tensorflow that is called \"tfrecords\" converts the csv file into a format that is readable. the proposed hhra model uses two per-trained models of tensorflow object detection api that are faster r-cnn-resnet and ssd-mobilenet for training the dataset. model evaluation will be done during and after training. lastly, it will demonstrate how to export the model to android for detect the action type using android camera. under the object detection algorithm utilizing deep learning, there are many parameters that are learned from the data. fig.1 describes the basic block diagram for proposed the hhra model in this paper. the setting of tensorflow object detection api of the models that used in this work as following as www.ijacsa.thesai.org"
"confidentiality of research participants is of critical importance in data sharing (cf. [cit] . the upload policy for the project specifies that data should be anonymized before uploading by removing all of the 18 possible unique identifiers specified by hipaa. the investigators submitting data are responsible for anonymization, but once data are uploaded a curator will doublecheck the data to ensure that no identifying information remains. because high-resolution structural images may contain information about facial structures, all structural images will have facial features removed prior to sharing."
"although the statistical maps obtained in the analyses described above include more than 200,000 voxels, a significant amount of information is carried in the coordinated activity of a much smaller number of large-scale neural systems, which can be identified using dimensionality reduction methods such as independent components analysis (ica). to characterize the largescale networks that emerged across the different tasks in the dataset, statistical (z) images for each contrast/task/dataset were submitted to ica using the fsl melodic tool [cit] after spatial smoothing (6mm fwhm). [cit] ), we first specified 20 components in order to identify a small set of large-scale networks. the components identified in this analysis are shown in figure 2 . a number of these components reflected the basic sensorimotor aspects of the tasks, including visual regions (components 1, 3, and 19), auditory regions (component 10), and motor regions (components 15 and 20) . others reflected higher-order networks, including fronto-parietal (component 2) and cingulo-opercular (component 5) [cit] and the left-hemisphere language network (component 6). in addition, there were components reflecting the \"default-mode\" network generally identified during the resting state (component 4) as well as one component reflecting coherent white matter signal (component 14). [cit], which were based on meta-analytic maps from the brainmap database."
"the openfmri database currently contains 18 full datasets from seven different laboratories, with a total of 347 unique scanning sessions, all of which are available for download directly from the web site. the database remains heavily skewed toward datasets from the poldrack lab, but is increasingly diverse with the addition of new datasets. the site is currently accepting uploads, and has a number of datasets in the process of curation in addition to those currently available for download. [cit], there have been 914 downloads of full datasets from the database, and four publications using data from the openfmri database [cit] ."
"the first effort to openly share fmri data was the fmri data center (fmridc) [cit] recently outlined the history of the project and discussed its impact and the lessons learned in the project. the fmridc amassed 107 fmri datasets which remain available for shipment via physical media. data obtained from the fmridc were used in at least ten papers that presented novel analyses, utilizing both single datasets as well as mega-analyses combining multiple datasets [cit] . these ranged from analyses of task-related connectivity [cit] to one of the earliest studies of the \"default mode\" in alzheimer's disease [cit] to an exploration of consciousness that combined data across multiple studies [cit] . the fmridc also aroused controversy within the neuroimaging community early in its existence when it was announced that some journals (including the journal of cognitive neuroscience and proceedings of the national academy of sciences) would require authors to submit their data to the center [cit] . this reluctance of the community to participate in data sharing via fmridc was likely due to a number of factors including a lack of social consensus at the time regarding the value of data sharing as well as concern about the significant amount of effort required to submit datasets to the fmridc. ultimately the project discontinued addition of new datasets due to a lack of continued funding, but the data remain available and it clearly played a role in establishing the utility of sharing complete fmri data sets., this project has already shared nearly 5000 subjects' worth of resting state fmri data collected from centers around the world, making the data openly available via the web. initial mega-analysis (i.e., reanalysis of the full combined dataset, as opposed to meta-analysis of summary statistics) of this dataset provided novel insights into the stability and variability of resting state networks [cit], and other groups have already used the data to make new discoveries about the organization of resting brain networks [cit] . a limitation of the initial fcp dataset was that there was very little phenotype data included other than sex and age; however, more recently this group has begun prospectively sharing data with a greater amount of phenotype information, including the deeply-phenotyped nki-rockland sample [cit] ."
"a single shot multibox detector (ssd) [cit] by researchers from google. the ssd is a rapid singleshot object detector for multiple classes. it utilizes a one feedforward convolutional network to assume classes straightforward and anchor stabilizer without needing another step for each proposal classification process. the important feature of ssd is the employ of multi-scale convolutional bounding box outputs connected to various feature maps at the highest of the network. the vgg-16 was applied as the core network because of its effective performance in high-quality image classification functions and transfer-learning training in order to enhance the results. the bounding box technique of ssd is driven by szegedy's project [cit] on multibox, and an inception mode convolutional network is used. the multibox's loss task mixed two significant parts that performed their path to ssd. the first part is confidence loss that measures how confident the system is of the objects of the calculated bounding box. however, the second part is location loss, which measures the distance of the network's predicted bounding boxes from the ground truth ones through the training process."
"previous work (using some of the same data analyzed here) has shown that it is possible to classify which task a subject is performing using a classifier that was trained on other individuals performing a broad range of tasks [cit] ). [cit] but using different dimensionality reduction and classification methods. we trained a classifier to identify the task being performed by each subject out of 26 possible tasks. to reduce the dimensionality of the dataset, the whole-brain data from run 1 were regressed against the spatial ica components obtained from the run 2 data (in order to maintain strict separation of training and test data). ica was estimated using several different dimensionality levels, in order to examine the relation between classification accuracy and degree of dimensionality reduction; in each case, the loadings on each component (ranging from 2 to 200 components) were used as features in the classification. twenty-six-way classification was performed using three methods: linear support vector machine (svm) (implemented in liblinear: [cit] ), non-linear support vector machine (with radial basis kernel, implemented in libsvm: [cit] ) and regularized logistic regression (lr) with an l2 penalty (implemented in the scikit-learn package: http://scikit-learn.sourceforge.net/). classifier parameters (cost parameter for svms, gamma for non-linear svm, and penalty parameter for lr) were optimized using the run 2 data, ensuring no crosstalk between parameter identification and classification testing. classification accuracy was assessed on the run 1 data using leave-one-out cross validation, and an empirical null distribution was obtained by performing the classification 1000 times using randomly permuted labels. accuracy for the 26-way classification for each method across all dimensionality levels is shown in figure 3 . accuracy rose incrementally as the number of components was increased from 2 to 100 and then remained relatively stable (around 50%) after 100. accuracy was quite similar for the two linear classifiers, and only slightly higher for the non-linear svm. all classification values were substantially greater than chance. [cit] . it should be noted that because some subjects contributed data to multiple datasets in the classification analysis, not all data points were independent; this likely led to reduced classification accuracy due to confusions caused by subject-specific rather than task-specific patterns. analysis of classifier confusion matrices showed that discriminability between tasks was compromised in some cases where the subjects overlapped, but in many cases also reflected overlap in task content across different datasets. a further analysis of the effects of non-independence is beyond the scope of the present paper but will be explored in future publications."
"the changes are made so that the variable num_classes, num_steps to pause the training earlier, fine_tune_checkpoint to point to the location of the model downloaded, and the input_path and label_map_path variables of the train_input_reader and eval_input_reader to point to the training and testing dataset, as well as the labels map. primarily, training data as well as evaluation data are needed during the training process. the training data is necessary to learn the model on these data, while the evaluation data is required to evaluate the accuracy of the trained model if the model has learned all the video frames for each class. region proposals are clustering-based method, which tries to group pixels and produce proposals based on the created clusters."
"one of the great advantages of large datasets like those in the openfmri database is the ability to examine the large-scale multidimensional neural space that characterizes different cognitive tasks. in order to examine this, we performed a hierarchical clustering analysis on whole-brain statistical maps after projection into the 20-dimension ica space depicted in figure 2 and averaging across subjects within each task; clustering was performed using ward's method with a euclidean distance metric as implemented in scikit-learn. the resulting dendrogram is shown in figure 4, and shows that there is a noisy but surprisingly consistent similarity in the neural activity patterns between tasks that engage common processes [cit] . in several cases, maps from similar tasks within the same dataset were clustered together (e.g., the pseudoword naming and letter naming conditions from ds007, both of which come from the same subjects), whereas in other cases, the same task from different datasets were clustered together; particularly striking is the fact that the classification decision and classification learning datasets from multiple studies are clustered together, even though they were collected on very different versions of the tasks and different scanner platforms. these results highlight the degree to which different tasks exist within a larger similarity space of neural activity, which could potentially provide insights into the latent neurocognitive bases of mental functions."
"an android platform has been used toward the object recognition applications [cit], which works on images taken through a built-in camera. android is gradually growing to be the commonly utilized platform amongst the smartphone technologies as a monitor. the user can have access to the correct human action from the object by which the required action is marked upon the detected object. the recognition of the object detection in a video frames on an android is completely developed. once the object is appropriately detected, it could be saved and accessible for future applications."
"although the benefits of sharing fmri data are clear, the challenges of doing so are even clearer. the sharing of fmri data is made difficult by a number of factors including large datasets, need for common data formats, complex metadata, and social factors."
"1) tensorflow's object detection notebook: as mentioned above after the requirements is completed, the detection process is accomplished by using tensorflow's object detection notebook. in pre-trained faster r-cnnresnet model, the bounding detection boxes for each frame from seven different human health-related actions consumed around 120 seconds over all the 50 frames of the testing dataset. while, the bounding detection boxes for each frame in pre-trained ssd-mobilenet model are finalized within a 95 seconds time span. the detection results in two pre-trained faster r-cnn-resnet and ssd-mobilenet models for samples of frames of each human health-related action are shown in fig.11 . the results displayed a high detection accuracy for all the actions. according to the ssd-mobilenet model, the results include diverse detection values ranging from intermediate to high values of different action frames.however, there are misdetections in the headache action where ssd-moblienet failed to demonstrate the bounding detection box in some frames and misdetection that shows the bounding detection boxes in a wrong action placement as shown in fig.12 ."
"frontiers in neuroinformatics www.frontiersin.orgis shown in figure 1 . the datasets currently available on the site have been organized according to this scheme. as shown in figure 1, each study is also associated with a set of key files that describe the conditions, tasks, contrasts, and mri data acquisition details (including order of scans) that are necessary for proper analysis. this scheme will likely need to be modified to accommodate unexpected features of future data sets, such as different types of task designs. in addition, while currently organized using flat text files, we envision that in the future this scheme will be migrated toward a more formal metadata representation scheme such as xcede [cit] ."
"the sharing of data has become commonplace in many parts of science, and the availability of large databases of shared data has led to impressive advances that could not have been made without such sharing. for example, the genbank database (http://www. ncbi.nlm.nih.gov/genbank/) contains all publicly available dna sequences, which currently number more than 100 million annotated sequences. using these data, a large number of data mining tools have been developed that allow computational gene discovery (i.e., mapping from sequences to specific genes) as well as prediction of the proteins that are encoded by a sequence. such tools have greatly increased the power of molecular biology and genomics research. an excellent example of the power of these tools comes from the outbreak of e. coli o104:h4 [cit] . the genetic sequences obtained from these organisms were made public on the internet, and within days researchers around the world had determined the genes responsible for the especially high virulence of the bacterium as well as its relation to other known e. coli strains. such applications highlight one of the most important benefits of data sharing: by combining shared data into large databases, it is possible to identify relationships between effects at different levels of analysis (e.g., genetic sequence and bacterial virulence) that otherwise would be much more difficult to identify."
"2) tensorflow in andriod: once the requirements are completed, as mentioned in sections (vi and viii-g), the model will be imported to an android phone (galaxy s6). this is the time for testing the video frames based on pretrained ssd-mobilenet model by capturing them from the phone's camera also bounding detection boxes have been visualize for each frame with the name of the action class and detection percentage accuracy as described in fig.13 . in order to evaluate the impact of the tensorflow in android for the detection results, the detection accuracy is improved up to high values and it speed up the detection time."
"one of the major goals of the openfmri project is to enable whole-brain meta-analyses. for this reason, a dataset must include task-based fmri data with coverage of the whole brain in order to be included in the openfmri database; missing data at the edges of the volume can be accommodated, but datasets including coverage of only a portion of the brain will not be included (similar to the inclusion requirements for the brainmap database). in addition, a high-resolution structural scan is necessary for each individual; additional structural scans, such as an in-plane structural image or diffusion-weighted images, are welcome if available but are not required. finally, the metadata necessary to perform a standard statistical analysis (i.e., event onset times and durations for each experimental condition) are required. in cases where trial-by-trial behavioral data is necessary to perform the primary analysis of interest then those data are required; in other cases the submission of behavioral data is encouraged but not required."
"development, curation, and further population of the site are currently funded by a set of linked grants from the national science foundation. [cit] point out, it is essential to have a plan for longevity once the initial funding period ends. we are hopeful that national funding agencies will continue to view this project as worth supporting, but cannot rely on this. the texas advanced computing center has committed to long-term storage and accessibility of the data, but continued operation of the project beyond the funding window will require volunteer curators. given the increased attention to data management and data sharing by federal funding agencies, it is possible that curation could also be supported by \"data managers\" funded by grants in participating labs. we will also explore other options for long-term funding such as development of a non-profit organization (similar to wikipedia)."
"to validate the hhra model's performance for action detection in videos, comprehensive examinations have been achieved on a special type of human action that has related on the health from the ntu rgb+d datasets. in order to accomplish the greatest predictable detection accuracy, sets of human actions frames with different health-related actions and various environments are tested. in this work, the detection process for testing the video frames of seven different human health-related actions is implemented in two ways to the evaluation of the detection performance using the tensorflow object detection notebook and the android camera in terms of accuracy and detection speed. the first way is by trying out the tensorflow object detection notebook with couple pretrained models (faster r-cnn-resnet and ssd-mobilenet). while the detection using android camera only uses the ssdmobilenet model because the tensorflow in android does not support the faster-r-cnn-resnet model yet. to perform the detection process in both ways, must export the model as a static inference graph trained on the human health-related dataset, as well as the corresponding label map. the tensorflow object detection api library provides the script, named export_inference_graph with using the latest checkpoint number at the last step that stopped the training process. it has used the 16.04lts gtx1070@2.80ghz x8 system to run the object detector on each frame from seven different human health-related actions of the ntu rgb+d dataset to detect the action type. the detection process has been applied on 50 frames of each video action for seven different actions."
"the web page for each dataset currently contains versioning information that describes any changes in the dataset. in addition, a revision history file is included with each dataset download. while the raw data are largely independent of any processing software, the distribution of processed data is made challenging by the constant stream of software updates for packages such as fsl. fortunately, the implementation of our processing stream within a high performance computing environment makes it relatively straightforward to reprocess the entire database within a relatively short time (generally within 1 day). for existing processed data, we will reprocess the data and release updated versions of the data for all major revisions of the fsl package, once they have been vetted and ensured to work properly with our processing stream. we do not expect substantial changes in results across major versions of the software, but if any such differences are noticed, we will first discuss with the software developers to ensure that they do not reflect software problems. if they are determined to be true methodological differences, then these differences will be described on the web site."
"the aim of feature extraction is to decrease a variably sized image to a packed set of visual features. typically, image classification models are built by applying strong visual feature extraction techniques. while they depend on either conventional computer vision approaches, (e.g. filter based methods, histogram techniques, etc.) or on deep learning approaches, they all have the same goal: extract features from the input of image which are appropriate for the task, and apply these features in a classification process to define the class of the image. in object detection systems, a convolutional feature extractor is a base network that is implemented in the input data to obtain advanced features. the collection of the feature extractor is supposed to be highly significant, which is because of the number and types of layers, the number of parameters and other characteristics, which immediately influence the www.ijacsa.thesai.org execution of the detector. in this paper, two feature extractors which have been selected are the most used in the area of computer vision."
"intersection over union (iou) is an evaluation metric applied to estimate the accuracy of an object detector on a relevant database as shown in fig.3 . concerning to evaluate database as shown in fig.3 . concerning to evaluate the model on the function of object localization, it must determine how strong the model predicted the location of the object as shown fig.4 . ordinarily, this is accomplished including drawing a bounding box around the object of interest. the localization function is normally evaluated on the intersection over union threshold (iou). the tensorflow object detection api usages \" [cit] metrics\" [cit] location an estimated instance is described as a tp while intersection over union (iou) is above 50% [cit] (eq.10): (10) one object can be associated with only one bounding box; however, if some bounding boxes are predicted for an object, one is regarded as tp and the others fp. however, if an object is without a predicted bounding box, which is associated with it, then it is recognized as an fn."
"deep learning networks have been developed and moved to a high level of sophistication in detection applications when a microsoft research released for a deep residual networks (resnets) [cit] . the resnets that are above 100-layer deep have demonstrated state-of-the-art accuracy for challenging recognition functions on imagenet [cit] and ms coco [cit] competitions that included object detection, image classification, and semantic segmentation. the validity of resnets has been confirmed by multiple visual recognition applications and by non-visual applications including speech and language. deep network is of significant interest in neural network architectures, however, deeper networks are further challenging to train. the residual learning structure helps the training of these networks and allows them to be deeper activate performance in both visual and non-visual functions. in the deeper network, the additional layers much better approximate the mapping than its conventional counterpart and decrease the error."
"as the training process progresses, the expectation is to reduce total loss (errors) to its possible minimum (about a value of 1 or less). by observing the tensorboard graphs for total loss for faster r-cnn-resent and ssd-mobilenet models (see fig.6 ), it should be possible to get an idea of when the training process is complete (total loss decrease with further iterations/steps(epochs)). the parameter num_steps defines how many training steps they will run before stopping. this number actually depends on the size of the dataset along with how long the user wants to perform the training of the model. the utilized metric for achievement is a mean average precision (map) which is a single number used to summarize the area under the precision-recall curve. the map is a measure of how well the model generates a bounding box that has at least a 50% overlap with the ground truth bounding box in the test dataset. the map value reached to higher confidence at 0.5iou (see fig.7 ) for each action class of both pre-trained models. the higher the map values the higher the detection accuracy (the higher the better). however, the ssdmobilenet takes a long time to reach the high map value compared to map's faster-r-cnn-resnet time. the classification loss curve in fig.8 indicates the validation of human action class which is classified and matched with the previous trained class. as the values of the classification loss decrease to zero, it shows that the classification accuracy is high and the efficiency of the detector performance becomes more advanced. the fig.9 (a,b) displays the results for both models where there are seven classification loss curves corresponding to each human action health-related class. generally, all actions have consistent decreasing classification loss values, which give the power of the model performance in the classification of each video frame of the similar class type. as for the localization loss curve in fig.10, it describes the predicate bounding box that matches with the ground-truth bounding box. as the loss value is reduced, less error will be present in the action detection and the intersection over union will be high, which all indicates that the detection of the action is in the right direction. the smoothed l1 loss is used for localization and is weighted."
"data sharing has revolutionized other areas of biomedical science, and we believe that it has the potential to do the same for cognitive neuroscience. the openfmri data sharing project has developed the infrastructure for the sharing of task-based fmri datasets, and has begun making datasets openly available online. we are optimistic that this project will help encourage widespread voluntary data sharing by providing a powerful resource that makes sharing as straightforward as possible. preliminary analyses of the database have confirmed the ability to classify mental states across individuals, as well as demonstrating the novel ability to classify the identity of individual subjects from their fmri patterns. in addition, multivariate analyses provide new glimpses into the multidimensional relations between mental function and brain function. we foresee many additional insights arising from these data as the database grows and other novel analysis methods are applied to the data."
"to demonstrate the potential utility of mega-analysis on a large task-based fmri dataset, below we present results from initial analyses of a subset of the current database [cit] . the datasets, tasks, and contrasts included in this analysis are listed in table 2 . three datasets from the current database were excluded from this analysis, due to small sample size (ds105) or exact replication of tasks and subjects from other datasets (ds006b and ds017b). most of the datasets include multiple runs, for a total of 479 images from 337 unique subjects for run 1, and 429 images from 317 unique subjects for run 2. all analyses reported below were performed on spatially normalized z statistic maps obtained for the contrast of interest using the processing stream described in table 1 . the data used to generate all for the foregoing analyses and figures are available from the openfmri web site at http://openfmri.org/dataset/ds900001, and the code used to perform all analyses is available at http:// github.com/poldrack/openfmri. thus, anyone should be able to run these same analyses on their own system in order to reproduce the results reported here."
"the paper is organized as follows. section ii discusses the related works. details of object detection technique is presented in section iii. section iv explains tensorflow object detection api technique. the feature extraction algorithm defined with details in section v. section vii presents the proposed technique that includes the experimental settings, training and evaluation processes and detection approach with experimental results. discussion of the significance of this work is discussed in section ix. finally, section x concludes this work."
"propagation-based phase-contrast tomography has become a valuable tool for visualization of three-dimensional biological samples, due to its high sensitivity and its potential in providing increased contrast between materials with similar absorption properties. we present a statistical iterative reconstruction algorithm for this imaging technique in the near-field regime. under the assumption of a single material, the propagation of the x-ray wavefield-relying on the transport-ofintensity equation-is made an integral part of the tomographic reconstruction problem. with a statistical approach acting directly on the measured intensities, we find an unconstrained nonlinear optimization formulation whose solution yields the three-dimensional distribution of the sample. this formulation not only omits the intermediate step of retrieving the projected thicknesses but also takes the statistical properties of the measurements into account and incorporates prior knowledge about the sample in the form of regularization techniques. we show some advantages of this integrated approach compared to two-step approaches on data obtained using a commercially available x-ray micro-tomography system. in particular, we address one of the most considerable challenges of the imaging technique, namely, the artifacts arising from samples containing highly absorbing features. with the use of statistical weights in our noise model, we can account for these materials and recover features in the vicinity of the highly absorbing features that are lost in the conventional two-step approaches. in addition, the statistical modeling of our reconstruction approach will prove particularly beneficial in the ongoing transition of this imaging technique from synchrotron facilities to laboratory setups. [cit] author(s). all article content, except where otherwise noted, is licensed under a creative commons attribution (cc by) license (http:// creativecommons.org/licenses/by/4.0/). https://doi.org/10.1063/1.4990387"
"the statistical weights reflect the reliability of our model. in the presented tomographic case, it is violated for the tungsten thread. thereby, the tungsten thread is selected by thresholding the flat-field corrected intensity at values lower than 0.5. afterwards, this mask is slightly dilated in order to cover the areas of the edge enhancement. furthermore, to avoid effects at the borders, regions within 15 pixels of the borders are also masked. the statistical weights are set to zero within the masked areas. finally, in order not to bias the comparison of the different reconstruction techniques to their statistical properties, additional statistical weighting of the measurements was omitted by setting weights outside of the masked area to one."
"in the case of propagation-based phase-contrast tomography, the aim is not to recover the traces under each tomographic angle but to directly recover the three-dimensional distribution of the material. in order to account for the projection process, we discretize eq. (4) and extend it to multiple angles"
"), regardless of the observed read counts ( figure s5 ) or over-dispersion variance ( figure s6 ). in addition, we also notice that the p-values computed from malax have a strong enrichment near 1, more so with increasing sample sizes (݊ 2 0 0"
"our first set of simulations was performed to evaluate the effectiveness of pqlseq in terms of heritability estimation. to do so, we simulated bsseq data or rnaseq data with a fixed heritability value that equals to either 0.1 or 0.3. we considered sample sizes ranging from"
"to overcome the artifacts introduced in the analytical reconstruction approach, one can make use of the additional weights w h i in iterative reconstruction techniques to mask out regions on the projections where the homogeneity assumption is violated, namely, in regions where the tungsten thread is present. the calculation of these weights is discussed in sec. vii. the upper parts of fig. 3 illustrate the position of the mask noting that due to the smearing of the pag algorithm, it does not cover the artifacts on the traces entirely. these weights are applied according to eq. (15) for stir and eq. (17) for sir."
", at an fdr of 10%, we identified 183 dm sites (whole significant sites) with pqlseq, 175 with macau, and 156 with malax. the reduced performance of malax in large samples as compared with other glmm methods presumably is due to the unusual enrichment of malax p-values near one in large samples ( figure s7 ). the power comparison results also suggest that, despite the difference in type i error control, both pqlseq and macau rank genes or sites similarly well in terms of their differential expression or differential methylation evidence, thus producing similar power at a fixed fdr for differential analysis."
", where pve values were set to be 15%, 25%, or 35% to represent different effect sizes. finally, we simulated the methylated read counts based on a binomial distribution with a rate parameter determined by the total read counts . we then varied one parameter at a time to generate different scenarios. in each scenario, conditional on the sample size, total read counts etc., we performed 10 simulation replicates, each consisting of 10,000 sites."
"then, we notice that the exponentials in (17) are less or equal to unity on [ 0, β ] . hence, i 2 is bounded from above by a sum of two integrals of polynomials which can easily be calculated, to yield the following inequality:"
"pqlseq fits two forms of glmm that include the poisson mixed model (pmm) for modeling rnaseq data and the binomial mixed model (bmm) for modeling bsseq data. these two different types of sequencing data have different data structures. specifically, rnaseq studies collect one read count for each gene as a measurement of its expression level. in contrast, bsseq studies collect two read counts for each cpg siteone methylated count and one total count -as a measurement of the methylation level at the cpg site. the ratio between these two counts in the bsseq data represents approximately the methylation proportion of the given cpg site. therefore, we use two different types of glmm to model rnaseq and bsseq data. for both data types, we examine one genomic unit (i.e. gene or cpg site) at a time."
"evaluating the derivative along z, assuming that the contact-and detector-planes are sufficiently close to each other such that the intensity evolves over this distance in a way that is linear in z, 11 yields"
"in the paper, we have primarily focused on illustrating pqlseq for simple glmms with two-variance components: one component models sample non-independence due to the covariance matrix, while the other component models independent over-dispersion. however, pqlseq can easily accommodate multiple variance components. indeed, we have implemented pqlseq so that it can fit glmms with multiple variance components. glmms with multiple variance components can be particularly useful when there are multiple sources of variance that need to be accounted for [cit] . for example, one can use multiple variance components to account for population stratification, family relatedness as well as independent environmental variation. alternatively, one can use multiple variance components to account for sample nonindependence due to cell composition heterogeneity across samples, batch effects as well as independent noise. exploring the use of glmm with multiple variance components in various genomic sequencing studies is an interesting future direction."
"as a remark, although implicitly assumed in the derivation, the requirement for monochromatic x-rays is not particularly strict for this imaging technique as a change in energy only slightly alters the fringes. in our case, we use a commercially available polychromatic laboratory source to validate our reconstruction algorithms. moreover, our approach can be applied to more strongly absorbing objects without any modifications by changing the corresponding values for the complex refractive index."
"a transverse and a longitudinal slice recovered by the fbp acting on the projected thicknesses recovered by pag are depicted in fig. 4 . the areas where tungsten is present have large values and yield severe artifacts. moreover, these areas are smeared out covering features in their vicinity."
"in the following, the details of the implementation of the derived algorithms are presented along with the conventional tomographic reconstruction approaches and regularization techniques used for comparison."
). the effectiveness of pqlseq in controlling for type i error in moderate to large samples suggest that pqlseq is particularly well suited for differential analysis in large sequencing data.
"our first algorithm is based on the propagation of the x-rays. we further manipulate eq. (7) by neglecting the cross-term zd½r ? exp fàltðr ? þg á r ? tðr ? þ, where the squared brackets indicate the scope of the r ? operator, by assuming that at a particular position in the wavefield, the product of the intensity gradient and the phase gradient is comparably small, and end up with the following simplified model:"
"we have illustrated the benefits of using pqlseq to perform glmm analysis on rna sequencing and bisulfite sequencing data. we have shown that pqlseq is the only method currently available that can produce unbiased heritability estimates for sequencing count data. in addition, pqlseq is well suited for differential analysis in large sequencing studies, providing calibrated type i error control and more power than standard lmm methods. pqlseq is implemented as an r software package with parallel computing capacity, can accommodate both binary and continuous predictor variables, and can incorporate various biological or technical covariates as fixed effects. with simulations and real data applications, we have shown that pqlseq is a useful and efficient tool for analyzing genomic sequencing data sets that are becoming increasingly common and increasingly large."
"where r ? describes the coordinates orthogonal to the propagation direction. knowing the material's specific attenuation coefficient l, the real part of the deviation of the complex refractive index from unity d, as well as the wave number k (dependent solely on the energy of the x-rays), we can recover the intensity and the phase from the trace t(r ? ) that corresponds to the projected thickness of the sample along the x-ray direction, namely"
"in terms of scalability, existing algorithms to fit glmm are generally computationally expensive due to an intractable high-dimensional integral in the glmm likelihood [cit] . for example, the frequentist method mixed model association for count data via data augmentation algorithm (macau) relies on a bayesian strategy of markov chain monte carlo (mcmc) sampling to numerically approximate the integration in glmm. however, though accurate, mcmc based strategy is computationally inefficient for large sample size: it takes macau several days to analyze moderate-sized rnaseq or bsseq data with a few hundred individuals. to overcome the computational bottleneck of mcmc-based approaches, recent studies have started to explore alternative approximation strategies to fit glmm. for example, in bisulfite sequencing studies [cit], the mixed model association via a laplace approximation algorithm (malax) relies on a laplace approximation to improve computational speed. however, the computational improvement of malax over macau is relatively marginal (approximately two-folds). in non-genomics sequencing settings, a score test based approximate algorithm has also been recently developed to apply glmm to analyze large-scale gwass [cit] . however, score test based strategy is not well-suited for genomic study setting where the null model varies for every genomic unit tested (e.g. gene or cpg site). therefore, scaling up glmm to thousands of individuals remains a challenging task."
"in terms of accuracy, existing algorithms to fit glmm rely on different approximations and these different approximations may work well in different settings. for example, in the field of biostatistics, it has been shown that while some glmm algorithms may produce accurate p-values for differential analysis tasks in small studies, other glmm algorithms rely on asymptotic properties of the likelihood and can only produce accurate p-values when sample size is relatively large [cit] . therefore, exploring the behavior of different glmm algorithms in different settings will be informative for practitioners. in addition, as we will show below, existing glmm algorithms in genomic sequencing studies cannot yet provide accurate heritability estimates."
"the published rnaseq data was collected from lymphoblastoid cell lines (lcls) of 431 individuals from the hutterites population in south dakota, which is an isolated founder population [cit] (50bp single end reads) in indexed pools of 12. reads were trimmed for adaptors using cutadapt (reads less than 5 bp discarded) then remapped to hg19 using bowtie indexed with gencode version 19 gene annotations [cit] . to remove mapping bias, autosomal reads were processed through wasp [cit] . gene counts were quantified using htseq-count [cit] and verifybamid was used to identify sample swaps [cit] . following these mapping and quality control steps, we obtained expression count measurements for 23,367 genes. we kept genes that have read counts greater than five in at least two individuals to focus on a final set of 17,312 genes. we also used the hutterites pedigree information to compute the kinship coefficients between pairs of individuals and used them as the matrix in the model. we then fitted a pmm for each gene in turn without any covariates to estimate gene expression heritability. for de analysis, we used sex as the predictor variable (i.e. male vs female) to identify sex-associated genes. to compare performance among different methods for de analysis, we permuted phenotype sex 20 times to obtain a null distribution. we used the null distribution to estimate the false discovery rate (fdr). finally, to explore the influence of batch effects for heritability estimates in pqlseq, we extracted the top principal components (pcs) from the gene expression matrix and treated them as covariates in the model. we considered including a different number of top gene expression pcs that range from 2 to 200."
"we provide a brief overview of the pqlseq method in the materials and methods section, with algorithmic details available in the supplementary text. briefly, pqlseq fits a poisson mixed model (pmm) for modeling rnaseq data and a binomial mixed model (bmm) for modeling bsseq data. in both data types, pqlseq examines one genomic unit (i.e. a gene or a cpg site) at a time, produces an estimated heritability ݄ ଶ, and in the case of differential analysis, also computes a p-value for testing the genomic unit association with a predictor variable of interest, where the predictor variable can be either continuous or discrete."
". we also varied mean observed read count values (ߤ) and over-dispersion variance values ( ߪ ଶ ) from low, moderate, to high, in order to examine how these parameters impact type i error control. in these simulations, we examined one genomic unit (i.e. site or gene) at a time and computed p-values using different methods."
"in the following, we demonstrate some benefits of our fully iterative algorithm stir compared to previous implementations, such as combinations of pag with fbp or pag together with sir. the sample consists of a perfusion tube made of polyethylene (pe) as well as acrylic glass (pmma) spheres that have been crushed with pliers. in addition, a thread made of tungsten is added. this example is motivated by limitations of the discussed imaging techniques when the single material assumption is not met. this includes for instance resolving soft-tissue features in the vicinity of bone structures. the projections are obtained at an effective propagation distance of 28 mm with a source-to-sample distance of 45 mm and a sample-to-detector distance of 74 mm. the effective voxel size corresponds to 2.56 lm. in fig. 3(a), the flat-field corrected intensity at the detector plane for the first angle is depicted. as anticipated, pe and pmma have similar absorption and phase-shifting properties. in contrast, the tungsten thread is highly absorbing. figure 3 (b) shows the trace of fig. 3(a) recovered by pag. the parameters for d and l are chosen for pmma. therefore, pe and pmma are accurately recovered. the difficulties in whole-sample reconstruction arise from the trace of the highly absorbing tungsten where the assumed absorption and phase shifting properties are not met. this means that the trace of the tungsten is smeared out, covering features located in the vicinity that are lost from now on, which results in a significant drawback for two-step reconstruction techniques. fig. 3 . intensity and trace of the first projection. in (a), the flat-field corrected intensity for the first angle is shown. the associated recovered trace using pag is depicted in (b). in both images, the identical mask, which completely covers the tungsten wire on the projection during the reconstruction, is shown only on the upper half of the images."
"we validated the approximations of our approach before comparing the proposed algorithm stir with widely used two-step approaches, namely, recovering the traces with pag before recovering the spatial distribution of the material by fbp approaches or sir techniques for tomography. we find that our approach allows for improved reconstruction of features in the vicinity of highly absorbing objects compared to two-step approaches. furthermore, stir is well suited as a refinement step to the conventional reconstruction approach to improve the image quality with statistical modeling, no intermediate phase-retrieval, and the use of regularization techniques."
"minimization according to eq. (12) for the itie algorithm leads to the trace depicted in fig. 2(b), which coincides well with the properties of the sample. as the initial guess, a plane of zeros is used. figure 2 (c) compares the result with the pag algorithm for the central row. in blue the profile of the according row of fig. 2(b) is plotted. using the pag algorithm as in eq. (8) yields the dashed line-plot in orange for the central row. in green the difference of the two line profiles is shown."
"to illustrate the effects of the different reconstruction techniques, the regions marked in the following comparison refers to the transverse as well as longitudinal slices. the mean values of the pmma spheres do not change significantly over the different reconstruction techniques, making the iterative approaches as quantitative as the conventional approach. the main differences between the various reconstruction algorithms arise from the tungsten thread. as mentioned before, the fbp reconstruction of the traces leads to the fact that the tungsten thread is smeared out, rendering the features in the vicinity useless and resulting in streak-like artifacts throughout the whole image. replacing the fbp with a sir reconstruction, the areas corresponding to regions that are masked out on the traces contain hardly any signal in the reconstructed volume. however, due to the smearing of the pag algorithm, the areas in the vicinity of the tungsten thread cannot be correctly recovered and artifacts arise from these regions. finally, the one-step stir approach can circumvent these problems by masking out the tungsten thread entirely on the projections by using the same mask as before. consequently, features in the vicinity are recovered and the volume does not suffer from severe streak artifacts. furthermore, while the fbp reconstruction cannot make use of any regularization techniques, the edges of the sample are not as sharp and the noise level is higher than in the iterative approaches that incorporate total-variation regularization. unfortunately, the edge-preserving properties of this regularization technique emphasize the streak artifacts of the sir method. however, the stir method most strongly benefits from the noise reducing and edge preserving capability of the regularization. in conclusion, although the stir method provides improved image quality and is able to recover features in the vicinity of highly absorbing objects, there are still small residual streak artifacts arising from the tungsten thread leaving space for further improvements. the supplementary material provides a difference map of the transverse slice between the fbp and stir reconstruction and discusses the differences in more detail."
"and/or estimating the heritability parameter ݄ ଶ . both tasks require the development of computational algorithms to fit glmm. unfortunately, fitting glmm is notoriously difficult, as the glmm likelihood consists of an n-dimensional integral that cannot be solved analytically. to overcome the high dimensional integral and enable scalable estimation and inference with glmm, we develop an approximate fitting method based on the penalized quasi-likelihood (pql) approach [cit] that is also recently applied to gwas settings [cit], thereby alleviating much of the computational burden associated with glmm. with the pseudo-data, we can perform inference and update parameters using the standard average information (ai) algorithm for lmms [cit] . by iterating between the approximation step of obtaining the pseudo-data and the inference step of updating the parameter estimates via the ai algorithm, the pql approach allows us to perform inference in a computationally efficient fashion. to improve computational speed further, we also take advantage of the parallel computing environment readily available in modern desktop computers nowadays and implement our method with multiple-thread computing capability using rcpp. we refer to our method as the penalized quasilikelihood for sequencing count data (pqlseq), which is freely available as an r package at www.xzlab.org/software.html and https://cran.rproject.org/web/packages/pqlseq/index .html."
"in this section, the convergence of the algorithm is graphically illustrated. the objective of this work is to show that the bound µ k derived in section iv under some rather stringent hypotheses is in fact a good approximation of e (ν k ). furthermore, we aim to show that our hypotheses can be relaxed and our results extended to more general cases."
"now consider the double integral i 2 . by breaking the integral overŷ k into positive and negative parts and noting that t is odd, the following relation can be established:"
"the actual measurements in the detector plane are denoted byỹ i . the first part of this equation is denoted as the data-term. due to our noise model, it penalizes the quadratic differences of our forward model to the actual measurements. to account for the fact that different datapoints vary in their significance, it is reasonable to set the statistical weights w i to the inverse variance of the measured data. in practice, the variance is directly estimated from the measured intensity. the second part of eq. (11) is referred to as the regularization-term. the regularizer r 0 acts on the traces and incorporates prior knowledge on the sample. usually, the regularizer penalizes solutions where noise is present. the coupling parameter between the data-term and the regularization-term is denoted by k and has to be chosen accordingly."
"generalized linear mixed model (glmm) has recently emerged as a powerful statistical tool for the analysis of high throughput genomics sequencing studies [cit] . the main application of glmm in these genomic sequencing studies is so far restricted to differential analysis, which aims to identify genomic units (e.g. genes or cpg sites) that are associated with a predictor of interest (e.g. disease status, treatment, environmental covariates, or genotypes). common analysis examples include differential expression analysis in rna sequencing (rnaseq) studies [cit] and differential methylation analysis in bisulfite sequencing (bsseq) studies [cit] . effective differential analysis with sequencing data often requires statistical methods to both account for the count nature of sequencing data and effectively control for sample non-independence -a common phenomenon in sequencing studies caused by individual relatedness, population structure, or hidden confounding factors [cit] . glmm accomplishes both tasks by relying on exponential family distributions to directly model sequencing count data and by introducing random effects terms to account for sample non-independence. in effect, glmm generalizes both the linear mixed model (lmm) that has been widely used to control for sample non-independence in association studies [cit], and over-dispersed count models (e.g. negative-binomial, beta-binomial) that have been widely used for differential analysis in sequencing studies [cit] . by combining the benefits of the two commonly used methods, glmm properly controls type i error and improves power for differential analysis [cit] ."
"although the calculations which follow can be conducted with other measurement noise distributions, bounded or not, they are made much simpler by assuming a distribution with a compact support."
"while the existing applications of glmm in genomic sequencing studies have been primarily restricted to differential analysis, the similarity between glmm and lmm begs the question on whether glmm can also be applied to estimate heritability for sequencing count data. heritability measures the proportion of phenotypic variance explained by genetics and is an important quantity that facilitates the understanding the genetic basis of phenotypic variation. the standard tool for estimating heritability is lmm, which has long been applied for heritability estimation [cit] or snp heritability estimation [cit] for various quantitative traits in the setting of genome-wide association studies (gwass). in the setting of genomics studies, lmm has also been recently applied to estimate gene expression heritability [cit], methylation level heritability [cit], as well as various other molecular traits heritability [cit] . however, lmm is specifically designed for analyzing quantitative traits. in genomic sequencing studies, the application of lmm requires a priori transformation of the count data to continuous data before heritability estimation [cit] . transforming sequencing count data may fail to properly account for the sampling noise from the underlying count generating process, and may inappropriately attribute such noise to independent environmental variation --thus running the risk of overestimating environmental variance and subsequently underestimating heritability. in contrast, glmm directly models count data, and as will be shown in the present study, has the potential to become a more accurate alternative than lmm for heritability estimation in genomic sequencing studies."
"in the following, we outline the mathematical formulations and assumptions around the image formation process that builds the foundations of our iterative algorithms. in general, the image formation process can be divided into two parts: the interaction of the incoming x-rays with the sample and the free space propagation to the detector. first of all, an expression is derived that recovers the wavefield in the contact-plane directly behind the sample. second, the relationship between the measured intensity at the detector-plane and the wavefield in the contact-plane is explained."
"as opposed to conventional x-ray absorption imaging, which relies solely on the attenuation of the x-rays in matter, phase-contrast imaging (pci) is sensitive to x-ray phase shifts. this technique is becoming more and more important in laboratory and preclinical studies, yielding distinct advantages for the visualization of weakly absorbing details that are common in biological and medical samples. by extending pci to computed tomography (ct), 1 it has become a valuable tool for three-dimensional visualization of thick and complex samples due to its high sensitivity and its potential in providing increased contrast between materials of similar absorption properties."
"we compared four different methods (pqlseq, macau, gemma, and malax) in the bsseq based simulations, and compared three different methods (pqlseq, macau and gemma) in the rnaseq based simulations as malax is only applicable for bsseq data. for gemma, we normalized data following previous recommendations [cit] . specifically, for rnaseq data, for each gene in turn, we divided the number of read counts mapped to the gene by the total read depth, and quantile transformed the normalized data to a standard normal distribution. for bsseq data, we used \"m\" value transformation following [cit] by dividing the number of methylated reads by the number of unmethylated reads followed by a log2-transformation. the normalized data is"
"), regardless of heritability values ( figure s9 ), pve ( figure s10 ), read counts ( figure s11 ) and over-dispersion variance ( figure s12 ). the similarity in power between macau and malax in bsseq based simulations are consistent with the previous study [cit] . however, macau/pqlseq can be slightly more powerful than malax when sample size is large (݊ 2 0 0"
"; figure s7 ). finally, we found that the p-values from pqlseq are highly correlated with that from macau across a range of sample sizes ‫ݎ(‬ 2 varies from 0.96 to 0.99; figure s8 ), with increasingly large correlation for increasingly large sample size."
"where the laplace operator is rewritten as a matrix b ik using the five-point stencil finitedifference method. from now on, the indices i, k run over the total number of pixels in the corresponding planes. finally, we can establish a noise model that accounts for the statistical properties of our measurements. for simplicity, we restrict ourselves to a gaussian noise model although other noise models could be used as well. however, a pure poisson model does not hold for ccd cameras, as commonly used in x-ray phase-contrast imaging. in addition, a poisson model is well approximated by a gaussian model for the number of counts observed in any reasonable imaging situation. we establish a cost-function of the following form:"
"which now runs over all pixels and all tomographic angles. the measurements under different tomographic angles are denoted byỹ h i . the regularizer r now acts on the volume. now, we minimize eq. (15) by an iterative method for solving unconstrained nonlinear optimization problems to end up with the optimal distribution of the material t ã coinciding with our measurements according to their noise statistics and prior knowledge, written as"
"in the following, we use total-variation regularization 37 that relies on the assumption that our volume is piecewise constant and only considers the discrepancies between neighboring voxels, which can be written as"
"as we want to omit any intermediate steps, we insert eq. (13) into our discrete model for propagation, eq. (10). thus, our forward model describes the expected intensity distribution on the detector for all angles depending solely on the distribution of the material"
"which is the basis of our iterative reconstruction algorithm. as a first step to obtain our forward-model, which relates the quantity we are interested in to our measurements, eq. (9) is discretized, merging the coordinates orthogonal to the propagation direction into one dimension as an intermediate step, we compare this approach with a more flexible iterative algorithm (itie) to achieve this goal. the volume is recovered by filtered back-projection (fbp) or more advanced statistical iterative reconstruction techniques (sir). finally, we introduce a transport-of-intensity based iterative reconstruction algorithm (stir) that recovers the volume directly from the measured projections. those algorithms shown in orange are described for the first time in this paper."
", at an fdr of 10%, we identified 230 dm sites with pqlseq, 226 with macau, but only 219 with malax. the power of malax decreases, however, with higher heritability. for example, when"
"additionally total-variation regularization is used according to eq. (18) . the strength k of the regularizer is chosen in a way to make the noise level for both iterative reconstruction techniques compatible. due to the variability of the artifacts, the parameters are chosen empirically rather than by defining regions, where well-defined noise characteristics have to be fulfilled. the volumes to reconstruct are initialized with zeros in the iterative approaches to make the outcome independent of the starting conditions. this however increases the number of iterations significantly until a stable state is found. thereby, 100 and 800 l-bfgs iterations are chosen for sir and stir, respectively, ensuring that a stable state is reached."
"our second set of simulations was performed to evaluate the effectiveness of pqlseq in controlling for type i error for differential analysis under sample non-independence. sample non-independence is a common phenomenon in sequencing studies and can be caused by individual relatedness, population structure, or hidden confounding factors [cit] . failing to properly control for sample non-independence can lead to inflated type i errors [cit] . to examine type i error control of different methods, we performed null simulations and simulated outcome variables in terms of methylation levels or gene expression levels that are independent of the predictor variable of interest . however, these outcome values are correlated among individuals/samples, with correlation determined by a heritability value of either 0.1 or 0.3. we considered sample sizes ranging from"
"although successfully handling highly absorbing objects, our algorithm still imposes the homogeneity assumption for the remaining materials. recent multi-distance algorithms successfully implemented at synchrotron facilities can relax this assumption to allow for the reconstruction of heterogeneous objects. 16 however, our approach is designed for single-distance methods with laboratory sources in mind, where the statistical nature of our algorithm benefits the most. future work could investigate how our model can be extended to allow for more than one material, using, for instance, concepts used in iterative multi-distance algorithms, for example, sophisticated regularization techniques."
recovering the three-dimensional distribution of the material from its traces according to eq. (13) is analogous to the reconstruction of the absorption coefficients from line-integrals in absorption tomography. it can be performed by fbp or sir techniques.
"as our results focus on the validation of our reconstruction techniques compared to wellknown approaches, we omit a detailed evaluation of the statistical nature of our algorithms in order not to bias our comparisons and hence only evaluate the benefits of using binary weights to emphasize the limitations of pag þ sir compared to stir. in principle, our one-step approach is capable of handling the noise properties directly from the projections with arbitrary noise models, which is a huge benefit compared to two-step approaches, where for an iterative tomographic reconstruction, the statistical weights have to be applied on the traces. with the recent advances in x-ray sources including liquid-metal jet tubes and compact synchrotrons, propagation-based pci is increasingly transferred from synchrotron facilities to laboratories. with the comparably small flux and high noise levels of laboratory sources, our statistical approach has the potential of improving image quality significantly, where a correct modeling of noise is crucial. furthermore, our versatile forward model can easily be extended to include, for instance, source and detector models directly into our reconstruction framework."
"thereby, the trace t(r ? ) is the projection denoted by the operator p of the three-dimensional distribution of the sample t(r) along the x-ray paths"
"in both bsseq and rnaseq based simulations, we found that all three glmm methods (pqlseq, macau and malax) are more powerful than lmm method (gemma) across a range of simulation settings. the higher power of glmm methods comes from their proper modeling of sequencing count data as demonstrated in previous studies [cit] . among the different glmm methods, we found that the performance of pqlseq, macau and malax are almost identical to each other when sample size is small (݊ 3 0 0"
"where we denote the intensity part of the wavefunction in the contact plane as i(r ? ) and accordingly its phase as /(r ? ). the intensity in the detector plane is denoted byĩðr ? þ. if we again assume a homogeneous object, we can use eqs. (2) and (3) to make the intensity distribution in the detector-planeĩðr ? þ solely dependent on the trace t(r ? ) of the object"
the goal is to minimize the cost-function l in order to find the optimal trace t ã coinciding well with the measurements according to the noise statistics and prior knowledge of the regularizer. this can be written as
"one drawback is the computational cost of stir. this cost is not necessarily a consequence of the more sophisticated forward model compared to absorption ct. the projection operations remain the most time-consuming operations that are also present in sir, and the additional overhead of the operations related to the propagation of the x-rays, namely, the laplacian operations, is small. for now, adapting the solver to minimize our non-linear model in fewer iterations and making more explicit statements about the optimization problem is a challenge that could be addressed in the future. however, the number of iterations relies heavily on the initial guess of the iterative algorithms. for instance, for absorption ct, one would use the fbp as the start image; consequently, pag þ fbp would be a decent start image for propagation-based phase-contrast ct by stir and would significantly improve the speed of convergence. in order not to bias the comparison between the different reconstruction techniques by such an initial guess, we accepted the increase in computational time to show that our algorithms converge, regardless of the initial guess, even from an array of zeros."
". we varied mean observed read count values (ߤ) and over-dispersion variance values (ߪ ଶ ) from low, moderate, to high, in order to examine how these parameters impact heritability estimation accuracy. heritability estimates from different methods for different sample sizes in the bsseq based simulations are shown in figures 1a and 1b . heritability estimates for rnaseq based simulations are shown in figures 1c and 1d . heritability estimates from different methods for increasing ߤ are shown in figure s1 and estimates for increasing ߪ ଶ are shown in figure s2 . across all simulation settings, pqlseq is the only method that produces approximately unbiased heritability estimates. in contrast, lmm implemented in gemma consistently produces downward biased estimates across different sample sizes, and more so for high heritability values (i.e., 0.3) than for low heritability values (i.e., 0.1). the downward bias of lmm presumably stems from the fact that lmm fails to model the count data generating process and inappropriately attributes the count generating noise to environmental errors. increasing sample size (figure 1 ) or over-dispersion variance ( figure s2 ) does not alleviate the downward bias of gemma. however, increasing the mean observed read counts alleviates the lmm estimation bias in rnaseq data ( figure s1 ), presumably because the normal approximation in lmm becomes appropriate with high read counts. on the other hand, macau produces consistently upward biased estimates across sample sizes, and more so for low heritability values (i.e. 0.1) than for high heritability values (i.e. 0.3). increasing the observed read counts ( figure s1 ) or over-dispersion variance ( figure s2 ) does not alleviate the upward bias of macau. the upward bias in macau presumably stems from its inaccurate latent variable approximation algorithm in small samples. indeed, with increasing sample size (e.g."
"and solved using iterative methods for solving unconstrained nonlinear optimization problems. due to the fact that this approach minimizes the simplified tie iteratively, we refer to it as itie."
"both the above two applications of glmm for differential analysis and heritability estimation require accurate and scalable inference algorithms to accommodate the increasingly large genomic sequencing studies that are being collected today. indeed, several genomic projects have already collected sequencing data on hundreds of individuals [cit], and the recent topmed omics sequencing project further aims to sequence a few thousands of individuals in the next couple of years. compared with small sample studies, large genomic sequencing studies are better powered and more reproducible, and are thus becoming increasingly common in genomics. in addition, large-scale population sequencing studies pave ways for accurate estimation of heritability for various molecular traits. unfortunately, existing algorithms for fitting glmm in genomic sequencing studies are not scalable. in addition, as will be shown in the present study, existing glmm algorithms do not always produce calibrated p-values for differential analysis nor accurate heritability estimates."
"in this paper, we analyze the convergence of limbo in a more general context: we suppose an unknown measurement noise is present at the input of the quantizer and study the influence of this noise on the convergence of the method. more specifically, the convergence rate of the method is investigated and a lower bound of the expected value of the correlation coefficient between the nominal and the estimated system parameters is found and expressed as a function of the variance of the measurement noise. it is shown that the derived lower bound can be safely used as an accurate prediction of the expected value of the correlation coefficient. the structure of the article is the following. in section ii, the system and its model are introduced. in section iii, the limbo algorithm is derived under its general form. then, the convergence of the proposed algorithm is studied in section iv and graphically illustrated in section v. finally, concluding remarks and perspectives are given in section vi."
"over the past decades, microfabrication of electronic devices such as micro-electro-mechanical systems (mems) has considerably developed. as their characteristic dimensions become smaller, these devices become increasingly afflicted with dispersion and become increasingly sensitive to changes in their operating conditions. typical sources of dispersions and uncertainty are variations in the fabrication process or environmental disturbances such as temperature, pressure and humidity fluctuations [cit] . it is then usually not possible to guarantee a priori that a given device will work properly under all operating conditions, and expensive tests must be run before the commercialization decision is made. to cut these costs, it is desirable to implement self-test (and selftuning) features such as parameter estimation routines, so that devices can compensate the variations in the fabrication process and adapt to changing conditions. unfortunately, standard identification methods based on parameter estimation [cit] do not lend themselves easily to implementation at a microscopic scale. their integration requires the implementation of high-resolution analog-todigital converters (adcs), which may require long design times and result in large silicon areas and increased power consumption. on the other hand, parameter estimation routines based on binary observations are very attractive since they only involve the integration of a 1-bit adc [cit], which requires minimal design and results in minimal silicon area and power consumption and, consequently, in minimal added costs."
"for bsseq based simulations, in each simulation replicate, we simulated methylation values for 10,000 sites and considered two general simulation settings. in the null settings, we simulated 10,000 non-differentially methylated (non-dm) sites to examine the methylation level heritability estimation accuracy and type i error control. in the alternative settings we simulated 1,000 dm sites and 9,000 non-dm sites to examine power. these non-dm or dm sites are simulated using the following procedure. specifically, for each site in turn, we simulated total read counts the two parameter values correspond to the median estimates from the published bsseq data [cit] . we then simulated the genetic random effects and the environmental random effects given a fixed heritability ݄ ଶ (0.1 or 0.3) and a fixed value of over-dispersion variance (ߪ ଶ ൌ 0.5, 1.2, or 2). again, the over-dispersion variance values correspond to the lower quartile, median, and upper quartile of the overdispersion variance inferred from the bsseq data [cit]"
"as an intermediate step, we establish an iterative tie-based algorithm (itie) that recovers the trace from the measured intensity in the near-field and includes the possibilities of statistical weighting of the measurements and regularization techniques on the traces. as our main finding, we present a statistical tie-based iterative reconstruction approach (stir) for reconstructing the distribution of the material from tomographic measurements acquired in the near-field regime. our algorithm accounts for the noise statistics of the measurements, recovers the threedimensional distribution of the sample without the intermediate step of recovering the traces, and makes use of regularization techniques. figure 1 illustrates this together with the common reconstruction approaches discussed previously."
". the value of β changes between 10 −3 and 10 0 . thus, the signal-to-noise ratio (snr) lies in average between 64.77 db, i.e. an almost absence of noise, and 4.77 db."
"analogous to the image formation, reconstructing the spatial distribution of the material usually requires two steps as illustrated in fig. 1 . the classic reconstruction process uses eq. (8) (pag, as the phase-retrieval component of the reconstruction) to retrieve the traces from the measured intensities obtained by illuminating the object under different angles. the threedimensional volume is then reconstructed by a filtered back-projection algorithm (fbp, the computed tomography component of the reconstruction) from these traces. however, for the reconstruction of the volume from the traces, one could also use more sophisticated sir algorithms that make use of prior knowledge via regularizing techniques as well as additional weights on the traces."
"with the availability of more powerful computers, we believe that our approach can prove to be beneficial for propagation-based phase contrast ct in the fields of medicine, biology, and manufacturing, using x-rays, visible light, electrons, or neutrons, in particular, for applications with low flux and high noise levels or when there are spatially close sample materials with quite different optical properties. our approach should work for all applications that until now rely on the pag algorithm for phase retrieval as for instance imaging of bones, lungs, and brains in biology or examining cracks or defects in materials science."
") heritability values. a heritability value of 0.1 corresponds approximately to the median heritability estimate in our real data analysis (see below), while a heritability value of 0.3 corresponds approximately to the 85th percentile of the expression heritability estimated in the real data [cit] ."
"evidently, in the case of tomography, a distinct wavefield is generated under each tomographic angle. the origin of the subsequent derivations is the transport-of-intensity equation (tie), 35 which describes the evolution of the x-ray wavefield intensity due to propagation. it can be derived by inserting eq. (1) in the paraxial helmholtz equation and isolating the imaginary part. 11 the tie has the following form:"
"to explore the influence of batch effects on heritability estimates from pqlseq, we follow the original study [cit] and extract the top principal components (pcs) from the gene expression matrix and treated them as covariates in the glmm. intuitively, removing batch effects would likely reduce measurement noise and subsequently improve heritability estimates. indeed, we found that the [cit] . the ݄ ଶ estimates from pqlseq gradually decrease after the plateau with the addition of more gene expression pcs, presumably because the later pcs do not necessary capture batch effects and may sometimes represent true biological/genetic effects."
"in the following, we validate the derived algorithms and compare them with common reconstruction approaches using datasets obtained at an x-ray micro-tomography system (xradia 500, carl zeiss, usa). the energy assumed in the following examples is 13 kev, as motivated in ref. 40 . the values for d and l for the different measurements are extracted from the xraylib library 41 according to the material, density, and energy."
"validation of the iterative phase-retrieval algorithms on a projection. the flat-field corrected intensity distribution of the 250 lm teflon plate at the detector plane is depicted in (a). the trace retrieved with the iterative phase-retrieval algorithm itie is shown in (b). this algorithm is compared with the pag algorithm in (c), where the line profiles of the center row of the traces for both algorithms can be seen in the upper part, while the lower part illustrates the differences between the two methods."
"the two methods coincide except for small deviations at the edges of the teflon plate, making this approach applicable for quantitative phase retrieval, as for instance performed in analyzing the air volume in lungs. 42 the small deviation can be explained by the neglection of the cross-terms in eq. (9) . in principle, these additional terms could be included in the iterative framework. furthermore, solving the equation in the spatial domain avoids wrapping artifacts at the borders due to the fact that the borders in the spatial domain can be mirrored or clamped to the edges. although the analytical algorithm is much faster, it lacks possibilities for weighting the projection, for instance, according to its statistical properties, masking of features and regularization techniques on the projected thicknesses."
"here, we develop a new method and a software tool to enable scalable and accurate inference with glmm for large-scale rnaseq and bsseq studies. we also perform extensive simulations to comprehensively evaluate our method together with several other existing methods in various simulation settings to give out recommendations on glmm based differential analysis and heritability estimation for practitioners. our newly developed method is based on the penalized quasi-likelihood (pql) approximation algorithm [cit], applies to glmm with two or more variance components, and with an efficient implementation, is capable of utilizing the parallel computing environment readily available in modern desktop computers. with the multiple-thread computing capability, our method can improve computation time for glmm analysis of large-scale genomic sequencing data by at least an order of magnitude, making glmm based differential analysis and heritability estimation applicable to hundreds or thousands of individuals. importantly, as we will show below, our method is currently the only available method that can produce unbiased heritability estimates for sequencing count data. we refer to our method as the penalized quasilikelihood for sequencing count data (pqlseq). with extensive simulations and comparison with lmm or other existing glmm methods, we illustrate both the advantage and limitation of our method. finally, we apply our method to analyze a large-scale rnaseq study in the hutterites."
"overall, our null simulation results show that different glmm methods can be either conservative (macau) or anti-conservative (pqlseq and malax) in small samples. however, all methods can produce calibrate type i error control in reasonably sized samples (pqlseq and malax for"
"where j now runs over all voxels of the three-dimensional volume and h indicates the respective projection angle. for simplicity, avoiding too much notation overhead, the three spatial dimensions of the volume are again merged into a single dimension. the first part of the image formation process is a linear operation and can therefore be described by a matrix multiplication, denoted by a h ij ."
"all algorithms are implemented on a heterogeneous computer consisting of multiple central processing units (cpus) and graphical processing units (gpus). the implementation of the cone-beam projection operation, as described in eq. (13), and its transpose operation for calculating the gradients of the cost-functions are described in ref. 38 . like the projection operations, regularization is also implemented on multiple gpus using opencl. the remaining calculations of the models as in eqs. (10) and (14) are implemented in python on multiple cpus. an optimization routine implemented as a python wrapper to the limited-memory broyden-fletchergoldfarb-shanno routine (l-bfgs) described in ref. 39 is used. thereby, the cost-functions as described in eqs. (11) and (15) and the corresponding gradients have to be calculated."
"to study the monotony of f (2) (ν k ), we check the sign of its derivative, for which an analytical expression can also be calculated:"
"the goal of this section is to define the five cluster types studied in the paper. these five include the three mentioned in the introduction plus two additional variations. to do so, we first introduce the five slicing operators upon which the cluster definitions are built. these range from fine-grained statement-level relationship using control and data dependences to the coarser relation based on function-level control-flow reachability. finally, we define the five cluster types."
"experimental study of stdp [cit] shows that different types of neurons, for example excitatory/inhibitory neurons in different parts of the brain, implement a surprisingly rich repertoire of learning kernels. it is reasonable to assume that the brain employs such learning mechanisms to implement different computational functions. in this respect, an interesting fourth class of models appropriate for studying stdp, which might be called functional models, aims to derive, or to justify, specific stdp learning kernels based on normative computational principles [cit] ."
"if the motions to be observed are not known a priori, then non-template based methods are required. the zerovelocity crossing (zvc) approach identifies points where the velocity value changes sign, i.e., when a joint segment changes directions [cit], as segment points. although a fast algorithm, zvc tends to over-segment, particularly with noisy data or as the number of dofs increases. since zvc methods do not consider motion templates, it is difficult to tell which crossing points can be safely ignored."
"this complexity raises an interesting question: are there approximations that provide better scalability without significant loss of precision? the answer appears to be \"yes.\" one recent example is static execute before (seb) [cit] . this coarser relation is based on function-level control-flow reachability. preliminary experiments indicated that the seb approximation works well in practice [cit] . extending this work, this paper makes the following contributions: a) cluster comparison: it presents a more careful and extensive look at the comparison of slice-based [cit] dependence clusters and the recently introduced seb-based [cit] dependence clusters. to do so, it considers two hybrid cluster types that are computed at the coarser function-level but based on the higher-precision dependences from the sdg. all three types of clusters are empirically compared using two different clustering metrics."
"where _ u 2 á _ u 1 is kosco dhl rule. as a second case, we consider the porr-wörgötter rule [cit] (eq 3). the g-dhl generates this rule using the following coefficients:"
"such cases are byacc and ctags, where we can observe that regx outperforms area in precision of identifying linchpin functions. a typical linchpin for byacc can be seen in figure 3 . comparing it to the unreduced msg, we can observe that there was no significant drop in the average slice sizes, but the cluster structures changed so that the plateau representing a cluster in the c seb case disappeared. this change could not be captured by area metrics, only by regx."
"this paper studies the relation between several kinds of dependence-clusters including two existing kinds and a previously unstudied hybrids. this is done using a general framework for defining dependence clusters, which it instantiated using five different slicing operators. in summary, the coarser approximation works well and thus extends dependence cluster and linchpin analysis to a wider range of programs. thus, our research expands the applicability of dependence cluster analysis in software engineering practice."
"the current view is that large dependence clusters hinder many different software engineering activities, including impact analysis, maintenance, program comprehension and software testing [cit] . in light of this view, this paper compares two approaches to clustering. we also consider two hybrid cluster definitions, which conceptually lie between the levels of abstraction offered by the previously proposed types."
"the steps of the procedure used to search the biophysical mechanisms are as follows: (a) identify with an automatic procedure the g-dhl components and parameters fitting the target stdp data set; (b) define the temporal profile of the two pre-/post-synaptic factors of each found component, and the ltp/ltd effects caused by the component; (c) identify possible biophysical processes having a temporal profile similar to the one of the identified factors; (d) design experiments to verify if the hypothesised biophysical processes actually underlie the target stdp phenomenon in the brain. we now give an example of how to apply the steps 'a' and 'b', and some initial indications on the step 'c', in relation to the bi and poo's data set [cit] . the example aims to only furnish an illustration of the procedure, not to propose an in-depth analysis of this stdp data set."
"a decade ago, binkley and harman demonstrated that software was often not easily separable [cit] when they empirically observed that programs often include large clusters of mutually dependent components. the presence of these dependence clusters complicates software maintenance and evolution. for example, if the impact of a change involves any part of a cluster then it will involve the entire cluster. furthermore, because large clusters include much of a program's code, it is very likely that some member of the cluster will be encountered. thus, clusters, which can encompass 80% or more of a program [cit], can severely inhibit the effectiveness of engineers and the tools that support them."
"automatic procedure to fit stdp data sets with g-dhl. g-dhl can be used to obtain particular stdp kernels by hand-tuning its parameters, for example to fit stdp data to some degree of approximation. this can be done on the basis of the synaptic updates caused by the different g-dhl components, shown in figs 6 and 7, and it is facilitated by the linear-combination structure of the rule."
"we want the two metrics to be comparable for a given program, so we normalize them to the interval [cit], where 0 means no clusterization and 1 indicates maximum clusterization. for the area metric, we normalize using the maximum possible area. more formally, for a given program p and slice type s:"
"in this demonstration we present an open-source implementation of our system together with a java api and a web interface for online measurement of semantic similarity. we also introduce a method for offline calculation of the ppr stationary distribution for multiple starting nodes. moreover, we release the compressed semantic signatures for all the 118k synsets and 155k words of wordnet 3.0. 76 2 align, disambiguate, and walk (adw) adw uses a two-phase procedure to model a given pair of linguistic items:"
"each motion was performed twice, with ten repetitions each. for each subject, the first motion set was used for the template training, then both motion sets were used for testing the proposed algorithm. the templates used were subject specific, so the templates generated from one subject's motions was used to segment only their own motion."
"an alternative method is to employ probabilistic algorithms, and look at changes in signal variance [cit] or probability distribution [cit] to indicate segmentation points. online template construction has also been proposed [cit], by clustering together previously segmented sequences to generate new templates on the fly."
"as a conclusion to the set of experiments overviewed above, we may say that seb could be a viable alternative to dependence cluster analysis, because: 1) seb is a lightweight analysis and is more scalable than sdg-based slicing. this topic has not been covered in the present paper, but previous works (e.g., [cit] showed the viability of the method for very large systems as well."
"having defined the five slicing operators, the next step is to introduce the five dependence cluster kinds studied in the empirical analysis. the term dependence cluster was introduced by binkley and harman [cit] as a set of program elements that mutually depend upon one another. dependence clusters are formalized in terms of mutually dependent sets:"
individual synsets. we used the ukb package 2 to generate the semantic signatures for all the 118k synsets in wordnet 3.0. each signature is truncated to the top 5000 most significant dimensions and compressed for better space utilization.
"the rule proposed by gerstner and kistler combines the possible multiplications between the power functions of degree 0, 1, and 2 of the activations of the pre-and post-synaptic neurons. the elements multiplied are therefore fu"
"to answer the first question, we compared the different rankings using correlation analysis. we computed kendall and spearman correlations between the rank lists for each program and each combination of metrics. results are shown in tables v and vi, respectively. since the two sets of results are very similar we discuss only kendall in detail."
"first thing to observe is that the area-based rankings are very similar: c measures, typically used in information retrieval for similar purposes [cit] . average precision is more appropriate for ranked information retrieval than traditional precision and recall, which do not take the ranks into account, because it does not require the selection of an arbitrary set of retrieved documents. in short, ap is precision (at each rank position) at each relevant document (linchpin in our case) averaged over the number of relevant documents, while the map value is the mean of the average precision values over all sets of queries (programs in our case). we computed ap values for each combination of program and ranking (determined by the different metrics and slice types), using both the gold and silver standard linchpins as the relevant documents. the most obvious thing to observe from the data is that the metric based ranking performs exceptionally well, especially for gold standard linchpins. in other words, if the existence of linchpins is evident, it can be found by metric-based approach with high success. particularly, ap is 1.00 for most of the programs in the gold standard category with the area metrics. typically, the regx metric is also a good indicator in these cases as it usually \"follows\" area in the case of high clusterization and evident linchpins. for silver standard linchpins, the situation is slightly different. map values are again very high almost in all cases, but here we can distinguish interesting cases that require further discussion."
"after introducing the types of clusters considered, the bulk of the paper presents an in depth analysis of the clusters identified by each type of cluster and the linchpins found within each. this is followed by a discussion of threats to validity, related work, and finally a summary."
the cuhk-03 dataset contains 13164 images of 1360 identities. about 7365 images are used to train the cnn model and all the rest is used for testing. it collects from 5 pairs different cameras on the chinese university of hongkong campus.
"in this paper, motivated by the focal loss proposed for the typical classification/identification model, we propose the triplet focal loss for the widely-used verification model. with the help of the exponential mapping function, the more hard the input triplet sample, the more penalty it will get relatively. thus the triplet focal loss function can automatically focus on ''hard'' triplet samples. the experimental results on three popular benchmark datasets verified the effectiveness of the proposed method and achieved competitive results with some representative state-of-the-art methods. also, it is worth noting that the proposed method nearly does not introduce any extra computational cost for both the training and testing phase."
"for each measurement of the semantic similarity between two linguistic items, adw requires the semantic signatures for the two items to be calculated. moreover, the alignment-based disambiguation of a pair of textual items requires the computation of all the semantic signatures of all their content words."
"f. vertex-level vs. function-level analysis rq1.4 deals with the difference between vertex-level and function-level clustering. to summarize our findings from previous sections, the different clustering levels generally tend to show similar behavior. this is especially true for programs where there are large to huge dependence clusters. in these cases the difference is merely in the average slice size but the clusterization structures are very similar. also, with programs which are obviously clusterless, this property is found by all slice types (and the corresponding metrics). there are only a few cases where there is observable difference between clusterization patterns at different levels (examples include bc, sudoku, wdiff, byacc), which suggests at this point (together with the overall small difference in the slice sets themselves) that function-level analysis is indeed a good approximation to vertex-level analysis."
"2. by using the ppr algorithm on the wordnet network, the two disambiguated items are modeled as high-dimensional vectors, called semantic signatures. to this end, adw initializes the ppr algorithm from all the nodes in the semantic network that correspond to the disambiguated senses of the linguistic item being modeled. the resulting stationary distribution, which has wordnet synsets as its individual dimensions, is taken as the semantic signature of that item."
"to enable automated measurement and analysis, the system must measure the human movement and identify exercise movement segments from the time series data. human movement can be measured via either motion capture systems [cit] or ambulatory sensors such as inertial measurement units (imus) [cit] . given the measured time series data, segmentation is the process of identifying the starting and ending locations of each movement of interest. if the patient is performing more than one type of exercise in a given recording session, identification (i.e., labeling) of each segment with the appropriate exercise type is also required. both segmentation and identification are made more difficult due to the variability observed in human movement. motion can vary between individuals due to differing kinematic or dynamic characteristics between individuals, and as well within a single individual over time, due to short term factors such as fatigue, or long term factors such as recovery or disease progression. moreover, these factors can introduce both spatial and temporal variability. thus, for rehabilitation, a good segmentation algorithm must be able to handle both temporal and spatial variations. a computationally light algorithm is also desired, in order to perform segmentation on-line, to allow real-time feedback to the patients."
"in a relevant work, kosco [cit] highlighted some key elements of differential hebbian learning (dhl), also introducing this name. first, he shifted attention from correlation to causality and as a consequence stressed the importance of considering that a \"cause temporally precedes its effect\". second, he proposed that to capture causality one should focus on concomitant variations rather than on concomitant activations as in the hebb rule. he thus proposed a learning rule leading to \"impute causality\" when the activations of the two neurons change in the same direction, and to impute \"negative causality\" when they move in opposite directions, based on the first derivative of the activation of the neurons:"
"the dhl rules proposed in the literature are special cases of the g-dhl rule. as a first case, we consider the kosko learning rule [cit] (eq 2). g-dhl generates this rule with the following parameter values:"
"to the best of our knowledge, current dhl rules are basically two: one proposed by kosko [cit] and one proposed by porr, wörgötter and colleagues [cit] . these rules modify the synapse in specific ways based on the temporal relation between the pre-and post-synaptic events. formulating other ways to modify synapses based on event timing is the first open problem that we face here."
the rest of the paper is organized as follows: section 2 reviews the related works with this paper. in section 3 we elaborate the proposed triplet focal loss. section 4 gives the experimental results and analysis on parameter tuning. in section 5 we draw the conclusion and describes the future work.
"note, that it is to be expected that the lines do not cross in the combined msgs because of the underlying slice-set containment relationship; however, in a few cases we can observe such phenomenon. in each such case, the crossing is an artifact of the scaling used on the y-axis."
"in the context of dhl, learning kernels can be computed by integrating (summing) over time the multiple instantaneous weight changes caused by the learning rule:"
"multiplications involving higher-degree powers, and other elements of the sum, might be needed to include other hebb rules. for example, a power 4 is needed to represent an interesting hebb rule implementing independent component analysis [cit] :"
"in our final set of experiments our goal was to verify to what extent clusterization metrics can be used to guide the linchpin identification process (rq3). in the previous section we explained how the brute-force method of linchpin identification results in a set of reduced cluster structures and associated metric values. we use these \"reduced\" metric values in form of their relative change percentage compared to the unreduced ones. thus we obtain six (potentially) different rankings of functions for each program. figure 4 shows examples of how different the rankings can be. the series of bars are given in decreasing order of a relative decrease of the metric compared to the unreduced program. note, that since the rankings are computed individually, the bars at the same rank position may represent different functions. the upper part of the figure shows the results for program bc, where we can clearly observe that the first element in the rank list is much higher than the rest in all three area metrics and in one of the regx metrics (in this case the first element was the same), which clearly indicates a linchpin. however, the other example in this figure (from program ctags) exemplifies a different pattern. here, just by looking at the ranks we could not clearly say if the first one or more elements might be linchpins. in fact, in the case of this program it turned out that the ranking according to regx was closer to manual assessment than the area-based. to address the research question rq3, we assessed these rankings from two angles: (rq3.1) how do they compare to each other (i.e. is there any significant difference among them), and (rq3.2) how well do the rankings reflect the manual assessment presented in the previous section?"
we now illustrate with an example the idea of using the components found by the g-dhl regression to heuristically search for biophysical mechanisms possibly underlying a target stdp data set. this example involves the bi and poo's data set [cit] analysed in the previous section. the idea relies on the observation that each multiplication factor of the g-dhl components identified by the regression procedure has a temporal profile that might correspond to the temporal profile of the pre-/post-synaptic neuron electrochemical processes causing the synaptic change.
"on dukemtmc-reid. without our proposed triplet focal loss, dpfl [cit] achieves 60.6% map and 79.2% rank-1 accuracy respectively. while our method increases the map to 63.17% and rank-1 accuracy to 80.07%. furthermore, compared with the re-ranking [cit], the method obtains 81.02% map and 85.55% rank-1 accuracy, exceeding start-of-the-art by 20.42% and 6.35% separately."
"most learning rules used in bio-inspired or bio-constrained neural-network models of brain derive from hebb's idea [cit] for which \"cells that fire together, wire together\" [cit] . the core of the mathematical implementations of this idea is multiplication. this captures the correlation between the pre-and post-synaptic neuron activation independently of the timing of their firing."
"among the verification models, the widely used triplet loss, which is introduced by weinberger and saul [cit], is incorporated to train cnn as an embedding function [cit] and aims to optimize the embedding space such that the distances between samples with the same identity are much shorter than those of samples with different identities. with the rapid growth of the training dataset, the possible number of the triplets increases cubically, making the training of all the possible triplets impractical. what is worse, a large fraction of the triplets would not provide informative supervision after a few steps of learning, as they are so easy that the model can quickly learns to correctly distinguish them. intuitively, being told over and over again that the same person has similar looking (easy positives) and different people have different appearances (easy negatives) can not make one being good at re-identifying people, whereas seeing the pictures of the same person with very different poses, viewpoints (hard positives) and different people with very similar looking (hard negatives) can greatly improve the skill of distinguishing people."
"the section 'results' shows how this procedure produces an accurate and stable fit of several different stdp data sets. this outcome might appear to be limited by the fact that the g-dhl rule involves many parameters. this is not the case because: (a) g-dhl can be seen as a set of dhl rules corresponding to its eight components; (b) each combination of the g-dhl components (formed by 1 to 8 components) is considered as a single model to perform an independent regression and the model comparison procedure penalises the models using a higher number of parameters; (c) as a consequence, the best model usually has only few components/parameters, about 2 or 3 (in addition to κ, τ 1, τ 2 ) ."
"during the on-line segmentation phase, a small sliding window is passed over the observation data, noting the local peak values and zvcs of each of the dofs. a zvc is declared if the velocity makes a zero-crossing, or if is very low for several timesteps. local peak values are tracked by an internal buffer. if the current window has a peak value higher then the stored peak value, the peak value in the buffer is updated accordingly. to avoid noise spikes in the velocity data from affecting the template matching, the peak buffer value is attenuated if it does not contribute to a match after several seconds, to prevent a large spike in the velocity from preventing feature matches. if a given dof observes a sequence of zvcs and peaks that matches a known template, then the algorithm has located a potential segment point."
"mathematically, this gives rise to a compound structure of the g-dhl rule which is formed by a linear combination of multiple components. in this respect, the capacity of g-dhl to capture different stdp phenomena is linked to the power of kernel methods used in machine learning [cit] . the linear form of the rule facilitates its application through manual tuning of its parameters, as shown here and in some previous neural-network models of animal behaviour using some components of the rule [cit] . the linear form of the rule also facilitates the automatic estimation of its coefficients when used to capture stdp data sets, as also shown here."
"note that although this relationship to kernel methods is relevant, it is also important to consider that the kernels of the g-dhl rule are not directly designed to capture the 'timedelay/weight-update' mapping of a specific stdp dataset, as it would happen in machine learning kernel-based regression methods. rather, the g-dhl kernels are generated by the step-by-step interaction of different combinations of the pre-/post-synaptic events and their derivative positive/negative parts. thus, the fact that the resulting kernel profiles form a set of basis functions covering the inter-event interval in a regular fashion is a rather surprising and welcome result. as shown in section 1.3 in s1 supporting information, if the events are asymmetric then none of the eight kernels overlap and they form an even more dense set of regularly distributed basis functions."
"existing differential hebbian learning rules. since dhl rules have been contrasted to the hebb rule [cit], we start by presenting the continuous-time formulation of it:"
"the exemplars are also used to train hmms with the baum-welch algorithm [cit] . an 8-state left-right model is used. the gaussian observation functions are initialized by k-means clustering. the threshold for recognition (t r ) is also determined, via leave-one-out cross-validation (loocv)."
"4 using adw figure 1 shows a sample usage of the adw api. the getpairsimilarity method in the adw class receives six parameters: the two linguistic items, the disambiguation method (alignment based or none), the signature comparison method, and the types of the two inputs. adw supports five different types of input: 3"
"same dhl rule to compute the update of the connection weight, here the 'causal' porr-wör-götter dhl rule (but any other dhl rule might have been used to show the point)."
"this rule leads to an asymmetric learning kernel (see section 1.1 in s1 supporting information). indeed, when the activation u 1 of the pre-synaptic neuron has a transient increase before an increase in the activation u 2 of the post-synaptic neuron, then u 1 mainly overlaps with the positive portion of the derivative _ u 2, rather than with its following negative part, so the weight is enhanced. conversely, if u 1 has a transient increase after a transient increase of u 2, then u 1 mainly overlaps with the negative portion of the derivative _ u 2, so the weight is depressed. this mechanism based on the derivative works only if the activations of the neurons exhibit a smooth increase followed by a smooth decrease ('event'). in the case of a sharp activation (e.g., a neuron spike), such smoothness can be obtained by filtering the signals before applying the rule, for example with a low-pass filter. this filtering indeed formed an integral part of the original proposal of the rule [cit] . for higher clarity and control, however, here we will separate the core of dhl rules from the filters possibly applied to the signals before the rules."
"the algorithm was tested on five subjects performing three types of physiotherapy exercises: knee extensions while seated, squats, and hip flexions while supine. the average age of the participants was 22 years old. the experiment was approved by the university of waterloo research ethics board, and signed consent was obtained from all participants."
"to make the classification of linchpins more structured we applied a 5-level scheme: 5 (cluster broken), 4 (almost a 5), 3 (a bit of breaking is evident), 2 (almost a 1) and 1 (clearly no breaking or just a drop in slice size). for each program we then identified all potential linchpin functions (separately for each slice type), resulting in the total of 69 functions for the whole subject set. we then further classified the linchpins into what we call a \"gold standard\" and a \"silver standard,\" which form the basis for further statistical analysis. these two classifications capture the author's intuition as to the level of clusterization with the gold standard being more rigorous and representing very obvious cases, while silver standard is more relaxed (and is a superset of the gold standard). we decided not to include any of linchpin candidates falling into categories 1-2, thus sliver standard included candidates of categories 3-5, and gold standard consisted of only category 5 linchpins. note, that in this step we deliberately did not use any predefined thresholds of the metric values to decide on the categories, we relied on visual inspection only."
"regarding step 'c' of the procedure, directed to identify possible biological correspondents of the component factors identified in step 'b', we now discuss some possible candidate mechanisms that might underlie the factors identified for the bi and poo's data set. note that these brief indications are only intended to show the possible application of the procedure, not to make any strong claim on the possible specific mechanisms underlying such stdp data set."
"general differential hebbian learning (g-dhl) rule. while the gerstner-kistler's systematisation relies on power functions of neuron activations, the systematisation of g-dhl relies on the positive and negative parts of the derivatives of such activations. to show this, we first give a more accurate definition of the events on the basis of which such derivatives are computed. as mentioned, an event is intended here as a portion of the signal, lasting for a relatively short time, featuring a monotonically increasing value followed by a monotonically decreasing value. the section 'from neural signals to events' discusses how g-dhl can be applied to any signal, for example directly to the neural signals, thus responding to events embedded in them, or to pre-filtered signals, thus responding to events generated by the filters."
"to answer rq1.2, we performed the following study. a proven approach to investigate cluster structures is to use the monotone slice-size graphs (msgs). an msg for a program is a graphical representation of all the program's slices drawn in monotonically increasing order along the x-axis from left to right. an msg allows easy visual identification of cluster structures as horizontal plateaus in the graph. figure 1 shows msgs for the original (unreduced) versions of the investigated programs. in this work, we show a combined msg in which all three slice types are shown on the same graph. note, that since these slice types share the same slicing criteria, the x-axis of the msg is common, but due to the different granularity of the slice elements, the slices of slice fv were scaled on the y-axis. the msgs enable a manual identification of various clusterization patterns. the first observation one can make about the graphs in figure 1 is that the three slice types typically produce msgs with very similar shape. in only a few cases is there a significant difference (e.g., sudoku). a more careful investigation revealed some subtle differences though, which we will address in a later section."
"in this section, we report the experimental results on three popular and relatively large benchmark datasets: market1501 [cit], cuhk03 [cit], and dukemtmc-reid [cit] . firstly, we give brief descriptions of the datasets respectively."
"as a second contribution of the paper, the section 'using g-dhl to fit stdp data sets' deals with the second open problem-understanding if and how g-dhl can be used to capture known stdp phenomena. to this end, the section first illustrates how the g-dhl synapse update caused by a pre-and post-synaptic spike pair can be computing analytically rather than numerically, and then it presents a collection of computational tools to automatically search the rule components and parameters to fit a given stdp data set."
"we computed dependence clusters, clusterization metrics, and other program parameters based on the slices. to aid linchpin determination we used the brute-force method first employed by binkley and harman [cit] . for each slice type, we excluded each potential linchpin function one by one, and then recompute the dependence clusters and the clusterization metrics. big relative changes in metric value is taken as an indicator of the presence of a linchpin."
"qi zhang received the b.e. [cit] . she is currently pursuing the master's degree with northwestern polytechnical university. her research direction includes image analysis, pattern recognition, and machine learning."
"to apply g-dhl to spike pairs, we first outline the procedure used to derive the formulas to compute g-dhl analytically, rather than numerically as done so far. the procedure is illustrated in detail in section 2.1 in s1 supporting information in the case in which one assumes that spikes and traces are described with some commonly used formulas. sections 2.7 and 2.8 in s1 supporting information show a method that leverages these formulas to use g-dhl to fit stdp data sets; examples of this fitting are shown in the section 'results'."
"semantic similarity quantifies the extent of shared semantics between two linguistics items, e.g., between deer and moose or cat and a feline mammal. lying at the core of many natural language processing systems, semantic similarity measurement plays an important role in their overall performance and effectiveness. example applications of semantic similarity include information retrieval [cit], word sense disambiguation [cit], paraphrase recognition [cit], lexical substitution [cit] or simplification [cit], machine translation evaluation [cit] ), tweet search [cit], question answering [cit], and lexical resource alignment [cit] ."
"before presenting the formulas, we discuss two important points. the closed-form formulas for synaptic updates by the g-dhl rule have two main advantages. first, they allow the mathematical study of the g-dhl rule (see sections 2.2 and 2.6 in s1 supporting information). second, the formulas allow a computationally fast application of g-dhl by computing the synaptic update through a single formula rather than as a sum of many step-by-step synaptic updates as done in its numerical application, an advantage exploited in the computationally intensive simulations of the section 'results'."
"at this point we manually classified each program according to its clusterization level. following the practice used in previous research, we used the five-level likert scale (\"none\", \"small\", \"medium\", \"large\" and \"huge\"), which allowed us a systematic and relatively fine grained analysis of the cluster structures. the result of the assessment is shown in the columns 2-4 of table iii. as mentioned, clusterization was quite similar in many cases, but there are significant differences as well."
"investigating the functions of different stdp kernels is not in the scope of this work. however, assuming that the variety of learning kernels discovered through stdp experiments supports different functions relevant to neural processing and that analogous functions might be needed in artificial neural networks, it is important to understand the computational mechanisms that might generate such a variety of learning kernels. in this respect, an important question is this: is there a dhl learning rule, or a set of them, that can generate the complete variety of learning kernels found in the brain? some existing research shows how different stdp learning kernels can arise from the same biophysical mechanisms [cit], or from the same dhl-based model [cit] . however, these studies propose specific mechanisms to address a sub-set of stdp data sets rather than proposing a general way to systematically reproduce stdp learning kernels. understanding the extent to which dhl can capture the known stdp phenomena, and how this can be done, is thus a second important open problem that we address here."
"during the ensuing decade, studies have replicated the initial findings with c programs, uncovering clusters in java codes [cit] and in legacy cobol systems [cit] . dependence clusters are also known to be detrimental to the software development process where they hinder a variety of activities including maintenance, testing, and comprehension [cit] ."
"as can be seen from table 1, the baseline method achieves 61.49% map and 77.76% rank-1 accuracy. with the help of the proposed triplet focal loss, the map is increased to 72.21%, with 10.72% gains and the rank-1 accuracy is increased to 87.92%, with 10.16% gains. after reranking [cit], the map and rank-1 of the baseline method are boosted to 70.82% and 78.74%. while the map and rank-1 of the proposed method are boosted to 85.88% and 90.17%, still with 15.06% and 11.43% gains over the baseline method respectively."
"the simulations also show how, through the use of suitable filters, one can apply g-dhl to any pair of signals independently of their complexity. the g-dhl can also be directly applied to the initial signals without any pre-filtering, as done in the examples of the section 'the g-dhl captures different dhl rules'. in this case the rule will work on the events already present in the signals. g-dhl can even be applied to capture the temporal relations between changes not resembling 'canonical' events (i.e., a transient increase followed by a transient decrease). for example, assume there is a first signal having a constant positive value and a second signal that is generally constant but also increases of a random amount at each second. even if none of the two signals exhibits canonical events, one could still apply some g-dhl rule components to capture some information. for example, the component ½u 1 ½ _ u 2 þ would train a connection weight keeping track of the sum of all increases of the second signal. in general, however, the lack of canonical events prevents a useful application of some g-dhl components (e.g., in the example just considered the g-dhl differential components would leave the connection weight unaltered). a second observation concerns the fact that in the simulations of fig 4 we used ½ _ u þ and ½ _ u à as filters to detect events in the signals. these are the same functions used inside the g-dhl rule. this is not by chance. indeed, when such functions are used inside the g-dhl they are employed to detect two 'sub-events' inside the original-signal neural event, namely its 'increasing part' and its 'decreasing part' that are then temporally related with those of the other signal. this observation suggests that one might generate other versions of the g-dhl rule by using other types of filters, in place of ½ _ u þ and ½ _ u à, inside the rule itself."
"2) results on dukemtmc-reid dataset table 2 shows that the map and rank-1 accuracy of the baseline method is 54.50% and 74.1% respectively. while our method increases the map to 63.17%, with 8.76% gains and rank-1 accuracy to 80.07%, with 5.97 gains, respectively. with the help of re-ranking [cit], the map and rank-1 of the baseline method are boosted to 68.31% and 77.69%. and those of our proposed method are boosted to 81.02% and 85.55% respectively. compared with the baseline method, our method get 12.71% gains for map and 7.86% gains for rank-1 accuracy."
"the data was recorded with a set of imus, and translated to joint angles via an extended kalman filter. the subjects were modeled as a 5 dof system. motion capture data was collected simultaneously; ground truth manual segmentation was determined by a human observer using video playback of the motion capture data."
"due to its simplicity, the zvc algorithm requires no template training time and very little segmentation time. however, as noted in the previous section, this algorithm is very inaccurate. fixed-window hmm uses the baumwelch algorithm to train the hmms based on exemplar data, and requires a significant amount of training time. its segmentation time is also very long, as it needs to run the forward algorithm numerous times at each time step, once for each template available. the feature-guided hmm requires the most training time. although the hmm training component is identical to the one utilized the fixed-window hmm, the additional training time comes mainly from the loocv method to determine likelihood thresholds. the feature extraction component takes very little time. with the feature-guiding, the proposed algorithm is able to more intelligently determine when to apply the forward algorithm, and decrease the segmentation runtime significantly."
"the four sdg-based slicing operators slice vv, slice vf, slice fv, and slice ff differ on the slicing criteria considered and on the set of elements returned as the result of the slice. the first one, slice vv, is the traditional vertex-level slicing [cit] where the slicing criterion is an sdg vertex and the result is a set of vertices, v. with such a slice the criteria is dependent on each of the vertices found in v. the second, slice vf, has the same slicing criteria, a vertex, but produces a set of functions f rather than a set of vertices. in this case, the criteria is dependent on the (entry-point vertices of the) functions in f."
"with respect to other approaches for modelling stdp, dhl represents a complementary tool in the toolbox of the modeller and neuroscientist. first, dhl differs from 'phenomenological models'. although simple and elegant, these models update the synapse based on mathematical functions directly mimicking the synaptic changes observed in empirical experiments in correspondence to different inter-spike intervals [cit] . instead, dhl rules compute the synaptic update on the basis of the step-by-step interactions between levels of and changes in the neural variables of interest. dhl rules also differ from 'biophysical models'. these models can reproduce many biological details but have high complexity and rely on phenomenon-specific mechanisms (e.g., [cit] ). instead, dhl rules reproduce fewer empirical details but at the same time, after the systematisation proposed here, they represent 'universal mechanisms' able to capture many stdp phenomena."
"as expected, slice sizes were comparable with the different slice types. this experiment confirmed our earlier findings that the seb slices are not much larger than slice ff slices: the difference is between 3%-48%, the average being 11%, and the outliers are all in the very small programs. the difference between the two function-level slices slice ff and slice fv are also very similar: on average it was 12%."
"time is however very important for brain processing and its learning processes [cit] . differential hebbian learning (dhl) rules [cit] are learning rules that change the synapse in different ways depending on the specific timing of the events involving the pre-and post-synaptic neurons. for example, the synapse might tend to increase if the pre-synaptic neuron activates before the post-synaptic neuron, and decrease if it activates after it. as suggested by their name, dhl rules use derivatives to detect the temporal relations between neural events. here we will use the term event to refer to a relatively short portion of a signal that first monotonically increases and then monotonically decreases. events might for example involve the activation of a firing-rate unit in an artificial neural network, or the membrane potential of a real neuron, or a neurotransmitter concentration change. dhl rules use the positive part of the first derivative of signals to detect the initial part of events, and its negative part to detect their final part. by suitably multiplying the positive/negative parts of the derivative of events related to different signals, dhl rules can modify the synapse in different ways depending on how their initial/final parts overlap in time."
"in figures 2 and 3 we present examples of gold and silver standard functions, respectively. if we compare the msgs shown with their unreduced versions from figure 1, we can clearly see the effect of linchpin removal: in the case of gold standard it is significant, while it is less pronounced with silver standard. the example, in the case of barcode all slice types produce significant cluster break, however with ed only c seb produces a break, the slice-based clusters do not vanish, they are just reduced. considering example silver standard reductions (figure 3), the reduction for go is significant, however, a big cluster remains. program byacc is the least evident: in fact, with"
"where _ u 2 á u 1 is the porr-wörgötter dhl rule. the g-dhl rule can generate many other possible dhl rules. as an example, fig 3 shows how different combinations of the g-dhl components can generate a 'causal' rule (similar to the porr-wörgötter rule), a truly 'anticausal' rule (using kosko's expression), a 'coincidencedetection' rule (similar to the kosko rule), and a causal rule not changing the synapse for intervals around zero (called here 'flat-at-zero causal rule'). these examples were not chosen arbitrarily: the section 'results' will show that each of these rules models one class of stdp processes found in the brain."
"dukemtmc-reid is a subset of dukemtmc dataset for image-based person re-identification. the original data set contains 85 minutes of high resolution video captured from 8 different cameras, and provides manually labeled bounding boxes. person bounding boxes are annotated from the videos for every 120 frames, yielding in total 36,411 bounding boxes with ids. there are 1,404 identities appearing in more than two cameras and 408 identities (distractor id) who appear in only one camera. the dataset split is given by the contributors by randomly selecting 702 ids as the training set and the remaining 702 ids as the testing set. in the testing set, one query image for each id in each camera is selected and the remaining images are put into the gallery. eventually, it contains 16,522 training images from 702 people and 2,228 query images from another 702 people, and a search gallery of 17,661 images."
"owing to its crucial importance a large body of research has been dedicated to semantic similarity. this has resulted in a diversity of similarity measures, ranging from corpus-based methods that leverage the statistics obtained from massive corpora, to knowledge-based techniques that exploit the knowledge encoded in various semantic networks. align, disambiguate, and walk (adw) [cit] . the measure is based on the personalized pagerank (ppr) algorithm [cit] applied on the wordnet graph [cit], and can be used to compute the similarity between arbitrary linguistic items, all the way from word senses to texts. [cit] reported state-of-the-art performance on multiple evaluation benchmarks belonging to different lexical levels: senses, words, and sentences."
"this section introduces the five slicing operators used to create five instantiations of the general definition of a dependence cluster. the first four compute (backward) slices as the solution to a reachability problem over a program's system dependence graph (sdg) [cit] . an sdg is comprised of vertices, which essentially represent the statements of the program and two kinds of edges: data dependence edges and control dependence edges. a data dependence connects a definition of a variable with each use of the variable reached by the definition [cit] . control dependence connects a predicate p to a vertex v when p has at least two control-flow-graph successors, one of which can lead to the exit vertex without encountering v and the other always leads eventually to v [cit] . thus p controls the possible future execution of v. when slicing an sdg, a slicing criterion is a vertex from the sdg."
"the need to repair and improve software involves a number of challenging tasks such as impact analysis [cit], defect detection [cit], software reuse [cit], and regression testing [cit] . such tasks are facilitated by source code that is easily separated (e.g., loosely coupled). for example, the complexity of understanding the impact of a change is reduced if only a subset of the code need be considered. easily separated code also simplifies more complex tasks such as extracting software product lines from legacy applications [cit] . in general, the software maintenance and evolution process is aided by separable software."
"to reduce the need to apply the forward algorithm to each segment candidate, a two stage recognition process is applied. for each motion, two types of templates are prepared: a feature profile and an hmm profile. given an exemplar motion from which a template is to be created, the feature extraction notes the locations of velocity crossings and velocity peaks of the exemplars. since the motions examined in this paper are rehabilitation exercises and thus exhibit regular patterns of flexion and extension cycles, the feature template would be expected to consist of a zvc, then a positive or negative peak, then another zvc, the opposite peak, and a final zvc. the magnitudes of the peaks (v p ) and peak-to-peak times (t pp ) are also stored in order to reject insignificant motions from triggering a feature match. only significant dofs of the motion are used for template matching at the feature stage. significant dofs are identified by calculating the standard deviation of each of the joints in the template and grouping them via 2-means clustering, and selecting the dofs grouped with the higher centroid."
"the final conclusion is that in the cases where the drop in clusterization metric area is significant it will indicate the existence of a linchpin function with great probability. however, where the metric changes and the associated ranking are less evident, regx could be an additional source of information."
"to characterize programs with dependence cluster formations in an objective way, some objective measurements are required (this addresses rq1.3). the traditional approach for this purpose is to use the area under msg, that is the sum of sizes of all slices [cit] . this metric, used as an indicator of relative change after the removal of a program element may be used to detect linchpins. however, it has the drawback that it does not take into account the structure of cluster formations, so it produces false positives. for example, when there is a uniform reduction in slice size, which has no effect on the cluster structures. thus it may be misleading. in previous work [cit] we experimented with several other measurements that in certain situations are better indicators of clusterization. in this paper we will rely on two clusterization metrics: the area under msg, denoted area, and the regularity metric regx, defined in the mentioned article."
"1. the pair is first disambiguated using an alignment-based disambiguation technique. let a and b be two linguistic items to be compared, and s w be the set of senses of a word w in the item a which is to be disambiguated. the alignment-based disambiguation measures the semantic similarity of each sense in s w to all the senses of all the words in the compared item, i.e., b. the sense of w that produces the maximal similarity is taken as its intended sense. the procedure is repeated for all the other words in a and also in the opposite direction for all the words in b."
"unfortunately, a natural and inevitable aspect of all programs are dependences between components (e.g., statements, functions, or classes). a dependence between two program components means that the execution of one component influences the other [cit] . both software engineers and the tools they use must be aware of the connections dependences cause in virtually every software engineering task involving the multiple components."
"seb is computed using a lightweight program representation called the interprocedural component control flow graph (iccfg) [cit], which is composed of individual component control flow graphs (ccfgs) for each procedure of the program. each ccfg represents a procedure's intraprocedural control flow graph (cfg) [cit] but only call site nodes and corresponding flow edges are retained. the iccfg consists of the ccfgs of each procedure connected by call edges from each call site (in a component) to the entry node of the called function. a reachability algorithm is then used on the iccfg, similar to the sdg reachability algorithm, to compute a slice. for any function f, slice seb on the criterion f produces the set of functions on which f depends (i.e., the functions that are predecessors of f according to the seb relation)."
"in this subsection, we compare many experiment results in different data sets. from table 4, table 6 and table 5, we show some experiment results in different table. on market1501, in achieves a 69.01% map and obtain 82.2% for rank-1 owing to using the triplet loss. the deep up to 87.7% in rank-1 by using mutual learning, it is close to our results on rank-1. triplet focal loss achieves a 72.21% and a rank-1 87.92%,exceeding both of them. on dukemtmc-reid, the dpfl method is a 60.6% map and a 79.2% rank-1. the triplet focal loss results are 72.21% and 87.92%, which is higher than other results. on cuhk03, our proceed get the results that the map is 51.28% and the rank-1 is 58.61%."
"the porr-wörgötter learning rule was the first dhl rule used to model empirical data on stdp [cit] . in this respect, its learning kernel resembles the kernel observed in the most studied form of stdp [cit] . this resemblance was also used to formulate hypotheses about the biophysical mechanisms underlying the target stdp data [cit], an interesting idea also followed here."
"the wordnet graph is constructed by including all types of wordnet relations, and further enriched by means of relations obtained from princeton annotated gloss corpus 1 . the graph consists of 117,522 nodes (wordnet synsets) which are connected by means of more than half a million nondirected edges."
"words. we also generated semantic signatures for around 155k wordnet 3.0 words. to this end, for each word we initialized the ppr algorithm from all the synsets that contained its different senses. the word signatures can be used for faster computation of similarity, if it is not intended to perform alignment-based disambiguation on the items."
"in earlier work [cit], we showed that the seb relation is less precise than the sdg-based slices, but only to a small amount. this suggests that seb may be a viable alternative for the analysis of large and complex systems (for which the traditional slicing algorithms are difficult to scale). these early experiments were done using a different set of tools and subject programs. in the present work, we replicate and extend the comparison to include all five slice types."
"the results show that in the first simulation the connection weight tends to decrease because the increase-changes of the first signal tend to follow the increase-changes of the second signal. in contrast, in the second simulation the connection weight tends to increase as the increase-changes of the first signal tend to anticipate the decrease-changes of the second-signal. overall, the simulations show how deciding on the filters to use to associate events to the changes of interest is as important as deciding on which dhl rules to use."
"in the experiments, elements of c and e are either sdg vertices or program functions, so both area and regx can be interpreted for all slice types by substituting the respective criteria and program element sets. table iii provides the area and regx metrics for each unreduced program and the three investigated slice types. these metrics serve as a baseline when searching for linchpin functions. comparing the manual cluster classifications from table iv to the metric values, generally it is not obvious which metric best reflects the level of clusterization. there are obvious cases such as programs copia and go, where large dependence clusters can be easily identified by large area values. however, there are cases, such as wdiff, byacc, and ctags, where area alone is not enough to determine clusterization. here, regx could provide additional information about the regularity of different set sizes. for instance, in the case of wdiff we observe a medium seb-based cluster, which is reflected by the relative high value of regx while the same metric for the other two slice types is low, indicating the absence of clusters. another example is ctags, which is classified as highly clustered. in this case the regx metrics are bigger than area metrics, which indicates that in these cases regx may be a better indicator than area."
"in this subsection, we thoroughly compare the proposed method with the baseline method. as the proposed triplet focal loss is implemented based on the batchhard triplet loss, the batchhard triplet loss [cit] on top of resnet-50 backbone model is chosen as the baseline method. table 1, table 2 and table 3 lists the reid performance: in the metric of both rank-1 accuracy and the map, of the proposed triplet focal loss compared with the batchhard triplet loss, on market-1501, dukemtmc-reid and cuhk-03 dataset respectively."
"recently, a lot of efforts have been devoted to the research of person re-identification (reid) [cit], which aims to re-identify the same person across cameras with nonoverlapping visual fields. although it has achieved rapid development in recent years, the task still remains far from being resolved due to the great challenges resulted from the variations of view, pose, lighting condition and occlusion, etc. as depicted in figure 1 . conventionally, the reid task is tackled by two separate phases, namely carefully feature designing and effective distance metric learning. the emergence of the deep convolutional networks has combined the two phases together via an end-to-end learning scheme and therefore, introduced more powerful and robust feature representations. generally speaking, the cnn based method for reid can be broadly categorized into two groups: identification model which utilizes a classification loss function such as cross entropy, and verification model which incorporates a metric learning loss function such as triplet loss function. although the classification model can make full use of the labels [cit] compared with the verification model, the verification model [cit] can achieve competitive or even better reid accuracy."
"early findings that blocking nmda receptors (nmdars) can prevent both ltp and ltd, while a partial blocking can turn an ltp effect into an ltd, has led to the proposal of several calcium-based models of synaptic plasticity (e.g., [cit] ). one view proposes that two independent mechanisms can account for the classic stdp learning kernel [cit] . this is in line with the two components, and their factors, found by our g-dhl based regression of bi and poo data set. the first component was an ltp 'positive-derivative/positive-derivative' compo-"
"physical rehabilitation is a branch of modern health care that focuses on the development, maintenance and restoration of body movement and function, particularly after injury or surgery. a rehabilitation session consists of a physiotherapist's assessment of the patient's current condition, as well as the performance of physical exercises recommended by the physiotherapist. typically, the physiotherapist will supervise the performance of the exercises to determine patient progress, as well as provide corrective feedback. technology to measure and analyze human motion has the potential to provide the physiotherapist with more accurate tools for assessment and progress measurement, as well as to provide the patient with real-time feedback."
"2) the seb sets are only slightly less precise than slice sets. 3) typically, there are no big differences between the cluster structures formed by sdg and seb-based slices. 4) linchpins can be identified in both approaches using a metrics based method with area and regx, and the identified linchpins are aligned in most of the cases in slice and seb-based analyses. the other conclusion we may make from the experiments is that area is a good indicator of clusterization in many cases, especially where the average slice sets are big as well. however, regx may be called upon in less evident situations, which can highlight subtle differences in cluster structures."
"xing wei received the b.s. [cit], where he is currently pursuing the ph.d. degree with the institute of artificial intelligence and robotics. his research interests include computer vision, image processing, and machine learning, specifically in the areas of 3d scene understanding, largescale visual recognition, object detection, and segmentation."
"when given an anchor sample x a, eqn. (1) tries to make the distance in the embedding space between the anchor sample x a and positive sample x p which belong to the same identity closer than that of the anchor sample x a and negative sample x n which belong to different identities, by at least margin m. triplet loss can avoid the disparate clusters for the same identity when it is optimized over the whole dataset for long enough. while the disadvantage is that with the rapid growth of the training dataset, the possible number of the triplets increases cubically, making the training of all the possible triplets impractical."
"shizhou zhang received the b.e. and ph.d. [cit], respectively. he is currently an assistant professor with northwestern polytechnical university. his research interests include content-based image analysis, pattern recognition, and machine learning, specifically in the areas of deep learning, image classification, and image parsing."
"although we chose a set of twenty open-source and industrial programs of various sizes and from different domains, external threat arises from the possibility that the programs selected are not representative of programs in general, causing uncertainty of the generality of the conclusions. the manual identification of linchpin functions is subjective raising threats to validity of the metrics-based linchpin identification results. the other possible threat to construct validity arises from the potential faults in the slicers. a mature and widely used slicing tool (codesurfer) and slicing algorithm implementations used in previous research were used to mitigate this concern."
"four different methods are included in the package for comparing pairs of semantic signatures: jensen-shannon and kullback-leibler divergence, cosine, and weighted overlap [cit] . weighted overlap is a rank similarity measure that computes the similarity of a pair of ranked lists in a harmonic manner, attributing more importance to the top elements than to the bottom ones. [cit] reported improvements over the conventional cosine measure when using weighted overlap in multiple tasks and frameworks."
"instantiating in definition 2, each of the five slicing operators produces the following same-slice-size cluster types shown in table i . for ease of reading, the same-slice-size clusters defined in definition 2 are referred to simply as \"clusters\" parameterised by the above notations in the rest of the paper. the experiment presented in this section empirically explores the relationships between the clusters produced by the different slicing operators and the corresponding linchpins. we first compare the underlying slices and the clusters by objective measurements (rq1). then, we use a manual classification of cluster structures and associated linchpins to investigate the clusters and their breaking (rq2). finally, we consider how two metrics, area and regx [cit], perform at cluster and linchpin analysis with respect to a manual classification (rq3). based on the results we verify to what extent a less expensive dependence analysis, such as seb, can be used as a proxy for a more precise and more expensive analysis with respect to dependence clustering. in greater detail, we frame the investigation using the following research questions:"
"harder triplet sample means that larger d a,p and smaller d a,n, with the help of the exponential mapping function, larger distances will be enlarged much more than those smaller distances. the more hard the input triplet sample, the more penalty it will get relatively. thus, with the help of the exponential kernel, the loss function can automatically focus on ''hard'' triplet samples."
"on cuhk03. we also compare our proposed triplet focal loss with other methods on cuhk03. as can be seen from table 6, our method achieves 51.28% map and 56.86% rank-1 accuracy. with the help of the re-ranking [cit], the map is increased to 65.62% and the rank-1 accuracy is increase to 64.79%."
"kosko learning rule indeed implies strengthening of the synapse when the pre-and postsynaptic neurons activate at the same time, as their activations increase/decrease at the same time, and to weaken it when their activations do not fully overlap in time (see section 1.1 in s1 supporting information). however, the rule's learning kernel is symmetric in time as it does not discriminate the sign of the temporal difference between the events. the timing of events is a central element of most quantitative definitions of causality (e.g., in granger causality, a popular statistical approach to capture 'causality' [cit] ). the importance of the temporal ordering of neural events was articulated by porr, wörgötter and colleagues [cit] who proposed the learning rule:"
"b) linchpin identification: this paper also explores the identification of linchpins -a single program element (e.g., a statement, a global variable, or a single function) that holds a dependence cluster together. specifically linchpin functions are investigated. when the dependences of a linchpin function are removed, the cluster(s) of the program vanish. at present, it is not well understood what makes a particular program element a linchpin, how linchpins can be identified, or when there is a refactoring that removes a linchpin. as a first step, it is useful to be aware of a program's linchpins. the experiments show how linchpin functions can be effectively and automatically identified."
"finally, we illustrate the example query images and their ranking lists for our method on market-1501 test set in figure 4 . the leftmost images are the queries and the top-10 images in the ranking list of the gallery images are listed after the queries. green boundary is added to the true positive and red to false positive."
"the java source code can be obtained from adw's github repository at https://github.com/pilehvar/adw/. we also provide a java api, an online demo and the set of pre-computed semantic signatures for all the synsets and words in wordnet 3.0 at http://lcl. uniroma1.it/adw/."
"time series data segmentation for human movement is an active area of research. a common approach is to rely on known motion templates to assist in the identification of motions. dynamic time warping (dtw) is one example of a template-based method. dtw [cit] identifies the temporal variations between the observed motion and the motion template by selectively warping the time scale of an observation sequence to the template, based on a distance metric. therefore segments can be accurately segmented and identified even with significant spatial and temporal variation between the observation and the template. however, dtw becomes very computationally expensive with higher dimensionality, preventing it from being utilized on-line."
"the rest of the paper addresses the two open problems indicated above in the following ways. as a first contribution of the paper, the section 'g-dhl and the systematisation of dhl' considers the first open problem-how different dhl rules can be generated in a systematic fashion-by proposing a general framework to produce dhl rules. in particular, the section first reviews the dhl rules proposed so far in the literature; then it presents the g-dhl rule and shows how it is able to generate the existing dhl rules and many others; and finally it shows how one can filter the neural signals to generate events that correspond to the features of interest and can use memory traces to apply the g-dhl rule to events separated by time gaps."
"while still not well understood, clusters of dependence seem to be an inherent property of source code; thus, current research has focused on understanding their formation and the possibilities for their removal or reduction. one of the challenges in studying dependence clusters is the complexity of the underlying program dependence analysis. the original work used dependences from a program's system dependence graph (sdg) [cit] whose computation involves solving several difficult whole-program data-flow problems such as determining the modified global variables that result from a procedure call [cit] and determining the points-to set for each pointer variable [cit] ."
"the market-1501 dataset is collected in front of a supermarket in tsinghua university. a total of six cameras are used, including five high-resolution cameras, and one low-resolution cameras. there exists overlap among different cameras. it contains 32668 annotated bounding boxes of 1,501 identities. each annotated identity is present in at least 2 cameras and at most 6 cameras, so that cross-camera search can be performed. a recommended dataset split is given by the contributors and there are 751 identities in the training set and 750 identities in the testing set."
"clearly distinguishing between the information processing done by filters, which associate events to the features of interest of the neural signals, and the effects of g-dhl, which modify the synapse on the basis of the temporal relation between those events, is important for best understanding g-dhl. it is also important for the application of g-dhl that involves a sequence of two operations related to such distinct functions: (a) the application of filters to detect the events of interest (in some cases this operation might be omitted, as discussed below); (b) the application of the g-dhl to the resulting signals encompassing such events. the function of filters related to the generation of events should not be confused with their possible second use to create memory traces and smooth signal with discontinuities, e.g. neural signals with spikes (see the section 'traces: overcoming the time gaps between events'). fig 4 presents the results of two simulations showing how different filters can be applied to the same signals to detect different changes of interest, and how this leads to different synaptic updates even when using the same dhl rule. the example also shows that the g-dhl can be applied to any type of complex signal beyond the simple ones used in previous sections. the two simulations are implemented through three steps: (a) both simulations start from the same pair of signals: these might represent the activation of two firing-rate neural units linked by a connection; (b) the simulations apply different filters to those signals: the first applies a filter to both signals that generates an event for each 'increase change'; the second applies a filter to the first signal that generates an event for each 'increase change', and a filter to the second signal that generates an event for each 'decrease change'; (c) both simulations then use the examples of learning kernels generated by the g-dhl rule. the signals involved events generated with a cosine function (as in fig 1) . in the examples, the g-dhl coefficients were set as follows (the rule names are arbitrary): causal rule:"
"formed by two factors. the first factor was a pre-synaptic factor (½ _ u 1 þ ) lasting about 30 ms, compatible with a short-lived effect involving the pre-synaptic glutamatergic neuron spike and affecting the post-synaptic nmdars [cit] . the second factor was a postsynaptic factor (½ _ u 2 þ ) lasting about 7 ms, compatible with a back-propagating action potential (bap; [cit] ). the second component was a 'positive-derivative/signal' ltd component (½ _ u 1 þ u 2 ) formed by two factors: a relatively slow pre-synaptic element, (½ _ u 1 þ ), lasting about 30 ms, and a slow post-synaptic element, (u 2 ), lasting about 50 ms. different biological mechanisms might underlie these two factors. in this respect, there is evidence that post-synaptic nmdars might not be necessary for spike-timing-dependent ltd [cit], while this might be caused by metabotropic glutamate receptors (mglur; [cit] ), voltage gated calcium channels (vgcc; [cit] ), pre-synaptic nmdar [cit], or cannabinoid receptors [cit] ."
"feature-guided hmm segments data with high accuracy. it does so by reducing the comparison space of the observation data by looking at significant dofs, velocity peaks and zvcs, to estimate locations of segment potentials. when such segment potentials are located, the sequence under examination is checked with against an hmm to find the most accurate segmentation and identify the motion. we plan to apply the algorithm to a larger dataset to assess the algorithm against a wide set of motion types. a specific weakness of the current implementation is that subject-specific templates are used. this is impractical in clinical settings, as it would require that each patient provide a template a priori. in future work, we will test the algorithm by using a single set of templates against the motion data of other multiple users, in order to assess how well this algorithm scales against interpersonal variations."
"3) results on cuhk-03 dataset table 3 gives the comparison results of our method with the baseline method on cuhk-03 dataset. the map of the baseline method is 47.29% and the rank-1 accuracy is 52.71%. our method achieves 51.28% map, 3.99% gains compared with the baseline and 56.86% rank-1 accuracy, 4.15% gains compared with the baseline, respectively. the re-ranking [cit] technique consistently improves the performance of both the baseline and the proposed method. after re-ranking, the map and rank-1 accuracy of the baseline method are 56.46% and 56.93%, respectively. and those of our proposed method are 65.62% and 64.79%, respectively. compared with the baseline method, our method gets 9.16% gains for map and 7.86% gains for rank-1 accuracy."
"events are important for g-dhl because it uses the increasing part and the decreasing part of the pre-and post-synaptic events to capture, through the derivatives, their temporal relation. indeed, the increasing part of an event marks its starting portion whereas its decreasing part marks its following ending portion (see section 1.2 in s1 supporting information). the time overlap between these portions of the events allows g-dhl to detect their temporal relation, as we now explain in detail."
"four columns in the middle of table iv contain the number of manually identified linchpins organized by gold and silver standard and by the different slice types. it can be observed that c fv and c ff did not produce any differences, while in several cases c seb clusters resulted in different linchpins (typically more could be identified with this slice type). in the last column of the table we listed the most obvious linchpin functions, the ones that can be identified with all cluster types, and are typically parts of the gold standard."
we then used the clustering information from both the unreduced programs and the versions where potential linchpins had been removed to create input data for the manual and automatic characterization of the programs in the subsequent phases of our study. we used monotone slice-size graphs (msgs) [cit] and the relative changes in the clusterization metrics for this purpose. table ii lists the subject programs used in the experiments. columns 2-6 provide basic metrics and a description of the programs. the programs cover various domains and are taken from previous studies on dependence clusters. the source codes for the subject programs are available online (see section vii).
"looking forward, this line of research is promising. future projects will consider finding more efficient linchpin identification methods, such as heuristic algorithms. another interesting topic is to consider sets of potential linchpins rather than single linchpin functions. finally, we could work towards forming a better understanding of the effects of dependence clusters on every day software engineering. to this end, we plan to empirically investigate the relationship between dependence clusters and software quality."
"finally, the similarity of the two linguistic items is computed as the similarity of their corresponding semantic signatures. we describe in section 2.2 the four different signature comparison techniques that are implemented and offered in the package. note that the two phases of adw are inter-connected, as the alignment-based disambiguation in the first phase requires the generation of the semantic signatures for individual senses of each word in an item, i.e., the second phase."
"figure 2 provides a snapshot of adw's online demo. two items from two different linguistic levels are being compared: the fourth sense of the verb fire 4 and the phrase \"terminating the employment of a worker.\" the user can either choose the input type for each item from the drop-down menu or leave it to be automatically detected by the interface (the \"detect automatically\" option). the online demo also provides users with the possibility to test similarity measurement with no involvement of the disambiguation step."
"where u 1 and u 2 are respectively the pre-and post-synaptic neuron activations, and _ w is the instantaneous change of the connection weight. since this hebb rule captures the pre-and post-synaptic neuron activation co-occurrence, it is 'symmetric in time': the more two neurons activate distantly in time, the lower the synaptic update, independently of the temporal order of their activation. indeed, even in this dynamical formulation the hebb rule is not a dhl rule (see section 1.1 in s1 supporting information)."
"the g-dhl-based automatic regression applied to the four data sets found two components (hence parameters) for each of them: data set of fig 10b) . the results also suggest interesting possibilities with respect to caporale and dan's classification [cit] . the first data set is characterised by ltd for negative spike delays and ltp for positive ones. accordingly, the g-dhl-based regression found two components (the first, ½ _ u 1 þ u 2, with a negative coefficient"
"parallel to the first two, the output of the final two operators, slice fv and slice ff, is a set of vertices v and a set of functions f, respectively. these two differ in that the slice is taken with respect to an entire function instead of a single sdg vertex. for function f, this is done by taking the union of the slices for each vertex that represents source code from f ."
"g-dhl relies on two main ideas. the first idea, elaborated starting from previous proposals [cit] (see also [cit] ), is that the derivative of an 'event', intended as a monotonic increase followed by a monototic decrease of a signal, gives information on when the event starts and terminates. this information is used by g-dhl to update the connection weight depending on the time interval separating the pre-and post-synaptic neural events. the second idea is that the actual synaptic update can rely on different combinations of the possible interactions between the pre-/post-synaptic events and their derivatives, thus leading to a whole family of dhl rules."
"the other interesting observation from the data is that the difference between pairs of slices that differ in the criteria used, but not the counting granularity (slice vv vs. slice fv and slice vf vs. slice ff, respectively) was small, about 2% on"
"we assessed the implementation of adw on two evaluation benchmarks: similarity judgement correlation on the rg-65 dataset [cit] and synonym recognition on the toefl dataset [cit] . given a set of word pairs, the task in judgement correlation is to automatically compute the similarity between each pair and judgements are ideally expected to be as close as possible to those assigned by humans. the closeness is usually measured in terms of correlation statistics. in the synonym recognition task, a target word is paired with a set of candidate words from which the most semantically similar word (to the target word) is to be selected. table 1 shows the results according to the spearman ρ and pearson r correlations on rg-65 and accuracy, i.e., the number of correctly identified synonyms, on toefl. we show results for two sets of vectors: full vectors of size 118k and truncated vectors of size 5000 which are provided as a part of the package. as can be seen, despite reducing the space requirement by more than 15 times, our compressed vectors obtain high performance on both the datasets, matching those of the full vectors on the toefl dataset and also the cosine measure."
the section 'discussion' closes the paper by analysing the main features of g-dhl and its possible development. all software used for this research is available for download from internet (https://github.com/goal-robots/cnr_140618_gdhl).
"the velocity magnitudes and peak-to-peak distance must exceed v p and t pp respectively in order for a potential segment point to be declared. this prevents noise, such as when the subject is stationary, from triggering the feature match. the template and window edge combination that results in the highest likelihood value over the threshold t r is declared a segment. all window edge combinations are resampled so they are all equal length, to prevent the forward algorithm from favouring shorter sequences. following hmm template matching, the recorded peak magnitude and zvcs are reset, and the feature search resumes at the next time step."
"for verification purposes, a partially faulty test set has been generated by introducing anomalies in a fraction of the patterns. more precisely, the signal values of a generic test pattern are altered by a random noise (with probability 02 . 0  actually, to account for the variability of the training and test sets, five ensembles (each constituted by 150  k groups with randomly sampled signals) have been generated for each group size m and for each of the five ensembles the computation of the accuracy and robustness indicators has been 5 times cross-validated. the overall ensemble performance associated to the generic group size m is finally computed as the average over all cross-validations of all different ensembles. figure 6 right), i.e. they are less robust. notice that, despite the performance differences are very small, at least for the case of undisturbed signals, there is a trend revealing a tentative optimal group size. shows the best compromise between accuracy and robustness and is chosen as the ensemble groups' optimal size. concerning the ensemble size, in general, high signal redundancy and, correspondingly, large numbers of groups guarantee better accuracy and robustness. nevertheless, using a large number of groups in the ensemble leads to a considerable increase in the computational cost of training and testing the models, with only a slight improvement of the ensemble performances."
"the fundamental characteristic of diversity in the ensemble models has been generated by randomly sampling the signal features in the groups (by the rfse procedure) and a fraction of the patterns for training the corresponding reconstruction models (by the bagging technique). the dimension parameters of the ensemble (i.e., the size of the groups and the number of groups in the ensemble) have been optimized by a direct wrapper approach."
this paper presents a novel approach to produce realistic crowd behaviors and rbas crowd simulation behavior model is proposed. the algorithm simulates the agents as ellipse with particular sizes. the agents have sense of environment and plan their own path to avoid collisions. the shaking and repelling effect in agents have been reduced using the body contact and sliding forces. the interactions among agents have been modeled based on personal reaction space. this modification in sfm model makes the model more realistic as every parameter has been modeled from the results of phycology and video tracking.
"and (6) where is the impact angle estimation and is the calculated distance between agents a and b at simulation times t and t+1. is the radius of agent which is its semi minor axis of ellipse and is the threshold distance between two bodies, which depends on situations during simulations and changes during normal and panic behaviors,. the decreases in case of panic behavior and increases in case of normal behavior. it is normally equals to the personal area (as mentioned in section c), during normal situations and in case of herding behavior and turbulent flows, it decreases to intimate area."
"a cnn recognizes objects by performing convolutions, pooling, rectified linear unit (relu) application and other operations on an entire image. [cit] captured widespread attention for deep learning when they obtained superior recognition results on the imagenet large scale visual recognition challenge (ilsvrc) by increasing the depth of the cnn model and adopting relu and dropout [cit] technology. the cnn performs an exhaustive selection process on the image to find all possible bounding boxes for all objects. the features of these regions are extracted first; then, an image recognition method is used to classify them. during the recognition process, non-maximum suppression (nms) is used to obtain the candidate boxes with the highest probability of classification. the softmax function is used to form the final output before the last fully connected layer. the softmax activation function is expressed as follows:"
"the development of technology has increased the use of commercial wearable devices yet short battery life of these electronics products is still a bothering problem [cit] . at present, energy harvesting from human locomotion has been proven to be a convenient and promising way to continuously power wearable electronics products [cit] . various applications including but not limited to shoes [cit], clothing [cit], backpack [cit], and insoles [cit], have been exploited to convert human-generated mechanical energy to electrical energy. among them, shoes are an indispensable daily necessities in people's lives. it has attracted great attention from the research community as an energy harvester. because people generate a lot of energy when they walk and collecting energy through shoes is relatively simple and effective, the idea of charging electronic products with shoes emerged. the energy generated by the progress of walking can be converted into electrical energy to charge electronic products."
"after the training of the faster r-cnn model, tomatoes in separated, adjacent, overlapped, shaded states were tested and analyzed statistically. the detection accuracy of separated, adjacent, overlapped and shaded tomatoes was 95.5%, 93.8%, 78.4% and 81.9%, respectively. the results indicate that the proposed method has a lower detection rate for overlapped tomatoes and shaded tomatoes, which is due to the fact that a considerable part of tomatoes under complex environment have a higher cover rate and some tomatoes in the image are too small to detect. what's more, to better evaluate the detection performance of overlapped tomatoes, we randomly selected 80 tomato images in the dataset and counted the number of all overlapped tomatoes in the images. we use cover rate to define the degree of overlap, which is defined as the ratio of the number of observed pixels to the actual pixels. the tomatoes in the picture are divided into three categories according to cover rate, and the detection accuracy of the proposed method for tomatoes with different overlapping degree is tested. the detection accuracy of tomatoes with cover rate less than 30% is 81.5%, the detection accuracy of tomatoes with cover rate between 30% and 50% is 59.3%, and the detection accuracy of tomatoes with cover rate greater than 50% is 13.3%. the results indicate that this method has a lower detection rate for tomatoes with larger overlap."
"(iii) finally, socgraph provides a functionality to construct and visualize the individual graphs for particular time periods as well as to create an animation of graph evolution over time. the length of particular periods (measured in days) can be entered in the \"sliding window\" input of the \"animation\" field of the control panel. the user obtains the dynamic graph by clicking on the \"refresh\" button (left to the \"play\" button) of the player control strip. once the data is loaded a time line with the number of edges in each sliding window period appears in the \"graph view\" tab. to start the animation, the user should click the \"play\" button. the animation speed can be controlled by the adjustment of the value (in seconds) in the \"speed\" input. the playback can be paused with the \"pause button. note, that the player also allows step by step forward and backward rendering of the graphs, triggered by the corresponding player control buttons."
"in practice, this means requiring errors to have different signs and magnitudes distributed around zero, so that, from a mathematical point of view, this favourable situation for the generic signal i could be described in terms of a quasi-normal gaussian distribution, with mean equal to zero and standard deviation , of the i k groups' reconstruction errors of each"
"after research and analysis, the innovative multi-functional shoes analyzed in this paper uses the linear generators for energy harvesting, which simplifies the structure and improves the energy conversion rate. the heel support material adopts a rubber column, which satisfies the compression type variable similar to the spring and also reduces the impact of motion that increases the comfort of the shoes. moreover, the rubber column is similar in material to the sole itself, so there is no compatibility problem and durability increases. this innovative multi-purpose shoe is a versatile shoe that combines charging, vibration reduction and heightening."
"tomatoes are one of the most important and popular fruit crops. tomatoes offer humans many essential and beneficial nutrients such as antioxidants and vitamins c and a. as tomato demand increases, tomatoes are increasingly grown in greenhouses. however, manual harvesting is time consuming and costly, and as china's labor costs"
"medium-low; dependent on the type of regression model adopted to run the wrapper optimization of the ensemble dimension parameters, opt m and opt k ."
"a relevant issue for robust signal reconstruction is that each signal i be included in a suitable number of groups 1  i k, i.e. to have appropriate redundancy of signals representation in the ensemble. the average signal redundancy in the groups of the ensemble can be computed as:"
"however, in some instances, the area of overlap between tomatoes (as shown in fig. 7c ) is too large, and using the open function to remove the intersecting parts will lead to a considerable loss of tomato contour information. in these instances, to detect a single tomato, it is important to fully utilize the edge features of the overlapping area. traditional edge detection methods such as canny, prewitt and sobel are based on derivative filtering and are affected when the edges are blurred, noisy and inflexible [cit] . previous studies [cit] developed edge feature descriptors for detecting edges, but these methods have difficulty identifying edges in fuzzy states, and the edge detected is not continuous."
the empirical (real) cdf of the reconstruction errors is compared with the (ideal) cdf of a quasi-normal gaussian distribution () g f  of mean zero and given  and the maximum absolute distance
"finally, in order to give an overview of the advantages and limitations of the two approaches, table 2 summarizes the characteristics of the signal grouping structure and the ensemble reconstruction performances obtained by the rfse and moga approaches, respectively. signal grouping"
"where, d is the diameter of the rubber column. it is desirable that the rubber column ring has a stiffness of 140 n/mm. the size of the rubber column ring is calculated as follows:"
"in this work, a procedure is proposed for the reconstruction of signals coming from faulty sensors among a large set. the procedure is tailored for realistic applications where the number of measured signals is too large to be handled effectively by a single reconstruction model [2, [cit] . the approach is based on the subdivision of the set of signals into overlapping groups, the development of a reconstruction model for each group of signals and the combination of the outcomes of the models within an ensemble approach [cit] (figure 1 )."
"furthermore, looking at the results for pattern 731 (corresponding to the shut-down transient) one sees that the ensemble trained without bagging does not become aware of the disturbance affecting it (bottom-right graph) and the errors distribution remains almost unaltered, as evident from the comparison between the top-right and bottom-right graphs; instead, bagging allows recognizing the occurrence of the disturbance and the distribution of the models' errors is adjusted to become more similar to a quasi-gaussian distribution. in this view, the superior performance in the reconstruction of pattern 731 without disturbance obtained with bagging (top-right) can be explained by considering the transient part of the signal as disturbances, on whose reconstruction bagging has certainly positive effects."
"the robustness of both rfse and moga ensembles ( * e , figure 9 ) is considerably improved when bagging is applied as indicated by the increase of the corresponding ensemble output diversities on disturbed signals ( * out , figure 10 ). nevertheless, when performing bagging on the rfse ensemble, the increase of the output diversity on undisturbed signals ( out , figure 10) is not followed by an increase on the reconstruction performances on undisturbed data ( e , figure 9 ). in general, performing bagging degrades the model accuracy in reconstructing undisturbed signals."
this work intends to contribute a practical application of theoretical concepts of empirical ensemble modeling to a largescale problem of monitoring and reconstructing a large amount of signals recorded by sensors at a nuclear power plant. figure 1 . the multi-group ensemble approach to signal reconstruction two issues are central to the ensemble approach: (1) the construction of the signal groups and (2) the combination of the outcomes of the individual models developed on the basis of the groups.
"an empirical measure is here proposed to verify the diversity injected by the rfse in the resulting ensemble grouping structure in terms of the diversity in the signal composition of the different groups (the so-called input diversity). let us consider a generic ensemble of k groups with different sizes figure 2 . it is such that, high pairwise input diversity values are assigned to those pairs of groups whose fraction of common signals is relatively low (e.g. lower than 30%), whereas it penalizes group pairs with too many signals in common (e.g. more than 50%). in the special case that two groups have the same number of signals, i.e., 12 kk m m m , then 12, kk com  is 1 (and thus 12 (2) the ensemble input diversity in  is, then, simply computed as the average of the signals diversities:"
"after morphological processing on the tomato image, most of the background pixels are removed, as shown in fig. 7c, making contour extraction of the target tomato more precise and simpler."
"finally, the rbas model has been tested for different types of crowd situations such as laminar, stop-and-go wave, turbulence and herding behaviors. the evaluations of the model are performed as compared with sfm and empirical data. the love parade disaster event was also evaluated using our model. the results show that our rbas model outperforms in diverse situation and models the realistic agents in crowd. this makes this model to overcome the problems of models already in use. as evaluations show that the model is able to create real life situations, this model can be employed for performing behavioral analysis of crowd situations with confidence."
"although all the above studies, which utilize color or shape features and machine learning, have made some progress toward automatic fruit detection and localization, several issues remain in the ripe tomato detection problem: (1) some color-feature-based methods cannot recognize single fruits;"
"first, images in different states of the environment were chosen and all the tomatoes including those far from the camera were labeled in the images to ensure the robustness of the dataset. of the labeled tomatoes, some are separated (separated tomato, fig. 5a ), some are adjacent (adjacent tomato, fig. 5b ), some are overlapping (overlapping tomato, fig. 5c ), and some are shaded by leaves or branches (shaded tomato, fig. 5d ). these conditions increase the robustness of the training set to meet the needs of practical applications in tomato detection in complex greenhouse environments."
"in this paper, we introduced a method that can detect a single ripe tomato by combining ifs with the faster r-cnn image detection method. the proposed method has several advantages over traditional methods. first, we labeled ripe tomatoes in different configurations (e.g., separated, adjacent, overlapping, and shaded) in a large number of images to train the faster r-cnn detector. we identified candidate mature tomato regions in images using the trained faster r-cnn classifier. the results showed that the trained faster r-cnn classifier can accurately and quickly localize candidate ripe tomato regions. then, we transformed the rgb color space for the candidate tomato region to the hsv color space. different tomato samples were segmented manually and used to establish the gaussian density function to remove the background from single tomatoes detected by faster r-cnn to obtain the candidate tomato body. in some cases, subpixels or adjacent tomatoes that interfere with tomato contour extraction remain around the tomato body. therefore, we conducted morphological processing on the tomato binary map to remove these extraneous subpixels and separate connected tomatoes to reduce the extra contour obtained by edge detection. finally, we proposed a tomato contour extraction method to further detect tomatoes that uses the ifs edge detection method to obtain the edge and then applies a contour detection method to connect edge breakpoints and remove redundant edge points. together, these operations connect the tomato contour and help to obtain accurate values for a tomato's width, height and center."
"the primary goal of this paper is to assess the feasibility of combining deep learning with edge segmentation to detect individual tomatoes in complex environments. to achieve this goal, several sub-goals must be met, including (1) extraction of ripe tomato features using faster r-cnn to recognize and locate candidate ripe tomato regions in complex greenhouse environments; (2) utilization of the gaussian density function of hue (h) and saturation (s) in the hsv color space to remove background content from the candidate regions;"
"the problem of validating the signals recorded by sensors can be tackled by means of empirical models based on fuzzy logic [cit] and neural networks [cit] . in particular, auto-associative models have been applied to the validation of nuclear signals [cit] . nevertheless, the single-model approaches typically used for signal validation can only handle a limited number of signals whereas practical applications often deal with a very large number of signals [cit] ."
"in this paper, a kind of innovative multi-functional shoes is developed. in the shoe, there are two main parts. one is rubber column used as heel material, which plays a role in cushioning and damping during movement and making the shoe more comfortable to wear. the other one is a circuit system designed to realize energy harvesting, energy storage, and emergency charging of electronic products during walking."
"(i) a static graph can be computed and visualized over the complete dataset, providing an overview of all comentions for the requested persons available in the internet archive. this mode is triggered by the \"graph view\" button in the \"update\" field of the control panel on the right."
"the network structure used in this experiment has two convolutional layers, one max pooling layer and two fully connected layers. we adopted the relu activation function. the training architecture for ripe tomato detection based on faster r-cnn is illustrated in fig. 4 ."
(2) some shape-feature-based methods cannot quickly locate a single fruit; (3) some methods cannot fully handle fruit overlap and leaf occlusion; and (4) classical edge operators do not work well in detecting the edges of overlapping tomatoes.
"in general, bagging amounts to generating a number  is large, the individual bagging training sets overlap significantly and the probability of not including a training pattern in any of the bagging training sets is very small, so that all the training patterns are likely to appear in at least one bagging training set and some patterns will appear multiple times in a given set; instead, if the fraction is small, some bagging training sets can be completely disjoint and some training patterns might not appear in any bagging training set. the total amount of diversity injected in the ensemble, hereby called output diversity, is the result of the combination of the rfse and bagging randomizations."
". for an accurate and robust reconstruction of the test values, the groups' reconstructions, or, analogously, their associated reconstruction errors, must be diverse. in this sense, let"
"to embed this quality in agents, a collision control function counters for the estimation of all the three kinds of collisions. the agent detects the expected interaction response in one simulation cycle and changes its direction again in next simulation cycle based on estimation of next interaction. this cycle performs as cognitive cycle. the function used to estimate collisions between the agent a and the agent under impact b is given by impact angle estimation and impact distance estimation by at simulation time ."
"the focus of this work is to separate single tomatoes from overlapping tomatoes. in this paper, ifs is used to segment the target tomato contour to separate it from the overlapping tomatoes, and the illumination adjustment algorithm is adopted to compensate for the illumination of the candidate tomato region, reducing the probability of recognizing the uneven illumination position as the edge. for the contour of overlapped areas, especially the two ends of an overlapped contour, there is considerable interference. traditional edge detectors, such as the canny operator, are too sensitive to noise, but ifs can blur the region with large interference on the edge of a tomato and effectively detect the edge of the region by setting an appropriate threshold."
"finally, if the fraction becomes too small (e.g., 005 . 0  b  ), the ensemble performances considerably worsen due to the lack of data representation in the bagging training sets."
"as mentioned above, the solid insole is placed above the rubber column and is directly connected to the rubber column and the moveable part of the linear generator, as shown in figure 2 . when the heel moves down, the solid insole is pushed down. with the solid insole moving down, the rubber column is gradually compressed and the linear generator moves vertically downward. when the heel lifts up, the elasticity of the rubber column will restore it to the initial state, while the linear generator is driven to move vertically upward. in this reciprocating motion process, the moveable part of the linear generator also reciprocates to continuously generate electrical energy. compared with the rotational generator [cit], the linear generator is used as the energy generating device, and it is not necessary to convert the linear motion into a rotary motion, simplifying the structure and increasing the energy conversion rate."
"we used hadoop and spark [cit] technology on a dedicated 25 node computing cluster with 1.3tb main memory and 268 cpu cores to extract co-mentions of persons directly from the archived documents. we stored extracted names, patterns, urls, date of the crawl and additional miscellaneous information in a relational database, which can be accessed from the server process in real time. the crawled documents of mime type text have been encapsulated in 346,000 web archive (warc) files."
"in order to show the effects of bagging, two test patterns of signal 19 included in 25 19  k groups are here analyzed in details: the first (#196) is a signal measurement taken during normal plant operation, whereas the other (#731) corresponds to a measurement during plant shut-down. in figure 8, the effects of bagging on the errors distribution of the 25 groups including signal 19 on the two test patterns are reported in both cases of undisturbed (top graphs of figure 8 ) and disturbed (bottom graphs of figure 8 ) patterns. when the pattern is disturbed, performing bagging allows obtaining more diverse groups predictions (i.e. the groups' errors distribution is more similar to that of a quasi-normal gaussian) and thus a more accurate and robust reconstruction of the pattern's correct value. on the other hand, if the pattern is undisturbed, performing bagging can degrade the ensemble performance by reducing the groups diversity in the reconstruction of that pattern, i.e. by negatively affecting the distribution of the groups' errors and rendering it quite dissimilar to the gaussian cumulative distribution, as for pattern 196 (top-left graph)."
"although fast r-cnn achieves better results than r-cnn does, it expends a considerable amount of time on its selective search of the identified bounding boxes. in faster r-cnn [cit], the region proposal network (rpn) replaces the selective search for extracting the proposed regions. this replacement greatly improves the model's speed and the accuracy of the results, largely because selective search uses the cpu for calculations while rpn uses a gpu."
"the results show the applicability of the proposed method. first, the proposed method can be applied to the visual system for tomato location to increase the grasping accuracy of the tomato picking robot. in addition, it can be applied not only to tomatoes, but also to other partially shaded or overlapped fruit detection and location systems. however, there are two problems, one is that the computer visual system itself is greatly affected by the light, when the light is too weak or too strong, which result in the loss of image information. the other is that excessively large errors in the deep learning bounding box affect the tomato contour extraction, which leads to inaccuracies in calculating the tomato's height, width and center."
"in future work, we plan to improve the performance of the deep learning classifier to detect distant tomatoes and tomatoes with high cover rates in the image and increase the recall rate of tomato detection. further more, we will study multi-sensor fusion technology to solve the inherent problems of computer systems that rgb camera performs poorly in extreme light conditions. for example, the combination of infrared sensor and rgb camera can improve the performance of computer vision system in the case of too strong and too weak illumination."
"the opt k groups, each one constituted by opt m signals, are then generated resorting to the rfse technique and the corresponding pca-based reconstruction models are bagging-trained. figure 5 sketches the scheme of the overall algorithm for devising and verifying the ensemble of models for signal reconstruction. figure 5 . sketch of the ensemble algorithm for signal reconstruction"
"to accurately and quickly find ripe tomatoes in the bounding boxes obtained from the faster r-cnn, we collected a large number of ripe tomato images from different directions and at different times. after a large number of experimental analyses, the h and s color space values can be used to quickly distinguish a sample from the background. therefore, in this study, we converted the rgb images to hsv color space and established the gaussian density function of h and s to further segment tomatoes from backgrounds at a threshold of 0.85. the gaussian density function of h and s is defined as follows:"
"a model that achieves high precision at all levels of recall will have a high ap score, while a model that achieves high precision at only some recall levels will have a low ap score. both recall and precision are important for characterizing the performance of a detector. accuracy decreases as recall increases, but the precision of a detector with good performance remains high as recall increases, which means the detector will detect a high proportion of tp before it starts detecting fp. the trajectory of the precision-recall curve of the network is shown in fig. 11 ."
"finally, notice that bagging slightly contributes to rfse at increasing the undisturbed error e , whereas the effect on e  due to rfse is more evident when comparing rfse with moga. this means that too much diversity randomly introduced in the groups does not improve the capability of reconstructing undisturbed signals, which instead comes from having highly correlated signals (as in the moga groups); on the other hand, a robust signal reconstruction in case of disturbances is due to high models diversity (as in the rfse and moga approaches with bagging)."
"in this paper we introduced a demonstration of socgraph -a social graph extracting system for large networks from internet archive data. in contrast to other research concerned with graph construction from web related data, we are focused on the temporal evolution of social networks implicitly contained in the stored web pages. in our future work we plan to include pattern filtering techniques, integrate data from search engines and evaluate event identification, as well as sentiment analysis of the personal relationships and their evolution. our system will be offered as a service within the eu project alexandria as part of a web observatory accessible to social and computer scientists as well as to general public for social network visualization and evolution analysis. the demonstration is available on our web page: http://socgraph.l3s.uni-hannover.de and can be used with any web browser. additionally, the web page contains a summary of the demo applications as well as a short video tutorial."
"the proposed model introduces the realistic crowd behaviors based on cognitive and physiological findings and have been demonstrated in path planning, situation awareness. to design the realistic simulations the physical size of the agents is very important, it should be same as real life pedestrians. the physical size is important to simulate turbulent flows and egress analysis [cit] . the agents in the simulator have been designed as ellipses with derived dimensions from architectural graphic standards [cit] . the model is based on helbing model [cit] . the agents in our model have same forces as sfm [cit] . borrowing the concepts from physics, the social forces concept means that forces are being modeled based on laws of motion such that person movement is due to the resultant forces acting on it, which in result will produce acceleration and thus affect the movement of the person. the resultant force on an agent a is given by, is the sum of three main forces and one random force ;"
"the area under the p-r curve, known as the average precision (ap), can be used as a single metric to summarize the performance of an object detection model; p represents precision and r represents recall. the performance metrics selected for validation purposes in this study are as follows."
", be the reconstruction error on the t -th test pattern of signal i by group k . if the errors ( ), 1, 2,...,"
"the crowd simulation model has been developed on c++. the simulation model allows to create different scenarios and range of agents. the variable numbers of agents enter into the simulator at random positions and time. the agents have random birth rate (time to come in simulation) which is modeled by the poisson distribution. the time at which the particles enter is also randomly timed based on normal distribution. figure 3. shows the crowd simulator ) is plotted. different types of flow of agents during simulations were tested. the original trajectory is the path taken by the agent during normal conditions i.e., laminar flow, the agent passes through the simulated environment without many interactions, this behavior is also observed during line formation situation especially at train, bus stations and narrow alley, as even the density increases, the crowd acts as self-organized to let the flow the agents. the stop-and-go waves event also appears, the ratio goes upto 110 %. the effort of agents to reach the destination increases as the density increases. this effect is also termed as faster-isslower effect [cit] . during turbulent flows, the density increases from start with density, as the people are moving with much varying speeds in crowd. this turbulent flow describes the panic behavior in which agents mostly do collision avoidance more than situation awareness. during herding behavior, the effort to reach the destination increases, as the agents act like a group and follow the front agents. the agents access the situation and give up their primary path planning and do sliding motion among crowd and follow the other agents which are dependent on the flow of crowd also. this also demonstrates the group behavior. the effort to reach the destination in herding behavior is less than turbulent flow. these results have been demonstrated in the figure 4 to evaluate our simulation model with real data, we performed the simulation by mimicking the exact scene from the videos of eth walking pedestrian dataset biwi [cit], in which we collected the empirical data of spread of pedestrians in the street. the speed and occupancy of agents in street for 480 seconds is recorded. the same scenario was designed in simulation and first the sfm [cit] based particles were and then our model rbas was used. during these simulations, the data is collected using different flows such as unidirectional, counter, cross flow and all directional flows. the occupancy is defined as the area covered by the agents in case of simulation and by pedestrians in cases of eth video. the agents vary from 0 to 30 and it corresponds to same number of people as in video. the trajectories have been plotted and demonstrated to show speed density relationship in figure 4 (b) . the results show that our rbas model has the similar behavior as dataset, while particles in sfm model lacks to perform according to the situation."
"as shown in fig. 9c, the contours of the adjacent tomatoes in the target tomato contour are not removed. to remove these redundant contours, the redundant edge point removal method is required. first, the closed area of the main body of the tomato is filled, as shown in fig. 10a . then, a threshold method is employed to determine whether a point is a target point. the removal result is shown in fig. 10b, and the boundary of the target tomato is presented with a blue line in fig. 10c ."
"where, u 0 is the maximum vertical displacement of viscoelastic materials, f 1 is the force generated by the viscoelastic material when the displacement is u 0 . in this test, since the viscoelastic material is only pressed and not pulled, the force and displacement values for each point on the curve are positive, and the origin of the coordinate is not at the center of the ellipse. when processing data, the local coordinate system f'-u' can be established with the center of the curve as the origin, as shown in figure 7 . let the origin 0' coordinate of the local coordinate system be (x, y), the formula for calculating the equivalent stiffness of viscoelastic materials under compression in local coordinate system can be derived:"
"we will explore the social networks of barack obama (node nbo) and john mccain (node nmc ) [cit], roughly corresponding to the us presidential election,as an example. the weight per entry has been set to 0.2, which means that we allow data records with up to four additional persons. the edge weight filter has been set to 0.025, in order to not overload the graphs and focus on the interesting entities. the static graph is plotted in fig.3 . a dynamic graph has been constructed with a sliding window of 30 days. the series of graphs are shown in fig.4 . for instance we observe that hillary clinton (nhc ) node was connected to (nbo) [cit], corresponding to the time point, where hillary clinton endorsed barack obama and withdrew her candidacy. in following the sizes of both nodes, nbo and nmc, remain similar until november, where the actual election took place and nmc drastically reduced the weight already in following month after barack obama became a president of the united states. shortly before in september, sarah palins' (nsp ) talk on the side of john mccain had positive impact on the votes for this candidate. this fact is also reflected in our graph where nsp appeared in september connected to nmc ."
"the crowd of people is a complex design system. to handle the complexity, one way is to model the situation comprised of individual people into simulations and making the results based on them. such techniques are called agent based modeling and are related to social sciences areas and introduce randomness in model. they are already in use in different and diverse areas, such as molecular biology, economics and physics. human behaviors are complex emergent phenomena, which are difficult to encompass using just mathematical or physics knowledge. the basic steps to make simulations are 1) designing of agents with sufficient information which should enable them to behave same as pedestrians in simulation 2) creating different kinds of real life situations such as panic situation and evaluating the behavior of agents, which should be same as real life. there are different crowd simulation models which can be divided into three types 1) cellular automata 2) velocity based 3) flow based. cellular automata based discrete models have been in use mostly for architectural purposes [cit] . these models define the two dimensional space with fixed blocks and occupants can move from one block to another following the movement rules such as keeping in view the free space if not occupied by another agent or obstacle. these type of models failed to simulate the situations when high density crowd is encountered as agents get limited only to restricted moments, therefore during turbulent flow situation, it fails to portray well these situations. later, [cit] the game based models have introduced the plenty based payoff rules to improve the collision avoidance rules. but still it lacks in mimicking the turbulent flows in high density crowds efficiently. velocity based models emerged from research in game designs. [cit] design the agents to select their behavior mostly from choosing the direct path to the destinations and changing their velocities. these models design the agents to plan their paths ahead to avoid collisions but they also lack in planning the variable velocities in high density crowds as every condition changes during high density crowds due to turbulent flows. in flow based models, the most famous is social force model [cit], which have been used with different modifications [cit] . this model models the agents as particles which move under effect of different kinds of forces. the proposed rbas model is based on social force model and defines particles as agents which have capability to plan their paths to avoid collisions. during the turbulent flows, the agents behave as sliding bodies which gives the exact situation of a real crowd."
"second, we used faster r-cnn to train the labeled images using the network structure shown in fig. 3 . we fine-tuned our network and then tested the effect of the trained detector on the validation dataset. for example, an original image is shown in fig. 6a . faster r-cnn was used to recognize and locate tomatoes, as illustrated in fig. 6b . because background content such as leaves, stems and other tomatoes occurs near the target tomatoes, the tomatoes cannot be detected accurately. to accurately acquire a single tomato, further processing for the tomatoes in the bounding boxes created based on faster r-cnn recognition and location is necessary."
"the architecture of our system is schematically shown in fig. 1 . first, we analyze the web archive to detect comentions of entities in the web pages. in the next step, at the server, we extract the temporal statistics and construct the social graph by connecting extracted entity pairs using detected edges. finally, the user can access the application and create, visualize, modify and interact with the graphs by issuing new queries via a web browser based user interface. in the following we provide an overview of the system components and show how results are presented to the user in more detail."
"in this work, the rfse technique is exploited to ensure good global ensemble properties of group diversity, signal coverage, and redundancy. the group size parameters, i.e. the number of signals in the groups and the number of groups in the ensemble, are determined by means of a wrapper search aimed at maximizing the ensemble performance in terms of minimum reconstruction error. pca signal reconstruction models are trained on data sets constructed by bagging to inject further diversity in the models themselves."
"the optimal group size opt m is a problem-dependent parameter [cit], which is chosen within a pre-defined range of group, an ensemble of k groups of m signals is generated by randomly sampling the signals of the groups by rfse, k corresponding pca-based reconstruction models are trained on different data sets obtained by bagging and the ensemble reconstruction performance is computed. since the scope of the work is to provide a robust ensemble of groups, the ensemble performance is computed also on a set of test signals artificially disturbed (see section 4.1 for details on the disturbance procedure). the optimal group size opt m is the one corresponding to the ensemble of groups providing the best reconstruction performances on both undisturbed and disturbed signals. notice that searching for an optimal group size equal for all groups is necessary in practice to reduce the number of parameters to be optimized."
"the same situation can be calculated from the above studies. the energy harvesting in slow running and normal running is shown in table 3 . in 1 second 2-steps slow running, the average single foot force of a 70 kg male walking is 500 n. with reference to 0.5 hz, and the stiffness data of the rubber column under loading of 4 mm is 1063.95 n/mm. it can be calculated that the stiffness of the rubber column ring at this time is 174.5 n/mm, and the deformation of the 500 n force is 2.87 mm. also, it can be calculated that the dc voltage generated in 1 step can reach 3.932 v and the energy charged can reach 773.0 µj. it can fill 100 µf electrolytic capacitor in about 40 s. similarly, in 1 s 4-steps normal running, the average single-leg force of a 70 kg male running is 350 n, with reference to 2 hz, and the stiffness data of the rubber column under loading of 2 mm is 1386.36 n/mm. it can be seen from the calculation that the stiffness of the rubber stud at this time is 227.36 n/mm, and the deformation of the 350 n force is 1.54 mm. it is calculated that the voltage generated by the 1-step compression force and the restoring force can reach 4.711 v. regardless of the damage of energy, the voltage generated by the generator is theoretically 4.240 v after passing through the bridge rectifier, at this time, the energy charged of the capacitor is 898.9 µj. it only takes 18 s to fill a 100 µf electrolytic capacitor."
"concerning the bagging procedure, in general the results show that injecting diversity by training the models on different data sets improves the ensemble performances. furthermore, the reconstruction errors decrease (especially for disturbed signals) if the fraction of sampled training patterns b trn n in the bagging training sets becomes smaller. in fact, by so doing, many bagging training sets are completely disjoint and therefore highly diverse. as the optimal fraction, despite a slight worsening of the ensemble performances on the undisturbed set. such a low value can be explained by the fact that excluding many training patterns from the signal data set allows removing with high probability those spurious signal measurements (e.g. spikes) that usually compromise the reconstruction of all the signals in the single groups."
"). when the signal is undisturbed the highly correlated signals of the (less diverse) moga groups allow for a more accurate reconstruction, whereas as soon as the drift begins the more diverse rfse groups are capable of providing a more robust signal reconstruction."
where l is the maximum luminance value in the image and n i is the number of pixels at level i. the foreground and background probabilities of occurrence are given by
"where w k,i and w l,i are the individual node weights. we refer to such graphs as static graphs. to study the temporal evolution of the social networks we also construct dynamic graphs, consisting of sequences of static graphs for disjoint intervals of length ∆t within the time period t ."
"as previously stated, high diversity in the ensemble of models is beneficial to its performance of signal reconstruction. to this aim, in this work diversity is imposed onto the pca models by randomizing the features of the groups upon which they are built with the rfse technique [cit] and the data upon which they are trained with the bagging technique [cit] . optimization of the group size m and ensemble size k, is also carried out to improve the performance of the ensemble."
"(ii) for a specified sliding window a temporal statistical plot can be generated, showing the number of raw comentions as well as the weight of the persons of interest. this mode is triggered by the \"temporal statistic\" button in the \"update\" field of the control panel, the results are displayed in the \"temporal statistic\" tab of the demonstration interface."
"in a natural environment, tomatoes are distributed in various positions. for adjacent tomatoes, as shown in fig. 9c, it is highly probable that the contour calculated by the edge detector from an image will be connected with the contour of an adjacent tomato. therefore, a tomato edge contour detection method that connects edge breakpoints and removes redundant edge points was developed to extract the contour of a single ripe tomato. broken contours are completed using the edge breakpoint connection method. the edge breakpoint connection method for the tomato is a two-step process based on the breakpoint positions. in the first step, the edge of the tomato is truncated by the faster r-cnn bounding box, and the truncated edge breakpoint connection method is used. if there are two breakpoints on the same block edge, that edge is considered to be truncated by the faster r-cnn bounding box. to complete the contour, we connected the two breakpoints on the truncated edge. in the second step, the edge contour obtained using the edge method has a missing part. to connect the breakpoints on the nonborder edges, we used the cubic polynomial function to fit the curve of the missing contour from several points near the two matching breakpoints. in a natural environment, tomatoes are distributed in various ways. for adjacent tomatoes, it is highly probable that the contour calculated by the edge detected from an image will include the contour of an adjacent tomato. to remove redundant contours, a redundant edge point removal method is required. first, the closed area of the main body of the tomato is filled. then, a threshold method is employed to determine whether a point is a target point. for each point, we define the number of its 8 neighborhood pixels that have of a value of 1 as k. if k is less than a set threshold value t 1, the point belongs to the adjacent tomato. in this study, the threshold t 1 was set to 3."
"this work is partly funded by the european research council under alexandria (erc 339233), by the european commission under grant agreements 619525 (qualimaster) and 688135 (stars4all)."
"sensors contribute to the safe and efficient operation of modern plants by conveying information on the plant state to the automated controls and the operators. to avoid misleading information which may lead to unsafe and/or inefficient states of operation, it is important to detect sensor malfunctions and possibly reconstruct the incorrect signals. this requires monitoring the sensor performance and has the potential benefits of reducing unnecessary sensor maintenance and increasing confidence in the recorded values of the monitored parameters, with important consequences on system operation, production and accident management [cit] ."
"the performance test of the rubber column is carried out. and then the structure and geometric size of the rubber column is optimized according to its performance, human weight, and gait. finally, the stress and energy harvesting of the multi-functional shoes in different gait conditions are analyzed. the analysis results show that the developed innovative multi-functional shoe has good stability and versatility when working in different gait, and the rubber column is beneficial to improve the wearing comfort of energy harvesting shoes."
"indeed, the error scales are small (especially for the undisturbed signals,) and the error bars (which have not been inserted for visual clarity) are superposed. nevertheless, from an operative point of view, a decision on how many groups to use and which bagging fraction to adopt must be made. for this reason, 25  opt r is the value chosen for the desired signal redundancy and, correspondingly, 142  opt k is the fixed ensemble size. this choice allows having good ensemble performances with a low computational cost and furthermore it allows having an honest comparison with the genetic algorithm-based optimized grouping [cit] which is presented in section 4.4."
"in general, even though the methodology proposed in this work is obviously problem-dependent with respect to the optimal grouping parameters which must be discerned for the specific data set, these results show that it is applicable to any problem involving a large number of signals to be validated and reconstructed for which training a single model entails convergence problems (such as for the iterative training of auto-associative neural networks) and shows significantly less robustness."
"where, v 0 is the voltage generated by the linear generator, e is energy stored in the capacitor, c is the capacitance and v is the voltage across of the capacitor. in this paper, the electrolytic capacitor with a withstand voltage of 35 v and a capacity of 100 µf is selected. the maximum energy storage of the electrolytic capacitor:"
"the robustness of the rfse and moga ensembles has been then specifically tested for comparison on the reconstruction of faulty signals in case of multiple sensor failures. ten signals have been chosen as objects of the analysis. approximately, the first half of the signals has been left undisturbed as in the normal operation, while, in order to simulate a sensor failure, a linear drift decreasing the values of the signals up to 25% of their real values has been introduced in the remaining test values. the validation set has been linearly divided into training and test only once, i.e. without crossvalidation. figure 11 shows the results of the reconstruction of signal 214 (electrical power) obtained by the rfse and the moga approaches both trained with bagging procedure ( 0.02"
"for building the social graph, we select extracted pairs matching the user query and user defined parameters such as the time period t, and merge them into a network considering the node weight and the edge weight between two nodes as follows:"
"in this work, a wrapper optimization of the ensemble performance is carried out in order to determine the group size opt m (i.e. the optimal number of signals to have in each group, equal for all groups in the ensemble) and the ensemble size opt k (i.e. the number of groups in the ensemble, each one constituted by opt m signals)."
"when a large number of people gather in a confined space or area, they make a crowd. it is difficult to know that how they will move and pass through a confined space and how the buildings and pathways should be built such that people passing through them do not have situation of congestion and jamming and they pass easily without injuries or problems. it is cumbersome and dangerous to bring a large number of people along and to perform the experiments, therefore it is feasible to make the realistic simulation and making the model and then collecting real users data based on the results from that model."
"in this work, a novel approach to the reconstruction of the signal values from faulty sensors has been proposed, based on an ensemble of pca models. the set of sensor signals, too large to be handled effectively with one single reconstruction model, is subdivided into small, overlapping groups and a pca-based reconstruction model is developed for each group. the outcomes of the models are then combined by simple averaging to obtain the ensemble signal reconstruction."
a few connected pixels also exist that are outside the body of the tomato in fig. 7b . to remove as many of these as possible while retaining the target image. we can use the open function to separate overlapping tomatoes by interrupting the connection area in the overlapping area. this method is commonly used to separate objects with subtle connections in an image and eliminate noise. the open function is defined as follows:
controlled a priori and optimized based on accuracy and robustness. maintained on average after selecting randomly the groups' signals and evenly distributed among the signals.
"(3) separation of target tomatoes from adjacent tomatoes and removal of sparsely connected pixels based on morphological processing; (4) performance of illumination compensation on the tomato image using an illumination adjustment algorithm; (5) detection of a single tomato in an image block using ifs; and (6) calculation of tomato parameters such as width, height and center."
"in the demonstration we will primarily show how the socgraph time travel graph system works and how the social networks are constructed from the content of ia web pages. we will demonstrate the graphical interface usage for static and dynamic graph visualization. additionally, we can elaborate in more detail on the person pair extraction process and explain the underlying parallelization algorithms."
"the power acquisition device is a circuit system consisting of a linear generator, a storage circuit and a charging interface. the circuit system enable energy harvesting and storage during exercise, and then realize emergency charging of electronic products."
"(iii) the diversity of the groups and of the associated models: ensembles based on diverse models are generally more reliable and robust [17, 19, 21, [cit] ."
"in this work, the groups and ensemble sizes are derived by means of a wrapper optimization. to promote group diversity, signals groups are randomly generated resorting to the random feature selection ensemble (rfse) technique [cit] . the groups thereby created are then used to develop a corresponding number of signal reconstruction principal component analysis (pca) [cit] models. the models are then trained on different data sets randomly generated using the bootstrapping aggregating (bagging) approach [cit] in order to inject further diversity in the ensemble of models. the methods for both signal grouping and reconstruction modelling have been chosen simple, consistently with the idea that simplicity is an added value in nuclear safety, where possible. other more sophisticated methods to generate different training data sets exist, e.g. adaboost [cit], which seem worth consideration in future research for nuclear applications."
"in this section, we discuss in detail the method used in this study to segment the contour of a single ripe tomato. the flowchart of the recognition method for clustered tomatoes is shown in fig. 2 ."
"bx proposed the idea of the paper, completed the test, analyzed some experimental data, and wrote the paper. yl completed some of the writing of the article and the analysis of the experimental data."
"an additional advantage of adopting ensembles of diverse models is an increased robustness of the ensemble-aggregated output [17, [cit] . indeed, the conjecture is that, when performing the ensemble signal reconstruction, if the signal predictions obtained by the individual models are diverse (e.g. the reconstruction errors are different in magnitude and sign), their opportune aggregation will provide a more accurate and robust signal reconstruction [cit] . theoretical studies have investigated the concept of diversity amongst the models of an ensemble [20-23, 25, 26] and the ways of appropriately aggregating the outcomes of the diverse models [cit] ."
"the associate editor coordinating the review of this manuscript and approving it for publication was mohan venkateshkumar . rise, the adoption of agricultural automation processes is inevitable. such processes are of great significance for reducing agriculture labor costs and improving a country's industrial structure. therefore, it is necessary to develop automatic tomato pickers. although most agricultural robotsfruit harvesting systems in particular-use computer vision to detect fruit targets, accurate fruit detection is a challenging research topic. it is difficult to develop a vision system that functions as intelligently as a human and can easily identify fruit, especially in the presence of overlapping fruits or large leaf occlusions. the performance of the robot's visual system directly affects tomato picking and operational safety. improving the recognition rate of the visual system can increase the locating accuracy of the robot arm. in this study, we mainly aimed to identify ripe tomatoes based on a vision system. systems designed to count or harvest fruit require accurate detection schemes that can overcome challenges such as naturally occurring changes in illumination, shape, pose, color and viewpoint."
"in this study, we conducted multiple steps to extract the final contour of a single tomato. the tomato images were obtained using a consumer-level regular digital camera (usb) and a kinect v1.0 device at the jiangsu academy of agricultural and sciences, nanjing, china, from 9:00-11:00 a.m. and from 4:00-6:00 p.m. we acquired 800 sample images under different environmental states, including separated, adjacent, overlapping and shaded tomatoes. of these images, we used 600 images for training and 200 images for testing. we conducted numerous experiments to evaluate the accuracy and practicability of the proposed method. in section iii.a, the accuracy and regression rate of faster r-cnn for ripe tomato detection were evaluated. in section iii.b, the contour segmentation processes of tomatoes in four different states were described to show the applicability of our proposed method for tomato segmentation. in section iii.c, we report the results of comparison experiments conducted to analyze the error parameters for a single tomato."
"heuristic methods such as multi-objective genetic algorithms (mogas) [cit] have proved effective in scanning the large search space of possible groups 1 to optimize their individual properties, e.g. by maximizing the correlation of the signals in the groups [11, [cit] and minimizing their reconstruction errors [cit] . moga approaches have however shown some limitations in guaranteeing the mentioned global ensemble properties, e.g. diversity among the groups, adequate signal inclusion and redundancy, at the basis of the optimality and robustness of the performance of the ensemble [cit] . furthermore, the high computational cost required to run a moga search renders the method unfeasible for large-scale applications involving thousands of signals."
"have all the same magnitude and sign, then their aggregation (i.e., the ensemble reconstruction of the t -th point of signal i ) will be affected by a bias and will not be accurate; on the other hand, if errors are diverse in magnitude and sign (i.e. if the groups provide diverse reconstructions), their combination provides a more accurate estimation of the test pattern of the signal."
"the overall modelling scheme has been applied to the reconstruction of signals collected at a finnish nuclear pressurized water reactor. two additional case studies have been analyzed for validation purposes. the performances of the proposed approach have been compared with those of an equivalent ensemble obtained by means of a multi-objective genetic algorithm aimed at maximizing the intra-group signal correlation and the inter-group signal diversity. considering the difficulties of calibrating the ensemble dimension parameters and of maintaining the group signals diversity in the moga optimization, and its not negligible computational cost, the rfse approach has proved more efficient for large-scale applications involving hundreds of signals."
"where, v 0 is the induced electromotive force generated, and its unit is v; n is he number of turns of the coil; is the amount of change in the magnetic flux, and its unit is wb; t is the time taken to change, and its unit is s. in a uniform magnetic field, the product of the magnetic induction b of the magnetic field and the area s m of the direction perpendicular to the magnetic field is called the magnetic flux passing through this surface:"
"if a pixel point function value in the image is greater than the threshold, it belongs to a ripe tomato; otherwise, it belongs to the background. taking the right-hand tomato block shown in fig. 6b as an example for further segmentation, the ripe tomato was extracted from the background using the gaussian density function of h and s in the hsv color space. taking the tomato in the bounding box on the right in fig. 6b as an example, the process of obtaining the edge of the tomato is shown in fig. 7 . the original image is shown in fig. 7a . fig. 7b illustrates the result obtained using the gaussian density function of h and s, which leaves some subpixels at the edges of the target tomato. hence, the further segmentation for ripe tomatoes from backgrounds is necessary."
"the main function of innovative multi-functional shoes is to collect electric energy continuously during people's walking process, and at the same time to ensure the comfort of people walking. in order to achieve these functions, the multi-functional shoe mainly consists of the rubber column and the power acquisition device which consists of a linear generator, a storage circuit and a charging interface, as shown in figure 1 . at the heel of the shoe, a ring shaped rubber column is placed inside. and a linear generator is placed at the center of the ring. the solid insole is placed above the rubber column and is directly connected to the rubber column and the movable part of the linear generator, as shown in figure 2 ."
"future developments will be using this simulation model and creating a behavior detection algorithm. then, using that algorithm to embed behaviors in agents in crowd."
"a robust ensemble of models is expected to be able to reconstruct the signals from faulty sensors, e.g. due to random noises, offsets or drifts. in other words, when a faulty signal is sent in input to the pca models which include that signal, their ensemble should still provide in output a good estimate of the true value of the signal, by exploiting the information coming from the non-faulty signals in the groups of the ensemble."
"because of non-uniform illumination, the edge segmentation algorithm can easily identify locations with large differences in illumination on the tomato surface as edges. hence, before conducting ifs on an image, we conduct illumination compensation on the tomato image. there are many solutions to the problem of non-uniform illumination (e.g., retinex algorithm, histogram equalization algorithm, gamma correction). in this paper, a novel retinex-based light-processing algorithm is used for image illumination adjustment [cit] because this method works best with our tomato dataset. following this step, we transformed the image without background to grayscale and used ifs to perform edge segmentation on the grayscale image."
"we conducted three experiments in this study. one experiment analyzed the precision and recall of faster r-cnn for ripe tomato detection; the ap achieved was approximately 80% despite the complexity of the greenhouse tomato images used, which include adjacent, overlapping and obscured tomatoes. the second experiment presented the contour segmentation process of tomatoes in four different states. the results show the applicability of our proposed method for tomato segmentation. the last experiment validated the tomato localization performance. we conducted comparison experiments to analyze the parameter errors for a single tomato using the proposed method and faster r-cnn with the canny operator, compared to manual measurement. the proposed method is able to accurately calculate the width, height and center position of a single tomato in an image; the rmse values for the width and height are 2.996 and 3.306 pixels, respectively. the mean relative error percent (mre%) for the shifted horizontal and vertical distances of the center positions are 0.261% and 1.179%, respectively. if we use faster r-cnn without further contour detection to detect the tomato, the rmse values for the width and height are 7.915 and 8.436 pixels, respectively. these results demonstrate that the proposed method can localize the tomato center more accurately than faster r-cnn alone."
"first of all, the data set x of n patterns available is partitioned into a training set x trn (made of trn n patterns) and a test set x tst (made of tst n patterns). the former is used to train the individual models, whereas the latter is used to verify the ensemble performance in the reconstruction task."
the combination of the outcomes of the ensemble of models is performed by simple averaging [cit] . this way of combining the models outputs can be seen as an extension to a regression problem of the simple voting (sv) technique adopted in classification problems to combine the class assignments of the single classifiers constituting the ensemble [cit] .
"further diversity can be injected in the ensemble of models by training them on different data sets. in this work, this is achieved by the bagging technique which has proved successful in many applications [cit] ."
"for shaded tomatoes, the segmentation process is shown in fig. 15. in fig. 15a, the front tomato is shaded by leaves and the back tomato is obscured by stems. first, the candidate regions of the two tomatoes were detected by faster r-cnn, as shown in fig. 15b . the bounding boxes of the two tomatoes include areas of the other tomato, which also greatly increases the difficulty of obtaining the complete tomato contour. by using the gaussian density function of h and s, the leaves and stems of the tomatoes were removed, as shown in fig. 15c . next, morphological processing was used to extract the target region for each candidate region, and the good results shown in fig. 15d were achieved. fig. 15e shows the result of the illumination adjustment algorithm. in fig. 15f, since the canny operator has poor anti-interference performance, there are many interruptions on the contours where stem and nearby tomato cause errors in contour detection. fig. 15h presents the edge image produced by using ifs with a threshold of 0.1255. parameters a and b for edge detection masks were 0.3 and 0.75, respectively. as shown in fig. 15h, the visible parts of contours with the stem and the adjacent tomato were detected with fewer redundant edges inside the contours, and the proposed method was able to detect more blurred contours near the stem than the canny operator. the processed results for the edges in fig. 15h are shown in fig. 15i . although the left tomato was shaded and split into several parts by tomato leaves and stems, the proposed method still accurately detected a single tomato in the shaded state."
"this section intends to illustrate specific situations in which bagging is and is not effective in terms of the output diversity. indeed, the bagging procedure is intended to enhance the output diversity of the ensemble. according to the adopted output diversity measure for the generic signal i, the models built using the signals of its i k groups are diverse if they make different errors in reconstructing the signal. more precisely, it is conjectured that the most beneficial diversity is achieved if the reconstruction errors for each signal's test point are distributed as a quasi-normal gaussian function (section 3.3)."
"the power acquisition device is placed on the heel and the central axis of the linear generator is perpendicular to the sole. the principle of the linear generator is shown in figure 3 . the two series connected coils of the linear generator are symmetrically wound to the guide device in the opposite direction, resulting in the same phase of the electrical signals produced in the two coils. during walking, the movement of the permanent magnet in the linear generator is caused by the force of the heel received by the shoe containing the energy harvester. the movement of the permanent magnet causes a change in the flux linkage in the coil, thus inducing a voltage between the ends of the coil. the linear generator is also connected to a bridge rectifier to charge a capacitor, as shown in figure 4 . schottky diodes are used in the bridge rectifier due to their threshold voltage. dc voltage after tuning by the bridge rectifier:"
"the significance of the findings is to be framed within the practical problem tackled and goes beyond the numerical values of the results obtained. the ensemble approach proposed provides a feasible way for handling the validation and reconstruction of a set of a large number of signals,which cannot be handled by a single model. the method has proved its effectiveness, its low computational cost (an essential aspect for the application of any on-line sensor monitoring system) and its applicability to different signal sets. finally, even though the numerical values show sometimes little improvements with respect to the reconstruction error, these must be seen in terms of enhanced robustness and plant production and safety when the validated and reconstructed signals are effectively used during operation. in this respect, notice the criticality to have a robust and accurate reconstruction of those signals that are used by the plant control systems, for which a small error in the reconstruction can lead to wrong control actions, possibly with significant production losses and safety threats: this last aspect is part of currently ongoing research."
"in the case in which each group includes the same number of signals (i.e., m m k , k  ) and each signal is included in the same number of groups (i.e., r k i , i  ), eq. (8) becomes:"
"we identified the primary causes of error, showing that the accuracy of the final contour segmentation result depends on the accuracy of the initial location and the recall of faster r-cnn. we analyzed the detection results of faster r-cnn classifier for four kinds of tomatoes. the detection rate of overlapped tomatoes and shaded tomatoes is relatively low, which is due to the large ratio of overlapping and shading for a part of tomatoes in the natural environment. for separated and adjacent tomatoes, the reason that cause an effect on the detection rate is that there are a few very small tomatoes in the images which are difficult to detect by faster r-cnn. those with an cover rate of more than 50% (fig. 14a ) have a 13.3% probability of detection, and for those with too much cover rate cannot even detect (seen in fig. 14b ). we found that the detection rate will decrease with the increase of cover rate of tomato."
"regarding the integration of the outcomes of the individual models, many techniques can be adopted ranging from the simple arithmetic average to weighted average [cit], local fusion [cit] and dynamic integration techniques [cit], with increasing computational costs. in practice, there is no single combination rule that is universally recognized better than the others and it is still not clear that more sophisticated and complicated aggregation techniques are actually beneficial to the ensemble performance; then, for the same reason stated above and in absence of any other prior information, the simple arithmetic average is used as a valid choice for combining the models predictions [cit] ."
"the rubber column is made into a ring shape, so the power acquisition device can be placed in the middle of the rubber column ring. the rubber column ring is made of viscoelastic materials. the material is kind of passive damping material with certain stiffness and damping. that means, the material has the compression type variable similar to the spring. when an external force is applied to the viscoelastic material, it is compressed and deformed; when the external force is removed, it can quickly return to its original state. when the viscoelastic materials is constantly deformed, its damping characteristics can reduce vibration and have a cushioning effect. therefore, the use of the viscoelastic rubber column as a heel support material can reduce the impact during exercise and improve the comfort of the shoe."
"considering that different overlapping groups have some signals in common, or, dually, different signals have some groups in common, the total number of signals (with repetitions) in the ensemble of groups can be expressed either by summing the number of signals k m constituting each group k k,..., 2, 1  (i.e., the groups' sizes) or by summing the number of"
"the graphical user interface of socgraph is accessible through a web browser, see fig. 5 . the \"person\" input field is expecting a query such as a name of a person or a list of person names. the \"time span\" selectors can be used to choose a time period of interest. three filters are exposed to the user. it is possible (a) to limit the maximum number of considered database entries through providing the value in the \"maximal number of entries\" input field, filter the nodes (b) by their weight through the adjustment of the \"weight per entry\" slider and filter the edges (c) by their weight through the \"edge weight\" slider. the node weights are normalized in the range from zero to one and a color scheme ranging from blue to red is utilized to indicate particular weight value from small to large, respectively. overall, there are three modes of the socgraph operation, namely:"
"the viscoelastic material is a high molecular polymer that dissipates vibration energy during deformation and has both viscous and elastic properties. because the rubber column uses a highly damped viscoelastic material, it has a vibration damping function, which can cushion and reduce vibration, reduce sports impact and improve the comfort of the shoes. in order to determine the performance parameters of the rubber column, the mechanical properties of a cylindrical viscoelastic material are tested under sinusoidal load of different frequencies and displacement amplitudes on the fatigue testing machine, as shown in figure 5 . the diameter and the thickness of the cylindrical viscoelastic material are 70 and 35 mm, respectively. and the loading conditions of the test are shown in table 1 . ideally, the force-displacement relationship of viscoelastic material under axial sinusoidal load is an ellipse, as shown in figure 6 . it can be deduced from figure 6 that the equivalent stiffness of viscoelastic materials is:"
"for a given (optimal) group size opt m, the (optimal) number of groups opt k in the ensemble can be determined by fixing the desired (optimal) signal redundancy opt r, viz.:"
"the approach proposed has shown considerable robustness in reconstructing signals when in presence of disturbances and drifts. at the same time, it allows to control and optimize the ensemble dimension (number of groups and of signals in the groups), which is fundamental for practical applications. on the other hand, not considering in any way the optimization of the models reconstruction capability is paid by a (small) loss of accuracy in reconstructing undisturbed signals. finally, the performances of the overall rfse approach have been verified on two different case studies providing similar satisfactory results and, in general, the methodology here presented has proved to be fast and robust and is currently object of further improvements."
"where, the unit of is wb, the unit of b is t and the unit of s m is m 2 . in the 1 s 1-step slow walking, the average single-leg force of a 70 kg male walking is 700 n, and the shape variable of the rubber column ring is 4.74 mm. assuming that the permanent magnet is a uniform magnetic field, the magnetic field strength b is 1.21t, and the coil with a diameter of 50 mm is two series 1,000 turns. moreover, when walking 1 step, the pressing and lifting time of the foot is equal. the voltage generated and energy harvested by the walking step is calculated as follows:"
"the way to assess such a detector is to plot the receiver operating characteristics (roc) curve, which represents the detection probability (p d ) versus the false alarm probability (p fa ) for different thresholds y. given the constrained false alarm probability a, the corresponding value of the threshold that maximises the detection probability can be found just by using the estimated roc curve."
"by applying the above similarity scores of mirnas, novel disease-related mirnas were predicted based on rwr algorithm (see 'methods' section). to evaluate the performance of the similarity scores of mirnas, leave-one-out cross validation of 5710 known experimentally confirmed mirna-disease associations, including 265 diseases with at least two mirnas, were used for this assessment. for a disease of interest, each known mirna of this disease was left out as the testing case, and the remaining mirnas of this disease were used as seed nodes. all the mirnas except the mirnas of this disease were considered as candidate mirnas. we then examined how well the testing mirna ranked relative to the candidate mirnas. if the ranking of this testing mirna exceeded a given cutoff, we regarded this mirna-disease association as successfully predicted. as a result, an area under the roc curve (auc) of 0.9296 was achieved (fig. 4), which demonstrated that our mirna functional similarity was effective in recovering known experimentally confirmed disease-related mirnas."
"here, the pwbpa method was utilized for calculating similarity between disease sets, and semfunsim was used as computing the similarity of pair-wise diseases. this is because that the semfunsim method was proven to obtain the best performance [cit] . alternatively, other state-of-art methods could also be chosen to construct mfsn."
"sequential measurements of the 14 [cit] with the acquisition setup shown in fig. 2 . these spectrum power measurements were collected in sample batches with a duration of 10 min, separated by intervals of 15 min. we have restricted our analysis to this band due to the limited bandwidth of our antennas, though the highactivity and the presence of all kinds of transmissions, both data and voice, observed within this band seemed to be a remarkable scenario to test and validate the proposed model."
"in these measurements the 3 khz channelisation can be identified, but some of the detailed hf characteristics in section 2 cannot be directly identified as these are strictly power measurements. for example, a power increase in a particular channel can be observed in these measurements, but we are not able to distinguish if it is due to a collision finally, once measurements were obtained, they were between users or it is due to a strong interference close to processed to obtain sequences of '1' and '0' values which our broad-band transceiver."
"in addition to the global performance of the prediction model, performances for normal and high activity in the hf band are also presented in fig. 12 . as it was previously stated, the 14 mhz amateur band has a different behaviour depending on the day: there is a huge amount of activity at weekends, especially when amateur contests are scheduled, whereas there is a normal amount of activity on weekdays. both situations have been considered in the training stage of the hf primary user dynamics model, so, the trained model will be able to predict whether a primary user will be present in the channel or not within the next minute in both situations."
"due to the differences in the selected structures for the high-level model and the three sumbodels, two different training protocols were followed. whereas the number of states of the high-level model was pre-fixed -3 -due to the prior classification of the observations, the number of states for each submodel still had to be chosen. on the one hand, in training the proposed ergodic high-level model, our major concern was related to the initialisation of the values due to the ergodic structure itself. this structure had to be trained to be independent on the initial point, assuming that its execution could begin at any time, i.e. any of the three defined states could be the first. thus, the transition matrix a, containing the estimated probability of going from one state to another regardless of the previous history, was initialised with different random seeds to reinforce the model stability, and trained on previously classified observation sequences including one minute-long available, unavailable and partially available segments."
"the average error rate of the prediction model with a prediction span of one minute is plotted in fig. 12 . due to the time restriction on the data acquisition process which lasted for 10 min, most of the test sequences actually have a duration of nine minutes and thus, the maximum time span of the acquired knowledge is 8 min."
"considering the previous issues, it is likely that as long as cognitive radio principles are introduced in hf stations, the radio frequency spectrum could be exploited in a more efficient way as hf stations will become aware of their surrounding environment and learn from it. in order to achieve such improvements in the band, these cognitive stations shall have the capability to change their operating parameters in order to adapt their transmissions to the available spectrum holes which are to be predicted by using the acquired knowledge."
"a comparative study of ale-based stations with hfdvl stations using the proposed hf prediction model in terms of link capacity is presented in this section to illustrate the prospects for using the proposed hf prediction model. first of all, it is important to remark on the differences that would exist between ale-based stations and cognitive stations using the hf prediction model. both stations will transmit when the channel is available and will not transmit when the channel is occupied by another user, but the main difference relies on the decision to transmit or not when a channel is partially available. the alebased station will transmit or not depending on what it has listened to at the start of the time slot: if the channel is occupied it will not transmit but if the channel is free, it will transmit even though a primary user could try to transmit later on the same slot. however, the hfdvl-based station using the hf prediction model will be able to detect the spectrum holes and transmit during these short time intervals not occupied by primary users."
"data sets of dissetsim are from open source databases, and they are listed in table 1 . do [cit] records disease names. it provides terms for calculating disease similarity. generif [cit], omim [cit], gad [cit] and ctd [cit] are manually curated databases of disease-related genes. all of diseases in these databases are mapped to terms in do based on sidd [cit] . go annotation (goa) [cit] includes functional annotation of genes. humannet is the gene functional network of human. in addition, hmdd v2.0 [cit] contains diseaserelated mirnas, diseases of which were manually mapped to terms in do by oahg [cit], psb [cit], and semfunsim methods [cit] have been implemented for calculating the ssd."
"(2) algorithm layer. five algorithms of measuring the similarity between do terms have been implemented, which include resnik's, lin's, wang's, psb, and semfunsim methods. and the method named pwbpa for calculating the ssds were also implemented."
"functional similarity between mirnas could be calculated based on their related disease sets. similarities of each pair-wise mirnas are utilized to establish a mfsn. node of the network represents mirna. weight of edge is the functional similarity score. then, disease-related mirnas were prioritized using the network ranking algorithm named random walk with restart (rwr) [cit] . the random walker starts on one or several seed nodes and then randomly transits to neighboring nodes considering the probabilities of the edges between the two nodes. and the probability to return to the seed nodes is supposed as γ. then, rwr algorithm can be defined as following:"
"the learning problem was established in terms of the optimisation of the hmms' parameters (5) through the maximisation of p(0\\x), that is, to identify the model which maximises the probability that the observation sequences used for training were actually generated by the model. the baum-welch algorithm [cit], based on the expectation-maximisation method, was used to train both submodels and the high-level model. randomly initialised matrices were used for the first iteration of the baum-welch method, as no prior knowledge on the structure of the sequences was provided, and the amount of collected data seemed to be quite enough."
"once channels are processed in time and frequency domains, the signal detection problem is set out to transform the power samples into normalised values. for this purpose, two hypotheses were defined:"
"by applying dissetsim to the inputted disease sets of mirnas, the similarity score of each mirna pair could be obtained. using mirna as node and similar mirnas as edge, the mfsn was constructed based on various similarity cutoffs. as shown in fig. 3a, the number of links dramatically decreases when the cutoff increases from low value to high value. when the cutoff is equal to or bigger than 0.7, the link numbers remain relatively stable. therefore, we use 0.7 as cutoff for the mfsn. in total, 1042 mirna-mirna functional associations between 346 mirnas were obtained as mfsn (fig. 3c) . similar to the most of the reported biological networks, the degree of this mfsn also shows a scale-free distribution [5, 9, [cit] . it means that most of the mirnas only have a few functionally similar mirnas, and a few of mirnas have a numerous functional similar mirna (fig. 3b) ."
"in the frequency domain, the collected spectrum is uniformly divided into channels of 3 khz bandwidth as in the hf band; and the mean power is computed for each channel. additionally, as the vsa computes the fft of the wide-band measurement of 500 khz span every two seconds approximately, the time resolution was defined in samples of approximately two seconds long. finally, to complete the pre-processing step and obtain a reliable representation, time integration of the samples of the previous eight seconds of each channel is performed in order to best characterise the time evolution of the transmissions and avoid samples which represent the nulls of the hf channel and the typical impulsive noise present in this band."
"as the best of our knowledge, non-coding rnas (ncrnas) attract more and more attentions because of their important regulation roles in molecular level. however, the lack of protein limits the identification of their function. here the application of our tool in constructing mfsn and predicting mirna-disease associations provides a novel way to help for exploring the function of mirnas especially for prioritizing"
a hidden markov model is defined as a doubly embedded process with an underlying stochastic process that is not observable. this hidden process (state) can only be evaluated through another set of processes that produce sequences that actually can be observed [cit] . an hmm for discrete symbol observations is defined by the the following elements:
"real measurements from the hf band have been recorded to train and test the proposed model based on hmms. the acquisition setup was located at idetic facilities in las palmas de gran canaria and it is shown in fig. 2 . we have used our own broad-band transceiver [cit] followed by a vector signal analyzer (vsa) [cit] and systemvue software [cit], both from agilent technologies, to collect the time data from the yagi antenna and convert it into frequency data (spectrum) by means of the fast fourier transform (fft) with the required characteristics. several parameters, such as span, resolution bandwidth and number of bins for the fft, must be adjusted while a suitable balance between them has to be observed to obtain the desired spectrum information. as it is shown in fig. 2, our broad-band hf transceiver was required in the measurement system to collect data from the antenna which was then sent onto the vector signal analyzer as the sensitivity of the vsa was not high enough and could not be used to reliably identify most of the hf signals in the band."
"from the initial set of collected data, once the 70% have been put aside for the training of the hf primary user dynamics model, the remaining 30% were left to evaluate the trained model and validate it. as the hf primary user dynamics model was based on three submodels and a highlevel model, the evaluation protocol required two steps: first, submodel likelihoods were evaluated with binary one minute-long sequences indicating the channel occupancy, and afterwards, the high-level model probabilities were calculated for each observation sequence of ten or nine minutes."
"in this study, the known mirnas of a disease were considered as seed nodes. the unknown mirnas of it could be scored based on rwr on the mfsn. after ranking the mirnas based on the scores, disease-related mirnas could be prioritized."
"bearing in mind the principles under cognitive radio and that the main goal of this model is to make an efficient use of the hf band, we propose its use to predict the activity of primary users in a particular hf channel. the following procedure was designed to predict whether a channel will be used by a primary user within the next minute or not, and is based on the combination of the highlevel model and the underlying submodels parametrised as hmms, as described in the previous sections. besides, due to the time variability of the hf channels and the absence of a low-delay feedback channel in hf half-duplex links, we have chosen a one minute prediction span, which is henceforth restricted for the designed prediction module."
"assuming t 2 is the set involving t 2 and all of its ancestor terms, the similarity between t 1 and t 2 is defined as following by wang's method [cit] :"
"furthermore, the use of this prediction model in our hfdvl-based stations would improve the efficient use of the band as it could be able to detect the spectrum holes and transmit when the predicted channel is classified as partially available. due to the fact that the ale-based station would not transmit due to the high collision probability with a primary user in a partially available channel, it has been shown that the hfdvl-based station could reach a higher channel capacity than an ale-based station."
"as shown in fig. 12 for the global performance, when the secondary user has listened the previous minute and no past information is available, the model has an average error rate of 10.3% but, as the acquired knowledge of the channel increases, this error rate decreases monotonically. in fact, if the secondary user could access the channel state memory of the previous 8 min, the error rate is reduced to a 5.8%. for high-activity environments, the average error rate of the prediction model increases to 16% when the secondary user has only listened the previous minute and it is reduced to 9.6% if the secondary user has a channel state memory of the previous 8 min. however, the average error rate decreases (respect to the global performance) to 8.7% in normal-activity environments with a channel state memory of the last minute and decreases to 4.7% when the secondary user has listened the previous 8 min."
"each submodel was trained with a set of one minutelong observation sequences from the corresponding class as previously defined (see section 6) to derive a suitable model for the group at hand. consequently, the first submodel was meant to characterise available channel sequences, the second submodel for unavailable channel sequences and the third one for partially available channel sequences. these submodels were implemented as leftright hmms as the particular structure of the transition matrix they exhibit is quite suited to model the time evolution of the samples in the observation sequences, definitely better than ergodic models can, and with a reduced number of parameters to be trained."
acquired wide-band measurements have been translated into binary observation sequences where each one represents a 3 khz hf channel for ten or nine minutes. each sequence has '0' and t values which represent only-noise power (dbm) fig. 5 . estimation of the probability distribution of only noise-samples and occupied-channel samples and statistical fitting.
"assuming t 1 and t 2 can be related with m and n biological processes of go based on hypergeometric test, respectively, the similarity of t 1 and t 2 is defined by the psb method as following:"
"as it can be observed in fig. 6, the analytic roc could represent the mean behaviour of the acquired measurements as some experimental roc curves are above the analytic roc whereas others are below it."
"standard hf communications make use of the ale (automatic link establishment) protocol [cit] which is frequently presented as an example of a primitive form of cognitive radio. it is based on a listen before transmit strategy to access the spectrum and makes use of sensing, probing and monitoring techniques to assess channel utilisation and channel quality. but the ale protocol has two major drawbacks: it does not manage the spectrum as a whole, in a wide-band sense, but as individual channels; and, additionally, it does not manage the link in a transparent way to the user, as it only concerns the link establishment leaving the intelligent management to higher layers, usually outside of the radio equipment. moreover, even if hf stations make use of ale protocol, there are multiple collisions between them because they only transmit on their assigned channels that are expected to be available according to the experience of the radio operator."
abbreviations ctd: comparative toxicogenomics database; do: disease ontology; gad: genetic association database; generif: gene reference into function; go: gene ontology; goa: go annotation; ic: information content; mfsn: mirna functional similarity network; mica: the most informative common ancestor; ncrna: non-coding rnas; omim: online mendelian inheritance in man; psb: process-similarity based; pwapm: pair-wise-all pairs-maximum; pwbpa: pair-wise-best pairs-average; pwr: random walk with restart; sd: pair-wise diseases; ssd: the similarity score of pair-wise diseases; ssds: the similarity score of pair-wise disease sets.
where t mica is the most informative common ancestor (mica) of t 1 and t 2 . lin defines the similarity of t 1 and t 2 as eq. 3 [cit] :
"(1) database layer. this layer stores do, diseaserelated genes, and functional associations between genes. these are exploited by algorithm layer for calculating the similarity between disease sets."
"in terms of link capacity, this fact will cause a higher capacity achievement with the hfdvl stations than the ale-based stations when channels are partially available. the link capacity increase would then be related to the average percentage of time that the channel is actually available, provided that this is classified as a partially available channel. this situation takes place 69.73% of the time in our measurements. so, assuming that our hfdvl stations are transmitting an ofdm signal with 60 data subcarriers in a 3 khz bandwidth hf channel [cit], and that the noise power density is the same as the estimated for only-noise samples in section 5, the capacity could be estimated as"
"where t 1 and t 2 contains n and m diseases, respectively. t i and t j represents ith and jth terms of t 1 and t 2, respectively."
"in this contribution hidden markov models were used to model the primary users activity dynamics in the 14 mhz band. as it was described in section 1, such a model could be used by our hfdvl stations, acting as secondary users, to predict the presence of a primary user, preventing interferences in their own transmissions. therefore, this prediction would be the main source of information to avoid collisions with licensed users of the band, and consequently, to make the best possible use of the available frequency channels."
"the criteria followed to classify the observation sequences of each channel is based on the maximum time that a secondary user will need to transmit a data frame. two thresholds were defined: one for minimum percentage of occupation by a primary user in unavailable channels, in such a way that the secondary user can transmit its maximum length data frame, and another for the maximum percentage of occupation in available channels. in order to derive both thresholds, it is necessary to get the worst case of channel occupation as secondary users in hf data communications. this worst case scenario comes from assuming a maximum length for data frames to be transmitted by the secondary user. in this contribution, the frame of maximum length defined by the nato standardisation agreement stanag 5066 [cit] has been considered for our stations as secondary users. this standard establishes the profile for professional hf data communications and defines frames, called d-pdu, that have 46 bytes of overhead and up to 1023 bytes of user data. most hf data communications use a data rate of 600 bps or 1200 bps, and, with these parameters, the longest d-pdu frame resulting would last for 14.25 s (1069 bytes at 600 bps), which represents the worst case of channel occupation by the secondary user to be considered in our work. therefore, observation sequences with at least 45 s occupied within a minute were considered to belong to the unavailable channels group as secondary users will not be able to transmit the largest frame during the remaining 15 s. similarly, available channels were defined as those observation sequences with a maximum of 15 s occupation in one minute. finally, the rest of the observation sequences were classified as partially available channels where secondary users could transmit for less than 45 s to avoid collisions with primary users."
"where l is the number of measurements used for the computation, n c is the number of ofdm subcarriers, af is the subcarrier spacing, h,-(k) is the channel frequency response at subcarrier k and measurement i and a^ is the noise power density, resulting in an estimated link capacity of 22,378 bits per second. the capacity achievements of both stations will highly depend on the number of channels classified as partially available. if most of the sounded channels by both stations are available and only a few partially available, there will not be a great difference between both stations. nevertheless, if most of the sounded channels by both stations are classified as partially available, the hfdvl-based station with the hf prediction model will use the acquired knowledge to transmit in the spectrum holes without colliding with a primary user and, consequently, achieve a higher link capacity than the ale-based station."
"\"pair-wise-all pairs-maximum\" (pwapm) method and \"pair-wise-best pairs-average\" (pwbpa) method are optional for calculating similarity of pair-wise term sets [cit] . for comparing multiple aspects, the best measure is the pwbpa method, which is widely utilized in calculating similarity of do and go term sets [cit] ."
"due to the trans-horizon behaviour of hf communications and the limited bandwidth where all hf users around the world can transmit, the difference between primary and secondary users relies on the smart and cognitive capabilities of the hf user, so, in this work we will consider the primary ones to be the legacy users which transmit without resorting any smart procedure and consequently will interfere in our communications both on transmitter and receiver sides. on the other hand, our stations using the hfdvl (hf data+voice link) architecture [cit], which are also licensed for certain hf bands, hereafter will be considered as secondary users of the accessible frequency channels. consequently, our stations must be able to predict when the primary users start and finish their transmissions, as they can interfere with transmitter and/or receiver sides, in order to send our own data packages by filling silence slots within primary transmissions. to do this, we shall use our broad-band hf transceiver [cit] to select those channels predicted as available. this architecture would be able to enhance the efficient use of the hf band and the performance of our hf communication system, provided that a suitable model for hf can be found."
"as it can be observed in figs. 3 and 4, different degrees of activity can be identified in the acquired channels. there are several channels which are almost unoccupied for long-time slots, while other channels remain occupied for the same amount of time and could not be used to transmit. finally, there are other channels which could be used by a secondary user to transmit and make an efficient use of the band. so, we propose in this work a classification of the observation sequences prior to the stochastic modelling of the primary user's dynamics. this classification is based on the degrees of activity observed in the acquired measurements and the secondary user's behaviour in a cognitive radio system, that is:"
"long distance communications are feasible in the hf band thanks to the use of the ionosphere as a passive reflector. as the link establishment depends on a natural media such as the ionosphere, its propagation characteristics change depending on the time of day, atmospheric conditions, solar radiation and meteorological phenomena. but besides its propagation characteristics, the main limitation of any hf link is related to its use since there are multiple collisions of uncoordinated users around the world. therefore, it is important to remark on other characteristics which make this scenario different from the standard cellular one where cognitive radio concepts are usually being developed:"
these hf characteristics must be considered in the hf cognitive station as a whole where the prediction model proposed in this contribution will be included to predict the activity of hf primary users.
"where ic go and ic do represent ic based on go and do, respectively. n(p 1 ∩ p 2 ) and n(p 1 ∪ p 2 ) denote the number of common genes of p 1 and p 2, and the number of total genes of p 1 and p 2, respectively. assuming g 1 and g 2 represent related gene sets of t 1 and t 2, respectively. then, the similarity of t 1 and t 2 by the semfunsim method can be described as following:"
"sections 7 and 8 are devoted to describe how the three tasks were developed in order to train and test the model, and also, to use them for prediction using a trained model."
"although dosim [cit] and dissim [cit] implemented the disease similarity methods in r package and web interface, no tools provided the function to calculate the similarity score of pair-wise disease sets (ssds) currently. in this article, we designed and implemented an online tool dissetsim to calculate the ssds. five state-of-art disease similarity methods (resnik's, lin's, wang's, psb, and semfunsim) and the pwbpa method was implemented in the tool. the system is freely available at http://www.bio-annotation.cn:8080/dissetsim/."
"once both distributions are fitted, the false alarm probability (p fa ) and the detection probability (p d ) are computed to represent the analytic roc plotted in fig. 6 where"
"in cognitive radio systems two types of users are distinguished: primary and secondary users. primary users are those users that are holders of a license for a particular frequency band and therefore shall have prime access to it. whereas, unlicensed users transmitting in that band are taken as secondary users as they are allowed to make use of it, but required not to interfere with communications coming from primary stations."
"where p(x;hi) is the probability distribution function for occupied-channel samples, p (x; h 0 ) the probability distribution for only-noise samples and the threshold y of the detector is derived from the false alarm constraint"
"where p 0 denotes the initial probability vector, p t is a vector in which the ith element represents the probability of finding the walker at node i and step t, a is the column-normalized adjacency matrix of the network. the algorithm was performed until the difference between p t and p t+1 falling below 10 −10, which means all the nodes become stable."
"furthermore, depending on the day, the 14 mhz amateur band has different degrees of activity. there is normal activity on weekdays as can be observed in fig. 3, while a huge amount of activity can be found at weekends, especially in those when amateur contests are scheduled, as can be observed in the collected data shown in fig. 4 . the 14 mhz amateur band is in frequencies from 14,000 to 14,350 khz and it is divided into three sub-bands:"
"samples or occupied-channel samples, respectively. subsequently, these observation sequences are segmented into smaller sequences with a duration of one minute and classified in order to reduce the variability of the trained submodels (see section 7)."
"where n is the total number of genes annotated by diseases, and n t is the number of genes annotated by t. assuming t 1 and t 2 are two diseases, the similarity of them is defined by resnik as following [cit] :"
"the second part describes the processes involved in the implementation of the hf primary user dynamics model detailed in the block diagram in fig. 1 . first, the acquisition of the measurements of the hf band is described in section 4. later, the measurement processing is depicted in section 5, while observed sequences are then classified and segmented according to the criteria enunciated in section 6."
the information captured by the vsa was processed to obtain the spectrum power of the whole acquisition band. both frequency and time domain processing were applied to the collected data to obtain a time-frequency representation of the activity in each channel. samples representing the mean power in a 3 khz channel within a time slot of two seconds were obtained in this preprocessing step.
"resources for calculating the similarity score of pairwise diseases (ssd) mainly includes the vocabularies of diseases and disease-related genes. the frequently used disease vocabularies contain online mendelian inheritance in man (omim) [cit], medical subject headings (mesh) [cit], and do [cit] . omim records the names of genetic disorders without providing semantic associations between them. mesh provides a hierarchy of terms in biomedical domain. it contains 16 categories, of which only c and f03 involve disease names. in comparison with omim and mesh, do has been established around the concept of disease, and it aims to provide a clear definition for each disease. the disease-related genes are scattered in the databases, such as gene reference into function (generif) [cit], omim [cit], genetic association database (gad) [cit] and comparative toxicogenomics database (ctd) [cit] . it is better to use relationships of all of these databases."
"on the other hand, the non-ergodic, left-right structure of the submodels largely reduced the complexity and cost for their training; unlike the high-level model for which no specific structure could be established. the number of states for the submodels, designed as leftright hmms including three-states-long transitions (i.e. on each intermediate state, the incoming and outgoing transitions is equal and fixed to 4), was chosen according to the procedure previously described, based on the likelihood scores computed on every one minute-long observation sequences. for these submodels, matrices a and b were randomly initialised with a common seed, while ti was initialised as in (7) due to the left-right structure which forces the models to begin at the first state. different configurations for these models including 20-45 states were trained with their own one minute-long observed sequences including binary samples containing information on channels' occupancy: the available channel submodel was trained with all sequences classified as available channels, and the same was done for unavailable and partially available channel submodels."
assuming t 1 is the set involving t 1 and all of its ancestor terms of ontology. semantic contribution of term t to t 1 is represented as following:
"where p 1i and p 2j is the ith and jth significant related biological process terms of t 1 and t 2, respectively. sim(p 1i, p 2j ) represents similarity between two processes p 1i and p 2j, which is defined as eq. 8:"
"the proposed hf primary user dynamics model is described in section 7, where the training and validation processes are presented; whereas the prediction scheme developed and tested on top of this model is described in section 8. finally, conclusions on this work are drawn in section 9."
the rest of the paper is organised as follows: the first part includes a description of the challenges of applying cognitive radio principles in hf communications in section 2 and a brief introduction to hidden markov models in section 3.
"in this contribution we have proposed a primary user dynamics model for the hf band based on hidden markov models. due to the regulatory bandwidth restrictions and propagation characteristics of this frequency band, the use of this model to make predictions within the next minute of the activity in a particular channel is considered to be extremely helpful for cognitive stations acting as secondary users like our hfdvl based stations or any other standard hf modem using a 3 khz hf channel."
"for calculating disease similarity. and the pwbpa method was implemented for calculating the ssds. two tools involving pairsim and batchsim provide the function to obtain the ssds by inputting a pair-wise disease sets and multiple disease sets, respectively. the functional similarity of mirnas could be calculated based on our system. here, the similarity of each pair-wise mirnas was calculated. and then a mfsn was constructed based on mirna similarity. the network was further utilized to predicate disease-related mirnas based on rwr. the high auc (0.9296) shows the mfsn is very suitable for predicting potential relationships between diseases and mirnas."
"in order to acquire spectral information from the whole band we have selected 14,175 khz as the central frequency and a span of 500 khz, though the collected span by vsa was actually wider than 500 khz. with these parameters we obtained measurements of the amateur band and also channels outside of it that correspond to other radio stations as it can be seen in the examples in figs. 3 and 4 . given that all kinds of transmissions and activities take place in the acquired bandwidth, the 14 mhz band measurements are fairly representative of the main activities and transmissions that can be found in the whole hf band."
"hmms are a powerful and robust tool for modelling stochastic random processes as they are able to model a large variety of processes achieving high accuracy with relatively low model complexity. they have been extensively used in a myriad of signal processing applications during the last 20 years, mainly for fitting experimental data onto a parametric model which can be used for real-time pattern recognition, and to make short-term predictions based on the available prior knowledge [cit] ."
"thus, a complete definition of an hmm involves: two model parameters (n and m), the specification of the observations symbols and the specification of a, b and iz. in practise the compact notation used for an hmm will be"
"once trained, submodels' accumulated likelihood scores were obtained according to the evaluation task described in section 3, and the model with the maximum likelihood (highest p(0\\x) among all trained models) was identified. however, a normalisation model built from the combination of all samples included was artificially introduced to normalise the likelihood scores and prevent spurious effects during their estimation. this slight modification causes the former likelihood values to be bounded and behave as averaged likelihood ratios resulting from comparing different models to a common one, and therefore allows positive and negative values. however, to compare the scores from two different models, the subtraction of these scores would still provide a relative cumulative log-likelihood ratio measure, independent from the intermediate artificial model introduced. their assessment was carried out attending to the results depicted in fig. 9 . focusing on the curves included in this figure and corresponding to unavailable and partially available channel submodels, it should be noticed that the log-likelihood scores reach a local maximum for the configuration in which forty states were included, while log-likelihood scores corresponding to the available channel submodel were very small (see curve scale) for any number of states. consequently, according to this result a suitable left-right hmm to model sequences included in the latter group could be built on any number of states from 20 to 45 without major differences. however, from a practical point of view we realised that if all submodels included the same number of states their comparison would be easier once the whole model was trained and used for prediction. therefore, all submodels discussed hereafter are left-right hidden markov models including forty states and up to three-states-long transitions as the one in fig. 10 . being the number of states for the submodels adequately chosen, each of the models was trained over a sample including 70% of the observed sequences corresponding to it, and the model parameters maximising p(0\\x) were then identified. the baum-welch algorithm was applied to randomly initialised hmm matrices a and b with the same seed used in the previous step. matrices were obtained after ten iterations of the algorithm to prevent over-fitting. the rest of the acquired sequences were evaluated on the proposed models to re-estimate likelihood scores and then compared to the values obtained on the training sequences to check that the models did not over-fit the training data. we specifically checked that these matrices represent stable models, that is, there is a transition with a high probability in each row of the transition matrices and if the transition probabilities along the matrix are analysed, the final state is always reached for any observation sequence."
"the proposed model has been trained and validated with real measurements collected from the 14 mhz amateur band, and is built from three interconnected submodels which describe three types of channels: available channels, unavailable channels and partially available channels. finally, we have used this model to predict the activity in a channel within the next minute and achieved an average 10.3% error rate when the acquired knowledge of the channel has a duration of one minute and reached an average 5.8% error rate when this knowledge is extended to the previous 8 min."
"correspond to occupied or available channel samples, respectively, as detailed in section 5. these sequences will be used as training and test sequences for the proposed model."
a model of the primary user's dynamics in the band shall be extremely useful to make the best from the acquired knowledge in predicting the activity of the primary user in the channel. a predictive model is derived in this work for the hf band based on a set of hidden markov models. it has been trained and validated on real data from the hf band as a first step towards a fully operative cognitive radio scheme for the band occupancy.
"the developed architecture shall operate according to the flow shown in fig. 11 . while a secondary user is sensing different channels in order to select the one that can be used to transmit without interfering with a primary user, it shall process the spectrum measurement corresponding to the last minute and use it as an observation sequence 0 t to evaluate the three submodels for available, unavailable and partially available channels. by evaluating the \"forward-backward\" algorithm, their likelihoods are computed and, instead of choosing the submodel with highest likelihood like in the evaluation process, the resulting probabilities can be used in the highlevel model as updated entries for its observation matrix b."
"merely as an example to illustrate this capacity increase, if we assume that channels are independent and there are 2 available channels, 1 occupied channels and 6 partially-available channels, the hfdvl station using the hf prediction model will achieve a global link capacity of 4 x c + 2 x (0.6973c) bits per second whereas the alebased station will achieve a lower global link capacity that will depend on the probability of collision if the ale-based station decides to transmit in a partially available channel."
"dissetsim has been implemented on a javaee framework and run on the web server (2-core (2.26 ghz) processors) of ucloud [cit] . the four-layer architecture involving database, algorithm, tools, and view layer is shown in fig. 1 the detailed description of the architecture is fixed as following."
"the wavefronts are fitted to zernike polynomials, as they are a very good description of wavefront aberrations. the zernike polynomials allow us to characterize the aberrations by decomposing them to their respective coefficients."
"in summary, of the studies published regarding mycotoxin analysis in herbal medicine, the majority of the samples were randomly collected from two sale terminals [cit], 10, 65 8 of 39 drugstore), while some studies reported the collection of samples from herbal medicine users [cit] . in the majority of these reports, a very small quantity of the lot was used in the end for contamination quantification. however, only few studies provided a detailed description of the sampling procedure used [cit] . the sampling step typically represents the largest source of error due to the extreme distribution of mycotoxins among kernels within the lot [cit] . therefore, a reasonable sampling plan will help to minimize the risk of misclassifying the product, which could further facilitate trade as well as provide consumer protection. thus, it is suggested that researchers pay more attention to the sampling procedure in the future studies."
"often, if the efficiency of the one step extraction is satisfactory, it is not necessary to repeat the extraction. otherwise, two types of solvents can be used successively to carry out a two-step extraction method in order to obtain increased extraction efficiency [cit] . in addition, extra management is required prior to extraction in the case of the detection of certain mycotoxins in herbal medicines. for example, pat is prone to combine with a protein that originates from herbs to generate the complexity of pat. therefore, in the case of pat determination in fructus crataegi and fructus mume, samples must be pretreated with pectinase in order to dissociate pat from protein and obtain the dissociated pat molecule [cit] ."
"the underwater detection technique has been widely used in oceanic explorations, fisheries, and military applications [cit] . in these detection techniques, turbulence is a common problem in various types of fluids. underwater turbulence has a significant effect on the momentum, heat, and diffusion of matter in water [cit] . in general, the effect of light beam transmission in water by 1 m is equivalent to the transmission of 800-1000 m in the air [cit] . water temperature, density, and water disturbance will influence the water refractive index during underwater light transmission. this causes a minimal angular scattering of light, wavefront distortion, and a reduction of system resolution [cit] ."
"it should be noted that extraction is preceded by a maceration step, which is helpful to obtain the highest possible extraction efficiency. the addition of water to wet the sample enables the release of analytes bound to the matrix [cit] . in addition, the maceration procedure allows water to dissolve and remove water-soluble substances that may form a barrier to prevent extraction solvent from reaching the herb. this step therefore increases the availability of analytes for extraction by the followed extraction step [cit] ."
"as shown in figure 2, there are two delay generators in the setup. the first delay generator is used to trigger laser pulses at a configured frequency. in order to have a better estimation on the arrival time of the laser pulses, a second delay generator is introduced. this delay generator is triggered by the photodetector when it detects the emitted laser pulse, and accordingly controls the range-gated shutter time. however, the delay generator has an unavoidable external trigger delay during the triggering process. this causes the first laser pulse to be missed when the second delay generator is triggered to open the camera gate. system calibration is necessary for an accurate determination of gate-opening time. this is one of the challenges in setting up an accurate system for obtaining a good quality turbulence reconstruction surface. the calibration of the system is performed in multiple stages. the calibration is split into two parts: the external trigger delay of the delay generator, and the effective delay of the camera. figure 3 illustrates our design of synchronization control, and the timing diagram of the important signals in the experimental setup. triggered by the photodetector when it detects the emitted laser pulse, and accordingly controls the range-gated shutter time. however, the delay generator has an unavoidable external trigger delay during the triggering process. this causes the first laser pulse to be missed when the second delay generator is triggered to open the camera gate. system calibration is necessary for an accurate determination of gate-opening time. this is one of the challenges in setting up an accurate system for obtaining a good quality turbulence reconstruction surface. the calibration of the system is performed in multiple stages. the calibration is split into two parts: the external trigger delay of the delay generator, and the effective delay of the camera. figure 3 illustrates our design of synchronization control, and the timing diagram of the important signals in the experimental setup. a range-gating process starts when the laser emits a pulse. the camera gate is kept closed at all times, and is only opened for a short time when the laser pulse returns to the camera after hitting the target. thus, only light that arrives at the sensor within the right timing window can contribute to the imaging process. after synchronization, the gated wavefront sensor samples the incident wavefront from different interfaces by means of a lenslet array. the wavefront is spatially sampled and focused by the lenslet array on the camera. triggered by the photodetector when it detects the emitted laser pulse, and accordingly controls the range-gated shutter time. however, the delay generator has an unavoidable external trigger delay during the triggering process. this causes the first laser pulse to be missed when the second delay generator is triggered to open the camera gate. system calibration is necessary for an accurate determination of gate-opening time. this is one of the challenges in setting up an accurate system for obtaining a good quality turbulence reconstruction surface. the calibration of the system is performed in multiple stages. the calibration is split into two parts: the external trigger delay of the delay generator, and the effective delay of the camera. figure 3 illustrates our design of synchronization control, and the timing diagram of the important signals in the experimental setup. a range-gating process starts when the laser emits a pulse. the camera gate is kept closed at all times, and is only opened for a short time when the laser pulse returns to the camera after hitting the target. thus, only light that arrives at the sensor within the right timing window can contribute to the imaging process. after synchronization, the gated wavefront sensor samples the incident wavefront from different interfaces by means of a lenslet array. the wavefront is spatially sampled and focused by the lenslet array on the camera. a range-gating process starts when the laser emits a pulse. the camera gate is kept closed at all times, and is only opened for a short time when the laser pulse returns to the camera after hitting the target. thus, only light that arrives at the sensor within the right timing window can contribute to the imaging process. after synchronization, the gated wavefront sensor samples the incident wavefront from different interfaces by means of a lenslet array. the wavefront is spatially sampled and focused by the lenslet array on the camera."
"in our synchronization control system, water flows into the tank at a distance of four meters from the wavefront sensing system. the flow rate can be controlled to create a turbulence condition. figure 6 shows the gated wavefront sensing system and the water tank control system. actually, water disturbance, density, temperature, and other non-uniform factors will cause a random change in the refractive index of macromolecules in water. underwater turbulence causes small changes of the refractive index (δn ≈ 10 −6 ), which caused a very small angular scatter of the beam. in general, the bidirectional reflectance distribution function (brdf) is an accurate description of the surface of the light reflection of the basic parameters. however, the underwater environment cannot be expressed by a simple negative exponential decay function. so, it is more difficult to measure its intensity distribution by conventional methods. the reflection ratio of light intensity in an underwater situation is about 20-30%. therefore, we use a 532-nm laser and a time-gated camera to reduce the impact of underwater reflectance, and get the focal imaging."
"despite the conventional types of absorbent available, currently, some advanced nanomaterials have been used for mycotoxin determination, including carbon nanomaterial and magnetic carbon nanomaterial. the primary advantage of carbon nanomaterials is their high adsorption capacities due to their unique electronic, mechanical, and chemical properties [cit] . graphene oxide (go) is the oxidized derivative of graphene, which is a type of representative carbon nanomaterial. go is rich in oxygen atoms on the surface, including epoxy, hydroxyl, and carboxyl groups. these groups play a critical role in the formation of hydrogen bonds or electrostatic interactions with organic compounds containing oxygen-or nitrogen-functional groups. in addition, go is able to adsorb aromatic rings from certain organic compounds through strong π-π interactions [cit] . recently, go was used for the first time in the pre-concentration step in the extraction of afs from traditional proprietary chinese medicines [cit] . the stacking between the benzene rings of afs and go, in addition to the hydrogen bonds formed between the oxygen containing groups contained in afs and go could be responsible for the adsorption of afs on go absorbent. however, in this report, a single pretreatment by go was unable to meet the requirement of sensitive detection. in an effort to remove as much interference as possible, a cleanup procedure involving an mgso 4 /nacl salt mixture was carried out prior to the go pre-concentration step."
"chromatographic techniques play a critical role in the analysis of mycotoxins in herbal medicine matrices. afs, which are regulated by various pharmacopeias, are commonly identified with hplc-fld using different derivatization methods. however, in recent years, the co-occurrence of multiple mycotoxins in herbal medicines has drawn great attention within the field. developments based on a modified quechers procedure or multiple functional column purification combined with lc-ms/ms has functioned to increase the scope of mycotoxins that can be simultaneously analyzed."
"because of the diversity of herbal medicine matrices, satisfactory results are not always obtained when using commercial spe columns. therefore, homemade cartridges have been proposed for mycotoxin cleanup steps in some cases. there are two critical points to bear in mind in regard to making homemade cartridges, including lower adsorbents of analytes and higher adsorbents of herbal medicine matrices, such as pigments. currently, silica gel, alumina, and kieselguhr are three adsorbent materials that are commonly used for mycotoxin cleanup in herbal medicine matrices. wu's group has published a series of studies regarding the use of homemade cleanup cartridges for the determination of mycotoxins in tcms. for example, the silica gel was used for purification of 35 mycotoxins [cit], while the mixture of silica gel and alumina was used to purify afb 1, afb 2, afg 1, afg 2, afm 1 and afm 2 [cit] . in addition, cartridges filled with equal proportions of alumina base, florisil, and kieselguhr have been used for cleanup of zen, α-zearalenol (α-zol), β-zearalenol (β-zol), zearalanone (zan), α-zearalanol (α-zal), and β-zearalanol (β-zal) [cit] ."
"in our synchronization control system, water flows into the tank at a distance of four meters from the wavefront sensing system. the flow rate can be controlled to create a turbulence condition. figure 6 shows the gated wavefront sensing system and the water tank control system. the actual delay time is calculated based on the principle of synchronization control, which was mentioned above. figure 7 shows the three signals on an oscilloscope, where the time difference between each other i.e., the external trigger delay of the delay generator, and the effective delay of the camera, can be determined. the output from the photodetector is the emitted laser pulses (in purple); the output from the delay generator (dg) 2 is triggered by the photodetector (in green); and the camera gate output (in blue) is used as a reference to indicate the round trip time from laser emission to gated wavefront sensing. in our experiments, the laser pulse frequency was set to 30 hz. to configure an accurate gate-opening time, we measured 50 times to calculate the standard deviation of the actual dg, and camera delay. from our calibration, a dg external trigger time delay is 116.2 ns, and the effective time gate delay is 11.4 ns. the actual delay time is calculated based on the principle of synchronization control, which was mentioned above. figure 7 shows the three signals on an oscilloscope, where the time difference between each other i.e., the external trigger delay of the delay generator, and the effective delay of the camera, can be determined. the output from the photodetector is the emitted laser pulses (in purple); the output from the delay generator (dg) 2 is triggered by the photodetector (in green); and the camera gate output (in blue) is used as a reference to indicate the round trip time from laser emission to gated wavefront sensing. in our experiments, the laser pulse frequency was set to 30 hz. to configure an accurate gate-opening time, we measured 50 times to calculate the standard deviation of the actual dg, and camera delay. from our calibration, a dg external trigger time delay is 116.2 ns, and the effective time gate delay is 11.4 ns."
"the wavefronts are fitted to zernike polynomials, as they are a very good description of wavefront aberrations. the zernike polynomials allow us to characterize the aberrations by decomposing them to their respective coefficients."
"in addition to of the commonly used solvents (e.g., methanol and acetonitrile), other solvent types such as ethanol [cit], acetone [cit], ethyl acetate [cit] and chloroform [cit] are also used sometimes for mycotoxin extraction from herbal medicines. it should be noted that if ethyl acetate is used as the extraction solution, an extra defatting procedure may be required prior to cleanup or detection due to the high levels of fatty matrix compounds that are known to be co-extracted with the ethyl acetate-containing solvent [cit] ."
"during the extraction step, acetonitrile is primarily used as the solvent. however, it has also been proposed to soak the dry samples in water or pbs/nah 2 po 4 buffer prior to the addition of extraction solvent, a step that is advantageous for later extraction [cit] . in addition, because certain mycotoxins are ph-sensitive, such as fbs, ota, and ochratoxin b (otb), proper formic acid is typically added to the acetonitrile in order to generate a low ph to prevent generation of the ionized form, which could contribute to satisfactory recoveries. for example, in the study that developed a method for the simultaneous determination of 22 mycotoxins in pheretima, different ratios of formic acid in acetonitrile were investigated. this study demonstrated that recoveries of fbs, ota, and otb were less than 10% when using 1% formic acid. while when the percentage of formic acid was increased to 10%, satisfactory recoveries of ota and otb were obtained. however, fbs recoveries were only at 40-70%. it was determined that when 15% formic acid was used, fbs recoveries were greater than 80%. this could be due to the fact that fbs contain more carboxylic acid groups relative to ota and otb, which requires a lower ph in order to maintain their molecular form (a more extractable form) [cit] ."
herbal medicines have been used for disease prevention and treatment worldwide. mycotoxin contamination represents one of the most critical toxicities present in herbal medicine and has been a concern globally. the effective control of mycotoxin contamination requires accurate and sensitive analytical methods.
"where x c and y c are the centroid position of the spot; i α ij is the αth high intensity value in this spot sub-area; x i and y j are the coordinates of the pixel in the whole spot image; and l and m are the numbers of the pixels along x and y directions in the window, respectively."
"aptamers are single-stranded (ss) oligonucleotides that are capable of recognizing target molecules with high affinity and specificity, similar to the properties of antibodies [cit] . compared with antibodies, aptamers offer significant advantages, including a lower cost and less labor intensive. in addition, aptamers that are immobilized on solid phases can be recycled, as they are easily regenerated within several minutes at room temperature [cit] . therefore, aptamers represent a promising tool for use in mycotoxin cleanup steps from complex matrices."
"another issue in mycotoxin determination is the development of a rapid detection method. conventional analytical methods always involve a more complex sample pretreatment protocol and a long detection time. these parameters limit their applications in high-throughput screening methods for mycotoxins. thus, an increased interest has developed in the generation of a rapid detection approach. however, while there have been several types of rapid methods that have been successfully used, these have only been used for single herbal medicine matrices. therefore, the challenge that exists for an analytical chemist is to broaden the application scope of the current methods, and work to develop a new rapid system that can use a variety of formats and platforms that are suitable for mycotoxin detection in herbal medicines."
"recently, ultra-high performance liquid chromatography (uplc) was used to analyze mycotoxins in herbal medicines [cit] . compared to traditional hplc, uplc was demonstrated to significantly improve chromatographic resolution and sensitivity and reduce the analysis cycle. these properties are suitable for high-throughput detection of trace complex mixtures. the technological progress of uplc deserves mention: [cit], waters launched and trademarked the waters ® acquity ® uplc ® fluorescence (flr) detector with a large volume flow cell. it can be used to detect afs without derivatization. [cit] combined uplc with flr in order to develop a method for the simultaneous analysis of afs and ota in ginger and other related products. compared to their prior work using hplc-fld and post-column photochemical derivatization [cit], a comparable afb 1 and afg 1 sensitivity and an obviously increased afb 2 and afg 2 sensitivity were achieved in only half of the measurement time."
"finally, mycotoxin analysis in herbal medicine would greatly benefit from the development of novel and effective sample extraction technologies. in addition, advances must be made in screening assays to enable on-site detection for mycotoxin analysis in herbal medicine."
"the slope matrix, or curvature matrix, is obtained for further reconstruction using an iterative method in orderto generate the surface. in this study, we use the zernike polynomials. a distorted wavefront w(x, y) is represented by a zernike polynomial as:"
"conventional spe spe columns with various commercially available packing have been utilized for mycotoxin cleanup [cit] . for example, spe cartridges bonding c-18 sorbent were used to purify afs from herbal samples. these are rich in fatty oils in order to protect the columns from damage in the subsequent test procedure [cit], or to perform simultaneous cleanup for afb 1 and ota in licorice roots and fritillary bulbs [cit] . in addition, a strong anionic exchange (sax) column was used for the purification of fbs from african traditional herbal medicines [cit], as well as certain herbs and spices [cit] ."
"quick, easy, cheap, effective, rugged, and safe (quechers) [cit] for pesticide determination [cit] . this technology includes an extraction/partitioning step using acetonitrile and salts followed by a cleanup step that is based on a dispersive solid-phase extraction (dspe) [cit] . due to its simplicity to operate, quechers-based approaches have been used increasingly within the field for the extraction and purification of multi-mycotoxins from herbal medicine matrices."
"currently, tlc methods are still recommended for the detection of af in any plant material in the usp [cit] . due to a low detection cost and less demand on equipment, tlc methods are sometimes applied for the screening of mycotoxins in raw herbal drug materials when fungal analysis and related mycotoxin contamination are explored. some examples are presented in table 5 . eighteen samples of 6 different types afb 1 [cit] 3."
"in the case of pre-column derivatization, the sample should be derived prior to the detection procedure. trifluoroacetic acid (tfa) is the commonly used derivatization regent [66, 75, 76, [cit] . in general, the pre-column derivatization process involves a complex and time-consuming concentration procedure that cannot be performed by an on-line operation."
"while some analytical methods have been proposed, most are only aimed towards a single matrix. thus, a more universal approach should be developed in order to extend the application scope, such as classifying the analytical method based on the basis of matrix variations."
"spe has been demonstrated to be a safe, efficient, and reproducible technique. however, because herbal medicines are rich in secondary metabolites, including pigments, flavone, essential oils, polysaccharide, and fatty acids that could interfere with mycotoxin analysis, even if the samples are purified by spe extraction, in most cases it still requires highly sensitive and selective detectors, such as mass detector to meet the requirements (table 4) . therefore, cleanup methods with higher specificity are necessary."
"additional reagents were sometimes required to assist in the extraction. for example, acid (e.g., formic acid, acetic acid) and salt were required for the analysis of mycotoxins. some studies have implied that the addition of proper ratio of formic acid to the extraction solvent could improve recoveries of afs and fbs [cit] . higher recoveries and lower mes were obtained with the addition of 1% acetic acid to the extraction solvent when 11 mycotoxins were simultaneously determined in malt [cit] . the addition of proper nahco 3 into the extraction solution could function to improve the recoveries of afs and ota in ginseng and ginger matrices [cit] ."
"gc was used as an alternate method for mycotoxin determination, especially when ms detectors are widely used. this allows for simultaneous qualitative and quantitative measurements of single or multiple analytes. an additional derivatization step, such as silylation or acylation, is commonly required following sample cleanup treatment. this is due to the fact that some mycotoxins are not sufficiently volatile at the column temperature, or cannot be converted into volatile derivatives [cit] ."
"we acquired the focal image with the flow rate of 0 l/min as a reference for wavefront reconstruction. four distances were selected for turbulence comparison, namely: 1 m, 2 m, 3 m, and 4 m. the water flow rate was set at 40 l/min. figure 8 shows the focal spot images from different distances in the water tank with turbulence. figure 7 . determination of the actual delay generator (dg) trigger and camera delay time. the output from the photodetector shows in purple color; the output from the delay generator (dg) 2 shows in green color; and the camera gate output shows in blue color."
"the selection of the extraction solvent depends on several things, including physical and chemical characteristics of the analyte, solvent cost and safety, the solubility of the non-analyte in the extraction solvent and subsequent processing steps following extraction. ideally, the extraction solvent should remove only the mycotoxin of interest from the sample matrix. however, due to the complex matrix of herbal medicines and the absence of a completely specific extraction solvent, the extraction solvent used should be adjusted according to the characteristics of both the analyte and associated matrix."
"currently, the most common solvents used for the extraction of mycotoxins from herbal medicines are methanol-water and acetonitrile-water (tables 3 and 4 ). however, in order to enable higher extraction efficiencies and lower matrix effects (mes), the extraction solvents still need to be compared across many studies. the improved efficiency of acetonitrile-based solvents compared to methanol has been demonstrated by some groups. it was demonstrated not only for the determination of single type of mycotoxin present in tcms, such as fbs [cit], but also for zen and its related mycotoxins [cit], don, nivalenol (niv) [cit], and the simultaneous detection of multiple mycotoxins [cit] ."
"as depicted in table 3, the method based on hplc/uplc with fld or uv for mycotoxin analysis in herbal medicine matrices typically involves a sample pretreatment step with sufficient selectivity such as iac. this greatly narrows its application, as iacs are not available for all mycotoxin types."
"chromatographic techniques play a critical role in the analysis of mycotoxins in herbal medicine matrices. afs, which are regulated by various pharmacopeias, are commonly identified with hplc-fld using different derivatization methods. however, in recent years, the co-occurrence of multiple mycotoxins in herbal medicines has drawn great attention within the field. developments based on a modified quechers procedure or multiple functional column purification combined with lc-ms/ms has functioned to increase the scope of mycotoxins that can be simultaneously analyzed."
"as shown in figure 11, our method is able to detect turbulence changes at different flow rates of water using the proposed gated wavefront sensing system. at the distance of 4 m from our sensing system (where turbulence originated from the water flow in), the higher the water flow rates, the more volatile the wavefront reconstruction model will be. this proves the validity of the proposed gated wavefront sensing system for detecting underwater turbulence."
herbal medicines have been used for disease prevention and treatment worldwide. mycotoxin contamination represents one of the most critical toxicities present in herbal medicine and has been a concern globally. the effective control of mycotoxin contamination requires accurate and sensitive analytical methods.
"in addition to the type of extraction solvent used, the extraction method is another critical determinant of the extraction efficiency. the conventional solid-liquid extraction technology used for mycotoxin extraction involves the use of ultrasonic extraction, homogenization, and shaking. vortexing and blending are also used sometimes for the detection of mycotoxins in herbal medicines. when selecting the extraction method, the matrix constitution should be considered. a recent report demonstrated that samples with different matrix types required their own specific extraction method. for example, in the case of matrices with high fatty oil and polysaccharide contents that are more viscous, an ultrasonography extraction method was found to be prone to aggregating the extracts and thereby prevented the dissolution of afs from the matrices [cit] ."
"however, cba has not yet been used extensively as a tool for routine mycotoxin analysis, due to the high cost and the complexity of the instruments required."
"the traditional wavefront sensing technique samples the incident wavefront by means of a lenslet array, which produces an array of spots on a detector, such as charge coupled device (ccd) camera. basically, wavefront sensing is a measurement method that compares the reference focus with the small disturbances of the focus centroid from the measured wavefront. the displacement of each centroid location enables the computation of the wavefront slopes. the target surface can be reconstructed using the slope matrix. the advantages of the wavefront sensing technique are the simplicity of configuration, real-time processing, and high dynamic range. as water temperature, density, and moisture disturbances can affect the refractive index of water, conventional wavefront sensing techniques cannot effectively detect underwater turbulence. the range-gated technique is a well-known method to enhance the image quality and eliminate the backscattering effects during the laser propagation in water. therefore, we propose a new method by combining the wavefront sensing and the range-gating principle."
"while useful for the analysis of mycotoxins, the nanomaterials used in the studies discussed above were all self-synthetized, which could limit the scope of the application of these materials. in addition, only afs, zen, and the four type a trichothecenes (t-2, ht-2, das and neo) were investigated. the appropriate nanomaterials are awaiting evaluation for numerous other types of mycotoxins."
"regarding the sensitivity and the dynamic range of the sensor, the gated wavefront sensor is limited by the focal length and the pitch of the microlens array, as well as the pixel size of the camera used. the microlens array we used in our setup has a focal length of 6.7 mm and a pitch of 150 µm, and the iccd camera has a pixel size of 17.8 µm. therefore, the corresponding sensitivity (θ min ) and dynamic range (θ max ) can be calculated accordingly. the calculation equations are as follows:"
"compared to pre-column derivatization, post-column derivatization was reported more. three types of post-column derivatization methods have been proposed including chemical, photochemical, and electrochemical derivatization methods. for chemical derivatization, iodine and pyridinium hydrobromide perbromide are typically used as derivatization reagents, and an additional pump and heating system is typically used [cit] . therefore, the post-column electrochemical and photochemical derivatization methods have obvious advantages, due to the fact that the operating procedures are easier to carry out and that they provide a higher sensitivity and wider linearity range [cit] ."
"where ∆d x is the average slope over a subaperture diameter in the x direction; ∆d y is the average slope over a subaperture diameter in the y direction; ∆x is the measured spot centroid displacement from the reference in the x direction; and f is the focal length of the lenslet array. after the focal spots were detected, we used the threshold method to improve the spot centroid extraction accuracy. there are many factors that can affect the signal-to-noise ratio (snr) of the time-gated camera, such as photo shot noise, dark current noise, and fixed pattern noise. in our situation, photo shot noise plays a major role in all kinds of noises. in order to improve the snr, we apply the thresholding algorithm. this algorithm filters out low-frequency and high-frequency noise, and improves the accuracy of spot centroid extraction in the wavefront reconstruction. the grayscale value of the light intensity of the spot image is given a weightage in order to highlight the influence of the central light intensity. on the basis of roughly determining the position of the centroid of the light spot, a window where the light spot is located is determined according to the position of the obtained centroid. accordingly, the center of each spot can be calculated as follows:"
"the experimental setup to detect underwater turbulence using the proposed gated wavefront sensing technique is shown in figure 5 . a wedge hb 532-nm wavelength was used as the laser source, and the 9450 series gated intensified ccd (iccd) camera system, combined with a lenslet, were used as the gated wavefront sensing system. the water tank created turbulence at a distance of four meters from the laser source. the laser pulse passed through the collimation system, traveled toward the turbulence, and back to the gated wavefront sensing system, where the image was captured. as explained in section 2.2, two delay generators were used in the synchronization control to the range gate at the distance of interest. experiments were carried out in a long water tank. the turbulence level was simulated by controlling the flow rate of the water. other environmental factors, such as the temperature of the water, were kept constant, as we only considered the effects of spatial turbulence due to the flow of water."
"the reconstructed wavefront is then calculated using equation (5) and the coefficients found. the deconvolution of blurred images can then be done using the wavefront obtained. the acquired image, v, and its correlation with the original image, u, can be described as:"
"there could be clear differences in the recovery as a result of varying the percentage of organic solvent. wang and co-workers carried out a study to investigate the effect of five different methanol/water ratios (75%, 80%, 85%, 90% and 100% methanol) on the simultaneous extraction of afb 1 and ota from licorice roots and fritillary bulbs. this study demonstrated that the highest extraction efficiency was obtained using a methanol/water ratio of 85% [cit] . another group reported a method for simultaneous determination of seventeen mycotoxins in puerariae lobatae radix. in this study, acetonitrile/water (90:10, v/v) was selected as the extraction solvent after comparing the extraction efficiency of three different ratios (80%, 90% and 100%) of an acetonitrile/water solvent system [cit] ."
"in regard to the subsequent cleanup procedure of quechers, c 18, primary secondary amine (psa) and graphitized carbon black (gcb) are the typically used sorbents. it should be noted that psa is prone to absorption of acidic mycotoxins such as fb 1 and fb 2, while the gcb adsorbent is prone to adsorption of mycotoxins that possess a planar structure, such as afb 1, afb 2, afg 1, afg 2, and st [cit] ."
"in this paper, we proposed a novel method to detect underwater turbulence using a gated wavefront sensing system. the proposed method incorporated wavefront sensing and the range gated approach for effective underwater turbulence detection. based on the operating principle of this technique, the laser emission and camera gating are simultaneously controlled to only capture the reflection from a known distance. the turbulence condition can be detected from the resulting wavefront reconstruction accordingly. an experimental platform was setup to validate the proposed gated wavefront sensing system. our experimental results prove that the proposed method can detect underwater turbulence conditions at different distances, and for different levels of turbulence. the aggregated wavefront distortion is successfully detected at increasing depths of the turbulent media. due to the effectiveness of the proposed method, it has good potential, which will significantly benefit applications in underwater imaging, laser communication, oceanic exploration, etc."
"sample pretreatment has always been a challenging step for trace analysis in herbal medicine matrices. because of the sophisticated and various compositions of herbs, especially in the case when multiclass mycotoxins are simultaneously determined, sample preparation protocols must often be optimized in order to increase the extraction efficiencies. significant advances have been made in the cleanup protocol with the emergence of traditional iac and spe. novel nanomaterials have been applied as absorbents, which were demonstrated to improve specificity relative to conventional types. in addition, aac and mip columns were introduced as an alternative to iac, as they can be recycled in order to reduce costs."
"over the last decade, the use of herbal medicines has expanded across the globe and gained considerable popularity. as a result of cultural and historical influences, herbal medicines remain an important part of the healthcare system in china, india, and africa [cit] . in recent years, the utilization currently, numerous published reviews reported the occurrence of mycotoxin contamination in herbal materials and related products. these reports indicated that mycotoxin contamination in herbal medicines is considered a global issue, particularly in the case of developing countries [16, [cit] (figure 1 ). to date, more than 40 mycotoxins have been detected in herbal medicines [cit] . the typical examples of these mycotoxins are shown in table 1 . in addition to the toxicity effects of mycotoxins themselves, the presence of mycotoxins in herbal medicines may also function to decrease the medicinal potency, lead to drug interactions, and potentiate adverse effects that could influence the safety of these herbal remedies [cit] . due to the hazardous effects associated with mycotoxins, approximately 100 [cit] . national regulations have been established for numerous mycotoxins, including the naturally occurring afs and aflatoxin m1 (afm1), the trichothecenes deoxynivalenol (don), diacetoxyscirpenol (das), t-2 toxin (t-2) and ht-2 toxin (ht-2), fumonisin b1, b2 and b3 (fb1, fb2 and fb3), agaric acid, ergot alkaloids (ea), ochratoxin a (ota), patulin (pat), phomopsins, sterigmatocystin (st) and zen [cit] . however, in the case of medicinal plants, official regulations regarding the presence of only afs and ota in medicinal herbs are shared globally in pharmacopoeias, national, and organizational regulations. in general, the current legal limit for afb1 in herbal medicines ranges between 2 and 10 μg kg −1, while the limit for combined aflatoxin b1, g1, b2 and g2 (total afs) ranges from 4 to 20 μg kg −1, and the limit for ota ranges from 15 to 80 μg kg −1, as depicted in table 2 ."
"in regard to the phase separation steps, magnesium sulfate and sodium chloride are typically used in order to reduce water in the sample. this is sometimes added along with anhydrous trisodium citrate and sodium citrate dibasic sesquihydrate due to the fact that the citrate system has an amortizing role, making ph-sensitive mycotoxins, such as fbs, acquire satisfactory recoveries [cit] ."
"along with the popularization of modern ms technology, lc-ms/ms has been used increasingly for the quantitative analysis of mycotoxins in herbal medicine matrices in recent years. as depicted in table 4, an upward trend is seen to be developing for the method used for the simultaneous determination of mycotoxins with great chemical diversity. this is something that is not achievable using hplc with fluorescent or uv detection. currently, it has been demonstrated that as many as 35 different toxins can be detected within one run by lc-ms/ms in herbal medicine matrices [cit] ."
"the reconstructed wavefront is then calculated using equation (5) and the coefficients found. the deconvolution of blurred images can then be done using the wavefront obtained. the acquired image, v, and its correlation with the original image, u, can be described as:"
"the high-performance liquid chromatography with fluorescence detection (hplc-fld) method has also been proposed to use for the determination of ota in herbal medicines [cit] . in addition, the simultaneous determination of afs and ota using hplc-fld was recently developed [cit] . this is likely attributed to the successful application of alflaochra test tm immunoaffinity columns in the herbal medicine matrix pretreatment, allowing for the simultaneous purification of af and ota."
"in this study, we combine wavefront sensing and the time-of-flight (tof) range-gated principle to detect underwater turbulence. the gated wavefront sensing technique provides a simple structure, high detection efficiency, and strong anti-interference ability [cit] . the timeliness deficiency of wavefront sensing could be remedied by introducing the time-of-flight technique. our setup simulates the underwater turbulence using different flow rates of water. we can elaborate the characteristics and principle of the range-gated imaging, which operates based on the tof principle in later sections. by using the proposed gated wavefront sensing system, the turbulence effect on the underwater detection is analyzed, and the experimental results are discussed in this paper."
"subsequent experimental results show that the sensitivity and the dynamic range of the sensor can be effectively measured. regarding the sensitivity and the dynamic range of the sensor, the gated wavefront sensor is limited by the focal length and the pitch of the microlens array, as well as the pixel size of the camera used. the microlens array we used in our setup has a focal length of 6.7 mm and a pitch of 150 µ m, and the iccd camera has a pixel size of 17.8 µ m. therefore, the corresponding sensitivity  "
"although a number of emerging mycotoxins have been investigated in herbal medicine, little attention has paid to their conjugates, which can escape conventional analytical detection of parent (free) forms. in the future, it is recommended the related research be extended in herbal medicine samples. for example, estimating the potential risk of some samples that are commonly used and particularly vulnerable to mycotoxin contamination (e.g., coix seed, malt, medicated leaven), as well as performing novel types of conjugated mycotoxin screening and preparing their reference substances."
"iac, a method based on the interaction between antigen and antibody, exhibits some merits, including a minimal loss of mycotoxins and a maximal elimination of interfering substances. therefore, compared to spe extraction, the utilization of iac as a cleanup procedure could greatly improve the specificity of subsequent analysis, thereby lowering the requirements of the detector."
the centroid locations of the focal points are then compared with the reference focal points. the displacement of each centroid location reflects the wavefront slope. the wavefront information obtained at a water velocity of 0 l/min is used as the reference wavefront. the schematic diagram of the wavefront sensor is shown in figure 4 .
"where p is the noise from image acquisition and the inaccuracies in wavefront reconstruction, and h denotes the convolution operator resulting from the reconstructed wavefront. the deconvolution of the acquired image would then be a minimization function [cit] . in order to obtain the original image, u, the following equation is used:"
"the development of iac has greatly improved the specificity of the cleanup method. however, the antibodies used in iac sorbent have some down sides, including expensive cost, cross-reactivity, and poor tolerance to organic solvents."
"we design a gated wavefront sensing system to detect the turbulence effect underwater, as illustrated in figure 1 . the system components are a pulsed laser, beam-splitting (bs) prisms, a collimator, a photodetector, the wavefront sensing assembly (e.g., lenslet and gated intensified ccd camera), and delay generator for system triggering and synchronization. the range-gating process starts when the laser emits a pulse towards the target in the water tank. as the light travels, it is absorbed and scattered. the camera gate is kept closed at all times, until the laser pulse returns to the camera after hitting the target. once the reflected laser pulse is received, the camera gate is closed again to avoid any scattering effect interfering with the original image."
"in addition to afs, there have been reports of using iac to purify other types of mycotoxins from herbal medicines, including ota [cit], zen [cit], and citrinin (cit) [cit] . in addition, certain iacs that can be used to clean up multi-mycotoxins have been proposed for the purification of mycotoxins from herbal medicines, such as aflaochra test tm immunoaffinity column (vicam, milford, ma, usa) [cit], and aflazearal test tm immunoaffinity column (vicam, milford, ma, usa) [cit] ."
"in addition to thin-layer chromatography (tlc) methods, chromatographic methods, such as liquid chromatography (lc) and gas chromatography (gc) coupled to a specific detector, are the most commonly used techniques to date for obtaining highly accurate results. in the case of single mycotoxin analysis, e.g., af, ochratoxin, the traditional lc with a fld detector is the most widely used method for herbal medicine matrices. currently, the co-occurrence of multiple mycotoxins has gained increasing attention. therefore, liquid chromatography-tandem mass spectrometry (lc-ms-ms) is the technique choice for the simultaneous determination of various mycotoxins that belong to different chemical families."
"as shown in figure 2, there are two delay generators in the setup. the first delay generator is used to trigger laser pulses at a configured frequency. in order to have a better estimation on the arrival time of the laser pulses, a second delay generator is introduced. this delay generator is light passes through a series of components that are separated by a distance along the light path, r. this can be expressed as follows according to the tof principle:"
"considering the low residue level of mycotoxins (generally at µg kg −1 level) and the complex chemical composition of herbal medicine samples, a cleanup step was required prior to instrumental analysis in most cases (tables 3 and 4 ). this cleanup step may function to further concentrate mycotoxins in addition to removing sample impurities. a variety of cleanup methods have been implemented and shown to contribute to the accurate measurement of mycotoxins in herbal medicine, including solid phase extraction (spe) and immunoaffinity column (iac)."
"finally, mycotoxin analysis in herbal medicine would greatly benefit from the development of novel and effective sample extraction technologies. in addition, advances must be made in screening assays to enable on-site detection for mycotoxin analysis in herbal medicine."
"as shown in figure 11, our method is able to detect turbulence changes at different flow rates of water using the proposed gated wavefront sensing system. at the distance of 4 m from our sensing system (where turbulence originated from the water flow in), the higher the water flow rates, the more volatile the wavefront reconstruction model will be. this proves the validity of the proposed gated wavefront sensing system for detecting underwater turbulence. using the reference wavefront obtained from a flow rate of 0 l/min, the wavefronts above in figure 11 were obtained. as the turbulence generated is spontaneous and impromptu, repeated measurements would not yield similar wavefronts. therefore, these wavefronts were characterized based on the relative difference between the above spot images and the reference 0 l/min spot image. it can be seen that the peak-to-valley ratio of the reconstructed wavefronts relative to the reference wavefront increased as the flow rate increased. therefore, there was indeed distortion in the wavefronts due to the induced turbulence from the turbulent flow of water at 4 m."
"sampling plays a critical role in how precise the determination of mycotoxin levels are due to the fact that the molds that generate mycotoxins do not grow uniformly on the substrate and existing contamination in natural samples is not homogeneous. [cit] demonstrated that the actual mycotoxin concentration of a bulk lot cannot be determined with 100% certainty due to the variability associated with each step in the mycotoxin test procedure. thus, the sampling procedure could dramatically impact the final results regarding the determination of mycotoxins [cit] ."
"in regard to afs, the european union (eu) has set a limit of 5 μg kg −1 for afb1 and 10 μg kg −1 for total afs in nutmeg, ginger, turmeric, and pepper [cit] . however, the european pharmacopeia (ep) has implemented stricter limits for the presence of af in herbal drugs, with the limits set to 2 μg kg −1 for afb1 and 4 μg kg −1 for total afs [cit] . the same limit is set for the presence of af in herbal drugs, which was set by the british pharmacopeia (bp) [cit] . germany has implemented a limit of 2 μg kg −1 for afb1 and 4 μg kg −1 for total afs in any materials that are used in manufacturing of medicinal products (including medicinal herbal products) [cit] . in the usa, a limit of 5 μg kg −1 has been implemented for afb1 and 20 μg kg −1 for total afs was established by the united states pharmacopeia (usp) for certain types of raw medicinal herb materials, as well as their powder and/or dry extract [cit] . identical limits have been set by argentina for herbs, herbal materials, and herbal preparations that are used in herbal tea infusions [cit] . in addition, canada has implemented the same legislation regarding products that contain ginseng or any substance derived from this source, including evening primrose oil, sugar cane, sugar beets, and cottonseed [cit] . in china, a total of nineteen different types of traditional chinese medicines (tcms) medicinal herbs are regulated in due to the hazardous effects associated with mycotoxins, approximately 100 [cit] . national regulations have been established for numerous mycotoxins, including the naturally occurring afs and aflatoxin m 1 (afm 1 ), the trichothecenes deoxynivalenol (don), diacetoxyscirpenol (das), t-2 toxin (t-2) and ht-2 toxin (ht-2), fumonisin b 1, b 2 and b 3 (fb 1, fb 2 and fb 3 ), agaric acid, ergot alkaloids (ea), ochratoxin a (ota), patulin (pat), phomopsins, sterigmatocystin (st) and zen [cit] . however, in the case of medicinal plants, official regulations regarding the presence of only afs and ota in medicinal herbs are shared globally in pharmacopoeias, national, and organizational regulations. in general, the current legal limit for afb 1 in herbal medicines ranges between 2 and 10 µg kg −1, while the limit for combined aflatoxin b 1, g 1, b 2 and g 2 (total afs) ranges from 4 to 20 µg kg −1, and the limit for ota ranges from 15 to 80 µg kg −1, as depicted in table 2 ."
"multi-walled carbon nanotubes (mwcnts) are another type of carbon nanomaterial that is comprised of several rolled-up graphite sheets. mwcnts have been demonstrated to adsorb type a trichothecenes and therefore, were used as spe sorbents for the purification and enrichment of mycotoxins in maize, wheat, and rice [cit] . in recent studies carried out by han's group [cit], mwcnts were incorporated with magnetic material to form magnetic-spe adsorbents. these could be collected using an external magnetic field and recycled with a simple washing step, thereby achieving a rapid and easy protocol. first, magnetic-spe adsorbents were successfully applied to purify four type a trichothecenes (t-2, ht-2, das and neosolaniol (neo)) in coix seed, and were subsequently used for the simultaneous enrichment and purification of zen and four type a trichothecenes in salviae miltiorrhizae radix et rhizoma."
"while some analytical methods have been proposed, most are only aimed towards a single matrix. thus, a more universal approach should be developed in order to extend the application scope, such as classifying the analytical method based on the basis of matrix variations."
"where p is the noise from image acquisition and the inaccuracies in wavefront reconstruction, and h denotes the convolution operator resulting from the reconstructed wavefront. the deconvolution of the acquired image would then be a minimization function [cit] . in order to obtain the original image, u, the following equation is used:"
"the aptamer-based lateral flow assay is an attractive method due to its inherent advantages of using aptamers compared to antibodies. commonly, nucleic acid-based aptamers exhibit greater resistance to rough chemical conditions, including ph, ion strength, and organic solvent. this enables aptamers to better retain high sensitivity and specificity. in this respect, aptamers are preferable for use in complicated matrices. aptamer-based lateral flow assays have been developed for the rapid screening of ota in red wine [cit] and for afb 1 in corn sample [cit] . recently, an aptamer-based lateral flow strip was proposed for the on-site detection of ota in astragalus membranaceus, a frequently used tcm [cit] (figure 2 ). in the test protocol, one gram of a test sample is simply extracted using 2.5 ml of methanol-water (80:20, v/v) and diluted four folds with a working buffer in order to eliminate interference from the matrix and methanol. following optimization of critical testing parameters, a visual lod of 1 ng ml −1 was obtained with higher specificity, and the screening procedure could be carried out in 15 min. a total of one of nine samples was found to be positive for ota, and these results were in agreement with lc-ms/ms analysis. we note that the proposed method was only applied towards astragalus membranaceus matrix, and the suitability of other matrices should be further studied."
"rapid screening assays are critical tools for monitoring mycotoxin levels in herbal medicines. most immunological techniques can be utilized for rapid screening. in general, these techniques are qualitative assays that demonstrate either the presence or absence of mycotoxins in herbal medicines. screening assays have superior properties, including the speed at which the analysis is carried out, the simplicity of sample preparation, and the low cost per analysis. however, these methods also possess drawbacks as well, with the most notable one being reliability, as false-positive results are often obtained. however, different types of immunological techniques are continuing to emerge and are being rapidly developed."
"due to the complicated chemical constitution in herbal medicines, interference from sample matrices may be encountered at the retention times of analytes, resulting in incorrect identification of analyte. as a rule, these results require further confirmation using more reliable detectors, such as mass spectrometry, which is most commonly used in studies. however, some studies have also proposed an alternative strategy for the analyte confirmation. this strategy involves comparing hplc chromatograms of sample extract with derivatization and underivatization for the fluorescence intensities of afb 1 and afg 1 . in addition, they propose to perform re-measurements by adjusting the polarity of the mobile phase in order to overcome interference problem [cit] . this technique provides a practical approach for laboratories without an expensive mass spectrometer."
"herbal medicines, which are also referred to as phytomedicines or botanical medicines, have played a critical role in world health for thousands of years. according to the world health organization (who), \"herbal medicines include herbs, herbal materials, herbal preparations and finished herbal products, that contain as active ingredients parts of plants, or other plant materials, or combinations\" [cit] ."
"molecularly imprinted solid-phase extraction (mispe) devices for mycotoxin detection have become commercially available. [cit] has demonstrated the use of mip-based spe columns for a cleanup protocol for ota in ginger. this study demonstrated that mip exhibited a similar recovery compared to iac. in addition, following a simple regenerated procedure, the mip-based spe column exhibited excellent stability, and could be reused at least forty-one times and obtain greater than 80% ota recovery rates with ginger samples."
"another issue in mycotoxin determination is the development of a rapid detection method. conventional analytical methods always involve a more complex sample pretreatment protocol and a long detection time. these parameters limit their applications in high-throughput screening methods for mycotoxins. thus, an increased interest has developed in the generation of a rapid detection approach. however, while there have been several types of rapid methods that have been successfully used, these have only been used for single herbal medicine matrices. therefore, the challenge that exists for an analytical chemist is to broaden the application scope of the current methods, and work to develop a new rapid system that can use a variety of formats and platforms that are suitable for mycotoxin detection in herbal medicines."
"while high sensitivity was achieved, no significant advances were reported in relation to gc or gc-based methods for the analysis of mycotoxins in herbal medicine matrices in the past five years. this is likely attributed to the increasing application of lc-ms technology, which is more efficient in regards to time and can be carried out in the absence of complex derivatization procedure."
"ota, and these results were in agreement with lc-ms/ms analysis. we note that the proposed method was only applied towards astragalus membranaceus matrix, and the suitability of other matrices should be further studied."
"in addition, the method for sampling bulk and retail herbal material packages has been included in the guidelines published by the who in regard to quality control methods for herbal materials [cit] . in terms of sampling from bulk material, when a batch consists of five containers or packaging units, a sample must be taken from each. in addition, it is also recommended that in the case of batches with 6-50 units, samples from five should be taken. in the case of batches including greater than 50 units, samples must be taken from 10% of the individual units, and the number of units must be rounded up to the nearest multiple of 10. in regard to sampling material from retail packages, when each wholesale container (box, carton, etc.) is selected for sampling, two consumer packages must be taken at random. in the case of small batches (1-5 boxes), a total of 10 consumer packages should be taken."
"synchronization control between the pulsed laser and camera is particularly important in the proposed gated wavefront sensing system. laser pulses are controlled in conjunction with the shutter speed of the camera to capture the returning pulse from the target, which contains the information required for wavefront reconstruction. figure 2 illustrates the working principle of the synchronization control."
"in our gated wavefront sensing system, a lenslet array is used to focus the incoming wavefront onto a range-gated camera. this microlenslet array partitions the reflected wavfront into a larger number of smaller wavefronts, each of which is focused on a small spot on the sensor. the spatial displacement of each spot is a direct measure of the local slope of the incident wavefront as it passes through the lenslet. the integration of these slope measurements can reconstruct the shape of the wavefront."
"the experimental setup to detect underwater turbulence using the proposed gated wavefront sensing technique is shown in figure 5 . a wedge hb 532-nm wavelength was used as the laser source, and the 9450 series gated intensified ccd (iccd) camera system, combined with a lenslet, were used as the gated wavefront sensing system. the water tank created turbulence at a distance of four meters from the laser source. the laser pulse passed through the collimation system, traveled toward the turbulence, and back to the gated wavefront sensing system, where the image was captured. as explained in section 2.2, two delay generators were used in the synchronization control to the range gate at the distance of interest. experiments were carried out in a long water tank. the turbulence level was simulated by controlling the flow rate of the water. other environmental factors, such as the temperature of the water, were kept constant, as we only considered the effects of spatial turbulence due to the flow of water. actually, water disturbance, density, temperature, and other non-uniform factors will cause a random change in the refractive index of macromolecules in water. underwater turbulence causes small changes of the refractive index (∆n ≈ 10 −6 ), which caused a very small angular scatter of the beam. in general, the bidirectional reflectance distribution function (brdf) is an accurate description of the surface of the light reflection of the basic parameters. however, the underwater environment cannot be expressed by a simple negative exponential decay function. so, it is more difficult to measure its intensity distribution by conventional methods. the reflection ratio of light intensity in an underwater situation is about 20-30%. therefore, we use a 532-nm laser and a time-gated camera to reduce the impact of underwater reflectance, and get the focal imaging."
"however, cba has not yet been used extensively as a tool for routine mycotoxin analysis, due to the high cost and the complexity of the instruments required. [cit], 10, x for peer review 26 of 37"
the purpose of extraction is to remove mycotoxin from the herbal medicine matrix as much as possible into a solvent that is suitable for subsequent cleanup or direct analysis. the extraction solvent and method used are the two most important considerations for the extraction procedure.
"high-performance liquid chromatography (hplc) with a fld detector is perhaps the most commonly and widely used approach for af determination in herbal medicine matrices, and has been recommended in pharmacopeia in numerous countries and regions [44, 47, [cit] . afb 1 and afg 1 fluorescence is significantly quenched using aqueous solvent mixtures in reverse-phase chromatography. therefore a derivatization reaction is typically carried out for determination. over the past fifteen years, both pre-and post-column derivatization protocols have been proposed."
"synchronization control between the pulsed laser and camera is particularly important in the proposed gated wavefront sensing system. laser pulses are controlled in conjunction with the shutter speed of the camera to capture the returning pulse from the target, which contains the information required for wavefront reconstruction. figure 2 illustrates the working principle of the synchronization control."
"in our gated wavefront sensing system, a lenslet array is used to focus the incoming wavefront onto a range-gated camera. this microlenslet array partitions the reflected wavfront into a larger number of smaller wavefronts, each of which is focused on a small spot on the sensor. the spatial displacement of each spot is a direct measure of the local slope of the incident wavefront as it passes through the lenslet. the integration of these slope measurements can reconstruct the shape of the wavefront."
"the centroid locations of the focal points are then compared with the reference focal points. the displacement of each centroid location reflects the wavefront slope. the wavefront information obtained at a water velocity of 0 l/min is used as the reference wavefront. the schematic diagram of the wavefront sensor is shown in figure 4 . slope over a subaperture diameter in the y direction; x  is the measured spot centroid displacement from the reference in the x direction; and f is the focal length of the lenslet array. after the focal spots were detected, we used the threshold method to improve the spot centroid extraction accuracy. there are many factors that can affect the signal-to-noise ratio (snr) of the timegated camera, such as photo shot noise, dark current noise, and fixed pattern noise. in our situation, photo shot noise plays a major role in all kinds of noises. in order to improve the snr, we apply the thresholding algorithm. this algorithm filters out low-frequency and high-frequency noise, and improves the accuracy of spot centroid extraction in the wavefront reconstruction. the grayscale value of the light intensity of the spot image is given a weightage in order to highlight the influence of the central light intensity. on the basis of roughly determining the position of the centroid of the light spot, a window where the light spot is located is determined according to the position of the obtained centroid. accordingly, the center of each spot can be calculated as follows: ∂y, which are defined as:"
"although a number of emerging mycotoxins have been investigated in herbal medicine, little attention has paid to their conjugates, which can escape conventional analytical detection of parent (free) forms. in the future, it is recommended the related research be extended in herbal medicine samples. for example, estimating the potential risk of some samples that are commonly used and particularly vulnerable to mycotoxin contamination (e.g., coix seed, malt, medicated leaven), as well as performing novel types of conjugated mycotoxin screening and preparing their reference substances."
"in order to standardize the sampling procedure for mycotoxin testing, commission regulation (ec) no 401/2006 was set in order to lay down the sampling methods and analysis for the official control of mycotoxin levels in foodstuffs [cit], respectively [cit] . for example, with spices, the incremental samples should be taken depending on the weight of the lot. in the case of lots that weigh equal to or greater than 15 tons, 100 incremental samples should be taken from sub lots that make up a 10 kg aggregate sample weight. in the case of samples weighing less than 15 tons, 5 to 100 incremental samples should be taken depending on the lot weight, resulting in an aggregate sample weight of 0.5 to 10 kg. [cit] /27/ec also regulates the sampling methods utilized for af analysis in spices. certain distinctions exist between these two regulations. [cit] /27/ec, when the weight of the lot is less than 15 tons, 10 to 100 incremental samples should be taken that make up a 1 to 10 kg aggregate sample weight [cit] ."
"sample pretreatment has always been a challenging step for trace analysis in herbal medicine matrices. because of the sophisticated and various compositions of herbs, especially in the case when multiclass mycotoxins are simultaneously determined, sample preparation protocols must often be optimized in order to increase the extraction efficiencies. significant advances have been made in the cleanup protocol with the emergence of traditional iac and spe. novel nanomaterials have been applied as absorbents, which were demonstrated to improve specificity relative to conventional types. in addition, aac and mip columns were introduced as an alternative to iac, as they can be recycled in order to reduce costs."
"in this paper, we proposed a novel method to detect underwater turbulence using a gated wavefront sensing system. the proposed method incorporated wavefront sensing and the range gated approach for effective underwater turbulence detection. based on the operating principle of this technique, the laser emission and camera gating are simultaneously controlled to only capture the reflection from a known distance. the turbulence condition can be detected from the resulting wavefront reconstruction accordingly. an experimental platform was setup to validate the proposed gated wavefront sensing system. our experimental results prove that the proposed method can detect underwater turbulence conditions at different distances, and for different levels of turbulence. the aggregated wavefront distortion is successfully detected at increasing depths of the turbulent media. due to the effectiveness of the proposed method, it has good potential, which will significantly benefit applications in underwater imaging, laser communication, oceanic exploration, etc."
"we acquired the focal image with the flow rate of 0 l/min as a reference for wavefront reconstruction. four distances were selected for turbulence comparison, namely: 1 m, 2 m, 3 m, and 4 m. the water flow rate was set at 40 l/min. figure 8 shows the focal spot images from different distances in the water tank with turbulence. the dark area in this focused image is a result of the light sensor receiving insufficient light intensity, which was caused by the camera's shutter, the underwater scattering light conditions, or other unspecified factors. it will have a slight effect on the calculation of the center shifts. therefore, we used the thresholding method in the centroid extraction to reduce this error and get a better reconstructed image. in order to observe the underwater turbulence effectively, we used the zernike polynomials for wavefront reconstruction. in our underwater situation, the gradient reconstruction method is considered. underwater turbulence is affected by many factors, such as water temperature, the dark area in this focused image is a result of the light sensor receiving insufficient light intensity, which was caused by the camera's shutter, the underwater scattering light conditions, or other unspecified factors. it will have a slight effect on the calculation of the center shifts. therefore, we used the thresholding method in the centroid extraction to reduce this error and get a better reconstructed image. in order to observe the underwater turbulence effectively, we used the zernike polynomials for wavefront reconstruction. in our underwater situation, the gradient reconstruction method is considered. underwater turbulence is affected by many factors, such as water temperature, density, etc. we focused on one factor, which is the water flow rate. due to the way that turbulence simulation is controlled by the water flow velocity changes in the water, we considered the gradient reconstruction method. this method has a faster processing speed. it can reconstruct objects more specifically. the offset was obtained by subtracting the center of reference focal spots from the measured focal spots and obtain the reconstruction coefficient. accordingly, we reconstructed the entire wavefront. figure 9 shows the wavefront reconstruction results from different distances in the water tank with turbulence. it is more intuitive to show underwater turbulence at different distances. in general, the existing detection methods are ineffective for detecting underwater turbulence, because of the scattering problem. as shown in figure 9, we are able to detect turbulence changes at different distances using the proposed gated wavefront sensing. the reconstructed wavefronts were calculated in radians where the coefficients were multiplied by 2/  .  is the wavelength. the vertical axis is the phase in radians, and the xy plane, is the space where the phase is measured. at the distance of 4 m from our sensing system (where turbulence originated from the water flow in), the wavefront reconstruction model is relatively volatile. at distances further away from the water inlet, the turbulence effect becomes weaker, and the reconstructed wavefront image gradually flattens out, as shown in figure 9 . this verified that the underwater turbulence can be detected by the proposed gated wavefront sensing system."
"denote the partial derivatives of the kth zernike polynomial in the x and y directions within the mth rows and nth columns' subaperture, respectively. c k is the coefficient of the kth zernike polynomial to be solved. s xmn and s ymn are the measured wavefront slopes in the x and y directions on the mth rows and nth columns' subaperture, which is a known amount that can be obtained by measurement. the coefficient matrix c can be calculated using the least squares method as:"
"subsequently, this study [cit] thoroughly validated the applicability of aac for use with various types of tcm matrices, including fruits, seeds, rhizomes, roots, flowers, grasses, leaves, and animals. satisfactory recoveries and enrichment purification effects were obtained using the aac-based cleanup method, and this result further indicated that aac possesses a promising application prospect in trace analysis."
"in our next set of experiments, we set the distance at 4 m (where the flow rate can be controlled to create a turbulence condition) and varied the velocity of water flow. figure 10 shows the focal spot images acquired from the same distance i.e., 4 m, with different water flow rates. figure 11 using the reference wavefront obtained from a flow rate of 0 l/min, the wavefronts above in figure 11 were obtained. as the turbulence generated is spontaneous and impromptu, repeated measurements would not yield similar wavefronts. therefore, these wavefronts were characterized based on the relative difference between the above spot images and the reference 0 l/min spot image. it can be seen that the peak-to-valley ratio of the reconstructed wavefronts relative to the reference wavefront increased as the flow rate increased. therefore, there was indeed distortion in the wavefronts due to the induced turbulence from the turbulent flow of water at 4 m."
"while less investigated compared to af and ota, the determination of some other mycotoxins in herbal medicine matrices using hplc-fld was proposed, including cit [cit], zen [cit], and fbs [cit] ."
"the first ratio ac quantifies the relative loss of optimality of the robust solution with deterministic data, and the second ratio wc measures the relative increase of the optimal value of the algorithm in the worst case. therefore the ratio wc measures the maximum protection that a robust solution can provide, while ac is the percent increase in cost for this protection . figures 1(a) and 1(b) show the topology generated by running lmst and lrmst algorithm in the first case 1 as 100 nodes are uniformly distributed in the area, where the red lines represent different edges in the topology structure generated by the two algorithms. it can be seen from the figure that lrmst algorithm tends to select some edges whose estimated distance is slightly longer."
"big data system allowed syriatel company to collect, store, process, aggregate the data easily regardless of its volume, variety, and complexity. in addition, it enabled extracting richer and more diverse features like sna features that provide additional information to enhance the churn predictive model."
"nodes can acquire information about their neighbor nodes, including id and the distance between them, and the distance can be obtained by some distance measurement algorithms. this can be accomplished by sending a beacon message at maximum transmit power."
"this social network is also used to find similar customers in the network based on mutual friend concept. each customer has 2 similarity features with the other customers in his network, like jaccard similarity, and cosine similarity. these calculations were done for each distinct couple in the social network, where each customer will have two calculations in the network. to reduce this complexity, customers who don't have mutual friends are excluded from these calculations. the highest values for both measures are selected for each customer ( top jaccard and cosine similarity for similar syriatel customer and top jaccard and cosine similarity for similar mtn customer). jaccard measure: normalize the number of mutual friends based on the union of the both friends lists, [cit] ."
"burez and van den poel [cit] studied the problem of unbalance datasets in churn prediction models and compared performance of random sampling, advanced undersampling, gradient boosting model, and weighted random forests. they used (auc, lift) metrics to evaluate the model. the result showed that undersampling technique outperformed the other tested techniques."
"moving the data from outside sytl-bd into hdfs was the first step of work. the data is divided into three main types which are structured, semi-structured and unstructured."
"the second important feature is days of last outgoing transaction. as shown in fig. 7a, most churners stay longer period than non-churners without making any transaction. the third important feature is total balance since most churners had low balance compared with the active customers regardless of the reason of churn, fig. 7c shows the distribution of total balance feature for churners and non-churners customers."
"we believe that big data facilitated the process of feature engineering which is one of the most difficult and complex processes in building predictive models. by using the big data platform, we give the power to syriatel company to go farther with big data sources. in addition, the company becomes able to extract the social network analysis features from a big scale social graph which is built from billions of edges (transactions) that connect millions of nodes (customers). the hardware and the design of the big data platform illustrated in \"proposed churn method\" section fit the need to compute these features regardless of their complexity on this big scale graph."
"the rest of the paper is organized as follows. we first introduce a literature review of related work in section 2. in section 3, some results based on 0-1 robust discrete optimization theory are given and proved. topology control protocol of lrmst under uncertain distance is designed in section 4. in section 5, simulation results are considered. we conclude the paper in section 6."
"various researches studied the problem of unbalanced data sets where the churned customer classes are smaller than the active customer classes, as it is a major issue in churn prediction problem. [cit] compared six different sampling techniques for oversampling regarding telecom churn prediction problem. the results showed that the algorithms (mtdf and rules-generation based on genetic algorithms) outperformed the other compared oversampling algorithms."
"many previous attempts using the data warehouse system to decrease the churn rate in syriatel were applied. the data warehouse aggregated some kind of telecom data like billing data, calls/sms/internet, and complaints. data mining techniques were applied on top of the data warehouse system, but the model failed to give high results using this data. in contrast, the data sources that are huge in size were ignored due to the complexity in dealing with them. the data warehouse was not able to acquire, store, and process that huge amount of data at the same time. in addition, the data sources were from different types, and gathering them in data warehouse was a very hard process so that adding new features for data mining algorithms required a long time, high processing power, and more storage capacity. on the other hand, all these difficult processes in data warehouse are done easily using distributed processing provided by big data platform."
"the generated dataset was unbalanced since it is a special case of the classification problem where the distribution of a class is not usually homogeneous with other classes. the dominant class is called the basic class, and the other is called the secondary class. the data set is unbalanced if one of its categories is 10% or less compared to the other one [cit] . although machine learning algorithms are usually designed to improve accuracy by reducing error, not all of them take into account the class balance, and that may give bad results [cit] . in general, classes are considered to be balanced in order to be given the same importance in training."
apache flume is a distributed system used to collect and move the unstructured (csv and text) and semi-structured (json and xml) data files to hdfs. figure 3 shows fig. 1 hortonworks data platform hdp-big data framework 2 https ://hadoo p.apach e.org/docs/r1.2.1/hdfs_desig n.html. 3 https ://spark .apach e.org/. 4 https ://hadoo p.apach e.org/docs/curre nt/hadoo p-yarn/hadoo p-yarn-site/yarn.html. 5 https ://zeppe lin.apach e.org/.
"there are two telecom companies in syria which are syriatel and mtn. syriatel company was interested in this field of study because acquiring a new customer costs six times higher than the cost of retaining the customer likely to churn. the dataset provided by syriatel had many challenges, one of them was unbalance challenge, where the churn customers' class was very small compared to the active customers' class. we experimented three scenarios to deal with the unbalance problem which are oversampling, undersampling and without re-balancing. the evaluation was performed using the area under receiver operating characteristic curve \"auc\" because it is generic and used in case of unbalanced datasets [cit] ."
apache sqoop is the distributed tool used to transfer the bulk of data between hdfs and relational databases (structured data). this tool was used to transfer all the data which exists in databases into hdfs by using map jobs. figure 4 shows the architecture of sqoop import process where four mappers are defined by default. each map job selects part of the data and moves it to hdfs. the data is saved in csv file type after being transported by sqoop to hdfs.
"we assume that data uncertainty affects only the elements of cost vector c. although it is unlikely to know the exact distribution of these coefficients in c, we can estimate its mean value and its range in typical applications. specifically, the model of data uncertainty we consider is as follows."
"in this paper, the feature engineering phase is taken into consideration to create our own features to be used in machine learning algorithms. we prepared the data using a big data platform and compared the results of four trees based machine learning algorithms."
"the solution we proposed divided the data into two groups: the training group and the testing group. the training group consists of 70% of the dataset and aims to train the algorithms. the test group contains 30% of the dataset and is used to test the algorithms. the hyperparameters of the algorithms were optimized using k-fold cross-validation. the value of k was 10. the target class is unbalanced, and this could cause a significant negative impact on the final models. we dealt with this problem in our research by rebalancing the sample of training by taking a sample of data to make the two classes balanced [cit] . we started with oversampling by duplicating the churn class to be balanced with the other class. we also used the random undersampling method, which reduces the sample size of the large class to become balanced with the second class. this method is the same as the one used in more than one research papers [cit] . it gave the best result for some algorithms. the training sample size became 420,000."
"the data used in this research is collected from multiple systems and databases. each source generates the data in a different type of files as structured, semi-structured (xml-json) or unstructured (csv-text). dealing with these kinds of data types is very hard without big data platform since we can work on all the previous data types without making any modification or transformation. by using the big data platform, we no longer have any problem with the size of these data or the format in which the data are represented."
"after transporting all the data from its sources into hdfs, it was important to choose the appropriate file type that gives the best performance in regards to space utilization and execution time. this experiment was done using spark engine where data frame library 10 was used to transform 1 terra byte of csv data into apache parquet 11 file type and apache avro 12 file type. in addition to that, three compression scenarios were taken into consideration in this experiment. parquet file type was the chosen format type that gave the best results. it is a columnar storage format since it has efficient performance compared with the others, especially in dealing with feature engineering and data exploration tasks. on the other hand, using parquet file type with snappy compression technique gave the best space utilization. figure 5 shows some comparison between file types."
the model also was evaluated using a new dataset and the impact of this system to the decision to churn was tested. the model gave good results and was deployed to production.
"as we all know, wsns are generally deployed in harsh environments. distance between nodes is seriously affected by uncertain external factors, such as measurement error, actual interference, and attacks. consequently, we pay more attention to the robust solution that is efficient for all likely distance measurements. when determining the value of adjusting parameter γ, we set it to its maximum possible value in the following."
"spark engine is used to explore the structure of this dataset, it was necessary to make the exploration phase and make the necessary pre-preparation so that the dataset becomes suitable for classification algorithms. after exploring the data, we found that about 50% of all numeric variables contain one or two discrete values, and nearly 80% of all the categorical variables have less than 10 categories, 15% of the numerical variables and 33% of the categorical variables have only one value. most of some variables' values are around zero. we found that 77% of the numerical variables have more than 97% of their values filled with 0 or null value. these results indicate that a large number of variables can be removed because these variables are fixed or close to a constant. this dataset encounters many challenges as follow."
"is the normalized weight of the directed edge from n to m. the same description is used for sender rank. due to the random walk nature of the eqs. (1) and (2), pr and sr will be stable after a number of iterations. these values indicate the importance of the customers since the higher values of pr(m) and sr(m) corresponds to the higher importance of customers in the social network. other sna features like the degree of centrality, in and out degree which is the number of distinct friends in receive and send behavior were calculated. the feature neighbor connectivity based on degree centrality which means the average connectivity of neighbors for each customer is also calculated [cit] ."
"the mst problem has direct applications in the design of networks, including computer networks, telecommunications networks, transportation networks, water supply networks, and electrical grids [cit] . there are now two algorithms commonly used for finding an mst, namely, prim's algorithm [cit] and kruskal's algorithm [cit], and both are greedy algorithms that run in polynomial time. in lmst protocol, each node constructs its local mst. in order to enhance the robustness of lmst, we should achieve the robust counterpart of mst problem in the first place."
"the data used in this research contains all customers' information throughout nine months before baseline. the volume of this dataset is about 70 terabyte on hdfs \"hadoop distributed file system\", and has different data formats which are structured, semi-structured, and unstructured. the data also comes very fast and needs a suitable big data platform to handle it. the dataset is aggregated to extract features for each customer."
"we built the social network of all the customers and calculated features like degree centrality measures, similarity values, and customer's network connectivity for each customer. sna features made good enhancement in auc results and that is due to the contribution of these features in giving more different information about the customers."
we found that syriatel dataset was unbalanced since the percentage of the secondary class that represents churn customers is about 5% of the whole dataset.
"since we don't know the features that could be useful to predict the churn, we had to work on all the data that reflect the customer behavior in general. we used data sets related to calls, sms, mms, and the internet with all related information like complaints, network data, imei, charging, and other. the data contained transactions for all customers during nine months before the prediction baseline. the size of this data was more than 70 terabyte, and we couldn't perform the needed feature engineering phase using traditional databases."
"wireless sensor networks (wsns for short) consist of a large number of sensor nodes which are characterized by the constrained battery, memory, and processing power. recently, wsn has been popularly used in a wide range of applications such as environment monitoring, military target tracking and surveillance, natural disaster relief, biomedical health monitoring, real-time monitoring, vehicle tracking, and so on. wsn has attracted the attention of both the academic and the industrial research communities in the last few years."
"local cluster coefficient is another sna feature, it's ranked fifth in importance to predict the churn since the customers with very low lcc value as shown in fig. 9b are less likely to churn. this could be justified because some customers are using these personal gsms for business objectives. they need to preserve their numbers in order not to lose any of their customers. most of these customers have more than two gsms. they communicate with lots of people, most of these people don't know each other (there is no interaction between them). a sample of customers with very low lcc were contacted to check this case. the results show that most of them were related to cafes, restaurants, shaving shops, hairdressers, libraries, game shops, medical clinics, and others."
"as a critical issue in energy-efficient wsn design, topology control is carried out to form an optimized network topology structure by adjusting the communication range of nodes with the desired properties such as connectivity and coverage while reducing energy consumption and increasing network capacity [cit] . this technology can enhance the efficiency of routing and mac protocol, lay a foundation for the data fusion, time synchronization, and target localization, and prolong the survival time of the whole network [cit] ."
the top important features that contribute to predict the churn were ranked using gain measure [cit] . the high gain value of the feature means the more important it is in predicting the churn. the important features according to xgboost algorithm are presented in fig. 13 before and after merging sna and statistical features.
"there is a representation of each service and product for each customer. missing values may occur because not all customers have the same subscription. some of them may have a number of services and others may have something different. in addition, there are some columns related to system configurations and these columns have only null value for all customers."
"the optimality of all above algorithms is guaranteed depending on the accuracy of distance between nodes. however, in many applications, the distance measurements are subject to uncertainty as they are indirectly estimated through signal strength or due to unfriendly conditions during the wsn's deployment or operation. on the other hand, since sensors have limited energy, when designing the topology protocol, distance is an important factor that influences the energy consumption because sensors transmitting data consume an amount of energy proportional to approximately the square of the distance [cit] . the effect of ignoring distance uncertainty in the efficiency of the operation of wsn is unclear."
"as displayed in fig. 11 and depending on tables 2 and 3, we confirm that xgboost algorithm outperformed the rest of the tested algorithms with an auc value of 93.3% so that it has been chosen to be the classification algorithm in this proposed predictive model. gbm algorithm occupied second place with an auc value of 90.89% while random forest and decision trees came last in auc ranking with values of 87.76% and 83% sequentially. figure 12 shows the roc curves for the four algorithms."
"since we have data related to all customers' actions in the network, we aggregated the data related to calls, sms, mms, and internet usage for each customer per day, week, and month for each action during the nine months. therefore, the number of generated features increased more than three times the number of the columns. in addition, we entered the features related to complaints submitted from the customers from all systems. some features were related to the number of complaints, the percentage of coverage complaints to the whole complaints submitted, the average duration between each two complaints sequentially, the duration in \"hours\" to close the complaint, the closure result, and other features."
"many approaches were applied to predict churn in telecom companies. most of these approaches have used machine learning and data mining. the majority of related work focused on applying only one method of data mining to extract knowledge, and the others focused on comparing several strategies to predict churn."
"the experts in marketing decided to predict the churn before 2 months of the actual churn action, in order to have sufficient time for proactive action with these customers."
"many topology control algorithms are proposed under the assumption that the distance between nodes is accurate. these methods available are based mainly on traditional optimization or heuristics, without giving full consideration of the effects produced by the interference, errors, and malicious attacks in modeling the networks, so the robustness is poor. in kneigh algorithm [cit], by broadcasting its id at maximum power, every node in the network knows its neighbor set and distance-based ordering of the neighbors. given this information, every node computes its -closest neighbors and its transmit power is set to the minimum value needed to reach the farthest neighbor node. drng algorithm [cit] deals with the heterogeneous wsn with nonuniform transmission ranges; with two node positions as the center and the distance between the two nodes as the radius, two circles are generated. if there is no other node in the intersection of two disks, the two nodes are called neighbor nodes. in life algorithm [cit], mst is generated using the edge covering value as the weight, and the calculation of edge covering value requires information about the distance between nodes. to overcome the drawback of requiring the exchange of global information of mst, [cit] introduced lmst protocol, which computes a local approximation of the mst. after receiving the information of neighbors at maximum transmit power, each node constructs its local mst by applying classical prim's algorithm with the distance between nodes as the link weight. in lmst each node determines its transmit power, that is, the minimum power needed to reach the farthest node in its one-hop neighbor set. by deleting or adding edges in the lmst topology, a bidirectional network formation is constructed in the end. lmst algorithm preserves that the produced network is connected and the upper bound of node degree equals 6."
"another similarity measure is the cosine measure which is similar to jaccard's. on the other hand, this similarity measure calculates the cosine of the angle between every two customers' vectors where the vector is the friend list of each customer [cit] ."
"the goal of 0-1 robust discrete optimization is to give the optimal solution of the problem when cost coefficients change in the uncertainty set, and then the optimal solution is called robust solution. robust solution ensures the optimality of the problem in the worst case; that is, all the uncertain cost coefficients change, but at this time it is too conservative. to control the conservatism degree of the robust solution, we introduce a parameter γ called adjusting parameter [cit] ."
"the social network analysis features had a different scenario, when the best sliding window to build the social graph and extract appropriate sna features was during the last four months before the baseline, as shown in fig. 11b . adding more old data will adversely affect the performance of the model. the highest auc value reached by using only sna features was 75.3%. depending on the above two different scenarios, the last 6 months of the raw dataset was used to extract the statistical features, while the last four months of that dataset was figure 10 presents the best sliding window to extract sna features in orange and the blue one is for statistical features while the red line represents the baseline."
"problem (12) presents a robust model under distance uncertainty for the problem of mst, and theorem 3 shows that the robust solution of problem (12) can be obtained by solving only one deterministic mst problem. protocol rmst is summarized in algorithm 1."
"the average node degrees of lmst algorithm and lrmst algorithm are compared in figure 2, which are the average results of 50 simulations. the topology structure constructed by lmst algorithm in two cases 1 and 2 is the same, so that the two curves of lmst coincide. as can be seen from figure 2, the topology generated by lrmst algorithm in the two cases is very sparse; the average node degree is less than 2.2. since the average node degree of the spanning tree is 2-2/, it is shown that the topology structure generated by lrmst algorithm is also very close to the spanning tree. figure 3 indicates the values obtained from formula (14) after running lmst and lrmst algorithm, and they are the mean results of 50 simulations. we can see from figure 3 that ac is much smaller than wc in the two cases, and, with the increase of the node number, especially in the second case 2, wc increases very fast and its increment speed is much higher than that of ac . therefore, lrmst algorithm can provide the solution which exhibits an important improvement in the worst case performance at the expense of a small loss in optimality, and the more nodes become, the more superiority of lrmst algorithm can be reflected. as we know, distance uncertainty increases from case 1 to 2 . with increasing uncertainty, we observe a faster increase in wc with little change in ac, which shows that the robust solution becomes more attractive as the distance uncertainty increases. from this observation, it can be concluded that lmst algorithm is very sensitive to the uncertainty, while lrmst algorithm changes a little in two cases. lrmst algorithm can provide the solution which exhibits an important improvement in the worst case performance at the expense of a small loss in optimality, and the more nodes become, the more superiority of lrmst algorithm can be reflected."
many research confirmed that machine learning technology is highly efficient to predict this situation. this technique is applied through learning from previous data [cit] .
"the data was processed to convert it from its raw status into features to be used in machine learning algorithms. this process took the longest time due to the huge numbers of columns. the first idea was to aggregate values of columns per month (average, count, sum, max, min ...) for each numerical column per customer, and the count of distinct values for categorical columns."
"there are extensive literatures [cit] that have discussed topology control algorithms, and they are designed with accurate distance generally. however, if we consider the existence of uncertain factors, the optimality of these algorithms is very difficult to guarantee. a more successful strategy can be a solution that is less optimal for a particular distance vector but obtains efficient solutions for all likely distance measurements. therefore, the robustness of algorithm is involved."
"customers' churn is a considerable concern in service sectors with high competitive services. on the other hand, predicting the customers who are likely to leave the company will represent potentially large additional revenue source if it is done in the early phase [cit] ."
"some features such as contract id, msisdn and other unique features for all customers were removed. they are not used in the training process because they have a direct correlation with the target output (specific to the customer itself ). we deleted features with identical values or missing values, deleted duplicated features, and features that have few numeric values. we found that more than half of the features have more than 98% of missing values. we tried to delete all features that have at least one null value, but this method gave bad results."
"by adding sna features with the statistical features to the classification algorithms, the results increased significantly. as presented in table 2 and fig. 11c, the addition of both types of features made a good enhancement to the performance of the churn predictive model, where the max reached value of auc was 93.3%."
"the second concern taken into consideration was the problem of the unbalanced dataset since three experiments were applied for all classification algorithms. these experiments are: (1) classification with undersampling technique, (2) classification with oversampling technique, (3) classification without balancing the dataset. table 3 shows that both xgboost and gbm algorithms gave the best performance without any rebalancing techniques, while random forest and decision tree algorithms gave a higher performance by using undersampling techniques."
"the importance of this type of research in the telecom market is to help companies make more profit. it has become known that predicting churn is one of the most important sources of income to telecom companies. hence, this research aimed to build a system that predicts the churn of customers in syriatel telecom company. these prediction models need to achieve high auc values. to test and train the model, the sample data is divided into 70% for training and 30% for testing. we chose to perform cross-validation with 10-folds for validation and hyperparameter optimization. we have applied feature engineering, effective feature transformation and selection approach to make the features ready for machine learning algorithms. in addition, we encountered another problem: the data was not balanced. only about 5% of the entries represent customers' churn. this problem was solved by undersampling or using trees algorithms not affected by this problem. four tree based algorithms were chosen because of their diversity and applicability in this type of prediction. these algorithms are decision tree, random forest, gbm tree algorithm, and xgboost algorithm. the method of preparation and selection of features and entering the mobile social network features had the biggest impact on the success of this model, since the value of auc in syriatel reached 93.301%. xgboost tree model achieved the best results in all measurements. the auc value was 93.301%. the gbm algorithm comes in the second place and the random forest and decision tree came authors' contributions aka took the role of performing the literature review, building the big data platform, working on the proposed churn model. he conducted the experiments and wrote the manuscript. aj and kj took on a supervisory role and oversaw the completion of the work. all authors read and approved the final manuscript."
"in this paper, we use the robust discrete optimization theory to deal with distance uncertainty for topology control 2 international journal of distributed sensor networks problem in wsn. considering that the distance between nodes is uncertain, a distributed topology control algorithm named as lrmst (local robust minimum spanning tree) is proposed. with the uncertainty increasing, the robust solution for the problem provides a significant performance improvement in the worst case at the expense of a small loss in optimality when compared to lmst algorithm."
"for the first time, the robust optimization theory is used for solving the optimization model of wsn in the literature [cit] . when the distance between nodes is uncertain, robust counterparts are given and solved of minimum energy consumption problem, maximum data extraction problem, and maximum network lifetime problem; the experiment results show that the robust optimization model has very good performance in practice. consequently, combined with the theory and methodology of 0-1 robust discrete optimization, this paper will research the distributed topology control algorithm for wsn, aiming at solving the uncertainty problem for wsn in practical application scenarios. belonging to problem-driven studies, this paper will have great theoretical and practical value in solving the uncertainty problem of topology control in practical applications of wsn."
"the sixth important feature is the percentage of transactions to/from other operator, this value becomes bigger for churners. the explanation here relies on the effect of friends on the churn decision, since the affiliation of most of customer's friends to the other operator may be evidence of the good reputation or the strong existence of the competing company in that region or community. therefore, this can result in the customer being influenced by the surrounding environment, so he moves to the competing company. the higher value of this feature may increase the likelihood of churn, fig. 7d displays the distribution of this feature."
"the cosign similarity is useful when the customer is in the phase of leaving the company to the competitor, where he starts building his network on the new gsm line to be similar to the old being churned, taking into consideration that the new line has a small friends list compared with the old one. these features are used for the first time to enhance the prediction of churn, and they have a positive effect along with the other statistical features. the distribution of the main sna features are presented in fig. 9 . table 1 shows some calculated main sna features with illustration."
"depending on what was mentioned previously and as shown in figs. 11c, 12c, we belive that social network analysis features have a good contribution to increase the performance of churn prediction model, since they gave a different insight to the customer from the social point of view."
"as presented in fig. 13, adding the social network analysis features changed the ranking of the important features. the mtn cosine similarity was the most important feature since the customers with higher mtn cosine similarity are more likely to churn regardless of the other features like balance, internet usage, and in/out calls. figure 9a displays the distribution of this feature by analyzing this feature, most of the customers generally stayed active for a period of time before terminating or stopping the use of their gsms. this case probably happens because the customer needs to make sure that most of his important incoming calls and contacts have moved to the new line. in other words, the customer could wait for a period of time to make sure that most of his important people have known the new gsm number. this case also could be justified as the customer need to finish the remaining balance in the gsm before he stops using it. figure 14 shows an example of a tracked churned customer. this figure presents the phases of moving his community to the other operator's gsm. the customer bought gsm from the competitor in week 7 and terminated syriatel's gsm in week 14 before being out of coverage in week 13 and week 14. the result of the cosine similarity is also displayed in the same figure."
"in order to build the churn predictive system at syriatl, a big data platform must be installed. hortonworks data platform (hdp) 1 as the development user interface, ambari 6 to monitor the system, ranger 7 to secure the system and (flume 8 system and scoop 9 tool) to acquire the data from outside sytl-bd framework into hdfs. the used hardware resources contained 12 nodes with 32 gigabyte ram, 10 terabyte storage capacity, and 16 cores processor for each node. a nine consecutive months dataset was collected. this dataset will be used to extract the features of churn predictive model. the data life cycle went through several stages as shown in fig. 2 spark engine was used in most of the phases of the model like data processing, feature engineering, training and testing the model since it performs the processing on ram. in addition, there are many other advantages. one of these advantages is that this engine containing a variety of libraries for implementing all stages of machine learning lifecycle."
"we started training decision tree algorithm and optimizing the depth and the maximum number of nodes hyperparameters. we experimented with several values, the optimized number of nodes was 398 nodes in the tree and the depth value was 20. random forest algorithm was also trained, we optimized the number of trees hyperparameter. we experimented with building the model by changing the values of this parameter every time in 100, 200, 300, 400 and 500 trees. the best results show that the best number of trees was 200 trees. increasing the number of trees after 200 will not give a significant increase in the performance. gbm algorithm was trained and tested on the same data, we optimized the number of trees hyper-parameter with values up to 500 trees. the best value after the experiment was also 200 trees. gbm gave better results than rf and dt. we finally installed xgboost on spark 2.3 framework and integrated it with ml library in spark and applied the same steps with the past three algorithms. we also optimized the number of trees, and the best value after multiple experiments was 180 trees."
"the fourth feature in importance is average of radio access type where most of the churners had more 2g internet sessions than 3g sessions, as the speed and quality of 2g internet sessions is much less than these of 3g sessions. figure 7b shows the distribution of this feature where the average rat is lower for most of the churners compared with that of non-churners. the customers are more likely to churn if they are heavy internet users and there is a better 3g coverage provided by the competitor. by analyzing this feature, 68% of churners are internet users, 65% of them have low average radio access type value."
"we focused on evaluating and analyzing the performance of a set of tree-based machine learning methods and algorithms for predicting churn in telecommunications companies. we have experimented a number of algorithms such as decision tree, random forest, gradient boost machine tree and xgboost tree to build the predictive model of customer churn after developing our data preparation, feature engineering, and feature selection methods."
"the features related to imei data such as the type of device, the brand, dual or mono device, and how many devices the customer changed were extracted."
we did not find any research interested in this problem recorded in any telecommunication company in syria. most of the previous research papers did not perform the feature engineering phase or build features from raw data while they relied on ready features provided either by telecom companies or published on the internet.
"1. customer data it contains all data related to customer's services and contract information. in addition to all offers, packages, and services subscribed to by the customer. furthermore, it also contains information generated from crm system like (all customer gsms, type of subscription, birthday, gender, the location of living and more ...). this data has a large size and there is a lot of detailed information about it. we spent a lot of time to understand it and to know its sources and storing format. in addition to these records, the data must be linked to the detailed data stored in relational databases that contain detailed information about the customer. the nine months of data sets contained about ten million customers. the total number of columns is about ten thousand columns."
"finally, we filled out the missing values with other values derived from either the same features or other features. this method is preferable so that it enables us to use the information in most features for the training process. we applied the following:"
"in the context of uncertain scenario, finding an optimal robust solution involves solving the above min-max problem (3) . for many classical 0-1 discrete optimization problems, finding such a robust solution is np-hard. however, the following theorem shows that if the deterministic 0-1 discrete optimization problem (1) has a polynomial solution, then the 0-1 robust discrete optimization problem (3) also has a polynomial solution."
"other features like customer age is also ranked at the seventh place in importance since the customers who are less than 32 years old have more likelihood to churn than the others. this can be explained by the fact that young people are always looking for the best to meet their needs in better, higher quality, and less expensive services as the volume of communication, the use of internet, and other services are much higher compared to services of customers of different ages. figure 6 shows the distribution of this feature regarding the churn class. the social power factor feature is the third sna feature that is considered one of the top important features to predict the churn. as presented in fig. 9c the higher power factor value means the less likely to churn. as also shown in fig. 7e, the customers with high signal errors and dropped calls are most likely to churn."
the results were analyzed to compare the performance regarding the different sizes of training data. dealing with unbalanced dataset using the three scenarios were also analyzed. the first main concern was about choosing the appropriate sliding window for data to extract statistical and sna features. how much historical data is needed in features engineering phase?
"in fig. 11, m1 refer to the first month before the baseline and m9 refer to the ninth month before baseline. the features of month n are aggregated from the n-month sliding data window (from month 1 to month n). as fig. 11a presents, we can confirm that increasing the volume of training data to get statistical features increases the performance of the classification algorithms. however, the addition of the oldest three months did not provide any enhancement on model performance. when only using statistical features, the highest value of auc reached 84%."
"the percentage of the retained customers from offered dataset was about 47% from all customers predicted to churn. in other words, about half of the customers changed their mind regarding churn decision when they got a good offer. this result was very good for the company, increased the revenue and decreased the churn rate by about 1.5%."
"many topology control algorithms do not take into account the distance uncertainty; in fact, the distance is given by a distance measurement algorithm, which results in distance between nodes being uncertain. when the distance is uncertain, using the 0-1 robust discrete optimization theory, a distributed topology control algorithm is proposed. simulation results show that lrmst algorithm is robust for uncertain data along with the increasing number of nodes, compared with lmst algorithm. this paper has introduced the robust optimization into the topology control technology of wsn for the first time, which provides a new idea for the future research. the robust discrete optimization theory can also be introduced into other research directions of wsn, such as location technology and routing protocol."
"flume agents transporting files exist in the defined spooling directory source using one channel, as configured in sytl-bd. this channel is defined as memory channel because it performed better than the other channels in flume. the data moves across the channel to be finally written in the sink which is hdfs. the data transformed to hdfs keep in the same format type as it was."
"we did many rounds of brainstorming with seniors in the marketing section to decide what features to create in addition to those mentioned in some researches. we created many features like percentage of incoming/out-coming calls, sms, mms to the competitors and landlines, binary features to show if customers were subscribing some services or not, rate of internet usage between 2g, 3g and 4g, number of devices used each month, number of days being out of coverage, percentage of friends related to competitor, and hundred of other features."
"the designed architecture of flume in sytl-bd. there are three main components in flume. these components are the data source, the channel where the data moves and the sink where the data is transported."
"wsns are generally deployed in harsh environments to monitor the target field and detect the occurrence of important events. since there are measurement error, actual interference, attacks, and other external factors that seriously affect the topology control problem, new robust topology control technologies have to be established for wsn under uncertain environments. in addition, given the energy constraint, the dynamic change of network topology and the relatively poor communication quality, another challenge, have to be faced for the wsn designers: the need to operate distributedly."
"the collected data was full of columns, since there is a column for each service, product, and offer related to calls, sms, mms, and internet, in addition to columns related to personnel and demographic information. if we need to use all these data sources the number of columns for each customer before the data being processed will exceed ten thousand columns."
"graph-based features are extracted from the social graph. the graph is a weighted directed graph. we built three graphs depending on the used edges' weight. the weight of edges is the number of shared events between every two customers. we used three types of weights: (1) the normalized calling duration between customers, (2) the normalized total number of calls, sms, and mms, (3) the mean of the previous two normalized weights. the normalization process varies according to the algorithm used to extract the features as we see in the formulas of these algorithms. based on the directed graphs, we use pagerank [cit], sender rank [cit] algorithms to produce two features for each graph."
in this section we use description logic and commentary to describe how each ontology re-engineering challenge is overcome by using the basic patterns of the seshat ontology. in the following description logic we define ⨄ as the disjoint union operator where ⨄ ≡ ⨆ where ⨅ ⊑ ⊥.
"a key feature of seshat is that many uncertain facts must be recorded (challenge 4, §3). we deal with this through the intermediate qualification node in a seshat variable value. from this we define four properties: definitevalue, valuesfrom, maxvalue and minvalue. this enables a given variable to have a single value, a bag or a range:"
"r we show that in order to maximize the throughput, it is needed to use a rate per round that is smaller than the rate required to meet the outage constraint ."
"in order to model the additional context required by the qualified nature of a seshat variable, each is modelled as an owl class and a property pointing from the appropriate seshat entity to that class (challenge 2, §3). in order to keep the data model compact a large number of data pattern upper classes are defined for each variable. by exploiting multiple inheritance and owl dl's complete class definitions it is possible to overload the class definition to provide a toolbox of assertions which can be automatically classified and constrained by an appropriate owl reasoner (challenge 8, §3). each value type associated with a variable is either an xsd datatype or a custom owl class definition, often with a declared set of allowed values. at the variable definition level in the seshat ontology it is possible to associate a unit of measure with data values."
"our ambition for the seshat ontology goes beyond constraining, structuring and classifying the uncertain and sparse (although voluminous) historical time series data that forms the basis of the seshat: global history databank. in future work we will enrich the knowledge model by adding semantic relationships between seshat time-series variables to support domain knowledge-based quality assurance. this will enable, for example, the identification of inconsistent statements about a historical society's military metal technology and the metals used for agricultural tools. the current ontology reflects the modelling foci in the original seshat codebook and several areas would benefit from generalization or extension. two high priority areas are (1) the creation of richer models of the politico-geographical relationships between historical societies as this will add greater flexibility to the model and (2) adding support for inferred variable values in addition to collected values as this will reduce data collection effort and improve consistency. similarly the ontology will be extended for publication as linked data. for example, creating interlinks between seshat and the web of data or mapping seshat to common linked data vocabularies like geosparql to make it more easily consumed."
then we generalized an intervalvalue to be either an interval or an uncertaininterval which is defined as the disjoint union of the three types of temporal uncertainty:
"we also investigate the tradeoff between the energy efficiency η ee and the throughput c cc,m of harq-cc. our study reveals that the energy efficiency and the throughput cannot be maximized jointly, which shows that there is a tradeoff between these two quantities. this tradeoff is investigated in more detail in section viii."
again it is necessary to have a parallel definition of an uncertaintemporaldatavariable for variables that refer directly to xsd:datatypes instead of owl classes. these parallel definitions are all available in the online version of the seshat ontology.
"in this section, the analytical expressions for the average number of transmissions e(t r ) and the throughput c cc,m of harq-cc are numerically evaluated and illustrated for the case of perfect feedback channels. the impact of using an optimal rate per round r 1 on the throughput c cc,m is highlighted and discussed."
"the contributions of this paper are: an identification of challenges for converting social science codebooks to rdf, a description of the seshat ontology, new ontology design patterns for uncertainty and temporal scoping, a case study of the seshat ontology deployed in a data curation system and finally the lessons learned."
"the databank's information structure comprises of a range of units of analysis, including polities, ngas (i.e. 'natural geographic areas'), cities and interest groups [cit] . these are associated with temporally-scoped variables to allow for a combination of temporal and spatial analyses. each variable currently consists of a value, typically marking a specific feature \"absent/present/unknown/uncoded\", and indicating levels of inference, uncertainty or scholarly disagreement about this feature. in addition to the values, which are used for statistical analysis, variables contain explanatory text as well as references to secondary literature. where it is not possible to code variables due to missing or incomplete source data, variables are sometimes coded by inference (for example, if it cannot be ascertained if a given feature was present for a certain time period, but it is known to be present in the time periods immediately before and after, the feature would be coded 'inferred present'). by linking descriptions of past societies to both sources and coded data amenable to statistical analysis, the databank thus combines the strengths of traditional humanistic and scientific approaches."
"with k 0 (·) being the modified bessel function of zeroth order. note that χ follows the same distribution as η. using (4), we can express the ergodic capacity of the double rayleigh channel with csi at the transmitter as follows"
the objective of this work is to investigate the performance of harq-cc with a fixed outage probability. the main contributions of this work are listed as follows.
r we compare the performance of harq-cc to a fixed time diversity scheme and illustrate the significant throughput gain achieved by the use of harq-cc.
"with γ a (·) being the incomplete gamma function [22, eq. using (16) and (24), the -outage capacity c m for the case of double rayleigh channel is approximated as"
"r we compare the performance of harq-cc and harq-ir over double rayleigh channels. we show that harq-ir has a larger throughput, but harq-cc is simpler to implement, has a lower decoding complexity, and requires less memory resources."
"where γ −1 a (·) is the inverse of the incomplete gamma function γ a (·). the throughput c cc,m of harq-cc over rayleigh channels can be derived using (18) and (21) as"
"in fact due to owl's inability to create properties that have a range of both datatypes and objects it is necessary for us to create 4 additional properties named definitedatavalue, datavaluesfrom, maxdatavalue and mindatavalue and parallel class definitions (definitedatavalue etc.) to the above to allow variables to have data or object properties. the base range for data values is rdfs:literal rather than owl:thing."
"in this section, we first explain the harq-cc scheme and how it operates. then, we derive the a generic expression of the throughput of harq-cc without assuming a specific distribution of the fading. afterwards, we conduct the analysis of the throughput of harq-cc in the case of single rayleigh channel and double rayleigh channel."
"proof: see appendix c. so far, we have provided expressions for the average waiting time w and the sojourn time t so j for the case of perfect feedback channels. for the case of imperfect feedback channels, the average waiting time is denoted by w i, while the sojourn time is referred to as t so j,i . the average waiting time w i is expressed"
"this gives a flexible and compact notation (challenge 8, §3) for defining certain or uncertain temporal scopes (challenge 6, §3). we currently use gregorian dates, which we project back in time using the common interpretation of iso 8601 that allows for greater than 4 digit dates if preceded by a minus sign (challenge 13, §3)."
"the exercise of re-engineering the seshat codebook into an owl dl ontology has provided us with valuable experiences in the areas of social science codebook translation, data uplift to rdf, owl modelling and linked data publishing. each of these is summarized in table 1 and further discussed below. the overwhelming experience of developing the seshat ontology from the wikibased codebook is that taking a semantic web approach will add a lot of value. however given the emphasis on fixing the data quality issue in the wiki it has proved necessary to move to owl for the ontology rather than using a linked data/rdfs approach. this is because the demands of data validation and the imprecision of what gómez-pérez terms \"frankenstein\" linked data ontologies were ultimately incompatible. in general the process has helped the domain experts too as they have had to clarify and make explicit the semantics embedded in the codebook. the biggest hurdles in terms of owl modelling have been the lack of support for a property top that spans both object and datatypes. this has created a doubling-up of the data patterns required. in terms of the future, by moving to a natively rdf-based system it is hoped to be able to automate the exploitation of the vast quantity of structured data produced by the semantic web community and of course this would not be possible in a manual approach based on the wiki without a lot of brittle, custom development."
"where e(t s,i ) refers to the average number of successful transmissions with imperfect feedback. the expression of e(t s,i ) can be derived as"
"we can observe from fig. 14 that the average waiting time w i drops as the feedback error probability fb increases. fig. 15 depicts the sojourn time t so j,i for different values of the feedback error probability fb . the value of m has been set to 15, the rate r 1 per harq round has a constant value with respect to the snr. from fig. 15, it can be observed that as the feedback error probability fb increases the sojourn time t so j,i increases. note that the sojourn time represents the required time for successful packet delivery. as fb increases e(t s,i ) increases and consequently t so j,i increases."
"r we analyze the performance of harq-cc over both rayleigh and double rayleigh channels and provide analytical expressions for the throughput, the -outage capacity and the average number of transmissions of harq-cc."
"we define a fixed time diversity scheme as a communication scheme that retransmits the same packet over m rounds without the use of acknowledgment indicating the success/failure of a packet transmission. subsequently, this system is non-adaptive and has a fixed time diversity equal to m . the throughput of the fixed time diversity scheme is obtained as"
"in this section, we analyze the performance of harq-cc over double rayleigh channels with imperfect feedback channels. in this realistic case of imperfect acknowledgements, the received ack/nack does not always match the transmitted acknowledgement. we recall that an ack sent from the receiver to the transmitter indicates that the packet has been received correctly at the receiver. once an ack message is received, the transmitter moves on to the next data packet. a nack message indicate that the packet has not been successfully received. when the transmitter receives a nack, it sends the same packet in the next transmission rounds until it receives an ack or maximum number of retransmissions m is reached."
e(t s ) denotes the average number of transmissions required to successfully transmit a data packet. it can be shown that e(t s ) can be expressed in terms of e(t r ) as
the flexibility of wiki-based collection means that the data collected needed extensive cleanup before analysis. the ontology must eliminate this workload. 4. every historical fact (seshat variable value) recorded was potentially subject to uncertainty. the historical and archeological record often does not permit definite statements of the sort normally recorded by rdf triples. 5. each seshat variable assertion is temporally scoped. this is because historical facts are typically only true for a certain period of time. 6. each temporal scoping was potentially subject to uncertainty. many historical dates are unknown or only have known ranges of values.
"note that the optimization problem can only be solved numerically. the optimal value of α, which maximizes the throughput, will be denoted by α * . from our investigation, we notice that the value of α * varies depending on the maximum number of retransmissions m and the snr value."
"generic expressions are derived for the -outage capacity, the average number of transmissions, and the throughput of harq-cc, which are valid for any fading channel. afterwards, analytical expressions of the throughput of harq-cc over rayleigh and double rayleigh channels are obtained. we formulated the throughput optimization problem and determined the optimal rate per round such that the throughput of harq-cc is maximized. moreover, we investigated the delay experienced by poisson arriving packets and derived analytical expressions for the average waiting time and the average sojourn time in the buffer when using harq-cc. we considered in our analysis perfect and imperfect feedback channels."
"there have been many initiatives that tackle the challenge of representing historical data using semantic web technology. one important standard is cidoc crm [cit] published by iso. it has the broad remit of defining an ontology for cultural heritage information. in contrast to seshat, its primary role is to serve as a basis for mediation between local representations of cultural heritage resources such as museum collections. hence the term definitions and subsumption hierarchy are incomplete, there is no full grounding of datatypes, for example as xsd:datatypes but instead the lowest level is abstract types such as string. the rdfs-based ontology definition the standard includes is not the primary reference but a derived one. nonetheless the fp7 ariadne infrastructure project 10 has made progress with using it as a basis for linked data publication and interworking between collections. there is great potential for future collaboration with the seshat consortium in terms of data sharing."
"here we describe re-engineering an owl ontology from the structured natural language codebook (thesaurus) developed by the international seshat: global history databank initiative 1 [cit] . this evolving codebook consists of approximately 1500 variables used to study human cultural evolution at a global scale from the earliest societies to the modern day. each variable forms a time series and represents a single fact about a human society such as identifying the capital city, the capital's population or the presence of infrastructure such as grain storage sites. the variables are groupedmeasures of social complexity, warfare, ritual, agriculture, economy and so on. however the historical and archaeological record is incomplete, uncertain and disagreed upon by experts. all these aspects, along with annotations need to be recorded. an example variable definition in the codebook is: \"polity territory in squared kilometers\". an instance of this variable, showing uncertainty and temporal scoping of values is \"polity territory 5,300,000: 120bce-75bce; 6,100,000:75bce-30ce \"."
"in our analysis, since there is no closed-form expression for the cdf accuracy for different values of the snr and m . moreover, as the value of m increases the gamma approximation becomes more accurate."
"close collaboration with the domain experts that developed the codebook was necessary. several workshops have been held to understand their modelling concerns and describe our approach. developing a common understanding and hence appropriate model of data unreliability and uncertainty was the most conceptually challenging topic. three separate sources of uncertainty were identified: (1) within the codebook there was a syntax defined for variable bags of values or ranges (2) some apparently boolean variables were assigned enumerated values of \"uncoded, present, inferred present, absent, inferred absent, unknown\", and (3) the codebook syntax allowed multiple experts to disagree on a value. it was discovered that the use of \"inferred\" and \"uncoded/unknown\" tags in the dataset instances went wider than the variable definitions of the codebook and hence these represented generic patterns that needed to be available for all variables, not just those specified as an enum. modelling of values, bags and ranges was straightforward ( §4.3). the concept of an \"inferred\" value was added as an attribute for any value to indicate a human researcher had gone beyond the direct evidence to infer a value. both unknown and uncoded were collapsed into one concept that of epistemic incompleteness -a statement of the limits of human knowledge about the past, given the expertise of the person asserting it (in the seshat wiki a research assistant would put uncoded and an expert unknown but our prov logs could distinguish these cases)."
"in addition to data validation and quality assurance, a key use of the ontology within seshat is the generation of customised, dataset-specific, high usability user interfaces for data entry, import, interlinking, validation and domain expert-based curation. this requires the development of form generation tools for presenting ontology elements and widgets that streamline data entry and constrain the entered data to be syntactically and semantically correct. as this form generation technology develops it may produce new design patterns for the structure of the seshat ontology."
r we explore the performance of harq-cc over rayleigh and double rayleigh channels. we show that the severity of fading in the case of double rayleigh channels leads to a significant degradation in the throughput compared to classical rayleigh channels.
"r we derive analytical expressions for the pdf of the instantaneous signal-to-noise ratio (snr), the outage probability, and the ergodic capacity of double rayleigh channels."
"in harq-cc, the transmitter sends a data packet and waits for the receiver feedback. if the transmitter receives an nack, it sends the same data packet again until a maximum of m rounds is reached or a successful decoding occurs. the receiver combines all received versions of the same data packet and performs maximum likelihood decoding. as the number of harq rounds increases, the coding rate decreases. the lowest coding rate corresponds to the case where m harq rounds are utilized for a given data packet. the capacity c m in bits/symbol of harq-cc after m rounds is expressed as"
"the success of linked data has seen semantic web technology widely deployed. however in many domains such as social sciences, despite a strong tradition of quantitative research, linked data has made little headway. this stems partially from a lack of social sciences research ict infrastructure but also from the challenges of describing human systems with all their uncertainties and disagreements in formal models."
the average waiting time w for a data packet is the time spent by the packet in the buffer of the transmitter starting from the arrival of the packet until the start of the first harq round associated with that packet. the average waiting time w is determined using the pollaczek-khinchin equation [cit]
"the paper structure is: §2 background on seshat, §3 ontology re-engineering challenges, §4 the seshat ontology and design patterns §5 deployment of the ontology in the rdf-based data collection infrastructure, §6 lessons learned for social sciences ontology development, §7 surveys related work and §8 is conclusions & future work."
"this work can serve as a framework for designing and optimizing communication systems with double rayleigh fading. there are several communication scenarios for which the underlying fading follows a double rayleigh distribution. these scenarios include urban vehicle-to-vehicle communication systems, amplify-and-forward relaying, as well as keyhole channels."
"the ontology has been developed at trinity over the last 18 months. no formal ontology engineering process has been followed exactly. we used an iterative development model where the domain was explored in group sessions and requirements established. then individual knowledge engineers worked on surveying the literature and generating solutions for specific aspects of the model. then new versions of the combined model were developed. then hand-coding of instance data was done to evaluate the consequences of designs. the ontology was primarily written in turtle in a syntax-highlighting text editor. using protégé for editing has several drawbacksturtle comments on development are silently dropped, the import of a file often reduces properties to annotations if protégé cannot understand them, additional meta-data and comments were generated. rdf validation has been periodically performed with the rdf2rdf 8 command line tool. more recently the ontology has been validated by the dacura quality service [cit], a custom owl/rdfs reasoner that can check an ontology for a wider range of logical, typographical and syntactic errors. in addition the ontology has been used for testing the dacura data curation tools being developed for seshat. the ontology was split into an upper part containing basic patterns and a lower part containing the ontology of the seshat codebook based on those patterns."
"in this paper, we investigated the performance of harq-cc over double rayleigh channels from an information theoretic perspective. in our analysis, the transmission rate is adjusted to the average snr such that a fixed target outage probability is not exceeded which ensures link reliability. we provided analytical expressions characterizing the statistical properties of double rayleigh channels, such as the pdf of the instantaneous snr, the outage probability, and the ergodic capacity."
"is the inverse of the incomplete gamma function γ a (·). the throughput c cc,m of harq-cc over double rayleigh channels can be derived using (18) and (24) as"
"1. the codebook was specified in semi-formal structured natural language designed for human consumption. while a common approach in social sciences it is not often studied in ontology engineering, e.g the methodology for ontology reengineering from non-ontological resources [cit] doesn't consider it. 2. the ontology must not depend on custom reasoning or triple-stores. rather than moving beyond rdf triples to specify qualified relations or temporal scoping it must be possible to use standard, state of the art, scalable triple-stores. 3. the ontology must be expressive enough to support data quality validation."
"in the initial stages of the project, the database was implemented in a wiki, however, as the number of coded variables has been rapidly growing, it was decided to move the seshat data to an rdf-based triplestore. based on the dacura data curation platform, this will facilitate all steps of the seshat research process, from data gathering, validation, storage, querying and exporting down to analysis and visualization. the purpose of creating the seshat ontology was not simply to translate or uplift an existing dataset to rdf for publication as linked data. instead we wished to use the ontology at the heart of a set of rdf-based tools that would produce a step change in the data collection and curation capabilities of the seshat consortium by improving data quality, productivity and agility (fig 1) . the primary goal of the formal owl model is to enable data quality management as even uncertain facts can be omitted, mis-typed, duplicated, inconsistent and so on. this creates a huge data cleaning overhead before statistical processing in the pre-owl system. later we hope to extend the utility of dl reasoning to support inference, fact reuse and other advanced features."
"two main references were used as a basis for representing time -the w3c draft time ontology in owl (henceforth owltime) and the w3c prov-o ontology. owltime is attractive since it makes explicit the granularity of representation, for example in cases where the historical record only records a year but no month or day, whereas prov-o uses a simpler structure for time whereby activities are directly linked to an xsd:datetime value using the prov:hasbeginning and prov:hasend properties. in contrast owltime uses 4 intermediate nodes for each time value in an interval. neither specification has any support for uncertainty in time assertions or nongregorian calendars (although cox [cit] has recently extended owltime to handle this)."
"note that the average number of transmissions e(t r ) is independent of the snr. in fact, the transmission rate c m per harq round depends on the snr. as the snr increases, the transmission rate increases as well. in contrast to systems that have a fixed transmission rate, we have a variable rate, which is adjusted to the snr such that a fixed target outage probability is achieved. this setting is in line with contemporary communication systems, where a certain quality of service should be guaranteed to users, independently of the channel conditions. for a given value of, the average number of transmissions e(t r ) is only a function of the maximum number of rounds m . hence, for fixed values of m and, the average number of transmissions e(t r ) is constant for all snr values."
"r we analyze the performance of harq-cc with imperfect feedback taking into account acknowledgment feedback errors. in this realistic setup, we characterize the average number of transmissions and the throughput."
"we have then extended the definition of an instantvalue to be either an instant or uncertaininstant, which is defined as a thing having two or more assertions of the atdatetime property:"
"dbpedia [cit] of course contains many historical facts that are of interest to seshat and it is hoped that by leveraging the work already done there it will be possible to quickly import candidate data for seshat, to be then curated by the seshat research assistants and domain experts. nontheless the current dbpedia data is not in a format suitable for processing as time series and does not comply with the conceptual models underlying the seshat codebook so mapping techniques will have to be employed. through the aligned project we are collaborating with the aksw group at the university of leipzig and it is planned to establish a virtuous circle whereby dbpedia extracts crowd-sourced facts from wikipedia, seshat uses those facts as input to their historical time-series, the seshat team curates and refines the facts and publishes them as high quality linked data which in turn is available to dbpedia+, the new multisource, improved quality version of dbpedia in development by the dbpedia community. this integration will be trialed in year 3 [cit] ."
"the seshat codebook is primarily aimed at collecting geo-temporally scoped time series variable values describing two main units of analysis -the polity, representing an independent historical human culture or society and the natural geographical region (nga) which is a unit of data collection or analysis defined spatially by a polygon drawn on a map. in the rdf-based approach we use three named graphs to represent the dataset: v, the data value graph which is described by the seshat ontology; a, the annotation graph (based on open annotation) where textual annotations of data values are held and p, the provenance graph (challenge 12, §3) where w3c prov statements are recorded that describe the annotation and variable value lifecycles as they travel through the data curation system ( fig. 1 ). the seshat ontology extends the set of units of analysis by creating a hierarchical structure of entity classes as seen in fig.1 . each of these entities has a set of seshat variables associated with it. each variable value for an entity is associated with geographical and temporal scoping information."
"the average sojourn time t so j in the buffer is the average time elapsed from the packet arrival until its successful reception at the receiver. the average sojourn time is written as the sum of two terms: (i) the average waiting time w and (ii) the time for successfully transmitting a data packet [cit], i.e.,"
finally there are a wide range of historical time series data collection efforts in the social sciences that are not rdf-based or publishing linked data. most of these have much more limited scope than seshat. for example sabloff's datasets describing the limited geographic region of mongolia throughout time [cit] or the database of religious history [cit] that has similar geo-temporal scope to seshat but deals only with religion rather than all aspects of human cultural evolution.
"our approach, based on triple efficiency concerns, has been to re-use the expressive owltime:datetimedescription directly linked to a qualified variable object via the atdatetime, hasend and hasbeginning properties in the prov-o pattern. i.e."
"the final pattern needed is the ability to express temporal constraints as part of the qualification of a seshat variable (challenge 5, §3). to do this we build upon our uncertain representation of time above to add scoping properties to the variable qualification class. hence we first define the temporalscoping as the disjoint union of the temporal types:"
to illustrate the use of the previous sections we define here an example seshat datatype variable based on xsd:datetime. in order to enable quality analysis and constraint checking we need to make this as strongly typed as possible. this means that all our data accessor properties must be restricted to using a single datatype (xsd:datetime in this example) and the base type of uncertaintemporalvariable. we do this by declaring the 4 restriction classes (one for each data accessor property) and the intersection of these with our base type:
"r we analyze the delay performance of harq-cc and derive analytical expressions for the average waiting time for the packet (time elapsed between the first packet transmission and the packet arrival) and the average sojourn time in the buffer for perfect and imperfect feedback. the remainder of the paper is organized as follows. section ii investigates some characteristic quantities of double rayleigh channels. section iii conducts generic analysis of harq-cc before deriving analytical expressions for the -outage capacity, the average number of transmissions, and the throughput over rayleigh and double rayleigh channels. in section iv, we formulate the throughput optimization problem of harq-cc and determine the optimal rate per harq round that maximizes the throughput. in section v, the performance of the system with an imperfect feedback channel is explored, while the delay model is studied in section vi, and the energy efficiency is investigated in section vii. the derived analytical expressions are numerically evaluated, illustrated, and interpreted in section viii. finally, section ix provides some concluding remarks."
"a. background d ouble rayleigh fading channels represent an important category of communication channels. the fading distribution is modelled as double rayleigh if the transmitted signal undergoes a cascaded rayleigh fading. the study of double rayleigh fading channels has gained a lot of interest due to its applicability to various communication scenarios. these communication scenarios include vehicle-to-vehicle (v2v) channels, amplify-and-forward relaying, and keyhole channels."
"there are also a large number of other curated rdf datasets describing historical locations and facts such as pleiades 11 that focuses on ancient names, places and locations. nonetheless these datasets are typically based on controlled vocabularies rather than formal semantic data models and rdf is provided as a dump that transforms the internal representation. this gap presents an opportunity for seshat as a provider of high quality native linked data with strong consistency assurances. once again it is hoped that seshat will work with these other dataset publishers in the future."
"one special type of value in the seshat codebook is one that is inferred from the historical record by the person entering the data, rather than by reference to a historical source. this is modelled as a new type but it is always a form of definite value:"
"where c m is the -outage capacity. the termp denotes the average power consumption, which is expressed as the product of two terms: (i) the transmission power p per harq round and (ii) the average number of transmissions e(t r ), i.e.,"
"our analysis has revealed that harq-cc allows improving the achievable communication rate compared to fixed time diversity schemes. to maximize the throughput of harq-cc, the rate per harq round should be less than the rate required to meet the outage constraint. our comparison of the performance of harq-cc over rayleigh and double rayleigh channels shows that double rayleigh channels have a higher severity of fading and result in a larger degradation of the throughput. our analysis demonstrated that harq-ir achieves a larger throughput compared to harq-cc, while harq-cc is simpler to implement, has a lower decoding complexity, and requires less memory resources. in the case of imperfect feedback channels, the increase of the feedback error probability results in the degradation of the system throughput due to the increase of packet loss. our analysis revealed that it is not possible to jointly maximize the energy and the throughput of harq-cc. therefore, we have explored the tradeoff between the energy efficiency and the throughput of this scheme."
"current data collection in seshat uses a wiki based on the natural language codebook. this is unsustainable as data quality assurance is impossible and better tools are required to manage the collection, curation and analysis of the dataset. in addition it is desired to publish the dataset as linked data to enable other scholars to build upon the seshat work. the new tools will be rdf-based using the dacura data curation platform developed at trinity college dublin 2 [cit] project 3 . this paper investigates the research question: what is a suitable structure in rdf to represent the seshat codebook that will support data quality assurance? our technical approach is to develop an owl ontology describing the codebook based on a set of design patterns for seshat variables that capture the requirements for variable uncertainty, temporal scoping, annotations and provenance while producing a compact, strongly typed data model that is suitable for quality assurance in a very large dataset."
"however in addition to these types of uncertainty it is important for seshat data collectors to be able to express the presence of epistemic incompleteness, i.e. that a search has been performed and that, to the extent of the current author's knowledge, the data value is not present in the historical record. in this case we set the variable to unknownvalue which carries these semantics and record the author in the prov graph. this leads to the full definition of an uncertainvariable in seshat:"
"the study of past human societies is currently impeded by the fact that existing historical and archaeological data is distributed over a vast and disparate array of databases, archives, publications, and the notes and minds of individual scholars. the scope and diversity of accumulated knowledge makes it impossible for individual scholars, or even small teams, to engage with the entirety of this data. the aim of 'seshat: the global history databank' is therefore to systematically organize this knowledge and make it accessible for empirical analysis, by compiling a vast repository of structured data on theoretically relevant variables from the past 10.000 years of human history [cit] . in this way, it becomes possible to test rival hypotheses and predictions concerning the 'big questions' of the human past, for example the evolution of social complexity 4, the deep roots of technologically advanced areas 5, or the role axial age religions play in explaining social inequality 6 . seshat data is currently manually entered either by domain experts (historians, archaeologists and anthropologists), or by research assistants whose work is subsequently reviewed and validated by domain experts. the aim is to move to quality assured data collection facilitated by customized software that can automatically import data from existing web resources such as dbpedia. a central requirement for the seshat information architecture is a flexible and agile system that allows for the continuous development of the codebook (which structures the data), the adaptation of variables to different research interests and theoretical approaches, and the participation of a large number of additional researchers and teams."
"where e(t r,i ) is the average number of transmissions with imperfect feedback and its expression is provided in (31) . the term e(t 2 r,i ) stands for the second-order moment of the number of transmissions with imperfect feedback, which can be expressed as"
"for analyzing learned representations, we performed unsupervised k-means clustering. afterwards, the feature representations of all training views are assigned to the closest cluster center. since k-means randomly permutes cluster identities in each run, we always identify the permutation of cluster names with the highest overlap to ground truth shape categorization."
"there has been a lot of work in software engineering on the idea of formal specifications of code [cit] ]. this body of work shares with us the idea of having some representation of the purpose of code at a higher, declarative level, that is independent from the code itself. it also has the ambition to provide algorithmic help to the programmer in assuring that the code meets the specification, or at least drawing the programmer's attention to discrepancies between the two representations. but formal specifications are expressed in a mathematical language that most programmers find difficult to write. such languages are also entirely unsuitable for beginning programmers, which are our target user community here."
"code search systems can be distinguished by how programmers can query them. [cit] ] includes a good survey of code search techniques, including formal specifications, type systems, design patterns, keywords, ontologies, tagging, and test cases. however, these code search systems have limited ability to reason about purposes that can be accomplished in a variety of ways, and their understanding of natural language is limited at best. procedurespace uses annotations to reason about purposes and leverages both general and domain-specific natural language background knowledge."
"after a sample task to familiarize the users with interacting with the zones interface, they were presented with a project containing many sprites, each exhibiting some read-ily observable behavior, and were instructed to duplicate the behavior of one or two sprites, using zones if they wanted. they were not given cues for how to describe that behavior so that their queries would be as natural as possible. all participants were able to successfully imitate at least one behavior with the help of reused code from zones. finally, all participants left zone searches as new annotations, validating our search-as-annotation paradigm."
"the matrices u k and v k represent each row and column of a as a vector giving its position along the k axes that represent the highest-variance dimensions of the data. the entries along the diagonal of σ weight each axis, roughly indicating its importance in describing the patterns of relationships among the items of the matrix. an important parameter for the blending technique is the layout of the data matrices, since that determines which elements overlap and thus how they can be related. cs relates code fragments to code structural features, while background knowledge relates english concepts to english features. the"
"we illustrate our suggestions with a prototype purposeoriented code reuse system. its frontend, called zones, demonstrates an integrated interface for connecting natural language with scratch code fragments to make comments that help programmers find and share code. the procedurespace backend demonstrates important concepts in how to relate ambiguous and unambiguous representations as it reasons jointly over static code analysis, zones annotations, and background knowledge to find relationships between code and the words people use to describe what it does."
"\"searching by goal is a really different way of programming,\" said one participant in our preliminary user study. we sought to understand whether the zones interface (both concept and implementation) helps programmers make and use connections between natural language and programming language. all participants in our two-task user study successfully used the zones interface to find code that they could use in their project, and annotated both new and existing code in a variety of ways. we were surprised by the number of different ways that people learned from their interactions with zones."
logical reasoning systems used by formal specification tools have the advantage that they provide mathematical assurance of the validity of implementations. the downside is that their reasoning is not applicable to many practical programming problems. the approximate inference used by our system trades guarantees of correctness for the ability to make plausible inferences efficiently across a wide variety of programming situations.
"when people choose entirely different words to describe their goals (and in user studies we found they often do), most search systems would be left with no relevant results. but procedurespace incorporates background knowledge about how words relate."
"natural language has often been seen as desirable as a highlevel specification or programming language because it is a natural medium for communicating goals and ideas with a human collaborator. various attempts have been made to interpret natural language as computer instructions directly, from cobol to sql to several modern attempts, including pegasus [knöll [cit] . however, since programs must execute unambiguously, many previous attempts at natural language programming have required the use of unnaturally precise wording. natural language representations of program present many challenges, but we think that managing ambiguity is a core challenge that has not yet received sufficient attention."
"each zone is an annotation that links a code fragment with a statement of purpose. these annotations are collected into a matrix: ad(purpose, fragment) counts the number of times that fragment was annotated with purpose. the purposes are actually stored as (purpose, purpose) tuples to distinguish complete annotations from words that will later be extracted from them. the initial annotations were entered by one of the authors. in various types of interactions with users, including the user study, other annotations were contributed, for a total of 100 annotations at the time of these experiments."
"once we have used blending to construct procedurespace, the search tasks required to power the zones interface become straightforward vector operations. each entity is a vector in the k-dimensional vector space: the u matrix gives the position of each english word, purpose phrase, code feature; the v matrix locates code fragments and english features. since that all entities are in the same vector space, search operations can be expressed as finding vectors with high inner products. to find the vector p of a purpose statement composed of english words w i, you simply sum the corresponding vectors:"
"the matrix ad only accounts for purpose statements that are equivalent with respect to string equality. to allow inexact matches of annotations and search queries, we construct a matrix aw that relates code fragments with words and phrases extracted from their purpose statements using standard natural language processing techniques (e.g., lemmatization and stopword removal). since similar annotations yield similar words and phrases, they will be counted as more associated."
"in this paper, we present one initial example of a system that can work with informal representations of software. our interface, called zones, uses informal statements of the general purpose of code fragments to help programmers find and share reusable code. a zone is an active association between a code fragment and a brief natural language statement of its purpose-\"what's this for?\"-not unlike a comment. programmers can describe the purpose however they think about it; the statement need not be precise, comprehensive, or even grammatical. by only giving one line, the zones interface encourages purpose statements to be short. figure 1 shows an example where a programmer has used a zone to annotate a short fragment of scratch code. other purpose statements from our users, who were developing video games, include \"stay on path\" and \"bounce ball around room.\""
"we were surprised by the number of different ways that people learned from their interactions with zones. one novice programmer corrected a flaw in his understanding of code while studying the search results for the annotation he was about to give it. a more experienced participant reported that the zones interface encouraged her to think from a higher-level perspective. frequently, participants appreciated learning something from seeing another person's code, even if their goals were different or their understanding incomplete."
"we have presented a system for unsupervised learning of a visual feature representation that clusters views of objects with similar 3d shape. the system learns invariance to pose variations of the objects and color variations of objects within categories. the shape information is autonomously derived from the movement fig. 1 . cluster center distances. representations of single object views cluster in the learned feature space. each subplot depicts normalized pairwise distances between the cluster centers of any given object in the learned features space. left: without any inter-object similarities learned (i.e., completely unsupervised), cluster center distances are undetermined and change between simulations. center: after additional supervised presentation of 100 view pairs between objects of similar shape, features of objects with similar shape cluster but keep a similar distance to unrelated shape clusters. right: based on the similarity of their physical movement trajectories (\"self-supervised\"), most features of objects with similar shape cluster as in the supervised case."
"broadly speaking, procedurespace aims to relate incomplete knowledge about two types of entities: purpose statements and code fragments. purpose statements are handled with simple natural language processing techniques; code fragments are represented by characteristics from static analysis. the knowledge about purpose statements is incomplete both because they are themselves ambiguous and because the system's understanding of natural language is limited. and knowledge about code fragments is incomplete in that it is generally not known what characteristics of the code are relevant to accomplishing the goal and what parts are merely implementation details. however, associations between code fragments and purpose statements disambiguate each other: procedurespace, in effect, learns about purpose statements from the code they describe, and learns about code by studying how people describe its purpose. procedurespace additionally uses semantic background knowledge about english words and phrases to help understand the natural language statements. figure 4 shows an overview of procedurespace's reasoning: it understands words like \"follow\" and \"chase\" by relating them to commonsense background knowledge (such as \"follow is a kind of movement\"), examples of code that people have said causes something to chase something else, and characteristics of the structure of that code. this paper does not seek to establish that these particular kinds of data are necessary and sufficient for a natural language code reuse system, but rather to show how to relate these disparate kinds of data."
"while many advanced techniques have been developed for static source code analysis, procedurespace uses a deliberately simplified approach, focusing instead on combining static code analysis with natural language. the result will be akin to a quick glance at the code, rather than an in-depth study. the basic goal of the code analysis is similarity detection: two annotations might be similar if the code fragments they apply to are similar. for example, many different examples of code that handles gravity (or \"falling\") all include a movement command conditioned on touching a color in the sky (or not touching a color on the ground). in other languages, such features could also include constraints about types (e.g., \"returns an integer\") or interactions (e.g., \"uses synchronization primitives\")."
"to account for the sparsity of annotations and comments, procedurespace also extracts tokens from identifiers: names of variables and events (analogous to function or method names). token extraction additionally includings splitting underscore joined and camelcased strings. each element of the resulting word-code matrix, w c(token, fragment), counts the number of occurrences of token in fragment.."
"we suggest that programming environments should be able to help programmers work with the informal representations that are a natural part of the programming process. one particular difficulty in working with informal representations of software is that they can be ambiguous. we suggest that given today's large-scale code repositories, programming environments can, instead of shunning ambiguity, manage it by relating ambiguous statements to concrete examples."
"given the six matrices of relationships, we then combine them using blending. to apply the blending technique to a set of matrices d i, align the labels (filling in zeros for missing entries), then add the matrices together:"
"additionally, this work is related to affordance learning approaches from the developmental robotics community [cit] . these approaches also show autonomous learning of affordances but use sophisticated robotic actuators, whereas we have shown our approach only for simulations. however, these approaches tend to employ much simpler visual features (e.g., nearest neighbor classification in the pixel space), whereas our approach focuses on the learning of visual feature representations with invariances to strongly changing visual stimuli."
"one kind of knowledge is knowledge specific to the target domain. for scratch, many projects are games, so helpful domain-specific knowledge includes facts such as \"arrow keys are used for moving\" and \"moving changes position.\" such knowledge would enable us to relate an annotation about \"arrow keys\" with an annotation about \"position,\" for example. the matrix ds encodes a small, manually entered knowledgebase about simple games."
"our aim is not to assure the code does what the specifications say, but to give the programmer access to a body of alternative implementations of the specifications, and also the novel capability of reasoning backward from the code to specifications. by allowing programmers to express what may indeed be informal specifications in natural language, we hope to improve the accessibility of specifications as a programming methodology."
"users of our system searched for a variety of goals and expressed them in a variety of ways. figure 9 shows selected results for some queries that users performed. the first search, \"gravity,\" exactly matched the annotation of a code fragment, so it was returned as a search result. this result illustrates that indirect reasoning through code structure and natural language background knowledge rarely disturbs exact matches. for \"follow player,\" neither of the two results were exact annotation matches. the example code from figure 6 was annotated \"follow\", but the first result is one that matches both those code features and the word \"follow.\" the second match is very interesting because it inexactly matches at least two different kinds of data. the only common code structure is the presence of pointtowards:, which evidently procedurespace found to be associated with the behavior of following. but many code fragments contain pointtowards:; evidently this one was chosen because it also contained the word chase. using a combination of common annotation data and background knowledge, procedurespace related \"follow\" (the query word) and \"chase,\" and used this relationship to find a code fragment that is very different than what was annotated but nonetheless relevant."
"one main reason we chose to use scratch for these initial experiments is the ready availability of a large quantity of potentially reusable code fragments. scratch projects all have the same general structure, and project context has a limited effect on the behavior of individual lines of code, so scratch code tends to be more reusable a priori, despite the general lack of software engineering discipline in the programmer community. also, while the language lacks procedures or functions in the traditional sense, the event handler design makes concurrent modularity idiomatic and greatly simplifies task coordination, which tends to make a large amount of code reusable without significant modification. the scratch website [monroy-hernández [cit] hosts over 300,000 projects, many already reusing code from other projects, all shared under a free software license. a qualityfiltered sample of 6376 projects was used as the corpus for these experiments. many of these projects are simple video games, so that domain will figure strongly in our examples."
"while informal software representations are applicable to a broad variety of programming scenarios, we focus in this paper on novice programmers. our environment for these experiments is scratch, a graphical programming language and development environment designed for use primarily by children and teens, ages 8 to 16 [cit] ]. scratch programming language components are represented by blocks that fit together like puzzle pieces to form expressions; stacking code blocks vertically causes them to execute in sequence. scratch code is mostly comprised of concrete instructions that cause sprites to move around a stage, change their appearance, play sounds, or manipulate a pen, in response to various events, including user input and named messages from other sprites. the language includes control flow statements, boolean and mathematical expressions, and sprite-or project-scoped variables. though the detailed techniques of this paper are somewhat tailored to scratch, the general approach should be adaptable to other languages."
"these challenges preclude a straightforward or even multistage information retrieval approach. and since we allow purpose statements to be informal and ambiguous, formal reasoning techniques are particularly ill-suited. procedurespace approaches this challenge with an unconventional technique: it uses the sparse and imprecise relationships between several different kinds of information to place them all in the same vector space (hence the name), where search queries become simple vector operations. this technique, called blending [cit], is similar to standard information retrieval techniques, but uniquely illustrates a simple way of using multiple types of imprecise data together. the result is a representation that unifies syntactic knowledge about programs with semantic knowledge about goals."
"in extracting structural features, procedurespace treats the code as a simplified syntax tree. for each code fragment, procedurespace extracts various types of simple structural within these feature types, it is not necessary to enumerate all possible features beforehand. rather, for each feature type, an extraction routine generates all the features of its type that apply to a particular code fragment."
"the zones interface presents a unique set of requirements for its backend, called procedurespace. purpose statements could be treated as code search queries, but they would include abstract characteristics that would not match a keyword in an identifier, type, or comment (if present). and though we assume that some code fragments in the corpus have been annotated with one or more purpose statements, those annotations often differ in word choice and level of detail from the purpose statement that is queried for, and few code fragments are annotated. finally, zones searches may also go in reverse: from code to possible purpose statements."
"the process of authoring a program can be described as going from ambiguous and informal representations about purpose permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. and approach (mostly contained in the minds of programmers) to unambiguous, highly structured representations of instructions (mostly contained in computers). code reading involves the reverse process: relating structured representations to general ideas about what the program does and how. current development environments provide great assistance when working with formal representations, through intelligent completion, refactoring, and debuggers, for example. but relating formal representations that computers can use with informal representations that programmers can use remains, for the most part, the sole responsibility of the programmer."
"consider the code fragment in figure 6, which makes a sprite chase or follow another sprite. table 1 shows examples of the code structural features extracted for the example code. we construct a matrix cs that relates code fragments to the structural features they contain. the rows of cs are the 14145 distinct code structure features that were extracted; the columns are the 127473 analyzed code fragments. (the order of the rows and columns does not matter for this kind of analysis.) an entry cs(feature, fragment) is 1 if fragment has feature, 0 otherwise. to keep long code fragments with large numbers of features from having a disproportionate effect, we normalize all code fragments to have unit euclidean norm. figure 7 shows a sample of the final matrix cs."
"another kind of knowledge is general world knowledge, such as \"balls can bounce\" and \"stories have a beginning.\" without such knowledge, the system may be entirely unaware that an annotation of \"bounce\" may be relevant to find code for \"moving the ball.\" conceptnet [cit] provides a large database of broad intuitive world knowledge, expressed in a semantic network representation (e.g., ball\\capableof/bounce). rarely is a single conceptnet relation a critical link in connecting two concepts; rather, the broad patterns in conceptnet, such as which features typically apply to things that people desire or can do, help to indicate how english concepts are similar to each other. though many other lexical resources are available, conceptnet uniquely provides a broad coverage of goals and actions, in a plain-language form that is amenable to imprecise reasoning and usage in a user interface."
"invariant shape recognition is a hard problem because many three-dimensional objects undergo extreme appearance variations when they rotate in-depth or light conditions change. slowness learning can be used to learn this invariance: even if views of the same object are very different, they more often occur in close temporal relationship during object interaction than views of distinct objects [cit] . such models allow object identification since different views of the same object form clusters in the learned feature space and views of visually distinct objects typically cluster in distinct regions. however, objects of similar categories do not generally appear more often in close temporal relationship than objects of different categories. the relative distances of object clusters thus remain undetermined after sfa learning, and minimal noise can cause object clusters to permute their positions in different simulations. here, we extend a model for invariant object recognition such that a small autonomously generated additional input signal determines the relative positions of object clusters and thus implements a meaningful shape similarity measure. although in general the problem of deriving 3d object shape from a single view is ill-defined, humans can often already guess an object's shape from a single view and predict how it would move when it is agitated. we demonstrate here a model of this behavior for a limited object set."
"marvin minsky said, \"if you understand something in only one way, you don't understand it at all.\" we believe that when computers can work with programs both in natural language descriptions and in code, they can come closer to really understanding what we want them to do."
"we have shown that without additional self-derived training views, most object representations cluster roughly equidistantly, i.e., they are distributed as localized clusters on a hypersphere in the learned feature space. although the sfa optimization itself is deterministic and guaranteed to find the globally optimal solution, the positions of the object clusters permute in different simulations under the addition of a small amount of noise. theoretically, a single mini-sequence consisting of one pair of views between two distinct objects a and b already leads to a symmetry breaking such that the clusters of a and b have a smaller distance to each other than any other cluster pair. this sensitivity to small perturbations is an ideal basis for learning from few examples, either supervised (as shown in the central subplot of fig. 1) or autonomously (as shown in the right panel of the same figure) . practically, some intermediate visual cues from the visual hierarchy will likely bias these cluster distances in simulations, even when no temporal inter-object relationship has been experienced during training (left panel of fig. 1 ). however, as we have shown, object shape can not trivially be clustered in the pixel space. instead, object color is a more prominent cue. thus, there is a bias in the input space to cluster by color and not by shape and the fact that the resulting representations become color-invariant and shape-specific demonstrates that the input categorization bias is small and can be \"overwritten\" even with few examples. additionally, we have shown some robustness to noise in the self-derived additional shape training views, since the trajectory clustering does not perfectly coincide with the shape clusters."
"the finally, the programming environment could help the programmer document and distribute the result in a way that permits others to understand and utilize it. an informal survey of open-source code repositories suggests that a large amount of code has similar goals and subgoals, but it was not reused, due perhaps to differences in environment, constraints, or libraries, or simply being unable or unwilling to find and incorporate that other code. a development environment that can recognize the similarity of the programmer's intent to code already written, especially before the programmer has committed to a structured encoding of the task at hand, might encourage and facilitate code reuse. and by additionally capturing the process of refinement of high-level goals and maintaining links between the subgoal tree and the code, the development environment would be better equipped to help the programmer adapt that code to different scenarios or environments, even those that may come up at runtime."
search-oriented systems like codebroker and blueprint only directly benefit consumers of reusable software; users still have to publish their completed code manually. zones makes it natural to share adapted or newly written code.
"omitting either the purpose statement or the code turns annotation into search. if you provide an english statement of purpose and click the search button, the system searches for code that accomplishes that purpose (see figure 2) . alternatively, you can mark some code and then search, which means: find how other people have generally described the purpose of code like this (see figure 3) . the search results are presented as a floating sidebar. the code for selected results appears within the zone. you can then immediately try running the program with the transplanted code, make modifications first, or simply scroll through the results to get a general sense of different approaches to your task. selecting the zeroth result returns to whatever code was in the zone before, if any. a second click on the search button collapses the sidebar, leaving behind only the code and the search query, which then functions as an annotation, using exactly the language you chose when looking for the code to begin with. in this way, search queries seamlessly become annotations; though the code may require further tweaking, the annotation remains unless the programmer explicitly removes it. when the project is shared, that pair of annotation and code fragment is added to the database, either as new information or as a new explanation for existing code. or, if in fact none of the search results were relevant (likely because the corpus of annotations is still small), code built within the bounds of that annotation becomes the first example of how to do that. the next programmer to look for related code fragments or purposes will benefit from the added annotated code. it could be said that by capturing both successful and unsuccessful search interactions, zones learns how programmers describe purpose."
"where α i is a weighting factor that is adjusted to maximize the interaction between the datasets. (for these experiments the weighting factors were set manually, though [cit] ] presents an automated technique.) then compute the singular value decomposition (svd), truncating to k singular values:"
"this work is related to slowness-based invariant object recognition. while invariant object recognition with much more complex objects has been shown earlier [cit], we restrict the object complexity here to four shapes and three colors for the sake of simplicity. while it seems likely that our system can find invariant representations for visually more complex objects, ground truth shape categories are less evident and thus objective evaluation is harder. however, one shape representation could be evaluated as better than another if it facilitates a given task. such an integrated approach is an interesting subject for further research. the main difference to existing slowness-based invariant object recognition systems of our approach is the systematic integration of top-down cross-object similarity of discrete objects, specifically for learning 3d shape categories. a similar hierarchical model architecture has earlier been shown to model most known functional aspects of hippocampal spatial codes (i.e., place cells, head direction cells, spatial view cells and grid cells [cit] ). as the hippocampus is crucial as a memory hub and well-known for time-delayed replay of previous experiences [cit], we hypothesize that if replay sequences of objects with similar movement trajectories occur more often than those of different shape, i.e., out of experienced temporal context but in new task-specific context, the hippocampus could implement a mechanism similar to the one proposed here."
"perhaps the scarcity of tools for working with informal representations is in part because informal representations are ambiguous. since the end products of programming should be as unambiguous as possible, many programming researchers have been understandably averse to allowing ambiguity even in high-level specifications. however, we suggest that this aversion may be unhelpful because it tends to force programmers to prematurely commit to some particular and precise way of thinking before they can dialogue with computers about their programs [cit] ]. instead, we suggest that programming environments could permit programmers to describe the desired program in more natural and informal terms, even if the terms themselves or the relations between them are ambiguous, then assist the programmer in the task of concretizing the structure and content of the program into an unambiguous executable form."
"first we analyze the learned representations after training the hierarchical network with videos of objects dropped into the box with a tilted floor without any further shape-related training. for this purpose, we compute the average values (i.e., the cluster centers) of each object in the slowest ten components in the highest layer of the hierarchical network. the left panel of fig. 1 shows the distance matrix between all pairs of clusters after normalizing the highest distance to 1. as expected, most clusters are roughly equidistant (with the exception of the cone clusters). these distances fluctuate for repeated simulations due to the small amounts of noise injected into the hierarchy. in this representation, a view of a green cube, for example, is on average as similar to a red cube as to a red sphere. except for the distances between cones, there is no evident clustering of views from objects with same shape or color."
"given the design matrix φ, vector of target outputs y, and a suitably chosen model structure f (φ, θ), estimation of model parameters θ can be expressed as a regularized optimization problem"
"this is an enhancement of downsampling where redundancy in the resulting design matrix is eliminated by replacing the matrix with its r most significant principal components, as determined using pca [cit] . for convenience, r was fixed as m/2, but can also be chosen as a function of the explained variance. for the problems considered here, the choice of m/2 ensures that the pca"
this dataset is designed to simulate a scenario where only an unknown part of the input time series determines the target output. the ith observation of the input time series is defined as
"the proposed methodology derives from a functional learning setting in which the time-series input space is reconstructed by means of gp inference, and the unknown shape function is parametrized as a weighted sum of gaussian functions. this combination allows for a number of interesting properties, including closed-form solution (and hence efficient numerical computation procedures), enhanced interpretability through shape function analysis, easy incorporation of time-series derivative information, and the possibility of using the extracted information as input data to other machine learning methodologies."
"in order to make solving (9) tractable, a parametrization of the shape functions β (j) (t) is adopted. many possibilities exist including, e.g., algebraic and trigonometric polynomials, splines, multilayer perception neural networks, and radial basis function (rbf) expansions. here, we adopt an rbf expansion in the form of a linear combination of gaussian densities to represent β (j) (t), i.e.,"
"in industrial modeling, one of the challenges associated with \"big data\" is how to condense the information contained therein into a form that is suitable for modeling without incurring significant information loss [cit] . in this paper, we consider a specific modeling scenario frequently encountered in industrial environments, especially those involving batch production such as chemical [cit] and semiconductor manufacturing [cit], namely, where the input information for the model is conveyed in the form of time series. the presence of time-series input data can increase modeling computational costs by several orders of magnitude due to the explosion in input dimensionality (potentially hundreds or thousands of samples instead of a single value) or it may not even be possible to directly generate the design matrix required for modeling."
"with many industries investing heavily in advanced process monitoring technologies and infrastructure to support collection, integration and archiving of process data from heterogeneous sources, the future is big data. this brings both opportunities and challenges; opportunities such as eliminating costly metrology using soft sensors and optimizing maintenance scheduling using predictive maintenance models; and challenges such as dealing with the data deluge and making the best use of the available data."
"3 ). note that the values of q and r are determined by the resolution we require when optimizing the hyperparameters. in practice, this can be quite low. furthermore, good hyperparameter estimates can often be determined off-line using heuristic techniques or by conducting a pilot study, leaving only the linear parameter estimation steps with an overall complexity of o(npn"
"as a practical demonstration of safe, we consider in this section its application to a benchmark soft sensing problem from semiconductor manufacturing, namely estimating the etch rate of a plasma etch processing tool from optical emission spectrometry (oes) measurements recorded during processing [cit] ."
comparing fig. 11 (exp. 2) with fig. 10 (exp. 1) allows the impact of employing irregular/reduced sampling to be assessed. and greater than 33% with the other approaches. the results thus confirm that safe is more robust to sampling variability than the other methods evaluated.
"the ridge regression formulation can be extended to cover nonlinear regression models f without giving up the desirable convexity features of the optimization problem, by employing the so-called kernel trick [cit] to embed a nonlinear projection of φ in a reproducing kernel hilbert space (rkhs) [cit] resulting in a linear regression problem. this is achieved by expressing the ridge regression solution in dual form"
"building on the concepts introduced in section ii, the proposed safe methodology will now be presented. we begin by considering the ideal case where we have complete knowledge of the time-series functions x (j) i (t). the classical regression model can then be generalized to the functional regression paradigm by defining f as"
"the ramp dataset is synthesized to highlight the benefits of including time-series derivative information as features in the design matrix. accordingly, the input time series is generated as"
"m achine learning methodologies are applied in many industrial areas to create models of observable phenomena from representative datasets [cit] . thanks to the increasing availability of data from on-line sensing of processes in modern industries [cit], they are assuming a fundamental role in decreasing measurement costs and enhancing process quality. a typical example is provided by soft sensing technologies [cit] that have proliferated, e.g., in biotechnology [cit] and manufacturing [cit] under a range of different names, including virtual sensing, virtual metrology, statistical sensing, and inferential estimation. soft sensors are statistical models that provide an estimate of quantities (outputs y) that may be unmeasurable or costly/time-consuming to measure based on more accessible \"cheap to measure\" variables (inputs x )."
"1) it employs a holistic method for generating features in a supervised fashion that are optimally orientated toward prediction. 2) it is able to work with multiple heterogeneous time-series signals where each one can have a different sampling rate, be nonuniformly sampled and/or be of different lengths. this makes its a powerful tool for industrial informatics as many industrial datasets are characterized by the fusion of datastreams with different sampling and duration characteristics. often significant preprocessing effort is needed to align these disparate date streams. the safe methodology alleviates this burden, and thus is a promising tool in the increasingly big data world, where automated approaches are becoming a key requirement. furthermore, an important consideration when developing risk mitigation strategies for industrial processes is effective process monitoring. safe contributes to this goal by providing enhanced models for soft sensing and predictive maintenance."
"one of the added benefits of the safe methodology is that time-series derivative information can easily be included in the modeling process. this is facilitated by the availability of functional expressions for the time series (14) . in particular, taking advantage of the properties of the gaussian density function [cit], the first and second derivative ofx"
"in this scenario, the output, as defined by (21), is the expected value of the second-order sample statistical moment of the observed data (sample variance), hence one would expect that using statistical moment features should yield the best results. however, while the statistical moment features substantially outperform the pca, kpca, and downsampling feature extraction methodologies, fig. 7 shows that the safe technique again yields the best prediction performances. this somewhat counter-intuitive result arises because of the high variance of the sample second-order central moment estimator at low sample sizes. as illustrated in fig. 8, at the sample sizes defined for this dataset (35-45), the estimator is highly imprecise. this dataset therefore shows how the safe methodology, albeit relying exclusively on local features, can outperform methods that generate globally defined features even when the target phenomenon is global by its very definition."
"a second practical consideration is that we typically only have a finite number of noisy, irregularly sampled data points for each time-series x (j) i (t), as opposed to their continuous function representations. in order to overcome this issue, we introduce a gaussian process (gp) approximation to the unobserved time series. specifically, denotingx i (t), we can writê"
"while safe was originally motivated by modeling problems in batch industrial processes (where the input data are the time evolution of sensor readings during the batch run and the output is a scalar indicator of the final product quality) [cit], the methodology is applicable to any time-series-intensive learning environment, e.g., evoked potential studies in neuroscience [cit], dynamic biological process modeling in genetics [cit], and financial data analysis in economics [cit] . it should also be noted that safe was conceived and developed for supervised learning problems; the potential for extending the methodology to unsupervised or semisupervised problems, and any benefits this might bring, have not been investigated to date."
"in this formulation, the regression coefficients generalize to continuous shape functions β (j) (t) and the contribution to the prediction of the target output y i of each time series is obtained as the weighted integration of the time series with the shape function as illustrated in fig. 1 . in this setting, the modeling task becomes one of estimating the shape functions β. to this end, the sum squared error cost function in (4) is generalized to"
"plasma etching is ubiquitous in modern semiconductor manufacturing due to its ability to provide precise control of the resolution and directionality of etch. in a typical plasma etching processing tool [cit], gases are introduced into a vacuum chamber and ionized to generate a plasma which then interacts both chemically and mechanically with the masked wafer surface to etch away the exposed surface. to achieve the desired precision on critical feature dimensions, the key parameter which needs to be controlled is etch rate [cit], but this information is not available in real time as it can only be determined through a costly postprocessing metrology step. however, using oes monitoring of the plasma, which allows the changes in the plasma chemistry during etching to be observed indirectly, soft sensing solutions can be developed for etch rate prediction."
"in this section, the capabilities of safe (with and without the time-series derivative extension) are demonstrated using three specially constructed synthetic case study datasets. comparative results are provided for a number of alternative time-series feature extraction methodologies as follows."
"this is simply pca-downsampling using the kpca [cit] . kpca produces nonlinear transformations of the data and hence can produce more efficient representations if underlying relationships are nonlinear. here, we employ gaussian kernels. for consistency with safe, the design matrices generated by each methodology are used to develop ridge regression-based linear models as described in section ii. the root mean squared prediction error (rmse) of these models, computed on test data and averaged over 500 monte carlo simulations, is then used as the performance metric for comparisons."
"it is expected that, thanks to its built-in noise filtering and smoothing capabilities, the proposed safe methodology can cope with irregular sampling intervals in an efficient way. while studies regarding the impact of intermittent sampling rate on filtering exist in the literature [cit], their effects on feature extraction quality are largely unexplored."
"the capabilities of the safe methodology with respect to competing time-series feature extraction methodologies have been demonstrated by means of simulated examples and further validated using a practical semiconductor manufacturing soft sensing problem. the prediction optimized features extracted by safe come at a price as simpler approaches to feature extraction require less computational effort; however, the results presented show that safe is able to consistently outperform its competitors over a range of input-output relationship and data conditions, including situations where the target output is determined by global features of the input time series."
"in mathematical terms, the scenario in question is to identify a model f of the phenomenon under consideration by exploiting a training dataset s of n observations of the phenomenon, where s is defined as (1) with x i, the ith observation, consisting of a set of p time series defined as and y i is a scalar target value. the predictor function f is chosen to be optimal in the sense that, given a set of independent observations of the phenomenon (test set)"
"in practice, due to the locality of support of gaussian basis functions, d is diagonally dominant, hencer can be approximated as (5) with the corresponding ridge regression solution given by (6) ."
"in this paper, a novel safe methodology has been defined and presented that addresses these challenges for modeling problems where a scalar output to be predicted is a function of one or more time-series data streams. the safe methodology has two main characteristics."
