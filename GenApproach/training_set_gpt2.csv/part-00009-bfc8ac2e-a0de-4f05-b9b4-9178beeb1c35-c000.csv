text
"we have also solved the problem of semantic variation of the set of terms filtered by an unsupervised algorithm of classification, based on a structure of conceptual vectors. on the other hand, with the envisaged prospects for the further of this work, several aspects should be mentioned: -addition of a treatment of pruning of the terms from the corpus (global analysis) by merging the different legends between them -adding a treatment concerning the recognition of the named entities since the named entities are considered more easily identified in the texts and can be chosen as indexing terms."
"in the initial partition step, we replace the clustering coefficient with the itg coefficient for the directed graph as the referenced calculation value in the ranking step. in the partition refinement step, we found that the greed of the original algorithm cannot be ignored where the structure of the original algorithm cannot converge efficiently. we improve the structure of iteration in our algorithm by adding the flag bit flag to label the current iteration, which has reached the maximum dwcc updating limit value or not."
"the information transfer gain clustering coefficient (itgc) of node i in a directed network can be summed by the 15 different independent results in table 3 in formula (11) volume 5, 2017 as follows:"
remark: the centroid of the first term to be included in the class (if it is not exists class which does not support this term) is equal 0 (the angular distance of two terms identical is equal to 0).
"for the experiment whose results are presented in table 11, we used the following parallel computing environment to test the performance of ditg-dwcc: eight slave nodes were used in deployment with spark version 2.1.0. and hadoop table 10 shows that with the scale growth of the dataset, our ditg algorithm has very good distributed performance in handling large-scale directed networks, better than fastgn, oslom and infomap in terms of time consumed."
"the coefficients α 1, α 2 and α 3 represent the weightings attributable to criteria defined previously. the set of candidates of terms selected by this heuristic are composed by two words (bigrams case), three words (trigrams case) ... they are represented by the set selected-terms. this set may contain terms that are included in other words; it must then prune these superfluous terms and retain the terms considered to be most discriminating. this elimination is done according to the scores associated to each term:"
"the pgm theory can always be classified by two conditions: (1) edge based directed or undirected attributes, and (2) abstraction based different levels. based on directed or undirected attributes of edges to classify pgm, there are three classes: (1) a directed graph model called the bayesian network (bn) [cit], where the network structure is a directed acyclic graph; (2) an undirected graph model called the markov network (mn) [cit], where the network structure is an undirected graph; and (3)a local directed model including both directed edges and undirected edges, that is composed of a conditional random field (crf) and a chain graph (cg). based on the different levels of abstraction, there are two classified classes: (1) a random variable based probability graph model such as bn, mn, crf and cg, and (2) a template based probability graph model."
"after finishing the directional improvement above and the construction of dwcc i, we can implement the vector influence based clustering coefficient community detection model by the scd (scalable community detection) algorithm framework of arnau prat-pérez."
"-choose a vocabulary to identify in an image and describe its objects in a manner more rational -also, choose a vocabulary and a simple style to locate the type of scene (scene in the countryside, urban landscape, inside, outside, day, night ...)."
"for the identification of compound words, we propose a heuristic which analyzes the terms of the text by group \"n-grams\" and whenever the term is a compound word we mark it, remove it from the analyzed sentence, we move to following the term \"n-grams\" and we update the current sentence and so on until there is no further term to deal with. in this method we privilege the short term (composed of 2 or 3 words), this choice is justified by the fact that in the french language, there are more than 70% of compound words that include two or three words, hence the following heuristic:"
-the fifth step has as task the pruning of terms and aims to eliminate redundant terms and unnecessary terms resulting from the filtering process. it performs the fusion of all the terms obtained after a series of filters and finally forms the basis of final index.
"by obtaining the maximum value of dwcc, we simulate the progress to reach the local optimum solution, which can be found in figure 6 . according to the experimental process, we have set up the iteration stop condition to meet one of the following two conditions:"
"similarly, the terms « de l'image », « l'image », « une image », « à l'image » are considered identical to the term \"image\" retained as term reference in the index base."
"algorithme4 nd filter: consideration of the builders called \" syntactic patterns\" the previous step has enabled us to make an initial selection of terms formed from 2 words, 3 words by a number of measures. each candidate thus selected possesses already a lexical labelling (treetageer), and to be always kept as a candidate among the selected, it must be submitted to another filter. this second filter is based on the principle of syntactic patterns. we have a base of syntactic patterns which contains a collection of syntactic configurations extracted from an automatic learning of syntactic patterns carried out on a corpus from training. a syntactic pattern is a rule about the order of concatenation of grammatical categories formed for a noun phrase. for our implementation we selected a combination of 128 patterns extracted from a corpus by a manual learning procedure done on a sample of examples of legends. this filter allows to examine each candidate and to check if it is among the existing configurations in the database of patterns built. in the affirmative case, it will be retained otherwise it will be removed."
"our problematic is this: we already have a model of description of images [cit] where the descriptors associated with the images are all non-textual. is there a way, and how to expand and consolidate these non-textual descriptors of our model with a structure of textual index?"
legend labeled and structured to isolate separately each noun phrase to process its elements. we propose a formalism of representation of the phrases in the form of noun phrases extracted from verbs.
"algorithme5. description of filter -labeling each word m i by examining its characteristic profile (long-short or frequent-rare), this profile is determined according to the frequency of the word in the corpus and its size."
"the pgm theory uses graph structure to represent the joint probability distribution of variables. in recent years, it has become the research hub for uncertainty reasoning solutions. the pgm representative theory combines knowledge of probability theory and graph theory. in graph theory, the relationship of random variable dependency can be represented and provides an effective representing frame for statistical multivariable modelling."
"with extensive experiments in artificial network datasets and real-world, large network datasets derived from online social media, it has been proved that our algorithm is more accurate and faster than several traditional and well known community detection methods such as fastgn, oslom and infomap in directed networks. our itg-dwcc algorithm has acceptable precision and has obvious advantages regarding time complexity."
"however, the cpm algorithm is very strict with the limitation of connections among cliques, which causes high time complexity. [cit], ahn proposed the link clustering algorithm (lca) [cit] by setting up a model based on edges rather than on nodes. in the calculating process, it used the jaccard coefficient to calculate the similarity of connected edges, which makes the existence of overlapping communities natural. [cit], filippo presented a measuring function, which used a q value based significance function as the detection object function and invented the order statistics local optimization method (oslom) [cit] . the oslom algorithm is the first algorithm for community detection in directed weighting edge networks. lca is also constructed based on an optimization significance function value. [cit], yang and leskovec improved the lpa algorithm by optimizing group nodes for community attached relation target functions and presented the bigclam algorithm [cit] . by redefining the overlapping community in different communities, this algorithm produces good results in large-scale networks."
"in formula (6), par g (x i ) is the corresponding random variable of the parent node of node x i in graph g. in the directed network, which we study in this paper, the information influence of a source node on the other nodes brought by the directed transmission of information will also influence the forming of the corresponding community structure. because of the fact that in real social networks, fans are always gathering around an opinion leader to generate community structure, this phenomenon can influence the information transfer model in the whole network. for figure 2 -(a), we can suppose that node i is the information transferring source for nodes j and k. based on the probability graph theory, the change of information transferring probability in each edge reflects the information transfer gain received by the end nodes. according to the above definition of itgp, we get the formula of itgp in figure 2 -(b):"
"first, from the traditional undirected clustering coefficient diagram in figure 1-(a) and 1-(b), we can deduce the edge information propagation probability and direction. in an actual directed social network, the vector influence clustering coefficient should be deduced from figure 2 . we suppose that when all edges in figure 2 -(a) and 2-(b) are bidirectional, they can be equal to figure 1-(a) and 1-(b) respectively. when all the edges are bidirectional, the information transfer gain path (itgp) of all the edges is equivalent to that of an undirected network. tracing back to formula (2), we can split the information contribution (ic) of each edge in the actual existing e i edges among k i nodes."
dataset 1: an artificial dataset generated by three communities randomly. we generated it by forming an edge in the community with probability 0.5 and forming an edge between communities with probability 0.25.
"-legends which describe scenes associated with the images, they express interpretations that we can do for the images and are specified in an uncontrolled free language."
"most studies show that indexing by multi-terms offer a distinct advantage because the information searched for is focalised in a privileged way in nominal groups. the different works of extraction of the complex terms from textual corpus use three approaches: statistical or numerical analysis, linguistic or structural analysis and hybrid or mixed analysis. statistical analysis is based on the study of the contexts of use and the distributions of the terms in the texts; the linguistic analysis exploits linguistic knowledge, such as the morphological or syntactic structures of the terms whereas the hybrid analysis uses the two analyses."
"to improve and to refine the list of terms obtained during the previous filtering, we consider this time a filter based on a criterion of calculation of measurement concerning the amount of information associated with each candidate term examined. a term with a potential of information sufficient, tends to be a more important and more discriminative term in the noun phrase where it is located. this filter assigns a score for each term regardless of the context to which it relates. this score is proportional to the size of terms, their distribution and their type in the corpus, hence:"
"the overlapping community detection method has a different construction than the previous algorithms. the representative algorithms are as follows. [cit], palla proposed the clique percolation method algorithm (cpm) [cit], which was based on the characteristic that the edge of an internal community has tight connections and is easy to form a clique, which finally consists of a community."
"based on different analysis targets, current community detection algorithms can be divided into four groups: the hierarchical clustering approach, the matrix spectrum analysis approach, the edge based approach, and the maximum clique based approach [cit] . focusing on large-scale networks, there are three categories of community detection algorithms: the modularity value optimization based method, the random walk based method and the overlapping community detection based method."
"the methodology we propose aims to enable the development of a text-based index from a set of terms (single words, compound words,) extracted from the corpus of images described by legends. such a base is more refined and ensures the property of semantic variation. other stages of our project are not described in this article and are subject to other publication work."
"for figure 2 -(a), because there are no edges to connect node j and k, it is a simple situation to analyse. we assume the itg (information transfer gain) of node i to node j and k, which can be defined in formula (5-1):"
"remark: the threshold s, corresponding to the allowable score, is a parameter determined empirically by the expert and used for examine the combination of terms to test."
"we note d: set of legends, p: set of sentences of the corpus, v: set of verbs that includes the empty set and s: set of syntagms that can be formed from the words. 2)procedure of decomposition: as the filters operate on noun syntagms, it is then necessary to transform automatically the textual format of the legends into a canonical format expressed in our model. the method adopted for this kind of formatting consists in analyzing each sentence and to locate from its syntactic structure the verb (the sentence is already labeled by treetagger and syntex). the position of the verb in the sentence allows to split this phrase into two parts, the left part and the right part. we proceed by cutting each sentence around the verb, this decoupage gives two syntagms: a left syntagm and another right and we continue recursively this process of decoupage while the obtained syntagms contain always a verb. the stopping criterion of this recursive process of decoupage into noun syntagms (left and right) stops once the examined syntagm no longer contains the verb. this sentence is supposed to be already labeled by treetagger and syntex. the decomposition procedure described above turns this specification in form (triplet):"
"we will present in this article firstly the problematic addressed and then situate the perimeter of the study from the context in which we are working. we will describe in the third section an overview of the state of the art of our problem. in the fourth section, we will detail the complete treatment chain which will permit, from the corpus of legends images, to extract a collection of terms serving as a base of linguistic index for the corpus. we analyze in the fifth section the obtained results of our experiments from a corpus and finally we will finish the study with a conclusion and perspectives."
"-eliminate the redundant terms and the terms included in there terms. -initialize the index i with the first class from the first term drawn randomly from l for each term t drawn from l do * determine the nearest class to the direction of semantic proximity. * if the required class exists then we insert the term t in this class and we update the centroid of conceptual vectors for this class. *if the required class does not exist then we create a new class with the term t and we take as centroid, the conceptual vector of the term t. endfor 3) explanations: we clarify in details our algorithm a) fusion of the lists: this step is necessary because it allows gathering all the terms in order to treat them together and consequently to facilitate the elimination of the duplicated terms. b) elimination of doubled blooms and terms included in other terms: this operation allows starting from the list previously built to remove all the redundant terms and to remove as well the terms included (non-specific) in other terms. for example between the term « données » and « données numérique»\" we remove «données ». the latter problem was already treated during the process of filtering at the local level (sentence). the treatment here concerns the global level (legend). c) initialization: the index to be build will contain classes of the most relevant terms. the purpose of these classes is to make our index not fixed in relation to queries on the legends and to solve the problem of semantic variation of the terms. the initialization of the index at the beginning is carried out with the first term (taken randomly from the class l) with its conceptual vector. d) determination of the nearest class to the considered term: we are going to adapt the notion of conceptual vectors [cit] for the calculation of measures that allow us to determine the best class to which the term considered will be affected. the notion of conceptual vectors is a formalization of the projection of the linguistic notion of semantic domain in a vector space. it is therefore possible to construct vectors which each component corresponds to a concept. the comparison between two vectors is done using the angular distance. for two vectors x and y to be compared, the angular distance d a (x, y) is equal to x*y/║x║*║y║ and empirically if this distance is inferior to π / 4 the thematic proximity is considered near, if this value is higher than π / 4 the proximity is low and if it is around π / 2 the proximity is the near zero. from these principles, we will first build for all the terms found in the list l the conceptual vectors and for this purpose we chose an external resource wolf (wordnet the free french) which is a free semantics lexical resource for french. wolf has been built from the princeton wordnet and various multilingual resources [cit] . to determine the closest class for the term t, we first calculate the angular distance of the conceptual vector of t with the conceptual vector centroid of each class and retain the lowest angular distance and as the different vectors have not necessarily the same number of components, we do this calculation on the elements of the neighbourhood thematic [cit] . e) updating the centroid of the conceptual vectors of the class: from the centroid of the conceptual vectors of class, we can to project all the vectors of the terms of this class. the centroid of a class contains all the closest terms to the terms belonging to this class. the updating of a centroid consist to take into account the calculated angular distances between the terms with all the elements of the class searched and the centroid of the current angular distances (before inserting the term into the class)… . we consider, d1, d2, ... dn,the various angular distances between the terms and all the terms of the class ci and ci-1 the centroid of the angular distances of the class before the addition of the new term in this class."
"through analysis and comparison, we are sure that in the social network nodes studied in this paper, the information propagation probability has close relationship with directed edge relationships and the specific propagation approach. we chose the random variable probability based bayes information propagation network to set up our model."
"there are two distinct ways to perceive and understand an image. the surface structure is what we see and the deep structure is what the picture really means. describe the surface structure of the image [cit] consists to enumerate the elements that it contains and describe her deep structure, is to explain the meaning of the image and interpret the scenes it contains . basing on this principle, we consider three types of legends: -legends with a simple description and a more precise specification of content of images, done in natural language."
"for the experimental protocol, we consider a corpus of 200 images uploaded to the site: http:/www.pascal-corpus.htm. we have captioned these images in two distinct ways: -a first class (100 images) contains the short legends (15 to 30 words in average)."
"to solve the problem of traditional community detection algorithms in large-scale social networks and directional social networks such as wechat without accurate simulation models and high algorithm executing efficiency and precision, this article starts with the triangle group, which is the basic structure of the community and modelling on the local impact of vertex in networks. by using directional vector information spread, probability calculation theory and probability graph theory modelling those vertices with huge influence on directed social networks, this paper constructs directed clustering coefficients for vertexes. we also construct the target function for measuring the efficiency of community detection and distributed parallel community detection algorithm of our model for large-scale social networks. ultimately, by conducting experiments on large-scale artificial network datasets and real network datasets, the precision and novelty of our algorithm is verified."
"we made some directional improvements to the statistical parameters in formula (12) which can be found in table 4 because we are handling directed networks. with the stable values of parameters provided, the calculation of formula (12) has constant time complexity. the updating process of calculated itg values of each node and dwcc i only occurs when the structure of a community has been changed. after experiments, the whole time complexity of our algorithm in the entire graph is o(nm), where m is the number of times a community structures changes."
": the purpose of decoupage of the phrases into noun syntagms is neither used for of comprehension purposes nor for translation purposes, but legend standardized treetagger"
"this method attempts to bring the idea of small-scale community detection algorithms, which are based on modularity optimization, into large-scale community detection. through the optimization of modularity, a fine-grained community detection result is obtained. a function of modularity q is proposed by newman and girvan [cit], which is defined as:"
"four representative datasets were selected to evaluate our algorithm: an artificial simulation dataset, two classic datasets in community detection and a real world dataset for connected closely crowd calling record in a city of china."
"corpus of images (2) information. these approaches called hybrids also operate the systematicity, rapidity and independence from the domain of statistical algorithms. the intervention of linguistic knowledge and approved statistical calculations allow at these approaches more performance and to obtain more satisfactory results for the linguist on the phenomena which it seeks to observe and describe. this is the reason for which we opted for the choice of this approach."
"first, this paper put a classic probability graph and clustering coefficient together and proposed a new vector influencebased clustering coefficient itg for measuring directed graphs. then, we combined the definition of target function iterative optimization of dwcc with directed modularity in the community detection of directed networks. in the iterations, due to the independent optimal movement calculation, we can perform a paralleling operation in the most timeconsuming step."
"community detection in these large-scale social networks plays an important role in the study of topologies and the architecture of networks. because of the huge quantity of vertices and edges in a large-scale network and its complex structure, common traditional graph analysis approaches cannot perform the research such as layered architecture analysis and knowledge attaining within reasonable execution times. therefore, the high processing efficiency and accurate results of community detection algorithms are necessary to detect potential community structures from the huge directed complex network."
"in formula (3), the triple is the structure that connected with node i, includes node i itself and two other nodes, and there exists at least two edges among node i and the other two nodes:"
the cpu frequency of master node is intel(r) xeon(r) cpu e5-2440 v2 @1.90 ghz. memory is 16 gb with 4 tb hard disk. jdk version is 1.8.0 131.
"first-pass: the text to be analyzed in this morphosyntactic labeling sentence is already standardized moreover the compound words, if they exist, are identified and marked. treetagger takes as input a text file that it splits into words by assigning a morphological label to each word and the lemma of the word and then sends back the result of the labeling in the output file.we note that the compound words were been previously treated (cf., recognition of compound words) and are considered for the analyzer treetagger as unknown words and the tip used in our analyzer is to concatenate all components of the analyzed compound word in an atomic unit."
"in sum, a good annotation of images from legends must absolutely meet the following requirements: -distinguish the surface structure from the deep structure of the image during the process of describing."
"in the above 27 arrangement cases, because node i is the source node, we can find some symmetry results. for example, figures 4-(b) and 4-(c) are symmetry results. in table 3, we use the same colour blocks as the symmetry results, and finally we can get 15 independent results; we separated the sub-graphs into figure 4 and figure 5 ."
"in an actual social network relationship, two friends of someone can be friends of each other, and this attribute can be called the clustering characteristic of networks [cit] . the network average clustering coefficient reflects the microscopic clustering characteristics of a network and has become an important index of adjacent nodes that connect closely. the node clustering coefficient definition of a network is: in a network with n nodes, one node i has k i edges connected to it and other nodes, i.e., node i has k i, neighbours. if among the k i nodes, there are e i edges, the clustering coefficient of node i is:"
"b) use of syntex [cit] : .syntex is a syntactic analyzer of corpus which allows to extract from a corpus a list of names and noun phrases, structured by syntactical dependency relationships. this analyzer takes as input a legend split out in words (simple words and compound words) with each word is associated with a grammatical category (result of treetagger). at output, syntex produces two formats, the first to represent the syntactic dependency relations (subject, direct object ...) between words and the second to represent the networks of syntagms (verbal, nominal, adjectival ...) structured by the relations head and expansion. these two indicators allow us to locate and identify among the examined terms, the substantives, the subjects..."
"our methodology aims to select the complex terms from the legends that are associated with images in a corpus. a first selection of these terms is done then filters are applied to this set of terms (composed of noun phrases) to retain at the end of this filtering process that the discriminative terms used for the indexation of the legends. this methodology is then based on a set of filters operating on noun phrases from a set of heuristics, and proceeding by successive elimination of irrelevant terms. the methodology proposed in this paper consists of several steps:"
"when setting up a model for a traditional social network detection algorithm, network is often processed as an unweighted and undirected graph, ignoring the direction of edges. however, in online social networks, edge direction always contains important information. online social networks have the characteristic that the important nodes (such as opinion leaders) are always information propagation originators. in this paper, based on the classic probabilistic graphical model (pgm) theory from the turing award winner pearl, we extract the direction of information propagation edge between different social network nodes to a directed vector and propose a vector influence clustering coefficient model with both information propagation direction and information propagation probability."
"in this paper, section 2 shows the current related work on community detection algorithms in large-scale networks. section 3 introduces our itg model in detail and the itg-based directed network community detection algorithm itg-dwcc. section 4 shows the comparison of experimental results of the itg-dwcc algorithm and other classical algorithms in artificial network datasets and real-world network datasets. section 5 gives the conclusion."
"another famous algorithm is the lpa (label propagation algorithm) [cit] series-based large-scale community detection algorithm. the time complexity of lpa is o(n 2 ) where n represents the node number of the network. compared to other complex machine learning algorithms, lpa has lower complexity and better clustering efficiency. [cit], raghvan improved lpa by providing the rak algorithm [cit], which was based on community detection operation with an approximate linear direct ratio when network scale increased. through the predefined target function, it simplifies the complexity of lpa and uses network structure as a guide to detect community structure. its result in the karate club network and the american university football network shows the good performance of rak. however, there are some special drawbacks of the rak algorithm in experiments in benchmark networks, and it needs improvement. [cit], gregory improved rak by providing copra, (community overlap propagation algorithm) [cit] which is an algorithm that focuses on mining overlapping communities. in the copra algorithm, every single node has a number of community labels. furthermore, the propagation process of copra includes multi-information of community, which contributes to the increase in execution time cost in each iteration process. while there are lots of overlapping communities, it results in the random selection of fault labels."
"the weighted triangle number wt(x, c) means that based on the focus vertex x, the formed triangle numbers with the definition of itg can be recorded as a weighted triangle. at the same time, the weighted node number means that the useful and valid node numbers are recorded as weighted nodes in definition of itg around the focus source vertex x."
"the global project of our team aims to develop and validate a methodology of application regarding the indexing and searching of images based on techniques from artificial intelligence, statistical techniques and linguistic techniques. the context of our problem within the scope of this proposal is limited to point 2."
"to respond to this problematic, we will at first segment each legend into sentences, split each sentence recursively into triplets (left syntagm, verb, right syntagm) so that both syntagms obtained become nominal (for each triplet). we extract from these syntagms, the relevant terms which then become descriptors. these multi-terms chosen for the indexation make it possible to designate semantic entities or concepts better than single words, and offer a better representation of the semantic content of a legend."
"is the weighted number of the triples using node i as the top vertex; its weight is the weighted sum of the six itg i_triple (t ) values of different types of triples multiplied by its counted number number(t ). similar to undirected clustering coefficients, itgc has the same characteristic, measuring the tightness of the graph to form tight communities."
"in networks, communities are divisions composed of vertices and edges, which are called groups and clusters. community structure has the basic characteristic that vertices in the same community are connected closely with each other and vertices in different communities are connected sparsely. furthermore, information spreads faster inside a community than among different communities."
"-for the long legends, it is strongly recommended to define a heuristic on the thematic salience that will enable to approach the other treatments without ambiguity."
"in terms of scalability, because the step of calculating the best movement can be deployed in a distributed environment, our itg-dwcc algorithm has an advantage for largescale networks of parallel computing, which can be used for efficient community detection on large-scale directed networks. to test the parallel performance of our distributed itg algorithm (ditg-dwcc), we used some real directed large calling record networks from a calling graph in a city of china; it can be found in table 10 . the experimental results of ditg compared to other classical directed community detection algorithms are presented in table 11 ."
"based on optimized iteration of the target function of arnau prat-pérez, the partition refinement step was related to three functions of possible increase of wcc and their similarity, which means that the three different functions can be converted to the united calculation process of wcc i . in the process of directional improvement, we also need the estimation of dwcc i to reduce time complexity of our algorithm, which can be found in the following formula (12) :"
"we proposed in this article a new paradigm for indexing based on the notion of syntagmatic term. this paradigm involves the translation of nominal phrases extracted from the corpus to be indexed into syntagmatic index terms having a complete semantic. the method used is hybrid and it combines numerical and linguistic filters. these filters are not expensive in time of calculation and easy to implement. this is an induction to improve the accuracy rates. the results of the filtering process still remain well relative, they depend on the size of the corpus (more the corpus is bigger, more the results are better) and depend on the pre-treatments done on the texts (recognition compound words, elimination of ambiguities ...) and depend especially on the nature of the legends. in other words, a short text with a style of description more simple may lead to better results and accuracy more efficient."
"however, all the algorithms above do not focus on directed large-scale networks for community detection and have long execution times and low accuracy, so it is necessary to construct a highly efficient algorithm to make improvements."
"random walk based methods have the characteristic that information is spread easily in the internal, high-density community. different from modularity optimization based methods, this type of approach focuses on the process of information propagation or some physical element permeation while it attains the community structure fast and effectively. some classical representative algorithms are as follows."
"-the third step allows, using analyzers treetagger [cit] and syntex [cit], to lemmatize the legends, to annotate them and to generate structures of syntactic dependencies."
"second-pass: in the first pass treetagger achieved labeling of the sentence without treating compound words (considered as atomic words and unknown), it was therefore necessary to make corrections to all these words from the previous treatment of the compound words with the new label nc (lexical label for each component of the compound word) while respecting the output format of treetagger."
"our follow-up research will include: (1) optimizing the stop condition in the iterations to fetch up the greediness of the algorithm, (2) implementing efficiency of our parallel algorithm and experimenting in a distributed environment, and (3) integrating our algorithm to form a visualization analysis software application."
"the marking mark_it procedure consists in concatenating all the elements that make up the compound word, so that they are in atomic form. with the analyzer treettagger, this word is considered as an unknown word. during the second pass of our analyser, it is during the second pass of our analyser that we could then identify the compound word for labelling. 2) labeling : to facilitate the extraction of the terms in the sentence to be examined, you need a morpho-syntactic analysis and a syntactic analysis of surface. considering the tools available on the web for french and the performances of these, we decided to use treetagger [cit] for morpho-syntactic annotations and syntex [cit] for annotation of the syntactic dependency relations. the processing chain for this type of analysis is: [cit] : treetagger is a system that was developed by h. schmid in the project \"tc\" at the university of stuttgart. it is a tool that allows annotating text with information about the parts of speech (kind of words: nouns, verbs, infinitives and particles) and information of lemmatization. the contribution of treetager in our case is to assign to the legend taken from the morphological categories (noun, verb, adjective ...) which facilitates the analysis of the legends linguistically. our morpho-syntactic labelling procedure including treetagger operates according to two passes:"
"among the works based on a linguistic approach, we can mention, lexter [cit] who defines a methodology for modelling terminology of the french language. there is also fastr [cit] who uses meta-rules to locate in a corpus, variants of terms from a list of initial terms. [cit] exploits the linguistic and lexical features known in the texts in general and apply them to the texts within other areas such as grammatical labelling, extraction of the concepts and the semantic relationships between words. all these methods are based entirely on corpus learning which requires an annotation cost in terms of time and skill required. even if the linguistic approaches enable obtaining good results, producing electronic dictionaries in which the reuse of semantic information hidden in large databases of terminology, are still too costly in time and effort, to be easily integrated into the part of a flexible approach."
"research into large-scale social networks is becoming increasingly important. due to the convenience of connection to each other, the scale of online social networks is enlarging at an unprecedented rate. [cit], for example, the number of global twitter users was more than 500 million, including 200 million active users. wechat users number was more than 600 million, with 400 million active users."
"-a second class (100 images) contains the long legends (more than 100 words). we excluded in our experimental protocol the analysis of surface annotations legends because this type of description is generally simple and explicit. we chose to test our system with the annotations deeply to better evaluate our approach. among the recommendations retained on the form and the style granted for the legends, we can list what follows: -describe, the best possible, the contents of the image."
"a term is a unit made up of a word (simple word or compound word) or a group of words. to extract all the relevant terms of the legends, it is necessary to carry out the standardization of the texts of legends."
"3)filtering of candidate terms: our approach for the selection of the relevant terms for indexing images from the legends is described by a processing chain using four filters that are presented in the following: each term t (n-gram) is written in the form of n words x 1, x 2, x 3, ... x n with x i full or empty word. the measures considered here are calculated according to three criteria and allow to express the bond strength existing between the different words xi (full) regardless of empty words, because the frequency of these latter which is much higher than the frequency of the full words. the criteria adopted are: * criterion of presence: all xi words must be present except the empty words."
"by setting up the information transfer gain path (itgp) model of the situation in figure 2 -(a) and figure 2 -(b), we get 9 different itg sub-figures for figure 2 -(a) in figure 3 according to formula (5-1), (5-1), (5-2), we calculate the corresponding itg results in table 1: the corresponding itg i value of each figure can represent the weight in each type of directed triple. by the statistics of all directed triple types in the graph and the summation by weight, we can calculate all the weight distributions of directed triples in the directed graphs in table 2 and table 3 . similarly, by extending the method to itg calculation of triangles, we obtain 27 different itg figures in figure 4 and figure 5 . using figure 4 -(a) as an example, node i, node j and node k are all connected with bidirectional edges, while all the edges had been defined as 1 previously. the itg sum of node i and k consists of single directional itg of node i and node j and the relayed itg from k to j starting from i. we see in formula (9) that: figure 4 and figure 5 ."
"there are two tendencies in the images research models, the research based on the text and the search based on the content. in the textual approach, the image search uses the textual descriptions. once an image is described by the natural language or by an annotation using keywords, the image search compares the strings of characters corresponding to the image with requests. this mechanism is simple to be implemented, however, in real application, it is often noted that the resources with the keyword are not pertinent and the resources that are pertinent do not contain the same keyword. so, the most obvious problem of the image search based on the text is that it can sometimes reveal too many duplicated images or no image at all. in general, the indexing of images takes little advantage of the many existing work in the textual information search and the natural treatment of languages. the main difficulty faced by the authors is in the selection of the most pertinent indexing terms to describe an image and especially the consideration of the relationships between these terms."
"the previous results do not hold the step of terms pruning. the processing which constitutes the chain of analysis of legends operates sentence by sentence and extracts the candidate's terms for the indexing. this list of terms can be redundant, including the ones with the others… pruning intervenes only for the set of the four filters. the fusion of all the lists resulting from the sequence of filters for each sentence (belonging to the same legend) and their pruning slightly improves the results in terms of high precision (cf. tablev)."
"but this tendency in pronunciation is already known to have existed in the nineteenth century spoken afrikaans or cape dutch. one need only to look at abubakr effendi's bayanudin (an explanation of the religion) written in 1869, to recognize this fact. a typical example is changuion 0844: xix) paddak which appears as padak in the bayanudin (effendi 1869: 296) . there are, however, other examples of cape dutch or nineteenth century afrikaans pronunciations which have lingered on in cape muslim afrikaans. examples of these are danebol and moddras for the standard afrikaans dennebol and modder. these words, with morphological structures which conform to their pronunciation in cape muslim afrikaans, are already listed by changuion in 1844 (see changuion 1844: xi and xviii) and continue to be pronounced in this mode in cape muslim afrikaans."
"the dominance of malayu from the end of the eighteenth century, and its use as the first medium of instruction at the first slave madrasah in dorp street [cit], explain the more frequent borrowing or inheritance of lexical units and items from this language. it is impossible to think of madrasah or islamic religious education at the cape without three distinctive malayu terms -batja, meaning to read; ai-ya, meaning to spell, and toellies, meaning to write~ these three lexical units are very strongly wedged into the spoken afrikaans of the cape muslim community. however, malayu, though the most dominant, was not the only slave language from which lexical items were borrowed or inherited by cape muslim afrikaans. in standard afrikaans, too, we can trace the etymological roots of the word tronk (prison) to tllrunkll, a buganese and sunda word. the word tronk could thus have come into afrikaans through the buganese and balian slaves [cit] ."
"in this section, we present the experimental results obtained on facial expression dataset. the experiments are conducted in c++ with opencv 2.2 in ubuntu 12.04 operating system on a computer with intel xeon x3430 processor 2.40 ghz with 4 gb ram. the extracted features are fed to libsvm for training. polynomial and rbf kernels are used for experimental purpose."
"cape muslim afrikaans also, to a lesser degree, borrowed from other languages like french and english with which it came into contact. french accounted for such words as aspris (on purpose, from french -expres); kardoesie (packet, from french -cartouche); sa-vette (towels, from frenchserviette) and tamaletjie (a sweet, from french -tablette)."
"it was thus through the madaris or musiim religious schools that the distinctive lexical units and items of cape muslim afrikaans were transmitted and perpetuated. though the medium of instruction in the madaris over the years was afrikaans, this afrikaans was not infrequently laced with lexical units or items borrowed or inherited from the ancestral slave languages of the cape muslim community. it is a known fact that as a result of their diversity of origins, the eastern slaves brought with them seven main languages and fourteen dialects (crawfurd 1820) . these languages and dialects were of the malayo-polynesian family. the slaves from africa again brought with them a variety of portuguese creoles [cit], while malayo-portuguese was the trading lingua franca in the south east asian archipelago prior to the arrival of the dutch (bird 1883: 19-21) . of these languages, malayu, and its distinctive arabic script, known as tawi, came to predominate at the cape from the beginning of the nineteenth century. archival evidence, however, indicates that not only malayu but sunda, buganese and macassar were also written at the cape [cit] ."
"emotions are normally displayed by visual, vocal, and other physiological means. one of the important way humans display emotions is through facial expressions. facial expression is one of the most powerful ways that people bring together conversation and communicate emotions and other mental, social, and physiological cues. [cit] s. [cit] s, facial expression recognition system have enthralled many researchers from numerous disciplines, such as the fields of robotics, psychology, and computer vision. moreover, it has gained prospective applications in areas such as human-computer interaction systems, image retrieval, face modeling, patient monitoring systems for pain and depression detection, face animation, drowsy driver detection in vehicle surveillnace etc.,"
"the borrowing and inheritance of lexical units from the ancestral slave languages and languages of contact with cape muslim afrikaans did not constitute a simple process. it involved complicated morphological, syntactical and phonological adaptations so that these lexical units, at least in orthoepic practice, conform to the acoustic nature of cape muslim afrikaans. this is evident from the changes in pronunciation of the lexical units inherited from the m;llayo-polynesian ancestral languages. it is even more evident in the way grammatical and lexical morphemes are used as affixes to create for these \"foreign\" words new syntactical functions and new orthoepic sounds. the results of this is that gemixte, for instance, hardly sounds english in cape muslim afrikaans."
"as already noted, the core vocabulary of afrikaans comes from dutch. cape muslim afrikaans, as a variety of afrikaans, is also dependent on this dutch core vocabulary for its essential communication. it, however, also borrowed extensively from arabic, the religious language of this community, and the language from which theological tracts were translated."
"then too, as is the custom with the translation of arabic religious works in most non-arabic speaking muslim communities, no effort was made to translate the keywords or fundamental concepts of islamic thought [cit] : xxxvi) but then, too, their transmission is not in isolation of the human element. these linguistic features and lexical items are perpetuated, with all the nuances of the community's distinctive humour, through the madrasah and even the mosque. it was thus at madrasah that my six-year old niece, nurtured in an english speaking home environment, came to learn the lexical item \"is jou brein dlln ghilmilnienglll?\" (are you really that stupid?), and uses it freely and with great humour, even with adults, who fail to comprehend what she considers to be elementary."
"the word warraiskap was specifieally created by imam abdurakib ibn abdul kahaar (1898) to convey the deep sense of psychological worry which results from a state of spiritual impurity -and thus he writes: \"om wegh te niem die warraiskap wat kom van gadath of onrainnighaid\" [cit] : 135) . hisham neamatullah effendi (1894) created the word opbouwens, as a concept for 'systematic construction', and imam abdurahmaan kassiem gamieldien (1907: 2) coined the word verghinskap for a 'divine gift of providence'."
"similarly, as a translation for the arabic alimun (the all-knowledgeable), he creates the word wieter, conveying the feeling that god alone is knowledgeable of all things. for the arabic qadirun (the all-powerful), the new word kraghtagh is used; while the islamic concept of predetermination is brilliantly conveyed by the created word wiler (the one who wills) for the arabic concept muridun, a noun which expresses the idea 'that everything is predetermined by the will of god'."
feature is a descriptive portion extracted from an image or a video stream. visual data exhibit numerous types of features that could be used to recognize or represent the information it reveals. these features exhibit both static and dynamic properties. classification or recognizing an appropriate video relies on competent use of these features that provide discriminative information useful for high-level classification. the following subsection present the description of the feature used in this study.
"this notion of cape dutch as essentially an \"uncultured patois\" [cit], was a popular one at the time. it contributed considerably to afrikaans ultimately acquiring the derogatory nicknames of ''kombuistaal'' and \"hotnotstaal\". at the same time it revealed the negative race relations which prevailed at the cape. this perceived limitation of the afrikaans or cape dutch vocabulary did not prevent the cape muslim arabic-afrikaans writers to embark on the translation of involved theological and theo-philosophical arabic works. one of the wonders of these translations is that, despite the limitation of the vocabulary at their disposal, these translators managed to capture the spirit of the work they translated, and rendered it in a language which has remained meaningful even to the present generation of cape muslims. then, too, the easy and simple language used, did neither distort, nor diminish, nor distract from the meaning of the original text."
"many of the distinctive features of the acoustic nature of cape muslim afrikaans could be attributed to the use of the arabic alphabet to transcribe the sounds of the essentially western germanic language they were speaking. the arabic alphabet could not convey all the sounds of their afrikaans mother tongue. not even the use of the arabic phonetic script, which became their preferred alphabet for writing, could bridge this inadequacy. the inadequacy of the arabic orthography to represent distinctive afrikaans sounds, therefore, greatly influenced their orthoepic practice. this i illustrated by the morphological changes which occurred in the arabic writing form of the simple afrikaans word hulle at different times in the development of arabic-afrikaans [cit] . these historical morphological changes are illustrated in figure 1 ."
"in the process of translating, these writers were required to build a functional vocabulary, a vocabulary which must not only be able to express the thoughts of the authors they were translating, let alone their conveying their own nuances, but which will be understood and be meaningfully real to their prospective readers. it was in this process of vocabulary building, as we shall see in the course of this essay, that the genius of the cape muslim arabic-afrikaans writers really came to the fore."
"arabic loan words in cape muslim afrikaans came mainly via the arabicafrikaans publications. not all of the arabic words borrowed became bridged into cape muslim afrikaans. the custom, as in other languages, not to translate the fundamental concepts and keywords of islamic thought from arabic, however, also led to these key words and concepts immediately becoming part of cape muslim afrikaans. hence it is difficult to say whether such key words as malaa'iekat (angels); kietaab (book); siefaah (attribute); and dja-iez (acceptable) were brought directly into cape muslim afrikaans from arabic. these key words and concepts could have come into cape muslim afrikaans via one or other of the malayo-polynesian languages."
"this creole form of dutch, from which afrikaans emerged, and which was of great concern to changuion (1844), was essentially a lower class language (de lima 1844: 9) in the nineteenth and early twentieth centuries. it had a rather restricted and limited vocabulary. or, as elffers (1908: 5) puts it, \"a language fit for daily use, though lacking in expression for modern ideas, as well as in technical terms\". in fact, cape dutch or afrikaans was, at the end of the nineteenth century, considered by some to be too simple a language with no literary future. this view was clearly articulated by the colonial linguist, theophilus hahn (sabp: 1882), in a paper he read at the south african public library on 29 april 1882. he said further:"
"the more extensive\" use of arabic loan words, and the relative absence of malayo-polynesian inherited lexical items, in the arabic-afrikaans publications could be attributed to two main factors. the vast majority of the arabic-afrikaans publications produced were translations from arabic. it would have been easier to have borrowed words from this language than from any of the malayo-polynesian languages. in any case many of the basic arabic-islamic religious terms were already absorbed into malayu, the religious language of the cape muslims in the beginning of the nineteenth century. it was possibly through malayu that such arabic terms as salaah (prayer); niekah (marriage ceremony); zakaah (the poor rate); kiefai-jat (funeral); masjied or masiet (mosque); niesaab (taxable property); rieziek (bounty); dalul (quranic substantiation) and waajieb (necessary) were absorbed into cape muslim afrikaans."
"svm generally applies to linear boundaries. in the case where a linear boundary is inappropriate svm can map the input vector into a high dimensional feature space. by choosing a non-linear mapping, the svm constructs an optimal separating hyperplane in this higher dimensional space. the function k is defined as the kernel function for generating the inner products to construct machines with different types of non-linear decision surfaces in the input space. the table1 shows the svm kernel functions used in this work. the dimension of the feature space vector (x) for the polynomial kernel of degree p and for the input pattern"
"'n boer maak 'n plan, maar 'n slams het 'n plan (a boer makes a plan, but a muslim has a plan) in the vocabulary of cape muslim afrikaans lexical items from both the indogermanic and the malayo-polynesian families of languages are evident. these lexical items show the strong influence that these two families of languages had on the genesis of afrikaans. this places afrikaans in a unique position of being the only modern language which can claim ancestry from the contact of different families of languages. here at the cape the indo-germanic and malayo-polynesian language families, which have a common ancestor in sanskrit, made contact with khoesan. the common denominator was the domi-nant syntactical arrangement of the different languages spoken at the cape. this syntactical arrangement follows essentially the soy (subject-object-verb) word order. the germanic dutch thus had the same syntactical arrangement as the malayo-polynesian languages of the slaves and indigenous khoesan."
"this work concentrated on classifying facial expressions into four emotions: happy, disgust, neutral and surprise using texture features extracted from gray level co-occurance matrix. the results are proving that glcm features based svm is giving higher classification rate of 90 %. the system can be extended to extract higher-order statistical texture feature from images and taking into account some of the strange facial expressions."
"in the facial expression database, there are 13 subjects and each subject has 75 images showing different expressions.these face images are collected in the same lighting condition using ccd camera.around 700 images are used for training and 250 images are usedfor testing. fig 4 shows the sample images taken from the facial expression database."
"dr theophilus hahn saw cape dutch or afrikaans as essentially the language of people of colour who were resident at the cape. it was transmitted by them to whites on isolated farms and those white children they served as \"nurses and ayahs\"."
the interdependency of the malayo-polynesian and arabic languages in the creation of afrikaans equivalents for the arabic religious concepts are no more clearly seen than in the creation of composites. examples of these composites are graana-salaah (prayer for the eclipse) -a sanskrit-arabic combination; kierie-slaam (best greetings) -a sunda-arabic combination; while tuan-koeber (gravedigger) is an arabic-malayu combination.
"all this led to cape muslim afrikaans having a lexicon of its own. this essay looks at only some aspects of that lexicon, and notes some of its lexical units and items. there are some lexical units, like gesitnaai for the standard afrikaans naai or naaiwerk, and nematjies for the standard afrikaans netnou, though still extensively used, which are not even discussed. some of these lexical units already existed in nineteenth century cape muslim afrikaans. my first observation, therefore, is that there is no complete lexicon of cape muslim afrikaans. such a lexicon may broaden our understanding of cape afrikaans and hopefully lead us to re-evalute some of the existing philological studies dealing with the genesis of afrikaans."
"the system is evaluated with rbf kernel and it has been noticed that performance of svm with rbf kernel gives better recognition accuracy when compared to polynomial kernel. this seems to indicate that polynomial kernel best fit the data for recognition. the approach using this kernel gives an accuracy of 90%. table 4 shows the recognition rate with various svm kernel functions. further the performance of the system in terms of accuracy with respect to the number of images is depicted in fig 5. it shows that, if the number of images is less than 100, then svm is not able to capture the sequence information needed for expression recognition and the recognition accuracy is also very less. if the number of frame is 100 or greater, then there is a significant improvement in the recognition performance of svm."
"the strength of the bridging of an arabic word into cape muslim afrikaans appears to be determined by the grammatical usage of such words through the affixing of afrikaans grammatical morphemes. the changes in grammatical function are indicated, for example, with arabic root words such as:"
"what this essay shows is that apart from the recognized differences in pronunciation between cape muslim and standard afrikaans, there are also marked differences in their vocabularies. cape muslim afrikaans also depends on dutch for its core vocabulary. unlike standard afrikaans, it has retained many lexical units of· cape afrikaans which existed prior to standardization. furthermore, it inherited some lexical units from the ancestral slave languages of its speakers and borrowed others from the languages with which it came into contact at the cape. arabic began to make contributions to its lexical inventory when arabic theological tracts were being translated into their mother tongue, cape muslim afrikaans."
"but apart from such direct appropriation with orthoepic change, some malayo-polynesian lexical units inherited, changed to perform new syntactical and grammatical functions in cape muslim afrikaans. thus the malayu verb slamblee (to slaughter) would take on the afrikaans grammatical morpheme er, to form the noun slamblie-er. similarly the buganese derived verb soppang (dignified), with affixing of the afrikaans grammatical morpheme heid, becomes an abstract noun soppangheid (dignity). another example is the affixing of the afrikaans grammatical morpheme se to the malayu word djapandoelie (ancient) to form the adjective djapandoeliese in the lexical item djapandoeliese tyd (for 'ancient time'). or merely by changing the word order of composites, which in the malayo-polynesian syntactical arrangements is for the noun to be followed by the adjective, to conform more closely with the afrikaans syntactical patterns, a new distinctive afrikaans ring to a malayo-polynesian composite is created. the classical example is mannie-kamer (bathroom) from the malayu kammar-mandi, where the adjective mandi, meaning bath, follows the noun kammar, meaning room."
"thus, as a translation for the arabic word baqin, which comes from the theological concept baqa, which means that 'god has neither a beginning nor an end', he creates the word aitwaghdieren. in tenns of the arabie dictionary [cit] : 68), baqa is defined as 'eternal' or 'immortality'. to convey the idea that god is eternal, ta ha gamieldien uses the dutch uitwacht, which conveys an idea of 'out watching', as his core word and adds the dutch morpheme deren to change its grammatical function from a verb to an abstract noun. aitwaghdieren is a particularly interesting word, virtuauy telling you, no matter how patient you are, god will outwait you in the final consideration."
"probably the most creative aspect of vocabulary building was through the process of neology. apart from the affixing of morphemes to change the grammatical function of inherited lexical items and other loan words bridged into cape muslim afrikaans, words were also created to convey exclusive meanings. typical examples are werksloon and maaksloon. in both these examples the afrikaans word beloning is interpreted to mean 'reward from god'. thus in the first instance werksloon would imply 'activities for which god will reward the individual' (i.e. such as prayer or indulgences in spiritual activities). the word, however, also acquired the meaning 'good deeds', and it is in this sense that it is currently more frequently used. similarly maaksloon, which means acrumatda~d5 'creation', also has the implied meaning that the process of creation is a reward for humanity."
most systems of recognition for facial expressions can be divided into the following phases: 1) face localization: to detect the face present in an image.
"but be that as it may, the same learning formulations continued to be used from generation to\"generation since the early nineteenth century. from some of these recorded formulations in student notebooks or koples boeke of the 1840s and 1860s, some of the etymological roots of some very frequently used cape muslim afrikaans words could be traced. it also demonstrates the ereolization which has occurred in dutch, and from which creolized form afri-kaans emerged. a passage from one of the 1860 koples boeke in my possession reads as follows:"
"this allowed for easy word switching from one language to another within a constant syntactical framework. it, however, means that the inherited lexical items or erfgoed of afrikaans are not only derived from dutch. the lexical item baie (many), for instance, is not borrowed but inherited from malayu. its extensive usage in the afrikaans language testifies to this fact. the concepts \"erfgoed\", \"eiegoed\" and '1eengoed\" formulated by boshoff [cit] : 144) are, therefore, in need of a serious re-appraisal. \"erfgoed\" are not necessarily only those lexical units and items which came into afrikaans via the germanic dutch."
"most of the lexicons of afrikaans are more exclusive than inclusive .. in the compilation of the lexicons, emphasis is placed on words which are generally accepted in standard afrikaans. many lexical items of the non-standard varieties of afrikaans seem to be ignored or are unknown to the compilers. this in tum has implications for philological studies on the genesis of afrikaans. in the case of cape muslim afrikaans, its distinctive lexical units and items are not only evident in the spoken, but also in the written language of the cape muslim community. then too, cape muslim afrikaans is used to convey moral and religious ideas in their places of wor~hip and other religious institutions. such usage of a language and its lexical items shows evidence of sophistication. therefore, cape muslim afrikaans cannot be regarded as \"uncivilized\". in view of this, the lexical items and units of cape muslim afrikaans need to be recognized and a lexicon of cape muslim afrikaans formulated. the more extensively used distinctive lexical units of cape muslim afrikaans must be incorporated into standard afrikaans."
"following therefore the arabic order of syntax, ouwal-moereeds (first members); asal-niesaab (original taxable property) and waajieb-niesaab (necessary tax) should read moereed-ouwal; niesaab-asal; and niesaabwaajieb. this does not happen. with the creation of these composites the syntactical arrangements of afrikaans are observed."
it was through the creation of composites that neologisms greatly enriched the limited vocabulary of the cape dutch or cape afrikaans which the early cape muslim arabic-afrikaans writers had at their disposal. these composites in tum enriched the vocabulary of cape muslim afrikaans. all kinds of language combinations in the creation of such new words exist. here follow a few examples showing how the combination of lexical units from different languages creates new lexical units for cape muslim afrikaans:
"vierghiefnies van al die koewaat (evil) it is obvious that ghaniemand is deri ved from gha-nie-saam; ghawietenskap is derived from wietenskap, and galeikenis is derived from ghalaikienies, as they appeared in their nineteenth century usage in the passage above. it is also very evident that the sentence \"allah ta 'aalaa het nie ghaleikenis nie\" (god the most high has no partner), which is frequently used in bo-kaap madaris, has its origin in the nineteenth century."
"similarly, words such as rieziek-giever (giver of bounty), opbouwens (systematic construction), slamblie-er (used as a noun -slaughterer, but from the malayu verb slamblee, to which the afrikaans grammatical morpheme er is added to form the noun) are regularly encountered in the arabic-afrikaans publications! [cit] . these words, together with malayu words such as ghielap [xilap] (from the malayu leilat for light-ning), ghoentoem [gun tum] (from the malayu guntur -thunder); manniekamer (from the malayu kammar-mandi -bathroom) or simply djamang (toilet) or graa-na (eclipse) which had acquired a distinct afrikaans pronunciation, were already strongly bridged into the afrikaans of the cape muslims in the nineteenth century [cit] : 38) . then there are older words such as kasm (oath), listed by swaving (1830: 336) and memme (foster mother), listed by teenstra (1830: 348) which were already in use in the 1820s, and which show the tremendous influence of the ancestral languages of the slaves on the spoken afrikaans of the cape muslims. these words, which are still in daily use by a vast section of afrikaans speakers, have never been absorbed into the vocabulary of standard afrikaans."
"it is the normal practice for these learning formulations to be dictated by a teacher or wriuen on a chalkboard and transcribed by the student into a notebook called the koples boek. the student is then required to memorize the learning formulation and recite it from memory in the presence of the teacher on the next day. if the student recites the lesson proficiently, a new learning formulation or lesson is given and the process repeated. it is interesting to note how this process in itself lends two lexical items to cape muslim afrikaans."
"it was in the dutch linguistic milieu that afrikaans emerged. dutch, therefore, provided the essential core vocabulary of the afrikaans language. when the cape muslim arabic-afrikaans writers embarked on the process of writing their cape dutch mother tongue in arabic script, they sincerely believed that they were writing dutch. this is evident from the use of the dutch wij (wlli in arabic script) instead of the afrikaans ons in the passage from the 1860 koples hoek above. what they were in fact writing, as already noted, was a creolized dutch, a form of dutch which had already moved away from the pristine purity of the original language."
"~-.. apart from its acoustic nature and orthoepic practice, cape muslim afrikaans has some distinctive words which make its vocabulary different from standard afrikaans. some of these words existed in the vocabulary of cape afrikaans or cape dutch long before its standardization. words such as uiwe (onions), maskie (eventhough), ver-effe (a little while ago), and werksloon (good deeds) [cit] : 39ff), were already in use prior to the establishment in 1875 of the genootskap van regte afrikaners, the organisation which first actively promoted afrikaans as a language."
"from the translation of the matha il abi laith it is obvious that such lexical units as gienage, from the dutch diegene, instead of the standard afrikaans die ene; liewendag, instead of the standard afrikaans lewende; maker, instead of the standard afrikaans skepper -in reference to god; and afgekom instead of openbaar (revealed) [cit] . ta ha gamieldien also illustrates how willing the arabie-afrikaans writers were to create afrikaans abstract nouns by affixing the grammatical morphemes heid and skap to existing verbs. this is seen in such lexical units as strygeit, tweiwelheid and afriekenskap."
"cape muslim afrikaans, as a distinctive variety of afrikaans, only survived because it acted as a vehicle for the transmission of religious ideas. it was the medium of instruction in the islamic religious schools or madaris, the language for lectures and translations of the sermons in the mosques. its extensive use for religious purposes in tum strengthens its usage for social communication. but it was its use as the medium for written communication, although in the arabic script, which facilitated the dissemination and helped the perpetuation of its lexical inventory. the dissemination and perpetuation of its lexical inventory were further facilitated by the fact that its speakers became a reading community. [cit] the cape muslims were considered one of the most literate muslim communities in the world [cit] : 349 [cit] : 3) finally, i suggest that there is a need for a compilation of a comprehensive lexicon of cape muslim afrikaans. such a lexicon will take several researchers a considerable time to compile. it is my hope that this essay is a positive start in that direction."
"idioms and expressions very much constitute part of the lexical inventory of a language. in fact, eksteen (1984: 137) argues that the lexicon of a language is composed of its words, its morphemes and its idiomatic expressions. it is, however, in the creation of its idiomatic expressions that a language creatively utilizes its words and morphemes. thus, it is their creation of idioms and expressions which shows the ingenious creativity of the cape muslims with the perceived limitations of cape dutch or cape afrikaans. it is also their creativity with idioms and expressions which gives cape muslim afrikaans its humour. these idioms and expressions, despite their lack of sophistication at times, facilitate the flow of cape muslim afrikaans as an effective means of communication. hence to show their total adequacy, members of the cape muslim community will jokingly say:"
precision (p) and recall (r) are the commonly used evaluation metrics and these measures are used to evaluate the performance of the proposed system. the measures are defined as follows: the work used f-score as the combined measure of precision (p) and recall (r) for calculating accuracy which is defined as follows:
"the process of memorization is called faam-maak, a composite of two lexical morphemes from two different languages -faam, from malayu, and maak from afrikaans. as a verb, faam-maak is so strongly bridged into cape muslim afrikaans that it even takes on a past tense form, faamgemaak. the process of proficiency in recitation or recall is called lanja, an inherited malayu lexical item. thus in cape muslim afrikaans it is not uncommon to hear: hy het die storie goed faamgemaak, en is so lanja as hy dit vertel, dat jy nie wiet (weet) of dit die waarheid is nie."
"given the kind of linguistic situation the cape muslim community found themselves in, and considering the limitation of the language code at their disposal,the cape muslims had to exploit the communicative potential of this restrictive code to arrive at utterances which most appropriately satisfy their communicative needs. the speaker, langacker (1973: 54) tells us, has to make his language fit the situation, no matter how unusual. he therefore has, on occasions, to construct a phrase or a sentence in order to express a meaning which a single word is unable to express. it is with the creation of such phrases and sentences -which with regular use come to convey precise meanings as idioms and expressions -that linguistic creativity and diversity grows. in a sense, to be creative in language, one needs to be innovative. in this regard, the cape muslim community has indeed been innovative in its spontaneity with its idioms and expressions."
"reproduced by sabinet gateway under licence granted by the publisher [cit] .) switched to confonn to the afrikaans, rather than the arabic, syntactical arrangements. it is nonnal in arabic, as with the malayo-polynesian languages, for the noun to be followed by the adjective. thus one talks of sheikhun-kabfr (the great sheikh) or kitabun-aswadun (the back book) which, in a direct translation, would read: sheikh-great and book-black respectively."
"this paper deals with emotion recognition from face expressions. the method has been evaluated with facial expression database [cit] where the subjects expresses emotions such as normal, smile,disgust and surprise. all the images in the database are of size 63 x 63 pixels. gray level co-occurance matrix has been computed for all the images and second-order statistical texture features that are extracted from the glcm matrix is given as input to the support vector machine for classification. the rest of the paper is organised as follows : section 2 describes the feature extracion. expression"
"then, too, cape muslim afrikaans inherited vital lexical items from the malayo-polynesian family of languages which the ancestors of its speakers brought with them as slaves from south east asia. there seems to be an interesting pattern with regard to the borrowing and inheritance of lexical items and units."
"one of the arabie-afrikaans writers most creative with neologisms is sheikh abdullah ta ha gamieldien. [cit] translation of the matha 'il abi laith (the questions of abi laith), a philosophical exposition on the dogma of belief by the tenth century islamic philosopher nasr ibn abi laith al-samaqandi. to convey the philosophical meanings of the arabic concepts expressing the attributes of god, taha gamieldien embarked on an ingenious process of neology to create powerful afrikaans nouns. these nouns very effectively expound the perceived functional meaning of god as conveyed by the arabie concepts."
"there are, however, also other words which came into cape muslim afrikaans via the arabic-afrikaans publications. many of these words have a malayo-polynesian equivalent which seems to be preferred in the colloquial cape muslim afrikaans. thus the verb soembai-ing (malayu) is preferred to saltiti, the arabic verb for 'prayer'. similarly soeboeg (malayul as the name for the morning prayer, is preferred to arabic fajr. it is always poewasa (malayu) for the act of fasting, seldom soum, its arabic counterpart. we also talk of maskawie (malayu), instead of mahr, the arabic for 'dowry'. there are many such examples."
"poewa$a [puwasa] sadjie [sad'ci] (verb) samba [samba] (verb) slam at [slamat] soembai-ing (verb) soempa [sumpa] though malayu predominates as the language of origin of these inherited lexical units, it is also interesting to note the influence of sanskrit. this could be attributed to the tremendous contact which the malayo-polynesian languages had with hinduism prior to the advent of islam in the region; and the influence which sanskrit exerted not only on the writing traditions and alphabets, but also on the vocabularies of the languages of the south east asian archipelago (crawfurd 1820). these inherited malayo-polynesian lexical units, however, have been completely bridged into the spoken afrikaans of the cape musl~ms, but in this process of brid~ng they have also changed in. their pronunciation. they have become so afnkaans in sound that they, at times, are not immediately recognized by speakers of the language from which they were originally inherited."
"from the words and word constructions in the arabic-afrikaans publications, it becomes apparent that all three aspects of vocabulary building, which s.p.e. boshoff identified as \"erfgoed\", \"eiegoed\" and '1eengoed\" [cit] : 144), come into play. words were certainly inherited from their ancestral malayo-polynesian family of languages; words were also borrowed from arabic, english and dutch to extend their core vocabulary. then, too, there is definite evidence of concept construction. this creative process of neology helped to accurately project their theo-philosophical thoughts."
"our study differs from previous amplitude-modulated itds studies in two ways. first, the listening task and spatial hearing assessments were different, as previous studies measured either itd discrimination or intracranial lateralization of stimuli containing independent fine-structure and envelope-based temporal disparities. second, prior studies used periodic carriers with periodic modulators and 100% modulation depths. for speech, however, signals are more complex with envelope modulations that are shallower and less temporally consistent relative to the modulators use in fixed-frequency stimuli. in addition, the filtering by hrtfs (i.e., the frequency-dependent ilds) further affects the depths of the ongoing envelope temporal modulations between the ears. reducing the depth of envelope modulations has been shown to result in poorer itd thresholds [cit] . our findings lend support to this notion, as the temporal envelope cues available to the nh subjects listening to get vocoder simulations did not appear to provide sufficient information to produce accurate sound localization (figure 3, nh get panel). however, the similar patterns in errors across azimuthal locations suggest that the nh get stimuli produced the most comparable performance to bici localization in nh listeners."
"the first-order polynomial is the linear component and describes the slope of the relationship between absolute target and absolute error. there were significant linear effects for all of the vocoder conditions, except for the nh nu condition, meaning that as the absolute target angle increased, the absolute error increased. the nh n0 condition had a significantly larger linear effect compared with the bici ff model, as can be observed in the steeper slope (figure 3, figure 3 . observed data and multilevel regression model fits for effect of listening condition on absolute error across azimuthal locations. for each listening condition (panels), the across-subject average absolute error (point, mean) and standard error (error bars, ae se) are plotted as a function of the absolute target angle with the solid line representing the model fit. the bici ff model fit was treated as the reference and is plotted (thin line) on each of the panels displaying the nh listening conditions for visual comparison. table 3, condition:linear for statistical summary). apart from the differences in slope, there were differences in the degree of curvature which were captured by the second-order polynomial. this is the quadratic component and describes the degree of slope change (i.e., single inflection) in the data. the quadratic components of the nh nu and nh n0 conditions were significantly different than those from the bici ff data. the nh vas, nh get, and bici ff conditions were similar in that they all lacked a significant quadratic effect (see table 3, condition:quadratic for statistical summary)."
"to compare localization performance patterns between nh and bici listeners, a multilevel regression analysis [cit] was used to model and analyze the data. the pattern of errors (figure 2(a) ) and distribution of responses (figure 2(b) ) across target locations for each condition exhibited a rough symmetry moving from the central to lateral target locations on either side. due to this observation, the absolute localization errors were collapsed as a function of target deviance from the center speaker (0 ), that is, the absolute target angle. figure 3 plots the absolute error as a function of absolute target angle (mean and ae se) for all nh and bici localization data. these patterns of error across azimuth were modeled with third-order orthogonal polynomials (i.e., linear, quadratic, and cubic components) and fixed effects of listening condition (i.e., interactions on the intercept component) on all absolute target terms. the ff bici errors are plotted on the far left, and the model fit of the bici ff data are also plotted in each panel (figure 3, solid red line) . the model fit of the bici ff data were treated as the baseline model for comparison and parameters were then estimated for the vocoder conditions (see table 3 for statistical analysis using orthogonal polynomials, the intercept component corresponds to the overall average of the measure of interest [cit], which in this study was the absolute error. first, there was a significant effect of the bici ff condition on the intercept component, indicating that this condition resulted in an absolute error (21.17 ) that was significantly different from an absolute error of 0 (see table 3 ). next, the intercept components were compared across conditions. conceptually, this is roughly like a t test comparing the tested conditions to the baseline model [cit] . for example, the average absolute error for the nh vas condition was 11.87 less than the bici ff condition, which was found to be significant, and the average absolute error for the nh nu condition was 7.57 greater than the bici ff condition, which was also significant (see table 3 ). all the nh listening conditions resulted in significant intercept effects, which indicate that the average absolute error for the nh conditions was significantly different than the bici ff condition. that is, the average absolute localization error in bici users was significantly higher than the nh vas condition and significantly lower than all three of the nh vocoder conditions."
"the bond percolation is a generic approach for the sir model and can be instantiated to any specific diffusion model. its advantage over other methods is that it allows us to estimate the influence degree of all the nodes in the network simultaneously regardless of the size of network. it does not require any approximations or assumptions to the model to improve the computational efficiency, e.g., small diffusion probability, shortest path, maximum influence path, etc., that were needed in the existing approaches. we instantiated it to the independent cascade (ic) model, but the same technique can be applied to other instantiations, e.g., linear threshold (lt) model."
"comparing the performance between the two noisevocoded conditions (figure 3, nh nu and nh n0 panels), the nh nu stimuli resulted in extremely poor localization across all azimuthal locations. in contrast, localization of the nh n0 stimuli was biased toward the speaker location at 0 (figure 2 (a) and (b)) and rapidly decreased at more lateral positions. the nh nu data are in agreement with previous psychoacoustical studies, which have demonstrated that as the signal correlation between the ears decreases, the perceived sound image becomes more diffused and lateralization abilities deteriorate [cit] . in addition to this, neural itd encoding of broadband signals, such as speech, depends critically on a high amount of binaural coherence in the spectrotemporal features of the acoustic signals [cit] . [cit] investigated the effects of binaural decorrelation on neural spatial coding and behavioral responses to spatial cues in the barn owl. localization performance in barn owls for noise burst containing itds (and no ilds) rapidly deteriorated as the interaural correlation of the signals presented was decreased [cit] . furthermore, responses of itd sensitive neurons in the owl's optic tectum declined rapidly as interaural decorrelation increased, thus these neurons exhibited less itd tuning to the decorrelated stimuli. similar findings have been shown in human cortical imaging studies [cit] ). [cit] found that activity in heshl's gyrus increased with increasing interaural correlation of white noise stimuli and that posterior auditory regions also showed increased activity for the high coherence stimuli, primarily when sound localization was being performed. thus, the lack of interaural correlation in the nh nu stimuli may explain why these stimuli were difficult to localize."
"many different approaches have been presented to render trees in real-time. most often simple billboards or impostors [ssk96] are used, or automatically generated billboard clouds [ddsd03, gssk05] which are sets of billboards that better preserve occlusion and parallax effects. however, these representations are well suited for distant objects and trees, but they are typically over simplified and close views reveal the low quality. it is also not possible to achieve coherent shading of the scene or to adapt the level of detail smoothly and without noticeable artefacts due to the planar nature of billboards [lest06] ."
"interestingly, there was noticeable intersubject variability with regard to how nh subjects performed on the same vocoder conditions. to illustrate the various patterns of response distributions observed for different subjects, we show the nh get condition, which was found to be most comparable to bici ff performance (see earlier). data from these subjects were chosen to depict the variability observed in performance across nh subjects for the vocoded stimuli (see nh get, figure 4(b) ) that coincided with the variability observed in bici ff response distributions. similar response distributions are plotted in each column. for example, the two bici subjects (figure 4(a) ) and the two nh subjects (figure 4(b) ) in the first column distributed responses to more central locations. the center column (figure 4(b) and (e)) shows subjects whose majority of responses were grouped around spatial locations on either side. the column on the far right (figure 4(c) and (f)) shows distribution patterns in which the majority of responses grouped into three spatial locations (i.e., left, center, and right). the similar variability in response distributions for both nh subjects listening to nh get stimuli and for bici subjects listening in the ff suggest that individuals distribute responses differently given degraded auditory input whether the signal is acoustically degraded or presented via electrical stimulation."
"basic models of information diffusion over a network often assume that each node has three states, susceptible, infective, and recovered from the analogy of epidemiology. a node in the susceptible state means that it has not yet been influenced with the information. a node in the infective state means that it is influenced and can propagate the information to its neighbor nodes. a node in the recovered state means that it can no longer propagate the information once it has been influenced with the information, i.e., immune. the sir model is a typical one among such basic models and well exploited in many fields [cit] . to be more concrete, the sir model is a discrete-time stochastic process model, and assumes that a susceptible node becomes infective with a certain probability when its neighbor nodes get infective, and becomes subsequently recovered. in particular, it is known that the sir model on a network can be exactly mapped onto a bond percolation process on the same network [cit] ."
"in summary, the rep technique computes the set of all the redundant edges ep q m, and replaces the set of edges on q m as follows:"
"the large increase in overall localization errors for nh subjects listening to vocoded stimuli occurred whether the original signal's acoustic tfs was replaced with uncorrelated (nh nu ), correlated (nh n0 ), or synchronized-pulsatile (nh get ) stimulation. replacing the acoustic tfs during ci speech encoding creates binaural stimulation in which the tfs-itds that are presented to the auditory system do not correspond with the ilds presented, and results in conflicting acoustical cues (i.e., each cue points to a different location). studies show that when presenting the nh auditory system with conflicting itd and ild cues via vas techniques, both the neural representation of these cues [cit] and localization performance [cit] become altered compared with when the cues are consistent with how they are naturally experienced. in the present study for instance, the acoustic carriers in the vocoder stimuli would still activate the neural circuitry responsible for extracting itds; however, as they do not contain the acoustically appropriate itd information, a neural representation of inconsistent itd/ild cues is more than likely to be encoded. in fact, binaural interference may occur in which the acoustically inappropriate lowfrequency itds would dominate an individual's perceived lateral position [cit] and may be responsible for the inaccurate localization observed in both bici users and nh subjects listening to vocoded stimuli. although auditory deprivation has been shown to result in degraded sound localization [cit], the extent to which degraded neural circuitry in bici users is responsible for poor localization has not been previously studied. because we observed similar localization deficits in individuals with intact auditory systems listening to bici simulations (figure 4), we posit that horizontal-plane localization deficits are attributable to the signal processing in cis more so than the compromised auditory systems of bici users."
"here, \"utadahikaru\" is the twitter account of hikaru utada who is a japanese american singer known as one of the most influential artists in japan. these results demonstrate that the influence degree centrality can serve as a novel measure that extracts influential nodes in terms of information diffusion which are not identified by existing measures."
"for the nh n0 stimuli, although the signals were completely correlated between the ears, the dominant lowfrequency tfs-itd cues contained in each of the stimuli across all spatial locations pointed to the center. thus, the pattern of errors (figure 3, nh n0 panel) for this condition, that is, the higher degree of errors for more lateral locations, can be explained because subjects on average perceived the sound image to be coming from more central locations (figure 2(b) ). in the studies mentioned earlier [cit], itds were applied to stimuli with various degrees of correlation between completely uncorrelated (n u ) and correlated (n 0 ) noise tokens. here, we were also able to investigate whether applying individualized hrtf filtering (i.e., containing all the natural acoustical cues) and physiologically meaningful temporal envelopes (i.e., speech) to the n u and n 0 noise carriers could produce accurate sound localization. previous studies have investigated the contribution of envelope itds cues to intracranial lateralization in nh listeners [cit], 2011 [cit] ) . [cit] also reported that auditory model predictions of localization accuracy based solely on envelope modulations were worse than the predictions based on fine structure. our data are in agreement with this study, demonstrating that itd cues in the envelopes of ff speech stimuli are not sufficient in moving the nh n0 sound image across the spatial locations."
"trials were self-initiated by pressing a button on a touch screen monitor placed in front of the subject and positioned such that it had a minimal effect on the acoustic stimuli. following stimulus presentation, a graphical user interface (gui) with an image of an arc representing the arc of loudspeakers was displayed on the touch screen monitor. subjects indicated their response by placing a digital marker anywhere on the arc image corresponding to the perceived sound source location. to facilitate perceptual correspondence between the spatial locations in the room and the arc image on the touch screen, visual markers were placed at 45 increments both along the curtain in the room and on the gui."
"nonvocoded stimuli. nh listeners were tested with stimuli consisting of 10 monosyllabic consonant-nucleus-consonant (cnc) words spoken by a male talker. each speech token (beam, cape, car, choose, chore, ditch, dodge, goose, knife, and merge) was filtered by the hrtf measurements to create vas stimuli for each spatial location. these stimuli provided a control condition for comparison to performance measured with the vocoded stimuli. to make comparisons of sound localization performance between nonvocoded and vocoded stimuli, test stimuli were low-pass filtered to match the bandwidth of the vocoder. a fourth-order butterworth filter with an 8-khz cutoff frequency was applied to the original speech stimuli prior to hrtf-filtering for vas presentation. subjects confirmed that the perceived loudness of the vas stimuli presented through the headphones matched the loudness of ff stimuli presented from the loudspeaker array. for the bici listeners, ff sound localization performance was measured using four bursts of pink noise each with a 10-ms rise-fall time and 170 ms in duration. [cit] ) . vocoded stimuli. vocoded stimuli were generated by processing the vas stimuli through an eight-channel vocoder, spanning a range of 150 to 8000 hz. the stimuli were band-pass filtered using fourth-order butterworth analysis filters with evenly spaced center frequencies as calculated using the greenwood function [cit] . the center and corner frequencies of the analysis filters used in the vocoders are shown in table 2 . signals were then half-wave rectified and low-pass filtered at 50 hz by a second-order butterworth filter for envelope extraction and sideband removal. the envelopes of each band were then used to modulate one of three different acoustic carriers (identified by the subscript): uncorrelated noise ( nu ), correlated noise ( n0 ), or gaussian-enveloped tones ( get ). the modulated carriers for each frequency band were then summed together separately for left and right channels to create the final stimuli. the carriers used in this experiment were chosen to simulate different aspects of cochlear implant stimulation. the nh n0 and nh nu stimuli were intended to simulate the presence and absence of coordinated binaural stimulation, respectively, while preserving level and envelope cues imposed by the hrtf filtering of the stimuli. wideband noise carriers were generated independently for each channel, and prior to envelope modulation, were band-pass filtered with the same analysis filters described earlier. for the nh n0 stimuli, the same noise carrier was used for both ears. for the nh nu stimuli, different noise carriers were used for each ear. the nh get pulse trains were used to simulate ci electrical stimulation. a 100-hz get pulse train centered at each of the center frequencies in table 2 [cit], where the bandwidth of the gaussian pulse was equal to the bandwidth of the corresponding band-pass filter. the left and right signal envelopes were used to modulate the get pulse train. it should be noted that the same get pulse train was used to carry the left and right signal envelopes, which means the timing of the envelope and tfs of the get pulses were synchronized between the left and right ears before modulation with the speech envelope. hence, the left and right ear signals varied only by the envelope cues extracted from the vas stimuli."
"we, third, examined how the performance of the two pruning techniques changes as the network structure changes using many different networks that are synthetically and systematically generated by extending the ba and cnn method in addition to the verification by the two real networks. we confirmed that the rep technique is effective when the quotient graph (a dag obtained after decomposing the graph realized by applying the bond percolation to the original directed graph) has a large number of feed forward motif patterns and the mcp technique is effective when the quotient graph has a large number of components with in-degree 1 or out-degree 1 and a small number of components of large size. in general the mcp technique is more effective than the rep technique. use of both techniques is always better than the single use of either techniques."
"the experimental approaches reported here aimed to simulate the effects of various aspects ci speech encoding and bilateral stimulation on horizontal-plane sound localization. one issue may be that the independent signal processing, variable channel selection, and highrate electrical stimulation introduces interaural decorrelation into the signals presented to the auditory system, in addition to impeding the ability to deliver itd information [cit] . a likely consequence is that the spectrotemporal representations of acoustic signals are not being presented at the two ears with a high amount of binaural coherence. several studies in nh listeners have reported that a reduction in the interaural correlation is perceived by the listener as a broadening of the sound image [cit] . the nh nu stimuli in the present study simulated this potential reduction in interaural correlation due to independent signal processing and variable channel selection. additionally, the nh n0 stimuli tested whether localization could be improved by creating of a more punctuate sound image via the temporal synchronization of the spectrally random carriers across the ears."
"the proposed method inherits the good feature of the bp method. it is a generic framework to estimate the influence degree centrality under the sir model setting without need for any approximations and assumptions. with this improved efficiency, it is now possible to estimate the node influence of every single node of a network with one million nodes and analyze the existence of epidemic threshold. we further confirm that the inluence degree centrality can identify nodes that are deemed indeed influential which are not identifiable by the existing centrality measures."
"we, second, demonstrated that the proposed method can estimate the epidemic threshold of the ic model even for a huge twitter network with 1000k nodes in reasonable time by examining the relation between the diffusion probability and the average influence degree, and showed that the epidemic threshold depends on network structure and for the two real-world networks, we tested the twitter network spreads information more easily than the cosme network. further, it is confirmed that the nodes identified as influential by the influence degree centrality based on the sir model are not necessarily the same or similar to those identified by the other existing centralities, and the influence degree centrality can identify those nodes that are deemed indeed influential but are not identifiable by the other existing methods."
"in this paper we present a rendering technique for complex botanical scenes based on pruning. pruning techniques (stochastically) reduce geometry by simply excluding some parts of the model, for example leaves, from the rendering and correcting contrast and the total rendered area by scaling the remaining leaves [chpr07] . we improve upon previous methods in several respects:"
"the paper is organized as follows. we briefly explain the related work in sect. 2 and the bp method in sect. 3. we then introduce the proposed method (rep and mcp techniques) in sect. 4. the experimental results for real networks are given in sect. 5, and the performance analysis for synthetic networks is given in sect. 6. we conclude the paper in sect. 7 summarizing the main achievement and future plans."
". then, we can summarize the dcnn method as an algorithm which repeats the following steps l times from a single node and an empty set of links:"
"we, first, tested our algorithm using two real-world networks, one with 40k nodes and the other with 1000k nodes. the experimental results confirmed that the new pruning techniques improve the computational efficiency by an order of magnitude over the existing bond percolation method which is already three orders of magnitude faster than direct monte carlo simulations."
"to optimize the rendering priority we need to determine which parts of a model are close to the boundary and will potentially be part of the silhouette, and which regions exhibit a high or low density of geometry."
"as conventional centralities, we examined the betweenness centrality, the closeness centrality, the hub centrality, and the pagerank centrality for network g. here, the betweenness betw(v) of a node v is defined as"
"twenty subjects participated in this study. ten nh subjects had audiometric pure-tone thresholds below 15 db hl for octave frequencies spanning 250-8000 hz with no asymmetry in hearing thresholds exceeding 10 db at any of the frequencies tested and were native speakers of american english. subjects were either students or staff at the university of wisconsin-madison and were paid for their participation. ten postlingually deafened bici users with ci24 and ci512 family of implants, and who used freedom or n5 speech processors (cochlear ltd., sydney, australia), participated in this study. all subjects had a minimum of 1 year of bilateral implant experience. bici subjects traveled to the university of wisconsin-madison for testing and received payment, per diem, and were reimbursed for all travel-related costs. the profile and etiology of the bici users are shown in table 1 . all experimental procedures followed the regulations set by the national institutes of health and were approved by the university of wisconsin's human subjects institutional review board. the bici subjects were tested only on ff sound localization to compare their performance with that of the nh subjects listening to vocoded stimuli. thus, no hrtf measurements were made for the bici users, and they were not tested with any vas or vocoded stimuli."
"for this we analyse the occlusion of each tree by testing the visibility of a set of sample points distributed on its iso-surface. in principle, there are various possibilities to perform the visibility test, for example ray casting, or using some form of pre-computed visibility information. to facilitate real-time rendering without pre-computation, we use an image space approach relying on the depth buffer of the camera image only. obviously, this depth buffer is not available before actually rendering the geometry. however, if we assume smooth camera movement we can exploit frameto-frame coherency and test the visibility of sample points using the depth buffer and transformation of the previous frame. for abrupt movements, or sample points that are projected outside the viewport, we conservatively assume full visibility and thus render the models at possibly higher detail than actually necessary."
"for each nh subject, individual hrtf measurements were made for the 19 loudspeaker locations, using a blocked-ear technique (møller, 1992) . subjects were asked to face the front (i.e., speaker position 0 ) and to remain stationary during each stimulus presentation. golay codes (200 ms long, five repetitions) were used as probe signals for hrtf recordings, and the in-ear responses were recorded by a blocked-meatus microphone pair (headzap binaural microphones, ausim, mountain view, ca) placed in the entrance of each ear canal. microphone output signals were amplified (mp-1, sound devices) and recorded using a tdt rp2.1 at 48 khz. traditionally, hrtfs are defined with reference to the sound pressure in the middle of the head with the listener absent (møller, 1992) . to obtain an hrtf for a particular source location, the microphone recordings at the ears can be divided by the response measured with only the microphone at the location in the center of the loudspeaker array. this effectively removes the loudspeaker frequency characteristics in hrtfs. however, in this experiment, the loudspeaker characteristics were not removed from the digital filters used to synthesize the vas stimuli because we were interested in preserving these characteristics for a direct comparison of ff localization performance to headphone presentations. thus, the loudspeaker frequency characteristics were not removed from the hrtfs."
"the dynamical behaviors of the sir model have been widely studied in physics literature. one such important analysis is to examine the epidemic threshold p * g of a network g, where most nodes of the network remain uninfected (i.e., a small outbreak) if the probability that a susceptible node receives information from its infective neighbor is smaller than p * g, and the number of infected (recovered) nodes rapidly increase (i.e., a large outbreak) if the probability becomes greater than p * g [cit] . we must be able to estimate node influence very efficiently to make this kind of analysis feasible because we need to estimate the average influence degree. in this paper, we focus on the node influence based on the sir model, and regard it as one of the centrality measures and refer to it as the influence degree centrality for convenience sake."
"the present study created a realistic bici simulation in nh listeners by combining virtual acoustic space (vas) and vocoder techniques, and then directly compared this nh performance with that tested in bici users in the same ff testing environment. we measured head-related transfer functions (hrtfs) for each nh subject, and individualized vas speech stimuli were created for localization testing to ensure comparable localization performance in the control condition. each subject's vas stimuli were then processed using either a noise or get vocoder, and localization performance was measured. the baseline data from this work have the potential to lead to investigations of the numerous additional factors that might affect bici sound localization, such as electrode mismatch and spread of current [cit] while circumventing the confounding variable degrees of neural degradation associated with bici users."
"in this paper, we introduced precision and recall as a measure of quality for rendering complex geometry with pruning. we further improved on previous methods by applying model-specific geometry reduction and optimized scaling as well as view-optimized pruning. we evaluated our method by means of a user study that indicates a considerable improvement compared to naive and purely stochastic pruning. however, our work also raises new questions. one interesting direction of future research is to consider more than just correct and incorrect pixels in pr, for example by accounting for deviations in the normals, measuring contrast and colour differences, or to evaluate how visible differences predictors can improve the measure and whether their use amortizes."
"considering the viewing distance and thus the projected size of a model is of course one important aspect for choosing the geometry level. however, occlusion also has impact on the required detail: partially occluded trees, or trees that are completely surrounded by others, do not contribute considerably to the scene's appearance and thus should be rendered using less geometry. in this section we show how we can determine occlusion of trees at run-time, and control our rendering accordingly."
"we employed two large social networks, where all the networks are represented as directed graphs. here, we adopt the notation for a link in which the link creator is the target node in order to emphasize the direction of information flow."
"were smaller for both nh listeners (vocoded conditions) and bici users. overall, the main finding of the multilevel regression analysis was that the nh get condition produced localization performance that was most similar to that measured in bici users."
"1. simulated ci speech encoding resulted in large sound localization deficits in nh listeners, and overall errors were comparable with those measured in bilaterally implanted patients. 2. among the vocoders used to process free-field speech envelopes, the get vocoder produced the most similar patterns of localization error across azimuth to those observed in bici users. 3. although these data were obtained with ci simulations, they nonetheless lend support to the notion that ci speech encoding in the present day bilateral listening mode contributes to sound localization difficulties in bici users. the crux of this finding assumes that the bilateral vocoder simulations described in this study approximate the interaural cues available to the binaural circuits of bici users; however, the integrity with which binaural cues are preserved at the level of binaural neural circuitry is currently unknown, and more than likely varies across patient and devices type. 4. nh listeners exhibited a similar intersubject variability in error patterns to that observed in the bici users, suggesting that individuals employ different strategies when identifying a sound source location whether auditory signals are degraded acoustically or provided electrically. 5. future studies using the techniques presented here could efficiently investigate the potential success of novel sound encoding strategies aimed at improving spatial hearing abilities in bilaterally implanted patients."
"the second one is a network extracted from a set of message posts from \"japanese twitter\", 5 24, 2011), when the massive earthquake and consequent tsunami in eastern japan occurred on march 11, 2011. we used the network constructed from the follower links between these users, which resulted in a network consisting of 1,088,040 nodes and 157,371,628 directed links. we refer to this huge network as the twitter network."
"cochlear implants (cis) are used at increasing rates to provide hearing to individuals with severe-to-profound hearing loss. many patients receive bilateral cochlear implants (bicis) in an effort to improve spatial hearing abilities, such as sound localization and speech understanding in noisy environments, relative to the single-ci listening mode. numerous free-field (ff) studies have established that compared with unilateral ci use, bilateral cis improve sound localization accuracy along the horizontal plane [cit] . [cit] reported that root mean square (rms) errors along the horizontal plane for 17 postlingually deafened adult bici users were overall 30 lower for bilateral implant use compared with unilateral use. other studies have also shown similar effect sizes. despite the added benefit of having two implants, bici users still demonstrate large deficits in spatial hearing performance compared with normal-hearing (nh) listeners [cit] . [cit] reported that overall errors for adult bici users were on average 29 compared with the 7.6 observed for nh listeners, demonstrating that bilateral stimulation alone does not restore sound localization abilities. such a gap in localization performance could arise from a number of fundamental differences between nh listeners and bici users; however, investigating possible sources for these localization deficits has been complicated by the variability in performance across bici patients due to numerous factors. variable periods of auditory deprivation can result in differing amounts of neural degeneration [cit], and human temporal bones studies have demonstrated that the extent of auditory nerve survival can vary significantly among cochleae [cit] . for bici patients, such issues are further complicated by the likelihood of asymmetrical neural degeneration between the two ears, as many patients may undergo hearing loss at different rates between the two ears. despite extensive research on horizontal-plane sound localization in bici users, little is known about the relative contributions of degraded neural circuitry."
"one of the more interesting findings of this study was that the bici and vocoder conditions exhibited an additional inflection that was not observed in the nh vas model. this is captured by the cubic component (i.e., third-order polynomial) and indicates the degree of a second inflection in a curve. specifically, the cubic component describes the dip in the response curve around the 60 target angle, which did not emerge in the nh vas condition (figure 3, nh vas panel compared with all other panels). this indicates that the error rates proximal to 60"
"studies of the structure and functions of large complex networks have attracted a great deal of attention in many different fields such as sociology, biology, physics and computer science [cit] . it has been recognized that developing new methods/tools that enable us to quantify the importance of each individual node in a network is crucially important in pursuing fundamental network analysis. networks mediate the spread of information, and it sometimes happens that a small initial seed cascades to affect large portions of networks [cit] . such information cascade phenomena are observed in many situations: for example, cascading failures can occur in power grids (e.g., the august 10, 1996 accident in the western us power grid), diseases can spread over networks of contacts between individuals, innovations and rumors can propagate through social networks, and large grass-roots social movements can begin in the absence of centralized control (e.g., the arab spring). understanding these phenomena involves dynamic analysis of diffusion process. thus, the node influence with respect to information cascade is a useful measure of node importance, and it is different from the existing centralities because diffusion dynamics are involved."
"our immediate future work is to extensively evaluate the proposed method for various instantiations of the sir framework including the lt model by using large real networks in a variety of fields. needless to say, it is also necessary to mathematically clarify the performance difference between the proposed method and the existing bp method in terms of computational efficiency. our results obtained by the synthetic networks has laid a basis toward this direction. in several real-world networks, there exist phenomena in which the sis model is more suitable than the sir model [cit], where every node is allowed to be activated multiple times. it is known that the sis-type independent cascade model on a network can be exactly mapped onto the ic model on a layered network built from the original network [cit] . thus, note that the proposed method developed for the sir setting can also be applied to the sis setting. our future work includes evaluating the proposed method in the sis framework."
"recall is the quotient of correctly identified items (true positives) and all relevant items (sum of true positives and false negatives). again translated into this scenario: the ratio of correctly set pixels and the number of correctly set pixels plus the number of pixels that should have been rendered, but which are not covered by the pruned model. thus, pr are defined as (true positives tp, false positives fp and false negatives fn): table 1 gives an overview of the relevant pixels sets, which are shown in figure 1 for a simple example. more formally, we denote the set of all pixels covered by the original model as p orig, and the set of pixels rendered for the simplified model as p simplified and thus get:"
"it should also be noted that the hrtf recording procedures used in the current study may not be representative of the bici condition for most patients, as the majority of ci processors use microphones that are placed behind the ear. however, the aim of the present study was to exclusively investigate the effect of ci speech encoding on nh sound localization. as such, the hrtf measurements made with microphones placed in the ears ensured that individual-specific acoustical cues for each subject were intact prior to vocoder processing. additionally, the acoustical cues captured by microphones in the ear should be natural for nh listeners, so no adaptation to these cues or training was required for the vas stimuli."
"the variability in performance observed for both bici and nh listeners (figure 4) suggests that the localization strategies used by individuals are different, whether auditory signals are degraded acoustically or provided electrically. recent studies involving sound source identification (i.e., ability of listeners to identify objects from the sound of impact) in quiet have shown that listeners regularly use different listening strategies that result in similar performance accuracy, but for different levels of variability in performance [cit] . the current study suggests a similar notion for sound localization of degraded auditory signals. for example, bici subjects icf, ico, and iby (figure 4 (a) to (c)) had similar overall rms error ($ 25 -26 ), but very different response distributions. although such intersubject variability is often attributable to the fact that these listeners used bicis for hearing, it was observed that nh listeners also exhibited similar variability when listening to vocoder processed stimuli. for instance, nh subjects stl and taq (figure 4(d) and (e), respectively) had similar overall rms errors ($ 38 -39 ), but the errors were accounted for by very different error distributions. while subject stl responded to mostly central locations, taq distributed responses around left and right locations. such observations indicate that the intersubject variability observed for bici users may not solely reflect factors that are often considered to be the root of localization errors, such as peripheral factors, but may be the result of individuals employing different strategies for making decisions about the location of a sound source when given degraded auditory input."
"for comparisons with previous bici ff studies, sound localization performance was evaluated by computing the overall rms error for all 190 trials per condition. assuming a uniform distribution of random responses (i.e., guessing), chance performance was calculated to be 75.6 within the full range of responses and 39.7"
"the get vocoder was used to simulate the electrical pulse trains delivered during ci stimulation. similar techniques to those reported here have been used previously to test sound localization in the median plane [cit] . given the extensive literature on lateralization/discrimination of itds in amplitudemodulated stimuli [cit], one could speculate that the temporal modulations of speech envelopes would provide an additional cue for sound localization in the horizontal plane. although the ability of bici users to perform sound localization tasks based solely on envelope itds has not been investigated directly, their ability to discriminate and lateralize such stimuli has been studied [cit] . [cit] showed that the best envelope itd thresholds in bici users were on the order of 150 ms and that envelope itds could induce monotonic changes in the lateralization of the auditory image. it should also be noted that the itd thresholds measured for click trains were significantly lower than for speech tokens. however, previous studies do not indicate that envelope itd sensitivity will translate into accurate localization of ff sound sources."
"the authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: this study was supported by grants from the nih-nidcd (r01 dc003083 and r01 dc01049) and in part by a core grant from the nih-nichd to the waisman center (p30 hd03352)."
"rendering of natural scenes with vegetation as rich as in the real world has been a motivation of computer graphics research ever since. the complex visual appearance and the inhomogeneous structure of botanical objects makes real-time rendering of large scenes a challenging task that extends to this day. the obvious main reason is the tremendous amount of geometry that is needed to represent trees and plants. storing as well as rendering such objects with full detail is beyond the capabilities even of modern graphics hardware. however, even if processing and rendering the data were possible, then the small sub-pixel details due to the complex geometry can still cause aliasing artefacts."
"developing efficient methods that enable us to find influential nodes in a social network is a fundamental problem in social network analysis, and many studies have been made on this problem."
"for rendering botanical models, most real-time applications resort to a representation with relatively few textured polygons recreating the original model. for video games these models are often created manually, although billboard clouds [ddsd03] can be used to obtain such reduced models automatically. note that these representations do not provide a 'continuous' level of detail and switching between different levels is prone to popping artefacts. we compared our results to billboard clouds by evaluating the rendering quality according to our measure q (section 2.5). we observed that the rendering quality varies strongly with the view direction when using billboard clouds, and significantly less with our optimized pruning. figure 11 shows this comparison where the parameters of our pruning are adjusted to match the average quality of a billboard representation. note that another applications of our pr-measure can be the billboard cloud generation itself, where it can be used to identify bad views for which the billboard representation needs to be improved."
"localization performance measured here for the bici subjects was comparable to previously reported bici data [cit] . [cit] reported an average overall error of 29.1 ae 7.6 for 22 bici users localizing speech stimuli. [cit] reported overall localization errors of 28.4 ae 12.5 for 17 postlingually deafened bici users. more recently, localization of virtual sound sources has been reported for five bici users with an average precision error of 23.4 ae 2.3 for noise stimuli roved at a similar level ( ae 5 db) to our study . the average across-subject rms error and response variability measured in bici subjects in the present study (27.9 ae 12.3 ) is consistent with these previous findings. interestingly, despite all the disadvantages that bici users face (i.e., independently functioning devices, reduced spectral information, current spread, electrode mismatch, varying etiologies, and amounts of neural degeneration), the average rms for these listeners was lower than the nh average rms for all three vocoder conditions. one possible reason for these lower rms errors could be the stimuli used to test bici localization. the pink noise stimuli may have provided additional directional information, and were repeated four times, providing multiple looks. another reason for the lower overall rms error in bici users may be because these listeners had more experience with degraded auditory input, as all the bici subjects had a minimum of 1 year of listening experience with their cis, whereas nh subjects were tested acutely. [cit] demonstrated that nh localization along the median plane while listening to get-vocoded vas stimuli significantly improved with training, although the performance was not as accurate as listening to unprocessed stimuli. thus, it is reasonable to hypothesize that with more experience, the nh subjects in this study could possibly exhibit performance that would be similar to the overall rms errors measured in the bici subjects."
"the first one is a network extracted from \"@cosme\", 4 a japanese word-of-mouth communication site for cosmetics, in which each user page can have fan links. a fan link (u, v) means that user v registers user u as her favorite user. [cit] and extracted a large weakly connected network consisting of 45,024 nodes and 351,299 directed links. we refer to this directed network as the cosme network."
"precision and recall (pr) are well-known statistical classifications or measures for exactness and completeness. they are widely applied in the domain of information retrieval and are closely related to sensitivity and specifity to measure the performance of binary classification algorithms, such as support vector machines and bayesian networks [rij79] ."
"although estimating influence degree centrality for large networks is a time-consuming and difficult task, the proposed method enabled us to approximately calculate the influence degree within a reasonable time even for huge social networks. thus, for the huge twitter network, we evaluated whether or not the influence degree centrality can actually provide a novel concept in comparison with conventional centralities."
"the mcp technique recursively performs pruning components with in-degree 1 or out-degree 1 in the network g m . here, we define the sets of components with in-degree 1 and out-degree 1 by eqs. (5) and (6), respectively:"
"several centraility measures have been proposed in the field of social science. the well-known centrality measures include, but not limited to, degree centrality [cit], eigenvector centrality [cit], katz centrality [cit], pagerank [cit], closeness centrality [cit], betweenness centrality [cit], and topological centrality [cit] . however, some centrality measures (e,g., closeness centrality and betweenness centrality) require to use the global structure of a network for computing the value of each node, and their computation become harder as the size of a network increases. thus, several researchers try to efficiently approximate such centralities [cit] . notable feature of the existing centrality measures is that they all are defined only by network topology. node influence is different from them in that it is defined through dynamical processes of a network. therefore, it can provide new insights into information diffusion phenomena such as existence of epidemic threshold which the topology-based centrality measures can never do."
"rendering complex scenes is only possible if we reduce the level of detail for distant trees and only render with high quality when trees are close to the camera. using the prscore from section 2.2, we define the quality q of a rendering as the distance of the pr-vector to the optimal value (1, 1):"
"in the next subsection, we explain experimental results for computation time. all our experimentation was undertaken on a single pc with intel(r) xeon(r) cpu x5690 @ 3.474 ghz, with 198 gb of memory, running under linux."
"in our proposed method, the rep technique is applied before the mcp techniques, because it is naturally conceivable that the rep technique increases the number of components with in-degree 1 or out-degree 1. clearly we can individually incorporate these techniques into the existing bp method. hereafter, we refer to the proposed method without the mcp technique as the rep method, and the proposed method without the rep technique as the mcp method. since it is difficult to analytically examine the effectiveness of these techniques, we empirically evaluate the computational efficiency of these three methods in comparison with the existing bp method."
"for responses that were within the correct left or right hemisphere. the vas techniques employed here were ); this is consistent with previous findings that localization of ff sounds is generally better than for virtual sounds [cit] . additionally, the average rms error for the nh vas condition here was comparable to the average lateral rms error (12.4 ae 2.2 ) recently reported for nh subjects listening to vas stimuli . these findings indicate that the vas techniques reported here provided an adequate representation of ff stimulus presentation. figure 1 shows the across-subject average rms error (bar, mean) and standard deviation (error bars, ae sd) for nh subjects in the four conditions tested as well as the bici subjects. there was a clear increase in the average rms error between the nh vas (11.2 ae 1.7"
"the bp method was shown to be very efficient, three orders of magnitude faster than direct monte carlo simulation in computing the node influence degree [cit] . our contribution is to have made the influence degree centrality we extensively evaluate the proposed method using two large real social networks, compare the computation time, 2 and show that the proposed method significantly outperforms the existing bp method. the mcp technique is found to be more effective than the rep technique. use of both techniques is always better than the single use of either technique. we further examine how the performance of the two pruning techniques changes as the network structure changes. for this purpose we extend the ba and cnn methods, and systematically generate synthetic networks with different structure. we reconfirm the above results and identify the important factors that are decisive in controlling the performance."
"precision is defined as the ratio of correctly identified items (true positives) to both correctly and incorrectly identified items (sum of true positives and false positives). in our case, when rendering a pruned model it is the ratio of pixels that are correctly set, that is they would have been rendered for the full-detail model as well, and the total number of set pixels."
"measurement of hrtfs and behavioral sound localization testing were conducted in the same sound booth. the booth had internal dimensions of 2.90 â 2.74 â 2.44 m (iac, rs 254 s), and additional sound-absorbing foam was attached to the inside walls to reduce reflections. a tucker-davis technologies (tdt) system 3 was used to select and drive an array of 19 loudspeakers (cambridge soundworks) arranged on a semicircular arc of 1.2 m radius. loudspeakers were positioned in 10 increments along the horizontal plane between ae 90 and were hidden behind a dark, acoustically transparent curtain. subjects sat in the center of the array with their ears at the same height as the loudspeakers. for ff localization testing, stimuli were calibrated to output at 60-db sound pressure level (spl) using a digital precision sound level meter (system 824, larson davis; depew, ny) placed at the center of the arc where the subject's head would be positioned. the vas stimuli were presented via in-ear headphones (er-2, etyomtic research) using the tdt system 3 with a 48-khz sampling rate and were calibrated so that the perceived output level of the headphones matched that of the ff presentations. headphone calibrations were made using the sound level meter and an artificial ear coupler (2-cc coupler, g.r.a.s.; larson davis, depew, new york). all stimulus presentations and data acquisition were done using custom matlab software (mathworks, inc., natick, ma). all analyses were carried out using r software version 3.0.2."
"we view the dynamic process of information diffusion as an important ingredient to evaluate the importance of a node in a social network and consider that the node influence degree shares the same role that other existing topologybased centrality measures have. unlike the existing centrality measures, the influence degree centrality is not easily computable because it is defined to be the expected number of information spread. we proposed a method that can estimate the influence degree of every single node in a large network simultaneously under the framework of sir model setting. more specifically, we proposed two new pruning techniques called redundant-edge pruning (rep) and marginal-component pruning (mcp) on top of the existing bond percolation approach which reduces the node influence estimation problem to the problem of counting the reachable nodes from each single node in the directed graph realized by bond percolation on the original directed graph."
results showed that the overall accuracy and speed of the plagiarism detection system improved by applying the kullback-leibler symmetric distance to reduce the plagiarism detection search space. the system's performance without search space reduction was precision 0.73 and recall 0.63. when the search space reduction step was applied performance improved to a precision 0.75 and recall 0.74. the execution time also reduced substantially from 2.32 to 0.19 seconds.
"wind speed forecasting is one of the basic components of wind power forecasting. the wind speed forecasting for the next 30 min to 6 h can be classified as short-term wind speed forecasting [cit], which can meet the need of power producer on managing the grid operations, reducing the negative impact of wind power volatility in power grid operation [cit] . the existing wind speed forecasting excellent generalization performance and are suitable for very short-term and short-term wind speed forecasting [cit] ."
"for example, [cit] paraphrased a passage with an anti-anti plagiarism system 2 -a simple automatic tool for word replacement. the paraphrased passage was analysed by two well-known commercial plagiarism detection services and both failed to detect plagiarism. in addition, the best system [cit] in the 2nd international competition on plagiarism detection [cit] achieved a recall of more than 0.99 and precision of 0.95 when detecting verbatim (exact copy) plagiarism. however, none of the systems which took part in the competition achieved a recall of more than 0.28 for manually paraphrased (simulated) cases of plagiarism (the precision score varied). [cit] extracted syntactic features using a context free grammar to identify modified text. results showed an improvement in performance using these features. recently, chong et. al. [cit] applied various preprocessing and nlp techniques (e.g., tokenization, sentence segmentation, part of speech (pos) tagging, chunking and dependency parsing) to normalise documents and found that it improves the performance of existing plagiarism detection approaches. [cit] also showed that applying parsing to normalize the effect of word reordering improves performance for plagiarism detection. however, these approaches fail to identify semantic similarity between a pair of documents."
"in sorp, the speed of sound can also affect the second term of equation (29) which is used to calculate the holding time in the opportunistic data forwarding. in an underwater environment with variable propagation delays at different depths, the network can be divided into some layers based on the depth. during the deployment time, each node knows which layer it belongs to based on the depth information. therefore, in each layer, the more accurate value of the sound speed can be considered in the holding time calculation to increase the opportunistic data forwarding performance in an underwater environment with variable propagation delays."
"in sorp, the number of retransmissions also reaches the least amount possible. this is because sorp utilises the opportunistic routing by using an adjustable forwarding area in different network densities. furthermore, when the network is dense, more suitable nodes with the higher epa can be placed on our forwarding area which can reduce the retransmission possibility to the minimum."
"the updating phase is used to keep updating the nodes about the neighbouring nodes status. periodical updating 6 phase is designed to send an updating packet in each updating interval without the need to receive any ack from the neighbouring nodes. initially, all nodes are considered as regular nodes in the network. however, void and trapped nodes are detected using the received information from the neighbouring nodes over the time."
"the forwarding number in sorp is always lower than other protocols. first of all, sorp has more flexibility to resize and place the forwarding area in a particular position. however, dbr uses all neighbouring nodes with lower depth at each hop, and wdfad-dbr only can resize its auxiliary areas and not its primary forwarding area. second, sorp can also suppress the packet transmissions from all voids and trapped nodes. however, dbr cannot deprive the void and trapped nodes from packet forwarding and wdfad-dbr can only prevent the void nodes from the packet transmissions and not the trapped nodes. third, finding the shortest path for each data packet towards a destination in sorp also has the contribution in the reduction of forwarding number in comparison to dbr and wdfad-dbr."
"query expansion, the process of adding search terms to a query, has been previously used in ir to deal with problems of vocabulary mismatch [cit] . applying query expansion will typically improve retrieval performance, particularly recall [cit] . for instance, the query 'car' could be expanded to 'car cars automobile vehicle'. the process of query expansion can be applied to an initial query, reformulated query or both. moreover, the addition of expansion terms to original query terms can be combined with term re-weighting. for example, expansion terms can be assigned less weight than original ones."
", can affect the end-to-end delay . as mentioned before, the speed of sound may vary from 1450 to 1540 / in an underwater environment. thus, the distance between the source node and the sink can be traversed with variable speeds; however, this variation does not affect significantly because the speed value fluctuates around 1500 / ."
"our proposed framework for candidate document selection (see section 3.1) uses an ir-based approach and incorporates query expansion to identify obfuscated documents. previous studies have attempted to take into account the modifications in the documents for identifying text reuse and plagiarism [cit] . however, these have not been applied to medline citations. the approach most similar to the one presented here [cit] was used to retrieve candidate plagiarised documents in free text. as far as we are aware, the proposed ir-based approach using query expansion based on umls metathesaurus has not been previously used for retrieving candidate documents from medline."
"as can be seen from figure 5, the importance of the same type of features varies greatly in different dates according to the analysis result with ad dataset. for example, in the analysis results of pcc, feature pt is higher than the rest periods on 17 april significantly. table 4 shows that the same correlation analysis method has different importance order for the same feature in different time dates. the analysis results with yd data sets cannot reflect the differences. table 4 can be used to compare the redundancy analytical capability between features with different correlation analysis methods. the overlap between features is the redundancy of information, and the redundancy of the same type of features is higher. as can be seen from table 4, the historical wind speed category features xt has the highest frequency. it can be seen from figure 4 that the training time of models increases with the increase of parameter k, and the mape elevation of the parameter k in the interval from 30 to 45 is only 0.03%. in order to ensure the efficiency and accuracy of the new method, the data set with the value of parameter k is set as 30."
"it should be noted that we only consider a single source at each simulation run to mitigate the impact of a mac protocol on our routing protocol performance. due to the fact that there is only one source at each simulation run, there is no other routing path generated from another source that can interfere in the mac layer to access the channel. for the case of having one source in the network, there is almost no competition between nodes to access the channel at each forwarding hop. therefore, when an intermediate node receives a data packet, it senses a free channel and forwards the data packet. according to this fact, we can consider that the time for any csma back-off is negligible in our simulation setup."
"a limitation of both etblast and using sips is that they are unable to identify similar medline citations when the original text has been substantially altered, such as by paraphrasing or replacing words with synonyms [cit] . the authors suggest the use of such approaches which can identify 'smart duplication' [cit] as well as to \"analyse grammar and extract meaning from sentences rather than rely on word comparisons only\" [cit] ."
"the first part of equation (29) is considered to ensure the priority among the candidate forwarding nodes for packet forwarding based on the two-hop advancement and, the second part of the equation is intended to compensate the propagation delays from the current forwarding node to all candidate forwarding nodes. if a high value is assigned to, the holding time of packets is increased which in turn leads to longer end-to-end delay and less transmission reliability; however, it contributes to more energy saving since a greater number of redundant packets can be suppressed. in contrast, if a low value is set for, the end-to-end delay and packet failure rate are decreased; however, energy consumption increases due to the inability to suppress some lower priority nodes. therefore, there should be a trade-off between the energy consumption and latency in determining the amount of which can be set according to the performance objectives during the network operation."
"for these experiments, the source collection is fromed from 19,569,568 [cit] medline/ pubmed baseline repository. the collection of suspicious documents contains 260 citations from the deja vu database that have been manually examined and verified as duplicates. these citation pairs are selected because they do not have a common author, making them potential cases of plagiarism [cit] ."
"another group of location-based protocols, like gedar [cit], dcr [cit], and gr+dtc [cit], exploits a network topology control scheme in which all void nodes can move vertically to be connected to a nonvoid node. however, topology adjustment of nodes consumes high energy which is justifiable only when used in long-term applications."
"the size of forwarding area has a significant impact on duplicate packets suppression in a dense area and also on reliability enhancement in a sparse region. therefore, the forwarding area is dynamically resized at each hop in sorp to respond to the demand according to local density. at each hop, each forwarding node can select a forwarding range according to the density of the area ahead. the number of nodes with the lower depth can be extracted from the neighbouring table to adopt the forwarding area size. if the number of regular nodes with lower depth is more than a maximum threshold, the minimum forwarding range, which is set to /4 in our model, can be selected to store in the data packet header to suppress the largest possible number of duplicate transmissions. on the contrary, if the number of regular nodes with lower depth is less than a minimum threshold, the maximum forwarding range, which is set to in our model, can be adjusted to increase the reliability of packet transmission. as an example in figure 4, the number of eligible nodes is very limited. thus, in order to maintain the transmission reliability, the maximum forwarding range is considered to include more candidate nodes in the forwarding set. for other cases, the forwarding range is selected from the interval [ /4, ] accordingly."
"with the assistance of noise signals, eemd makes up for the defects of emd's prone to model-mixing by using the characteristics of the uniform distribution of the noise spectrum [cit] . however, the residue noise during the signal reconstruction is difficult to be tolerated, which affects the efficiency of eemd decomposition. the fallowing improvement has been carried out based on eemd:"
where n q is the total number of queries to be combined and s q d ð þ is the similarity score of a source document d for a query q. the top k documents in the ranked list generated by the combsum method are marked as potential candidate source documents.
"as shown in figure 6 (c), the latency of sorp is lower than other protocols. first of all, sorp and wdfad-dbr take advantage of two-hop advancements to calculate the holding time, but dbr uses only one-hop advancement. second, as the traversed distance and hop number for sorp are lower than other protocols, its end-to-end delay also becomes less compared to other protocols. third, in sorp, the candidate forwarding nodes with higher expected packet advancement are selected without regarding their locations which are beneficial to reduce the latency; however, in wdfad-dbr, most of the packets forwarding are limited within a fixed primary forwarding region which reduces the flexibility and consequently increases the latency. in other words, the candidate forwarding nodes with better progress towards the sink may be located on the outside of the primary forwarding region, and ignoring them may increase the latency in wdfad-dbr. fourth, the holding time calculation in sorp is more optimised compared to wdfad-dbr, because sorp takes into account the exact value of propagation delay from the forwarding node to all candidate forwarding nodes and not only the worst case value for the holding time calculation, as it can be observed from the second part of (29) . finally, in sorp, the number of collisions and retransmissions reaches the least amount possible which is useful to reduce the packet delivery time."
"the underwater acoustic communication channel described in section 3 is employed in our simulation model. for the sake of simplicity, we summarise the simulation parameters in table 2 . in our simulation model, we use csma mac protocol without using its rts/cts and ack mechanism to offset the effects of high propagation delay in the underwater environment. when a forwarding node senses a free channel, it forwards the packet; otherwise, a back-off algorithm is invoked. after three times back-off, the forwarding node discards the data packet. the nodes also need to listen to the channel continuously to suppress any duplicate packet which is already relayed by another candidate forwarding node. we also use the implicit ack to reduce the energy consumption of the sorp model since no extra packet is required to confirm the delivery. in the implicit ack model, when a sending node overhears that one of its neighbouring node forwards a packet which is already in its buffer, it can consider it as an ack [cit] . the packet is dropped after three retransmissions."
"orelm avoided solving the sparse matrix by converting the elm's target function into a manageable convex relaxation problem. in addition, the alm method is adopted to deal with the convex relaxation problem, and the ability of prediction model to deal with discrete data is strengthened, the generalization performance of elm is improved [cit] ."
"for query expansion approaches in the medline corpus most of the queries are at the \"same\" rank and there is little difference in number of queries for \"lower\" and \"higher\" ranks. a possible reason for this is that there is little performance difference between various query expansion methods (see table 5 )."
"variable propagation delays in different depths and temperatures are another issue that may affect the performance of an underwater routing protocol. the speed of sound in underwater, ]"
"in the first 30 dimensional feature set on 17 april, the pcc method selected 15 dimensions wind speed category features, and the mi method selected the 14 dimensions wind speed category features, while the cmi method only selected the 11 dimensions wind speed category features. meanwhile, the cmi method took the standard deviation of absolute humidity (115 dimensional) and the variance of air pressure (121 dimensional) into the first 30 dimensional feature set to replenish the information on humidity and air pressure. similar phenomena occurred in other periods. it can be seen above that cmi method can improve the information integrity of feature subset."
"in order to evaluate the impact of updating intervals on sorp performance, we conduct extensive simulations at varied updating intervals of 25 s, 50 s, and 75 s, and their results for packet delivery ratio, average end-to-end delay, and energy consumption are shown in figures 7(a) -7(c), respectively. when the updating interval is lowest (e.g., 25 s), it means that the control packets can be exchanged more frequently, resulting in providing fresher information for the nodes. thus, when the network is sparse, and the updating interval is lowest, the packet delivery ratio increases, and the latency and energy also decrease. however, by increasing the network density, the performance improvement is degraded. this is because, in a dense network, the number of packet collisions is increased by the high frequently control packet transmissions, resulting in performance degradation in packet delivery and endto-end delay. most importantly, the energy consumption is highly increased because sending a large number of control packets more frequently."
"different journals (dj) [cit] . out of 79,383 highly similar citation pairs identified using etblast [cit], only a subset of 2,106 citation pairs have been manually examined and verified as true duplicate citation pairs. among manually examined duplicate citation pairs, 265 pairs are written by different authors and 1,841 pairs have shared authors (sa). although highly similar citation pairs are identified at title and abstract level, [cit] suggested that highly similar duplicate citation pairs with no shared author are potential cases of plagiarism. table 4 shows an example of a potential plagiarism case in the medline corpus. it can be noted that there are five exact matches in both texts whose length is greater than five tokens (shown in bold). these long exact matches are unlikely to occur by chance. in addition, there are also other, shorter exact matches."
"in the routing phase, in a passive participation manner, all void and trapped nodes exclude themselves from the packet forwarding candidates, providing an opportunity for the regular nodes to increase the packet delivery probability in each transmission. sorp is a combination of receiver-based and sender-based approaches [cit] in which the forwarding node selects the best candidate node in terms of the expected packet advancement and then other candidate forwarding nodes decide about their cooperation for packet forwarding based on their distance to the best candidate node. the expected packet advancement is somehow determined to trade-off between packet advancement and the packet delivery probability [cit] ."
"regarding optimal parameter values (see section 4.2), the best results are obtained using a single sentence as a query (q). the optimal value for the number of source documents retrieved against each query (n) is 10. the optimal value for the weight assigned to an expansion term (w ) is 0:1."
"there is also a group of pressure-based and soft-state routing protocols like vapr [cit], ivar [cit], ovar [cit], and llsr [cit], which can keep the packets away from the void and trapped nodes during the packet forwarding by using the reachability information such as hop count distance and forwarding direction. however, they are not as scalable as a stateless approach. sometimes changing the status of a node affects the status of many other nodes resulting in high overhead."
"sorp is able to find the shorter routes for delivering the packets in comparison to other protocols. first of all, considering the two-hop advancements in wdfad-dbr and sorp contributes to finding the shorter routing path compared to dbr, which uses only one-hop advancement. second, sorp has the flexibility to place its forwarding area everywhere to cover more candidate forwarding nodes with greater advancement; however, other protocols are deprived of such ability. for instance, the primary forwarding area of wdfad-dbr is confined to the vertical movement with less tendencies to move diagonally upward; however, sorp does not have this limitation."
"on the other hand, when the updating interval is highest (e.g., 75 s), the packet delivery ratio is reduced because the neighbouring tables gradually become outdated with the passing of time. moreover, all of the information about the void and trapped nodes may become obsolete due to node movement. the latency also increases because of relaying the packets over the nonoptimal paths using the outdated information. although the energy of updating phase decreases by reducing the number of control packets, the total energy is still high due to inefficiency in suppressing the duplicate packets. thus, the updating time should be set carefully to reach the best performance metrics trade-off for sorp."
"source: gammaglutamyl transpeptidase is an enzyme primarily located in the brush border of the proximal convoluted tubules of the kidney. its unique localisation in the renal cells most easily damaged by ischaemia and its ease of assay provides the rationale for its use in the measurement of renal ischaemic injury. using a standard experimental animal model, canine urinary gamma-gt activity was shown to be increased up to 70-fold following 90 min of unilateral renal ischaemia and was significantly raised following only 5 min ischaemia. the urinary gamma-gt was used as a measure of ischaemic injury associated with renal transplantation in man and 20 consecutive patients undergoing kidney transplant were studied by daily 24-hour urinary gamma-gt estimations and excellent correlation was obtained between raised enzyme activity and the clinical diagnosis of transplant rejection. rewrite: the sites of ischaemic injury within the kidney are reviewed and the diagnostic value of measurements of plasma and urinary enzymes in renal ischaemic injury and in renal homotransplant rejection in experimental animals and man is examined. gamma-glutamyl transpeptidase (gamma-gt) is an enzyme primarily located in the brush border of the proximal convoluted tubule of the kidney. its unique localization in the cells most easily damaged by ischaemia and its ease of assay provide the rationale for its use in the measurement and diagnosis of renal ischaemic injury. gamma-gt activity was measured in dogs undergoing varying periods of renal ischaemia and under conditions of local renal hypothermia and was shown to be a sensitive indicator of ischaemic injury. twenty consecutive patients undergoing renal homotransplantation were studied by daily estimation of their 24-h urinary gamma-gt activity; excellent correlation was obtained between raised levels of this enzyme and the clinical diagnosis of transplant rejection. is unimportant. as long as a source document appears in the top k documents the averaged recall score will be 1, regardless of whether it appears in the first or kth rank. table 5 shows the results of the experiments for the top 1, 5, 10, 15 and 20 candidate source documents. as expected, retrieval performance increases as the number of retrieved documents increases. overall it can be noted that our proposed ir-based approach for retrieving candidate documents performs well in identifying real cases of plagiarism. performance further improves when query expansion is applied."
"and the probability of no collision is by substituting from equation (24), the probability of no collision among neighbouring sensors during the updating interval can be written as"
"in order to verify the effectiveness and advancement of the new method, the optimal feature subset selected by the ad data feature selection is combined with different predictors. meanwhile, to ensure the comparison results have wider ramifications, classification and regression tree [cit] (cart) which can automatically complete the feature selection process according to the training set sample and obtain the optimal feature set is used to predict the wind speed samples of different period of time. figure 7 shows the prediction curves of the ad-cmi-orelm method by taking the four predicted days as examples."
"architecture. an uwsn has a 3d network topology consisted of anchored, relay, and sink nodes, as shown in figure 1 . in a multisink network model, sink nodes are placed on the water surface as the final destinations of all packets. they are equipped with both acoustic and radio modems for underwater and land communications, respectively. anchored nodes are placed at the bottom of the ocean to sense and collect information and deliver it to one of the sinks by using the relay nodes, which are deployed at different depths in between. upon receipt, sink nodes can transmit the collected information to the monitoring centre via satellite for further analysis [cit] 33] . the position of relay nodes is continuously changed in the horizontal direction due to the water current: however, this movement is negligible in the vertical direction [cit] . it is also assumed that each node knows its current depth by using an embedded pressure gauge. furthermore, each node can detect received signal strength indicator (rssi) which is used to measure the distance between two nodes [cit] ."
"it can be seen from table 9 that the error indexes of ad-ami-orelm model adopted by the new method still have obvious advantages in the prediction results obtained in all models, and the validity of the new method is proved again. in order to explain the difference in the prediction performances of different seasons, table 10 presents the statistical indicators of wind speed in different seasons. as shown in table 10, the range of wind speed is similar in the four seasons, but according to variance value of the wind speed, wind speed in spring and winter is more volatile and violent. therefore, the rmse of the new method in spring and winter is higher than that in summer and autumn."
"a number of approaches have been proposed for the candidate document selection problem. one approach retrieves candidate documents using the kullback-leibler symmetric distance method, kl d (see equation (1)) [cit] . documents are modelled as probability distributions and compared using kl d . documents are converted into probability distributions by removing stop words, stemming [cit] and then computing tf.idf weights for the remaining word unigrams. assume p d is the probability distribution generated from d, a document in the reference collection, and that q s is the equivalent distribution for s, a suspicious document. the kullback-leibler symmetric distance between them (over a feature vector x) is computed as follows:"
"the umls metathesaurus is a large database of more than 100 multi-lingual controlled source vocabularies and classifications, which contains information about concepts (related to biomedical and health), concept names and relationships between concepts. the basic units of the metathesaurus are concepts, whereby the same concept can be referred to using different terms. one of the main goals of metathesaurus is to group all the equivalent terms (synonyms) from different source vocabularies into a single concept. thus, a concept is a collection of synonymous terms. each concept in metathesaurus is assigned a unique identifier called a concept unique identifier (cui)."
"where is the transmission loss due to the propagation phenomena, and is the transmission loss imposed by the shadow zone. because of the shadow zone occurrence, sometimes some neighbouring nodes cannot communicate although they are within the transmission range of each other. in section 4.4, it will be discussed how the shadow zone and variable propagation delays are addressed in our proposed model."
"input terms are mapped to umls cuis using metamap. the umls metathesaurus's mrconso table is then consulted to identify synonymous terms for each cui and these are used for query expansion. two approaches are used for mapping input terms to umls cuis: (1) cui mapping with wsd and (2) cui(s) mapping without wsd. in the former case, synonymous terms for query expansion are selected from only one mapped cui; whereas in the latter case, additional search terms can be selected from any of the mapped cuis. once input terms are mapped to cuis, synonymous terms in english that are marked as preferred are selected as additional search terms from the mrconso table. we were unable to find a suitable resource find a suitable resource to decide with synonymous term(s) should be used to create expanded queries. therefore, each input term is expanded with a single additional search term which is selected at random. table 3 shows examples of expanded queries created using the umls metathesaurus (where w is the weight assigned to an additional search term/phrase). an additional search term is added to a query term in two ways: (1) treating multi-word input and additional search terms as phrases (see examples of wsd phrase and without-wsd phrase) and (2) treating multi-word input and expansion terms as a sequence of single words (see examples of wsd and without-wsd)."
"for terrestrial sensor networks, greedy routing protocols equipped with some void-handling techniques seem fully mature and efficient to address this issue [cit] . however, due to the different characteristics of uwsns, these protocols are [cit] . quite impractical to be applied directly in underwater environment [cit] . that is because, first, all communications voids in uwsns are three-dimensional [cit], which requires different treatments than two-dimensional holes in the terrestrial networks. second, the mobility of underwater nodes with water currents makes the void mobile. a mobile void area can also result from the surrounding environment [cit] . third, using the marine environment as a communication channel imposes some restrictions on the routing performance. low available bandwidth, slow propagation speed, ineffectiveness of global positioning system (gps), and a lossy environment are a number of such restrictions [cit] . finally, there are some restrictions on the energy consumption of underwater sensors due to the difficulties of replacing or recharging their batteries, and also using the acoustic communication for transmitting the packets [cit] . therefore, designing an efficient void-handling technique to improve the greedy routing protocol efficiency in uwsns is crucial."
"a further common approach to the problem of candidate document selection involves the use of techniques from information retrieval (ir). for example, in many of the international competitions on plagiarism detection [cit] ir-based approaches were used by the majority of the participating groups for the candidate document retrieval task. using this method, documents in the reference collection are converted to fixed length word n-grams and indexed. n-gram representations of the suspicious document are also created in the same way and used to query the index. if the number of matching fingerprints between suspicioussource document pair is above some pre-defined threshold then the source document is marked as potential candidate document. however, these approaches only aim to detect candidate documents that have been copied verbatim with minor changes."
"depth-based, or pressure-based, routing protocols are simplified to use only depth information to route the packets, instead of using the full 3d geographical coordinates [cit] . in this category, a node is a void node if all of its neighbouring nodes have a higher depth value than itself. dbr [cit], dbmr [cit], and eedbr [cit] are depth-based protocols with no mechanism to address the void problem. in these protocols, each node relays the packet to a neighbouring node with lower depth until the packet is delivered to one of the sinks on the surface."
"to analyze the necessity of the feature selection with historical neighborhood data and the analysis advantage of redundancy feature set gained by cmi, the feature correlation on 21 january, 17 april, 18 july and 7 [cit] was analyzed according to the data of adjacent (ad) and the data of the whole year (yd). figure 5 shows the after the normalization analysis results of pearson correlation coefficient [cit] (pcc), mi and cmi. table 4 shows numbers of the most important 30-dimensional features."
"ceemd solves the defect of emd being prone to mode-mixing, and eliminates the effect of white noise induced by eemd by frequency domain complementation [cit] ."
"underwater sensor networks (uwsns) are widely used to support aquatic applications such as environmental monitoring, exploration of ocean resource, early warning systems, and seismic and volcanic prediction. underwater sensor nodes are deployed in different depths of the region of interest to collect aquatic information and forward them to any one of the sinks on the surface [cit] . sink nodes then deliver the accumulated information to the monitoring centre via the terrestrial radio links for further analysis, as shown in figure 1 . each underwater sensor node takes advantage of the acoustic transmission due to restrictions on the use of radio waves in the underwater environment. each node also is equipped with a pressure gauge to measure its depth when deployed in the water [cit] . underwater routing protocols are aimed to improve the packet delivery with minimum cost in uwsns in which greedy routing protocols are the most prominent approaches [cit] ."
"in conclusion, based on the mi between features and target variables, the cmi method reduces the redundancy between features as another indicator of feature evaluation. it has not only evaluated the contribution of features to the accuracy of prediction model, but also ensured the low redundancy of the corresponding feature arrangement modes. therefore, cmi can reduce the effect of redundant information between features on feature selection results."
"the information about concept names, key features associated to each concept name (e.g., language, name type and source vocabulary) and concept identifiers is stored in the mrconso table. the entire concept structure of metathesaurus, therefore, is stored in this file that contains information in multiple languages and each entry is either marked as suppressed or preferred. table 1 shows three entries in english in the mrconso table for the term \"gammaglutamyl transpeptidase\" with cui c0202035. in this example, \"gamma glutamyl transferase measurement\", \"gamma glutamyl transpeptidase measurement\" and \"gtp measurement\" terms can be used as synonyms for the original term \"gamma-glutamyl transpeptidase\"."
"in this section, we assess the performance of sorp against those of dbr, and wdfad-dbr in terms of packet delivery ratio, energy tax, end-to-end delay, forwarding number, and traversed distance."
"the correlation between wind speed and meteorological factors was different in different time of year. in order to meet the requirements of wind speed forecasting model in different time periods, a method of daily low redundancy wind speed features selection is proposed. expect the data of the predicted day as target date, the data of k days before and after the target date in the former four years is selected as training set, then analyze the correlation between features with this data set. meanwhile, data of the week before target date is selected as the validation set, which can ensure feature selection process get feature subsets with strong pertinence based on the meteorological characteristics of different forecast periods. the validation set can also the optimal feature set guaranteed to the meet of target date requirements. figure 3 shows the data set on 6 [cit] as an example. [cit] as the target for prediction (the test set). to test the generalization performance of the new method, new method is used to build the optimal prediction model for every predicted day."
"in this section, the network model is described in detail including the network architecture, acoustic propagation model, sensors mobility model, and shadow zone and variable propagation delays."
"sorp has the lowest energy consumption among the routing protocols. first of all, in sorp, when a forwarding node encounters a dense area, its forwarding area size and position can be adjusted to suppress the redundant packet transmissions. however, in a dense network, many nodes may be placed in the upper hemisphere of dbr and also in the fixed primary area of wdfad-dbr (auxiliary area) leading to increase in the number of packet receptions, transmissions, and collisions. thus, using a fixed location forwarding area for opportunistic routing is not able to achieve an appropriate trade-off between lower energy consumption and higher packet delivery ratio. second, in a sparse density for dbr and wdfad-dbr, many packets are dropped due to get stuck in the trapped and void area resulting in more energy waste. however, sorp can reduce the energy waste of stuck packets by excluding the routes leading to the void nodes. third, the updating phase in sorp is performed at significantly lower energy cost compared to wdfad-dbr. this is because the number of control packets in sorp is efficiently reduced by eliminating the need for ack packets; however, wdfad-dbr consumes high energy at updating phase by requiring the nodes to send and receive the ack packets."
"where is the volume density of the nodes. here ∫ denotes the integral over volume, and ∫ v is equal to when the sensor density follows a uniform distribution. therefore, for a uniform sensor density, equation (14) reduces to"
"however, compared with the other methods, the new method still has the same proportion of improvement, which further proves the effectiveness and advancement of the new method."
"the candidate forwarding set selection in sorp also has other advantages over some existing methods in the literature. first, in sorp, the forwarding area position can be changed adaptively depending on the relative position of regular nodes. whereas in some routing protocols (e.g., fbr [cit] and wdfad-dbr [cit] ), the position of the forwarding area is fixed which decreases the chance of finding the routing paths with lower distances and more regular nodes. second, in sorp, a candidate forwarding set without hidden node can be established using only one-hop neighbouring information; however, in some other opportunistic routing protocols (e.g., hydrocast [cit], vapr [cit], and ovar [cit] ), two-hop neighbouring information is required to exclude the hidden nodes from the candidate forwarding set which resulted in higher overhead. third, in sender-based routing protocols (e.g., hydrocast, vapr, and ovar), the current forwarding node should put the id of all candidate forwarding nodes within the packet header which increases the packet length and consequently the packet failure probability [cit] . however, sorp is a combination of sender-based and receiverbased approach which is required to include only the id of the best candidate node within the packet header, not including the ids of other candidate nodes. this feature contributes to achieve higher reliability at each transmission by decreasing the length of the data packet. finally, in a group of opportunistic routing protocols (e.g., hydrocast, vapr, and ovar), the current forwarding node should perform a forwarding set selection which is a complex and timeconsuming procedure and may affect the end-to-end delay. however, in sorp, the best candidate node has already been selected during the updating phase, and each receiving node only decides to join the candidate forwarding set based on the allowed distance to the best candidate node."
"however, in sorp, the forwarding time is computed using the two-hop advancement. upon receipt of a data packet, each candidate forwarding node ( ) searches its neighbouring table to find a node with the lowest depth ( +1 ) and then calculates the holding time based on the depth difference between the previous hop forwarding node ( −1 ) and the next-hop neighbouring node ( +1 ). it then updates the depth and id of the best candidate node in the date packet header and sets a forwarding timer according to the holding time. if the forwarding timer expires and another transmission of this packet is not yet heard, it transmits the packet. otherwise, the packet must be dropped."
"the energy tax is measured in millijoule (mj) in terms of the average energy consumed per node and per message to successfully deliver a packet to a sink node. the energy tax of routing protocols in different node density is shown in figure 6 (b). as can be seen, the energy tax of routing protocols mostly intends to decrease with the increase of node number, as more data packets can successfully be delivered in a well-connected dense network."
"on 21 january, figure 6 shows that the range of wind speed is very wide (minimum 1.55 m/s to 26.07 m/s), and wind speed increased from 2.43 m/s to 18.52 m/s rapidly, which brought extremely high requirement to prediction models. in the other three predicted days, the wind speed was less than 10 m/s and there was a plummet in wind speed. in particular, wind speed on 18 july was up to 17 h in a continuous random fluctuation period below 5 m/s. however, it can be seen from figure 7 that, although situations of the four predicted days brought challenges to the prediction model, the new method can accurately fit the trend of wind speed, which proves the effectiveness and advancement of the new method. table 8 shows the prediction error of predictors construct with different optimal feature subsets. it can be seen from table 8 that, no matter in which predicted day, the mape and rmse generated with the optimal feature subset obtained by cmi are significantly less than the ones based on pcc and mi. this indicates that low redundancy feature subset can effectively improve the prediction accuracy of the model. models established with orelm generated lower mape and rmse, which proves that orelm introduced the specification parameters c and adjusted the target function of elm effectively reduces the effect of outlier data on the prediction precision and improves the generalization performance of the model. with the optimal feature subset obtained by cmi are significantly less than the ones based on pcc and mi. this indicates that low redundancy feature subset can effectively improve the prediction accuracy of the model. models established with orelm generated lower mape and rmse, which proves that orelm introduced the specification parameters c and adjusted the target function of elm effectively reduces the effect of outlier data on the prediction precision and improves the generalization performance of the model. the comparison experiment proves that the new method can obtain better forecast results than ad-cart method, which once again proved that the performance of feature selection can be improved by using adjacent samples as validation set and the effectiveness of orelm as a predictor. ad-cmi-orelm model also obtained the minimum mape and rmse in each prediction day. take the experimental result of 21 january as an example. the ad-cmi-orelm model decreased by 15.81% compared with mape of the worst model ad-pcc-elm, while rmse decreased by 20.6%. ad-cmi-orelm was reduced by 4.6% and rmse by 11.4%, compared with the suboptimal model ad-mi-orelm. there is a similar increase in the other three predicted days, which proves the effectiveness and advancement of the new method. meanwhile, due to the frequent and larger fluctuation of wind speed on 21 january, the rmse was slightly worse than that of the other three predicted days."
"determining whether plagiarism has occurred is ultimately a human action; however, automated tools can assist with the process [cit] . various factors can signal plagiarism, such as inconsistencies in writing style, unexpected use of advanced vocabulary, incorrect references and shared similarities with existing materials. broadly speaking, approaches to detecting plagiarism (whether manual or automatic) can be categorised into two main problems. intrinsic plagiarism detection relates to identifying stylistic inconsistencies within a text that give rise to questions regarding its authorship; extrinsic plagiarism detection relates to identifying the possible sources of a suspicious document [cit] ."
the data of training set was pretreated by ceemd to reduce the effect of outlier data on prediction model. cmi is used to reduce the redundancy of feature selection results. construct orelm predictor with low redundancy optimal feature subset to improve the generalization ability of the predictor.
"where p(x) and p(y) represent the marginal probability distribution functions of sample x and y respectively. p(x, y) represents the joint probability density function for sample x and sample y."
"the process of plagiarism detection from large document collections, such as medline, is commonly treated as a two-stage process [cit] . the first stage, called candidate document selection, involves identifying a set of candidate sources from a document collection for a given suspicious document. this is followed by the second stage, referred to as detailed analysis, which makes an exhaustive comparison of the suspicious document with all candidates to identify (and align) similar sections. the focus of this paper is the first stage of the extrinsic plagiarism detection process-candidate document selection-that can improve the overall speed and accuracy of extrinsic plagiarism detection systems [cit] . the set of \"candidate documents\" should be carefully chosen from the document collection because any source document missed at this stage will not be identified in the detailed comparison stage."
"among the artificial neural network methods, extreme learning machine (elm) has the advantages of extremely fast learning speed and generalization performance compared with traditional neural network, but it easily runs into partial optimization problems [cit] . outlier-robust extreme learning machine (orelm) improves the elm generalization ability by introducing standard parameters, and is more suitable for forecasting wind speeds which have characteristics of high randomness [cit] ."
it shows the average number of transmissions by each node during the simulation time. the forwarding number has a considerable impact on the energy expenditure and number of packet collisions. the results for the forwarding number are shown in figure 6(d) .
"by using an adaptive forwarding area, the aims of suppressing the duplicate packets in a dense network, or including a greater number of candidate forwarding nodes in a sparse network, is achieved. in a sparse scenario, sorp may allow a duplicated transmission to increase the packet delivery probability; however, in a dense area, the candidate forwarding set is limited to suppress the duplicate packets. furthermore, sorp takes advantage of the relative distance and two-hop depth difference information, i.e., one node below and one above, to further optimise the holding time calculation. algorithm 1 details the two phases of sorp: updating phase and routing phase. each entry of neighbouring table contains the neighbouring node id and depth, the distance between the current node and the neighbouring node, time of receiving the packet, and invalidation time of the neighbouring node. the relative distance between each pair of nodes can be computed via the difference between the initial and received signals strengths (i.e., rssi) [cit] . if all the nodes are homogeneous in terms of the transmission power, the initial signal strength is known to each node. by using the thorp propagation model [cit], each node computes its distance to all neighbouring nodes and keeps it at the neighbouring table."
"geographic routing does not need to discover and maintain the full path from the source to the destination which makes it scalable to be used in the large networks with many nodes. instead, only the information of one hop or two hops is maintained in each node which eliminates the need for updating the long route path via the high overhead routing tables and routing messages [cit] . however, in some cases, greedy forwarding may fail because the forwarding node cannot find any qualified node with a positive advancement towards the destination. if such, the packet is dropped even though there is a valid path from the sender to the destination. this phenomenon is called the communication void or local maximum [cit] . different factors such as permanent or temporary obstacles, sparse topology, shadow zones, and unreliable nodes or links are considered as the most common reasons for this phenomenon happening [cit] ."
let us assume the network density is uniform. the probability of at least one sensor exists above the forwarding can be determined from equation (15) as follows:
"to clarify these definitions, we consider an example shown in figure 2 . node is a regular node because it can see another regular node in its neighbourhood with the lower depth. nodes and are the void nodes since there exist no nodes above them. thus, in a greedy depthbased forwarding, when a void node is considered as one of the candidate forwarding nodes, it obtains higher priority to forward the packet since it has lower depth than other candidate nodes. subsequently, transmission by a void node may suppress other candidate nodes from forwarding the packet, resulting in packet loss. for instance, when node forwards a packet, nodes and are the potential candidate forwarding nodes because their depths are smaller than that of the current forwarding node. however, node timer is expired earlier because it has the lowest depth. therefore, node forwards the packet while there exists no receiving node above it. it also may suppress node from the packet forwarding which eventually leads to packet delivery failure."
"it is defined as the ratio of the number of packets successfully received by the sink nodes to the number of packets sent by the source node. the results for the packet delivery ratio in different node density are shown in figure 6(a) . for all these protocols, pdr increases by increasing the number of nodes, since the network topology becomes more connected which reduces the probability of void occurrence, as it has been discussed in section 4.1.1. equation (17) indicates that the probability of void occurrence decreases if the node density increases."
"addressing the void problem and the energy-reliability tradeoff during opportunistic data forwarding are perhaps the most challenging factors when designing routing protocols. in this paper, we have proposed sorp, a depth-based stateless routing protocol which can bypass both void and trapped areas by applying a novel preventative void-handling technique. sorp exploits the local information obtained from the updating phase, detects the void and trapped nodes for exclusion from the candidate forwarding set, adjusts the forwarding area based on the network density and, finally, calculates the holding time for each forwarding node based on the energy-reliability trade-off constraints. we also analysed the sorp performance in the presence of shadow zone and variable propagation delays, and we discussed that how sorp can inherently deal with these issues as well. the simulation results have demonstrated that sorp significantly decreases packet loss, energy consumption, end-to-end delay, forwarding number and traversed distance in sparse to dense scenarios. as our future work, we plan to design a crosslayer protocol to share the load of the updating phase with a reservation-based duty-cycle mac protocol in order to increase the energy efficiency."
". the parameter decides the holding time of packets at each node. the simulation results for packet delivery ratio, average end-toend delay and energy consumption in different values are plotted in figures 9(a)-9(c), respectively. the parameter is set with three different values of 0.2, 0.4, and 0.6 . with a larger, a node holds a packet for a longer time which increases the average end-to-end delay; however, more duplicate packets can be suppressed, which results in more energy saving. with a smaller, the average end-toend delay is reduced; however, more nodes may relay the same packet, resulting in more energy consumption. in terms of packet delivery ratio, when is smaller, it lets the transmission of more duplicate packets which can increase the reliability in the sparse scenarios. on the other hand, with larger, there is plenty of time to suppress more duplicate packets which can decrease the packet delivery chance in the sparse areas."
"to compare the effect of the correlation with ad and yd data sets on predictive models, the feature subset gained with different data sets are combined with the predictors respectively. the forward feature selection is carried out for different forecast days in the condition of different training set and same validation set. mape is used to evaluate the prediction accuracy of different method with different feature subset. figure 6 shows the error curve of test set in the four predicted days include 21 january, 17 april, 18 july and 7 [cit], respectively. as shown in figure 6, the optimal feature subset is determined by the mape. comparing figure 6a,b, error curves of feature selection with the ad data set converged rapidly and achieved the minimum mape, while the error curves of feature selection based on the yd data set converged slowly and the minimum of mape is larger."
"we carried out an analysis to determine the percentage of queries for which the ranking is \"higher\", \"lower\" or remains the \"same\" when query expansion is applied (see table 6 ). the rank of a query (suspicious document) was considered in the top 20 documents."
"as can be seen from figure 5, the importance of the same type of features varies greatly in different dates according to the analysis result with ad dataset. for example, in the analysis results of pcc, feature p t is higher than the rest periods on 17 april significantly. table 4 shows that the same correlation analysis method has different importance order for the same feature in different time dates. the analysis results with yd data sets cannot reflect the differences. it can be seen from figure 4 that the training time of models increases with the increase of parameter k, and the mape elevation of the parameter k in the interval from 30 to 45 is only 0.03%. in order to ensure the efficiency and accuracy of the new method, the data set with the value of parameter k is set as 30."
"to analyze the necessity of the feature selection with historical neighborhood data and the analysis advantage of redundancy feature set gained by cmi, the feature correlation on 21 january, 17 april, 18 july and 7 [cit] was analyzed according to the data of adjacent (ad) and the data of the whole year (yd). figure 5 shows the after the normalization analysis results of pearson correlation coefficient [cit] (pcc), mi and cmi. table 4 shows numbers of the most important 30-dimensional features."
"this section describes the dataset used for evaluation (section 4.1), how the approach was implemented (section 4.2) and the evaluation measure (section 4.3) used to evaluate the various query expansion methods."
"c0202035 eng gamma glutamyl transferase measurement c0202035 eng gamma glutamyl transpeptidase measurement c0202035 eng gtp measurement \"eng\" means that entry is in english language. additional information from the table is omitted for brevity."
"if we assume that each node randomly broadcasts its data packets within interval, there is a probability of collision which depends on how many nodes are involved in this particular interval. let us assume that there are choices (time slots) and sensors in an overlap region. if there are only two sensors participating, the probability of collision for the second sensor is 1/ . if there are sensors participating, the probability of collision for -th node is ( − 1)/ ."
"it can be seen from table 3 that the wind speed sequence obtained after pretreatment with ceemd is forecasted and got the highest the accuracy. compared with raw data, rmse decreased 0.8437 m/s and mape decreased 13.93%, which proved that ceemd effectively reduced the effect of outliers on wind speed forecasting and improves the prediction accuracy."
"retrieve similar documents (and potentially the source documents of the suspicious text) from the index. 4) results merging: the top n documents returned against multiple queries are merged to generate a final ranked list of source documents. a standard data fusion approach, combsum [cit], is used to generate the final ranked list of documents by combining the similarity scores of source documents retrieved against multiple queries. in combsum the final similarity score, s finalscore, is obtained by adding the similarity scores of source documents obtained against each query q:"
where ℎ is the depth difference from the current forwarding node to the candidate node and ℎ +1 is the depth difference from the candidate node to the next-hop candidate node with the lowest depth and is the weighting coefficient which is within the interval [cit] . is the predefined maximum delay which should be long enough to be able to suppress the transmission of lower priority nodes before relaying the packet.
"as tables 5 and 6 show, the optimal mape of ad-orelm decreased by an average of 4.8% compared with yd-orelm, the optimal mape of ad-orelm decreased by an average of 3.5% compared with yd-elm, the optimal mape of ad-bpnn decreased by an average of 3.7% compared with yd-bpnn, which proved that analyzing feature correlation with ad data can improve the performance of feature selection."
"hydrocast [cit] represents a pressure-based routing protocol which consisted of two parts: greedy pressure-based routing algorithm and a local lower-depth-first recovery method. in hydrocast, void nodes and recovery paths can be discovered in advance, by making use of the depth properties of deployed nodes. void nodes then try to discover a recovery path to a nonvoid node by using a 2d surface flooding method. however, the issue of the concave void area which can appear in deeper regions is not addressed in this protocol. moreover, the recovery route discovery and maintenance incur high overhead, especially when the path is very long."
"in the following, definitions and criteria to select the candidate forwarding set are presented. it is followed by a discussion about the forwarding area features and its advantages over some existing methods in the literature. finally, the forwarding time calculation to prioritise the candidate forwarding set members is proposed."
the probability of void occurrence depends on the value of and . it indicates that the probability of void occurrence for a node increases if either of these parameters (node density or node coverage) decreases.
"in order to further illustrate the effectiveness of the new method, the 7 [cit] to constitute a test set and verify the prediction effect of different models. the average error of each model in different seasons is shown in table 9 ."
"as can be seen from figure 5, the importance of the same type of features varies greatly in different dates according to the analysis result with ad dataset. for example, in the analysis results of pcc, feature pt is higher than the rest periods on 17 april significantly. table 4 shows that the same correlation analysis method has different importance order for the same feature in different time dates. the analysis results with yd data sets cannot reflect the differences. table 4 can be used to compare the redundancy analytical capability between features with different correlation analysis methods. the overlap between features is the redundancy of information, and the redundancy of the same type of features is higher. as can be seen from table 4, the historical wind speed category features xt has the highest frequency. the feature importance in the whole year table 4 can be used to compare the redundancy analytical capability between features with different correlation analysis methods. the overlap between features is the redundancy of information, and the redundancy of the same type of features is higher. as can be seen from table 4, the historical wind speed category features x t has the highest frequency."
", where x i represents the input matrix, y i represents the output matrix. assume that g(x) represents the activation function, and the number of hidden layer nodes is l. the iterations of the orelm algorithm are as follows:"
"for performance evaluation model, using root-mean-square error (rmse) and mean absolute percentage error (mape) as the index to evaluate model performance, which are widely used in the wind speed forecasting field. the rmse and mape are calculated as follows:"
"motivated by the above considerations, in this paper, we propose a stateless opportunistic routing protocol (sorp) to address the unique challenges of uwsns. sorp proactively detects the void and trapped nodes and bypasses them during the routing phase using a passive participation approach. in sorp, the void and trapped nodes can locally be discovered without imposing any significant overhead, which makes it a simple and scalable approach for void detection and bypassing in uwsns. furthermore, by eliminating the need for ack packets during the updating phase, less energy is consumed by sorp. it can also adjust its forwarding area according to the density of area ahead which is beneficial to increase the reliability in a sparse area, or reduce the energy consumption resulted from the duplicate packets in a highdensity area [cit] . the holding time calculation is also further optimised by using the estimated distance and two-hop depth advancement. we also analyse the sorp performance in the presence of shadow zone and variable propagation delays, and we show that how sorp can inherently deal with those issues as well."
"to analyze the necessity of the feature selection with historical neighborhood data and the analysis advantage of redundancy feature set gained by cmi, the feature correlation on 21 january, 17 april, 18 july and 7 [cit] was analyzed according to the data of adjacent (ad) and the data of the whole year (yd). figure 5 shows the after the normalization analysis results of pearson correlation coefficient [cit] (pcc), mi and cmi. table 4 shows numbers of the most important 30-dimensional features."
"the unified medical language system 4 (umls), a set of tools and resources to assist with the development of biomedical text processing systems, is used to carry out query expansion. our approach uses two main umls resources (the metathesaurus and metamap) which are now described, followed by an explanation of how they are used for query expansion."
"performance is compared against the the kullbackleibler distance method (see section 2). this approach is based in pairwise comparison of documents which would be computationally expensive for the source collection of over 19 million citations used by the ir-based approach. consequently a randomly selected subset of 3 million citations, which include the sources for the 260 plagiarised citations, is used as source collection for experiments with the kullback-leibler distance approach. note that an implication of this decision is that the kullback-leibler distance approach has the advantage of a significantly smaller search space from which to identify source documents."
"the reorder feature gained by pcc, mi and cmi are respectively combined with orelm, elm and back-propagation neural network [cit] (bpnn) for forward feature selection. parameters of elm and bpnn are set according to relevant references [cit], the specification parameter c of orelm is set to 2 −10 [cit] ."
"in this section, the detailed results of our simulation study using aqua-sim [cit], an ns-2 based simulating software for underwater acoustic networks, to evaluate the performance of sorp against those of dbr and wdfad-dbr in a multisink architecture are presented."
"the best results are obtained when input and additional search terms are used as phrases in the query expansion process. a possible reason is that there are many multiword phrases in biomedical text which are treated as a single term. when similarity is computed between a query term and a source document higher similarity scores are obtained for matching phrases and therefore sources of plagiarised documents are detected. regarding, wsd and without-wsd, there is little difference in performance. this is likely because additional search terms are randomly selected and an appropriate resource is not used for the selection of additional search terms (see section 3.2)."
"it should be noted that there is no hidden node within the candidate forwarding set if the forwarding range is set less than /2 since all nodes are placed within the transmission range of each other. for instance, in figure 3, all distances between each pair of candidate forwarding nodes are less than, which leads to having a candidate forwarding set without including any hidden terminal node inside."
"the cmi method can calculate the correlation between the target feature and the predicted target with condition of the lowest redundancy between the target feature and the selected features. the mi method uses the probability density function to define the correlation between variables x and y, the formula is as follows [cit] :"
"interval length decision. the updating time interval has a main impact on the sorp performance and should be determined precisely. in the updating phase, it is assumed that each node selects a random slot time to send a control packet. a distribution of picking a random time slot where all time slots have the same probability is called uniform distribution."
"as can be seen, sorp always has higher pdr than those of dbr and wdfad-dbr. it is because sorp can exclude all the routes leading to a trapped or void area, and give the chance of packet delivery to the regular nodes. on the other hand, dbr does not have any void-handling technique, and there is also no mechanism for the trapped area avoidance in wdfad-dbr. furthermore, sorp can adjust the size of forwarding area according to the node density of area ahead which increases the chance of packet delivery by including more candidate forwarding nodes when density is low or by reducing the number of collisions when density is high. wdfad-dbr also has the similar flexibility to cope with the different node density; however, its primary forwarding area is a fixed auxiliary area which may not cover the best possible forwarding nodes in terms of expected packet advancement. finally, the updating phase in sorp can be performed more frequently which can provide the fresher information for the packet forwarding. however, due to high overhead of wdfad-dbr in sending and receiving the ack packets, the updating phase should be performed less frequently leading to performance reduction for packet delivery."
"the reorder feature gained by pcc, mi and cmi are respectively combined with orelm, elm and back-propagation neural network [cit] (bpnn) for forward feature selection. parameters of elm and bpnn are set according to relevant references [cit], the specification parameter c of orelm is set to 2 −10 [cit] ."
"for the updating phase, there are some issues which should be addressed properly to increase the network performance. first, broadcasting a control packet at the same time by regular nodes may result in collisions in the network. thus, in order to prevent the collision problem, each node selects its transmission time randomly from a predefined interval. second, if the updating operation is carried out very frequently, it may lead to increase in energy consumption. thus, the updating phase period should be carefully specified depending mainly on the water current speed. moreover, in our model, the nodes are exempt from sending back any ack to the sender node which leads to more energy saving. furthermore, in comparison to a data packet, the control packet size is so small which can be transmitted using a lower transmission power. the neighbouring table can also be updated by the information extracted from a data packet which contributes in extending the updating phase duration and consequently more energy saving."
"however, compared with the other methods, the new method still has the same proportion of improvement, which further proves the effectiveness and advancement of the new method."
"to compare the effect of the correlation with ad and yd data sets on predictive models, the feature subset gained with different data sets are combined with the predictors respectively. the forward feature selection is carried out for different forecast days in the condition of different training set and same validation set. mape is used to evaluate the prediction accuracy of different method with different feature subset. figure 6 shows the error curve of test set in the four predicted days include 21 january, 17 april, 18 july and 7 [cit], respectively. as shown in figure 6, the optimal feature subset is determined by the mape. comparing figure 6a,b, error curves of feature selection with the ad data set converged rapidly and achieved the minimum mape, while the error curves of feature selection based on the yd data set converged slowly and the minimum of mape is larger. tables 5 and 6 show the prediction effect of predictors based on different optimal feature subsets."
"the goal of the candidate document retrieval task is to identify all the source document(s) for each suspicious document while returning as few non-source documents as possible. it is important for all source documents to be included in the top ranked documents returned by the system since otherwise they will not be identified during later stages of processing. consequently, recall is more important than precision for this problem."
"elm performs the prediction by minimizing the training error, but it is prone to reduce the generalization performance of the model. orelm introduces specification parameters c to improve elm generalization ability. orelm uses 1 − norm to replace 2 − norm, and converts the target function from: min y − hβ"
"in order to further illustrate the effectiveness of the new method, the 7 [cit] to constitute a test set and verify the prediction effect of different models. the average error of each model in different seasons is shown in table 9 ."
"in order to investigate the impact of various sink number on sorp performance, we set the number of sinks at 1, 3, and 5, respectively. the simulation results for packet delivery ratio, average end-to-end delay, and energy consumption are shown in figures 8(a) -8(c), respectively. by having more the number of sinks on the surface, the number of void and trapped nodes is decreased. thus, as can be observed from figure 8(a), the pdr is higher with more sinks. on the other hand, with only one sink, a lot of nodes become a void or trapped node which reduces the chance of each forwarding node to find more qualified regular nodes at each hop. with having more sink nodes, the latency is also lower because a packet should traverse a lower path to reach the closest sink. however, with one sink, the packet should traverse a longer distance, especially when the source node is placed at the corner of the network topology, resulted in a higher end-to-end delay. with more sinks, the energy consumption decreases because packets can be delivered to a sink in a shorter path and with the minimum number of hops. on the contrary, with fewer sinks, energy consumption increases because a packet should traverse a longer distance to reach a sink."
"several routing protocols have been proposed for uwsns over the past few years. in some of these protocols, the void nodes can be detected and avoided thoroughly; however, few of them can deal with the trapped nodes issue. the trapped nodes are those that involving them in packet forwarding leads a packet to become stuck in a void node [cit] . hence, an efficient stateless routing protocol should proactively discover the trapped nodes in a preprocessing phase and avoid them during the packet forwarding [cit] ."
"(1) a low redundancy feature selection is carried out with the cmi method, which reduced the effect of redundancy between features on prediction accuracy and complexity of a model."
"our approach requires three parameters to be set: the number of sentences used to formulate a query (q), the number of source documents retrieved against each query (n) (see section 3.1) and the weights assigned to the term added by the query expansion approach (w ) (see section 3.2) ."
"in this section, we review some routing protocols which have been proposed for uwsns by classifying them into two groups: location-based and depth-based. the main difference between them is related to the location service which is responsible for determining the position of the nodes. the routing protocols in the location-based category assume that underwater nodes are aware of full coordinates of themselves with the aid of some localisation services [cit] . in this category, a node is called a void node, if it cannot find any other neighbouring node with the shorter geographical distance to the destination [cit] ."
"lucene, 8 a popular and freely available ir system, is used for the experiment. the source collection is indexed. documents are pre-processed by converting the text into lower case and removing all non-alphanumeric characters. stopwords 9 are removed and stemming is carried out using the porter stemmer [cit] . terms are weighted using the tf.idf weighting scheme."
"delays. in the presence of the shadow zone, sometimes some neighbouring nodes cannot communicate although they are within the transmission range of each other [cit] ."
"during the training set construction, the parameter of k effects the prediction accuracy and efficiency of models directly. to obtain the appropriate value of parameter k, multi-step prediction models was constructed with different parameters k for 364 days (total 52 weeks) [cit], and series of statistical experiments was carried out. figure 4 shows the weekly average error curves with different values of parameter k. it can be seen from figure 4 that the training time of models increases with the increase of parameter k, and the mape elevation of the parameter k in the interval from 30 to 45 is only 0.03%. in order to ensure the efficiency and accuracy of the new method, the data set with the value of parameter k is set as 30."
"where v (, ) is the average signal-to-noise ratio over distance with frequency which can be calculated using eq. (3). accordingly, the delivery probability of a data packet with size bits over distance can be expressed as follows [cit] :"
every control packet should be delivered to all 1-hop neighbouring nodes before none of these neighbours send their control packets. let △ denote the maximum propagation delay which can be calculated using
is the relative distance between the current forwarding node to the candidate forwarding node which can be calculated based on the difference between initial signal strength and received rssi [cit] . is the depth difference sum of two hops which can be calculated by
"it is defined as the average delay time taken from the moment of the creation of packets at the source node until successfully being delivered to the sink node. in a multisink architecture, due to the fact that a packet may be delivered to different sink nodes at different times, the shortest end-to-end delay is used in the simulation results. as can be observed from figure 6(c), the end-to-end delay for each protocol decreases with the increase of node number because each forwarding node can find more qualified nodes with greater advancement towards the surface when the network is dense. the nodes with more advancement also have shorter holding time which reduces the total holding time of the packet."
"the cmi method can control the redundant between features in four predicted days accurately. in the first three predicted days, the feature subsets obtained by cmi introduces the standard deviation of absolute humidity (115 dimensional) and pressure variance (121 dimensional) in the premise of low redundancy, makes the optimal subset of features obtained by cmi is more advantageous than those from mi and pcc in information integrity, and the smaller mape is obtained. on 7 november, cmi ensured that the information integrity in the optimal features subset was achieved, resulting in lower mape than mi and pcc. in this review, it can be proved that using cmi to carry out low redundancy forward feature selection can effectively improve the prediction accuracy and reduce the dimension of feature subsets."
where ( ) represents the transmission power with frequency at the forwarding node. decoding of the received packet can be done when at the receiver is greater than the detection threshold.
"ceemd is used to pretreat the signals of wind speed time series. the signal is decomposed into a series of imfs: 1 imf,...,imf n . ceemd parameters are determined by reference to existing literature and statistical experimental conclusions [cit], adding a 5 db signal-to-noise ratio white noise to the original signal and the number of iteration is 500. if the high-frequency imf of wind speed signal fluctuates repeatedly in a very short time, it is indicated that the mode contains more non-cyclical components. although the volatile component contains a small amount of wind speed information, the large number of outliers in the mode can greatly affect the accuracy and stability of the prediction model. therefore, ceemd was used to decompose the wind speed samples to 8 to 10 imfs, and filter out the highly volatile imfs, the remaining imfs reconstructs the new wind speed time series. thus, the effect of outliers on prediction model is reduced, and the prediction accuracy of model is improved."
"the rest of this paper is organised as follows: section 2 discusses different existing techniques for plagiarism detection and query expansion; section 3 presents our proposed approach; section 4 describes the experimental setup (implementation details, dataset and evaluation measures); section 5 presents the results and analysis of the experiments; finally section 6 concludes the paper and discusses avenues for future work."
"which only relies on the information received from onehop neighbours for void detection and bypassing during the routing phase. in sorp, in addition to detecting the void nodes, the trapped nodes can also be locally detected and then become inactive in the routing phase to provide more chance for other regular nodes to forward the data packets. sorp has consisted of two phases: updating phase and routing phase."
"furthermore, trapped nodes can be detected using an event-driven approach. upon receiving a control packet from a void node, each node updates its neighbouring table and checks whether it still has any regular neighbour with lower depth than itself or not. if the void node is the only neighbouring node with the lower depth in the neighbourhood, in a similar way, receiving node announces itself as a trapped node by broadcasting a control packet including its id and its current status. similarly, when a node receives a control packet from a trapped node, it will update the status of the trapped node in the neighbouring table and checks its current status. if its neighbouring table does not include any node with lower depth except the trapped node, it also marks itself as a trapped node and broadcast a control packet to inform other nodes. this procedure stops when all trapped nodes in a local area are detected. in this way, all void and trapped nodes in different places of the network topology can locally be identified without needing to know the network topology."
"it can be seen from table 9 that the error indexes of ad-ami-orelm model adopted by the new method still have obvious advantages in the prediction results obtained in all models, and the validity of the new method is proved again. the comparison experiment proves that the new method can obtain better forecast results than ad-cart method, which once again proved that the performance of feature selection can be improved by using adjacent samples as validation set and the effectiveness of orelm as a predictor. ad-cmi-orelm model also obtained the minimum mape and rmse in each prediction day. take the experimental result of 21 january as an example. the ad-cmi-orelm model decreased by 15.81% compared with mape of the worst model ad-pcc-elm, while rmse decreased by 20.6%. ad-cmi-orelm was reduced by 4.6% and rmse by 11.4%, compared with the suboptimal model ad-mi-orelm. there is a similar increase in the other three predicted days, which proves the effectiveness and advancement of the new method. meanwhile, due to the frequent and larger fluctuation of wind speed on 21 january, the rmse was slightly worse than that of the other three predicted days."
"the traversed distance is defined as the average total traversed distance by each packet from the source to the sink node. due to the fact that each packet may be delivered to different sinks with various paths, the shortest traversed path is used in the simulation results. the traversed distance has an impact on other parameters, in particular on the amount of end-to-end delay. the results for traversed distance are shown in figure 6 (e). as can be observed, the average traversed distance decreases by increasing the node density. in sparse scenarios, the shortest path to the sink is not always covered by some nodes, and consequently, a longer path may be taken to reach the destination. on the other hand, in dense scenarios, the chance of finding the shorter routes is higher."
"5 is a key supporting tool for the umls [cit] . the objective of this program is to efficiently link terms mentioned in input text to concepts in umls metathesaurus. metamap performs syntactic/lexical analysis of the input text to map metathesaurus concepts to input terms. during the mapping process, it also includes the option of carrying out word sense disambiguation to attempt to select between candidates when there are multiple possible cuis for a term [cit] . table 2 shows example output generated by metamap with and without wsd. it can be noted that there are two meta mappings when the wsd option is not used, while there is only one meta mapping with wsd. also, during parsing, metamap treats the phrase \"gamma-glutamyl transpeptidase\" as a single term instead of treating it as two separate terms: \"gamma-glutamyl\" and \"transpeptidase\". metamap treats many multi-word phrases as single terms. table entries in english for the phrase \"gamma-glutamyl transpeptidase\", whose cui is c0202035"
"paul clough is a professor of information retrieval at the information school, university of sheffield. his research interests mainly revolve around developing technologies to assist people with accessing and managing information. he has published work in the areas of multilingual information retrieval, information access to digital cultural heritage, evaluation of ir systems, geo-spatial search, textbased image retrieval, plagiarism detection, text re-use, and search analytics. he is a coauthor of a book on multilingual information retrieval and contributor to more than 100 peer-reviewed publications."
"phase. the aim of the routing phase is to deliver a data packet to one of the sinks on the water surface. to accomplish a successful delivery, each packet is required to be delivered successfully at each hop towards one of the destinations. thus, to demonstrate the routing phase, we investigate the packet forwarding at each hop. as an example in figure 3, when the current forwarding node transmits a data packet, all the neighbouring nodes within its transmission range can receive this packet given that there is not transmission error and collision. due to high bit error rate, each forwarding node should take advantage of a group of candidate forwarding nodes at each hop to successfully relay the data packet [cit] . having only one candidate node to forward the packet may lead to the energy wastes, due to increasing in the number of retransmissions resulted from the high bit error probability [cit] . at the other extreme, if all receiving nodes participate in the packet forwarding, it is a naive flooding which also wastes the network resources. thus, an efficient routing protocol should employ some constraints to limit the number of participating nodes to obtain an ideal forwarding set in terms of expected packet advancement [cit] . moreover, during the routing phase, each node also can update the neighbouring table according to the information extracted from the data packet header. this feature aids to obtain fresh information about the neighbouring nodes and to extend the updating phase duration. data header includes the packet type, node id, node depth, the sequence number of the packet, the range of the forwarding area, and id of the best candidate node."
"it can be seen from table 3 that the wind speed sequence obtained after pretreatment with ceemd is forecasted and got the highest the accuracy. compared with raw data, rmse decreased 0.8437 m/s and mape decreased 13.93%, which proved that ceemd effectively reduced the effect of outliers on wind speed forecasting and improves the prediction accuracy."
to overcome the lack consider of the redundancy between features in the modeling process and the variation of wind speed characteristics at different time periods. this paper makes the following innovations to improve the pertinence and prediction accuracy of wind speed forecasting model:
"medline (medical literature analysis and retrieval system online) contains a large number of publications in the area of medicine and related fields. 1 new publications are being added at such a rate that it becomes difficult for individuals or groups to keep abreast of the information contained within it. as a result, it is possible that people may reproduce the same research carried out by others without the connection being noticed and resulting in duplication (and potential plagiarism). [cit] examined a set of over 62,000 citations in medline to identify highly similar citation pairs. they found that 1.39 percent of the citations were highly similar. a number of these (1.35 percent) had shared authors and were similar enough to be considered as duplicate publications. the remaining (0.04 percent) had no shared author and could be considered as potential cases of plagiarism. although the highly similar documents identified in this study are a small portion of the documents examined, given the size of medline it would suggest that as many as 117,500 citations are duplicate publications and 3,500 citations are potentially plagiarised. [cit] and are likely to be higher now.)"
"the packet delivery probability has an inverse relationship with the traversed distance. thus, a neighbouring node 8 wireless communications and mobile computing with the lowest depth may not necessarily be a qualified node for relaying the packet due to its less chance to receive the data packet without any error. in order to find the best candidate node, upon reception of a control packet, each receiving node computes the epa of the sending node and updates its best candidate node id if it is required."
"this section presents the ir-based approach to the identification of candidate source documents (section 3.1) followed by a description of how it can be extended by query expansion using resources from the medical domain (section 3.2). fig. 1 shows the process of retrieving candidate source documents using the proposed ir-based approach. the source collection is indexed with an ir system (an off-line process). in the ir-based framework, the candidate retrieval process can be divided into four main steps: (1) pre-processing, (2) query formulation, (3) retrieval and (4) results merging. these steps are described as follows: 1) pre-processing: each suspicious document is split into sentences using nltk [cit] . the terms in each sentence are converted to lower case. stopwords 3 and punctuation marks are removed. stemming (using the porter stemmer [cit] ) is applied to the remaining terms prior to indexing. 2) query formulation: sentences from the suspicious document are used to form multiple queries. the length of a query can vary from a single sentence to all sentences appearing in a document as reused text can be sourced from one or more documents and vary from a single sentence to an entire document. a long query is likely to perform well in situations when large portions of text are reused for plagiarism; on the other hand small portions of plagiarised text are likely to be effectively detected by a short query. therefore, the choice of query length is important in obtaining effective results. 3) retrieval: terms are weighted using the tf.idf weighting scheme and then text forming the query is used to 3 . a list of 127 english stop words from nltk [cit] was used."
"recall for the top k document, averaged across queries is used as the evaluation measure for these experiments. for a single query the recall at k (r@k) is 1 if the source document appears in the top k documents retrieved by the query, and 0 otherwise. for a set of n queries, the averaged recall at k score is calculated as:"
"in sorp, void node detection can be performed in a timer-based approach. in this way, each node can set a void-detection timer upon starting its operation. during this time, the node waits for a packet from the neighbouring nodes with lower depth. upon receiving a control or data packet from a neighbouring node with lower depth, receiving node resets its void-detection timer. if no neighbouring node with lower pressure is sensed before the expiration of the void-detection timer, node announces itself as a void node by broadcasting a control packet without any delay for its neighbours which are located below. sometimes nodes have already received some control packets from other nodes with lower depth; however, over time, topology is changed, and they may become a void node which again can be detected using the void-detection timer. a void detection can be announced immediately. there is no chance of collision between void announcement packets because void nodes are not within transmission range of each other. for example, if another void node is placed above node in figure 2, it is no longer a void node but becomes a trapped node for this newcomer void node. void-detection timer should be long enough, i.e., longer than the maximum range of the updating timer, to have a reliable outcome for the void and trapped node detection."
"the holding time which is used to calculate the forwarding time should satisfy some conditions to prioritise the candidate forwarding set members and also suppressing the duplicate packets. first, the holding time should decrease with the increase of the depth difference from the current forwarding node. second, the holding time should be long enough to let other candidate nodes hear the packet transmission before forwarding the same packet. in sorp, each candidate forwarding node calculates its holding time, ℎ, using where and ] are the transmission range of the node, and the propagation speed of sound in the water, respectively."
"meanwhile, ad-cmi-orelm has the highest prediction accuracy in every predicted days, tables 5 and 6 show the prediction effect of predictors based on different optimal feature subsets. as tables 5 and 6 show, the optimal mape of ad-orelm decreased by an average of 4.8% compared with yd-orelm, the optimal mape of ad-orelm decreased by an average of 3.5% compared with yd-elm, the optimal mape of ad-bpnn decreased by an average of 3.7% compared with yd-bpnn, which proved that analyzing feature correlation with ad data can improve the performance of feature selection. meanwhile, ad-cmi-orelm has the highest prediction accuracy in every predicted days, which can be preliminarily proved that the redundancy among the features of the optimal feature subset is reduced by cmi, which improved the prediction accuracy of models. table 7 shows types of the optimal features subset obtained by the ad-orelm method. in combination with tables 5-7, it can be seen that the redundancy of feature subsets can be reduced after appropriate reduction of similar features. this reduction also improved the prediction accuracy. meanwhile, adding new types of features with high correlation can enhance the information integrity of the feature subset and further reduce mape of the optimal feature subset."
"this paper describes and evaluates a new query expansion approach to the problem of candidate document selection for extrinsic plagiarism detection. in particular we have focused on cases when the plagiarised version has been highly obfuscated as this presents the greatest challenge to automated plagiarism detection systems. evaluation was carried out using the medline corpus, which contains potential real cases of plagiarism. results show that the irbased approach using query expansion outperforms a stateof-the-art approach, kullback-leibler symmetric distance, for candidate document retrieval task. query expansion using umls metathesaurus was applied to deal with paraphrased cases of plagiarism. in future work, we would like to further explore different methods for rank fusion and dealing with causes of obfuscation beyond term substitution, such as syntactic changes. number of queries for which the ranking is higher, lower or remained same using a query expansion. mark stevenson is a senior lecturer within the natural language processing group of sheffield university. his research areas include lexical semantics, word sense disambiguation, semantic similarity, information extraction, and text retrieval. his publications include a monograph, two edited volumes and more than 100 papers in journals, collected volumes, and international conferences. he has previously worked at stanford university, reuters ltd, and british telecom.s research centre (adastral park)."
"(a) the local maximum and minimum points of the original signal s are connected by a cubic spline to obtain the upper envelope e max and the lower envelope e min . (1) (d) repeat the above steps with h 1 until the number of extrema and zero crossings is equal or differ at most by one, and the mean value of e max and e min is zero. the remaining signal is the first intrinsic mode functions (imf). (e) remove imf 1 from the original s and repeat the iterations above until the signal cannot be decomposed, the remaining signal is the remainder function."
optimal values for these parameters were set automatically using three fold cross validation. the suspicious collection of the medline corpus was split into three folds with two being used to identify the optimal values for the parameters and the remaining third for evaluation. the results of the three runs are then averaged. 10
"in this section, we investigate the impact of each network parameter on the sorp performance, by changing the value of parameter while other parameters are constant."
"wind energy is one of the renewable energy sources that could replace fossil fuels. it has grown rapidly in the past decades. [cit], the cumulative installed capacity of wind energy around the world reached 43.29 gw [cit] . large-scale wind energy integration into power grid has brought operational problems because of the randomness and volatility of wind power generation [cit] . high precision wind power prediction is one of the solutions for optimizing the power reserves, which is used to balance the fluctuations of wind power. with the assistance of accurate wind power forecasting, the stability of power system operation and the adopt capacity for wind power could be improved [cit] ."
"ceemd is used to pretreat the signals of wind speed time series. the signal is decomposed into a series of imfs: 1 imf,...,imf n . ceemd parameters are determined by reference to existing literature and statistical experimental conclusions [cit], adding a 5 db signal-to-noise ratio white noise to the original signal and the number of iteration is 500. if the high-frequency imf of wind speed signal fluctuates repeatedly in a very short time, it is indicated that the mode contains more non-cyclical components. although the volatile component contains a small amount of wind speed information, the large number of outliers in the mode can greatly affect the accuracy and stability of the prediction model. therefore, ceemd was used to decompose the wind speed samples to 8 to 10 imfs, and filter out the highly volatile imfs, the remaining imfs reconstructs the new wind speed time series. thus, the effect of outliers on prediction model is reduced, and the prediction accuracy of model is improved."
"according to the deficiency of existing methods, a multi-step prediction method based on low redundancy feature selection is proposed. firstly, the ceemd method was used to pretreat the training set wind speed data. then, a low redundancy forward feature selection was conducted based on orelm and the order of feature importance gained by cmi. finally, the optimal short-term wind speed forecasting model is constructed with the optimal feature subset to predict the specific period wind speed. the feasibility and effectiveness of the new method are proved through the measurement data of the american wind energy technology center."
"during the training set construction, the parameter of k effects the prediction accuracy and efficiency of models directly. to obtain the appropriate value of parameter k, multi-step prediction models was constructed with different parameters k for 364 days (total 52 weeks) [cit], and series of statistical experiments was carried out. figure 4 shows the weekly average error curves with different values of parameter k. [cit] as the target for prediction (the test set). to test the generalization performance of the new method, new method is used to build the optimal prediction model for every predicted day."
"s lepian-wolf (sw) coding is a technique to losslessly compress correlated remote sources separately and decompress them jointly [cit] . numerous channel coding based sw coding schemes have been proposed [cit] . however, the fundamental assumption is that the correlation statistics needs to be known accurately a priori. actually in many real-world applications, such as sensor networks, the correlation statistics among sensors cannot be obtained easily and may vary over both space and time. since the decoding performance of distributed source coding (dsc) relies on the knowledge of correlation significantly, the design of an online correlation estimation scheme becomes an important research topic both in theoretical study and practical applications [cit] ."
"where ⊕ is the bitwise sum of two elements. however, in reality, crossover probability may vary over time, denoted by, and the perfect knowledge of timevarying crossover probability may not always be available at the decoder. thus, in the case without feedback channels, it is 1089-7798/12$31.00 c"
"when parsing individual ud graphs, we prune the grammar by deleting all rules that generate pos tags that are not present in the graph (or generate more instances of a pos tag than the tag's total frequency in the graph). we further delete all rules that contradict any lin features present in the input (only the +/− sign of feature values is considered). this step must be skipped if it would mean deleting both of a pair of rules, e.g. because a word has punctuation both before and after it. we can then use this pruned grammar to obtain the most probable parse of the udgraph and the corresponding string interpretation. the average parsing time of alto is around 2 seconds per sentence. in a few cases, sentence length would slow down parsing considerably; for all graphs that would take more than one minute to parse (less than 1.5% of the data) we fall back to a grammar that uses binary rules only, i.e. connects all edges of the graph one-by-one."
the weighting scheme described in section 3 is in many ways similar to the way psycholinguists think about grammatical rules. those rules that are based on fewer examples are used more rarely.
"rb vbd vbg prp . i really enjoyed reading it . these parses illustrate a more general phenomenon: since the probabilities of individual rules are roughly similar, the system prefers derivations with fewer rules, which attach more nodes at the same time. counterexamples with radically different rule probabilities are in principle possible, but on average the system prefers specific (more detailed) rules over generic (less detailed) ones, which makes the elsewhere principle [cit] an emergent, rather than an externally enforced, property of the grammar as a whole."
"in this letter, we consider another possible workaround for approximate inference. instead of using monte carlo techniques, we explore the possibility of deterministic approximation, in particular, through the use of expectation propagation (ep). comparing to monte carlo techniques, deterministic approximation typically is much faster but is less flexible. it may not work for all problems and is also more mathematically involved. in the following, we demonstrate how ep can be used for adaptive sw decoding. moreover, we compare the performance of the ep estimator with pbp estimator [cit] under the same setup. our simulation results show that the proposed ep estimator obtains the comparable estimation accuracy with less computational complexity comparing to pbp. further, the ep estimator does not depend on the initial estimation of crossover probability and offers a good real-time estimation for the crossover probability. as a result, a lower decoding error rate is possible comparing to a standard bpbased sw decoder."
"the original ep algorithm proposed by minka is applied to approximate a mixture of gaussian distributions in clutter problem [cit] . in this paper, our problem is to estimate timevarying crossover probability given observations from neighboring factor nodes (see (3) and (6)). thus, ep is extended to approximate a mixture of beta distributions instead of gaussian distributions in our problem."
"in the limiting case, singleton examples are rarely abstracted into rules, they are memorized as is, and the key mechanism for such examples to override the general rules, e.g. that mice overrides *mouses, is the same elsewhere principle [cit] that we see as a derived, emergent property of the system. perhaps one modification that would bring the system even closer to psychological reality would be to use morphological features when restoring the id-s. while this remains future work, we consider it a strong point in favor of xai that such questions can be raised: explainability makes it possible to leverage decades of psycholinguistic work, currently almost entirely ignored in the deep neural net paradigm which, in its laboratory pure form, pays no attention to biological or psychological evidence."
"the simplest approach to constructing a (weighted) irtg would be to simply include all rules \"observed\" in the training data, along with a probablity calculated from the relative frequency of a given configuration among all occurences of a head of a particular pos-tag. in practice we prune this grammar to include only those rules that are applicable to a given sentence and that are compatible with the value of the lin feature (see below), and parse each ud graph using a much smaller grammar. we may also add new rules to the pruned grammar to ensure a successful parsing process (that may or may not yield the correct results)."
"after generating a static list of irtg rules from the training data, we dynamically generate a reduced irtg grammar for each sentence. in a preprocessing step we read all ud graphs that are to be parsed, and for each node and its set of dependents we check if there's a rule in our grammar covering this subgraph. if there's more than one matching rule, we check if the lin feature is present in the input, which allows us to identify the single matching rule. if we identify a unique rule matching the subgraph, we add one to its frequency to increase the rule's probability. in other words, sufficiently specific patterns of the test data are used as additional training data. if no rules matching a subgraph are present in our static grammar, we add binary rules for each dependent, some of which rules may already be present in the grammar, in which case we increase their frequencies. this ensures that the grammar will cover the new subgraph but will prefer to build it from subgraphs we have already seen in the training data. if the lin feature is not present in the input, we add two rules per dependent, corresponding to each possible word order."
"0 is the initial crossover probability for sw decoding, 0 and 0 are shape parameters for beta distribution. 2. initialize the likelihood messages from the channel output"
"we illustrate the kind of decisions the parser must make through a simple example. consider the sentence in figure 4 . our system correctly predicted the word order based on the ud graph, the top parse involves attaching all dependencies of the predicate enjoy using the two rules in figure 5 (s-graph interpretations are omitted for readability). the second most probable derivation applies the three rules in figure 6 and would yield the incorrect surface realization i enjoyed really reading it."
"the algebraic language toolkit, or alto 1 [cit], is an open-source parser for irtgs that implements a variety of algebras to use as intepretations of irtgs, including the string algebra and s-graph algebra. an alto grammar file must declare all interpretation algebras and for each rtg rule provide mappings to operations in each of these algebras. figure 1 shows a minimal example of an irtg with two interpretations. the abstract rtg rule nsubj, so named after the corresponding ud relation, has two abstract arguments, designated verb and noun. the string interpretation establishes that the surface form of the second argument (noun) is to precede the first argument (verb). the ud interpretation adds a directed nsubj edge between the subgraphs corresponding to each argument, by a series of rename, merge, and forget operations. angle brackets after nodes indicate source names. in our s-graph grammars, every subgraph at every point of the derivation tion of ud graphs as follows: for each node in the graph we establish one generation step, which is responsible for attaching all its dependents to it. the ud graph depicted in figure 2 would hence correspond to the rtg rules in figure 3 (interpretations are omitted for better readability). note that we create rules that operate at the part-ofspeech level, lemmas can then be inserted by terminal rules generated separately for each sentence."
"where and are shape parameters, beta(, ) represents beta function. by comparing (2) with beta distribution (5), likelihood function (2) can be represented in terms of beta distribution with respect to variable as parameter:"
a factor graph for both sw decoding and correlation tracking is illustrated in fig. 1 . the factor graph is more or less the same as that used in our prior work [cit] . note that regions ii and iii in fig. 1 alone contribute the same factor graph as that used in traditional ldpc-based sw decoding. a block of
"we split the sentences from the train data into 80% train and 20% development sets for the inflection module. a full-scale hyperparameter search being prohibitively expensive, we only tried a few hyperparameter combinations and use the ones performing best on the dev set for the final submission. table 1"
the present study aims at comparing the concentrations of indoor air pollutants in residential dwellings in commercial and non commercial areas in delhi city and assessing its effect on the respiratory health of the occupants.
the regularization term r p (φ) characterizes the deviation of the level set φ from a signed distance function and also smooths the level set function φ.
"where τ 1 and τ 2 are positive constants, which balance the two terms for edge preserving regularization. î d and h (φ (x, y)) characterize the oil spill edges in the smoothed image domain and the level set domain, respectively. in contrast to the gradient of the original i d, which might misidentify noise as oil spill edges, the guided filtered representation î d reflects more accurate oil spill edges without noisy misidentification."
"the electromyography (emg) sensor used in this study comes from rsl steeper, the uk. the emg sensor can capture a low muscle signal. it has proportional control and built-in gain adjustment. the sensor requires voltage from 5 v to 19 v. the resulted signal from the emg has been amplified, rectified, and smoothed. the emg sensor is placed in normal healthy hands as shown in figure 6 . it shows that the user performs the flexion motion on the hand. flexion and extension motion on the hand is read by emg sensor and then the signal is used to drive the myoelectric hand. the used voltage from emg sensor is below 1.4 volts. the emg signal is filtered using first-order low-pass filter before it is processed by feedback control. the acquired and processed emg signal is presented in figure 7 . the selected linear actuator is actuonix pq 12-p with the maximum stroke length of 20 mm. the technical specification of the utilized linear actuator is summarized as show in table 2 ."
"admm requires two sets of variables. most existing admm strategies set the original optimization variables as one set of admm variables and use a set of artificial variables as the other set of admm variables. furthermore, they usually use the linear constraints to enforce the equality of the original variables and the artificial variables. the two sets of variables are then assigned to subproblems intentionally separated from the original energy minimization. therefore, most existing admm methods construct artificial variables and conduct intensional energy separation just for the purpose of efficient computation, and their linear constraints do not reflect practical relationships underlying data representations."
"gender is found to be a vulnerable factor with regard to the sbs score as well as respiratory problem score at both the sites. the womenfolk at the sample site complain of more symptoms relating to their respiratory health. the lung function test results also reveals that the lung ability is poor in women at the sample site when compared with the control site. the reason for this may be due to usage of biomass fuels, inadequate ventilation and old and improperly maintained houses. this finding also lends support to other studies [cit] that link household fuel use with adverse health outcomes. certain public health policy implications such as improved stoves with chimneys, use of cleaner fuels like piped compressed natural gas, better housing and kitchen design with improved ventilation may certainly help in controlling indoor air pollution."
"in winters, the pm 10 concentrations are towards higher side in almost all the households in the sampling site area. besides, the pm 10 and pm 2.5 concentrations are much higher in sample site than at control site."
"based on the test result, the five developed grip patterns can successfully grasp an object ranging in size, shape, and weight. astohand v3.0 myoelectric hand can successfully pick and grasp a fragile object like an egg by using visual feedback from the user / wearer. the study participant with transradial amputation can stably use the myoelectric hand. in the future research, the proposed myoelectric hand will be developed incorporating force feedback control for grasping fragile objects. provide a statement that what is expected, as stated in the \"introduction\" chapter can ultimately result in \"results and discussion\" chapter, so there is compatibility. moreover, it can also be added the prospect of the development of research results and application prospects of further studies into the next (based on result and discussion)."
"we use the region-scalable fitting (rsf) level set [cit] as a baseline for comparing segmentation accuracy. the rsf level set is one state-of-the-art image segmentation method. we have modified the rsf level set by incorporating the edge preserving regularization and developed our energy functional (20) in section iv-b. in order to validate the effectiveness of our framework, we conduct experiments for comparison as follows. applying the rsf level set method directly to blurry images and obtaining the oil spill segmentation results; first performing deblurring by minimizing (4) and then conducting graph cuts, distance regularized level set evolution (drlse) or rsf level set segmentation for obtaining the oil spill segmentation results; and operating admm for minimizing the proposed overall energy functional for obtaining the oil spill segmentation results. we test different methods subject to imposing gaussian blur and motion blur on sar images. however, the blur type is totally blind to the admm in the integrated deblurring and segmentation process. fig. 3 illustrates the visual results for different segmentation strategies subject to the gaussian blur. specifically, fig. 3(a-1) illustrates the blurry sar images that are blurred by an un- known gaussian. fig. 3(a-2) illustrates the segmentation results obtained by applying the rsf level set method directly to the blurry images. as the images are not deblurred before segmentation, the oil spill segmentation results thus obtained contain certain blurs with considerable detail loss. fig. 3(b-1) illustrates the deblurred results by just minimizing the deblurring energy functional introduced in section ii-c. figs. 3(b-2), 7(b-3), and 7(b-4) illustrate the segmentation results by applying the graph cuts, drlse, and rsf level set method to the deblurred images shown in fig. 3(b-1) . it is clear that these segmentation results preserve more details than those shown in fig. 3(a-2) . this validates that the separate deblurring and segmentation strategy outperforms the straightforward segmentation strategy. however, they still exhibit differences from the ground truth segmentation shown in fig. 3(d) . fig. 3(c) illustrates the oil spill segmentation results from applying our admm framework to the blurry sar images in fig. 3(a-1) . it is clear that our admm framework generates the most accurate segmentation results, and outperforms the straightforward rsf level set strategy and the separate deblurring and segmentation strategy. additionally, the visual experimental results subject to the motion blur are illustrated in fig. 4, where we observe that our admm method also outperforms the alternative comparison methods."
"there are more than 1000 urban slum clusters in delhi housing more than 1.5 million people [cit] for experimental work, five houses have been selected from the site i.e. three from low income group, one from middle income group and one from high income group, based on the distance from the commercial area. the first low income group household is closest to the railway station; the second is slightly far and the third household is near to the road."
"forced vital capacity (fvc), forced expiratory volume in 1 s (fev1) and peak expiratory flow rate (pefr) have been measured by spirometry. spirometry is done with the subject in a sitting position and wearing a nose-clip under the supervision of occupational and health specialists from lnjp hospital, delhi (american thoracic society [cit] )."
(2) the blurring kernel k b in (2) is considered as a parameter to be estimated for precisely characterizing the disagreement between the deblurred image i d and the blurry image i. the estimation of k b will be described in section iv. minimizing the energy term e b in (2) with respect to i d and k b results in deconvolution of these two components from the blurry image i. the energy term (2) thus restores the deblurred image i d by maximally neutralizing the interferences from the blurry kernel and the white gaussian noise on i.
"we integrate the fitness energy term introduced in section iii-a and the regularization terms introduced in sections iii-b, iii-c, and iii-d into the energy functional e s (φ, r 1, r 2 ) for segmenting oil spills in sar images, which is given as follow:"
"we integrate the separate minimizations of deblurring energy functional e d (i d, k b ) and the oil spill segmentation energy functional e s (φ, r 1, r 2 ) into the minimization of one overall energy functional as follows:"
"it is motivated by the edge-indicating regularizations in that the integration of the product of one edge indicator and one image presentation from different domains possibly slows down the level set evolution around edges and enables a detail edge characterization. in the light of this observation, the terms in (16) regularize the segmentation energy functional in terms of characterizing edge information in both image and level set domains and favor the preservation of edge details."
"lung function test is a tool for evaluating respiratory status. at the control site, the observed values for fev1 and fvc of the occupants are within the range. thus the lung function test reveals that the pulmonary ability of the occupants at control site has been much improved than at the sample site."
"although the two separate energy functionals e d (i d, k b ) and e s (φ, r 1, r 2 ) are summed in (18), which results in an overall energy functional. the coefficients a and b establish a linear mapping that correlates the segmentation indicator level set φ and the deblurred image i d . here, a more sophisticated relationship between φ and i d might exist. however, we use the linear mapping to characterize the relationship between φ and i d for two reasons. first, it establishes an informative correlation between the segmentation indicator φ and the deblurred image i d . the linear mapping constraint conveys such indicative information and thus enables the information interaction between the deblurring and segmentation steps. second and more important, the linear mapping constraint follows the standard form of the admm, which provides a reliable solution for the optimization problem in (18) and (19) ."
"the deblurring scheme presented in this section is a commonly used traditional strategy [cit] . based on this existing scheme, we will propose a novel segmentation framework that intrinsically guides the traditional deblurring scheme to operate in favor of an accurate segmentation, which will be presented in section iv."
"oil spills always exhibit irregular shapes. however, to achieve optimal segmentation, the iterative evolution of φ for optimization is supposed to be updated in a regular way. to preserve the regularity for updating the level set φ, which is necessary for an accurate computation and stable level set evolution, a distance regularization should be intrinsically incorporated into the variational level set energy formulation to maintain the regularity of the level set functional during the evolution [cit] . the distance regularization is defined with a potential function as follows:"
"since the study focus is on assessment of indoor air quality of the household kitchens, it was considered that the data collection technique would not be confined to descriptive data collection alone, but would also be supplemented by experimental data. the experiments are conducted in selected household kitchens to measure the concentrations of respirable suspended particulate matter (rspm i.e. pm 10"
"however, at the control site, the mig and hig households are showing increasing trend in the rspm concentrations. the pm 2.5 concentrations are 625.44µg/m 3 in sample site during winters; whereas the maximum concentration observed at control site is 213.72µg/m 3 ( fig. 2 ) it may be due to the fact that in sample site there are indoor sources like cooking activities, inadequate ventilation, higher occupancy along with penetration of outdoor air indoors. at the control site, the primary cause of higher rspm concentrations is infiltration through openings in the households that results into increases in pm 10 ."
"one possible way to segment oil spills from a blurry sar image is to conduct deblurring to the blurry image and then perform segmentation on the deblurred image [cit] . the flowchart for this separate two-step strategy is illustrated in fig. 1 . the two-step strategy is easy to implement. however, most existing deblurring methods [cit] tend to straightforwardly restore a clear image and do not necessary enhance the effect of the subsequent procedures such as segmentation. one classical method for deblurring images is deconvolution. recently, a number of new deconvolution strategies [cit] have been developed for achieving visually higher quality of the restored images. though these methods improve the visual quality of the restored images, they do not necessarily enhance information required by the subsequent procedures such as segmentation. in order to address this shortcoming, we formulate the problem of sar image deblurring and oil spill segmentation into one overall energy minimization framework. we present a novel oil spill segmentation framework in which the deblurring and segmentation comprehensively interact with each other. in this scenario, our framework does not consider the deblurring and segmentation as two separate operations but encourages the deblurring to operate in a way that favors an accurate segmentation. therefore, in our framework, the deblurring and segmentation operate in a complementary reciprocal manner such that their intrinsically combined effort essentially improves the effectiveness of oil spill segmentation over the separate deblurring and segmentation strategy."
"step three: in the third step, we fix i d and φ, and update the lagrange multiplier l according to (23) . it should be observed from (23) in (27) and φ k +1 in (30) in the current iteration. therefore, the iterative admm computation turns out to be a rotating update of the deblurred image i d and the segmentation indicator φ, where the two factors affect each other and the lagrange multiplier l plays a role of messenger for transferring information between them."
where t is a fixed learning rate. the major computation at this step involves the divergence of a gradient that requires o(n 2 ) operations.
"suppose a blurry sar image i for observing oil spills is formed in terms of the convolution between an original image i d and an unknown blurring kernel k b, along with additive gaussian white noise n, as follows:"
"specifically, we not only review the existing region-scalable fitting (rsf) segmentation method [cit] but also develop a novel oil spill edge preserving regularization term. the new term smooths images without heavily degenerating oil spill edges and thus improves the detail segmentation over the existing rsf method."
"the blurring kernel k b may contain unexpected abrupt changes especially when it is estimated from a noisy blurry image i. to address this drawback, we exploit a smoothing regularization term with respect to k b as follows:"
"the concentrations of nox are higher in lig and mig households in sample site during summers (fig 5) . usage of wood and kerosene stoves is prevalent in these households accompanied by infiltration of outdoor pollutants indoors. at the control site, the nox concentrations are within the permissible limits (ashrae [cit] )."
"most energy minimization methods perform oil spill segmentation on clear images and tend to ignore the situation that an sar image is blurred. the sar imaging is affected by environmental factors such as atmospheric turbulence [cit] and inaccurate estimation of the speed of the airborne devices [cit] for monitoring ocean. therefore, blurs are regular phenomena arising in sar images and pose inevitable interferences to oil spill segmentation."
". therefore, our admm form not only enables robust and efficient optimization but also encodes the interaction between sar image deblurring and oil spill segmentation."
"the summarized description of our novel admm framework is illustrated in fig. 2 . in contrast to the separate deblurring and segmentation shown in fig. 1, the admm framework maintains rotating information exchanges over iterations. as illustrated in fig. 2, the information exchanges arise in a two-fold manner, i.e. the outer cycle updates subject to the linear mapping constraint input: a blurry oil spill sar image i. i"
"in the first step, we fix φ and l, and compute the minimization of the overall energy functional with respect to i d as follows:"
"in the input command test in the form of step input and trajectory tracking, the sampling rate is 50 hz. the maximum voltage given to the linear actuator is 5 v. l293d motor driver ic is used for reversing the polarity of the dc motor to retract or extend the stroke of the linear actuator. based on figure 8, it can be seen that there is a delay from response to command less than 0.3 seconds. this can be due to the voltage applied to the actuonix pq 12-p linear actuator of 5 volts, while the maximum voltage that can be applied to the linear actuator is 6 volts. in the test of the input step with a command from 0 to 10.25 mm, the time constant is about 0.6 seconds. the initial stroke position of the linear actuator is 1.75 mm. the steady state error of feedback control that has been developed is less than 0.5 mm. the steady state error is less than the allowable steady-state error criteria in the proposed feedback control as shown in figure 3 . the proposed control has a good performance result to be implanted on the astohand v3.0 myoelectric hand for controlling the finger flexion and extension. in this test, the initial stroke position of the linear actuator is 1.3 mm. based on the test result as shown in figure 9, the stroke displacement command has been successfully followed by stroke displacement response signal in the form of a trajectory tracking of the actuonix pq-12 linear actuator. this test result from the figure proves that the proposed control has successfully performed a stroke displacement position control that can be used to adjust the stroke length of the actuonix pq-12 linear actuator according to the length of the given stroke length command. there is still a delay when the response signal follows the command, this happens because the voltage given in this test is 5 volts. the larger the stroke displacement length command, the greater is the time constant that occurs. larger time constant occurs when the stroke is retracted. the resulted steady-state error in the trajectory tracking is relatively low that is less than 0.5 mm. when the steady-state error reaches below 0.55 mm, the motor will be turned off by the controller. in the voltage drop test, it is aimed to test the proposed control as presented in figure 3 for reducing the power consumed by myoelectric hand. the astohand v3.0 myoelectric hand is turned on for 60 minutes. the hand is commanded to perform finger flexion and extension movements on the fingers simultaneously every second. in the test, the hand performs finger flexion and extension movements 60 times for one hour. the voltage value on the myoelectric hand battery is acquired and recorded every second. in the study, two 18650 rechargeable li-ion batteries that are connected in series connection are used as a power source of the myoelectric hand system. the batteries are used to power the arduino nano microcontroller, five linear actuators, three l293d motor drivers, an rgb led, and an emg sensor. figure 10 shows the voltage drop every second for 60 minutes on the myoelectric prosthetic hand batteries. in this test, the initial voltage value is 7.80 volts. then in the first minute, there is a voltage drop to 7.79 volt. until in the 60th minute, it is obtained that the final voltage value is 7.66 volt. based on the test in figure 10, the voltage drop on the myoelectric hand that is used every second for one hour is 0.14 volts. based on the result from the test, it proves that the proposed feedback control embedded in the myoelectric hand system serves to reduce excess power usage during myoelectric hand when it performs finger flexion and extension motion."
"to quantitatively evaluate the performance of different segmentation algorithms, we compute the recall ( figs. 9 and 10 illustrate the segmentation accuracies in terms of recall and precision for alternative methods over 50 now-pap sar, uavsar, and modis images containing oil spills. standard deviations are also marked on the top of corresponding accuracy bars. we can see that our admm framework outperforms the two comparison methods in terms of both recall and precision with relative small standard deviations. to make the quantitative evaluation one step further, we compare the convergence rates of different strategies by using the top oil spill uavsar image in fig. 6(a-1) . fig. 11 illustrates the convergence rates of different strategies with respect to iteration numbers. fig. 12 [cit] b implementation based on intel core i5-3470 cpu. we can see that our admm method not only produces the more accurate segmentation results but also achieves much better convergence rate than the other two comparison strategies. this efficiency benefits from the information exchange between the blurring and segmentation procedures in admm, which guides the deblurring to operate in favor of a more accurate segmentation and thus enables fast convergence to the minimum energy."
"we address this shortcoming by developing a blurry oil spill segmentation method that intrinsically incorporates the deblurring operation. a brief review of our method is illustrated in fig. 2 . in contrast to the separate deblurring and segmentation strategy in fig. 1, our method performs intrinsic interactions between the deblurring and segmentation and encourages the deblurring to operate in favor of an accurate segmentation. detailed descriptions of our novel framework are presented in the following subsections."
"in this research, the myoelectric hand has five grip patterns i.e. power grip, tripod, precision closed, hook, and active index to perform object grasping in the activity of daily living. the command for each grip pattern uses the same emg signal for finger flexion and extension. to select the grip pattern, user can touch the tactile switch on the back cover of the hand. the current working active grip pattern is indicated by an rgb led that lights up according to the selected grip pattern. on-off control is employed for controlling the stroke displacement of the actuators with the allowable steady-state error tolerance of less than 0.55 mm as shown in figure 3 . because the actuator system utilizes the non-backdrivable mechanism, the proposed control can maintain high grip forces and stop consuming the current from the battery. the control strategy as shown in figure 3 is developed under matlab/simulink software. the developed block diagram of the myoelectric hand operation system is presented as show in figure 4 . the voltage input read from the emg sensor is filtered using a first-order low-pass filter. the block diagram is embedded into arduino nano using simulink support figure 3 on the proposed myoelectric hand using emg as an input command signal. the electrical signal of the user muscle is read by the emg sensor which gives the voltage output signal, then multiplied by a gain into the value of the command x reference. the value of the x reference is the value of the stroke displacement length that is commanded to the linear actuator. the value of the x reference then is given to the control algorithm. it becomes the value of the commanded stroke displacement of the linear actuator. linear potentiometer in the linear actuator is used to measure the stroke displacement. the steady state error (e) allowed in the feedback control is less than 0.55 mm. the stroke length of linear actuator can extend from 0 to 20 mm."
"in poor households in the southern india, women typically do most of the cooking and spend most time indoors, thus they are subjected to high levels of pollution both from being close to the fire, and spending longer periods of exposure in the indoor environment [cit] )."
", and * represents the operation of convolution. the energy term that characterizes the disagreement between the deblurred image i d and the blurry image i is given as follows:"
"in this section, the proposed control strategy of five grip pattern mode will be tested experimentally. the developed feedback control will be tested using step and trajectory tracking command input. the efficiency of control strategy will be performed with voltage drop monitoring when the myoelectric hand is driven every minute. five grip pattern mode will be implemented and tested by using normal healthy hand. various object grasping tests will be conducted by using normal healthy hand incorporating five developed grip pattern. in the final test, astohand v3.0 myoelectric hand will be employed on the study participant with transradial amputation."
"radar (sar) provides an important means for monitoring marine oil spills [cit] because it has the advantage of all-weather and all-time observation ability. in order to make a timely damage assessment [cit] and spread control of oil spills, it is vital to accurately observe the oil spills through sar images. therefore, developing intelligent algorithms for segmenting marine oil spill regions from sar images has been an important research topic in the field of ocean remote sensing."
"iteratively implementing the three steps until convergence achieves the minimization of the admm form of the overall energy functional. here, the third step plays an extremely important role in our framework. for one thing, it fuses the information from both the deblurred image i k +1 d and the segmentation indicating level set φ k +1 . for another, the updated lagrange multiplier l k +1 conveys the fused information of the first and second steps for updating the deblurred i k +1 d image and the segmentation indicating level set φ k +1 in a new iteration. it is in such iterative way that the information of deblurring and segmentation is exchanged in the process of minimizing the overall energy function."
"in contrast to the artificial setting of variables, the two major sets of variables in our energy functional (20) are the deblurred image i d and the oil spill segmentation indicating level set φ, both of which are original optimization variables. furthermore, unlike the intensional energy separation, our energy functional practically consists of the deblurring and segmentation energy functionals, which are naturally separated. however, the separate energy functionals and their variables i d and φ are correlated by the lagrangian term l t (aφ + bi d − c) and the quadratic regularization term"
"where α 1 and α 2 are positive balancing constants, separately, i d is the deblurred image, and r i (x, y) characterizes the weighted averages of the intensities in a neighborhood of (x,y) and it represents the image approximation whose update scheme will be introduced in section iv-d."
"the rspm concentrations indoors at the sample site remain 200 µg/m 3 or more during winters. however, during summers, it reduces due to efficient dispersion. the co concentrations show small variations during summers. the nox concentrations are towards higher side due to usage of gas and kerosene stoves indoors and penetration of outdoor air laden with vehicular exhausts."
"to validate the effectiveness of the proposed admm framework, we commence by using the sar images with vv-polarization obtained from the nowpap database, 1 which contain differently shaped marine oil spill regions, as our test dataset. the images we used in the experiment are sar images including c-band sar images from the ers-1 and ers-2 satellites, and c-band asar images from the envisat satellite. these images containing oil spills with various shapes are captured in separate time by different sensors. we describe the information of sar image sources and sensor properties in tables i and ii, where the symbol \"-\" indicates unavailable information."
we exploit the rsf level set for characterizing the data fitting in the oil spill sar images. the rsf energy functional is defined as follows:
"interview surveys have been carried out to gather information on type of kitchen, type of fuel used, frequency and severity of respiratory symptoms experienced by the occupants in order to estimate the sick building syndrome (sbs). a scale of 0-15 has been used to estimate the sbs score [cit] ). the medical research council committee respiratory problem scale has been used (mrc [cit] ) for the spirometry analysis. student's t and two paired t-tests are used to find out pollutant concentration variations in different seasons. pearson's and spearman's correlations have been used for evaluating the effects of demographic variables on respiratory problem score [cit] )."
"we integrate the deconvolution energy term introduced in section ii-a and the smoothing regularization terms introduced in section ii-b into the energy functional e d (i d, k b ) for deblurring oil spills images, which is given as follows:"
population growth and fast developing economic activities have led to increase in ambient and indoor pollution in delhi. this study aims at assessing the indoor air quality in the households located near the commercial areas in delhi city.
"the co concentrations increase during summers (fig. 3) . in sample site, the co concentrations levels are high due to vehicular exhausts and emissions from the railway station. at the control site, i.e. at iit delhi campus, free passage to these vehicles is restricted. the i.i.t. delhi campus is much greener area with lot of open spaces, trees, and plants that help to reduce the concentrations of pollutants to a considerable extent. the concentrations of so 2 are towards higher side in lig households at the sample site area during summers (fig 4) . in mig and hig households at sample site and in all households at i.i.t. delhi campus, the so 2 is well within the permissible limits (ashrae [cit] ). the lig occupants are using kerosene stoves or poorly vented gas appliances. it causes high so 2 concentrations indoors. no significant seasonal variations in the so 2 concentrations have been observed."
"one possible way to detect oil spills in a blurry sar image is to conduct deblurring to the blurry image and then perform segmentation on the deblurred image [cit] . the flowchart for this separate two-step strategy is illustrated in fig. 1 . the two-step strategy is straightforward to implement. however, as the deblurring and segmentation operations are two independent procedures, they cannot communicate to each other. the deblurring operation tends to straightforwardly restore a clear image without considering enhancing the segmentation accuracy. the segmentation operation just aims to detect oil spill regions subject to data fitness but neglects providing indicative information for effective deblurring. such separate operations neglect the interaction with each other and do not consider maximizing their representational powers in terms of mutual benefit."
"our novel energy minimization framework for oil spill segmentation is summarized as follows. we first establish energy functionals for deblurring and oil spill segmentation separately. we then integrate the two energy functionals into one overall energy functional subject to one linear mapping constraint between the deblurred image and the segmentation indicator. the linear mapping not only conveys information between the deblurring and segmentation operations but also enables minimization of the overall energy functional in terms of an alternating direction method of multipliers (admm) [cit] . the admm decomposes one original problem into several subproblems using the separability of the overall energy functional such that each subproblem is easier to handle. on the other hand, it maintains information conveyed between the subproblems such that the subproblem solutions are compatible with one another and achieve the final solution of the overall energy minimization. we exploit this advantage of admm for conducting deblurring, which provides helpful information for oil spill segmentation, and thus establish a framework that simultaneously deblurs and segments oil spill sar images. experimental evaluation validates the effectiveness of our method for the segmenting blurry oil spill sar images."
"this section reviews the energy functional for neutralizing blurs in oil spill images. a deconvolution energy term for restoring images and a regularization term for smoothing kernels is developed separately, and then integrated for penalizing the process of deblurring."
"in this study, a low-cost anthropomorphic myoelectric prosthetic hand with five degrees of freedom (dof) has been designed and manufactured as a wearable robot for transradial amputee. the proposed myoelectric prosthetic hand is equipped with five grip pattern modes for grasping various objects. the proposed control has good performance result on the astohand v3.0 myoelectric hand for controlling the finger flexion and extension motion using emg sensor as an input. when the steady-state error reaches below 0.55 mm, the dc motor in the linear actuator will be turned off by the controller. the proposed feedback control strategy can reduce excess power usage during myoelectric hand performing finger flexion and extension motion."
"we have described how to exploit the admm for accurately segmenting oil spill regions in sar, uavsar, and modis blurry images. existing strategies tend to consider deblurring and segmentation as two separate steps and the deblurring procedure is not guided in favor of an accurate segmentation. to address this shortcoming, we have formulated images deblurring and oil spill segmentation in one overall energy functional, and established a linear mapping for characterizing the deblurring and segmentation relationship. we have used admm to minimize the overall energy functional. the iterative updates in admm not only fuse information both from deblurring and segmentation but also convey the fused information updates for deblurring and segmentation. therefore, our framework is able to perform effective deblurring in favor of an accurate segmentation. experimental results have validated that our framework outperforms the separate deblurring and segmentation strategy for detecting oil spill regions in blurry nowpap sar, uavsar, and modis images."
"according to the standard admm computation, minimization of the admm form of the overall energy functional is implemented in terms of iterating the following three basic steps."
"in order to further evaluate the proposed method, in addition to the experiments on the nowpap dataset, we test our method on three airborne l-band uninhabited aerial vehicle synthetic aperture radar (uavsar) images with hh-and vv-polarization and a spatial resolution of 6 m. the experimental results are illustrated in figs. 5 and 6 ."
"this section introduces the energy functional for segmenting oil spills in sar images. a level set energy term for measuring oil spill fitness, and regularization terms with respect to oil contour length, update regularity and oil edge preservation are developed separately. they are then integrated for characterizing the oil spill segmentation."
"in order to efficiently solve (24), which involves sophisticated operations such as convolution and gradient, we exploit the additive operator splitting method and obtain k k +1 b as follows:"
"i.i.t. delhi campus has been taken as one of the sampling sites for present study as control site because there is minimum commercial activities. the site selection criteria involves existence of different stratums of population based on the socioeconomic status i.e., 'high', 'middle' and 'low' income groups. in i.i.t. delhi campus, total 3 houses are selected for the experimental part i.e. one each from low, middle, and high income groups."
the gradient characterizes the oil and background variations over the image domain. the regularization term (3) penalizes abrupt changes in k b and thus avoids restoring an image with spikes.
"furthermore, we test our method and different comparison methods on moderate-resolution imaging spectroradiometer (modis) oil spill images and the visual results are given in figs. 7 and 8. modis is not sar and the experimental evaluations on the modis images validate that our framework is not only suitable for processing sar images but also potentially applies to other types of remote sensing images for segmenting oil spills."
"in this research, a low-cost anthropomorphic myoelectric hand based on 3d printing technology was developed using tendon-spring mechanism. feedback control strategy was developed for controlling the finger flexion and extension of five grip pattern modes. the feedback control strategy aimed to reduce excess power usage during myoelectric hand performing finger flexion and extension motion. linear actuator with potentiometer feedback incorporating non-backdriveable mechanism was utilized as the actuators. the proposed myoelectric hand was tested in various object grasping test using selected emg sensor. the hand was used to pick and grasp a fragile object like an egg by using human visual feedback."
"further, pearson's and spearman's correlations have been used to find correlation between age and respiratory problems, respiratory problems and gender (female population only) and education and occupation. the correlation coefficient value of -0.58 shows a high degree of negative relationship between women and respiratory problem score at the sample site. similarly, at the control site, correlation coefficient values (-0.37) at lig and (-0.54) at hig show a moderate negative relationship between gender and respiratory problem scores. hence, the female population is more vulnerable to the respiratory problems amongst the occupants at both sites."
"the majority of the respondents at the sample site complain of high sbs symptoms due to inadequate ventilation, high occupancy level, and improper maintenance of appliances and ignorance of the respondents about proper ventilation. a negative relationship existed between the sbs score and the income level of the respondents. the higher the income level, the lower the sbs score. thereby, showing that as the income level increases, the awareness regarding health issues also increases. table 3 :"
"where l is the lagrange multiplier to be optimized, and ρ is a positive balancing coefficient. the variables r 1, r 2, and k b are considered as intermediate variables in the admm form of the overall energy functional and thus are not global."
"generally, a digital image contains three distinguished types of redundancy; namely, inter-pixel redundancy, coding redundancy and psychovisual redundancy [cit] . since a digital image contains a huge amount of psychovisually redundant data, baseline jpeg focused on removing this redundancy [cit] . to achieve the required target, baseline jpeg follows the steps as illustrated in figure 1. for color images, the color planes are separated before the algorithm is applied and the rgb color domain is transformed to y c b c r color domain in order to separate the luminance and chrominance signals [cit] . the separation of color space is mandatory for color images, but transformation to y c b c r color domain in addition has been kept optional [cit] ."
the decoding phase of our proposed improvement is relatively simple. we look only for the candidate coefficients and simply multiply them by 2. then the baseline jpeg decoding steps are applied. figure 2 illustrates the selective quantization approach. however one problem that may be arised during decoding phase is mentioned and solved in section 5
"the rest of the paper is organized as follows. after a careful explanation of the background of the study, the suggested selective quantization approach is discussed in section 3. for more clarification, a detailed example is worked out in section 4. the anomalies introduced by our proposed modification are taken into consideration and techniques to handle them efficiently are presented in section 5. further lossless compression is proved by comparative compression ratio and peak signal to noise ratio analysis in section 6. in that section, an exact matching of psnr of our method with the psnr of baseline jpeg proves that the image information other than that are lost by baseline jpeg algorithm is indeed preserved in a lossless manner. finally, we discuss the application of proposed improvement for convenience of future research work in section 7 and conclude thereafter."
another branch of studies focused on further compression of jpeg images rather than the quality improvement [cit] . some of these studies [cit] approached a lossless manner for further compression as the proposed method does while the other tried to make further compression in a lossy manner. one research attempted to achieve better compression ratio along with better quality [cit] although it changes baseline jpeg algorithm considerably.
"although, at first glance, it seems that increasing one bit in category 2 will also increase total bit length of the coefficients, but it is not the usual case. it may only happen when there is no or very small number of candidate coefficients present in the image signal. however, signal coefficients vary in amplitude and it is almost impossible to predict for which images the propsed method may not work. in general, there are good number of candidate coefficients present in the image signal as studied in 6 and the total number of bits reduced is much greater than the number of times we increase one bit for category 1."
"the jpeg has four distinct modes of operation [cit] . the lossy sequential dct based mode (or baseline jpeg) is most popular among them since it can achieve highest compression ratio beside preserving maximum possible image quality. although the expanded lossy mode can achieve more compression, it has not been generalized to the end users since the amount of data loss is much higher. because the achievable compression ratio is lower, the lossless mode is also not in consideration. therefore, the baseline jpeg stands as a standard for compressing digital images [cit] ."
"for our case, ambiguities arise only for the coefficients ±1s. therefore, we take only category 2 into account. as stated earlier, we should consider the total bit length of category 2 as 5 instead of 4. now, whenever the decoder finds a coefficient starting with 011 (for dc only), it first decides the category as 1. now, only for category 1, it will consider the next 1 bit to find out its subcategory. if the next bit is 0, it will consider the coefficent as existing ±1 else it will consider it as resulting ±1. the final bit denotes the sign of the coefficient. if the final bit is 0, it is −1 and otherwise it is +1. the similar approach can be done for ac coefficients as well."
"the motivation of further compression of baseline jpeg is influenced by the fact that this format is in widespread use in almost all sorts of digital camera and other digital devices. yet, there have been very few researches that attempted to reduce the number of bits obtained by baseline jpeg images while keeping the image quality unaffected."
"in this paper, our study explores the redundancy of data in dct domain and reveals a straight forward way to make a further compression of baseline jpeg without any additional loss of image information. we incorporate a selective quantization approach with the baseline jpeg that essentially reduces a particular type of redundancy from dct domain. our proposed improvement has been able to obtain better compression ratio or lesser number of bits as compared to baseline jpeg standard."
"generally, all jpeg coefficients contain a base code or prefix code that helps the jpeg decoder find out the category the coefficient belongs to. if the category is n, the next n bits immediately after prefix code is considered for the coefficient being decoded. for example, let us consider a bit stream starting with prefix 100. the decoder finds the category 2 for this coefficent in a look up table manner [cit] . also, the decoder understands that the next 2 bits after 100 is to be considered for the coefficient being processed. now, all possible combinations of 2 bits is 4 and these are 00,01,10 and 11 respectively. since the decoder knows from table 1 that there can be at most 4 symbols (−3, −2, +2, +3) in category 2, it considers the first 2 bits combinations 00 as −3, 01 as −2 and so on. however, we will use a little bit different approach in order to distinguish our existing and resulting ±1s."
"since jpeg itself defines a variable quantization [cit], more compressing a jpeg file in a lossy manner is of no use. hence, only further compression of jpeg images in a lossless manner deserves attention. our proposed method, belonging to this category of studies, achieves better compression ratio as compared to jpeg without any additional loss of information. in addition, our proposed selective quantization approach is directly applicable to any further compression of jpeg images as referenced in this paper."
"in this paper we presented a novel compression technique for further compression of jpeg images without any additional loss of image information. although the optimal coding stage stands as a barrier while further compressing jpeg images, the application of suggested selective quantization approach before optimal coding eased the entire compression scheme. statistical evidence has been shown against the appearance of candidate coefficients and exact matching of psnr proved the suggested method as lossless. for both gray-scale and true-color images, experimental results clarified that the proposed method gained better compression ratio. moreover, the future scope of the proposed method has been discussed. the impact of jpeg images is widespread, incorporation of the proposed method with baseline jpeg will help solve the increasing problem of storing numerous digital images within a little given storage area."
"the baseline jpeg uses a default huffman coding table in entropy encoding stage so that the encoded bit stream can be uniquely decoded [cit] . however, run-length or arithmetic coding can also be applied in this stage without compromising the resulting number of bits since all of them are optimal coding [cit] . as an already optimally compressed bit stream can not further be compressed [cit], any effort to further compression of a jpeg coded bit stream is meaningless. however, since the baseline jpeg default huffman coding results different depending on the distribution of the frequency of the input data set, if the data set of an image applied for huffman coding is modified, obtaining more optimum result is still possible."
"the further studies of jpeg images can be categorized in two branches-one emphasizes the quality enhancement of jpeg images while the other took the further compression into consideration. being a lossy mode of image compression, jpeg sometimes losses important image information along with psychovisually redundant data. again, the quality-compression trade-off of digital images persuaded the researchers to improve or measure the quality of well compressed jpeg images [cit] ."
"the selective quantization approach is an easy, simple and convenient method for further compressing jpeg images with keeping the quality unaffected. this method is directly applicable on the further jpeg image compression techniques as referenced by this paper or on any data compression technique that processes a sequence of integers at least once. the compression ratio achieved by those studies can be enhanced in a lossless manner by incorporating the selective quantization approach. the further study of this research may include a detailed study that will help precisely decide what sorts of images may not totally be considered for further compression using the proposed method. the default jpeg quantization matrix can be re-organized following the selective quantization approach so that quality assurance is facilitated. implementation of selective quantization with baseline jpeg can be conducted for specialized computing devices, where storage requirement is a crying need."
"as shown in fig. 2, a quav is made up of four electric motors fixed on an x-shape frame. the earth-fixed coordinate ox o y o z o and the body-fixed coordinate o xyz are considered with the origin coinciding to the gravity center of the quav. in the earth-fixed frame, the z 0 -axis points upward, and the quav position is given by a vector [x, y, z] t . the quav orientation refers to as roll, pitch, and yaw, and is given by the vector [φ, θ, ψ] t which is measured with respect to the earth-fixed coordinate. actually, the entire model of the quav is composed by position dynamics, euler angles, angular velocity, propeller speed, and bldc motor dynamics."
"r ecently, increasing attention has been paid to vertical take-off and landing (vtol) unmanned aerial vehicles (uavs) pertaining to a wide area of vital applications, including patrolling for forest fires, traffic monitoring and surveillance rescue via hovering, tracking and coordination [cit] . recently, flapping-wing flying robotics have also attracted much attention by devising novel neuro-adaptive methods [cit] . compared with fixed-wing aircrafts, the rotary-wing uav possesses the significant advantage that it can take-off and land vertically in limited spaces and is easy to hover over the target. note that the quadrotor uav (quav) is a typical vtol-uav with simple mechanical structure and favorable maneuverability. in this context, as a remarkable platform of the uav, the quav has attracted numerous research [cit] ."
"since constrained actuator dynamics are sufficiently addressed, virtual control signals pertaining to euler angles, rotation squares, and armature voltages of nonreversible motors with input saturations and dead zones are reasonably constrained by the backpropagating constraint (bc) extraction. in addition, intermediate tracking discrepancies and complex unknowns are further be attenuated by a family of universal adaptive compensators (uacs). eventually, the lyapunov approach ensures that the entire closed-loop bcttc system is asymptotically stable, and trajectory tracking errors together with other signals are uniformly ultimately bounded."
"furthermore, we have provided a series of numerical results to validate the analytical results. we find that numerical results are in agreement with the analytical results for strong selection, yet may deviate from the analytical results for weak selection."
"in order to facilitate our control scheme, bc on intermediate signals are extracted from nonreversible actuator dynamics and saturations. key results are summarized as follows."
"the actual and reference trajectories in 3-d space are shown in fig. 5, from which we can see that the bcttc scheme can render the quav track the desired trajectory accurately in the presence of both mismatched and matched complex unknowns. individual positions, i.e., x, y, and z, and the yaw angle ψ together with their desired targets are shown in fig. 6, from which we can see that the quav using the bcttc scheme can track the desired individual trajectories with fast response and high accuracy, simultaneously, whereby tracking errors are shown in fig. 7 . intermediate tracking results for other states including euler angles, angular velocities, propeller speeds, and motor rotations are shown figs. 8-11, respectively, which demonstrate that accurate tracking of intermediate states can still be guaranteed under the constraints on propeller speeds, and motor rotations (shown in figs. 10 and 11) . eventually, control inputs to four motors are shown in fig. 12, from which we can see that nonreversible constraints and saturations have been effectively addressed."
"satellite plays a key role in the telecommunications networks. it is able to resolve the problem of last mile providing connections to those areas where no investment return will be possible because a large investment is required to bridge services between the local exchange and the customers. satellite, thanks to its broadband nature, is able to provide connection in the rural areas and isolated areas with the same investment of other areas. users has only to install a satellite terminal (see figure 11 ) and subscribe to the service and they are able to receive satellite information. moreover, thanks to the new rcs standard the customers can also use the network interactivity exploiting the return satellite channel that is faster than terrestrial one which is based on telephony network that are limited in providing high bit rates to subscribers. it will be necessary to perform a fiber cabling also between local exchange and subscribers in order to make faster the connections. big investments are required by telephone companies to perform a fiber cabling in order to guarantee high bandwidth to each subscriber. it is clear that the increase of multimedia services poses the problem of provide high bandwidth to the users by operators and in this context the use of satellite platform can represents an optimal solution in terms of costs. moreover the satellite segment guarantees also an overall integration with all communication technologies."
"we consider the case of 0vk xy v1. in this case, linking dynamics does not assume rationality of agents: adverse cd links may be kept and advantageous cc links may sometimes be broken."
"equating f c and f d or, equivalently, substituting them into eq. (12), we find that an unstable equilibrium x ã c [ (0,1) emerges when"
"in the last few years multimedia applications are grew very fast in all networks typologies and in particular in wireless networks that are acquiring a big market slice in the telecommunication field. this trend of applications has pushed the researchers to perform a lot of studies in the video applications and in particular in the compression field in order to be able of transporting this information in the network with a low impact in the system resources. in literature a lot of studies exist on video compression and new standard of compression are proposed in order to be able to transmit video traffic on different network technologies that often have resource problem in terms of bandwidth capacity and, then, a very performance compression algorithm can give a great support to network service provider in respecting the quality constrains otherwise nor achievable. the ubiquitous nature of all multimedia services and their use in overall telecommunications networks requires the integration of a lot of technologies that aim to improve the quality of the applications received by the users. first of all, it is clear that the traditional concept of best effort paradigm in the delivery of multimedia contents is not possible to adopt because it does not match with the users requirements. this type of approach try to do its best but it is unable of guaranteeing any form of users requirements. in order to address this type of problem, recently different quality of services architectures have been proposed capable of guaranteeing to the multimedia streams the users constrains. the most famous architectures are integrated services and differentiated services that manage different class of services in order to allow a traffic differentiation able to discriminate also a cost differentiation for the customers. it is easy to understand that this new type of applications is based on a constantly reliable reception of information that it is possible to have only through an appropriate network management. the new end users are always more quality aware and then they are more exigent and accordingly networks have to guarantee always more capacity in order to satisfy their users. as a consequence, there is a continuous and extensive research effort, by both industry and academia, to find solutions for improving the quality of multimedia content delivered to the users; as well, international standards bodies, such as the international telecommunication union (itu), are renewing their effort on the standardization of multimedia technologies. there are very different directions in which research has attempted to find solutions in order to improve the quality of the rich media content delivered over various network types [cit] . moreover, it is very important to determine efficient quality assessment. it is really important to know how the network behaves in terms of parameters of service for end users in order to take the correct decisions during the development, evaluation, construction and operation of network services."
"remark 2: in view of the squares of propeller and rotor speeds, i.e., w 2 i and w 2 ei in (13) and (16), respectively, together with (7), virtual control signals u u u 1 (·), u u u 3 (·), and u u u 4 (·) in (4), (11), and (14), respectively, are expected to be constrained for ensuring the positiveness and boundedness of speed squares. in addition, actuator dynamics with complex constraints arising from insensitive dead-zone voltages, bounded armature voltages, and nonreversible rotations have been completely formulated in (17)- (19), and thereby leading to constraints on control input nonlinearity u u u 5 (·). to our best knowledge, all aforementioned concerns on backpropagating cascade constraints and complex actuator dynamics have not been addressed in the literature."
"the rapid growth of multimedia application together with advanced development in digital technology and with the increased use of mobile terminal has pushed the research toward new network technologies and standards for wireless environment. moreover, everincreasing computing power, memory, and high-end graphic functionalities have accelerated the development of new and exciting wireless services. personal video recorders, video on demand, multiplication of program offerings, interactivity, mobile telephony, and media streaming have enabled viewers to personalize the content they want to watch and express their preferences to broadcasters. viewers can now watch television at home or in a vehicle during transit using various kinds of handheld terminals, including mobile phones, laptops computers, and in-car devices. the concept of providing televisionlike services on a handheld device has generated much enthusiasm. mobile telecom operators are already providing video-streaming services using their third-generation cellular networks. simultaneous delivery of large amounts of consumer multimedia content to vast numbers of wireless devices is technically feasible over today's existing networks, such as third-generation (3g) networks. as conventional analog television services end, broadcasters will exploit the capacity and flexibility offered by digital systems. broadcasters will provide quality improvements, such as high-definition television (hdtv), which offer many more interactive features and permit robust reception to receivers on the move in vehicles and portable handhelds. mobile tv systems deliver a rich variety of content choice to consumers while efficiently utilizing spectrum as well as effectively managing capital and operating expenses for the service provider. mobile tv standards support efficient and economical distribution of the same multimedia content to millions of wireless subscribers simultaneously. mobile tv standards reduce the cost of delivering multimedia content and enhance the user experience, allowing consumers to surf channels of content on a mobile receiver. mobile tv standards address key challenges involved in the wireless delivery of multimedia content to mass consumers and offer better performance for mobility and spectral efficiency with minimal power consumption. an important aspect of multimedia delivery contest is the possibility of make integration between different networks in order to be able of reaching users every-time every-where. then, it is possible to use a multi-layer hierarchic platform that use satellite [cit] segment together with wireless network based on 802.11 standard [cit] and with cellular network in order to have an ubiquitous coverage. in 2 order to grapple with the continuously increasing demand on multimedia traffic over high speed wireless broadband networks it is also necessary to make network able to deal with the qos constraints required by users. in order to provide quality of service to user applications, networks need optimal and optimized scheduling and connection admission control algorithm. these mechanisms help to manage multimedia traffic guaranteeing qos to calls already admitted in the system and providing qos to the new connection. in order to evaluate the quality of video traffic with the mobility it is important to examining the quality assessment techniques: subjective and objective quality assessment."
"in this chapter an analysis of the multimedia traffic over wireless and satellite networks has been shown. the importance of managing multimedia applications nowadays in the overall networks is an incontrovertible fact of our life. moreover, the rapid increased use of mobile terminal together with video and audio services have pushed the research and the researchers toward new standards and technologies capable of dealing with these new users requirements. an important aspect of multimedia delivery contest is the possibility of make integration between different networks in order to be able of reaching users every-time every-where. personal video recorders, video on demand, multiplication of program offerings, interactivity, mobile telephony, and media streaming have enabled viewers to personalize the content they want to watch and express their preferences to broadcasters. viewers can now watch television at home or in a vehicle during transit using various kinds of handheld terminals, including mobile phones, laptops computers, and in-car devices. the concept of providing television-like services on a handheld device has generated much enthusiasm. mobile telecom operators are already providing video-streaming services using their third-generation cellular networks. simultaneous delivery of large amounts of consumer multimedia content to vast numbers of wireless devices is technically feasible over today's existing networks, such as third-generation (3g) networks. the concept of mobility has push toward wireless solutions such as terrestrial wireless networks and satellite one. moreover, in the recent years new standards have been proposed for a integration of this two types of platforms giving the birth of hybrid solution like dvb-sh and sdmb standard. these standards provide integration between satellite and 3g networks in order to guarantee services also to those areas where the terrestrial infrastructures are impossible to install both for the particular territorial morphology and for economical issues."
"where the parameter u, measuring how profitable unilateral defection is, ranges from zero to one. note that this payoff matrix recovers the payoff ranking described above, twrwpws. we emphasize that eq.(3) describes a special case of general pd games, but it is widely used in biology and sociology [cit] ."
"a key result on stability analysis is summarized as follows. theorem 1: consider a complex quav system (4), (8), (11), (14), and (17), together with the proposed bcttc scheme (49), (50), (51), (69), (79), (92), (105) and (112)"
"we have explained intuitively how r enhances cooperation level. in addition, we can also show analytically that large r leads to cooperation by enlarging the cooperation attraction basin:"
"the subjective methods are not feasible during the design of a network. these are limited, impractical and very expensive. to overcome these problems have been developed, new methods that allow the calculation of values that represent the different combinations of factors of damage that could affect the network. the primary purpose of these methods is to produce an estimate of quality, providing the results as comparable as possible to mos values. the itu is proposing a method of objective testing, automatic and repeatable, which takes into account the perceived quality. different types of objective metrics exist [cit] . for the analysis of decoded video, we can distinguish data metrics, which measure the fidelity of the signal without considering its content, and picture metrics, which treat the video data as the visual information that it contains. for compressed video delivery over packet networks, there are also packet-or bitstream-based metrics, which look at the packet header information and the encoded bitstream directly without fully decoding the video. furthermore, metrics can be classified into full-reference, no-reference and reducedreference metrics based on the amount of reference information they require. the most known video quality metric is called peak signal to noise ratio (psnr) that is calculated simply as mathematical difference between every pixel of the encoded video and the original video. the other popular metric is the classical mean square error (mse) is one of many ways to quantify the amount by which an estimator differs from the true value of the quantity being estimated."
where the first factors are always positive and f 0 c~k cd k dd x c and f 0 d~( 1zu)k cc k dd x c zuk cc k cd x d are the payoffs of cooperators and defectors in a modified game with payoff matrix
"nowak and may first studied the pd game on regular lattices [cit] . subsequently, social dilemmas on regular graphs have been investigated [cit] 29] . many authors have also considered more complex networks, such as scale-free and small-world afterwards [cit] . it has been well recognized that network topologies can play a crucial role in the evolution of cooperation, in addition to the payoff matrix and the update mechanism."
"thus, w xy also represents the average fraction of xy links in the whole population in the stationary regime of the linking dynamics. let us now consider the dynamics of strategies (which occurs with probability w). a player i with strategy s i is selected at random, subsequently player j with strategy s j is randomly selected among i's current neighbors. player i compares the payoff with that of player j and takes strategy s j with probability [cit]"
"to sum up, we have established a discrete model to describe the stochastic linking dynamics analytically in terms of a markov chain. based on this linking dynamics, we have studied the coevolution of strategy and network structure. a simple condition for the evolution of cooperation is obtained analytically that becomes more accurate when selection is stronger. the rule shows that the less fragile cc links are, the easier cooperation emerges. the more fragile cd links are, the easier cooperation prevails."
"in order to make intensive insight into the superiority of the bcttc, quantitative comparisons using integrated absolute error (iae) and integrated time absolute error (itae) indices for tracking errors are summarized in table ii . clearly, it can be seen that the proposed bcttc scheme is significantly superior to the pd control approach. it should be noted that the pd control strategy cannot tackle constrained actuator dynamics. as a consequence, as shown in fig. 15, negative squares of rotor rotations reversely deriving from pd control input torques would unreasonably occur, and thereby leading to unreachable control efforts in practice and even destroying system stability. similarly, those methods taking rotor torques as control inputs would inevitably suffer from the aforementioned negative-square dilemma. in this context, the proposed bcttc scheme via bcs due to constrained actuator dynamics can definitely guarantee reasonable control signals which can be completely executed by actuators."
note that the actually desired control law u u u 3d can be derived from (49) . together with the following equations deriving from (7) and (13): where μ μ μ 4 is determined later. design a sliding surface as follows:
"in this paper, we address the trajectory tracking problem of a quav with backpropagating cascade constraints, complex actuator dynamics and mismatched unknowns within the entire dynamics (4), (8), (11), (14), and (17). our objective is to design a bcttc such that the complex quav can track the desired trajectories under mild conditions as follows."
"unfortunately, aforementioned cascade constraints have not been addressed to date [cit], although individual loops can facilitate the smc approach. 2) dealing with actuator dynamics: as analyzed above, actuator dynamics including transient responses and control input constraints would directly affect and limit the torque inputs to propeller rotation dynamics. clearly, treating actuator dynamics as input nonlinearities/uncertainties [cit], linearized dynamics [cit] or stationary mappings [cit] would hardly determine feasible input torques generated by propellers, and thereby resulting in uniformly unreachable regions within the desired control efforts. nevertheless, bldc motors in a quav are not allowed to rotate reversely such that uniformly upward thrust forces can be generated. in this context, it becomes empirical and risky to design control laws for torque inputs if bldc dynamics are omitted and torque control signals are directly fed into the electronic speed control (esc) module which generates 3-phase ac voltages via pwm signals. hence, incorporating actuator dynamics into the quav model is strongly desirable for pursuing high autonomy. however, to our best knowledge, few attention to systematically dealing with actuator dynamics including control constraints has been paid for a quav. in this paper, we focus on trajectory tracking control of a quav including cascade constraints, constrained actuator dynamics and complex unknowns, which is unsolved in the literature. by incorporating the smc and dsc approaches into a backstepping-like framework, a backpropagating constraints-based trajectory tracking control (bcttc) scheme is proposed by devising extraction tools for cascade constraints. in the presence of actuator dynamics, unmodeled dynamics, uncertainties, measurement noises and external disturbances, the entire quav dynamics are formulated in a vectorial pure-feedback form with unmatched unknowns whereby intermediate constraints and underactuated dynamics appear in a cascade mode, and make traditional backsteppingbased approaches unavailable. in this context, the bcttc framework using the smc is realized to circumvent both cascade constraints and underactuation issues, and recursively stabilize tracking errors. the dsc technique is further deployed to facilitate the derivation of intermediate signals."
"in this paper, the bcttc scheme for trajectory tracking of a quav with constrained actuator dynamics and complex unknowns has been proposed. unlike previous works, the entire quav system has been decomposed into five cascade subsystems connected by intermediate nonlinearities. in this context, smc-based subcontrollers have been recursively designed by addressing underactuation and cascade constraints, whereby the preceding subcontroller provides desired signals for the succeeding subsystem. in addition, first-order filters have been employed to avoid the smoothness requirement and decouple the iterative design within the backstepping-like procedure. by virtue of bcs, intermediate controls have been shaped within reachable regions determined by constrained actuator dynamics including saturations and dead zones. furthermore, uacs have been employed to dominate complex unknowns together with bc-based intermediate discrepancies. using the lyapunov approach, bcttc tracking errors can be made arbitrarily small and all signals are bounded. simulation studies have shown that the proposed bcttc scheme can achieve high-accuracy tracking under constrained actuator dynamics and complex unknowns, and is remarkably superior to previous approaches without addressing actuator constraints or inner nonlinearities."
"where μ μ μ 1 is a dynamic compensator determined later, χ χ χ 12d is a virtual control signal,χ χ χ 12d is the filtered output of χ χ χ 12d given by"
"where μ μ μ 5 is determined later, u u u 5 (·) is the nonlinear input constrained by saturation and dead zone in (18) and (19) . design a sliding surface as follows:"
"here, sat(·) is defined by (1), u 0 5i and u m 5i are the dead zone and the saturation of armature voltages, respectively, and 54 ] t is the ideally nominal control input, and"
"remark 6: the derivation of (74) from (73) can be obtained by assigning a given reference yaw angle ψ d . in addition, the first equation of (74) ensures the desired total thrust t d is reasonable, whereby possible saturation can be tackled later."
"remark 12: from (49), (50), (51), (69), (79), (92), (105), and (112), we can see that the computational complexity of the bcttc scheme is similar to adaptive approximation-based state-feedback approach."
"over the years have developed many models, more efficient and less expensive, able to consider more types of factors involved in the telecommunication networks. an overview of models of measurement is reported in the figure 1 below. they are divided substantially in two categories: subjective assessment and objective assessment."
"this critical value x ã c determines the attraction basin of cooperation (x ã c,1: cooperators take over when their initial frequency x c (0) is larger than this critical value, whereas defectors take over when x c (0) is less than this critical value. in other words, the evolutionary pd game with linking dynamics is similar to that of the coordination game in well mixed population where both cooperation and defection are best replies to themselves."
"nowadays multimedia communication over wireless and wired packet based networks is growing up. in the past, many applications were used for video downloads, while now they take up the share of all traffic on the internet. most mobile devices can actively download and upload photos and videos, sometimes in real time. in addition, voice over ip (voip) is heavily changing the voice telecommunications world and the enhanced television is also being delivered into the houses over ip networks by digital subscriber line (dsl) technologies. another issue in the multimedia revolution takes place inside the home environment: the electronics manufacturers, the computer industry and its partners are distributing audio and video over local-wifi networks to monitors and speakers around the house. now that the analog-to-digital revolution is going to be complete, the \"all media over ip\" revolution is taking place, with radio, television, telephony and stored media all being delivered over ip wired and wireless networks. figure 3 shows an example of different multimedia applications in a home environment. the growing and the emergence of communication infrastructures, like the internet and wireless networks, enabled the proliferation of the above mentioned multimedia applications (music download to a portable device, watching tv through the internet on a laptop, viewing movie trailers posted on the web through a wireless link). new applications are surely revolutionary, like sending voip to an apparently conventional telephone, sending television over ip to an apparently conventional set top box or sending music over wifi to an apparently conventional stereo amplifier. the exposed applications include a big variety of new multimedia related services but, unfortunately, the internet and the wireless networks do not provide full support for multimedia applications. the internet and wireless networks have stochastic and variable conditions influenced by many factors, however variations in network conditions can have heavy consequences for real-time multimedia applications and can lead to unacceptable user experience, because multimedia transmissions are usually delay sensitive, bandwidth intense and loss tolerant. the theory that has traditionally been taught in information theory, communication and signal processing may not be directly applied to highly time-varying channel conditions and, as a 6 consequence, in recent years the area of multimedia communication and networking has emerged not only as a very active and challenging research topic, but also as an area that requires the definition of new fundamental concepts and algorithms that differ from those taught in conventional signal processing and communication theory. it is clear that best-effort (be) ip networks are unreliable and unpredictable, expecially in wireless networks, where there can be many factors that affect the quality of service (qos), that measures the performance of a transmission system via parameters that reflect its transmission quality, such as delay, loss and jitter. in addition, congested network conditions result in lost video packets, which, as a consequence, produces poor quality video. further, there are strict delay constraints imposed by streamed multimedia traffic. if a video packet does not arrive before its \"playout time\", the packet is effectively lost. packet losses have a particularly devastating effect on the smooth continuous playout of a video sequence, due to inter-frame dependencies. a slightly degraded quality but uncorrupted video stream is less irritating to the user than a corrupted stream. controlled video quality adaptation is needed to reduce the negative effects of congestion on the stream whilst providing the highest possible level of service and quality. the applications based on streaming operations are able to split the media into separate packets, which are transmitted independently in the network, so that the receiver is able to decode and play back the parts of the bit stream that are already received. the transmitter continues to send multimedia data packets while the receiver decodes and simultaneously plays back other already received parts of the bit stream. the philosophy of playing back received packets allows the reduction of the delay between the transmission time instant and the moment at which the user views the multimedia content. having a low delay is of primary importance in such kind of systems, where interactive applications are dominant (for example, a video conference or a video ondemand architecture). the transmission of multimedia content can be categorized into three main classes: unicast, multicast and broadcast, depending on the number of senders and receivers. unicast 7 transmission connects one sender to one receiver (point-to-point connection, p2p), as downloading, on-demand streaming media and p2p telephony. the main advantage of unicast is that a feedback channel can be established between the receiver and the transmitter, so the receiver can return information to the sender about the channel conditions which can be used accordingly by the transmitter to change transmission parameters. in a multicast communication the sender is connected to multiple receivers that decide to join the multicast group; multicast is more efficient than multiple unicast flows in terms of network resource utilization, since the information must not be replicated in the middle nodes (it is obvious that in a multicast communication the sender cannot open a session toward a specific receiver). in a broadcast transmission, the sender is connected to all receivers that it can reach through the network (an example is a satellite transmission); the communication channel may be different for every receiver. one possible approach to the problem of network congestion and resulting packet loss and delay is to use feedback mechanisms to adapt the output bit rate of the encoders, which, in turn adapts the video quality, based on implicit or explicit information received about the state of the network. several bit rate control mechanisms based on feedback have been presented in the last few years. as the real-time control protocol (rtcp) provides network-level qos monitoring and congestion control information such as packet loss, round trip delay, and jitter. many applications use rtcp to provide control mechanisms for transmission of video over ip networks. however, the network-level qos parameters provided by rtcp are not video content-based and it is difficult to gauge the quality of the received video stream from this feedback. now, in the following paragraphs, multimedia transmission techniques will be deeply introduced, as well as different policies dedicated to evaluate the quality (and the distortion) of the perceived content."
"as already mentioned, the advantages of broadband satellite are undoubtedly significant: in addition to the ubiquitous availability of the service and the high speed of navigation available to users, in fact, are also to remember the cost lower than terrestrial connections, the possibility of subscribing services also with foreign companies, which means you have greater choice and range of commercial offers, and the opportunity to receive satellite tv channels on your personal computer at home or office. another technical advantage is the equipment concentrated in a single box, called set top box. another reason for choosing a return channel via satellite is the significant increase in traffic of terrestrial networks that often reduces the quality of service (qos). finally, the forward channel and return channel is available on the same medium. this enables better control of qos and network management by a single operator, which is not the case with terrestrial infrastructures that are not always handled by the same operator. qos parameters include point-to-point delay, delay variation and packet loss. these parameters are measured on a path point-to-point, where the delay of propagation of the satellite was taken into account properly. in contrast, however, is also worth pointing out those that may be the defects of this type of connection as the inability to upload direct from most of the services, the excessive investment for purchasing the necessary equipment for connection (satellite dish, lnb device, etc..), the problem of utilizing the phone line every time you make a web browsing (and the cost of calls made), the possibility of jamming signal that may occur due to the repositioning of satellites and weather situations, short delays in signal transmission due to the distance between the satellite and earth, and, not least, the difficulties of implementation and management of the connection that you can present to users are not particularly expert in the use of pc . the choice of the satellite connection is recommended, as well as to residents in areas not yet reached the service of terrestrial broadband, especially for users and companies that use the network to browse or download files of large size, but just recommended to all those realities, both professional and private, that are able to choose lines of traditional fast (even fiber optic) and require to send files on the network particularly heavy in terms of kb."
"the satellite digital media broadcasting (s-dmb) system [cit] consists in an overlay network based on satellite communications dedicated to terrestrial umts segments as it is possible to see in figure 16 . it is used when multimedia content, like tv programs, or non real-time multimedia services should be delivered to mobile nodes. this can be done by the use of geostationary satellites and low power terrestrial stations, which act like gap-fillers in order to cover urban and indoor environments. they can be located in the same places of mobile base stations. satellites and terrestrial repeaters communicate with synchronized signals and the end-user nodes can use such kind of signals to improve reception quality. the s-dmb system is based on 18 channels (at 128kbps) in 15mhz, fully compliant with the umts multimedia broadcast/multicast service (mbms). in this way, the integration among s-dmb/umts can become very useful. the streaming of tv programs on the own handset is suitable for end-users: it can be done wherever they are (waiting for a bus, for a taxi, relaxing in a park, etc.); the only needed thing is a 3g compatible phone, although new technologies are taking place, like dvb-h or t-dmb. the dmb is the natural extension of the digital audio broadcasting (dab) and there are two different versions of it: terrestrial or satellite dmb. figure 17 represents the countries where the broadcasting technologies are employed (europe, canada, australia, south africa, new zealand, india, china, south korea and turkey)."
"consumer the main target is the prosumer, a term used to describe a \"professional user\", which requires broadband, high-quality services and having the economic means to invest in relatively expensive equipment. the corporate customer is a further objective of the market, represented by a group of users who are behind a single terminal connected through a lan for example, to a single rcst (return channel satellite terminal). the consumer will probably be the last profile that feels the need to use a similar system, but it is true the fact that the rapid technological development, together with the growing need for high capacity bandwidth and services, it will certainly make a category of users realistic in the near future. let's look now at the applications on dvb-rcs:"
"remark 1: for the entire quav dynamics (4), (8), (11), (14), and (17), vectorial nonlinearities u u u 1 (·), u u u 2 (·), u u u 3 (·), and u u u 4 (·) are taken as virtual control inputs while the signal u u u 5 is referred to as the actual control input. as a consequence, a vectorial pure-feedback nonlinear system with interconnected nonlinearities can be innovatively established and is ready for backstepping-like controller design."
"remark 11: filters applied to virtual signals might cause high-gain problem pertaining to filter-backstepping (i.e., dsc) or high-gain observer design [cit] . in the bcttc scheme, unexpected magnitudes and/or peaks are actually saturated by bc-based constraints. the smc technique employed in subcontrollers is expected to enhance steady-state tracking accuracy via incorporating an integral term. actually, if integral gains k k k i are chosen as zeros, sliding-mode surfaces degrade to intermediate tracking errors."
"where sat(·) is defined in (1), w m ei and w m are maximal rotation speeds of motor rotors and propellers, respectively, \" u \" denotes the unsaturated signal of \",\" and saturation levels for u 3i are given by"
"the mpeg-2 transport stream (mpeg-2-ts) arises from the need to create a layer of data for a fault tolerant environment (see figure 13 ); and precisely, because this uses small packets. although originally targeted for the spread of video and audio encoded in mpeg2 for digital television, the mpeg2-ts is particularly suitable for the transport of ip datagram. the transport stream consists of packets in time division multiplexed belonging to different flows of information. as already mentioned, each packet is 188 bytes, four of which belong to the header and the rest are left to the payload of the data. the header contains 4 bytes of synchronization and identification (pid) of the package, the payload (184 bytes) contains the data to be transmitted, such as data, audio and video format in small packages elementary (packet stream element -pes). each pes is of variable length and contains a header and a body of data called pes data. the header of the pes includes a start code prefix, an identifier of flow, an optional length field, a pes-header and an optional number of bytes to fill. the remaining bytes are for data. standard dvb-rcs is born as an evolution of tdma networks where the carrier tdm (with bit rates up to 38mbps), transmitted from the master station, transports the data packets in time division multiplexed addressed to all terminals of the network, instead, tdma carriers (typically with bit rates up to 256kbps) are shared between peripheral stations that have to talk with the master station. in order to make interoperable different technologies it has felt the need, at the european level, to define a standard which regulates the proliferation of networks with a star standard and proprietary hard interfaced with other technologies. the other reason that has necessitated the establishment of a standard dvb-ip was the proliferation of interactive applications with higher volumes of information that could not be implemented with the dvb-ip standard in which the return channel, made by a terrestrial link with a modem, did not allow an adequate bit rate (up to 64kbps). the channel dvb-ip is referred to as direct channel or forward channel and the return channel rcs, return channel. the return channel has variable bit rate from 128kbps up to 204kbps and operates with tdma in multi-access (mf-tdma) in a dynamic way. the mf-tdma access allows to give a slot of tdma burst to terminals that request it in the pool of available frequencies. the channel dvb-ip or forward channel has a variable bit rate from 2mbps up to 45mbps and works with access tdm. the standard dvb-rcs can be implemented using a combination of ku or ka frequency for transmission and reception. the ka-band offers to the server provider economic transponders to get a higher spectral efficiency. such a system is designed to meet the needs of a wide section of users that can be distinguished into three categories:"
"the network topology is assumed to be static in the above work. however, social relationships between individuals are not eternal, but are continuously changing in the real world. therefore, the coevolution of strategy and network receives increasing attention [cit] ."
"furthermore, in order to demonstrate the superiority of the proposed bcttc scheme, comprehensive comparisons with a pd control scheme are conducted on previous settings. to this end, pd controllers are designed as follows: trajectory tracking result of the pd control approach and comparisons with the bcttc scheme are shown in figs. 13 and 14, respectively, and illustrate that the bcttc approach can accommodate complex unknowns, and thereby achieving nearly zero steady-state discrepancies which apparently appear in pd controllers."
"where (107), and leave onlyṡ s s 2 in (81) be driven by the input discrepancy u u u 2e which is closely related with the cascade sliding surface s s s 3 . in addition, as shown in fig. 4, the bcttc scheme is composed by four successive virtual subcontrollers in (69), (79), (92), and (105), and one actual subcontroller in (112). in this context, each subcontroller for an individual subsystem can be designed independently by using various approaches although the smc technique is exclusively employed in this paper. in essence, this significant advantage actually benefits from the bc-based cutting by virtue of bounded intermediate errors."
"where n i represents the neighborhood set of player i and m is the payoff matrix. instead of the general matrix of the prisoner's dilemma eq. (1) with four parameters, we consider a simpler payoff matrix,"
"a method to perform quality evaluation for multimedia applications is called subjective assessment and it represents an accurate technique for obtaining quality ratings. it is basically based on experience done by a number of users. typically, the experiments are made in a room where there are a certain number of persons (about 15-30) that they have to watch a set of video streams and, then, they have to give a rate based on own quality perception. on the basis of all rates given by all subjects included in the experiment it is formulated an average rate called mean opinion score (mos). it is clear that, being a subjective evaluation determined by the subjectivity and variability of the involved persons, this test is affected by the personal opinion that cannot be eliminated. in order to avoid this type of problem the experiments are made through precise instructions that give to the subject a set of precautions that they have to follow. moreover, also the environment used for the test is a controlled environment. in this way it is possible to perform a set of tests and provide a quality score that is a number that results from a statistical distribution. there are a wide variety of subjective testing methods. in literature different methods exist for giving a measure of the perceptual performance of subjects. the itu has formalized a series of conditions that are used for quality assessment in various recommendations [cit] . they suggest standard viewing conditions, criteria for the selection of observers and test material, assessment procedures, and data analysis methods. recommended testing procedures exist as absolute category rating (acr), degradation category rating (dcr), comparison category rating (ccr). in the test the presentation order of the video clips can be randomized between viewers in order to obtain a more realistic statistical score. after each video presentation, viewers are asked to judge its overall quality using the rating scale shown in figure 2 ."
t is the armature voltage vector of motors and is practically constrained by input nonlinearities including both saturations and dead zones due to irreversible rotation shown in fig. 3 as follows:
"the quav is a highly nonlinear system with underactuated constraints and strong couplings between actuator dynamics, and thereby leading to great challenges in controller design and synthesis. with the development of advanced control approaches, including sliding mode control (smc) [cit], dynamic surface control (dsc) [cit], fuzzy/neural control [cit], nonsmooth approaches [cit], etc., promising control schemes for the quav are pursued ceaselessly. in the literature, control methods of the quav can be actually classified into two kinds, i.e., model-based approaches including feedback linearization [cit], backstepping [cit], smc [cit], adaptive control [cit], model predictive control (mpc) [cit], robust control [cit], etc., and mode-free approaches including pid [cit], neural control [cit], fuzzy control [cit], etc. undoubtedly, the plant dynamics controlled would be dramatically simplified for linear/nonlinear pid controller design [cit] . backstepping-and smc-based adaptive robust control schemes can incorporate complex unknown dynamics and even uncertainties and/or disturbances by employing disturbance observers [cit] . furthermore, combining with model-free approaches, i.e., adaptive fuzzy/neural approximators, for uncertainties and/or unknown dynamics, tracking errors of an uncertain quav can be made bounded [cit] ."
"evolutionary game theory is an intuitive and convenient framework to study this puzzle. as a metaphor, the prisoner's dilemma (pd) has been widely used to investigate the origin of cooperation. in this game, two players simultaneously decide whether to cooperate (c) or to defect (d). they both receive r upon mutual cooperation and p upon mutual defection. a defector exploiting a cooperator receives t, and the exploited cooperator gets s. this can be formalized in the form of a payoff matrix,"
"the radio resource management (rrm) functionalities implemented at the satellite broadcasting access layer comprise two main separated but cooperated parts: packet scheduling and connection admission control (cac). the physical channels are multiplexed in the satellite gateway through a radio resource allocation procedure. that is responsible of the radio bearer configuration at the time of the admission for each session, which includes the estimation of the required number of logical, transport and physical channels and their mapping from logical channels to the transport, physical channels. the main function of a scheduling algorithm is to perform a time-multiplexing together with a qos differentiated service flows and adjust the transmit power setting for each physical channel according. the admittance decision of each incoming requested session is handled by the admission control function."
"efficient radio resource management and cac strategies are key components in wireless system supporting multiple types of applications with different qos requirements. cac tries to provide qos to multiple types of applications with different requirements considering both call level and packet level performance measures. a cac scheme aims at maintaining the delivered qos to different calls (or users) at the target level by limiting the number of ongoing calls in the system. call admission control (cac) schemes have been investigated extensively in each type of network. different approaches of cac exist in literature, centralized, distributed, traffic-descriptor-based, measurement-based and so on [cit] ."
"where j r is the moment of inertia of the motor rotor, j m is the inertia moment of the rotating element that turns to rotor of the motor, c e is the voltage coefficient of the motor. similar to previous works formulated by euler angles [cit], constraints on euler angles are naturally required to ensure the nonsingularity of matrix g g g 2 in (9) as follows."
"for instance, q (cc)(cc) is the probability that i t of type cc transforms to i tz1 of type cc. this occurs in the following cases:"
"2 is always negative for all permitted parameters. hence, x ã c (r) is a decreasing function of r. since (x ã c,1 is the attraction basin of cooperation. accordingly, increasing r enlarges the attraction basin of cooperation. in other words, it requires fewer cooperators to take over the whole population with larger r."
"increasing r allows cooperators to spread more effectively and can allow them to invade from initially small clusters [cit] . the quantity r characterizes the propensity of cooperators to form clusters. cooperation gains a foothold when r is sufficiently large. precisely, r is sufficiently large when uvr by eq. (19) . in this case, cooperator clusters expand and take over the whole population."
"nomenclature: throughout this paper, \" · \" denotes euclidean vector norm or frobenius matrix norm, respectively, and a saturation function sat(·) shown in fig. 1 is defined by"
"remark 9: note that the esc module is still required to be used for generating pwm waves which drive and regulate bldcs even though actuator dynamics have been completely addressed in the proposed bcttc scheme. unlike traditional esc modules which are open-loop control systems, the closed-loop esc can be achieved in the bcttc scheme, and thereby enhancing its regulation accuracy and robustness."
"normally voting period was not time-limited. after choosing their quality rating, assessors had to confirm their choice using an 'ok' button. furthermore, this eliminated the possibility of missing ratings in the test. after each video sequence and after to have gave a vote, a neutral gray background is, often, displayed on the video terminal during some second before the next sequence is presented. the test procedure and monitor selection adhered to the latest findings and recommendations for best practice from the video quality experts group (vqeg), a technical body supporting itu standardization activities. the use of this type of test has some disadvantages, first of all, the result of the test depend from uncontrollable attributes like experience, the mood, the attitude and culture, then, they are very expensive and impractical if you want to do frequently because of the number of subjects and tests are necessary to give reliable results. anyway, subjective assessment are invaluable tools for evaluating multimedia quality. their main shortcoming is the requirement for a large number of viewers, which limits the amount of video material that can be rated in a reasonable amount of time. nonetheless, subjective experiments remain the benchmark for any objective quality metric."
"the rest of this paper is organized as follows. in section ii, the quav dynamics and problem formulation are addressed. bcs on intermediate signals are derived in section iii. the bcttc scheme for trajectory tracking of a quav and stability analysis are presented in sections iv and v, respectively. simulation studies are conducted in section vi. the conclusions are drawn in section vii."
"dynamical networks can significantly boost cooperation compared to static networks. on the one hand, cooperation thrives if individuals are able to promptly adjust their social ties, because this allows cooperators to escape from defectors [cit] . similarly, cooperation is more likely to occur if the favored relationships between cooperators (cc links) tend to be less fragile than adverse social ties (cd links) [cit] . the latter result is consistent with our empirical intuitions and is widely observed in the real world. however, most of the works on this issue are investigated only by numerical methods and not by analytical approaches. this is mainly because it is difficult to describe the coevolution of strategy and structure of a network analytically."
"where j is the gaussian white noise with variance 1, f c and f d denote the average fitness of a cooperator and a defector, respectively. for large population size n, the stochastic term vanishes [cit] and we obtain"
"a third category of applications may be \"voice over ip.\" broadband connections of dvb-rcs allows a good control of the flow constant rate. the biggest drawback is that the configuration of a star-rcs will require a double jump to a satellite connection between two users. this problem disappears if one user is connected to a hub through terrestrial links (pstn or isdn)."
"we consider the coevolution of strategy and structure in the pd game. each player's strategy s can either be cooperation (c) or defection (d), denoted by (1, 0) t and (0,1) t, respectively. initially, the whole population of size n are situated on vertices of a regular graph with degree l, where nodes indicate individuals while edges denote the pairwise partnerships between individuals. we consider the case where the total number of agents n is much larger than the average degree l. the payoff of each individual is obtained by playing the pd game with all of its immediate neighbors:"
"if w is sufficiently small, the structure of the system is close to the stationary state when strategies change. in this case, the stationary distribution of linking dynamics determines the average fitness of individuals [cit] . then, we can employ the strategy dynamics from well mixed populations for our structured system. the average payoff of cooperators is given by"
"cooperation is ubiquitous in the real world ranging from genes to multicellular organisms [cit] . most importantly, human society is based upon cooperation. however this cooperative behavior apparently contradicts natural selection [cit] : selfish behavior will be rewarded during competition between individuals, because selfish individuals enjoy the benefits from the cooperation of others, but avoid the associated costs. therefore, the puzzle how natural selection can lead to cooperation has fascinated evolutionary biologists since darwin."
"the pd is characterized by the payoff ranking twrwpws. for repeated games, the additional requirement 2rwtzs ensures that alternating between strategies is less lucrative than repeated mutual cooperation. in the one shot pd, it is best for a rational individual never to cooperate irrespective of the co-player's decision. thus, defection is the nash equilibrium [cit] . however, the two players would be better off if they both cooperated, hence the dilemma. in an evolutionary setting, where payoff determines reproductive fitness, defectors can reproduce faster based on their higher payoff and cooperation diminishes -defection is evolutionary stable [cit] . several mechanisms have been proposed to explain the persistence of cooperative behavior, including kin selection [cit], direct [cit] and indirect reciprocity [cit], group selection [cit] as well as the network reciprocity [cit] . furthermore, the relationship between these mechanisms receives an increasing attention [cit] . both in animal and human societies, individuals interact with a limited number of individuals. the interactions of individuals are often captured based on the network of contacts. therefore, there has been an increasing interest in the influence of population structure on the evolution of cooperation."
"in this section, the bcttc scheme for a complex quav is elaborately established, in a recursive form, by employing an smc-based dsc framework with uacs for saturation and robustness. as shown in fig. 4, the entire bcttc scheme consists of five successive controllers, whereby the preceding control effort is used as the desired signal of the succeeding inner closed-loop. hence, a cascade backstepping-like control hierarchy is synthesized."
"these deviations are due to both the finite population effect and the approximation of linking dynamics by eq. (5). on the one hand, as mentioned above, the transition matrix eq. (6) is only an approximation based on the global frequency of cooperators, while they are also influenced by local frequencies in the simulations. on the other hand, we use the replicator equation to describe the strategy evolution. but the replicator equation is only an approximation of the strategy evolution when the population size is sufficiently large, which implies that small fitness differences can influence the dynamics. this explains why our theoretical predictions are less accurate for weak selection. therefore, we focus on strong selection in the following."
"s, and vnr segment, set r . in a, two vnrs need to be embedded, a line graph vnr r1 and a directed cycle graph vnr r2 . in the upper part of b and c, the construction of the set of segments belonging to vnr r1 and vnr r2 is shown, and their corresponding substrate segments are shown in the lower parts of b and c."
"a total of seven pairs of participants tested the system (labeled as p1-p7 in the succeeding text). because of software failure, data for one set (p2) were incomplete, and thus, the quantitative metrics for those are not available. qualitative responses are still included."
"consequently, the main motivation of this paper is to introduce a new efficient embedding algorithm to solve the vne grand problem, on real-time bases and including end-to-end delay constraint, while minimizing the overall power consumption in the substrate network. the proposed online power aware and fully coordinated vne algorithm, denoted by (opacovne), allocates the virtual nodes and the virtual edges during one stage according to more realistic constraints, such as nodes' processing power, links' bandwidth and end-to-end delay. moreover, the benefits of the new proposed algorithm will be reflected on realizing real time virtual network requests for 5g applications that are sensitive to delay, in addition to solving the vne problem for large networks, using less resources, and generating lower costs and higher revenues."
"in this manuscript, the concept of unidirectional telepresence is extended by teleporting two users in different locations simultaneously to each other's locations. this is referred to as bidirectional symmetric telepresence and was designed to push the technical boundaries of what was possible with robotic surrogate representations. while the concept of symmetric telepresence has been explored in virtual environments (via virtual embodiments), the form of telepresence described in this manuscript is mechanical in nature (and hence three-dimensional) because identical humanoid robots are used at both locations. this work is comparable with using bidirectional avatars but is a pilot demonstration of bidirectional working with robots. these humanoid robots are referred to as surrogates ( figure 1 )."
"to evaluate the system, we created a set of tasks that required the collaboration of two participants located at transcontinental sites (usa and uk). specifically, the tasks involved solving a series of 20 tic-tac-toe puzzles because this was likely to promote discussion via gestures. for each puzzle, the participants were presented with a partially completed tic-tac-toe game and instructed to come to an agreement on the next best move for either \"x\" or \"o.\" an example of two such puzzles is shown in figure 4 . as mentioned, our experimental design relied on the notion that the tic-tac-toe puzzle designs would encourage gesturing at the board. puzzles were chosen such that all squares on the board would be potential solutions, and thus, participants would likely gesture towards all nine squares over the course of the 20 puzzles. of course, certain squares can be expected to be favored by the participants because they are known to be good moves (e.g., the center square) in general. we note that participants may not always identify or converge on the optimal or best possible move on the board. the study did not test subjects on this aspect as the emphasis for this trial was on encouraging discussion with gestures. the full set of puzzles will be available at http://vr.cs.ucl.ac.uk/portfolio-item/robot-telepresence/ for those interested in exploring similar setups. the experimental setup at the site in usa showing the display surface, the control device, the robotic surrogate (robothespian), and the kinect device used to collect data for analysis of the interaction. this setup was mirrored at the site in uk with the only exception being the display surface, which was replaced by a traditional flip chart."
"in real worlds of 5g networks, services, applications, and users interact with the infrastructure network instantly and on real-time. therefore, in the context of networks' virtualization, the demands must be analyzed and allocated on the substrate network, online, utilizing the shared resources efficiently, adhering to the required service qualities as demanded, and at the same time keep updating the statue of the substrate network's resources regularly, in order to spontaneously evaluate the possibilities of allocating other virtualization demands when they arrive."
"a brief example to explain the proposed opacovne is shown in fig. 4 . at the top of the figure, four vnrs need to be embedded on the substrate network. each vnr has clear specific location demands, as well as cpu, bw, and delay. for example vnr 1 requests that, the location of virtual node 1.1 must be at the location of substrate node a, virtual node 1.2 to be located at b, and 1.3 at c. therefore, opacovne algorithm will first confirm the location constraints of the nodes, and if each pair of the located substrate nodes are connected by one direct fig. 4 . numerical example showing the basics of opacovne. at the top left, four vnrs need to be embedded on the substrate network as given at the top right. each vnr has clear specific location demands, as well as cpu, bw, and delay. in a,c, and d, vnr-1, vnr-3, and vnr-4 were allocated and the substrate network was updated, while in b, vnr-2 was rejected due to the delay constraint. substrate edge only, as shown in fig. 4a, where substrate nodes a and b are directly connected by the edge (a, b), as well as nodes b and c are also directly connected by edge (b, c) edges (a, b) and (b, c), which is also true, since each of them has 100 current bw units and 1 current delay unit. accordingly, vnr 1 will be embedded on substrate nodes and edges of path p abc, while substrate nodes d and e will be turned-off, since they are not utilized to minimize the total power consumption in the network. following the same procedure, vnr 3 and vnr 4 will be embedded on substrate paths p cde and p abcde respectively as shown in fig. 4c and d. however, for vnr 2 even though substrate path p abcd fulfills its loc, cpu, and bw demands, yet it was rejected, since delay demand of virtual edge (2.1, 2.4) was less than the current edge delay in the candidate substrate edge (a, d)."
"on the other hand, for experiment-3, average processing time was in the range of 37.96 ms as shown in fig. 7j, giving that the number of arriving vnrs was fixed to 4, but varying vnrs' lifetimes between 200 and 1000 time units. this result suggests that opacovne's processing time performance was slightly affected when varying vnrs' lifetimes."
"impact of delay on opacovne: the impact of end-to-end delay on opacovne, was negative in general over all simulations as shown in fig. 5 . however, since d-vine did not use delay as a constraint, the results of opacovne were shown just to reflect how much the performance of opacovne without delay is better than when it was included."
"one of the key features of such a paradigm is the closed-loop nature of the approach as seen in figure 3 . in specific, the actions of \"inhabiter 1\" drive those of \"surrogate 1.\" this in turn causes \"inhabiter 2\" to respond potentially with both gestures and verbally. the actions of \"inhabiter 2\" now drive those of \"surrogate 2.\" the process continues, and this results in each inhabiter collaborating indirectly with the other via their respective surrogates."
"5.9.6. average cpu utilization, cpu util it represents the average sn nodes' utilization trend after all simulation iterations. its defined as a ratio between consumed cpu cpu t i"
collaboration at a distance has long been a topic of interest to the virtual reality community. the system we have described shares many of the same software components as a collaborative virtual environment; the distinction is that the realization of the shared environment is carried out through physical manifestations: the robotic surrogates.
"the main target of the objective function for opacovne is to successfully accommodate all the demands of the arriving vnrs on online scenario, while minimizing overall power consumption in the whole substrate network, by putting into sleeping mode all idle resources. demands in the online case arrive at certain t r a and expire at t r e, therefore, the objective function must consider embedding vnrs during the time intervals specified by each related vnr r ."
"the overall performance of opacovne was compared to d-vine, which targeted minimizing the embedding cost while improving acceptance ratio, by mapping the virtual nodes based on the total flow passing through the edges of a candidate sn, and applies multi-commodity flow algorithm to map the virtual edges onto paths connecting the hosting sn nodes. table 2 provides a high-level comparison between opacovne and d-vine algorithms, listing their used strategies, and how they embed the virtual nodes and edges."
"opacovne's average acceptance ratios for experiments 2-4 had a decreasing trend with and without delay as shown in fig. 7e -g. for experiment-2, number of arriving vnrs per 100 time units varied between 2 and 10, the average acceptance ratio varied between 92.68% in the case of low number of arriving vnrs, and decreased to 78.39% when number of arriving vnrs was 10. same trend was reported for experiment-3, when vnrs' lifetimes varied between 200 and 1000, which resulted on average acceptance ratio between 94.49% for short lifetimes, down to 79.90% for lifetimes as long as 1000 time units. moreover, the average acceptance ratio for experiment-4, when number of virtual nodes per vnr varied between 2 and 10 resulted on 93.87% for small sized vnrs, decaying to 72.97% for larger vnrs."
"the proposed segmentation format guarantees full coordinated embedding of the virtual nodes and edges, since segments formulation provides direct way to check one-to-one, if each element in the substrate pair's segment has enough resources to host the demands of its counterpart from the virtual pair's segment, precisely, by comparing the first parameter in the virtual segment to the first parameter in the substrate segment, the second to the second, and so on for all the remaining parameters. if the results of 'all' checks are true, that is, each virtual node demand found a substrate node to host it, 'and', each virtual edge demand found a substrate edge to host it, the embedding of the virtual pair's nodes and edges will be realized, together in full coordination onto the corresponding substrate pair."
"the teleoperation paradigm shown in figure 2 can be extended to work bidirectionally. when instantiated simultaneously in two locations, it is possible for two masters and two slaves to function in parallel. this results in an architecture that supports symmetric telepresence as seen in figure 3 . the components of the \"teleoperation paradigm\" can be inferred in figure 2 ."
"the results from the trials show that participants were successful in using gestures to communicate. if they successfully agreed upon the best possible position for the next \"o\" or \"x\" without verbalizing their location, it indicated that the robotic surrogate's movements conveyed their intentions correctly. in analysis of videos of the participants performing the task, we saw a variety of collaborative strategies come to light: one participant pointing and the other just agreeing; one participant pointing and the other participant pointing at the same square to confirm (figure 8 top) or a more complex exchange where pointing was used to express differences of opinion. we also saw several failed communication attempts: including pointing with the untracked hand or gesturing at the board in a more complex way (e.g. painting lines on the board) that was not captured by the system. this was attributed to the fact that our surrogate control paradigm did not involve full body motion. instead, we only tracked gestures that were considered important for the task, in this case, this involved pointing towards the board (i.e., reaching out). these observations may suggest a need to fully interpret all user gestures because they could subconsciously use hand and arm gestures without knowing or considering how the system will be able to convey these. we observed the participants making several other communicative actions such as smiling at the robotic surrogate, waving good-bye, nodding, and shrugging. none of these were captured and relayed via the robotic surrogates to the other participants. the participants did not gaze frequently at the surrogate's face but tended to focus on the board and the surrogate's gestures near the board. we hypothesize that there may be more glancing towards the face if the robotic surrogate had a human-like appearance via the rear-projection display. a couple of participants noted a lack of information from the face in their interviews as discussed in the succeeding text. we do see participants looking towards the robotic surrogate as they gesture and also occasionally look at the surrogate's hands as if they are about to move although they do not. please refer to figure 8 middle and bottom."
"this is the main step in making sure that each pair of virtual nodes and their edge are embedded together, resulting on solving the two subproblems of the vne in full coordination altogether."
"to complete the above example, the bottom of fig. 2b accordingly, if all the above conditions were proven true, this guarantees that each virtual node and edge in vnr r1 will be hosted by their counterparts in substrate path p ae . therefore, segments formulation and the comparisons, made the embeddings of virtual nodes fully coordinated with the embeddings of the virtual edges, together on the same substrate path. the same comparison procedure can be followed to check if the demands of vnr r2 can be hosted by the directed cycle substrate graph p mno ."
"1. inclusion of end-to-end delay had significantly impacted vne process, as reflected by lower acceptance ratios across all simulations in the range of 16%, when compared to the ratios without delay. suggesting the importance of including end-to-end delay as a major vne constraint when applied on the delay sensitive 5g networks. 2. overall online performance of opacovne, measured by acceptance ratio was in average in the range of 63.64%, [cit] . 3. in terms of power consumption, opacovne managed to save 23.54% [cit] . 4. the results confirmed the effectiveness of the proposed segmentation approach, which allowed for a very precise and full coordination between the embeddings of both, virtual nodes and edges, together at the same time, without including any hidden resources. 5. the use of direct edges when embedding the virtual edges, guaranteed minimizing the use of substrate network resources to the minimum before activating others. in this way more substrate resources were left free to accept more virtual network requests, while insuring less power consumptions in the substrate network, and including end-to-end delay as a main constraint. 6. the drawback of the proposed algorithm is that, it had to make a trade-off between minimizing the total power consumption and accepting more virtualization demands, by limiting the use of the substrate network to minimum. nevertheless, the suggested algorithm managed to provide very solid performance compared to others."
"ead-vne along the simulation duration, confirming the better power aware performance of opacovne. fig. 6b presents average revenue in the substrate network, showing that opacovne revenues are almost near to those of ead-vne, and in average were less by 4.4%. noting that opacovne and ead-vne embed virtual nodes based on location and residual nodes' resources constraints, yet, in contrary to ead-vne, which uses shortest path to embed virtual edges, opacovne's way in embedding each virtual edge on a single substrate edge, may cause some vnrs to be rejected, which would reduce the overall revenues of opacovne in general. nevertheless, the fact of using direct edges by opacovne is precisely why its power consumption is better than ead-vne, since opacovne strategy uses least substrate network resources as much as possible, through using direct edges with no hidden hopes at all. this is confirmed referring to fig. 6c, which shows that opacovne strategy managed to turn-off more substrate nodes than ead-vne by 22.2%. table 6 lists specific settings for experiments 2-5 that were designed to evaluate opacovne in more details. the four experiments were specifically designed to evaluate the online power consumption and acceptance ratio behaviors of opacovne, when end-to-end delay was included as a major embedding constraint. in the second experiment, number of arriving vnrs were varied between 2 and 10 each 100 time units, while the third experiment varied vnrs' lifetime between 200 and 1000 time units, and the fourth experiment varied number of nodes per each embedded vnr between 2 and 10. finally, fifth experiment evaluated opacovne power consumption and acceptance ratios for five different substrate network topologies."
"in all experiments, values of sn delays were randomly assigned in the range 1-50 ms (5g [cit] ) and between 20 and 100 ms for vnrs. accordingly, each time opacovne attempts to embed any vnr, it carefully tries to guarantee embedding on substrate edges connecting directly the selected substrate nodes, having enough bandwidth resources and end-to-end delay within the demanded ranges."
"from the observations of all trials, it appears that participants became more comfortable with the collaboration over time. one participant directly commented (when asked \"did you find it easy to communicate with your collaborator?\") p2@siteinusa: after we got started. i was not sure how to talk to the robotic surrogate at the beginning."
"opacovne starts by listing all pairs of the vnr r, then using the demanded locations of the virtual nodes and edges, the algorithm will allocate the substrate pairs complying with the demanded locations' constraints, without using any hidden hops or edges, consequently, it will construct the candidate substrate topology path p sub similar to the topology structure of vnr r . it is assumed that the substrate network topology is physically fixed, therefore, the main elements formulating substrate network, such as number and connectivity of the sn nodes and edges, are also fixed and does not change, but only their capacities varies due to the consumption after each time interval t. -update all parameters of sn nodes and edges."
"this subsection compares the power aware performance of opacovne against ead-vne algorithm, which applies power aware with dynamic demands as well [cit] . table 4 compares the two algorithms, giving that for virtual nodes' embedding, both algorithms first use location constraint to select the substrate nodes, then embed the virtual nodes if enough residual resources are available. for the virtual edges, ead-vne uses shortest path, which may include additional edges, while opacovne uses direct edges connecting the pair of substrate nodes, without any hidden nodes or additional edges. table 5 provides general settings to compare opacovne against ead-vne."
"average edges utilization in opacovne resulted on 31.75%, and was lower than d-vine of 56.5% as shown in fig. 5d . this is mainly because opacovne is allocating precisely the same number of sn edges as the demanded edges without any hidden edges or nodes. however, the way d-vine embedding virtual edges using the multi-commodity flow algorithm, could have resulted on using longer paths including more sn edge than requested, which raises its overall edges' utilization."
"several qualitative and quantitative metrics were collected during the interactions at both sites. because this was an exploratory pilot study, a video analysis of all the participant interactions was performed to gain insight into the effectiveness and usefulness of the physical bidirectional robotic telepresence system. in this section, we discuss some of observations during the interactions including computing the latency times, the number of times participants pointed to a square during the interaction, general notes from observation of audio and video data streams, and interviews with the participants."
"furthermore, fig. 7c presents the results when fixing number of embedded vnrs per 100 time units to 4, while varying number of virtual nodes per vnr between 4 and 10, which resulted on slight increases in the average power consumption when delay was not included, suggesting that the change in vnrs' size has minor impact in general. however, this conclusion changed dramatically when delay was included, where power consumption was higher for low sized vnrs, but decreased for larger vnrs. this result on power consumption with delay would most likely be due to high rejections of large sized vnrs. finally, fig. 7d reported the average power consumption of opacovne for five different substrate network topologies, which were varied by increasing the number of edges per each of them. as shown, increasing number of connected edges per topology did not have any significant impact on opacovne's average power consumption in general, but when delay was include opacovne's power consumption showed slight decreasing trend for larger connected topologies, yet still it did not show any significant impact too. this is expected, since opacovne uses location constraint, suggesting that performance of opacovne will very slightly be affected, regardless of the size of connected edges in the infrastructure."
"all simulation results showed that impact of delay on vne process was clearly the most significant parameter among all varied variables while testing opacovne. specifically, referring to fig. 7e -h, opacovne's average acceptance ratios with delay, were less than when it was not included by 13.05%, 14.38%, 17.69%, and 17.39% for experiment-2 to experiment-5 respectively. similar trends can be seen by referring to opacovne's results for average power consumption and processing time shown in fig. 7 ."
"classic approach to handle the vne problem is usually conducted by dividing it into two subproblems with no coordination between them. the first one deals with allocating virtual nodes onto physical nodes, which is known as virtual node mapping (vnm) subproblem, and the other one, is virtual edge mapping (vem) subproblem, which embeds virtual edges onto physical paths, connecting the corresponding nodes hosting the virtual nodes in the physical network [cit] . along such process, vne usually trades off between minimizing embedding costs, utilizing less sn resources, and maximizing revenues through accepting as much as possible vnrs, while maintaining acceptable quality of services (qos). however, the fact that the two subproblems of the vne are solved without coordination, based on finding an acceptable embeddings for virtual nodes separate from embedding the virtual edges, would most probably result on rejecting some virtual demands, or including additional hidden sn resources [cit], accordingly, increasing embedding costs and consuming more power."
"is a ratio to represent how opacovne algorithm is performing, and is calculated by averaging and dividing the number of successfully embedded vnrs at each time interval by the total number of vnrs r [cit] ."
"once a successful embedding occurs, the algorithm updates all changed sn resources and moves to next vnr r+1 . however, in case that the selected sn set of segments does not have enough resources to accommodate vnr r demands, the algorithm rejects vnr r, and jumps to the next vnr from step-3. this process keeps on going until no more vnrs to be handled."
"the performance of opacovne algorithm will be evaluated based on acceptance ratio, total power consumption, saved power, total revenues, total cost, cpu and bw utilizations, and processing time."
"in all previous experiments, opacovne code was developed using eclipse ide for java developers, version: mars.2 release (4.5.2). the used machine was lenovo laptop, system model 20cls2rg00, processor intel(r) core(tm) i7-5600u cpu, 2.60 ghz, 2 cores, 4 logical processors, ram 8 gb, and the operating system was microsoft windows 10 enterprise."
"surrogates are defined as context-specific stand-ins for real humans. traditionally, manifestations of surrogates are referred to as avatars or agents depending on the entity controlling them, avatars are controlled by humans (traditionally referred to as inhabiters), while agents are controlled by computer programs. the term surrogate avoids having to explicitly differentiate between avatars and agents thereby allowing hybrid versions of control, that is, a surrogate may be an avatar at one instant and an agent in the next. physical manifestations of avatars or agents provide the ability to manipulate things in the remote environment. in this exploratory work (pilot), the focus is on providing users at either end, the ability to gesture using their surrogates. a shared collaborative task that involves solving tic-tac-toe puzzles is chosen to encourage inhabiters to gesture through their surrogates. the remainder of this manuscript is organized as follows. in section 2, important background literature and previous work in the area of telepresence is covered. section 3 contains a description of the system architecture used to control the robotic humanoid surrogates. the experimental setup and design are covered in section 4. section 5 is a discussion of the observations during this task for a small number of participants. conclusions and future work form the last section of this manuscript."
"because the heartbeats were implemented only one way, it was possible to differentiate the observed latencies at the two remote sites. the graphs (figure 6 ) show that the latency was quite variable, indicating, potentially, that route changes were occurring or there was significant load. it should be noted that an instance of teamviewer (remote desktop) was running on the machine in uk during the experiment, although this machine was not central processing unit bound, and the bandwidth used was well within the local network capabilities. thus, it could be inferred that it took approximately 1 s between the user gesturing and their robotic surrogate at the other end moving. the surrogate then took a small amount of time to reach its final destination as a result of its inherent actuation mechanism involving fluidic muscles, typically characterized by smooth and non-jerky responses."
"to make sure that a specific physical node is active, variable x ur i is used in the objective function formulation, which takes a binary value of 1 if substrate node i is active and assigned to host the virtual node u"
"exact simulation settings of ead-vne were applied on opacovne, assigning 4 vnrs to be embedded per 100 time units, each having average lifetime of 500 time units, and number of nodes per vnr were randomly distributed between 2 and 10. as shown in fig. 6a, the average power consumption per sn node using opacovne is 23.54% lower than . opacovne performance against ead-vne. in a, the average power consumption per substrate node using opacovne is 23.54% lower than ead-vne, while in b it shows that opacovne revenues are almost near to those of ead-vne, which in average were less by 4.4%, and in c it shows that opacovne managed to turn-off more substrate nodes than ead-vne by 22.2%."
"one of the central components of discussion in this manuscript is the ability to remotely inhabit a surrogate. for the purposes of this study, a commercial off-the-shelf robotic humanoid called the robothespian™ was used as the surrogate. the robothespian features a hybrid actuation mechanism consisting of fluidic muscles and dc motors along with passive compliance elements and offers 24 degrees of freedom. to support telepresence using this surrogate, an inhabiter's intent is realized and transmitted accurately in real time, while the closed-loop response of the robothespian's hybrid actuation mechanism is adapted to faithfully represent this action. the control aspects of this paradigm are not covered in detail here because the focus of this manuscript is on presenting the concept of symmetric telepresence."
"one pair of subjects (p4) both noted in interviews that they mostly used verbal communication for the task as they felt this was sufficient. however, data revealed that they did gesture using the robotic surrogate."
"the average processing time results for opacovne while performing experiments 2-5 are shown in fig. 7i-l . in experiment-2, the average processing time was about 25.60 ms when the number of arriving vnrs were low, then increased as the number of arriving vnrs was increased up to 7 per 100 time units. nevertheless, for larger number of arriving vnr between 8 and 10, average processing time decreased again, most probably due to an already loaded and limited substrate resources, which caused high rejected vnrs, therefore decreasing the processing time accordingly."
"an illustrative diagram showing a substrate network of nine nodes and twelve edges receiving two virtual network requests to be embedded, vnr-1 of four nodes and four edges, and vnr-2 of three nodes and two edges."
"the average acceptance ratio of opa- fig. 5 . overall performance of opacovne with and without end-to-end delay compared to d-vine. in a, the average acceptance ratio of opacovne was 63.64% compared to 67.10% for d-vine. in b, the average cost using opacovne was around 92.28 compared to d-vine of 169.20. in c, nodes' utilization using opacovne was in average around 37.36% compared to 33.80% for d-vine, and in d, average edges' utilization was around 31.75% lower than d-vine of 56.5%. covne was 63.64% compared to 67.10% for d-vine as shown in fig. 5a, which indicates the close similarity in the overall performance of both algorithms. however, the fact that opacovne strategy in selecting only direct edges to minimize the used resources when embedding virtual edges, in order to minimize the total power consumption, has negatively impacted the algorithm's acceptance ratio, yet opacovne still managed to show solid and comparable results to d-vine in general, while leaving more free resources to host more demands. more precisely, this is due to applying the segment technique which insures that the virtual nodes and virtual edges were mapped together on the sn path that, complies with the location constraints and has enough resources and acceptable end-to-end delay, without any hidden nodes or edges. similar to opacovne, d-vine maps virtual nodes on physical nodes having enough residual capacities, and maps virtual edges through applying multi-commodity flow algorithm to find the appropriate sn edge between the already mapped virtual nodes. nevertheless, the way d-vine selects the sn nodes to host virtual nodes, may result on selecting sn nodes that are further away from each other, which when applying the multi-commodity flow algorithm to map the virtual edges, could force using longer paths, therefore, consuming more resources, but would have better acceptance ratios. that was clear for the acceptance ratios of d-vine when the loads were between 4 and 7. however, the results of opacovne started to converge with those of d-vine, most probably due to the multi-commodity flow algorithm's way of selecting more edges, and allowing for splitting the flows between more than one substrate path, thus, raising the possibilities of congested edges, which in turn would cause degradation in the acceptance ratio. moreover, referring to fig. 5a, and focusing on the load of 8 vnrs per 100 time units, opavovne acceptance ratio started to get better than that of d-vine, most likely since it would always stick to using less edges during the embedding process, therefore, keeping more free resources to be used to fulfill the demands of the new virtualization requests."
"the opacovne methodology is shown in the flowchart shown in fig. 3 and the pseudo-code is shown in algorithm 1. at each time t, if a vnr r arrives, the code structures its set of segments, and based on the demanded locations for the virtual nodes, the code will build a similar sn topology to the vnr r topology, then it formulates the sn set of segments, and compares both sets to check if the candidate sn segments, have enough resources to host all demands of vnr r during the time interval. at each iteration, the algorithm keeps checking whether any vnr has expired, in oder to remove its demands from the hosting resources, and it updates the whole sn accordingly. in parallel with that, at each iteration cycle, opacovne evaluates the power consumption of each sn node if it is idle or loaded, and turns it off, if it has zero utilization, to save the overall power consumption in the sn."
"there are several avenues for future research. one could explore other important scenarios and natural interactions (e.g., involving trust) by designing sophisticated collaborative task. we plan to conduct a more systematic user study with more subjects and quantitative report of the experimental results (e.g., the contribution of physical manifestations with and without audio). enhancing the autonomous capability of the robot to augment human control is another important future research area."
"the average amount of allocated virtual processing powers multiplied by the processing unite price, and throughputs multiplied by the flow unite price, at each time interval t [cit] ."
"average cost: average cost per accepted vnr is shown in fig. 5b, confirms the successful strategy of opacovne in minimizing the use of substrate network resources as least as possible while embedding vnrs, by selecting direct edges between the hosting substrate nodes without any hidden hopes. therefore, in average opacovne cost was around 92.28 compared to d-vine cost of 169.20, which most likely tends to use more edges when embedding vnrs than opacovne. the processing power unite price, and throughput unite price were set equal to 1. sn nodes utilization: fig. 5c shows nodes' average utilization. sn nodes in opacovne are in average moderately utilized resulting on 37.36%, which is very close when compared to 33.80% for d-vine. this is a reflection of the similarity between both opacovne and dvine in utilizing sn nodes having enough residual resources to embed virtual nodes. also it is important to point out that both algorithms first select the sn nodes according to the location constraint, then they check for the residual resources afterwards. in the case of including endto-end delay, opacovne nodes' utilization is less than without delay, mainly since opacovne with end-to-end delay accepted less number of demands, thus has less utilized sn nodes."
"to support teleoperation and telepresence, a masterslave architecture is employed. the master uses virtual characters that can be controlled using generic input devices, one of which is a magnetic tracking device called the razer hydra. a calibration routine on the master allows users to map their motion to corresponding actions of the master virtual characters. this is a gestural interface and not a literal interface, that is, the motions of the inhabiter do not have to explicitly match the desired motions of the virtual character. the person controlling the virtual characters (or avatars) is referred to as an inhabiter. the inhabiter's intent is transmitted via a lightweight networking protocol to a slave (client) program. the slave has the same continuous avatar state representations as that of the master. a subroutine on the slave maps the motions of the active virtual characters onto any secondary hardware manifestations such as a humanoid robotic surrogate. in this case, the active avatar's motions are mapped to those of the robothespian™. the mapping is achieved via a custom routine that identifies the number of degrees available on the specific robotic surrogate, extracts the relevant data from the active avatar, and applies it in joint-space using a traditional pid controller (positionally or via velocity control). an illustration of the general architecture is provided in figure 2 . we refer to this as the \"teleoperation paradigm\" in the remainder of this manuscript."
"in the paper, we have shown that two remote participants can collaborate on a shared task that involved voice and gestural communication. in this pilot trial, we found that participants would gesture to communicate spatial positions and did not have to resort to voice generally to complete the task. the trials highlighted several potential directions for research and development, such as having the robotic surrogate appear to monitor the participant, capturing more of the participants' behavior, improving audio reproduction, and improving overall system latency."
"another commented (when asked \"how confident were you that the meaning of your gestures was conveyed to your collaborator?\") this again suggests that participants did not have a good understanding of the capabilities of such robotic surrogates initially because they are still unusual."
several participants mentioned that the robotic surrogate's movement was clunky and slow. this was expected because of the limitations of the hardware control loops and the distance between the two sites. improving the responses of such robotic surrogates also forms a component of our planned future work in the mainstream area of robotics and control systems.
"since virtual network embedding problems deals with making decisions, about an efficient utilizations for using the limited resources of the physical substrate network, the vne problems were traditionally modeled as an optimization problem of objective function, and constrained by controlling conditions, matching the resources availability against the requirements, while utilizing the scarce physical resources. this is usually referred to as integer linear programming (ilp) problem, which when solved will have positive, integer, and linear decision variables in their final optimal solution [cit] . however, optimal solution for vne as an ilp problem, implies introducing binary constraints to connect one edge only for each node, then, mapping all virtual nodes and edges on their physical counterparts having enough resources to accommodate their demands. accordingly, virtual edges associated with bandwidth constraints are usually treated as commodities between pairs of nodes, and therefore, embedding a virtual edge optimally is similar to finding an optimal flow for the commodity in any network model, which is an np-hard problem and will take huge amount of time to solve [cit] ."
"in light of that, the proposed segmentation approach allowed opacovne to coordinate virtual nodes and edges' embeddings together, on a corresponding substrate nodes and edges. this will guarantee high acceptance ratios, and efficiently utilizes the limited capacities in the substrate network without any additional recourses, therefore, resulting on minimizing the total power consumption in the substrate network. subsequently, opacovne's use of the segmentation approached, differentiated it from the vne's two steps approach adopted by literature, which used to apply greedy algorithm to embed all virtual nodes first, then run the shortest path algorithm to embed the virtual edges as a second step, with partial or no coordination with the nodes' embedding step."
"networks virtualization is an integral component of the current and future 5g core networks, offering network operators opportunities to consolidate their equipments into standardized high volume components. this is reflected by efficiently utilizing the physical network resources, through sharing them among several virtual networks (vn), as well as providing more flexibility to manage, expand, and shrink the physical network according to the vns' characteristics (3gpp tr 28.801, 2017; [cit] ) ."
"using these telepresence robots, remote coworkers can wander the hallways and engage in impromptu interactions, increasing opportunities for connection in the workplace [cit] . while such mobile robots have been introduced to support telepresence, the anthropomorphic nature of humanoid robots may allow for better conveyance of a person's remote physical presence. in addition, these humanoid robots could allow for manipulation of objects in the remote environment, thereby increasing the feeling of \"presence\" for a user. among such humanoid robots is the geminoid hi-1 [cit] developed to closely resemble a specific human. while not capable of manipulating objects in the environment, it was evaluated as being highly human-like but uncanny [cit] . related research includes the concept of animatronic shader lamps avatars [cit] . here, researchers use the technique where an image of an object is projected onto a screen whose shape physically matches the object. cameras and projectors are used to capture and map the dynamic motion and appearance of a real person onto a humanoid animatronic model. these avatars can potentially be used to represent specific visitors at a destination but are limited in their flexibility to gesture in the remote environment. another related concept is that of tele-existence [cit] where a user is given the sense that they are inside the robot itself. not all telepresence systems, however, support tele-existence, and this particular concept, although relevant, is in-fact not explored in this manuscript."
"as shown in fig. 7a-d, average power consumption and impacts of adding delay constraint on opacovne's performance were reported using different settings according to experiments 2-5. from fig. 7a, varying number of embedded vnrs between 2 and 10 per 100 time units, reflects the loading impacts on opacovne's power consumption, which showed increasing trend in general from 76% when the number of embedded vnrs were as low as 2, to 96% for 10 loaded vnrs each 100 time units. same increasing trends are also shown in fig. 7b, but in this case number of embedded vnrs were fixed on 4 per 100 time units, while varying their lifetime from 200 to 1000 time units. in both experiments, impact of delay on the overall power consumption resulted on less values than the cases when delay was not included, yet it showed increasing trends in both settings."
"experimental setup: figure 5 shows the setup of the experiment at the us site. this setup was mirrored at the site in uk. the robotic humanoid surrogate was capable of all nine different gestures required to point at each square of the tic-tac-toe board. when the participant at location a did not perform a gesture, their robotic surrogate in location b would default to returning to a neutral stance while observing the participant in its location. participants in either location used a magnetic tracking device called the razer hydra to inhabit their robotic surrogate. a kinect device was positioned appropriately in the experimental area to collect data for analysis including video and audio streams. participants were asked to answer a post-interaction questionnaire (table i) to correlate the qualitative and quantitative metrics collected during the study. priming: before each experiment, the participants watched an instructional video detailing their task and the usage of the system. participants were not made aware of the symmetric control system or told that their hydra gestures were mapped onto another robot. instead, they were simply informed that they would have to use the hydra device in order for the robot in their location to understand their intent (pointing gestures). participants were unable to view the other side because no video was used to support telepresence. the robotic surrogates were a direct means of telepresence, and as a result, they were forced to observe the robotic surrogate (of the other collaborator) in their own space to understand visual pointing cues. the participants were allowed to verbally communicate with the surrogate. the speakers on both sides were positioned in such a way to make the sound appear as if it was coming from the robotic surrogate itself."
"as a future work, the authors are planning to conduct further research, about reducing the power consumption of the opacovne algorithm and increase the acceptance ratio when non-direct edges are considered, considering end-to-end delay for specific 5g applications."
"during the interaction, all gestures towards the tic-tac-toe board performed by both the participant and the robotic surrogate were recorded in each location. once all interactions were complete, the log files showing the interactor's intent (gesture) and the corresponding robot's gesture (pose obtained) were verified. the intent of the inhabiter was found to be transmitted to their robotic surrogate via the master-slave architecture on all occasions. the data revealing the robotic surrogate's pose are viewed as a heatmap in figure 7 . the heatmap reveals that participants pointed to every square on the board at least once. in addition to this, tables ii and iii show the mean and standard deviation of the total number of times each participant pointed to each square on the board. this demonstrates that subjects in both locations were using gestures to communicate during the interaction. as an aside, the increased pointing to the middle center and bottom center squares was a result of the particular state of the tic-tac-toe board. the uneven distribution simply indicates that the set of puzzles was not \"rotation symmetric,\" and the bottom center square was a reasonable choice more often. if the puzzles were indeed designed to be \"rotation symmetric,\" the uneven distribution would be indicative of a mechanical or control problem with the robotic surrogate (e.g., the actuators would not have sufficient power to point to the upper squares). the robotic surrogate systems at both ends were checked and tuned before the experiment to mitigate this risk."
"to clarify what is meant by segments and how they guarantee fully coordinated embedding, segment definition, and segment formulations for a specific substrate path and for a vnr are presented as follows:"
"in general, network virtualization is realized by allocating sufficient physical resources to satisfy the requirements of a virtual network request (vnr), on top of a substrate network (sn) that has limited residual capacities. this process is known as virtual network embedding problem (vne) [cit], which could be solved for offline scenario, where all vnrs are known in advance, or for online scenario, where vnrs arrive the sn on real time bases and each have specific lifetime. nevertheless, the vne problem is a well known np-hard problem and can not be solved in polynomial time [cit] ."
"telepresence is a concept that has been widely studied by researchers for several years. [cit], pioneered the concept of mechanical telepresence where each motion of a person's hand, arm, and fingers was reproduced in a different room, city, country, or planet using mobile mechanical hands [cit] . the idea was to provide an ability to work in distant environments while allowing a user to see and feel what was happening, in other words, providing a sensation of \"being there.\" the process of enabling telepresence is sometimes referred to as \"teleportation.\" with this came several applications of the technology including the idea of remote surgery and applications related to space exploration. since then, researchers in robotics and virtual reality have identified several elements that can enhance the telepresence experience. their focus has traditionally been on unidirectional asymmetric telepresence. the research questions answered historically can generally be categorized into one of the following:"
"telepresence robots, including mobile telepresence robots and humanoid robots, have been studied by several researchers over the years. they provide a connection between a user and a distant participant or a remote environment to perform social interactions or specific tasks. mobile telepresence robots, such as mebot v4 [cit], prop [cit], anybots'qb, and the vgo [cit], allow a remote user to control the robot's movement around a space while the user converses with other users in that space."
"at sn node e. at the top of fig. 2b, the formulation of eq. (2) was applied to construct the set of segments, set r1, representing vnr r1, listing all demanded locs, cpus, bws, and the overall end-to-end delay. accordingly, set r1 is formulated as follows:"
"1. as a main contribution by this paper, impacts of end-to-end delay on the performance of the suggested online algorithm, opacovne, were deeply analyzed, representing direct application for virtualization in future 5g networks. 2. to minimize the total power consumption in the whole sn, segmentation approach was proposed to guarantee coordinating the embeddings of the virtual nodes and edges, while utilizing the least substrate resources to the minimum. 3. [cit] . 4. additional evaluations for the proposed online algorithm, opacovne, were also introduced, showing the impacts of end-enddelay, by varying number and size of the embedded vnrs, as well as varying their lifetimes."
"based on that, the proposed algorithm by this paper, opacovne is designed for online scenario, and its main objective is to successfully embed the vnrs, on real time, while minimizing the overall power consumption in the whole substrate network considering end-to-end delay as a main embedding constraint. the algorithm handles the virtual network requests one-by-one, and keeps monitoring and updating the substrate network frequently, to free up more resources for future usage by new virtual requests."
"it represents the average utilization of sn edges after all simulation iterations. and is defined as a ratio between consumed bw t ij, and the maximum bw, averaged over all sn edges [cit] ."
"(1) on any graph, let set r be a set of segments listing all demands of the virtual nodes and edges in a vnr r graph, and let set s be a set of segments listing all resources of the candidate physical nodes and edges in a substrate graph to host the virtual nodes and edges in the vnr r . the formulation of both segments is clarified in the following subsections, starting by the vnr segment:"
"this paper proposes a new solution for the two subproblems of the virtual network embedding problem, by converting the structure of any virtual network request into a collection of pairs of virtual nodes and their edge, where the demands of each pair will be listed together in a set format called segment, and converting the resources of an exact similar number of substrate network pairs into segment format as well. the motivation behind using the segmentation approach, is to facilitate allocating all virtual pairs belonging to a specific vnr, on the corresponding substrate pairs that have enough resources to host the demands of the virtual pairs' nodes and edges, together in full coordination, without using any additional hidden substrate resources, which guarantees consuming the least total power in the whole network along the virtualization process."
"as a part of the communication architecture, \"heartbeats\" were sent every 10 s between the server and the client ( figure 6 ). the client recorded the local time at which a heartbeat was received from the server and also recorded the time at which the heartbeat originated from the server. clock time difference and latency between the two locations were estimated from these data. to do this, a latency test was run between the two networks (usa and uk) situated reasonably close to backbones. this value was noted as est latency . the heartbeats were analyzed to identify a value with minimum clock difference. this was the most representative first estimate of the clockshift meas because anything greater can be attributed to latency or program delays. clockshift meas est latency is now the best estimate for the actual clockshift act between the two locations. this clockshift act was then subtracted from the vector of heartbeats received. the resultant is a vector of latencies corresponding to each heartbeat sent during a session. using these data, the mean latency per session (interaction between pairs of users) was calculated."
"however, in the case of experiment-5, which was designed to examine the impact of different substrate topologies on opacovne's performance while using location constraint, acceptance ratio trend slightly increased, from 86.98% for small connected substrates, to 87.96% for larger connected configurations as shown in fig. 7h . this result implies that solving vnes using location constraints was barely affected by the size of substrate network topology, compared to the resources' capacities of substrate network."
"1. [cit], as explained in subsection 6.1. 2. [cit], as detailed in subsection 6.2. 3. furthermore, four different experiments were also conducted as shown in subsection 6.3 to test the impacts of varying number, lifetime, and size of the embedded virtual requests, as well as the impacts of varying size of substrate edges, on opacovne's average power consumption, acceptance ratio and processing time."
"most participants did not report feeling unsafe around the robotic surrogate. we did observe participants stepping back and participants reported that they felt the need to step back, but this did not make them feel unsafe. the robotic surrogate (and its inhabiter) did not have knowledge (situational awareness) of its local environment. as a result, the robot sometimes invaded the participant's space thereby triggering an avoidance instinct in them. this behavior would typically not occur if people collaborating closely in a physical space had knowledge about each other (including via their robotic surrogates). in the interview, one participant said that they felt unsafe because they felt the surrogate was not looking at what they were doing. this suggests that the robotic surrogate needs to appear to be continually aware of the participant's activity, even if it is not interacting with the participant. this ties back into the \"situational awareness\" discussion of the surrogate. we also refer back to figure 8 where the participants frequently glance towards the robotic surrogate, perhaps to gauge whether it will move; one could investigate if the robotic surrogate should do the same. another participant said that they would have felt more comfortable around the robotic surrogate if they had known its capabilities. this is an interesting observation as it suggests that even as these robotic surrogates become more realistic in appearance, those interacting with them may not trust them because they understand that robots in general can have different capabilities than humans. we have covered some of the previous research regarding trust during human-robot interaction in the related work section of this manuscript."
"for any substrate graph consisting of similar pairs of nodes and edges as the vnr's pairs, without any hidden hopes or edges, reformulate its resources into a set of segments, listing the substrate pairs' loc, cpu, bw, and delay values together as shown in eq. (3):"
"the advantage of such an architecture is the support offered for multiple configurations involving several hardware devices, each of which can inherit different actions based on their parent avatar's characteristics. while not the focus of this manuscript, these robotic surrogates also support appearance changing via rear-projected faces."
"this is the differentiating aspect of opacovne algorithm compared to others, mainly because it facilitates solving the two subproblems of vne, virtual nodes' embedding problem and virtual edges' embedding problem, together, providing full coordination, while assuring not to use any additional substrate resources, to minimize total power consumption in the physical network. to do that, first opacovne will reformulate all pairs of the arriving vnr r into a set of segments, set r, following the format of eq. (2). then, it will construct the set of segments, set s, representing all pairs of the candidate sn topology path p sub following eq. (3). this will prepare opacovne to check the possibilities of embedding each virtual pair of nodes and their edge, onto a corresponding substrate pair of nodes and their edge, one-to-one, together, and without utilizing any hidden hopes or edges."
"to generally evaluate the new online power aware and fully coordinated virtual network embedding algorithm, opacovne, three sets of simulations were conducted in this paper:"
"the core idea of opacovne can be summarized as follows, for every virtual network request vnr r number r, opacovne will reconstruct it into a collection of pairs, in which each of them will be composed of two virtual nodes and their edge, then the demands of each pair will be listed in a set format, to be called (segment), representing all parameters of the two virtual nodes and their edge, including node's locations and their cpus processing powers, in addition to their edge's demanded bandwidth bw and delay. as a result of that, opacovne will consider all pairs of any vnr as a set of segments to be handled collectively. to perform the embedding process, opacovne will afterwards select a similar number of substrate pairs, each composed of two nodes and one edge only, which comply with the demanded locations of the virtual pair's nodes and edge, then it will reformulate their resources into segment format similar to the structure of the virtual segment format, representing the substrate pairs' parameters. ultimately, the algorithm will compare each parameter from the virtual segment to its corresponding parameter in the substrate segment, and if all comparisons were true, opacovne will embed the virtual nodes and their edge from each virtual pair onto its substrate counterpart."
"therefore, another strategy was developed, performing both vnm and vem as two separate subproblems, but with some coordination between their solutions, where vnm is performed according to predefined vem constraints to guide allocating the virtual nodes [cit] . however, even through this provides a sort of coordination between vnm with vem, still, virtual nodes could be embedded at physical nodes that are farther away from each other, which implies enforcing edges to be mapped at longer physical paths, as well as resulting on additional costs, power consumption, and maybe rejecting some vnrs. furthermore, regardless of the used strategy, vne used to be constrained by virtual node's processing capacities, and edge's bandwidth demands, but occasionally considering power consumption, or adding delay as an additional constraint. accordingly, the lack of considering more constraints throughout the vne process, most likely would result on degrading the overall quality of the whole embedding process, especially when considering network virtualization for 5g applications, which are sensitive to the amount of delay in the network for example (5g [cit] ) ."
"rest of the paper is organized as follows: section 2 provides related work. segments definition and formulation are presented in section 3, followed by an overview of the online scenario and its main objectives in 4. section 5 will introduce the system models, problem formulation, explanation of the proposed online algorithm, and discussion for its evaluation results will be presented in section 6. finally, section 7 concludes the paper and highlights some future work."
"another methodology to solve the embedding of virtual demands was suggested by . they used a strategy using the mathematical frameworks of path algebra, which finds all eligible paths between each pair of nodes in the sn to embed a virtual network request using any type of constraints. [cit], they ordered the sn paths using an algorithm based on path algebra mathematics, which ranks and choses sn nodes to host the virtual nodes, then, they search for the best sn path connecting the already chosen nodes if it exists, by selecting the path with highest order based on some determined metrics."
"the average amount of saved power when the proposed power reduction strategy was used. it is calculated after each time interval t, given by subtracting the total power consumed by the whole sn without power reduction strategy pc t−, from the total power consumed by all sn after applying the power reduction strategy pc t+ ."
"optimal solution for vne is known to be np-hard and computationally intractable, since it can be reduced to the multi-way separator problem, which is np-hard by itself [cit] . some of the main reasons highlighting why solving vne is challenging, could be due to randomness of the arrival of vnrs depending on users' demands, as well as topology and limited sn resources. however, the virtual edges' embedding problem is what makes vne problem exceptionally an np-hard, since it can be reduced to the unsplittable flow problem, which is np-hard [cit] . consequently, solving vne problem in polynomial time is not possible. therefore, majority of vne approaches followed heuristic or meta-heuristic algorithms to solve vne optimization problems in a reasonable polynomial time [cit] . accordingly, to solve the objective function, fully coordinating virtual nodes and edges' embeddings together, while minimizing the total power consumption in the whole substrate network, this paper proposes the online algorithm, opacovne, as a power aware vne heuristic, which can provide final results for the vne problem in milliseconds of time. its main advantage in reducing the total power consumption relies on using only direct edges between any pair of nodes, then utilizes the segmentation approach for comparing each element in the vnr r set of segments, set r, against their counterparts in the sn path's set of segments, set s, considering the following constrains, namely: loc, cpu, bw, and end-to-end delay. if the elements of the substrate segment has enough resources to accommodate the demands as stated by the elements of the vnr's segments, a successful embedding occurs."
"this paper introduced a new online embedding algorithm, opacovne, solving the two subproblems of vne process in full coordination using end-to-end delay as a main constraint. the algorithm restructures virtual nodes and edges' demands in one set of segments, and formulates an exact similar set of segments for a specifically selected physical path topology, which comply with the exact demanded locations to embed the virtual nodes. consequently, the embeddings occurs by comparing the two segments, one-to-one, and checking if each element in the physical segment can accommodate the demands of their counterparts from the virtual segment. in addition to that, to minimize the total power consumption in the whole substrate network, the proposed algorithm insures that each pair of the substrate network nodes hosting the virtual nodes, are directly connected by a single edge with no hidden hops or edges, which guarantees utilizing the least substrate network resource as low as possible."
"however, the general behavior of average processing time had an increasing trend when changing the size of vnrs or substrate network topologies as shown in fig. 7k and l. the processing time trend for smaller vnrs varied between 1.42 and 7.49 ms, while for substrate networks that has large number of connected pairs of nodes, the average processing time varied considerably between 15.59 ms for topology t1 where average number of substrate edges was 380, to 238.23 ms for the case of topology t5, which had 860 edges."
"these values confirm the importance of including end-to-end delay as a major constraint when solving vne problem, as a direct evaluation metric for real world 5g networks."
"however, in general any substrate or virtual directed graph can be reconstructed of one or multiple paths, where each path could be composed of multiples of the basic segment above representing the pairs formulating the corresponding paths. therefore, to generalize the concept of the basic segment in eq."
"in this way, the demands of vnr r1 are translated from a line graph topology, into a set of segments representing the demands of the two pairs formulating vnr r1 . the same procedure can be repeated for vnr r2 as shown at the top of fig. 2c ."
"(1) what are the factors that influence people to believe that they are in a different location (presence)? (2) how can we improve the ability of people to perform tasks in remote locations (teleoperation)? (3) how can we combine (1) and (2) so a person in the remote location and the person being \"teleported\" can effectively communicate or work with each other?"
"the overall performance results of opacovne showed that, without delay, it managed to minimize the average power consumptions in the substrate network by 23.54%, however, when end-to-end delay was factored in, performance of opacovne was degraded across all evaluation metrics, suggesting that, introducing end-to-end delay as a major constraint, had clear impact on the whole vne process, and therefore, it has to be one of the main metrics when evaluating real world 5g networks."
"we developed this scenario primarily to push the technical boundaries of what was possible with robotic surrogate representations. we found the use of physical robots for telepresence interesting because of issues with latency and timing that are perhaps not a major challenge with purely virtual avatars. in addition, the telepresence occurs in a physical environment at both locations allowing the trials to physical manipulation in the next stage of this work. we also note that the scenario has potential use in training or rehearsal scenarios where tactile and haptic cues are important. for example, a trainer and trainee could both have physical access to an engineering piece, where it is important that both have \"hands on\" access simultaneously to the piece. while the robotic surrogates we are using today are not able to manipulate objects, the next generation will be able to do so."
"appropriate body language is an evidence of copresence between participants. it is also evidenced by the responses of the participants to the questionnaires. specifically, we enquired how participants felt with regard to whether or not their collaborator was with them. in addition, we also looked at how easy they found it to communicate with their collaborator. when asked directly \"how much did you feel that your collaborator was here in the lab?\", seven participants reported yes to some extent, two could not say, and five reported no. one of the most interesting responses was the following: this might indicate that when focused on the task, the participants are only peripherally aware of the robotic surrogate. three participants said that the audio was a distraction, with comments \"it was like a telephone call,\" \"the sound came from the whole room,\" and \"... voice is far away ...\" producing an authentic sounding voice that originates from the surrogate is a top priority for future work."
"the virtual pairs' u and v segment is denoted by seg r uv, and substrate pairs' i and j segment is denoted by seg ij . the structure of both segments must be similar in format, which means that the number of parameters representing the virtual nodes and edges in the virtual segment seg r uv, are exactly similar to the number of parameters representing the substrate nodes and edges in the substrate segment seg ij, regardless the values of their parameters."
") formulas are built inductively from the sets of quantifier and binding prefixes qnt and bnd and atomic propositions ap, by using the following grammar:"
"φ. on the other hand, satisfiability corresponds to a solution problem, where a correct solution, some witness structure m satisfying φ, has to be produced."
"model checking and satisfiability are two of the most prominent decision problems in logic. many questions in various branches of computer science, from formal verification, to database theory and artificial intelligence, can be solved by encoding them as instances of these problems for a suitable logic. some examples are verification of hardware and software, planning and scheduling, query containment, reactive and controller synthesis."
"in this paper we investigate the connection between the two problems for a non-trivial fragment of strategy logic (sl, for short) [cit] . sl extends ltl by means of two strategy quantifiers, the existential ∃x and the universal ∀x, as well as agent bindings (a, x), where a is an agent and x a strategy variable. intuitively, these elements can be respectively read as \"there exists a strategy x\", \"for all strategies x\", and \"bind agent a to the strategy associated with x\". the main technical differences between the two logics is that sl considers strategies as first class citizens and can express properties requiring an arbitrary alternation of the strategic quantifiers, while atl only allows for at most one such alternation. from a semantic viewpoint, this entails that atl cannot encode arbitrary functional dependencies among strategies. the ability to encode such dependencies is crucial to express relevant multi-agent systems."
"we can now prove the first result, establishing the equivalence between the satisfiability of a fsl[cg] sentence ϕ and of the corresponding fol[cb] sentence fol(ϕ). intuitively, given a cgs g satisfying ϕ, we can construct a first-order model f g for fol(ϕ), where the strategies in g play the rôle of the domain elements in f g and where a relation r ψ is satisfied on a tuple of these strategies iff ψ is satisfied on the associated play. conversely, given a first-order structure f satisfying ϕ, we can build a cgs model g f for ϕ, where the domain elements in f play the rôle of the actions in g f and the plays are uniquely identified by the decision chosen at the initial state (see figure 1) ."
"as a final remark, observe that, although the fsl[cg] sentences used to formalize nash and immune equilibria, as well as rational synthesis, are exponential in the number of agents, their satisfiability can still be checked in pspace. indeed, every disjunct has polynomial size in the number of agents, and, given the set of agents a, can be computed in pspace independently of the other ones. thus, we can apply algorithm 1 to each of them separately and determine if it returns true for at least one. the same holds for any fsl [cg] sentence ϕ in disjunctive normal form, i.e., where ϕ can be viewed as a dnf boolean formula over the atoms ℘η."
"consequently, the boolean formula β is satisfiable, as required by the statement ( * ). sentence ϕ, verifies the corresponding criterion stated in theorem 4. more specifically, the existential search for both a set of atomic propositions ℵ and an implicant i of ϕ is done at line 2, while the universal search for a unifiable set of qb-pairs s is performed at lines 4 and 5. finally, the ltl satisfiability test is performed at line 6. the next corollary immediately follows from the fact that the three set variables ℵ, i, and s used in the algorithm only require a number of bits that is polynomially bounded by the length of ϕ. the external call to the ltl solver only needs polynomial space in the input formula (∆ s i ∪ ℵ) [cit], whose length is polynomial in the size of φ. corollary 2 (fsl[cg] satisfiability complexity). the satisfiability problem for fsl[cg] is pspace-complete."
"an immune equilibrium [cit], instead, ensures the existence of a strategy profile for which a deviation of one agent from its strategy cannot induce a decrease in the payoff of a different agent. also this property can be formalized via a fsl[cg] sentence as follows:"
"the above proof also shows that, if a fsl [cg] sentence is satisfiable, it is so on a bounded-width model, i.e., a model with a finite number of actions. this follows from the finite model property of fol[cb] [cit] ."
"the rational synthesis problem w.r.t. immune-nash equilibria is solvable if there is a strategy profile for all the agents in ag \\ e that, once extended with an equilibrium for those in e, identifies a play that satisfies their corresponding ltl goals. note that none of the above properties is expressible in atl [cit] ."
"as it is usual for predicative logics, fsl [cg] requires the notion of free variables and agents in order to formalize its semantics. intuitively, by free(φ) we denote the set of variables that are not bound by any quantifier and agents for which there is no binding mentioning it in the scope of a temporal operator, for any fsl[cg] formula φ produced by one of the above rules. for the sake of space, we refer to for the formal definition."
", where the term t ℘, is obtained by replacing every existential variable in the term t with the associated skolem function w.r.t. the quantifier prefix ℘."
", where trn ∀ (ϕ) computes the skolemization of the fol[cb] sentence trn(ϕ). observe that, being axm(ϕ) universal, the skolemization procedure applied to fol(ϕ) only affects the trn(ϕ) component. formally:"
"the translation function trn simply replaces each goal ψ, i.e., an application of a binding prefix to a ltl formula ψ, with an atom r ψ ( t ), where the variables in t are put in place of the positions the agents are associated with in the binding . notice that this translation abstracts away the ltl semantics of the formula ψ. as a consequence, two relations r ψ1 and r ψ2 can be satisfied on the same strategy assignment, even though the two corresponding ltl formulas ψ 1 and ψ 2 cannot hold on the same play, namely the conjunction ψ 1 ∧ψ 2 is unsatisfiable. to recover the semantic relation between the ltl formulas ψ 1 and ψ 2 in the first-order encoding we need to enforce that the conjunction of r ψ1 and r ψ2 cannot be true on the same assignments, by adding the constraint ∀ x.¬(r ψ1 ( x) ∧ r ψ2 ( x)) to the translation. the axm function generalizes this argument to arbitrary unsatisfiable set of ltl formulas. for a given set ℵ of atomic propositions true at the initial state of a cgs, let"
"we have studied the relationship between model-checking and satisfiability in strategy logic. in particular, we identified a fragment of sl that allows for conjunction of goals but prevents strategic quantifications and agent bindings within temporal operators. from a semantic viewpoint, this restriction inhibits agents from changing their strategies during an execution. despite this limitations, the resulting logic is still expressive enough to encode relevant game-theoretic properties of multi-agent systems, like existence of nash equilibria. as a main contribution, we have shown that the logic enjoys a pspace-complete satisfiability problem, while its modelchecking problem is strictly harder, being 2exptime-hard. this also provides the first decidability result for satisfiability in sl, other than the one-goal fragment sl [1g] ."
"since the length of fol(ϕ) is exponentially bounded by the size of ϕ and the satisfiability problem for fol [cb] can be decided, as mentioned above, in σ in order to obtain an exponential improvement on the above result, we now introduce a satisfiability criterion for fsl [cg]"
"intuitively, there is a set of agents a that cannot satisfy their goals, while all the others can do it, independently of the deviations of some different agent."
"the idea behind the derivation of fol(ϕ) from ϕ is rather simple: first we replace every ltl formula ψ in ϕ with a corresponding fresh first-order relation r ψ and, then, we axiomatize the ltl semantics, by determining which ltl formulas can be satisfied on the same path for a given labeling of the initial state. the intuition is that strategies in a cgs model of ϕ play the rôle of the domain elements in the first-order model of fol(ϕ) and a relation r ψ is satisfied on an assignment for the first-order variables iff ψ is satisfied on the play induced by the corresponding strategies. formally, fol :"
"finally, we can express the notion of rational synthesis [cit], a recent improvement of the classical reactive synthesis in the context of system design, where the adversarial environment is not a monolithic block, but a subset e ⊂ ag of all the agents, each of them having their own goal. the aim of the environment is thus to oppose the system agents ag \\ e, while still finding an equilibrium for its own goals. in case of an immune-nash equilibrium, we can formalize this problem via the fsl[cg] sentence"
"beside enjoying a decidable and even pspace satisfiability problem, as we shall show shortly, fsl [cg] is an interesting fragment as it allows to express non-trivial game-theoretic properties of multi-agent systems."
"the satisfiability decision procedure for the conjunctive-goal fragment of flat sl, namely fsl [cg], is based on a first-order characterization of the associated semantics. more specifically, we show the existence of a computable first-order sentence fol(ϕ), for each fsl [cg] sentence ϕ, that is satisfiable iff ϕ is satisfiable as well (see theorem 2). this sentence belongs to a recently discovered decidable fragment of fol, namely the conjunctive-binding fragment (fol [cb], for short), proved to enjoy a σ p 3 satisfiability problem [cit] . since its length is exponential in the size of ϕ, a direct reduction between the decision problems of the two logics would merely provide an expspace procedure (see corollary 1). to obtain a lower complexity, we present a satisfiability criterion (see theorem 4), whose verification can be carried out in pspace. the pseudo code of the entire procedure is reported in algorithm 1. notice that pspace-hardness easily follows from the fact that fsl [cg] naturally embeds ltl, whose decision problem is known to be pspace-complete [cit] ."
"theorem 1 and corollary 2, showing that satisfiability can be easier than model-checking, witness an already rare phenomenon. even more so, considering that the sl fragment involved is still quite powerful. in general, however, a clear understanding of the actual relationship between modelchecking and satisfiability is not only interesting from a theoretical perspective, but it may also carry important practical consequences. indeed, the usual approach to system design is to manually produce the system first and, then, check that it satisfies some required properties. the checking phase can in some cases be automated, by using, for instance, a modelchecking procedure. a second, more appealing, approach is to directly synthesize the system starting from the desired property alone. this could be done by solving, when decidable, a satisfiability problem for that property and taking the witness model produced by the decision procedure as the desired system. given the often much higher complexity of solving satisfiability, the first approach is almost always the most convenient, if not the only feasible one, despite the cost of the manual design. an immediate consequence of our result is that automatically synthesizing a multi-agent system satisfying properties expressible in fsl [cg] is not only a viable option, but actually preferable to the first design approach also from a complexity-theoretical viewpoint."
"as the satellite system is assumed to be powered by solar energy, the power consumption of leo is not included as power consumed in the grid in this paper. the total grid power consumption of the hybrid network consists of the base station power cost and the gateway (also worked hss and mme with computing and storage capability) power consumption."
", where p tb is the transmission power of senb which is related to the traffic load, α ′ is the increase coefficient and p b0 is the static power of senb. as we focus on the maximum capability of the network, all of the small cells in this paper are not put into a sleep mode."
"in this paper, we focus on the hybrid satellite and terrestrial network with c/u-plane split, and propose two main resource management schemes (crms and drms) to study the fundamental relationship between the network performance (se, ee, throughput and coverage probability) and key parameters, including the transmission power and density of senbs, the static power consumption, gateway and the bias factor. it is shown that, compared with the hybrid network drms, the system with crms strategy to realize u-plane cooperation can achieve about 136% throughput gain, 60% ee gain, and nearly 77% coverage probability gain in ultra-sparse networks. efficient resource management scheme is suggested for hybrid network in future 5g network. in future, the high throughput satellite in ka or ku band will be studied and the delaycoverage tradeoff in this hybrid network will be analyzed. in addition, broadcast of satellite with the intelligent cache in terrestrial network will be exploited to enhance the energy efficiency towards green 5g hybrid networks."
"compared with the terrestrial network, a satellite network could offer significant advantages in terms of the cognitive capability to maximize the utilization of radio resources, the wider spatial coverage to offer control signals to the whole country, and the ability to offload and cache content and realize more efficient multicast delivery. based on the soft defined features, the c/u split hybrid satellite and terrestrial network could be one of the key enablers in next generation systems to meet various customized scheduling and allocation schemes while maintaining coverage [cit] . the 5gppp (public private partnership) project has been set up in european union (eu) funding research towards the standardization to develop an integrated 5g standard [cit] . it is shown that the hybrid network can indeed provide end-user devices (ue) with adapted and scalable capacity, network coverage and access [cit] and satisfy various quality-of-service (qos) constraints [cit] . however, to the best of our knowledge, the performance study has not been analyzed under the c/u split hybrid satellite terrestrial network and the study of efficient resource management strategies are still remaining as open issues."
"on the one hand, as the leo is in non-geostationary orbit, the spot beam coverage can be maintained by the handover between leos. on the other hand, the height of leo orbit is much larger than the distance of terminals movement on the earth. thus the doppler effect can be neglected for simplicity. in addition, the line-of-sight (los) transmission channel is for simplicity considered as the main factor to determinate the receive power from the satellite, so that the pathloss becomes the dominant factor to be considered. the received power from the satellite can be derived as follows,"
"in this paper, the c-plane coverage are provided by the satellite for both pues and sues. however, there are two ways for the u-plane service transmission to the terminals:"
"under the centralized resource management scheme, the gateway will route the traffic from the external network to both satellite through uplink transmission and senbs through backhaul in terrestrial network. combined with the results in (4), the se of senb under crms is"
"in this section, the distributed and centralized schemes (drms and crms) are compared from various aspects and efficient resource management mechanism is suggested in the hybrid network."
"in this network, the always-on radio resource control (rrc) control signallings, mtc and the low-data-rate services can be guaranteed by the satellite. meanwhile, the on-demand high-data-rate requests can be satisfied by senbs. as the senb cannot provide seamless coverage, the ue beyond the coverage of terrestrial network keeps both radio resource control (rrc) connection and data transmission with the satellite, which is called primary user equipment (pue). for the ue within the coverage of senb, namely secondary user (sue), it keeps dual connection with both small cell and satellite simultaneously. senbs only take charge of the uplane dynamic resource allocation, while the rrc connection and mobility control are maintained in the satellite in c-plane based on the soft defined features. in addition, small cells are linked through backhaul to realize the cooperation between each other."
", where t hroughput b is the overall throughput of senb in the coverage of leo spot beam. for the centralized network, the gateway power consumption will not only be the static power p c, the backhual power consumption p gbh c, but also the transmission power from the gateway to satellite to send back all the required traffic in u-plane of satellite, which are modeled as:"
"the related information about user requirement preference of content, moving speed and direction and other context information are stored and kept updated in the satellite, utilized as home subscriber server (hss) and mobility management entity (mme). however, as the satellite system itself works as an energy constrained network with limited storage and computing ability, it is more realistic for the gateway (gw) to work as the storage and computing center and take responsibility of transmitting all the related traffic and information back to the satellite. simultaneously, all of the traffic of the hybrid satellite terrestrial network in both c-plane and u-plane shall be routed back to the external network."
"in this paper, we address the fundamental relationship between key performance indicators and serval main parameters, such as overhead cost, density of senbs, transmission power and access bias. it is shown that the hybrid network can achieve better performance by taking advantage of the u-plane cooperation between satellite and terrestrial networks. the major contributions of this paper are summarize as follows: fig. 1 . c/u split hybrid satellite terrestrial network."
", which is the sum of the coverage probability of senb p c sen bs and the coverage probability of leo p c leo ."
"under drms, the throughput of the hybrid network is the sum of u-plane throughput in small cells. according to the classical model of stochastic geometry [cit], the spectral efficiency of senb can be derived as follows:"
"in order to address the challenges to meet and exceed the expected key performance indicators, the revolution of advanced 5g infrastructures has already attracted lots of attentions from both academic research and commercial enterprise in the information and communications technology (ict) field to enable highly efficient, ultra-reliable, dependable, secure, privacy preserving and delay critical services. [cit] . the 3rd generation partnership project (3gpp) system standards are heading into the 5g era to further improve capacity and performance, as well as system robustness for better handling of exponential smart phone traffic growth [cit] . in release 12 of 3gpp, the small cells (senbs) enhancement scenario is set as one of the critical scenarios, where the architecture is designed based on the soft defined network (sdn) concept to enhance both the c-plane and u-plane services. new technologies, e.g., new carrier type (nct) and device-to-device (d2d), are also studied under this scenario as the main issues."
", where the path loss exponent α in (2) is 4 and the thermal noise is ignored because the terrestrial network is an interference limited network. based on our previous work [cit], the overhead of u-plane o verhead b is nearly 15%, thus the network throughput can be obtained by"
"in the hybrid satellite terrestrial network, illustrated in figure 1, the c-plane and u-plane are decoupled from each other, in which the whole seamless c-plane coverage is ensured by the satellite, and high-data rate requirements in hot spots are served by senbs. small cells in high frequency (e.g., 3.5 ghz) are placed within the coverage of one spot beam of the satellite in lower frequency (e.g., 2 ghz)."
"for a typical ue, it is certain that the c-plane access is linked to leo network, while the u-plane access strategy is based on the resource management scheme. under drms, the u-plane access strategy is based on the reference signal receiving power (rsrp) from all of the nearby small cells. by contrast, under crms strategy, the u-plane access strategy is based on the comparison among the received power from both leo system and senbs, shown as follows:"
"the coverage probability is defined as the probability that a randomly chosen user can achieve a target signal-tointerference-plus-noise ratio (sinr) t in u-plane. based on the stochastic geometry knowledge, the coverage probability of two strategies can be obtained:"
the energy efficiency of the network is modeled as the throughput of u-plane of the network per watt consumed in the power grid. so the ee of drms and crms of hybrid c/u split network can be expressed as follows:
"in contrast to the traditional cdn modus operandi, where users receive content reactively (i.e., after they request for the content), here, we argue that in the mobile space users' applications need to be updated proactively. therefore, the system's target is to disseminate the most up-to-date content to destination nodes before users actually attempt to read the latest news."
"notably, differences between the two methods are more pronounced on the opossum/human dataset, where again blastz aligns more bases (2.4% versus 2.0%), but the overlap is relatively small (28.2% of the total 63 163 of bases in blastz chain alignments are also contained in satsuma alignments, 31.9% of the total 55 799 of bases in satsuma alignments are also contained in blastz chains). the amount of blastz extensions, however, is a much larger fraction than when using the dog dataset, with 33.8% of all bases aligned by blastz residing in regions that extend off of alignments found by satsuma as well [for a discussion on the issue of determining alignment boundaries, and possible over-extending versus under-extending alignments [cit] ]. the fraction of stand-alone satsuma-only alignments (1.3% of all bases and 67% of satsuma-aligned bases) and blastz-only (0.9% of all bases and 38% of blastz-aligned bases) alignments is comparable. lastly, the genome of the chicken, being an avian reptile, is quite far diverged from human. here, satsuma aligns slightly more bases (12.5 versus 11.8 kb), with about half of the bases found by both methods and ∼2 kb being blastz extensions off of shared alignments, and satsuma detecting 5.7 kb of sequence undetected by blastz and 3.2 kb of sequence alignments reported only in the blastz chain. table 1 compares matches found by satsuma and blastz that at least partially overlap annotated exons (refseq) in the chicken and opossum genomes. while there are more exons found by blastz-only than are found by satsuma-only in the human/opossum alignments, satsuma appears more sensitive in detecting orthologous exons on the chicken/human dataset."
"in the current/traditional client-server model, content is pulled from the (cdn) server upon the user's request. instead, ubicdn attempts to prefetch, distribute and make content available to users by default (i.e., before the user attempts to check for updates). although someone might argue that prefetching vast amounts of data to mobile devices will result in waste of bandwidth resources (given that the user will only read/watch/listen to a small subset of updates), ubicdn utilises local d2d connectivity to complete content transfers. that said, it does not consume network bandwidth resources, while the impact on the device's energy consumption is negligible, as we show through our proof-of-concept testbed measurements in section iv. ubicdn consists of the following node groups:"
"algorithm 3: the power allocation for relay to forward the s signal in the second timeslot is equally allocated. in the first timeslot, subcarriers are equally allocated to k relays. and in the second timeslot, relays utilized the same subcarriers to forward the s signal."
"for the purpose of the present study, we assume that the d2d transmission capacity of wi-fi direct is equal to 31.25 mbps and the range of transmission of each device is equal to 60 meters, with this capacity being different based on the distance of two nodes (i.e., inversely proportional to the square of their distance). alternative direct communication, e.g., through bluetooth, is also an option, although the details are left for future investigation. the main difference between the two technologies lies in their transmission range capabilities and energy consumption."
"indeed, news applications for smartphone devices today i) mainly come in the form of text content, ii) they get regular updates during the day, and, iii) offer limited multimedia/video content which is mainly consumed when connected to broadband networks. the reason behind consuming video content under broadband connections is that users cannot afford (in data terms) to watch video content when connected to the cell network."
"there is a clear bottleneck between the modern smartphones' storage capacity, the requirements of mobile multimedia applications and the users' monthly data allowance. given the increasing trends of mobile video and the fact that teenagers are considered \"video natives, as they have no experience of a world without online video streaming\" [cit], there is a pressing need for an infrastructure that overcomes this bottleneck and natively supports mobile video delivery at scale."
"putting it all together, a 32gb smartphone can hold more than 10,000 minutes of youtube or iplayer video (i.e., 166 hours of streaming or 1,000 ten-minute videos) -see fig. 1 (right). that said, the average smartphone today is a smallscale, always-on, always-connected and mobile data-centre for gb/month (1 hour/day usage) short video clips, songs or multimedia headline feeds."
"modern smartphone technology is ubiquitous and supports a multitude of d2d connectivity (e.g., wi-fi direct, bluetooth) and internet accessibility options (e.g., lte, wi-fi), which can be exploited simultaneously [cit] . these connectivity opportunities together with the fact that modern smartphone devices have large amounts of storage available enables client-serverlike communication between mobile devices themselves. the concept of opportunistic networking exploits natural human mobility as an opportunity to facilitate content dissemination [cit] . mobility, which is usually perceived as a challenging and degrading factor in terms of the network's performance, can now be considered as an opportunity to disseminate content in mobile devices [cit] ."
satisfaction ratio (in % of destination nodes): the percentage of the destination nodes with updated content over all destination nodes at the end of each application update period.
"satsuma interfaces with the synteny browser mizbee [cit] ), a visualization tool designed for exploring multiscale conservation relationships in comparative genomics data. mizbee augments satsuma by enabling efficient browsing of syntenic data across a range of scales from the genome to individual orthologous matches, shown in figure 1 . the design of mizbee is grounded in perceptual principles, and includes numerous visual encoding techniques to enhance cues about conservation relationships such as proximity, size, similarity and orientation. the development of mizbee was guided in part by the data satsuma generates, and the scientific questions the users of satsuma ask. more information about mizbee, as well as freely available executables and example data from satsuma (stickleback versus pufferfish genome), can be found at http://mizbee.org."
"(1) optimal power allocation for given subcarrier allocation: as l n,2 (s, τ, α, β) is a concave function of s n,k,2, we can get the optimal power allocation by applying the karush-kuhn-tucker (kkt) condition [cit] . more specifically, calculating the derivative of function (22) with respect to s n,k,2 and making it equal to 0. the optimal power p * n,k,2 are given by"
"in each of the three scenarios analysed in the next section, the above metrics are evaluated with regard to the impact of: i) relay time, ii) number of source nodes, and iii) update message size. we also present energy consumption statistics of source and relay nodes in a user-operated cdn based on a proofof-concept prototype of ubicdn. in what follows, the above metrics are averages over 12 application update periods."
"where c1 represents the energy used by each relay to forward s signal in the second timeslot should be not larger than the energy it harvests in the first timeslot, and c2 ∼ c5 indicate the constraints of the subcarrier set."
"compared to the traditional battery energy supplies, energy harvesting (eh) technology can extract renewable energy from the surrounding environment, which is not always stable and controllable. magnetic induction and radio frequency (rf) are two common ways to realize wireless power transfer, which can supply stable energy. however, the power transfer range will be very limited through magnetic induction [cit] . with the continuous breakthrough of ultra-low-power semiconductors, rf plays a more important role in long-distance communication [cit], which has the ability to carry both information and energy. therefore, as a technology with broad application prospects, simultaneous wireless information and power transfer (swipt) has roused great concern from academia, and has been widely investigated [cit] ."
"in tables iii and iv we show preliminary results using real devices (galaxy tab a tablet and samsung note 4 smartphone) and transmitting different file sizes using wi-fi direct. these results are obtained with two static devices separated by around 10 meters and with a rssi value close to -50dbm. we carried out a set of experiments using file sizes of 5 mbs, 50 mbs and 100 mbs and we extrapolated these results to the number of messages sent by source and relay nodes using the ubicdn and fltcdn mechanisms during an update interval of 1 hour. in fltcdn, each source node has to transmit between 660-785 messages during one update interval (i.e., 660 messages in the 1 hour relay case and 785 in the no relay scenario), whereas each relay node transmits up to 210 messages (i.e., 1 hour relay case), including the overhead messages. the corresponding number of transmitted messages in the ubicdn mechanism are 41-62 messages for the source nodes and up to 3 messages for the relay ones. we also extrapolated from this energy consumption the percentage of battery consumed using a tablet (i.e., 7000 mah/25.9wh capacity) or using a smartphone (i.e., 3200 mah/12.4wh capacity). the energy consumed by the wi-fi direct application is measured using the trepn profiler tool [cit] ."
"in recent years, breakthrough progress has been made in the internet of things (iot) technique which has been widely applied in industrial automation, intelligent medicine, smart grids, etc. [cit] . according to estimates, the communication system will support more than 50 [cit] . in the meantime, these put forward larger requirements on energy supplies [cit] . therefore it is extremely urgent to design a green sustainable scheme which provides a larger transmission rate without requiring frequent battery replacements [cit] ."
"message overhead. given the population (and therefore device) density in urban environments, d2d proactive dissemination of content raises questions of overhead in terms of messages transmitted to non-interested recipients, or recipients that already have received the update. therefore, a second metric of interest is message overhead, defined as:"
"according to the above statistics, the ratio of \"storage capacity to network data allowance\" for cell-connected smartphones is roughly equal to 16 (32gbs of storage capacity and 2gbs of data per month), i.e., users can have at least 16 times more data stored in their devices than can send/receive over the mobile/cell network within a month 2 . adding a 64gb microsd card (presently costed around $20), the ratio of \"storage capacity to data cap\" skyrockets to 48."
"in order to ensure data integrity (i.e., content is what it claims it is and has not been modified by intermediate users), ubicdn integrates digital signatures (e.g., hmac) based on public key infrastructure (pki). this setup prerequisites that the digital certificates used by the application provider do not expire while the users are disconnected. this way, users can easily authenticate the content they are receiving. other security vulnerabilities, such as, eavesdropping, privacy violation, or denial-of-service (dos) attacks, are out of the scope of this paper. however, related literature provides ways to deal with such issues in d2d communications (e.g., [cit] )."
"from the top error bars, which denote the most popular application, it is interesting to note that the most popular application is updated in almost the same amount of destination nodes using either of the two dissemination mechanisms (see 2100 secs point in satisfaction ratio plot). however, information-aware transfers through ubicdn introduce only around 20% of message overhead, whereas in the fltcdn case almost 90% of the transmitted messages for that particular application is redundant (message overhead plot in fig. 2 ). on the other hand, due to the limited interest for the least popular application (bottom error bars), we see that the fltcdn mechanism manages to update 25% more destination nodes than ubicdn. based on that, we can deduct that a hybrid approach where popular applications are disseminated using information-aware connectivity and the least popular based on the floating content scheme would get the best of both worlds, resulting in higher satisfaction ratios and lower overheads. it is clear (and intuitively expected) that less popular applications require some kind of data mules to reach interested users, whereas popular ones can be updated with the minimum overhead, given that most users in the area are interested in this application's content. we leave the specific design details of this tradeoff for future investigation."
"relayed content. one of the main challenges in realising a user-operated cdn comes in terms of costs, that is, how many source nodes would be needed (and therefore, compensated) in order to achieve proactive distribution of content to the majority of destination nodes and at what timescales would delivery be achieved. relaying is an important feature of message distribution and therefore, the amount/percentage of messages that is being transmitted through relaying is a central metric in our evaluation."
"satisfaction ratio. in contrast to a fixed network cdn, where the target is to reactively serve user requests within deadlines (that is, after the user has requested for some content), the target of a mobile-focused distribution network should be (we argue) to proactively load content to users' devices. in the context of smartphone applications, proactively comes in terms of the \"application update period/interval\". that said, one of the main metrics used here is the satisfaction ratio, defined as:"
relayed content (in % of total transmitted messages): the percentage of messages delivered by relay nodes over all transmitted messages at the end of each update period.
algorithm 4: the power allocation for a relay to forward the s signal in the second timeslot is equally allocated. the number of subcarriers for each relay used to decode information in the first timeslot is consistent with the number of subcarriers utilized for forwarding re-encoded information in the second timeslot. the hungary method is used to perform the subcarrier allocation according to
"applications: we assume a given set of ten different smartphone applications, where each application generates content updates periodically (e.g., a news application like bbc news could update its content every hour). we call the update interval \"application update period/interval\". for the purposes of this study and unless otherwise stated, we assume that the application update period/interval is 1hour and that the default size of each update message is equal to 5 mb (although we experiment with updates up to 200mb -section iv-c)."
"we remind that we assume wi-fi direct for the d2d communication (i.e., significantly larger transmission capacity compared to bluetooth). this means that as long as the messages are relatively small, even the naive fltcdn mechanism manages to successfully transmit the updated content between two nodes that \"meet\" for a small period of time. on the other hand, when the messages are larger (e.g., large or hd video messages) a relay or a source node following the aggressive fltcdn mechanism does not manage to disseminate the updated content to other destination nodes during the contact period. note that in fltcdn, and assuming ten applications, a node has to deliver 2gb of data, assuming 200mb of message size. from this finding we further deduct that the proposed information-aware and application-centric d2d dissemination mechanism (ubicdn) is not only efficient due to the low overhead of the communication, but is also the only viable solution when larger amount of data needs to be transmitted. furthermore, in the helsinki city scenario, the majority of source nodes and all the relay nodes are slowmoving \"workers\". in a futuristic faster-moving vehicular d2d scenario, where vehicles meet for even smaller periods of time a naive information-unaware scheme (fltcdn) will perform bad even for very small update message sizes. this further enhances the necessity of a more sophisticated context-aware and application-centric dissemination approach."
"to determine shift positions, satsuma requires signals in the backtransformed function to be 1.8 times stronger than the sd of all values within windows of 256 (this is inefficient; improving this algorithm will dramatically speed up the process. in most cases, due to the chunking, nonhomologous sequences are compared, resulting in noise only without signals. on average, ∼250 distinct shift positions are evaluated). a subsequent step now goes back into letter space, shifts sequences accordingly, and determines the regions in both query and target where more letters match than expected: this is implemented efficiently and is linear in time given the overlap size between sequences, by requiring a minimum of 13 matching bases in local, adjacent windows of at least 28 bp in size. each match that is found is then tested by the match scoring algorithm (see below) and either rejected or accepted. the latter are collected and stored."
"3.1.1 specificity the alignment probability model decides which matches are regarded as homologous and which are not, and thus directly controls satsuma's specificity. consistent with expectations, the model predicts that the shorter the alignment, the higher an identity is required in order to be 'real' (fig. 2a) . most notably, for all different lengths, there is a marked and steep drop so that at length 100, an identity of 0.45 indicates almost certainly that the alignment is noise, but increasing the identity to 0.52 predicts the alignment to be almost certainly real. to test the actual accuracy, we implemented a null model: sampling random alignments between non-homologous sequences from two organisms (lizard and chicken), where one sequence was, in addition, complemented but not reversed to ensure that no homology was to be found, while preserving the non-randomness of local base composition [cit] . figure 2a shows that the model's prediction (for length 50, 100, 150 and 200 bp) resembles the observation from the null model very closely overall. we note that, for short alignments (∼50 bp), it is simple sequence repeats (preserved in structure when complementing sequence) that cause the observation to shift slightly to the right (data not shown)."
"we observe exponential increase in the satisfaction ratio (left plots in figs. 4 and 5) as the number of sources s increases (linear increase in the plots for logarithmic scale xaxis). this increase is more pronounced in the case of ubicdn and for small relay times. with a small number of source nodes ubicdn performs up to 40% worse compared to the corresponding floating content schemes, but this difference is only marginal when the number of source nodes increases to 5% of the destination nodes (i.e., 50 nodes)."
"from the satisfaction ratio plot in fig. 2, we observe that when only source nodes disseminate content (i.e., no relaying) already half of the destination nodes manage to retrieve the updated content (first point in left figure). when destination nodes start to relay the received content, even for a very small amount of time (i.e., 5-15 minutes), the satisfaction increases by up to an extra 40%. this is also obvious from the relayed content plot (right plot in fig. 2) where we see that the total number of messages distributed by the relay nodes can reach up to 80% of the total transmitted messages in the fltcdn case and 50% in the ubicdn case. however, increasing the relay time to more than 15 minutes (900 secs onwards) brings no substantial gain in terms of satisfied users (left plot in fig. 2 ). this result illustrates the fact that while some users move in the city centre and therefore can interact and receive updates, some others remain in non-reachable areas, e.g., offices or outskirts of the city. this result serves as an upper bound of the performance of the examined content dissemination mechanisms, given the specific settings - table ii. this result is more clear in fig. 3 where we depict the satisfaction cumulative density function (cdf), which shows how many users acquire the updated content over the duration of an application update interval (1-hour) for different relaying durations. from fig. 3 we observe that the vast majority of the updated destination nodes receive updates within 20-25 minutes after the release of the update, and only a small portion of them is updated towards the end of the application update interval. for example, for the \"15-min relay\" case, 62% of the destination nodes are updated at the end of the 1-hour update period, 70% of which are updated within the first 15 minutes (900 secs). the same holds for the floating content schemes too, even for relay times that increase up to the application update interval (i.e., 1-hour). comparing the performance of ubicdn's informationaware connectivity with information-agnostic floating content in fig. 2, we observe that the fltcdn scheme performs around 15% better in terms of satisfaction ratio. however, as expected with this \"aggressive\" approach, it creates at least four times more redundant transmissions (i.e., overhead), as shown in the message overhead plot (middle figure in fig. 2 ). in case of ubicdn, message overhead is caused by unfinished transfers due to users getting out of range of each other before the transfer completes. on the other hand, as expected, the fltcdn approach, which effectively, floods nodes with messages has severe consequences in terms of the energy spent by user devices. we present energy consumption results later on in this section."
we can use the normal cdf to estimate the probability p l of finding a local alignment of length n and identity at least x bases by chance
"through extensive simulations we find that in densely populated urban environments a small percentage of source nodes (e.g., around 5%) is enough to disseminate content to almost 70% of destination nodes (i.e., the population that has the specific application installed). application updates are based on information-aware d2d connectivity [cit], while transfers are realised through wifi direct. we test the concept of a ubiquitous, user-operated cdn (ubicdn) under a variety of network conditions and compare its performance with an information-agnostic d2d, floating content scheme. we measure the energy-consumption of the system in a small proof-of-concept prototype and find that if content distribution targets the right nodes and is kept to reasonable levels (in terms of exchanged messages), then energy consumption is not an issue (i.e., consumes roughly 1% of battery per hour)."
"according to ubicdn, whenever a source node is in range with a destination node it will first check for matching applications, and if found, the corresponding content will be compared. if the discovered node has out of date content the source node will push the update via a direct message to the destination node. these messages are also prioritised by the application popularity, meaning that more popular applications will be updated first and less popular afterwards. if relaying is also enabled, the updated destination node will become a relay node either for a fixed amount of time or until it transmits one successful update (i.e., single contact relay), depending on the node's settings."
"the genomes of flowering plants, such as the grasses rice (oryza sativa) and sorghum (sorghum bicolor), exhibit a number of large-scale duplications, rearrangements, as well as high levels of interspersed repeats, comprising 40% (rice) and 61% (sorghum) of the genomes [cit] . satsuma aligned the 390 and 750 mb genomes in 480 cpu hours, the genome-wide synteny plots of the 12 rice chromosomes against the sorghum genome are shown in figure 4 . in addition to capturing rearrangements ranging from several kb up to tens of mb, satsuma finds both orthologous as well as paralogous syntenic matches. we note that the high repeat content somewhat elevates the noise levels compared to vertebrate genomes; however, the syntenic signal is still clearly visible [for protein-based synteny maps [cit] ]."
", ∀n (26) where p s is the source's transmitted power. by extracting the common factors τ n,k,1 and τ n,k,2 respectively, equations (25) and (26) can be expressed as"
"in this section, simulation results are illustrated to evaluate the performance of the proposed algorithm. we consider the rayleigh fading channel with the central frequency given at 1.9 ghz. we set the distance from the source to each relay and the distance from each relay to the destination both as 2 m. the number of subcarriers is 32, and the noise variance is fixed to −80 dbm."
"here, the goal is to determine the probability that an alignment of length n represents a real biological signal and not random chance. let r denote the number of random matches in the alignment; then r ∼binomial(n,p) where p is the probability of a random match at a single site. the value of p depends on the gc content in both sequences; let n gc and ν gc denote the number of g's and c's in the query and target, let n at and ν at be the number of a's and t's. then the expected number of random matches is"
"from tables iii and iv, we can observe that the energy consumption for the ubicdn source nodes goes from 106.6 mwh in the best case (41 messages sent) when sending 5 mb messages, to 321.16 mwh in the worst case (62 messages) when sending 100 mbs (including the energy required for the group formation mentioned in section ii-a). this means the percentage of the battery consumed is between 0.86% and 2.59% for a smartphone, and between 0.41% and 1.24% for a tablet. in case of relay nodes the energy consumption goes from 7.8 mwh to 15.54 mwh, meaning from 0.06% to 0.12% of the battery for a smartphone and 0.03% to 0.06% for a tablet, respectively. from this analysis we can consider that energy consumption is not an issue in the case of the ubicdn case even if we assume large update messages. however, in the fltcdn case the non-application aware content delivery and the relatively increased message overhead will deplete quite fast the battery of a user's device for source . this means that a more sophisticated content aware dissemination mechanism is required in order not to discourage users from participating in a user-operated mobile cdn."
"opportunistic networks can increase network capacity [cit] and offload traffic from a cellular to a cellular-assisted deviceto-device network [cit] . they can also support communication and content exchange when the cellular infrastructure is under severe stress [cit], whereas at the same time, are the only means of communication when the network infrastructure is down or inaccessible due to natural disasters or government censorship. in opportunistic networks, connectivity among devices is intermittent and communication can be very lossy. this might decrease the possibility for successful content forwarding. however, the fact that user movement and mobility patterns have limited degree of freedom and variation, and rather exhibit structural patterns due to geographical and social constraints [cit], minimizes this uncertainty."
"we formulate a scheme by optimizing power and subcarrier allocation with the aim at maximizing the transmission rate, and employ the time-sharing strategy and lagrange dual method to solve this optimization problem."
"the teleost fishes stickleback (gasterosterus aculeatus, gasacu1) and pufferfish (tetraodon nigroviridis, tetnig1) feature small genomes for vertebrates, at 470 and 217 mb, s match probability model predicts that a single, gap-free alignment of given length, identity and gc/at composition is found by random chance given the target genome size (lizard, 1.7 gb) in light colors (y-axis), over the match identity (x-axis). this is compared to a null model (see section 2) shown in dark colors. (b) sensitivity: identical sequences of different lengths (50, 100, 150 and 200 bp) were inserted into non-homologous dna from the human genome and mutated by randomly changing bases over the entire region (base substitution rate, x-axis, top), resulting in a decrease in sequence identity (x-axis, bottom). each bar indicates that the alignment was correctly identified by satsuma given the length of the sequence to be found over increasing mutation rates."
"at the same time, there is a growing gap between the amount of storage that a smartphone possesses and the amount of network data that a user can consume per month (a.k.a. mobile data cap). the average smartphone on the low-end of the price spectrum holds roughly 16 gbs of storage capacity, while trends want this number to roughly double every year 1 ."
"to align query and target sequences, both are first cut into chunks (4096 bp), where the target chunks overlap by a quarter of the chunk size, and two chunks are compared pairwise sequentially. dna sequence is transformed into floating point sequence 'signals', where, in the case of nucleotide alignments, there are four signals for each sequence, one for each letter (a, c, g and t): initially, each position for which an unambiguous base occurs is set to 1, the remaining positions are set to 0. ambiguous base codes (iupac) are split among the letters so that they sum up to 1 (e.g. 'k' sets 't' and 'g' to 0.5 each). by using information entropy as measure, we de-emphasize simple sequence repeats, such as homo-polymer runs, to lessen the effect of overshadowing and undershadowing other signals (a long run of a's in the target, for example, will add a non-random signal over the entire window size that reflects the base composition in the query sequence), which we found to not significantly alter the results, but increases the efficiency of signal selection. after subtracting the mean from each signal to normalize over sliding overlap lengths and adding zero-padding of another 4096 bases (otherwise signals would look periodic), the cross-correlation theorem is applied by first fourier-transforming each signal using the fft algorithm, multiplying signals (one with the complex conjugate of the other, and then transforming back into letter/signal space). as a result (and after adding up the signals for each letter), we obtain a function that shows positive spikes indicating the number of positions the signals have to be shifted across each other so that a higher-than-expected number of matches can be found."
"for a runtime comparison, we ran blastz with default parameters on 140 pairs of randomly chosen, non-repeat-masked sequence samples from human chr1 and dog, using the chunking strategy (1 mb versus 30 mb chunks) described for the original humanmouse blastz alignments [cit] . from the mean runtime of 34 min, and given that the chunking required by blastz scales up its runtime at o(n * m), we estimate that aligning 250 mb of human chromosome 1 to the 2.4 gb dog genome would take ∼11 500 cpu hours, compared to 230 cpu hours for satsuma."
"1: given p s . 2: compute u n,n,k . 3: obtain the optimal subcarrier allocation s i 1,k *, s e 1,k *, s 2,k * using the hungary method."
"thus far, satsuma can overcome two intrinsic problems inherent to seeded alignment approaches: (i) no implicit or explicit assumptions are made about the pattern in which sequences have to match (as seeds require); and (ii) the need for repeat-masking (either with a repeat library, which might not be available for all genomes, or based on k-mer counting, which will make the aligner blind to gene families) is eliminated, since the runtime does not increase with repeated regions. however, even though the comparison of two sequence windows is fast, applying this algorithm exhaustively (i.e. each target is compared to each query window) results in computational runtimes of o(n*m), with n and m being the sizes of the sequences to be compared. while this might be acceptable for certain tasks, such as aligning fungal genomes or cdnas to larger genomes, it is impractical for whole-genome comparison. to make the runtime characteristic more linear with genome sizes, satsuma implements a 'synteny search' algorithm, analogous to the paper-and-pencil game battleship, which takes advantage of the fact that the order and orientation of homologous sequences is highly conserved even for distant organisms [e.g. human and opossum [cit] or human and chicken [cit] ]. first, query and target windows are mapped out on a grid as squares, and only a small fraction of the grid is searched for initial alignment hits. then, only the squares in the neighborhood of hits are searched iteratively for more hits, thus following syntenic stretches (sinking the enemy's long ships), so that, in the end, only a small fraction of the entire grid has to be looked at. this strategy, implemented as a highly parallelized process, dramatically reduces cpu time and allows for comparing large sequences (for implementation details of all algorithms, see section 2)."
"finally, we compute p real, the probability of finding exactly zero matches of equal or better quality by chance. if the target genome size is s, then the number of false matches can be approximated by poisson(2p l s) (the factor of 2 accounts for potential reverse complement matches). this gives the probability of zero random matches"
"satsuma is a sequence aligner that implements very different strategies than other programs, most notably a search strategy on a global level and cross-correlation at the local level. while this eliminates the need for repeat masking, another key feature gained by a seed-less approach is satsuma's innate ignorance of the exact nature of sequences: any two sequences of anything, as long as they can be translated into vectors or matrices of floating point numbers, can be aligned. as a practical implication, satsuma is, as of now, fully capable of incorporating ambiguous base codes into alignments at all stages, which, for example, allows for proteinprotein and protein-nucleotide searches. in the future, additional sequence information can easily be added to increase sensitivity beyond the nucleotide level: for example, only ∼40% of the genome sequences of mouse and human can be aligned to each other [cit], and this is probably close to the limit of what can be done with nucleotide sequence alone. however, it has been shown that local dna topography is conserved in some cases where the nucleotide sequence is not [cit] ). since topography can be represented as a single number per location, this information could increase the fraction of aligned genome overall if used throughout the entire alignment process in addition to nucleotide sequence, which might not only fill gaps in alignment coverage, but recover entire regions that were simply unalignable before."
"last, but certainly not least, we look into the energy consumption of user-operated cdns. energy is the price paid by the system in order to disseminate content in a d2d manner and therefore, cannot afford to be overlooked in our feasibility study."
"up until now, we assumed that each application update message is 5mb (e.g., a 2-3 minutes standard youtube video). in fig. 6 and fig. 7 we examine the impact of the update message size on the performance of ubicdn and fltcdn. the overall observations are similar to the previous experiments for all metrics, e.g., the satisfaction ratio is similar to the previous experiments, with fltcdn performing 10% − 15% better than ubicdn as long as the size of each update message is smaller than 100mb (left plots in figs. 6 and 7) . from that point onwards, the fltcdn scheme performance declines sharply to the extent that the information-aware mechanisms perform almost 90% better than the corresponding floating content ones (for update message size of 200mb)."
"the rest of the paper is organized as follows. in section 3, we introduce the system model and present the problem formulation. the proposed joint resource allocation solution is illustrated in section 4. section 5 provides simulation results and discussions. finally, conclusions are drawn in section 6."
"in terms of mobile data consumption, a standard definition youtube video consumes approximately 2mb/min of network data, translating to 120mb for one hour of streaming. this figure varies widely across platforms and can reach up to 20mb/min for high definition (hd) video -see fig. 1 (left) 3 . this means that one hour of youtube streaming per day (e.g., two 30min one-way commutes) consumes a 2gb monthly plan within 16 days, iplayer consumes a 4gb plan in 17 days and netflix consumes a 12gb data plan in just 20 days -see fig. 1 (middle)."
"in this paper we set off to build an information-aware and application-centric d2d connectivity framework and realise a distributed and ubiquitous content distribution platform, acronymed ubicdn. ubicdn focuses on smartphone applications that receive frequent (in the order of 1-3 hours) and heavy (in the order of 5mbs or more) updates. such applications are normally news applications, e.g., newspapers, national broadcasters, or topic-specific online tv-like channels, e.g., youtube, periscope. such applications receive updates regularly throughout the day, which, however, are not directly pushed to the user's device to avoid consuming their cellular data 6 ."
"message overhead (in % of total received messages): the percentage of messages rejected by the users as duplicates (i.e., nodes have already received the update) or messages of no interest (in the floating content mechanisms), or transfers that did not complete due to users moving out of range of each other, over the total received update messages at the end of each update period."
"we argue that the current client-server centric data transfer model leaves enormous amounts of resources unexploited. powerful end-user devices can act as data sources and take advantage of local connectivity (through wi-fi direct, bluetooth, google nearby 4 ). although these technologies have been 4 https://developers.google.com/nearby/ around for quite a while (the first bluetooth distribution came out more than a decade ago 5 ), there have been surprisingly few applications (mainly gaming and chat applications, e.g., firechat) that exploit such connectivity in a user-transparent way. that said, links between devices (with speeds that can reach up to 250mbps for wi-fi direct and 25mbps for bluetooth 4.0) remain largely unused."
"we have explored the feasibility of a user-operated, ubiquitous cdn (ubicdn) that targets distribution and dissemination of heavy video content to mobile devices. we have argued that in a mobile, cell-connected setting, heavy content needs to be pre-loaded to mobile devices, in contrast to broadband connected environments where content is delivered on demand. this is not least because mobile data consumption is limited by the users' monthly data caps. we build on the premise that modern mobile devices possess large amounts of storage and can act as data mules. our results show that a small percentage of source nodes are enough to prefetch content to the majority of mobile devices in urban settings, while relaying can also be kept to a minimum (around 15 mins per relay node). information-centric connectivity and data transfers keep energy consumption to a minimum, cancelling incentive concerns for mobile users to participate in the system. our results prove the feasibility of a user-operated cdn for smartphone applications. further investigation is needed in order to cover all corner cases of the system and also investigate incentives and business relationships."
"where ξ is the energy conversion efficiency of relays. the transmission rate from source to destination via relay k is the minimum of the rate achieved over two hops, which can be written as"
"destination nodes are configured as follows: out of the 1000 nodes 20% are assumed to be tourists. tourists choose random destinations (either total random points in the map or one of the seven \"points of interest\" (i.e., tourist attractions) in the city centre) to which they travel following the shortest path and wait randomly 2 − 15 minutes. the majority of the destination nodes, i.e., the remaining 80%, are assigned the working day movement model [cit], which allows them to travel to designated office spaces on the map and travel for other evening activities later in the day. all nodes start at their base/home and travel to their office, either directly by car (50% of nodes) or by bus (remaining 50%). once they reach the office, they spend 7 hours there and at the end of the office day there is a 50% chance the node will go for an \"evening activity\" and 50% chance it travels home for the rest of the day."
"in this paper we investigate the feasibility of a useroperated content distribution network for mobile smartphone applications. in terms of applications, we focus on newsrelated apps (e.g., bbc, cnn) that update their text and video content frequently throughout the day. source and destination nodes are the users' devices that have the application installed. in particular, we assume a fixed number of source nodes who are directly updated by the content/application provider through cell or wi-fi connectivity. exploiting mobility patterns in urban environments, the source nodes disseminate content updates to destination nodes through smart information-aware and application-centric connectivity [cit] ."
"1 https://thomas.vanhoutte.be/blog/2015/12/29/mobile-phone-timeline/ the price of a gb in a microsd card is between $1-$3 (i.e., between $35-$60 for 128gbs of storage). on the other hand, in terms of network data allowance, the average monthly datacap is at about 2gbs/month (ranging from less than 1gb in eu up to 30gbs in the us for contracts costing more than $40/month)."
"it can be observed that g 0 (α) is linearly correlated with r k . in order to maximize l 0 (r, α), the optimal r * k must satisfy"
"the battleship search is implemented in a completely asynchronous fashion, such that one process is the master (i.e. it continuously runs dynamic programming and re-sorts the pixel priority queue as data comes in), which farms out lists of pixels to be searched to slaves that run on a compute farm or a multi-processor machine. as soon as a slave finishes the task, it reports back with the results and fetches a new set of pixels. communication between master and slaves is implemented via tcp/ip. this asynchronous implementation has a number of implications: (i) at any given time, it is the best guess that decides how to prioritize pixels, without having the entire information that would be available in a linear implementation; (ii) computational load can vary throughout the process, i.e. one can take advantage of all free cpus available at any time, provided the computational cost to process a single set of pixels is low (∼5 min with the default parameters); and (iii) the exact sequence in which pixels are searched is undefined, since the order in which pixels are processed depends on the timing, number of available cpus, etc. and is thus not deterministic. we ran experiments under different conditions and found that, while the size of the search space might vary, the resulting alignments are very stable and only minimally depend on processing order."
"what we compare with: for comparison purposes we have extended the one simulator with an application-and information-agnostic content delivery scheme. according to this last scheme, source nodes blindly send blanket update messages to nodes they encounter without comparing content of the same application or checking whether the encountered node has the latest update or not."
"in the future work, we will consider the relay selection optimization, in which not all of the relays will be allocated subcarriers to forward information. moreover, we will study the amplify-and-forward relaying protocol in swipt enabled multi-relay iot system by considering the source and destination direct link. besides, it is also an interesting direction to study a two-way multi-relay swipt enabled iot system which is able to achieve higher spectrum utilization."
"the distribution of source nodes is as follows: out of the 50 nodes, we assume that 18 are buses that follow predefined routes, whereas the rest nodes (i.e., 32 in the default scenario) are assumed to be users that follow the working day movement model [cit] ."
"the scenario chosen for assessment was that of a busy city environment, namely helsinki city center, with a fixed population of users carrying a mobile device capable of d2d connectivity and support for multiple smartphone applications, and simulating different mobility models for multiple set of users defined in section iii-b."
"the aforementioned relaying swipt iot systems are based on single relay. to the best of our knowledge, swipt has not been applied in the multi-relay ofdm based iot system, which motivates our study. thus, in this paper we propose a joint resource optimization scheme in a swipt enabled multi-relay ofdm based iot system to obtain the maximum transmission rate, in which the transmission between the source and the destination is carried out through the assistance of multiple swipt enabled df relays."
"for the evaluation of the proposed user-operated cdn we extended the one simulator [cit] . one is a discrete event simulator for opportunistic network environments, and is capable of generating different node map-based movements using various models, routing messages using different dtn routing schemes and provides interfaces for application level extension."
"we approximate application popularity by a pareto distribution, given ample evidence from related literature that smartphone application popularity follows such distribution, e.g., [cit] . according to the used pareto distribution, 28% of the total number of installed applications in the mobile devices of the destination nodes are instances of the most popular application, whereas the least popular application counts only 4.5% of that number (i.e., skewed, heavy-tailed distribution, where some applications are installed in almost all devices, whereas the remaining are present in a smaller number of devices). moreover, applications are randomly assigned to users, but each user has at least one application installed."
"the matching results for eight image pairs are shown in fig. 7-14 . and the root-mean-square error (rmse), the number of correct matches (ncm), precision, recall rate and running time are listed in the table 3 . owing to the adoption of stable first-order statistics, our method can get better result, but the running time is large."
"image registration is a process of aligning images of the same scene which are acquired under different conditions, such as different times, various viewpoints or different sensors [cit] . it is a fundamental aspect of many problems in image processing, including image mosaic, image fusion [cit], transformation detection [cit], three-dimensional terrain reconstruction [cit], etc. moreover, many theories and applications are under the assumed premise that the registration has been done well. hence, the accuracy and the efficiency of image registration directly affect the follow-up applications."
"as shown in the table 2 and fig. 4, the gpfd obviously obtains more matches and more efficient results than gp. to be more convincing, we employ the gpfd method without ransac as a comparison and confirm that the image descriptor evolved by gpfd is robust to various noise. gp performs well for simple optical image, but it will be significantly affected in the presence of noise. there are lots of speckle noise between image pair 7. due to the speckle noise, some correct matches cannot be found and some pseudo matches are regarded as correct matches. this directly affects the matching performance of gp. while gpfd always achieves good registration performance. because the difference between gp and gpfd is terminal set, the terminals of gp are the max, min, mean and stdev. and the terminals of gp are easily influenced by noise. a simple experiment is used to verify this statement. randomly generate 1000 numbers between -1 and 1, add the gaussian noise with the noise to signal ratio of 0.01, 0.1, 0.2, 0.5, 1, 2, 5, 10, and compare the stability between the statistics. the noise robustness test results are shown in fig. 5 ."
"as shown in the table 3 and fig. 12, although sift, surf, rift, glpm and gp can get match results for image pair 6, the number of correct matches is small, and the recall rate is poor. the image pair 6 is obtained from different viewpoints and has complex non-rigid transformation. on the other hand, the image pair have subtle illumination change and image changes caused by glass reflection. these make the matching of feature points more difficult, and some real keypoints can not be found and some pseudo keypoints are regarded as real keypoints, so the number of correct matches are concentrated in the image with small changes. however, the proposed method still achieves better results than sift, surf, rift, glpm and gp, gpfd also is affected by complex image transformations, and the recall rate only reaches 0.6894. and later we will improve this problem by extracting more image information."
"where (x i, y i ) and (x i, y i ) are the coordinates of the ith matching keypoint pair, (x i, y i ) is the transformed coordinate, m is the total number of the matching points. 2) number of correct matches (ncm): the number of correct correspondences is used as the criterion to evaluate the robustness of the proposed gpfd method. in contrast with the sift algorithm, our method mainly increases two step: training descriptor and testing image pair."
"the rest of this paper is organized as follows. section ii describes the related work. in section iii, the proposed method is detailed. section iv introduces the datasets, the parameter settings, evaluation criterion and computational complexity analysis used in the gpfd process. in section v, experimental results and analysis are presented. section vi concludes this paper."
"as we all know, remote sensing images have sophisticated noise. the image pair 7 is two sar image and is acquired by different sensor with significant intensity difference. the result in table 3 shows that sift, surf, rift, glpm and gp fail to get good result especially surf algorithm. because the descriptors of the common methods are seriously influenced by significant difference of the image intensity. therefore, the descriptors of compared methods may not extract valuable features. and the distance of corresponding feature descriptors will not measure the real relation of two matching points. so these methods can not get enough reliable correspondences to compute the transformation matrix parameters. the proposed method obtains superior performance owing to the robustness of our method."
"motivated by the promising gp method, a novel gpfd method is proposed to extract feature vectors and evolve image descriptors to image registration. the method designs a set of simple arithmetic operators and first-order statistics in order to reduce noise interference. and it is robust and effective which is able to handle complex image transformation."
genetic programming is an evolutionary computation algorithm based on natural selection. it provides a way to find computer programs with the best fitness. the general steps of gp include the following.
"the implementation of all methods including the proposed gpfd method always need suitable parameter. parameters 1 available at: http://www.vision.ee.ethz.ch/datasets/index.en.html are vital important to algorithmic performance such as precision, time complexity. gpfd has a random initialization mechanism, so the proposed method is executed 30 times independently, and the average result of 30 times is taken as the final result. a brief summary of the gpfd parameters are shown in table 1 . it is worth mentioning that these parameters are determined by a large number of experiments, so as to better maintain the accuracy and efficiency."
"we compare the proposed method with the sift [cit], surf [cit], rift [cit], glpm [cit] and gp [cit] algorithms, and to be fair, ransac is used in all contrast methods. sift is a most representative method. sift and sift-based methods like surf have been widely used in image registration. rift is suitable for a variety of multi-modal images owing to its rotation invariance. glpm is a novel mismatch removal method for robust feature matching. and gp is a widely used evolutionary algorithm. experimental results show that the proposed gpfd method can get better performance than compared methods for various image registration task."
"developing feature descriptors has attracted many researchers and received increasing attention over the past few decades [cit] . the commonly used feature descriptors are gray-level co-occurrence matrix (glcm) [cit], histogram of orientated gradients (hog) [cit], local binary patterns (lbp) [cit], scale invariant feature transform (sift) [cit], speeded-up robust features (surf) [cit] . however, most feature descriptors are designed for specific purpose. for example, glcm and lbp are mainly used to volume 8, 2020 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/ texture images, sift and surf are mainly used to keypoints detection. if there is a new task that needs to be addressed, domain experts will be required in tuning parameters or changing policies to get good results. this will cause a lot of inconvenience to our research. genetic programming (gp) is a promising approach which utilises evolutionary computation (ec) principles to automatically evolve a program without human intervention and domain knowledge [cit] . in general, ec imitates darwin's theory of survival of the fittest by natural selection. it randomly generates the initial population as candidate solution, and chooses the individual according to its fitness. the individuals are used to replication, crossover and mutation operation, so as to produce a new population, and continuously cycle to produce better and better approximate solution."
"the overall process is shown in fig. 1-2 . the proposed method is mainly divided into two parts, training and testing. the training process of the overall algorithm is shown in fig. 1 . firstly, using sift [cit] to get tentative training set, and selecting manually some point pairs. then, calculate the first-order statistic of each image block. finally, getting the optimal image descriptor by the gpfd process, and then obtain the maximum feature distance. the test process is similar to the training process in the previous steps. as shown in fig. 2, the test process does not need to select point pairs. after calculating the first-order statistic of the image block, using the descriptor to get the feature vector of each image block. and the point pairs whose feature distance is smaller than the maximum feature distance obtained during the training process consist of the final correspondence set."
"the existing image registration methods are mainly divided into two categories: intensity-based methods and feature-based methods [cit] . intensity-based methods focus on the image's gray information, and align two images by calculating the similarity between pixel intensities of two images. the common measures of similarity are cross correlation (cc) [cit] and mutual information (mi) [cit] . these the associate editor coordinating the review of this manuscript and approving it for publication was kumaradevan punithakumar . methods consider the global information and can obtain accurate results in some cases. but intensity-based methods suffer from monotonous textures [cit] and illumination differences. feature-based methods look for salient features and use the correlation between those features to determine the optimal parameters of the geometric transformation. in general, these features include point features, line features, as well as region features. comparing with intensity-based methods, the feature-based methods are robust to complex geometric deformations and large illumination differences [cit] . therefore, feature-based methods are more suitable for image registration."
"it can be seen that the proposed method achieves significantly better or similar performance on almost all evaluation criteria than other methods. feature descriptor shows good results on test samples. this means that the evolved descriptors are suitable for many types of image pairs. and detailed analysis is described in the following. the image pair 1 has simple translation, so the matching of feature points is relatively easy, but our method can obtain more correct correspondences. and the table 3 and fig. 7 show that all the methods can align image pair 1. but robust terminal set is adopted in our method, and more correct correspondences are obtained. the proposed method gets superior rmse. the number of correct matches, precision and recall rate are worse than gp but better than others. it is because the image pair 1 has less noise and our method have not significant advantage. however, it also fully demonstrates the effectiveness of genetic programming in image registration."
"gp is widely used in many applications because of its hierarchical structural expression [cit] . it is especially effective in solving problems of artificial intelligence, machine learning, control and molecular biology. in the past decades, gp has attracted many researchers to deal with image-related problems, such as keypoints detection, feature extraction, feature selection, classification, object detection, image segmentation, image registration and image processing."
"in order to keep the individual structure simple, the function set in gpfd consists of five functions. four of these functions algorithm (3), then save the best descriptor. 5: by selection, crossover and mutation to get a new population. 6: go to step 2 if the termination criterion is not met. 7: best evolved descriptor. 8: get the feature vector and feature distance of the training sets and the test sets. 9: in the test sets, removing most outliers using the feature distance, and get the correspondence set c final . 10: calculate the model transformation parameters by least square method (lsm)."
"feature selection and feature construction are data preprocessing techniques used to reduce physical memory space and improve the effectiveness of algorithm. gp is introduced to construct feature and select informative features on high-dimensional classification problems [cit] . the final features contain a lot of useful information and maintain the the accuracy of classification in most cases. however, it may exist the issue of overfitting when the data has a skewed distribution with many outliers."
"where d nearest is the distance between a certain feature point in one image and its nearest neighbor in the other image, d second nearest is the distance between a certain feature point in one image and its second nearest neighbor in the other image."
"where x and y are vectors, m is the number of elements, and x i is the ith element of x. the fitness function [cit] evaluates the similarity between image blocks and is defined as follows."
"the image pair 4 and 5 have the transformation of illumination intensity. this will make image data information missing or vague and the mapping relation between these two images complex, which lead to different registration process. the results in the table 3 indicate that gpfd is able to obtain the best rmse, number of correct matches, precision and recall rate. and number of correct matches in our method is almost four times as many as rift. the recall rate of gpfd is obviously higher than other methods. and from the experimental results of two image pairs, gpfd is robust to illumination change."
"the degree of adaptation of individuals in a population depends on the degree to which they approach the real solution. therefore, it is necessary to choose a suitable fitness measure that gpfd mechanism can identify image blocks of the same class. measuring the distance between the feature vectors is the most common approach and is used in the proposed gpfd method. the χ 2 [cit] measures the distance between two normalised feature vectors. the two vectors must be normalised and have the same number of elements. the χ 2 is defined as follows."
"the image pair 3 has simple transformations such as translation and rotation. as shown in the table 3 and fig. 9, the precision of all methods are close to or equal to 1. gpfd obtains the highest rmse, number of correct matches, precision and recall rate. in addition, the terminal set used in the proposed method is not affected by rotation, so the image descriptor evolved by gpfd is rotation-invariant. in a word, gpfd can work very well and get good performance in some simple image pairs."
"to achieve the goal of our method, first a set of putative matches are constructed. then, gpfd is used to remove the volume 8, 2020 false matches contained in the putative set. there are two questions, how to build the set of putative matches and how to generate the test set and the training set. fortunately, there are several well-designed feature descriptors (e.g., sift [cit] ) can efficiently establish the putative correspondences between two images. therefore, sift operator is briefly introduced to constructing the training set and the test set. euclidean distance of sift feature is calculated, then nearest neighbor distance ratio (nndr) strategy is used to build correspondences. for each feature point in one image, the method searches the two nearest neighbors in the other image and compares their distances, which is defined as,"
"in recent years, gp has been applied to feature extraction, feature construction, region segmentation, image classification, object detection and image registration [cit] . gp can input the gray value, statistical information and neighborhood information of the image to the terminal of the program and optimize it according to different purposes of image processing. and gp has achieved success in image registration, though many existing methods are faced with the challenge of low accuracy and recall rate, especially when the image is complex, such as the presence of various noises, different degrees of rotation, illumination different, distortion deformation, etc., most of the methods can not get good matching performance."
"the main contributions of this paper are as follows: (1) a method of learning feature descriptors is proposed, which is optimized by gp to achieve excellent matching performance. robust feature descriptors are obtained by using improved first-order statistics (25th percentile, 75th percentile, mid and stdev), the noise problem caused by different sensors in remote sensing image is improved."
"1) initialising the population: generate an initial population consisting of the function set and the terminal set of the problem. the currently commonly used methods for generating random initial population have the full method, the growth method and the half-and-half method. 2) fitness evaluation: each individual of initial population is given a certain fitness according to the ability to solve problems. 3) selection,crossover and mutation: by performing the following operator operations, a new population is generated."
"where w d and b d are the within-class and between-class respectively. the aim is to seek out a tradeoff between w d and b d . that is to say, minimising the distance between two matched image blocks and maximising the distance except two matched image blocks."
"this paper proposes a novel gpfd method which can automatically evolve feature descriptors. unlike other feature descriptors, the proposed method does not need human intervention to tune parameters. in the gpfd method, a set of simple arithmetic operators and improved first-order statistics are designed as the function set and the terminal set to evolve feature descriptors for complex image registration. our algorithm includes three parts: 1) using sift to construct the training set and the test set; 2) utilizing gpfd to generate feature descriptors to remove a mass of false correspondences; and 3) removal of imprecise points to improve accurate by ransac algorithm. experiments on different image datasets confirm that gpfd performs well for image with highly distortion and noise. compared with other representative methods, gpfd is able to receive better performance including rmse, the number of correct matches, precision and recall rate. in conclusion, the proposed gpfd is a robust and accurate registration method."
"the proposed method is described in this section. to make this paper comprehensive, and this section provide essential details, the overall algorithm is introduced first, then the function set and the terminal set are illustrated, the fitness function is finally described."
"in this paper, gpfd has presented promising results in difficult image pairs. in the future, we will focus on studying a new approach to construct the training set and the test set to reduce the running time."
"the number of correct correspondences and accuracy are important criteria to evaluate the effectiveness of the proposed method. for our experiment results, the evaluation carries out using the following criteria."
"in fig. 6, the red line is the mid, the black + is the mean, the top and bottom of box are the 25th percentile and 75th percentile, respectively, the top and bottom of figure are the max and min, respectively. it is evident that the max and min fluctuate greatly after adding the noise of normal distribution. while the 25th percentile and 75th percentile are almost impervious to noise. the experiment explains that 25th percentile, 75th percentile, mid are very representative and robust first-order statistics, and can be used to evolve robust image descriptors."
"in this paper, a novel method, which can extract feature and evolve image descriptors, is proposed. the proposed method is called genetic programming of feature distance (gpfd). gpfd can automatically construct an image descriptor. the descriptor can detect a set of keypoints (such as angles and edges) and extract informative features from these keypoints. a simple set of arithmetic operators and first-order statistics (such as mid-value and standard deviation) are automatically combined into a set of formula representing image descriptors. what is more important, this method does not require human intervention to design keypoints and features, and it uses image pairs with various transformations to evolve image descriptors. to evaluate the performance of the proposed gpfd method, seven image registration datasets with varying degrees of difficulty will be used. the proposed method is compared to five methods including sift [cit], surf [cit], rift [cit], glpm [cit] and gp [cit] ."
"the image pair 2 suffers from rigid transformation and noise. so there are many wrong matches or similar pixels around the keypoints. and one to more matches are found because two keypoints are close enough in position or they have similar descriptors. this causes poor matching performance. by adopting robust first-order statistic, all the evaluation criterion of the proposed method are better than other algorithm in addition to the running time. and the experiment results instruction that our method is robust to noise."
"(2) the proposed method is offline learning, which does not require the addition of new information in the optimization process. and take the feature distance of the training set as the threshold, the initial error point pairs (outliers) are removed, thus reducing the human intervention."
"the image pair 8 is a sar image and a optical image from google earth with different imaging principle. gpfd gets the most optimal rmse, number of correct matches, precision and recall rate. precision by the proposed method is close to 1 and recall rate is evidently better than contrast methods. however, gp fails to achieve good performance because it is not robust to noise. and the presence of noise affects the matching result of image descriptor, and makes incorrect correspondences may be obtained. gpfd employs terminal set with rich statistical information, so image descriptors evolved by our method are more robust to noise and reliable."
"the gpfd is compared to five methods including sift [cit], surf [cit], rift [cit], glpm [cit] and gp [cit] . it is worth mentioning that since the gpfd uses ransac to eliminate a small number of mismatches, and to be fair, ransac is used in all contrast methods. gpfd has a random initialization mechanism, so the proposed method is executed 30 times independently, and the average result of 30 times is taken as the final result. the root-mean-square error (rmse), the number of correct matches (ncm), precision and recall rate are used to evaluate the performance of the proposed method."
"in some cases, the existence of incorrect feature matches will introduce significant errors or even failures in the sfm process. the mis-associated features should be adaptively handled to enhance to the accuracy of the sfm outcomes. to address this issue, the authors have developed an adaptive resection-intersection bundle adjustment approach that refines the 3d points and camera poses separately [cit] ."
"in this technique, collecting high-quality aerial images is crucial for accomplishing successful orthophoto generation and roi localization. the approach developed does not involve any manual process to choose favorable images among the raw images collected with the uavs. it also does not require any configuration of the parameters in the middle of the process. thus, the quality of the original images directly affects the accurate extraction and localization of the rois for visual inspection. here, we suggest some important guidelines for the best use of the technique."
"protrack is a company developing and selling products for video surveillance. a major application developed in the company is a far-field outdoor intrusion detection system based on a real-time video motion detection technology for a scanning camera. the system alerts for any moving object in the scanned area. the algorithm is robust to camera noise and can filter back and forth motion such as trees swaying in the wind. however, the main remaining gap concerns the ability to distinguish between significant and insignificant motion (e.g., alerting any human motion while filtering out animal motion) and to detect humans in an area where cars are usually moving and vice versa. for this reason, automatic classification of (detected) moving objects is needed. the main requirement is to classify according to the following four categories: \"walking man\", \"crawling human\", \"animal\", and \"car\"."
"the following weights were evolved for the 19 parameters: [cit] . note that some of the weights are set to very low values (0, 1, 2), while others have higher values. testing this set of weights in a weighted nearest neighbor framework yields on average 90% classification accuracy, i.e., a considerable improvement over standard, non-evolutionary methods."
"for our learning purposes we define the chromosome as a list of weights. for example, having 19 weights (for the 19 parameters) and allocating 6 bits per weight (so that the range of weights is 0 to 63), the chromosome consists of 114 bits. finally, the fitness function is defined as follows. during the learning phase, each organism (a set of weights) classifies (due to its weighted distance calculation) a set of test samples for which we know the expected correct classification. the higher the classification accuracy of the organism, the higher the fitness value will be. at the end of the learning process, we select the best set of weights."
"we first applied standard classification based on gaussian mixture model [cit], assuming that the features are distributed according to a multivariate gaussian distribution for every object class. given the training set, the mean vector mi and the covariance matrix covi are estimated for each object category ci. given the set fi of all the feature vectors belonging to class ci, the elements of the mean vector mi and the covariance matrix covi are estimated by:"
"first, a projection matrix, p, is computed for each image collected using the sfm technique [cit] . sfm automatically computes the 3d point cloud and the geometric relationship between the 2d image points and the 3d points in the world (scene). the sfm process first extracts features from each collected image and matches common features among the images. then, based on the matched features among the images, the relative positions and angles of each image are estimated, and represented as a simple projection matrix. all these results are generated solely from the set of images collected and no manual configuration is required. only good quality images having enough overlap with the other images are automatically selected and utilized in the sfm process. the sfm process is shown in figure 2a, where the geometric relationship is represented with the projection matrix, denoted as p i and the subscript indicates an image index. with this matrix, any 3d point in the world can be mapped to its corresponding 2d point in each image. this relationship is represented as"
"any x π on the plane π has a constant value of c π and will satisfy equation (4). notice that π now can be expressed in 2d form, which means c π is not correlated with a π or b π and thus, c π yields:"
"step 2 is to generate an orthophoto of the tbf from the collected images. the sfm technique is used to estimate the geometric relationship between each image and the tbf. by extracting and matching the visual features, this process conducts calibration of the camera parameters for each image including a projection matrix and radial distortion coefficient(s) as well as generating a 3d point cloud of the scene [cit] . the surface of the tbf can be automatically detected by fitting a plane to the 3d point cloud. then, the orthophoto is constructed by projecting each image onto the detected plane, followed by stitching and blending them. lastly, in step 3, the rois corresponding to the tris that the engineers select for inspection are extracted from the original images. the geometric relationship with each of the selected tris on the orthophoto and the original images is used to localize the corresponding rois. since the localized rois are a set of image patches cropped from those images, they contain detailed visual information of the tris. thus, the extracted rois enable robust vision-based visual assessment of the facades. both the orthophoto generation and roi extraction are developed to be fully automated without the need for manual manipulation. the only manual step is associated with the tri selection on the orthophoto in step 3. step 1 is to fly the uavs over the tbf to collect a large number of high-resolution images. to increase the likelihood of detecting damage, these images are captured from many different viewpoints and positions [19, [cit] . the entire region of the building façade that is needed for inspection should be covered by the images."
"the object classification algorithms presented below have a classic structure. a feature vector, which is a compact representation of the essential information about a moving object, is extracted for each moving object. a learning dataset containing samples of each motion category is then built and used in the learning stage to train the system and compute a classification rule based on the above feature vectors. for each detected object, the following feature values are calculated: velocity, acceleration, acceleration rate, shape ratio, velocity to height ratio, object size, average velocity velocity projection, average velocity along the x-axis, average velocity along the y-axis, and nine moment values (with respect to velocity variances), i.e., a total of 19 feature values. given a 19-feature set of values, the classifier will issue one of the four categories: \"walking human\", \"crawling human\", \"animal\", or \"car\"."
"third, motion blur should be avoided. motion blur is common in aerial image collection. it occurs when the object being recorded moves relative to the camera during the period of exposure. large relative movement can produce a lack of sharpness or artifacts (e.g., ghosting) on the images collected. to avoid this problem, translation, and angular movements of the camera (with respect to the scene) should be minimized while the shutter is open. multiple factors may affect motion blur including environmental conditions (e.g., low light or high wind), uav platforms (e.g., fast flying speed or non-isolated platform vibration), and camera parameters (e.g., long focal length or low shutter speed). to prevent taking blurry images, we recommend (1) flying uavs under good weather conditions (e.g., enough daylight and no wind) and with a slow speed, (2) isolating the vibration of the camera with respect to the uav platform using a multi-axis gimbal or vibration damper (e.g., rubber pad), (3) decreasing the exposure time without increasing the camera's iso because a higher iso produces higher light sensitivity but also more noise, and (4) zooming out the camera and maintaining a short focal length so that the relative scene change due to sudden angular vibration is minimized."
"step 2 is to generate an orthophoto of the tbf from the collected images. the sfm technique is used to estimate the geometric relationship between each image and the tbf. by extracting and matching the visual features, this process conducts calibration of the camera parameters for each image including a projection matrix and radial distortion coefficient(s) as well as generating a 3d point cloud of the scene [cit] . the surface of the tbf can be automatically detected by fitting a plane to the 3d point cloud. then, the orthophoto is constructed by projecting each image onto the detected plane, followed by stitching and blending them. lastly, in step 3, the rois corresponding to the tris that the engineers select for inspection are extracted from the original images. the geometric relationship with each of the selected tris on the orthophoto and the original images is used to localize the corresponding rois. since the localized rois are a set of image patches cropped from those images, they contain detailed visual information of the tris. thus, the extracted rois enable robust vision-based visual assessment of the facades. both the orthophoto generation and roi extraction are developed to be fully automated without the need for manual manipulation. the only manual step is associated with the tri selection on the orthophoto in step 3."
"this approach yielded on average 63% classification accuracy (i.e., given a object, on average 63% of the time it is classified correctly by the above method). a possible explanation for this low classification rate is that the gaussian model does not adequately capture the underlying feature distribution."
"the major advantage of the technique is that it can rapidly provide the orthophoto and localized rois from a raw collection of images for robust visual inspection. the orthophoto is useful for making annotation and documentation of damage locations because it provides an entire view of the building façade. this can streamline the current time-consuming process of manual documentation by human inspectors. also, applying an existing damage detection algorithm to the rois may fully automate the inspection process. such use of highly relevant rois greatly reduces false-positive and false-negative damage detection by processing many images captured from various viewpoints for viewing [19, [cit] ."
"to implement this new procedure, we have developed a technique to support rapid visual inspection of planar building façades using images collected by uavs. an orthophoto of each building façade is automatically constructed using a structure-from-motion (sfm) technique followed by image stitching and blending. the resulting orthophoto contains an entire view of the building façade so that inspectors can easily recognize and select target regions for inspection (tris). this orthophoto is geometrically connected to each of the original images. thus, once the engineers select the tris on the orthophoto, all image regions containing the corresponding tris, or the rois, are automatically identified and extracted from the original images. this approach will directly support field engineers in conducting rapid inspection. to demonstrate the capabilities of the technique developed, we collect images from a damaged façade of an actual building using uavs. we successfully generate the high-resolution orthophoto of the façade and extract the rois corresponding to the selected tris including a damaged window pane and external drainage pipe."
"* in this paper we briefly describe how protrack's module detects objects and extracts their feature vectors. we then discuss how these feature vectors can be used for standard object classification, as well as classification due to ga-based learning. our results demonstrate a significant improvement due to the latter."
"regardless of the degree of urgency, manual visual observation by human engineers is still the primary method for façade inspection [cit] . currently, several human inspectors are needed to physically visit every floor of each building. then, those inspectors evaluate the condition (e.g., crack or dislocation) of each component on the façade and annotate them on the corresponding engineering drawing (or layout), producing a damage map for each façade. in the worst cases, special equipment may be required to access the building from the outside to inspect the condition of the building's exterior, such as ladders or ropes for controlled decent. such manual process would become extremely tedious and inefficient at the site. however, unfortunately, there is not a viable rapid technique to streamline this important process."
the paths computed by the micro-trackers contain valuable information about the motion pattern of different parts of the detected moving object. this information can be used to distinguish between rigid objects (like cars) and non-rigid objects (like walking humans).
"in this study, we develop a vision-based approach for computer-aided rapid inspection for the tbfs. the technique developed here will automatically generate an orthophoto of the tbf using images collected from uavs. first, uavs collect a large volume of aerial images from the tbf by following the image collection guidelines developed in this study (see section 2.1). then, human inspectors select any region on the orthophoto where inspection is required and a set of rois corresponding to the region are localized from the high-resolution original images. since the localized rois contain various viewpoints of the region, human inspectors can perform a complete and robust inspection. the feasibility of the method is demonstrated using an abandoned building having several damaged components on its tbf. rois corresponding to two tris having a damaged window pane and drainage pipe are extracted by processing a set of original images collected using a commercial uav."
"as a result, the orthophoto of the tbf in figure 2e is successfully generated by automatically processing the raw aerial images. for the orthophoto, a total of 375 images (among 1254 images) are automatically selected as the set of images considered to be relatively parallel to the façade when the angle threshold is set to 20°. figure 7 shows the constructed orthophoto of the tbf and a selection of sample tris. in this study, we choose two tris using rectangular boxes, denoted as tri 1 and tri 2. tri 1 and tri 2 include a broken window pane with a hairline crack and a cracked external drainage pipe, respectively. although only two tris are chosen for this experimental validation, multiple regions with any sizes and locations may be selected with the technique developed. note that the orthophoto is only used for assigning the tris. we recommend that further documentation and the actual inspection is conducted using the rois extracted from the original high-resolution images. five samples of localized rois corresponding to each tri are presented in figure 8 . the number of extracted rois corresponding to the two tris is 22 and 27 (from a total of 1254 images), respectively. all rois have different resolutions and aspect ratios depending on the depth between the images and tbf and viewing angles. however, in figure 8, they are transformed into a square for this arrangement. these rois do satisfy the two conditions introduced in section 2.3. since the rois include details and multiple viewpoints of the tris, reliable vision-based visual inspection can be achieved. for instance, in figure 8a, the white vertical crack propagated from the broken region is only visible in the specific rois captured from certain angles. similarly, in figure 8b, the break in the drainage pipe is only identified when the corresponding region is not impeded by the branch. the images that are captured with an angle are helpful in identifying such damage. these two examples clearly illustrate the need for collecting a sufficient number of images from different viewpoints and localizing the high-resolution rois from the original images for conducting robust visual evaluation."
"an overview of the technique developed here is presented in figure 1 . the objective of the technique is to aid field engineers to perform rapid visual inspection of the target building façades (tbf) through a sequence of images collected using uavs. the technique can be used to inspect any target components (e.g., broken window panes, spalling concrete, etc.) located on a relatively flat surface of the tbf. building façade. this can streamline the current time-consuming process of manual documentation by human inspectors. also, applying an existing damage detection algorithm to the rois may fully automate the inspection process. such use of highly relevant rois greatly reduces false-positive and false-negative damage detection by processing many images captured from various viewpoints for viewing [19, [cit] . the remainder of this paper is organized as follows. section 2 begins with an overview of the technique and introduces the details of the technical steps, regarding image acquisition, orthophoto generation, and roi localization. in section 3, a case study is presented to demonstrate the capability of the technique using images collected from a full-scale building which contains damaged components on its facades. section 4 contains the discussion and conclusions."
"fourth, in figure 2d, a set of the images is projected onto the facade plane using the homography matrices. in section 2.1, although a large volume of images is collected from various viewpoints, rather than utilizing all the images it is best to only use a subset of suitable images for constructing the orthophoto. the backgrounds of the angled images do not have sufficient and regular resolution of the building regions and they may also include non-façade regions. therefore, images that are relatively parallel to the tbf are selected to generate the orthophoto provided that they cover the entire area of the tbf. for the experimental validation in this study, we set a threshold for the angle between the normal vectors of the façade plane and the image planes at 20°. note that although we use a set of mainly parallel images for orthophoto generation, the rois will be extracted from all images if they satisfy the constraints in section 2.3."
in this paper we presented a genetic algorithms based approach for automatically learning the feature weights for nearest neighbor classification. the results show a significant improvement over standard non-evolutionary classification methods.
"recent advances in image sensors and sensing platforms have been achieved, enabling automated or semi-automated vision-based visual inspection [cit] . incorporating vision sensors onto aerial sensing platforms will alleviate the spatial and temporal limitations that are typically associated with manual inspection in the case of large-scale buildings [cit] . for example, unmanned aerial vehicles (uavs) can collect a large volume of high-resolution images to cover the entire area of buildings in an efficient manner. with automated methods to use and process this data, computer-aided inspection can overcome time-consuming and risky human-based inspection for the building façades."
"as seen in figure 8b, the presence of unwanted foreground objects (e.g., branch, tree, street light) may obstruct the view of the tris in the rois. in such a case, the only possible solution is to collect images from additional viewpoints. a similar issue occurs when the geometry of the structure is complex. alternatively, one may further apply an image classification technique to filter out unnecessary rois and utilize only useful rois [21, 23, [cit] ."
"given two points in space, how should the above distance be computed? using the straightforward euclidean distance, without attaching different weights to the various features, might not be representative, as it is obvious that not all of the 19 features are equally important in deciding the classification of the object. instead, a weighted distance [cit] for feature vectors x and y is computed by"
"the homography matrix between the orthophoto and each raw image can be computed using equation (6) . with this matrix, the rois, which are high-resolution image patches, corresponding to any region on the orthophoto can be extracted from the raw images. in this step, engineers are asked to define a tri on the orthophoto, as illustrated in figure 3 . the user simply draws a polygon on the orthophoto that fully encompasses the tri. any region and shape of the polygon may be selected to define the tri. for example, in our experimental validation we simply draw a rectangle by dragging the mouse to select damaged components on the tbf. a set of 2d points (x i ) in each image corresponding to the vertices of the selected polygon (a π and b π ) on the orthophoto can be computed from equation (6) . the portions of each image within those points becomes the roi. the homography matrix i h maps the points on the plane  to those on the image i."
"first, the images must cover the entire region of the tbf. because the images are collected with and stored in uavs, there is no way for engineers to check if all images thoroughly cover the tbf with sufficient quality. thus, a well-established flight plan is prepared in advance depending on the shape and size of the tbf so that they can readily collect quality images on site. in general, engineers draw a virtual grid of the flight path on the entire area of the tbf and images are collected at a regular interval by following this grid. a depth (distance between the uav and the tbf) is determined based on the minimum resolution required in the images for effective visual inspection. the smaller the field of view (coverage) becomes, the higher the spatial resolution of the scenes containing the tbf, although more images would need to be captured to cover the entire tbf. the spacing (interval) of the images collected along the flight path is another important parameter and is directly related to the next guideline."
"the remainder of this paper is organized as follows. section 2 begins with an overview of the technique and introduces the details of the technical steps, regarding image acquisition, orthophoto generation, and roi localization. in section 3, a case study is presented to demonstrate the capability of the technique using images collected from a full-scale building which contains damaged components on its facades. section 4 contains the discussion and conclusions."
"manually setting the 19 weights in our case required an infeasible amount of trial and error. having reduced, however, the problem to that of parameter optimization, the values can be automatically optimized using genetic algorithms."
"in this study, we envision a new inspection procedure for the building façade as follows: after an event, inspectors fly uavs manually or autonomously using gps from a place that is safe from falling hazards. the uavs capture a large volume of images of the façades of the building. once the images are collected, we automatically construct a high-resolution orthophoto using those images. this orthophoto allows the inspectors to readily view the entire building façade. then, they select any region on the orthophoto that is suspicious or vulnerable to damage. the high-resolution image patches corresponding to the selected region, named regions-of-interest (rois), are extracted from the original images, enabling quality visual assessment. with this approach, the inspector can evaluate several buildings in just a short time, saving time that would be spent physically visiting each building. moreover, it is a much safer way to conduct an inspection of multiple damaged buildings without actually accessing them."
"we also applied is nearest neighbor (nn) classification [cit] . in this approach, each dataset sample contains d parameters (19 in our case) and a class label (for training). note that each sample can be represented as a point in d-dimensional space. given a new unclassified sample, it will be assigned the category of that training sample whose \"distance\" to the unclassified sample is the smallest. (that is, the unclassified sample is assigned the label of its nearest neighbor sample in the training set.)"
"an overview of the technique developed here is presented in figure 1 . the objective of the technique is to aid field engineers to perform rapid visual inspection of the target building façades (tbf) through a sequence of images collected using uavs. the technique can be used to inspect any target components (e.g., broken window panes, spalling concrete, etc.) located on a relatively flat surface of the tbf. step 1 is to fly the uavs over the tbf to collect a large number of high-resolution images. to increase the likelihood of detecting damage, these images are captured from many different viewpoints and positions [19, [cit] . the entire region of the building façade that is needed for inspection should be covered by the images."
"second, the image collection interval is carefully designed to ensure there is sufficient overlap between adjacent images. increasing the number of feature matches across multiple images is crucial for computing accurate geometric relationships between the images and the scene using the sfm technique. as a rule of thumb, more than 60% overlap with the adjacent images is recommended. however, this value varies depending on the image quality (e.g., resolution or signal-to-noise) as well as scene characteristics (e.g., unique texture) [cit] . to obtain sufficient and constant overlap, we suggest that images be captured using a regular time interval (e.g., continuous shoot mode in regular cameras) under a constant flight speed."
"numerous approaches have been proposed for object classification. some methods draw on shape information for object detection in still images [cit] . however, these methods may require large datasets for learning and considerable cpu power. also, false positives may pose a problem since the detection is performed on each video frame. other object classification methods [cit] in far-field video that are based on scene modeling assume a static camera, i.e., they are not applicable to a scanning camera."
"during tornado or hurricane events, nonstructural components on the façades or roofs of buildings are highly vulnerable to strong winds or airborne debris. these often cause serious damage to the building components, leading to disruption in the functions of the building and, potentially, jeopardizing the safety of its occupants. moreover, damage on cladding (e.g., spalling or crack), their anchorage to the walls, or window panes (e.g., crack or dislocation) may induce dangerous falling hazards to pedestrians on the sidewalk below, followed by restrictions on the use of the adjacent roads. thus, inspection of building façades is one very important task conducted during disaster recovery and must be conducted in a rapid manner [cit] ."
"an orthophoto is a planar image created by arranging and stitching the set of collected images after removing perspective and radial distortions [cit] . since the resulting orthophoto has a uniform scale in each direction, it will show a true aspect ratio of the target regions on the plane (a single façade surface, in this case). herein, we describe the process needed to construct a high-quality orthophoto from the aerial images collected and to identify the geometric relationship between each image and the orthophoto. with the orthophoto available, engineers can readily view specific areas of the tbf to select the tris for visual inspection."
"in this study, we develop a vision-based approach for computer-aided rapid inspection for the tbfs. the technique developed here will automatically generate an orthophoto of the tbf using images collected from uavs. first, uavs collect a large volume of aerial images from the tbf by following the image collection guidelines developed in this study (see section 2.1). then, human inspectors select any region on the orthophoto where inspection is required and a set of rois corresponding to the region are localized from the high-resolution original images. since the localized rois contain various viewpoints of the region, human inspectors can perform a complete and robust inspection. the feasibility of the method is demonstrated using an abandoned building having several damaged components on its tbf. rois corresponding to two tris having a damaged window pane and drainage pipe are extracted by processing a set of original images collected using a commercial uav. five samples of localized rois corresponding to each tri are presented in figure 8 . the number of extracted rois corresponding to the two tris is 22 and 27 (from a total of 1254 images), respectively. all rois have different resolutions and aspect ratios depending on the depth between the images and tbf and viewing angles. however, in figure 8, they are transformed into a square for this arrangement. these rois do satisfy the two conditions introduced in section 2.3. since the rois include details and multiple viewpoints of the tris, reliable vision-based visual inspection can be achieved. for instance, in figure 8a, the white vertical crack propagated from the broken region is only visible in the specific rois captured from certain angles. similarly, in figure 8b, the break in the drainage pipe is only identified when the corresponding region is not impeded by the branch. the images that are captured with an angle are helpful in identifying such damage. these two examples clearly illustrate the need for collecting a sufficient number of images from different viewpoints and localizing the high-resolution rois from the original images for conducting robust visual evaluation."
"the training dataset contains 100 samples of each of the four object categories. 75% of samples from each category were randomly selected for training and the remaining 25% for testing (i.e., total of 300 training samples, and 100 validation samples). this process was conducted several times, randomly selecting each time the training and testing sets."
"finally, from the visual inspection standpoint, images should be collected from a variety of viewpoints [cit] . facilitating the observation of the tri from various angles through the rois is a key benefit of the technique developed. to collect images that contain many viewpoints, engineers should fly uavs following the designed flight path multiple times while using different camera angles each time. alternatively, one can use a programmable gimbal so that the angle of the camera is cyclically changed during one flight. such angled images are also valuable because they serve to improve the performance of the sfm technique [cit] . since angled images contain more of the background scene, they provide more overlap with the other images, producing more accurate parameter estimation using the sfm technique, as mentioned in the second point above."
"in our experiment, suitable uav flight paths are designed based on the guidelines established in section 2.1. first, images are collected by flying the uav along a grid pattern. during this process the uav must first maintain a set altitude (vertical direction) and flies from one side to the other, and then repeats this process after changing its altitude. image acquisition along this pre-designed flying path is assured to have sufficient and consistent overlap between the images in both the horizontal and vertical directions and to also capture the entire test façade. second, the uav must also maintain a constant and low flying speed to avoid abrupt transitions. rapid transitions between the images will produce insufficient overlap between images and also makes the images blurry. the flying speed is determined based on the distance between the uav and the test façade, the camera's field of view (fov), and the image collection rate. in this experiment, the distance is set to roughly 4-5 m to capture the detailed appearance of the test façade. the fov and image capture rate of the camera are around 90° and one frame per second (1 frame/s), respectively. accordingly, the flying speed is determined as 0.5-1 m/s to produce more than 60% overlap between images. third, angled images must be collected from four different viewpoints. sample angled images and parallel images are provided in figure 6 . in this experiment, we fly the uav four times after changing the angles of the camera (parallel, right-and left-angled, and downward)."
"however, a major technical challenge in using those images for façade inspection is considering the trade-offs between the need for collecting close-up images and their localization. if the images are collected at a close distance to the building façades, it is difficult to know where the corresponding images were captured from. manual searching is laborious when the building façades are large or have repetitive patterns. although gps can be measured and recorded on the images, it does not provide sufficient accuracy regarding the camera pose and location (a typical error range is 5-15 m), and the signal is often interrupted by the roof and/or wall of the buildings [cit] . on the other hand, if the images are taken far from the building facade, vision-based inspection may not be feasible due to a lack of details. for instance, to capture crack damage, it is necessary to collect images at a close distance and under different viewpoints and positions for accurate inspection. these requirements are because the visibility of the crack depends on the viewing angles. since uavs do not selectively capture favorable images in an automated manner, a large volume of images needs to be collected and used for visual inspection [19, [cit] . thus, to enable efficient visual inspection using such a large volume of images, an automated technique should be incorporated to localize the close-view images captured from different viewpoints to the corresponding region on the building facades."
"the separation of each chromosome from the metaphase image is the major operation carried out in this stage. basic steps involved are: removal of cells from the dapi images, gradient computation and minima selection, computation of watershed transformation and binary mask creation. these are briefly presented in the following subsections:"
"karyotype is the tabular representation of human chromosomes in a cell. in this representation, the chromosomes are ordered by length from largest (chromosome 1) to smallest (chromosome 22 in humans), followed by sex chromosomes. karyotypes are very useful for accurately diagnosing the genetic factors behind various diseases. manual karyotyping is time-consuming, expensive and need well trained personnel. during the early period of chromosome analysis, researchers used grayscale images and features such as size, shape, centromere position and banding pattern for classification."
"to assess how other existing denoising algorithms affect our measurement of broadband power, and how they interact with our new denoising algorithm, we ran two different denoising algorithms, either alone or in combination with noisepool-pca. the two algorithms we tested were calm, or continuously adjusted least-square method [cit] and tspca, or timeshift principal component analysis [cit] . both of these make use of reference meg sensors which face away from the head and measure environmental rather than physiological fields. by design, these algorithms project out time series from the subspace spanned by the reference sensors, thereby reducing environmental noise, but not physiological noise. applying either one of these two algorithms alone to the 8 data sets reported above increased the broadband snr, evident in the group-averaged sensor plots (fig 12a, columns 3-4 versus column 2), and the increased snr in the 10 most responsive sensors (fig 12b, 2 nd and 3 rd bar versus 1 st bar in each plot)."
"it is important to consider how these effects interact. because the reduction in variability across epochs was the biggest effect of denoising (more than 2-fold), there was more than a doubling of snr, computed as the mean across epochs divided by the variability of the difference distribution. in sum, the spectral plots show that the variability in power across frequencies was little affected by denoising (fig 6a), whereas the distribution plots show that the variability in total broadband power across epochs was reduced considerably (fig 6b) ."
"gradient magnitude of the dapi channel image after cell removal is computed. sobel operators are used. since watershed algorithm produces over segmentation, can be control by reduce the number of allowable minima in the gray scale."
"the above analyses were conducted with a few specific choices made with respect to how we denoised: the number of pcs projected out (10 pcs), the size of the noise pool (75 sensors), and the number of 1-s epochs denoised at a time (one at a time). in separate analyses, we swept out a wide range of these parameters (s1 and s2 figs). these results showed that the most snr was gained for (1)"
"in contrast to intracranial recordings, in the extracranial measures of electroencephalography (eeg) and magnetoencephalography (meg), broadband responses have not been widely and reliably observed, particularly in the responses of individual sensors. one significant challenge in identifying broadband in extracranial measures is that non-neural noise sources, particularly from miniature saccades, can be confounded with experimental designs, making neurally induced broadband responses hard to isolate [cit] ."
"the 4 data sets acquired with an elekta neuromag at cinet were preprocessed in matlab (mathworks, ma, usa) using the identical code and procedure. the cinet data were acquired as 102 pairs of planar gradiometer signals (204 sensors). data were analyzed from each of the 204 gradiometers separately and paired into 102 locations for mesh visualization (e.g., the broadband signal-to-noise ratio for sensor 121 and 122 out of 204 would be averaged to show one signal-to-noise ratio in the position of sensor 61 out of 102)."
"the spatial pattern of broadband signals was qualitatively similar to the spatial pattern of the stimulus-locked signal, with bilateral posterior responses in the both-hemifield condition, and lateralized responses in the single-hemifield conditions (fig 5, individual example and group-averaged data) . however, the broadband responses had much lower signal-to-noise than the stimulus-locked responses, and in many of the individual subjects, broadband was not evident in one or more conditions; for example, in the both-hemifield condition, there was a clear medial posterior broadband response for s1, s2, and s3, with signal at least 5x above noise, but not for s4 (panel b in s5 fig) . the broadband responses were less reliable for the left-and right-hemifield conditions than for the both-hemifield conditions."
"the meg data were denoised using a new algorithm as described in detail in the material and methods section. in brief, for each subject a subset of sensors that contained little to no stimulus-locked responses were defined as the noise pool. once the noise pool was defined, the time series in each sensor and in each epoch was filtered to remove all signals not contributing to the broadband measurement. global noise regressors were then derived by principal component analysis from the filtered time series in the noise pool in each 1-s epoch. the first 10 pcs were projected out of the data in each sensor, epoch by epoch. the remainder of the analysis was identical to that used in the non-denoised data set (fig 2) ."
"the present work employs basically two major processing steps, segmentation and classification. marker controlled watershed transform is used for segmentation and region based bayes classification is used for classification."
"the columns represent snr values for the stimulus-locked signal (column 1), broadband signal without denoising (column 2), and broadband signal with one or more denoising algorithms. one scale bar is used for all stimulus-locked plots (column 1). a second scale bar is used for all broadband plots (columns 2-7) except for the left minus right plots (row 4, columns 2-7). other details as in fig 5. (b) broadband snr using different algorithms for both-hemifield (left), left-hemifield (center) and right-hemifield (right) stimuli. each boxplot is the change in snr from baseline (column 2 in panel a), averaged across the top 10 sensors per subject). top sensors were defined as the 10 sensors from each subject with the highest snr across any of the 3 stimulus conditions and any of the denoising algorithms (columns 2-7). the boxplots show the median (black horizontal line), quartiles (boxes), and total range (error bars) across subjects. three stimulus conditions, we then took the average snr from these 10 sensors without denoising or after applying noisepool-pca or another denoising algorithm. finally, we computed a two-tailed p-value by a paired t-test, with the pairing between analysis methods."
"the absorbed light is converted to heat, radiated in the form of fluorescence and/or consumed in photobiochemical reactions. the time-dependent heat production in brain tissue can be described by the bio-heat equation [cit], in which changes in tissue temperature can be calculated in time and space. these equations can also account for the buffering of temperature by blood perfusion. furthermore, laser radiation increases stored energy that results in the diffusion of heat away from the irradiated area in proportion to the temperature gradients generated within the tissue [cit] . therefore, the conclusion drawn from optogenetic experiments may be hindered if the direct heat effect of light stimulation is not accounted for."
"assuming that a laser beam in the z direction attenuates exponentially with the distance d in the tissue [cit], the irradiance can be defined as the radiant energy flux incident on the point of the surface, divided by the area of the surface. many laser sources emit beams that approximate a gaussian profile, in which case the propagation mode of the beam is the fundamental transverse electromagnetic mode (tem 00 ) [cit] . gaussian functions can assume multidimensional forms by composing the exponential function with a concave quadratic function [cit] . a particular example of a twodimensional gaussian function, in the x − y plane, is:"
"where, ρ a represents the numeric density (m −3 ) of the absorbers. similar equations are found in the literature to explain the scattering phenomenon [cit] ."
watershed transform of the resulting image is computed which results in tessellation of the image in to different regions. the watershed transform has the advantage that the lines produced are always form closed and connected regions and these lines always correspond to obvious contours of objects which appear in image.
"an important difference between fmri and meg is that with fmri, one is likely to obtain measurements from many parts of the brain which contain no stimulus-related response. in contrast, meg measures sources which propagate broadly across the head due to volume conduction, and thus there is the possibility that it will not be possible to identify a pure noise pool, i.e. a set of sensors that carry no stimulus-related responses. in this case, there is the data from subject s1 (left) and averaged across subjects s1-s8 by sensor (right). the top 3 rows show data from the 3 stimulus conditions (both-, left-, and right-hemifield) compared to blank, and the lower row shows data as the left-only minus right-only conditions. the dependent variable plotted for the single subject data is the signal-to-noise ratio at each sensor, computed as the mean of the contrast (stimulus minus blank) divided by the standard deviation across bootstraps (bootstrapped over epochs). for the group data, the signal-to-noise ratio is the mean of the subject-specific snrs at each given sensor. the same scale bar is used for all stimulus-locked plots. for the broadband plots, one scale bar is used for the first three rows, and a different scale bar with a smaller range is used for the fourth row. made with nppmakefigure5.m."
"noise pool selection. the noise pool was defined as the 75 (nyu) or 100 (cinet) sensors with the lowest stimulus-locked snr across conditions. the snr was computed by (a) dividing the mean response from the data by the standard deviation across bootstraps for each condition, and (b) taking the maximum of the three values (corresponding to the three stimulus conditions) for each sensor."
"several groups have reported high frequency signals measured extracranially [cit] . however, because of the low snr, the possible confound with eye movements and other artifacts, and the confusability of broadband signals with high-frequency narrowband oscillations, broadband signals are not typically reported in extracranial studies. we return to the relationship between our new measures and prior reports of high frequency extracranial measures in the discussion ('prior measures of extracranial broadband and gamma band responses')."
"a gray scale image can be considered as a topographic surface, where height of each point is related to its gray level. if we punch a hole in each local minimum and immerse this surface in water, the regions in the image will start filling up with water. immersion will starts from the points of minimum gray value. when water level in two or more adjacent basins will start merging, dams are built in order to prevent this merging. the flooding process will continue up to the stage at which only the top of dam is visible above the water line [cit] . watersheds are the lines dividing two catchment basins, each basins corresponds to each local minimum. figure 3 shows the watershed lines superimposed on dapi channel."
"for comparison, we conducted the above analyses on two synthetic data sets with known signal and noise ( s4 fig). for the synthetic dataset containing localized, stimulus-related broadband responses, the analysis reproduced several of the effects seen in the data. first, the denoising algorithm identified a noise pool of sensors that were not \"visually responsive\" (i.e. contained noise but no signal). second, the algorithm increased the snr in \"visual\" sensors (i.e., sensors with signal) but did not increase the snr in the sensors with noise only. third, the increase in snr resulted from a decrease in noise rather than an increase in signal. fourth, the snr increased as the number of pcs projected out increased from 0 to 10, and then gradually decreased. this was expected because the global noise was a mixture of 10 basis functions. for the synthetic dataset that did not contain localized, stimulus-related broadband responses, applying the algorithm did not add artefactual broadband responses."
original dapi chromosome image contains nuclei and debris along with chromosome. we must remove them based on the size and circularity before segmentation. figure 2 shows the image before and after blob removal.
we used matlab commercial software to simulate scattering and absorption characteristics in mice brain tissue. table 1 shows the parameters and respective values used for these simulations.
"eye tracking analysis. since an increase in microsaccade rate can induce broadband spectral components in extracranial measurements such as eeg or meg [cit], we checked in three nyu subjects (s6-s8) whether there was a difference in rate between the 'off' baseline periods and 'on' stimulus periods, and within the three stimulus (both-, left-, right-hemifield) conditions. microsaccades were identified as changes in position with above a relative velocity threshold (6˚/s) and a minimum duration of 6 ms, as reported in engbert & mergenthaler [cit] to analyze rate and direction of microsaccades as well as separating meg data into epochs that did and did not contain microsaccades."
"a large field 'on-off' stimulation experiment was used to characterize the stimulus-locked (steady state evoked field, 'ssvef') and broadband responses in visual cortex measured with meg. the two measures are reported below, both prior to and after applying our new denoising algorithm noisepool-pca."
"noisepool-pca defines noise regressors by applying pca on a subset of sensors. in principle, the procedure can capture any noise source contributing to the noise pool, including environmental, oculomotor, muscular, and neural. this differs from algorithms designed to remove environmental noise. hence noisepool-pca is complementary to these methods. after the initial step of rejecting bad sensors and bad epochs, we found that the most effective analysis was either noisepool-pca alone, or noisepool-pca following an environmental denoising algorithm such as calm [cit], tspca [cit], or tsss [cit] . for general usage, it therefore seems prudent to first reject bad sensors and epochs, and then to use an environmental denoising algorithm followed by our new method."
"we now consider the effect of denoising across sensors, subjects, and stimulus conditions. projecting out noise pcs substantially increased the signal-to-noise ratio of the broadband measurement in visually responsive sensors. for example, in the both-hemifield condition for subject s1, the median snr of the 10 most visually responsive sensors increased from 5 to 10 after denoising (fig 7a, solid blue line), similar to the example sensor shown earlier (fig 6b) ."
"group averaged broadband. two meg studies reported increases in high gamma power (60-140 hz) during recall of visual stimuli [cit] . these studies showed averages across subjects (22 or 24), so that it is not known whether there were reliable responses in individuals. moreover, these power increases were interpreted as a change in neural synchrony within this frequency range, whereas we interpret our results as arising from a change in the level of neuronal activity."
"to use noisepool-pca for other experimental designs, analyses, or scanners, one would need to change some input parameters. in addition to the experimental design matrix and data, required inputs include the experiment-specific functions to summarize the meg responses. in our experiments, one function computed the stimulus-locked signal and was used to define the noise pool. for most of our analyses, a second function computed the broadband power to evaluate the results. in principle, one could use a single function to define the noise pool and evaluate the data (as we did for denoising the stimulus-locked signal). for other experiments, one might use a function that computes the amplitude or latency of an evoked response, or the power in a limited temporal frequency band, or any measure relevant to the experiment. alternatively, one could run a separate localizer experiment to identify a pool of potential sensors of interest and a pool of noise sensors, and then manually enter the list of noise sensors to denoise the main experiment. unlike ica or ssp denoising, our algorithm will not work on resting state data for which no task design is available. there are several other optional inputs, such as the method to identify the noise pool, the accuracy metric (snr/r 2 ). here, we used the defaults for all optional inputs. the algorithm might not be suitable for experimental paradigms in which all sensors carry responses that are related to the structure of the experiment. however, it is likely useful for many sensory experiments or other paradigms that strongly drive responses in localized regions of the brain."
"we separated the meg signal into two components, one time-locked and one asynchronous with the stimulus. the stimulus-locked component was clearly visible in all subjects with minimal preprocessing. the asynchronous signal, spanning 60-150 hz, was visible with little preprocessing in some subjects and the mean across subjects. however, the snr was low compared to the stimulus-locked component. with our denoising algorithm, snr more than doubled, resulting in reliable, spatially specific broadband signals in all individuals. we showed in a subset of subjects that the broadband signals could not be explained by systematic biases in the pattern of fixational eye movements, supporting the interpretation that the broadband fields arise from neural activity rather than artifacts associated with eye movements."
"binary mask is created from dapi image after cell removal and superimposing watershed regions on it, in order to avoid the segmentation errors present due to unhybridization. binary mask is created by otsu's thresholding method [cit] . basic operation is, logical and operation between the watershed lines and blob removed dapi image."
"an automated method for m-fish karyotyping employing watershed segmentation followed by naïve bayes classifier is presented. mean and standard deviation are the features used for classification. improved accuracy is obtained by adding a post-processing method that reclassifies the small segments to neighboring segments based on bayes theorem. this method works for all probes and the results are better than pixel-by-pixel classification, which always produces noisy results. as the classification is done on the watershed regions, the computational time needed is also much less than the pixels by pixel approach. use of pre-processing techniques [cit] and manually corrected ground truth [cit] will further improve classification accuracy. future work is to extend this method for multichannel watershed and to test on much larger data set."
"a third challenge is the potential confound between broadband signals and narrowband gamma oscillations. narrowband gamma oscillations have been successfully measured with meg and eeg, particularly in visual cortex for high contrast gratings [cit] . the frequency range of these oscillations (30-100 hz) overlaps the broadband range, but the narrowband and broadband signals reflect biologically different processes [cit] 10] . the ability to measure one does not imply the ability to measure the other."
"synthetic dataset. to illustrate how the denoising algorithm works on a known signal, we generated two synthetic data sets and analyzed them with the same code used to analyze the meg data (s4 fig). the first synthetic data set was comprised of a mixture of 4 components in each of 157 sensors across 30 epochs (1-s each, with ms sampling). the 4 components were stimulus-locked, broadband, background local noise (uncorrelated across sensors), and background global noise (correlated across sensors). because the data set contains a mixture of correlated (global) and uncorrelated (local) noise, it provides a test of whether the algorithm can identify and remove the global noise sources. we defined all sensors in the back half of the mesh as responsive to the stimulus ('visual sensors') and the all sensors in the front half as not responsive ('non-visual'), implemented in the following way. the stimulus-locked component was a sinusoid, present in the visual sensors during stimulus epochs and absent during blanks. this component was identical across all visual sensors. the broadband component was pink noise (1/f), present in the visual sensors and absent in the non-visual sensors. the pink noise comprising the broadband component was defined independently for each visual sensor, so it was uncorrelated across sensors. the background local noise component was also pink noise, but unlike the broadband component, it was present in all epochs (stimulus and blank) and all sensors (visual and non-visual). this component was uncorrelated across sensors. finally, the background global noise component was a weighted sum of 10 basis time series. the weights for the different sensors were random but constrained to have a norm of 1. these time series of the 10 basis functions were independent of each other. because this component was a weighted sum of the same 10 basis functions, this component was correlated across sensors. the expectation was that the pca from the denoising algorithm would identify and remove the global noise, thereby increasing the snr of the estimated broadband response. the second synthetic dataset was generated without local broadband (i.e. setting the response amplitude to zero), while keeping the stimulus-locked, uncorrelated and correlated noise components. having a synthetic data set with a robust stimulus-locked and noise component allowed us to check whether the algorithm injects artefactual broadband signals when none is present."
"broadband vs. narrowband gamma. several groups have distinguished broadband power increases from narrowband gamma oscillations [cit] . gamma oscillations are observed in visual cortex for some stimuli (e.g., high contrast gratings) [cit], with a peak frequency between 30 and 100 hz and bandwidth of 10-20 hz. the broadband response occurs in many brain areas for many types of stimuli, spanning at least 50-150 hz, and can extend to lower and higher frequencies [cit] . robust gamma oscillations have been measured extracranially for grating stimuli [cit] . because this signal originates from synchronized neural activity [cit], it propagates efficiently over distances [cit], similar to the stimulus-locked signal we discussed above. these oscillations differ from the broadband fields reported here, which span a wider frequency range, have lower amplitude, and likely reflect asynchronous neural activity."
"stimulus-driven broadband brain responses can be quantitatively characterized in individual subjects using a non-invasive method. because we obtain high snr measures from short experiments, the broadband signal can be used to address a wide range of scientific questions. access to this signal opens a window for neuroscientists to study signals associated with neuronal spike rates non-invasively at a high temporal resolution in the living human brain. a) . prior to denoising, there is little difference between the broadband power in the two conditions, stimulus and blank. (c) same as panel b, but after running noisepool-pca on the synthetic dataset. as with actual meg data, we removed the harmonics of the stimulus-locked component (12 hz) . after denoising, the stimulus condition has more broadband power than the blank condition. the overall power declines for both stimulus and blank conditions (see fig 6 for similar results on meg data) . (d) snr as a function of number of principle components (pcs) projected out. the snr rises until 10 pcs are projected out, and then slowly declines, similar to meg data (fig 7) . this is expected because there is noise in each sensor mixed from 10 basis functions. the inset shows the noise pool"
"our bio-heat transfer results corroborate with recent studies found in the literature [cit] . these authors were the first to explores heat generation by light in optogenetic experiments and compare simulations with empirical measurements. our work, instead, explore the effect of bio-heat transfer in neurons and networks, in particular, with a few differences compared to the study by stujenske and colleagues [cit] . for instance, these authors used light absorption and scattering coefficients obtained from human brain tissue interpolated from different wavelengths while here we employ coefficients obtained from rodent brains in specific wavelengths used in optogenetic experiments [cit] . besides, we have calculated temporo-spatial photon flux in brain tissue. ultimately, photon flux determines the opening of channelrhodopsin pores, and these values could be directly used for simulation of channelrhodopsin activation [cit] ."
"we then bootstrapped across epochs to compute confidence intervals on the signal estimates (per sensor and per condition). for each of 1000 bootstraps, we sampled n epochs with replacement, where n is the total number of epochs in the experiment. we then computed the average response across epochs for each stimulus condition, minus the average across blank epochs. this provided one summary measure for each of the three stimulus conditions and each of the two dependent measures (broadband and stimulus-locked) for each of the 1000 bootstraps. finally, we took the mean of the summary measure across epochs (i.e., the sample mean) as the estimate of signal and the standard deviation across bootstraps (i.e., standard error of the mean) as a measure of variability (fig 2d and 2e) . because the standard deviation across bootstraps is potentially a biased measure of variability for some distributions, we checked whether this was the case in our data: we compared the standard error computed this conditions, and the number of rows is equal to the total number of epochs across the session. rows with no color are blank epochs. (e) summary metrics were computed separately for the stimulus-locked values and broadband measures, yielding three measures per sensor per data type. the summary metric was the mean across condition minus the mean across blanks, bootstrapped 1000 times. the bar plot show the mean across epochs and the standard deviation across 1000 bootstraps. made with function nppmakefigure2.m."
"it is observed that small segments are usually misclassified. to overcome this, small segments are reclassified to the most likely class of one of its neighbours by bayes theorem, so that it becomes the same class as one of these neighbours."
"head muscles can also cause spectrally broadband contaminants [cit], as can external noise sources, e.g., nearby electrical equipment. however, these noise sources are unlikely to be confined to occipital sensors and to co-vary with stimulus condition, and hence do not explain our broadband observations. moreover, it is likely that these noise sources, if present, were included in our noise pool, and hence noisepool-pca would have reduced their effects."
"metric to quantify the effect of denoising. to assess the reliability of broadband responses, both before and after applying our denoising algorithm, we summarized the responses in terms of a signal-to-noise ratio (snr). we defined snr as the ratio of the estimated mean broadband response to the standard error, as described above ('computation of stimulus-locked and broadband responses'). this definition of snr reflects the stability (but not the magnitude) of the estimated stimulus driven broadband response, and is similar to the snr metric used for assessing the related fmri denoising algorithm, glmdenoise [cit] . we are interested in the stability of the estimated broadband response level because the ability to distinguish the response level for different stimuli or experimental conditions is essential for addressing many scientific questions. the lower panel shows the distributions of the bootstrapped broadband power for the both-hemifield (blue), blank (gray), and both-hemifield minus blank (black, inset), prior to denoising (left) and after denoising (right). the snr is defined as the mean of the difference between both-hemifield and blank epochs divided by the standard deviation across bootstraps of the difference distribution (7.7 prior to denoising, 14.0 after). the effects of denoising are to reduce the mean power, and more importantly, reduce the standard deviation across epochs. made with function nppmakefigure6.m."
"the amplitude of the broadband signal we measure differed between participants. this could arise from differences in the level of neuronal broadband, as found with ecog measures [cit], from differences in physiological noise sources, or differences in cortical head geometry and meg sensitivity to visual cortex. due to the relatively small number of participants in the study, we did not try to infer which of these factors contributes most to individual differences in measured meg broadband."
"both the stimulus-locked and broadband signals were largest in medial, posterior sensors, as expected from activations in visual cortex [cit] . for the stimulus-locked signal, the bothhemifield condition tended to produce broadband signals in bilateral posterior sensors, whereas the single-hemifield conditions produced responses that were lateralized, with higher snr contralateral to the stimulus. this pattern could be seen in an example subject and in the average across subjects (fig 5) . the lateralization of the stimulus-locked signal was less clear in the average across subjects due to imperfect alignment of the sensors showing the largest differential response to the left-and right-hemifield stimuli. in each of the 8 individual subjects and in each of the 3 conditions, the stimulus-locked response was evident, with the signal at least 10x above the noise (panel a in s5 fig) ."
"optogenetics is revolutionizing neuroscience, but an often neglected effect of light stimulation of the brain is the generation of heat. in extreme cases, light-generated heat kills neurons, but mild temperature changes alter neuronal function. to date, most in vivo experiments rely on light stimulation of neural tissue using fiber-coupled lasers of various wavelengths. brain tissue is irradiated with high light power that can be deleterious to neuronal function. furthermore, absorbed light generates heat that can lead to permanent tissue damage and affect neuronal excitability. thus, light alone can generate effects in neuronal function that are unrelated to the genuine \"optogenetic effect.\" in this work, we perform a theoretical analysis to investigate the effects of heat transfer in rodent brain tissue for standard optogenetic protocols. more precisely, we first use the kubelka-munk model for light propagation in brain tissue to observe the absorption phenomenon. then, we model the optothermal effect considering the common laser wavelengths (473 and 593 nm) used in optogenetic experiments approaching the time/space numerical solution of pennes' bio-heat equation with the finite element method. finally, we then modeled channelrhodopsin-2 in a single and spontaneous-firing neuron to explore the effect of heat in light stimulated neurons. we found that, at commonly used light intensities, laser radiation considerably increases the temperature in the surrounding tissue. this effect alters action potential size and shape and causes an increase in spontaneous firing frequency in a neuron model. however, the shortening of activation time constants generated by heat in the single firing neuron model produces action potential failures in response to light stimulation. we also found changes in the power spectrum density and a reduction in the time required for synchronization in an interneuron network model of gamma oscillations. our findings indicate that light stimulation with intensities used in optogenetic experiments may affect neuronal function not only by direct excitation of light sensitive ion channels and/or pumps but also by generating heat. this approach serves as a guide to design optogenetic experiments that minimize the role of tissue heating in the experimental outcome."
"in this stage, for each region all the neighboring regions that share the same class are connected inorder to get meaningful class map. if regions are adjacent then those regions are connected in region adjacency graph (rag) and have a common boundary."
"possibility that a putative noise pool will in fact contain stimulus-related signal, and that this signal will be projected out of our sensors, hence reducing signal at least as much as we reduce noise. whether or not this is an actual problem in practice is an empirical question, and we return to it in the results. to preview, we find that the noise pool in fact contained little to no signal, and that the diffuse spread of signals across all meg signals was not a problem for our denoising algorithm."
"the goal of the noisepool-pca algorithm is to estimate, and then project out, noise in order to get a more reliable measure of signal. this approach is likely to yield unstable results when the noise level is too high, for example in the case of faulty sensors or brief periods with unusually large external disturbances. therefore, as a preprocessing step prior to the application of the algorithm, we first identified and rejected sensors and epochs with very high noise levels. this procedure followed simple rules based on the overall variance in the meg time series, described in the material and methods. there are other algorithms for identification of bad sensors or epochs such as the 'autoreject' algorithm [cit] and any such algorithm could substitute for our preprocessing step."
power spectrum density analysis and cross-correlation of action potentials were calculated from spike trains transformed in a series of 0 s (no spike) and 1 s (spike) with 0.1 msprecision [cit] . power spectral density analysis of binary spike series was performed using welch's method (pwelch command in matlab). cross-correlograms (ccgs) were calculated as described previously [cit] and then smoothed by a moving average filter with a span of 10 ms [cit] . cross-correlations over a lag range of ±0.1 s. synchrony index (si) is defined as the maximum value of the ccg.
"the effect of denoising the broadband signal was not uniform across the sensor array. in general, sensors where we expected visual activity (over the posterior, central part of the head) showed increased snr following denoising. in the example subject s1 as well as the average across subjects, the denoised broadband response was observed in bilateral sensors for the both-hemifield condition, and with a contralateral bias (relative to the midline) in the two lateralized conditions (fig 9) . for the both-hemifield stimulus, broadband responses were evident in sensors over the posterior, middle of the head in most individual subjects (fig 10) ."
"additionally, thermal boundary interactions occur over the surface area with the environment and are often characterized as convective and irradiative processes. laser irradiation process increases the stored energy from its initial state and, as a result, it diffuses the heat away from the irradiated area in proportion to the temperature gradients developed in the tissue. a quantitative characterization of the formation of these gradients and the heat flow that they drive are the focus of heat transfer analysis [cit] ."
"in this work, we first simulated the light propagation and absorption in the brain of mice in a typical optogenetic setup. figure 1a shows a diode pumped solid state -dpss laser source coupled to a multimode optical fiber that transmits light directly to the region where the brain implant was performed [cit] ."
"there are two other secondary patterns evident in these distributions. first, the mean broadband power of both the blank and stimulus condition decreased as a result of denoising (for the both-hemifield condition, 35.1 versus 26.1, prior to versus after denoising; for the blank, 28.2 versus 21.3). this was expected because projecting out signal reduces power. second, the contrast between the two conditions (difference between the means) reduced: 6.9 prior to denoising versus 4.8 after denoising. the combination of these two effects was that the percent difference was little changed, with broadband power from the contrast-stimulus about 25% more than for the blank before and after denoising. hence denoising did not increase the estimate of the percent signal change."
"where,â is the propagation direction of the plane wave relative to the absorber, p a is the absorbed power, and i w is the intensity of the wave. therefore, a medium with absorbing particles can be characterized by the absorption coefficient, µ a :"
"extracranial broadband signal strength is low. although having high snr after denoising, the meg broadband signal was nonetheless small relative to baseline-about a 13% increase. using nearly the identical stimulus, the broadband signal measured by ecog was~15 times larger (a~190% increase) [cit] . the discrepancy was much smaller for the stimulus-locked signal (almost 8-fold increase with meg vs. 21-fold with ecog). why are meg broadband signals small? first, meg sensors pool over a large area, so baseline power reflects activity from a large fraction of the brain, whereas visually driven broadband responses likely come from confined regions [cit] . in contrast, both baseline and visual responses in ecog electrodes arise from the same cortical patch. second, the amplitude depends not only on pooling area, but also phase coherence. if broadband signals arise from incoherent neural activity, and stimulus-locked signals from coherent (synchronous) activity, then the former will grow with the square-root of the number of sources, and the latter with the number of sources. since meg pools over much larger populations than ecog, the ratio of incoherent signal strength (broadband) to coherent (stimulus-locked) will be much lower. this logic is supported by modeling [cit] and empirical studies with intra-and extracranial measures, which found that the most coherent intracranial signals were best transmitted outside the head [cit] ."
"in principle, the snr increases could have arisen from increased signal, decreased noise, or both. to distinguish among these possibilities, we compared the signal level alone and the noise level alone before and after denoising. as in prior results, the signal was defined as the difference in broadband power between the contrast pattern and the blank (mean of data), and the noise was defined as the variability of this difference metric (standard deviation across bootstraps). for all three stimulus conditions in most subjects, the signal was largely unaffected by denoising, staying at a similar level or decreasing slightly, while the noise level went down substantially (fig 8) . these analyses indicate that the increase in snr from denoising (fig 7) was caused by a reduction in epoch-to-epoch variability of the broadband signal level, and not by an increase in the signal level, consistent with the results of the single example sensor (fig 6) . expressed as a percentage increase over baseline, the broadband response to the both-hemifield stimulus after denoising was~10.9±1.7% averaged across the top 10 sensors in each subject (mean ± sem across subject), and 12.6%±1.6% for the top 5 sensors. this contrasts with the much larger stimulus-locked response, which was a nearly 8-fold increase over baseline even prior to denoising (678%±226% increase over baseline for the top 5 sensors)."
"nonetheless, for a subset of subjects (s6-s8), we measured eye movements during the meg experiments and quantified the frequency of microsaccades, and the distribution of microsaccade direction, for each stimulus condition. each of these 3 subjects showed broadband responses in their denoised data (fig 10) . all three subjects showed a higher rate of horizontal than vertical microsaccades in every stimulus condition (fig 14a), consistent with prior observations [cit], but there was no systematic pattern in saccade frequency as a function of stimulus condition; for example, the stimulus condition with the most and with the fewest microsaccades differed across the 3 subjects. moreover, the subject with the highest broadband snr among these 3 (s6) had the lowest rate of microsaccades (~0.5 microsaccades / second). to test more directly whether microsaccades contributed to the measured broadband fields, we re-analyzed the data from these 3 subjects in two ways, either limited to only those epochs with microsaccades or only those epochs without microsaccades (fig 14b) . the broadband responses were evident in each subject in the epochs without microsaccades, indicating that this response is not entirely an artifact of microsaccades."
"from the above review work on the various approaches suggested by the researchers, one can conclude that region based classification approaches are superior to pixel by pixel approach in terms of accuracy and computational time. many researchers have tested their approaches only on small sets of selected images. for practical purposes, the systems must be capable of providing high accuracy on large data set. so, there is a need for research to improve the performance further on large data sets so that such automated systems for karyotyping can be acceptable for commercial purposes."
"to date, most in vivo experiments rely on light stimulation of neural tissue using fiber-coupled lasers of various wavelengths. blue and yellow lasers are broadly employed for optogenetic experiments, but due to poor penetration of these light frequencies in the brain, high laser power and/or fibers of high numerical aperture are often used to achieve functional stimulation of deep brain regions [cit] . hence, brain tissue is irradiated with high light power that can be deleterious to neuronal function, but surprisingly little attention has been paid on the effects of light stimulation itself in optogenetic experiments. absorbed light generates heat that can lead to permanent tissue damage. additionally, neuronal excitability is acutely affected by temperature through the changes in nernst equilibrium potential and by altering the gating properties of ion channels [cit] . thus, light alone can generate effects in neuronal function that are unrelated to the genuine 'optogenetic effect'. in modeling studies, an empirical factor (q 10 ) is used to multiply rate constants to add temperature dependence to the classical hodgkin and huxley formalism [cit] ."
"later, cao and wang [cit] presented segmentation of m-fish images for improved classification of chromosomes with an adaptive fuzzy c-means clustering. adaptive fcm was done by incorporating a gain field which models and corrects intensity homogeneity and also regulates center of each intensity cluster. intensity homogeneity is mainly caused by the image acquirement and uneven hybridization. it provides lowest segmentation and classification error and is better than fcm and afcm."
"these results are qualitatively consistent with intracranial measurements [cit] . however, it has proven difficult to measure extracranial broadband signals arising from neural activity. below, we discuss the significance of broadband responses, challenges in measuring them extracranially, and the generalizability of our denoising algorithm."
"we first illustrate the effect of denoising with an example from a single sensor in one subject (fig 6) . this sensor showed a broadband response both prior to, and after, denoising. the benefit of denoising was not evident when comparing the mean power spectra before and after denoising (fig 6a) . denoising did not reduce the variability in power across frequencies, nor did it increase the separation in the spectra for the contrast stimulus and the blank. instead, the effects of denoising are better appreciated by examining the variability across epochs rather than across frequencies (fig 6b) . the biggest effect is that the broadband power estimates became less variable across epochs, both for the blank condition and the stimulus condition. this is indicated by the narrower distributions in the response amplitudes for the two conditions (fig 6b, main panels) and for the difference between conditions (fig 6b, insets) . the standard deviation of the difference distributions decreased more than two-fold (from 0.91 to 0.34) as a result of denoising."
"the relation between the angle of incidence, θ 1, and the angle of refraction, θ 2, for the transmitted light is given by snell's law [cit] :"
"in this method, the minima selection value is very important and it was found heuristically after several experiments and fixed to 5. as this value increases, area of each region is increased and total number of regions is decreased. figure 6"
"the work by wang [cit] deals with fuzzy c-means clustering algorithm (fcm) based classification of m-fish images. this uses 24 different cluster centers, which are formed from 24 classes of chromosomes and a pixel is assigned to each individual cluster according to its nearest distance to the center. finding a cluster center is equivalent to minimizing the dissimilarity function; here, euclidian distance is used as dissimilarity measure. the advantage of fcm is that it can locate centers more accurately because here the membership values are from 0 to 1. it works better than k-means clustering and bayes classifier. use of image normalization techniques such as image registration, dimension reduction and background subtraction are also used, leading to improvements in accuracy."
"here, we sought to measure broadband signals quantitatively in the human brain using a non-invasive method (meg). in order for this important, spike-dependent signal to be useful, it is necessary to measure it reliably in individual subjects, with a high snr. a high snr is essential if this signal will be widely used to study differences across stimuli, tasks, or groups. we developed a novel, automated meg denoising algorithm adapted from prior fmri work [cit] . our experiments were designed to elicit spatially localized neural responses in visual the dartboards and was shown in the full field during blank trials between stimulus periods (fig 1) ."
"in clinical and research cytogenetic studies, automated computerized systems for human chromosome analysis are very essential since a small deviation from the usual number of chromosomes will result in physical abnormalities. chromosomes are structures located in nuclei of eukaryote cells that carry all the genetic instructions for making living organisms. normal human metaphase spread contains 46 chromosomes, 22 pairs of autosomes and sex chromosomes (xy: male, xx: female). chromosomes are present in every cell except red blood cells. chromosome analysis is done on dividing cells in their metaphase stage (different phases of cell division: metaphase, anaphase, and telophase). during metaphase, chromosome can be stained to become visible and can be imaged by a microscope. cells used for chromosome analysis are usually taken from amniotic fluid or from blood samples."
active research on karyotyping started since when the number of chromosome in human is found to be 46 [cit] . there are already a number of attempts proposed by various researchers to automate the process of karyotyping. we briefly review some of the major such works in this section:
"the identical experiments were conducted with 4 new subjects. as expected, all three stimulus types led to a large stimulus-locked response in the posterior sensors, with a peak snr of more than 10 in the group averaged data (fig 13a, column 1) . a modest, spatially specific broadband signal was measured from the data before denoising for each stimulus type ( fig 13a, column 2), with a peak snr of 1-2 in the group averaged data for all three conditions. unlike the nyu data, in the cinet data the noisepool-pca algorithm on the raw data did not generally result in an increase in the broadband snr (group data, fig 13a, columns 2 and 3; individual subjects, fig 13b, left side of each subplot) . however, when the raw data were preprocessed with the tsss algorithm (fig 13a, column 4), application of noisepool-pca increased the snr in all 3 stimulus conditions for 3 out of 4 subjects, and in 2 out of 3 stimulus conditions for the 4th subject. together, the noisepool-pca algorithm increased the snr by 2-3 fold, similar to the nyu data (both-hemifield: 3.0 to 5.4; left-hemifield: 0.70 to 2.6; righthemifield: 2.1 to 4.3; means across 4 subjects s9-s13, top 10 sensors each, for the tsss data and the noisepool-pca denoised tsss data). just as with the nyu meg data set, the combination of an algorithm tailored to find environmental noise (tsss) and our algorithm produced the most robust results, indicating that noisepool-pca and the environmental denoising algorithm removed at least some independent sources of noise."
"high frequency signals (~65-100 hz) have been shown from motor cortex measured extracranially [cit] . this signal was most evident in group-averaged data and some but not all individuals, and within a relatively narrow band (~20-30 hz wide). [cit] noted that better methods for measuring high frequency broadband extracranially would help resolve whether individual differences were due to measurement limitations or the lack of high frequency brain signals in some subjects. [cit] measured high gamma (65-80 hz) with meg over motor cortex in individual subjects, and speculated that these signals reflect cortico-basal ganglia loops, as the basal ganglia is known to produce narrowband oscillations peaked at 70-80 hz."
"reproducible computation and code sharing. all analyses were conducted in matlab. in the interest of reproducible computational methods, both the analysis code and the meg data for all results reported in this paper are publicly available via the open science framework at the url https://osf.io/c59sh/ (doi 10.17605/osf.io/c59sh). meg preprocessing. for some analyses, data were environmentally denoised using published algorithms prior to any further analysis. this enabled us to compare data denoised with our new algorithm alone, or with our new algorithm following environmental denoising. for the nyu data, we used either of two algorithms. one was the continuously adjusted leastsquare method (calm; [cit], applied to data with a block length of 20 seconds (20,000 time samples). the second algorithm was time-shifted principal component analysis (tspca; [cit], with a block length of 20 seconds and shifts of up to +/-100 ms in 1 ms steps. for the cinet data, the environmental denoising algorithm was temporal signal space separation ('tsss', [cit] ) (with default parameters, e.g. inside and outside expansion orders of 8 and 3, respectively; 80 inside and 15 outside harmonic terms; correlation limit of 0.98). overview of experimental design. large-field on-off stimuli were presented in 6-s blocks consisting of either both-, left-, or right-hemifield flicker, alternating with 6-s blocks of blanks (mean luminance). a run consisted of six stimulus and six baseline blocks, after which the subject had a short break. the figure shows the first half of one run. within a run, the order of both-, left-, and right-field flickering periods was randomized. fifteen runs were obtained per subject, so that there were 30 repetitions of each stimulus type across the 15 runs. the fixation dot is increased in size for visibility, and shown in gray scale. actual fixation dot was 0.17 degrees in radius (6 pixels the fieldtrip toolbox [cit] was used to read the data files (either environmentally-denoised or raw). for all subsequent analyses, custom code was written in matlab. using either the environmentally-denoised data or raw data, the signals were divided into short epochs. each stimulus type (both-, left-, or right-hemifield, or blank) was presented in 6-s blocks, and these blocks were divided into 6 non-overlapping 1-s epochs. we discarded the first epoch of each 6-s block to avoid the transient response associated with the change in stimulus. after epoching the data, we used a simple algorithm to detect outliers. we first defined a 'data block' as the 1-s time series from one epoch for one sensor. so a typical experiment consisted of~170,000 data blocks (157 sensors x 1080 1-s epochs). we computed the standard deviation of the time series within each data block, and labeled a block as 'bad' if its standard deviation was more than 20 times smaller or 20 times larger than the median standard deviation across all data blocks. the time series for bad data blocks were replaced by the time series spatially interpolated across nearby sensor (weighting sensors inversely with the distance). further, if more than 20% of data blocks were labeled bad for any sensor, then we removed the entire sensor from analysis, and if more than 20% of data blocks were bad for any epoch, then we removed the entire epoch from analysis. typically, two to seven sensors and 2%-4% of the epochs were removed per session for the nyu data. for the cinet datasets, almost no sensors or epochs were removed (one sensor and one epoch across all data sets). these preprocessing steps were implemented with the function npppreprocessdata.m."
"pca. following filtering, the next step in the algorithm was principal component analysis (pca). this identified the common components of the time series across the sensors in the noise pool. pca was computed separately for each 1-s epoch (fig 3c) . this means that denoising occurred at the same temporal scale (1-s) as the computation of the summary metrics. this differs from some denoising algorithms, in which noise regressors are identified over a much longer time period, e.g., several minutes [cit] . denoising at a short-time scale can be advantageous if the spatial pattern of the noise responses is not consistent across the entire experiment. as a control comparison, we also ran our algorithm by identifying pc time series on the entire duration of the experiment (~20 minutes) rather than epoch by epoch. (see results, 'control analyses for noisepool-pca algorithm'.) because the pcs were computed independently for each short epoch, many thousands of pcs were computed per experiment (~1,000 epochs projecting out pca components. the first one to ten principal components (pcs) in each epoch were projected out of the time series for all sensors, using linear regression. this resulted in ten new data sets: one with pc 1 projected out, one with pc 1 and 2 projected out, etc. up to 10 pcs projected out (fig 3d) . after projecting out the noise components, we summarized the data into a stimulus-locked and broadband component as described in fig 2. choice of denoising parameters. we conducted several control analyses to probe how choices made in applying the denoising algorithm affected the results of denoising (amount of snr gained). in particular, we systematically varied the number of sensors in the noise pool, the number of pcs projected out, and the number of epochs denoised at a time (s1 and s2 figs)."
"although spike fields generated from eye movements can be mistaken for broadband neural activity [cit], it is unlikely that our spatially-specific broadband measures were substantially contaminated by eye movement artifacts. this was confirmed by analyses of eye movement data, and the fact that middle posterior sensors where we observed broadband are not usually associated with meg spike field artifacts [cit] . a second eye movement confound, the electromagnetic fields arising from movement of the retina-to-cornea dipole, causes low frequency artifacts (4-20 hz; [cit] ) and therefore is unlikely to have affected our broadband measures ."
"subsequently, we simulated the effect of heat in single neurons and networks. we have also examined the additive effect of heat and light in simulations that included a channelrhodopsin-2 model [cit] . the bio-heat transfer was solved numerically using pennes' equation with the finite element method and temporal changes in temperature at a given point in space were applied to a single compartment neuron model (with hodgkin and huxley formalism)."
"saccadic eye movements are known to have a large influence on meg and eeg measurements. this influence can be especially pernicious when measuring high frequency broadband signals, because the spike field (meg) or spike potential (eeg) arising from extraocular muscle contraction can be spectrally broadband and can co-vary with task design; hence, it can easily be confused with broadband signals arising from brain activity [cit] . for visual experiments, the spike potential in eeg is especially problematic because it tends to affect sensors which are also visually sensitive (posterior middle). in contrast, the meg spike field is lateral, potentially influencing temporal and frontal sensors, with little to no effect on posterior sensors [cit] . hence spike field artifacts are unlikely to contaminate our visually elicited broadband signals, which are most clearly evident in the central posterior sensors."
"heat transfer is a known physical problem already modeled in many areas of knowledge [cit] . for biology, heat is inevitable when light propagates and is absorbed by biological tissues. the traditional bio-heat equation describes the change in tissue temperature over time that can be expressed at a distance d in the tissue. furthermore, blood perfusion occurs in living tissues, and the passage of blood modifies the heat transfer in tissues. [cit] has established a simplified bio-heat transfer model to describe heat transfer in tissue by considering the effects of blood perfusion, ω b, and metabolism, h m [cit] :"
"we used the stimulus-locked signal to identify the noise pool because this signal had a very high snr, and could easily by measured prior to running our noisepool-pca algorithm, and because we assumed (and confirmed by inspection) that sensors with broadband responses also had stimulus-locked responses. for most subjects, most of the sensors in the noise pool were located over the front of the head (see for example fig 3a) . filtering of time series. as described above, the broadband summary metric was derived from power at a limited range of temporal frequencies (60-150 hz, excluding multiples of the stimulus frequency). after defining the noise pool, the time series of all sensors in all epochs were filtered to remove signal at all frequencies not used to compute the broadband signal. hence the remaining time series contained power only at frequencies defining the signal of interest. this step was important because the noise pool, though selected for a low stimuluslocked snr, could nonetheless have contained a small, residual stimulus-locked signal. this residual signal would have been correlated with the experimental design (larger when stimuli were present than absent) and hence projecting it out of the data could have caused a systematic bias (see the script denoisingprojectinginvariance.m)."
"experimental design. one run consisted of six seconds flickering 'on' periods, alternated with six seconds 'off' mean luminance periods, repeated 6 times (72 seconds). the order of the both-, left-, or right-visual hemifield apertures was random. there was a fixation dot in the middle of the screen throughout the run, switching between red and green at random intervals (averaging 3 seconds). the subjects were instructed to maintain fixation throughout the run and press a button every time the fixation dot changed color. the subjects were asked to minimize their blinking and head movements. after every 72-second run, there was a short break (typically 30-s to 1 minute). each subject participated in 15 runs."
computation of stimulus-locked and broadband responses. data were summarized as two values per sensor and per epoch: a stimulus-locked and a broadband power value. these calculations were done by first computing the fourier transform of the time series within each epoch (fig 2a and 2b) .
"the fact that broadband responses were evident in a few subjects in some conditions indicates that it is possible to measure broadband fields with meg. however, if this signal cannot be measured reliably in many subjects and many conditions, then the practical value of measuring broadband with meg is limited. this motivated us to ask whether denoising the meg data could unmask broadband signals, making it more reliable across subjects and stimulus conditions."
"the stimulus-locked signal was then defined as the amplitude at the stimulus-locked frequency (12 hz). the broadband response was computed as the geometric mean of the power across frequencies within the range of 60-150 hz, excluding multiples of the stimulus-locked frequency (see also fig 2a and 2b) . the geometric mean is the exponential of the average of the log of the signal. we averaged in the log domain because log power is better approximated by a normal distribution than is power, which is highly skewed. these two calculations converted the meg measurements into a broadband and a stimulus-locked summary metric, each sampled once per second (fig 2c) . the two summary metrics were computed by the functions getstimlocked.m and getbroadband.m."
"to test whether the findings reported above generalize to other instruments and experimental environments, we conducted the same experiment using a different type of meg system, an elekta 360 neuromag at cinet. the cinet system contains paired planar gradiometers, in contrast to the axial gradiometers used in the yokogawa meg at nyu, and the scanner is situated in a different physical environment, with potentially very different sources of environmental noise. the preprocessing pipeline at this imaging center often includes a denoising step based on temporally extended signal source separation (tsss) [cit] . this additional experiment gave us the opportunity to ask several questions: (1) are broadband fields observed with a different meg sensor type and different physical environment? (2) does the tsss algorithm increase the broadband snr? (3) does our new noisepool-pca algorithm increase the snr of data that have already been denoised with the tsss algorithm?"
"in summary, we have shown that temperature increase caused by brain optical stimulation, with light intensities commonly used in optogenetic experiments [cit] can considerably affect neuron and network properties independently of opsin expression. moreover, the temperature can alter cellular responses to optical stimulation. as the usage of channelrhodopsin becomes widespread, studies tend to assume that optical stimulation elicits spiking activity without assessing cellular responses [cit] . thus, the whole cell current-and voltage-clamp assessment of the cell response to optical stimulation may still be necessary to determine optimal light stimulation protocols."
"control analyses. to investigate the validity of our algorithm, we ran multiple control analyses. in particular, it is important to rule out the possibility that the denoising algorithm produces significant results even when the data contains no sensible signal. to test this, we compared the difference in snr of denoised data with the following controls: (1) phase-scrambling the pc time series, and (2) using all sensors to define the noise with pca rather than only a subset of sensors that have little to no stimulus-locked signal. we also assessed the effect of identifying and projecting out pc time series equal in length to the entire experiment (~20 minutes), rather than pc time series matched in length to our analysis epochs (1-s) . this comparison tested the assumption that denoising in shorter epochs was advantageous, possibly due to the pattern of noise sources differing over the course of the experiment."
"in contrast, the snr of the 75 sensors in the noise pool was relatively unaffected by denoising (fig 7a, dotted blue line) . for these sensors, the snr both prior to, and after denoising, was close to 0. this indicates that these sensors did not contain any appreciable stimulusrelated broadband signal. this is important, because if the stimulus-related broadband response propagated to all sensors, then the noise pool would be contaminated by signal, and the algorithm might project out signal rather than reducing noise. the fact that the snr increased in sensors in the back of the head, but was zero in the noise pool, is consistent with the interpretation that the algorithm removed global noise (as intended) rather than removing global signal."
"extracranial measurements contain multiple noise sources. because extracranial broadband power is low, noise is a major impediment. in addition to neural noise [cit], fixational eye movements [cit], head muscle contraction [cit], and environmental perturbations [cit] produce noise measured by meg and eeg sensors. many of these noise sources are spectrally broad and hence particularly problematic when investigating neural broadband signals."
"presently used classification methods are based either on pixel-by-pixel or on region based classification algorithms. pixel-by-pixel methods either classify each pixel of the m-fish image or create a binary mask of the dapi image using edge detection algorithms, and then classify each pixel of the mask. in region based methods, the regions obtained by decomposing the image are classified."
"in this work, we model the optothermal effect in mice brain tissue produced by visible light laser sources (with a gaussian profile) in both continuous and pulsed modes [cit] to understand how heat can affect the transfer function of single neurons and how it can alter their response to photocurrents. we first approach the time/space numerical solution of pennes' bio-heat equation comprising the effects of blood perfusion and metabolism with the finite element method (fem) [cit] . we then simulate the effect of varying heat in two single neuron models (wang and buzsáki, 1996; [cit] ) that include a voltage and light-dependent current based on the channelrhodopsin-2 dynamics [cit] to demonstrate that heat itself can considerably alter neuronal dynamics."
"heat transfer simulations were accomplished using the computational modeling software, comsol multiphysics 4.4, that allows numerical solutions for partial differential equations based on the finite element method (fem) [cit] . laser heating was simulated considering two stationary conditions: continuous mode and pulsed mode. we used biological material with mice brain tissue characteristics (gray matter). the material properties were assumed to be constant and are shown in table 2 ."
"meg signal acquisition. data for the main experiment were acquired continuously with a whole head yokogawa meg system (kanazawa institute of technology, japan) containing 157 axial gradiometer sensors to measure brain activity and 3 orthogonally-oriented reference magnetometers located in the dewar but away from the brain area, used to measure environmental noise. the magnetic fields were sampled at 1000 hz and were filtered during acquisition between 1 hz (high pass) and 200 hz (low pass)."
"the paper is organized as follows: section 2 discusses some of the major existing work in the literature. image segmentation and classification processes are given in section 3. the comparative results obtained on standard database for the proposed approach and existing approaches are presented in section 4, and the section 5 concludes the paper."
"we first modeled the effect of temperature alone in a pyramidal cell model and in a network of basket cells known to generate gamma oscillations. we have implemented a single compartment ca1 neuron model described by migliore [cit] . he has implemented a multicompartment model in his original work, but here we only employ the soma with an inactivating sodium conductance (max. 30 ns), a delayed rectifier k + conductance (max. 10 ns), conductance from an m current (max. 0.6 ns) and from an h current (max. 0.3 ns). kinetics for all currents were download from modeldb (https://senselab.med. yale.edu/modeldb/, accession:2937)."
"this stage extracts the features used for classification. here, mean and standard deviation of each segmented area are the features used. then the intensities of the pixels belonging to that region are replaced with mean intensity of that region for each segmented area."
"in a subset of subjects (s6-s8), eye movements were recorded by an eyelink 1000 (sr research ltd., osgoode, on, canada). right eye position data were continuously recorded at a rate of 1000 hz. calibration and validation of the eye position was conducted by having the subject saccade to locations on a 5-point grid. triggers sent from the presentation computer were recorded by the eyelink acquisition computer. the same triggers were recorded simultaneously by the meg data acquisition computer, allowing for synchronization between the eye-tracking recording and meg recording."
"according to the mwsls scheme, we find that the event e n occurs only if the event e 1 n and e 2 n occur simultaneously."
"multiswarm optimization (mso) is a technique that is used to predict the optimal solution to nonlinear continuous optimization problems. the effectiveness and the productivity of many metaheuristic algorithms worsen as the dimensionality of the problem increases [cit] . to overcome this problem, we proposed the mmpso algorithm for obtaining the optimal solutions in a short time. the mmpso algorithm is a multipopulation-based metaheuristic optimization algorithm developed from the pso algorithm. in pso, the velocity of each particle is updated with the parameters pbest and best according to (4) [cit] . the proposed mmpso algorithm has multiple swarms. the velocity of each particle in each swarm is updated according to (6) . in this equation, there are two parameters that are different from (4), which are the mean pbest of all particles of that swarm (mpbest) and the best solution of all swarms ( sbest). this modification brings two advantages to pso. firstly, using the mpbest reduces the particles from going out of search space and reinforces the local search of each particle. secondly, each particle takes into account not only the gbest of its own swarm but also the sbest of all swarms, so that mmpso algorithm gets closer to the optimum solution faster."
"step 4 (determining the optimum weights of the mlfnns with metaheuristic algorithms). a well-trained mlfnns should have optimum weights and determining the optimum weights is a nonlinear optimization problem. metaheuristic algorithms can be used to solve this problem owing to the structure of the metaheuristic algorithm. generally, metaheuristic algorithms initialize with a random population. the fitness of each individual in the population is calculated according to the sse of mlfnns. the goal of the metaheuristic algorithms is to minimize the sse. therefore, metaheuristic algorithms search the problem space locally and globally and update the global best solution. the metaheuristic algorithms run until the stopping criteria, such as the number of iterations or the error rate, are met."
"proof: see appendix b. substituting (30) into (29), the asymptotic secrecy outage probability can be rewritten as (31), shown at the top of the next page."
"where t is the number of iterations, is inertia weight, i is the index of a particle in a swarm, d is the dimension of the problem, each particle has a position and velocity for each dimension, is the position of the ith particle in dimension d, v is the previous velocity of the ith particle in dimension d, is the mean value of the particles in a swarm, best is the best fitness value of a swarm, sbest is the best fitness value of all swarms, c 1 and c 2 are two positive constants representing acceleration factor, and r 1 and r 2 are two random numbers in the range [cit] . the pseudocode of the proposed mmpso algorithm is given in algorithm 2."
"mlfnns can be defined as a system by modeling the human brain functions. mlfnns consist of artificial neural cells linked to each other in various forms and are usually organized in layers. they can be implemented as hardware in electronic circuits or as software in computers. in accordance with the brain information processing method, mlfnn has the ability to store and generalize information after a learning process [cit] . some successful networks can be created with a single layer, although most applications require networks that contain at least three layers: an input layer, hidden layer, and output layer. a network consisting of a single layer can only predict linear functions. mlfnns remove the conflicting limits of single-layer systems with hidden layers located between the input and output layers [cit] . basically, all mlfnns have a similar structure to that shown in figure 1 . in this structure, neurons in the input layer are used to get inputs, while neurons in the output layer are used to carry outputs and all neurons in the hidden layers are used to aid system training [cit] ."
"in this part, we provide a comparison of csi requirement and complexity between the mwsls and mlsls schemes. according to the analysis above, we find that the csi of the main channel, i.e., h sr k and h r k d, are needed for both schemes. besides, the csi of the wiretap channel is also needed during the process of link selection under the mwsls scheme. hence, in this paper, we assume that the eavesdropper is normally an active member of the network, and the eavesdropper's csi can be obtained. based on this, the mwsls scheme has a higher complexity than the mlsls scheme due to a large amount of feedback overhead during the acquisition of eavesdropper's csi, which is the limitation of this scheme. from the analysis above, we can observe that the mwsls improves the security of the system at the cost of increasing the system complexity compared with the mlsls scheme."
"where n represents the number of samples in a dataset, is the output generated from the ℎ input, and is the target output of the ℎ input."
"step 2 (organization of the dataset for classification). in this study, the datasets are organized in two different ways for two different experiments. in the first experiment, 5-fold cross validation is used for comparing the proposed mmpso algorithm to the pso algorithm. in the second experiment, 80% training and 20% testing are used for comparing the proposed mmpso algorithm to previous research in the literature."
t denote the euclidean or l 2 vector norm and the transpose operation. the cumulative distribution function (cdf) and the probability density function (pdf) of the random variable γ are denoted as f γ (·) and f γ (·) respectively.
"the paper is organized as follows: in section 2, mlfnns are explained in detail. the used metaheuristic approaches pso and the proposed mmpso are clarified in section 3. the application of metaheuristic algorithms to mlfnns training is given in section 4. in section 5, the computational results are given and the paper is finalized with conclusions and future work."
"however, it is easily observed that the link or relay selection schemes adopted by most literatures above consider the link quality as the dominant selection metric, while there are no strict requirements for the buffer status. accordingly, the buffer of the corresponding relay with good link quality tends to be full or empty easily, which may result in the decrease of the available links."
"then we substitute (41) and (43) into (40), the exact cdf of the γ sr m 1,n e can be easily derived with the help of binomial theorem. following the similar approach, the desired expression of the asymptotic cdf can be obtained after some simple mathematical manipulations."
"for future work, the proposed mmpso algorithm will be used by intelligent systems to solve complex real-life optimization problems in various fields such as: design, identification, operational development, planning, and schedul ing."
"furthermore, for analyzing the computational complexities of the proposed mmpso and the pso algorithms, the cpu running times of each algorithm were measured by microsoft process explorer utility in seconds and are given in table 4 ."
"relay technique is seen as a promising way to extend the coverage of wireless networks and provides a variety of performance enhancements, which has attracted enormous research interest [cit] . however, in conventional cooperative relay networks, the half-duplex relay must follow a prefixed schedule for data transmission or reception [cit] . that is to say, the data packet is received by the selected relay in the first time slot, and then forwarded to the destination in the second time slot, which results in the consequence that the best channel cannot be utilized in a fast-fading environment, therefore limiting the performance of the network."
"motivated by these observations, we propose a novel maxweight secure link selection (mwsls) scheme to improve the secrecy performance of the dual-hop df buffer-aided relay networks. for comparison, the secrecy analysis of the max-link secure link selection (mlsls) scheme is also presented. the main contributions of this paper are summarized as follows:"
"in this section, the secrecy diversity gain, the average secrecy throughput and the end-to-end delay of the buffer-aided relay network are investigated, which can provide a comprehensive and effective method to evaluate the secrecy performance of the system."
"then we will proceed with the analysis of the end to end delay. in the buffer-aided relay network, the end to end delay of a data packet is the time interval between the packet leaves the source node and reaches the destination node, which can be expressed asd"
"represent the received snr at d and e. similarly, h r k e denotes the channel vector between r k and e, p r is the transmit power of r k . then, before introducing the proposed scheme, i.e., the mwsls scheme, we first review the popular mlsls scheme in the following."
"in order to delve into the buffer-aided secure link selection scheme, we first model the number of the data packets in each buffer as a state. thus"
"step 3 (modeling the structure of the mlfnns). the numbers of inputs and the number of outputs are determined according to the characteristics of the dataset. the number of inputs is equal to the number of attributes of the dataset. similarly, the number of outputs is equal to the number of classes of the dataset. the number of hidden layers is set to one for all problems and the number of nodes in the hidden layer is determined with ga, which is reported in a previous study by the authors [cit] ."
"as shown in table 4, the running time of the proposed mmpso algorithm is shorter than that for pso for all datasets. in addition, the proposed mmpso algorithm is suitable for parallel implementation and the runtime of the mmpso algorithm can be reduced to a much shorter time with parallel programming. finally, the performance of the proposed mmpso algorithm is compared with the performance reported in the literature for the hsa [cit], kha, ga [cit], and the fireworks algorithm (fwa) [cit] which split the data into 80% training and 20% testing, for six datasets. in order to make this comparison under the same conditions, six datasets are split into 80% training and 20% testing for this experiment. the proposed mmpso algorithm is executed ten times and the best results are selected. the proposed mmpso algorithm was executed with the same parameters as described in the previous experiment. the results of the 80% training and 20% testing experiment of mmpso algorithm and the literature reports for six datasets are shown in table 5 . when the results of the 80% training and 20% testing experiment in table 5 are analyzed, it is clear that the proposed mmpso algorithm yields better results than the other four metaheuristics according to sse, training ca and testing ca values for the iris, diabetes, and thyroid datasets. although the proposed mmpso algorithm obtained better sse and training ca results than other metaheuristic algorithms, it could not obtain the best testing ca result for the ionosphere and breast cancer datasets. for the glass dataset, the proposed mmpso algorithm obtained the best result only for the training ca. in summary, when looking at the results of the comparison in table 5, the proposed mmpso algorithm performed better classification results than other algorithms in general."
"therefore, the exact and asymptotic closed-form expression of the secrecy outage probability can be derived, and in order to obtain more insights, we will proceed with the investigation of the secrecy diversity gain, the average secrecy throughput and the delay in the following section."
"step 5 (testing the mlfnns). in order to determine the performance of the mlfnns training with metaheuristic algorithms, the classification accuracy is calculated according to (8) for each test dataset."
"when the results of the 5-fold cross validation experiment in table 3 are investigated, the mlfnns trained with the proposed mmpso algorithm obtained better sse, training ac, and testing ac results than the mlfnns trained with the pso algorithm for all datasets. as a result, these experimental 6 scientific programming results show that the proposed mmpso algorithm achieved good performance in the training process of the mlfnns in accordance with the pso algorithm. additional advantages of the proposed mmpso algorithm are that it searches the global space more efficiently and convergences the optimum results more rapidly. to provide these advantages, the proposed mmpso algorithm must minimize the fitness function sse more rapidly than the pso algorithm in the training process. the minimization of sses according to the iteration number in the training process is given in figure 2 for the lymphography, ionosphere, glass and diabetes datasets. for the lymphography, ionosphere, and glass datasets, the proposed mmpso algorithm better minimized the sse from the initial iteration to the end iteration. for the diabetes dataset, the proposed mmpso algorithm searches the global space more efficiently after the 90th iteration. furthermore, the proposed mmpso algorithm provides better sse for the initial iteration in all datasets."
"the mlsls scheme can increase the achievable secrecy rate by enlarging the difference between the quality of the main channels and the eavesdropper's channels. however, the impact of the buffer status is not taken into account in the mlsls scheme, hence the relay with good link quality may be selected frequently, and its buffer tends to be empty or full easily."
"furthermore, the exact and asymptotic cdf of γ sr m 1,n e can be regarded as the function of m 1,n, we denote it as p 1 m 1,n and p ∞ 1 m 1,n respectively. similarly, the exact and asymptotic cdf of γ r m 2,n de can be expressed as p 2 m 2,n and p ∞ 2 m 2,n . recalling the assumption of a symmetric channel scenario we considered, hence we have"
"furthermore, to determine the optimum structure of the mlfnns is an optimization problem and the classification accuracy of the mlfnns is directly affected by it [cit] . the determined structures of the wine 13 10 3 173 glass 9 12 6 198 shuttle-landing 6 8 2 74 ionosphere 33 4 2 146 balance-scale 4 5 3 43 breast cancer 9 8 2 98 diabetes 8 6 2 68 thyroid 21 12 3 303 mlfnns according to the ten benchmark datasets which are shown in table 2 . for the proposed mmpso algorithm and the pso algorithm, the acceleration constants 1 and 2 are set to 1.49 [cit], the random numbers 1 and 2 are generated in the range [cit], and the number of particles of a swarm is set to 20. for the proposed mmpso algorithm, the number of a swarm is set to three. for the initial population of the proposed mmpso algorithm and the pso algorithm, the weights of the mlfnns are generated at random numbers in the range [−10, 10]. all these experimental parameters are determined empirically. the sigmoid activation function is used in the hidden layer and the output layer of the mlfnns for training and testing. the maximum number of iterations is used as the stopping criterion. the classification process is applied with 5-fold cross validation, which estimates the mean of the sses obtained on five different testing subsets. the results of the 5-fold cross validation experiment of the proposed mmpso algorithm and the pso algorithm are shown in table 3 ."
"metaheuristic algorithms are generally used in many areas for solving different problems such as optimization, scheduling, training of anns, fuzzy logic systems, and image processing [cit] . in this study, we used two metaheuristic algorithms, the original pso and mmpso, which is a novel use of pso for determining the optimum weights of mlfnns."
") according to the current buffer status and then the weight is allocated to the corresponding link. next, this scheme selects the link with the largest weight among all secure and available links. from this proposed scheme, we can observe that the relay with the least packets in its buffer is selected at the first hop and the relay with most packets in its buffer is selected at the second hop. the mwsls scheme can be described as"
"where t is the number of iterations, is the inertia weight, i is the index of a particle in a swarm, and d is the dimension of the problem. each particle has a position and velocity for each dimension: is the position of the ith particle in dimension d, v is the previous velocity of the ith particle in dimension d, is the best fitness value of the ith particle, best is the best fitness value of all particles, 1 and 2 are two positive constants that represent the acceleration factor, and r 1 and r 2 are two random functions in the range [cit] . in (5), is the old position of the ith particle and +1 is the new position of the ith particle in the swarm. the pseudocode of the pso algorithm is given in algorithm 1 [cit] ."
"b. the stationary probability at state s n : π n now, we focus on the stationary probability π. firstly, we divide the sets of states which can be transferred from state s n within one step into two sets, denoted as 1 n and 2 n . specifically, if the source-to-relay link is selected, the buffer state will transfer from state s n to one of the states in 1 n . on the other hand, if the relay-to-destination link is chosen, the buffer state will transfer to another state in 2 n ."
"the application of the proposed mmpso algorithm and the pso algorithm to the mlfnns is implemented using c# [cit] . all experiments are carried out using a computer with an intel core i7 3840qm@2.00 ghz processor with 8 gb of memory with microsoft windows 8 operating system. ten different benchmark datasets from the uci repository [cit] are used to evaluate the performance of three metaheuristics, and the characteristics of these datasets are shown in table 1 . in general, the structure of mlfnns is represented by i-h-o where i is the number of nodes in the input layer, h is the number of nodes in the hidden layer, and o is the number of nodes in the output layer. the number of weights of the mlfnns with bias is calculated using (9), which represents the dimension size of the optimization problem at the same time [cit] ."
determining the optimal weights of mlfnns is a nonlinear optimization problem so metaheuristic algorithms can be used for mlfnns training. an application of metaheuristic algorithms to mlfnns training is explained step by step in the following text.
"in this paper, a novel mmpso algorithm is proposed for mlfnns training. the proposed mmpso algorithm based on mso technique has two advantages according to the pso algorithm. firstly, the proposed mmpso algorithm strengthens the particles to carry out a local search in the search space range. secondly, the proposed mmpso algorithm has multiple swarms and takes into account both the best solution of each swarm and the best solution of all swarms and thus it gets closer to the optimum solution. to evaluate the performance of the proposed mmpso algorithm experiments were conducted on ten benchmark datasets from the uci repository. according to the experimental results, the proposed mmpso algorithm yielded better performance than pso for all datasets. furthermore, the obtained experimental results were compared with the previous researches in the literature for six datasets. according to this comparison, the proposed mmpso algorithm showed a competitive advantage over the reported algorithms. in conclusion, the proposed mmpso algorithm showed good performance and can be adopted as a novel algorithm for mlfnns training."
"data-intensive mobile apps often rely on data located in the cloud. however, access to this data is likely over a lower-bandwidth and multi-hop connection, compared to the higher-bandwidth, single-hop connection that exists between a mobile device and a surrogate. pre-fetching anticipates data needs in order to minimize communication to the cloud and reduce latency. the surrogate, according to a defined pre-fetch algorithm, retrieves data from the cloud and stores it locally so that it is available to the mobile device when it needs it. access to the cloud is therefore only necessary when the data is not already available on the surrogate. figure 3 presents the main components of this tactic. the pre-fetching tactic requires a data staging client that runs on the mobile client and a data staging manager that runs on the surrogate. the data staging client handles all data operations on behalf of a cyber-foraging-enabled mobile app. before sending the data operation to the data staging manager, the data staging client captures and also sends along any pre-fetch hints that are used by the pre-fetch algorithm to determine and anticipate data needs. examples of pre-fetching hints include mobile device location, user profile and preferences, and the user's schedule. the data staging manager first executes the data operation against the local cache. if the operation is successful it returns the results of the data operation. if the operation is not successful the data staging manager obtains the data from the cloud data repository in the enterprise cloud, stores it in the local cache, and returns the results of the data operation to the mobile client. asynchronously, either periodically or triggered by certain conditions, the data staging manager will use the pre-fetch hints from the mobile client and any local data such as the user's access history as parameters to a pre-fetch algorithm that will calculate the data set that is likely to be needed next by the cyber-foraging enabled mobile app. it will then retrieve this data set from the cloud data repository and store it in the local cache so that it is available when it is needed by the cyber-foraging enabled mobile app. similarly, either periodically or in response to certain conditions, the data staging manager will sync the cache with the cloud data repository to ensure that data is consistent locally and remotely."
"a scenario for computation offload from a mobile device to a surrogate is the following: the user of a mobile device executes a cyber-foraging-enabled mobile application. the application offloads the computation-intensive portions of the application to a nearby surrogate, with minimal disruption to the mobile device user. computation offload extends battery life by offloading computation-intensive portions of an application to nearby surrogates with greater computation power. in addition, the single-hop proximity of surrogates combined with the use of wifi or short-range radio instead of broadband wireless (e.g., 3g/4g) also decreases latency [cit] and improves the user experience especially for highly-interactive applications. figure 2 shows the main components of this tactic with numbers that indicate the sequence of operations. the computation offload tactic requires an offload client running on the mobile device and an offload server running on the surrogate. this pair of components communicates to coordinate the offload operation. the cyber-foraging enabled mobile app invokes the offload client when it encounters a portion of code that has been identified as offloadable computation and passes it any app metadata that is required to set up the offloaded code. the offload client then coordinates with the offload server to set up the offloaded code so that it can be invoked by the cyber-foraging enabled mobile app. the offloaded code runs inside a container on the surrogate. examples of a container are a virtual machine, application server, web server, or the operating system. figure 2 shows the cyber-foraging enabled mobile app communicating directly with the offloaded code. an alternative is for the cyber-foraging enabled mobile app to always communicate through the offload client. this latter alternative has the potential for performance problems as the number of mobile clients using the surrogate increases. this is because the offload server becomes a bottleneck as all communication between mobile devices and the surrogate would go through this component. however, some systems that implement fault tolerance tactics (section 4.2) place the responsibility of detecting and managing disconnections in the offload client and offload server which therefore benefits from the single point of communication of the latter alternative. an example of the computation offload tactic is in the mobile agents system [cit] in which applications are manually partitioned into components that have to be executed locally and components that can be offloaded. these offloadable components are set up as mobile agents using the java agent development environment (jade). at runtime, the system determines if the agent marked as offloadable should be offloaded based on a comparison of local and remote execution times. variation: stateful computation offload. the computation offload tactic assumes that the offload operation is stateless. this means that no mobile app state needs to be transferred between the offload client and the offload server during the offload operation. this is what happens when the granularity of the offload operation is a module or class, a service, or a complete application (or server portion of an application). when the granularity of the offload operation is at the process or at the method level, the state of the program or object that contains the process or method being offloaded has to be transferred to the equivalent program or object on the surrogate. in this case, a state synchronization operation is invoked either periodically or on-demand before the offloaded code is executed to guarantee that the state is equivalent on both sides. an example of the stateful computation offload tactic is in the clonecloud system [cit] ."
"the local fallback tactic assumes that there is equivalent code for the offloaded computation on both the mobile device and the surrogate. because disconnection may happen at any point in the offload process, this tactic is best fit for stateless request-response operations that can be restarted on the mobile device if the operation fails. systems that implement the just-in-time containers tactic (section 4.3.1) with the local fallback tactic would require a component or a periodic clean-up process that destroys containers that are not being used in order to reduce the load on the surrogate. systems that implement the opportunistic mobilesurrogate data synchronization tactic need to be aware of the energy consumption on the mobile device for keeping data synchronized. also, while disconnected, it is possible that data may not be up-to-date, which may lead to incorrect results for applications that operate on time-sensitive data. the cached results tactic is best fit for asynchronous interactions between mobile devices and surrogates or applications that are not time-sensitive or require immediate results. in addition, it requires a mechanism for detecting disconnection from mobile devices. the alternate communications tactic assumes that the mobile device is enabled to use the alternate communication mechanism. in addition, depending on the type of interaction between the surrogate and the mobile device (i.e., responding to a single offload request or sending data periodically to the mobile device), the surrogate would require a mechanism to determine when connectivity has been restored so it can go back to the default communications mechanism. finally, the eager migration tactic requires the source and target surrogates to be connected. the impact on the user experience will highly depend on the bandwidth between surrogates. in addition, the system has to obtain any parameters for the algorithm that determines potential disconnection, such as the distance and communications quality between the mobile device and both the source and target surrogate."
"in an operational cyber-foraging scenario a single surrogate may support multiple mobile users. however, not all mobile users are offloading the same computation. some users may be executing a small task that does not require a large quantity of surrogate resources while others may be executing very computation-intensive tasks that require much more resources. to optimize resources on a surrogate, and therefore support a greater number of offload requests, the right-sized containers tactic creates a container for the offloaded code that is of the smallest size possible in order to run the offloaded computation, based on computation requirements metadata related to the offloaded code. in the thinkair system [cit], when a surrogate receives an offload request, the thinkair framework on the surrogate determines the configuration of the vm (or vms) to allocate for the task based on app requirements in the offload request that indicate the need for extra computing power (the sys-tem has six vm configurations which differ in terms of cpu and memory)."
"the just-in-time containers and right-sized containers tactics have a greater startup time than tactics in which the offloaded code is already running because they have to set up the container, which is the execution environment for the offloaded code. in addition, the right-sized containers tactic requires a surrogate to maintain different container configurations. the surrogate load balancing tactic requires the source and target surrogates to be connected. the impact on the user experience will highly depend on the on the bandwidth between surrogates. the source surrogate requires a mechanism to access the load level of all connected surrogates (or an external manager that maintains this information) in order to migrate computation to the less-loaded surrogate and keep the load on all the surrogates balanced."
"in an operational cyber-foraging scenario a single surrogate may support multiple mobile users. to decrease the load on a surrogate, and therefore support a greater number of offload requests, the just-in-time containers tactic creates a container and/or an instance of the offloaded code upon receipt of an offload request and then destroys the instance of the offloaded code when the offload request is completed. in the grid-enhanced mobile devices system [cit] a deputy object is created for each offload request (task) from a mobile device in the grid gateway. when the task is completed and the mobile device terminates the connection to the grid gateway, resources on the surrogate are released and the deputy object is destroyed."
"in the local surrogate directory tactic (section 3.4.1) the mobile device is responsible for populating and maintaining the list of surrogates on which it can offload computation. this is a rather static solution because as more surrogates become available in the environment there is no automated way of discovering these new surrogates or updating their metadata as changes occur. maintaining the surrogate directory in the cloud has the advantage of a centralized location for surrogate registration and metadata. all the mobile device needs to know is the network address of the cloud server that manages the surrogate directory. in addition, optimal surrogate selection algorithms can run in the cloud, which is an additional offload operation that can lead to battery savings on the mobile device. regarding trust, in this tactic the mobile device only needs to trust the cloud surrogate directory server assuming that the directory only contains trusted surrogates (section 4.4.1). in the mobile agents system [cit] the mobile device contacts a cloud directory service to get a list of available surrogates and selects the one with the highest communication link speed with the mobile device as well as the highest computing power."
"pre-provisioned surrogates have the advantage of shorter response time to offload requests from mobile devices because the offloaded computation or data staging elements already reside on the surrogate. in an operational setting in which surrogates support multiple clients, a surrogate should have minimal management capabilities that (1) help surrogate administrators to install capabilities and appropriate execution containers, and (2) maintain a list of these capabilities (similar to a service registry). this tactic requires a surrogate manager component to manage a capabilities repository, capability metadata to enable setup of capabilities on demand, and a capability registry that is used by surrogate discovery tactics (section 3.4) for advertising capabilities to mobile cyber-foraging clients. this tactic is not present in any of the systems, but could be integrated into any of the cyber-foraging systems that assumes that offloaded computation and/or data staging elements are already available on the surrogate at runtime."
"due to mobile device mobility or decrease in the quality of the communications channel between the mobile device and the surrogate, the mobile device might lose connectivity to the surrogate. the local fallback (section 4.2.1), cached results (section 4.2.3), and alternate communications (section 4.2.4) tactics for fault tolerance are reactive; that is, they perform a corrective action after the disconnection is detected. the eager migration tactic takes a more proactive approach and migrates the offloaded computation to a connected surrogate before it becomes disconnected from the mobile device so that it can continue operating. in the offloading toolkit and service system [cit], if the communication between the surrogate and the mobile device deteriorates based on reaching an established threshold for connection quality, the execution of the offloaded code is terminated on the current surrogate and migrated to a connected target surrogate."
in pre-provisioned surrogates (section 3.3.1) a mobile device can only execute applications that already exist on the surrogate. provisioning the surrogate from the mobile device has the advantage of enabling the execution of a greater number of applications because surrogates are provisioned at runtime. the mobile device sends the offloaded computation to the surrogate at runtime. the surrogate installs the computation inside an execution container and starts the application on behalf of the mobile device. in the vm-based cloudlets system [cit] application overlays are sent at runtime to the surrogate (cloudlet) and combined with a base vm to produce a vm with the running application.
"variation: intermediary cloud surrogate directory. the cloud surrogate directory tactic returns the address of the selected surrogate to the mobile device, which then contacts the surrogate directly. in systems such as large-scale mobile crowdsensing [cit] the cloud server does not return the surrogate address to the mobile device, but rather forwards the offload request to the selected surrogate and then returns the results to the mobile device. in this variation the cloud server acts as an intermediary between the mobile device and the surrogate."
"variation: dynamically-sized containers. the thinkair system [cit] also implements this tactic. if an error occurs at runtime that would indicate that the vm does not have the necessary computing power for the task, such as an outofmemoryerror error, the system starts a more powerful vm and moves the offload request to the newly started vm."
"the local surrogate directory (section 3.4.1) and cloud surrogate directory (section 3.4.2) tactics require a directory of potential surrogates to be maintained either on the mobile device or on a cloud server, respectively. having surrogates broadcast their availability and metadata to mobile devices removes the burden of having to maintain surrogate directories up to date. it creates a much more dynamic environment in which mobile devices can discover nearby surrogates without needing to know their addresses in advance or retrieving the addresses from a cloud server that could potentially not be available when needed. in the vm-based cloudlets system [cit] surrogate information that includes surrogate address is broadcast using an implementation of zeroconf (http://www.zeroconf.org/)."
any trust mechanism will be constrained by how the trust relationship is established. password-based approaches such as those employed by systems in which surrogates are owned by the mobile device user require users to be registered on the surrogate. hardware-based approaches such as tpm (trusted platform module) require surrogates to have tpm chips on them. systems that rely on third parties have to be connected to online authorities or require certificates and keys to be obtained from a central certificate authority.
"data-reliant cyber-foraging systems, as their name indicates, rely on stored data to fulfill their operations. the opportunistic mobile-surrogate data synchronization tactic keeps data synchronized during periods of connection such that the system can continue operating in periods of disconnection. there are no systems in the primary studies that implement this tactic for fault tolerance as described, but the principle of using distributed storage in the virtual phone system [cit], for example, is the same: to opportunistically keep data/state synchronized without placing the responsibility on the actual applications."
"the local surrogate directory tactic places the responsibility of surrogate identification on the mobile device user. if surrogate metadata changes or new surrogates are made available, a cyber-foraging system will not have an automated way of updating the surrogate directory. the cloud surrogate directory tactic requires the mobile device to know the address of the cloud server that holds the surrogate directory. the cloud server can become a single-point-of-failure if it becomes unavailable to mobile devices. in the cases that the cloud server acts as an intermediary it also becomes a potential bottleneck. the surrogate broadcast tactic offers the most flexibility but requires an agreement between mobile devices and surrogates on the broadcast protocol. regarding trust, mobile devices will require additional components to determine whether broadcast information is coming from a valid, trusted surrogate (section 4.4.1)."
"in order to leverage cyber-foraging, mobile devices need to be able to locate available surrogates on which to offload computation or stage data. a scenario for surrogate discovery is as follows: a mobile device needs to execute a computation-intensive task and has already decided that it will offload the task to a surrogate. the mobile device is able to locate all nearby surrogates and selects the surrogate that is the best match for the offloaded task."
a scenario for scalability/elasticity is the following: a mobile app is enabled for cyber-foraging and is leveraging a surrogate for computation offload that is also being leveraged by other mobile apps on other mobile devices. the surrogate is able to optimize computing resources either locally or by leveraging other connected surrogates so that multiple mobile devices can be supported with the goal of minimal effect on user experience due to surrogate load.
"variation: lazy migration. in eager migration the offloaded computation fully moves from a source surrogate to a target surrogate and the mobile device continues its interaction with the target surrogate. in lazy migration, the execution of the offloaded computation remains on the source surrogate but the interaction with the mobile device is handed off to the target surrogate. this means that all interaction between the mobile device and the source surrogate goes through the target surrogate that acts as an intermediary. this tactic is not present in any of the systems but was considered as an alternative for the offloading toolkit and service system [cit] . it was not selected because of the high bandwidth that already existed between surrogates to enable a fast full migration."
"as far as architectural tactics for cyber-foraging, this is the first attempt to codify design decisions in software architectures for cyber-foraging systems into a set of tactics."
"a scenario for data staging is the following: a mobile application is being used by multiple users to collect data in the field. upon detection that it is close to a surrogate, the mobile application offloads the collected data. when the operation is complete, the mobile device deletes the transmitted data to free up storage space. in addition, when the surrogate establishes connectivity to the main data center in the cloud, it forwards the data that was collected by the multiple users, where it is integrated into the enterprise data repository. an additional capability of the application is to provide data visualizations pertaining to the data collected by the user, the data collected in the region that is served by the surrogate, and the data collected by the entire set of users. therefore, data is pushed from the enterprise data center to the surrogate either on-demand or periodically so that the data is closer to the user and accessible even if the surrogate is disconnected from the enterprise."
a scenario for fault tolerance is the following: a mobile app is enabled for cyber-foraging and is leveraging a surrogate for computation offload. during the execution of the remote computation the mobile device loses connectivity to the surrogate. the mobile device detects the situation and executes the local copy of the computation instead with minimal effect on user experience.
"for mobile devices to leverage nearby surrogates they need to know where the surrogates are located; that is, they need to know their network address (i.e., surrogate ip address or url). a simple solution is for mobile devices to maintain a list of potential surrogates including any information that can help the mobile device to select the best surrogate in case more than one is available. the list can be static, or updated based on network conditions or offload execution data. an advantage of a local list is that it will potentially include only surrogates that are trusted by the mobile device. this tactic has two parts: one part involves a user interface to populate and maintain the surrogate directory; the other part involves the components that interact during the offload process to obtain the list of surrogates from the directory and ping each to see if it is available for offload. the cuckoo system [cit] has a component that maintains a list of surrogates. if the surrogate has a visual display, upon loading it shows a qr code that is read by the mobile device and then added to the list of surrogates it can use for offload. if it does not have a visual display, the resource description file for the surrogate has to be copied to the mobile device so that it can be added to the list."
"to be able to use a surrogate for cyber-foraging, it has to be provisioned with the offloaded computation and/or the computational elements that enable data staging. a scenario for surrogate provisioning is as follows: a mobile device needs to execute a computation-intensive task. instead of executing the task locally, it locates a surrogate and sends it a request to execute the computation on its behalf. the surrogate first checks if it already has the computation to support the task. because it does not, it sees if it can locate the computation in a cloud repository. because the surrogate is not able to locate the capability in the cloud, the mobile device sends the computation to the surrogate for installation. once the surrogate installs and starts the computation it notifies the mobile device that it is ready, executes the computation, and sends back the results of the computation."
we presented a set of architectural tactics for cyber-foraging that were obtained from the results of an slr in architectures for cyber-foraging systems. common design decisions present in the cyber-foraging systems were codified into architectural tactics for cyber-foraging and then grouped into functional and non-functional tactics. functional tactics provide the basic cyber-foraging operations and nonfunctional tactics are combined with the functional tactics to support required system qualities. each tactic has tradeoffs that were briefly presented as observations in each tactic category.
"cyber-foraging is an area of work within mobile cloud computing that leverages external resources (i.e., cloud serpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. vers, or local servers called surrogates) to augment the computation and storage capabilities of resource-limited mobile devices while extending their battery life. there are two main forms of cyber-foraging. one is computation offload, which is the offload of expensive computation in order to extend battery life and increase computational capability. the second is data staging to improve data transfers between mobile devices and the cloud by temporarily staging data in transit."
"systems that implement the runtime partitioning tactic (section 4.1.1) require developer input or static profiling to obtain the values or models that are used in the calculation of the optimization function that determines whether code should run locally or remotely. however, models tend to be inaccurate because (1) applications are not deterministic; (2) smartphones scale the cpu's voltage dynamically to save energy (i.e., dynamic voltage scaling); (3) energy models highly depend on hardware configuration, usage, and even the battery model of a mobile device; and (4) network quality is highly variable and often unpredictable [cit] . to account for this variability and take into consideration current conditions, once the offload operation ends, or periodically, the system updates the profiling data and models that are used by the optimization functions. in the maui system [cit] a solver+profiler component uses data from the annotated method (inputs, outputs and cpu cycles), the device energy model, network data obtained via a network monitor, and past program execution and network data to compute an energy-efficient program partition. once an offloaded method terminates, it updates the past program execution and network data to better predict whether future invocations of the method should be offloaded."
the computation offload tactics assume that offloaded computation already exists on the surrogate (loaded on the surrogate via a surrogate provisioning tactic at deployment or run time (section 3.3)) and that computation that is marked for offload is always offloaded. combining computation offload tactics with resource optimization tactics (section 4.1) enables the system to determine when it is optimal to offload and when not. the tactics also assume that the surrogate is always available for offload. combining the computation offload tactics with fault tolerance tactics (section 4.2) enables the system to deal with unavailable surrogates.
"in the trusted and unmanaged data staging surrogates system [cit] data is staged on a staging server in the surro- after the cache has been loaded with an initial data set, all data operations are routed to the staging server. if the requested file exists in the cache then the data operation takes place locally on the surrogate. if the file is not available in the cache it obtains the file from the file server and stores it in the cache, along with any other files that are predicted to be required based on the request."
"the pre-provisioned surrogate tactic requires a management component that provisions the surrogate with capabilities before surrogate deployment. the surrogate provisioning from the mobile device and surrogate provisioning from the cloud tactics requires a pre-established agreement on the format of the offloaded code (e.g., java class, python script, windows application). in addition, depending on the size of the offloaded code the tactics may require additional components to provide reliable communications. finally, in the surrogate provisioning from the cloud tactic the computation has to exist at the indicated location."
"in an operational cyber-foraging scenario the relationship between mobile devices and surrogates may be many-tomany, meaning that multiple mobile devices may be leveraging multiple surrogates for computation offload and data staging. the surrogate load balancing tactic enables surrogates to send offloaded computation or data to other lessloaded, connected surrogates in order to provide a better user experience to all mobile devices. in the cloud operating system to support multi-server offloading (cos) system [cit] application modules are implemented as salsa actors that are self-contained and therefore can easily migrate between a source surrogate and a target surrogate. when the source surrogate reaches a load threshold, it informs the cos manager, which determines the optimal target surrogate based on resource availability, communication cost with other actors, and the cost for migration, and prepares it for migration."
"the next section introduces the architectural tactics for cyber-foraging, grouped into functional tactics (section 3) and non-functional tactics (section 4). section 5 presents related work. section 6 concludes the paper and outlines the next steps in our research."
"all the data staging tactics require a surrogate provisioning tactic (section 3.3) to prepare the surrogate for data staging. in addition, they require a configuration in which the mobile device is connected to a surrogate and the surrogate is connected to the enterprise or cloud data center, even if connectivity is intermittent or periodic. combining data staging tactics with fault tolerance tactics (section 4.2) enables implementations in disconnected or intermittent environments. finally, if data is being modified on the mobile device, as in potentially the pre-fetching tactic, there needs to be a mechanism in place, either manual or automatic, to resolve any synchronization conflicts between surrogate caches and cloud repositories."
"the tactics were extracted from the literature based on (1) common components found in the studies, (2) quality attributes explicitly stated in the studies, and (3) quality attributes inferred from system and component descriptions. common design decisions were codified into architectural tactics and grouped into functional and non-functional tactics. functional tactics are broad and basic in nature and correspond to the architectural elements that are necessary to meet cyber-foraging functional requirements. non-functional tactics are more specific and correspond to architecture decisions made to achieve certain quality attributes. each tactic category presented in the paper will include a scenario, a short description of each tactic in the category including an example system, and general observations about constraints and tradeoffs. the computation offload tactic and the pre-fetching tactic for data staging will be explained in greater detail because many of the other tactics build on these two basic tactics. the full catalog of tactics is available as a technical report [cit] . the goal of the catalog is to serve as a reference for architects designing cyber-foraging systems."
"provisioning surrogates from the mobile device has the advantage of enabling the execution of a greater number of applications (section 3.3.2) compared to pre-provisioned surrogates (section 3.3.1). however, the size of the computation that is sent to the surrogate at runtime can be significant. in the examples for the maui system [cit], the size of the .net components transmitted at runtime is between 0.2 mb and 13.8 mb. in the examples for the vm-based cloudlets system [cit], the size of an application overlay is between 63 mb and 196 mb. an alternative is to send the location of the computation in the form of a url for the surrogate to download and install. the payload in this case is almost insignificant but the time to provision may be longer due to potentially higher and unpredictable latency between the cloud and the surrogate. however, the mobile device is not consuming battery due to high transmission costs. in addition, because the computation exists in a defined place in the cloud it is easier to update because it does not have to be sent to each mobile device after patches or upgrades. in the collective surrogates system [cit] a shell script is sent to the surrogate at runtime that downloads the application that corresponds to the offloaded code from an application repository on an internet server, installs the application and starts it."
"a scenario for runtime optimization is the following: a mobile app is enabled for cyber-foraging. upon request for execution of computation that has been targeted for offload, the mobile app first checks if it is better from a performance and latency perspective to execute the computation locally or remotely. given that the network conditions between the mobile device and the surrogate are not ideal, the computation is executed locally instead of offloaded to the surrogate."
"when a mobile device discovers a surrogate it expects a trustworthy surrogate execution environment, meaning that once an offload operation starts, code and data are not maliciously modified or stolen and that it provides trustful services. in the same way, a surrogate expects that a mobile device is a valid client and that it will not offload malicious code or use it as a vehicle to other code and data offloaded by other mobile devices. the trusted surrogate tactic adds this trust element to the interaction between a mobile device and a surrogate. the only system that implements a trust solution that uses a third-party trusted authority is the trusted and unmanaged data staging surrogates system [cit] . the user's idle desktop serves as the trusted third party that sits in between the file server and the surrogate. when the mobile client requests a file, it communicates with the data pump that runs on the desktop to obtain the key and hash for the requested data file. the data pump retrieves the data file from the file server and encrypts it before sending it to the surrogate for staging. it then sends the mobile client the key and hash for the file so it can be compared it to the hash of the file that is retrieved from the surrogate to determine if the file has been tampered with."
"due to movement of a mobile device to an area with no connectivity to the surrogate, problems with network quality, or service disruption, the mobile device may lose connectivity to the surrogate during the computation offload or data staging process. the local fallback tactic enables the cyber-foraging enabled mobile app to detect loss of connectivity and revert to local execution of the offloaded element. the maui system [cit] detects failures using a simple timeout feature that returns control back to the mobile device if a disconnect occurs and resumes running the method on the local smartphone. after every offload operation, maui returns program state as part of the results, which is applied to the local computation so that state is synchronized between the local and remote computation."
"offload requests from mobile devices are not always as simple as request-response interactions. some requests may take a long time to execute or may rely on data that has been gathered and maintained over time. in the case of disconnection between a mobile device and a surrogate during an offload operation, restarting the offload request or losing data is not desired. the cached results tactic enables a system to cache results and state on a surrogate until the mobile device is able to reconnect. in the grid-enhanced mobile devices system [cit] the mobile device periodically sends a keep-alive message to the surrogate to inform that it is still connected. before sending the results back to the mobile device, the surrogate checks the device status and if disconnected saves the results in a cache. when the mobile device reconnects, the surrogate gets the results from the cache and sends them back to the mobile device."
"this material is based upon work funded and supported by the department of defense under contract no. fa8721-05-c-0003 with carnegie mellon university for the operation of the software engineering institute, a federally funded research and development center. this material has been approved for public release and unlimited distribution (dm-0002040)."
one of the main findings from the primary studies is that there is very little discussion of system-level concerns that have to be addressed when moving from experimental prototypes to operational systems. one of these system-level concerns is security [cit] . a scenario for security is the following: a mobile app is enabled for cyber-foraging and is in the process of discovering a surrogate for computation offload. user and surrogate credentials are exchanged and validated before the offload process so that the mobile app and surrogate can interact according to agreed security policies.
"variation: client-side data caching. the tactic as described caches results on the surrogate and sends them to mobile clients upon request or reconnection. a variation of this tactic that is useful for data staging systems that implement the in-bound-pre-processing tactic (section 3.2.2) is to cache collected data on the mobile device and send it to the surrogate upon reconnection. the feel the world system [cit] collects sensor data that can be aggregated and/or transformed locally on the mobile device and uploaded to the surrogate in real-time if the connection is available, or at a later moment if it is unavailable."
"the next steps in our research are to create case studies that validate these tactics in real systems to demonstrate that they satisfy the functional and non-functional quality attribute responses that they are intended to promote. because tactics are not used in isolation, but rather combined to satisfy system requirements, the case studies will be analyzed to identify tactics that are commonly used together and codify them into architectural patterns [cit] for cyberforaging systems."
"data-intensive mobile apps are often used to collect data in the field, where internet connectivity might not be available to mobile devices or might be costly. in addition, although the field-collected data is valuable, it might be overwhelming for a device to transmit all data collected to the enterprise, especially if internet connectivity is a scarce resource. in these cases, a surrogate can pre-process -clean, filter, summarize, or merge -the data that is received from the mobile devices that it serves such that the data that is sent on to the enterprise cloud is ready for consumption and serves an immediate need. the mobile device can also batch data according to user or application preferences to conserve the energy spent on turning the radio on and off for communication. data collected on the surrogate can be uploaded to the cloud when network connectivity is available. in the large-scale mobile crowdsensing system [cit] crowdsensing participation apps gather data from one or more sensors on the mobile device and send them to a surrogate. applications running on the surrogate process the data streams coming from mobile devices locally and/or format and send the data to applications in the cloud."
"the runtime partitioning and runtime profiling tactics assume that there is equivalent code for the offloaded computation on both the mobile device and the surrogate. this aspect limits the direct reusability of legacy code because a version would have to be written for the mobile device or surrogate depending on the original platform of the legacy code. in addition, the optimization function should not be a computation-intensive task because it would then cancel the benefits of cyber-foraging. finally, data collection of app metadata to be used as optimization function parameters has to be gathered in advance using techniques such as static profiling. for the runtime profiling tactic, the cost of profiling is not negligible and can impact overall application performance [cit] . system designers need to consider the type and frequency of data to capture at runtime. finally, the resource-adapted computation tactic requires developing, profiling and maintaining different versions of offloadable elements."
"variation: user-guided runtime partitioning. the runtime partitioning tactic assumes a static optimization function. however, in some systems what to optimize is determined based on user preferences or input. in the powersense system [cit] the user can select a time saver option to minimize processing time or an energy saver option to minimize energy consumption. the system has a user interface on the mobile device to set these preferences."
"in the runtime partitioning tactic (section 4.1.1) a decision is made at runtime to execute code locally or remotely depending on an optimization function. in this tactic the local and remote code are identical. even though this makes development and versioning easier, computation ends up being limited to what can execute on the mobile device, which will always lag behind static elements such as surrogates in terms of compute resources (power, cpu, memory, storage) [cit] . resource-adapted computation enables cyberforaging systems to fully take advantage of the computing power of surrogates by adapting the computation to the resource on which it will be executing. in an image processing scenario, the object recognition algorithm that runs on the surrogate can be much more computation-intensive than the one that runs on the mobile device and can therefore deliver a much more precise result. in the cuckoo system [cit] the cuckoo framework generates an implementation of the same interface for a local and a remote service. initially, the remote implementation will contain dummy method implementations, which the developer has to replace with real method implementations that can be executed at the remote location. the real methods can be identical to the local service implementation, but may also be completely different, because the remote implementation can run a dif-ferent algorithm, use different libraries, or take advantage of parallelization on the more powerful surrogate."
"various simulations were carried out at 915 mhz to test the applicability of the proposed hyperthermia system model. two patch antennas having different substrate materials were used to irradiate three different head models and the field distribution inside the proposed chamber, with and without the presence of the human head model was computed. sar and thermal profiles were also calculated in different brain tissues as well as in the tumor."
"there have been many studies involving employee turnover analysis, for example, some studies [cit] have focused on management, sociology, psychology, etc. with the popularity of social media in real life, online professional social networking sites such as linkedin 1 and maimai 2 provide a large amount of personal career information. some studies [cit] have started to use data-driven methods to predict employee turnover behavior. however, existing research has mainly focused on the impacting factors, such as personal attributes, while ignoring employees' social network structure, which has led to the lack of some important information."
"for the link prediction task, we chose the public amazon 3 transaction dataset which consists of 278,677 comments from different customers on products, including the customers' ids, products' ids, and time of the comments. another dataset is the taobao 4 online user purchase records dataset provided by ant financial services. the subset we select contains 338,781 records with user_id, item_id and time_stamp information. we randomly select 70% of the dataset to train the embedding vectors of vertices and the remaining 30% for testing. to conduct the link prediction task, we also randomly generat an equal number of vertex pairs that are not connected as the negative examples."
"again, another observation worth noting is that the temperature distribution does not coincide with the sar distribution in brain tissues since the highest temperature exists in the tumor in the sagittal plane (see figure 12b, figure 16b ). in addition, higher temperatures are observed in the tumor compared to those computed when the head was irradiated by an antenna with a silicone substrate placed in the hyperthermia chamber model (figure 14) ."
"through the bipartite graph, it is easy to see that the employee turnover prediction problem can be transferred into an edge label prediction problem on the bipartite graph."
"for the input sequences of vertices, skip-gram can capture high-order proximity by assuming that vertices that frequently occur together in the same context of a sequence should be more relevant. we can obtain the embeddings of vertices by maximizing the log-likelihood probability:"
"to solve this problem, we first lean the network-based features as low dimensional vectors for employees and companies through dbge, and combine them with their basic information. then we feed the features to traditional machine learning classification algorithms to generate the employee turnover predictions."
the return loss s 11 for both antennas was calculated and their dimensions were optimized to fine tune the antennas to resonate at the desired frequency. figure 3 shows the return loss for the patch antenna with a silicone substrate when the antenna is placed at one of the focal points of the hyperthermia chamber model. a value of approximately −17 db is observed for the s 11 at 915 mhz.
"to the best of our knowledge, few studies have considered time dependence in bipartite graph embedding and used it to solve the employee turnover prediction problem. in order to solve the dynamic bipartite graph embedding problem, we extend the random walk method and propose a biased random walk method called horary random walk, which takes the chronological order of the edges into consideration when conducting random walks. after a horary random walk, each vertex in the bipartite graph is associated with a vertex sequence. then, we use the skip-gram model, which has been proven to have good embedding performance, to learn low dimensional vector representations from the sequences."
"through the dbge approach, we obtain low-dimensional vector representations of the vertices in the bipartite graph that preserve the chronological order. then we combine the network-based feature vectors with employee and company basic features, and feed them to various downstream machine learning models to solve the employee turnover prediction problem."
"in table 6, we summarize the prediction performance improvement of network-based features over the basic features under different learning models and evaluation metrics. from table 6, we see that ensemble learning and tree-based algorithms were greatly improved by dbge features. on the contrary, the logistic regression and naive bayes algorithms performed even worse with the dbge features, we think this is mainly due to the fact that the lr and nb models are not good at dealing with nonlinear features, while the features learned by graph embedding exhibit strong nonlinear characteristics. the most significant improvement is observed with the rf algorithm, where the corresponding acc, pre, recall, f 1 and auc measures are increased by 3.3%, 6.7%, 3.3%, 4.7%, and 3.3%, respectively."
"the semcad x version 14 (speag, zurich, switzerland), a commercial finite-difference time-domain (fdtd) based program, was used to compute the electromagnetic energy sar deposited in brain tissues and the tumor as well as the thermal profiles in these tissues. the sar is defined as the power absorbed into the unit mass of tissue."
"in the second configuration, the antenna with a foam substrate was placed at the other focal point opposite to the head ( figure 11a, b) . in this configuration, the 1:1 contrast between the tumor and the surrounding wm case was simulated. although the observations on the sar patterns were similar to those computed for the silicone antenna, much higher sar values are assessed in brain tissues. thus, indicating the advantage of using an antenna with a higher efficiency."
"based on the above considerations, in this paper we propose a graph embedding based approach. our solution is mainly motivated by the observation that employees' historical job records can be represented by a bipartite graph in which vertices are divided into two separate groups, i.e., employee vertices and company vertices, as shown in figure 1, from which we see three employees connected to four companies. an edge between an employee vertex and a company vertex indicates that the employee used to work in or is currently working at the company. each edge is associated with a timestamp and a label, where the timestamp indicates when the employee joined the company and the label indicates whether the employee quit the job after a specific timespan (e.g., one year). if the employee quit the job within the timespan, the label is 1, otherwise it is 0."
"where p r is the reflected power and p i is the input power. since s 11 is measured on a decibel scale, smaller s 11 indicates greater power coupled to the brain tissue. the 3d radiation patterns of the foam substrate antenna inside the hyperthermia system model with and without the presence of the head model were also computed."
"the sar was computed in the brain tissues when the deep seated tumor in the head was placed at one of the focal points of the hyperthermia chamber model system and the antenna was placed at the other focal point. in the first configuration, the antenna having a silicone substrate was used to irradiate the head (figure 10a, b) . localization of the sar was observed at the tumor in the axial and sagittal views, but the maximum sar was located at the ventricles in the sagittal view. sar distributions, although small compared to the ventricles, were also located in the outer thin csf layer of the head."
different chamber model sizes were simulated to ensure better focusing properties in the brain tumor. the final chamber size was selected based on the design that ensures a better localization and maximum sar deposition at the target region (tumor) in the head. the final model used in this study had a chamber major axis of 100 cm while the distance from the center of the major axis to the top of the chamber was 47. second focal point was selected where simulations have shown that the focusing properties into the tumor did not improve when sections were made at larger distances.
"it is worthwhile to note that the bipartite graph is highly dynamic and the temporal interdependence of an employee's past job records can affect her future turnover behavior. for example, if we observe that a person first worked in a company and then jumped to a university administrative department, it may be because she wanted a secure job, so it can be inferred that she had a lower probability of quitting her job over the next period of time. in contrast, for another person who shifted from one company to another company, this job-hopping may have been motivated by his aspiration for professional upgrading, so he would be more likely to leave again. consequently, how to embed the employee's work experience in a dynamic bipartite graph is a problem worth studying. in order to fully utilize the temporal information and solve the employee turnover prediction problem more effectively, in this paper we propose dbge, a new dynamic bipartite graph embedding-based approach."
the efficiency of an antenna is mainly dependent on the antenna's return loss s 11 -the ratio of power reflected to power input. s 11 is typically measured on a decibel scale as
"in order to assess how much the features learned by dbge contribute to the turnover prediction task, we calculate the importance of different features and output their rankings through the random forest learning model, as shown in figure 7, where bg_vec and bg_cmp_vec represent graph embedding features for employee and company vertices respectively. from figure 7, we can see that the bg_vec2 feature has the largest contribution to the employee turnover prediction task, followed by the bg_cmp_vec4 and bg_vec3, i.e., the top three most important features are all graph embedding features, showing the prediction power of our dbge approach. in sum, the above results indicate that our dynamic bipartite graph embedding based approach can significantly improve performance of the employee turnover prediction task."
"the focusing properties of the hyperthermia system model were investigated initially without the presence of the human head model. the electric field distribution inside the proposed system is depicted in figure 6 . it is observed that the radiated energy emitted by the antenna placed at one focus converges on the other focal point. thus, the geometrical focal point and the electromagnetic convergence area coincide. a 3-db focusing region of approximately 3 cm is observed in the absence of the head model. both antenna configurations show a similar electric field pattern inside the hyperthermia system model without the presence of the human head model."
"when a superficial tumor located in the head model was placed at the focal point of the chamber and irradiated by the foam substrate antenna, the proposed air filled hyperthermia chamber model was capable of heating the brain tumor using a single frequency ( figure 17a, b) . whole ellipsoidal chambers employed in another study [cit] to deposit energy into brain regions using different frequencies and dielectric matching layers may cause patients discomfort due to psychological factors compared to the hyperthermia chamber system proposed in this study."
"the focusing performance of the proposed hyperthermia system was tested by assigning different electrical properties to the tumor compared to the surrounding tissue due to uncertainties and the lack of precise values of these properties in the literature. several studies have shown that malignant tissue has a higher electrical conductivity and permittivity than normal tissue in the breast and the human liver at microwave frequencies [cit] . the difference between healthy tissue and tumor is attributed to the increased water content of the latter, which results in an increased permittivity and an increased conductivity [cit] ."
"liang zhao received the ph.d. [cit] . he is currently an associate professor with shenyang aerospace university, china. he has published more than 60 articles in international journal and conferences, including the ieee tpds, the ieee its magazine, the ieee tmc, the ieee icc, and so on. his research interests include vanets, sdvn, fanets and wmns."
"we utilized the fdtd method to design, model and simulate a low cost and easy to fabricate noninvasive air filled hyperthermia applicator system capable of heating deep seated as well as superficial brain tumors. accurate modeling of the head geometry (realistic 3d head model) is of vital importance since simple models do not correctly predict sar patterns. careful design and selection of the antenna as well as the partial half ellipsoidal chamber proved suitable for raising the temperature to sufficient therapeutic values in the target tumor within the brain. our results may form the basis for a clinical prototype. the operator of the proposed hyperthermia system only needs to place the center of the brain tumor at a pre-specified location (one of the foci) and excite the antenna at a single frequency of 915 mhz."
"several computational studies have proposed non-invasive hyperthermia applicators to heat superficial tumors [cit] . specifically, antenna arrays have been used for the application of hyperthermia to superficial regions of the head and neck [cit] . these applicators are not suitable for effectively heating deep seated brain tumors. focusing electromagnetic power into tumors deeply located in the brain possesses a challenge since high water content tissues such as blood and muscle absorb this power and rapidly attenuate wave propagation, thus preventing deep penetration into the brain. therefore, many research attempts were undertaken to develop applicators capable of noninvasively depositing electromagnetic energy into brain tissues without affecting surrounding healthy tissues."
theoretical studies [cit] and laboratory measurements [cit] were carried out to examine the possibility of using arrays of multiple antennas for heating deeper regions in a neck-mimicking cylinder as well as in phantom models [cit] developed a hyperthermia applicator using an array of multiple dipole antennas. the specific absorption rate (sar) was computed in a 3d model of the neck containing a lymph node tumor which served as the basis for their prototype. [cit] designed a reentrant cavity hyperthermia applicator to heat head and neck tumors using a homogeneous tissue phantom model to compute the temperature distribution in their model.
"the proposed system is designed to achieve focused power deposition in the tumor through constructive reflections off the upper chamber walls, but selective absorption of the electromagnetic waves takes place and causes power deposition in the ventricles as well. there is not good spatial correlation between sar and temperature, i.e., the peak temperature in certain brain tissues is not directly related to the peak sar value and location. similar observations were reported in other studies due to the exposure of the human head to microwave frequencies [cit] . this mismatch is attributed to the complex, multifactorial relationship between temperature and sar. the peak temperature not only depends on tissue properties such as thermal conduction, metabolic heat generation, the complex heterogeneous geometry of the brain structures, but also largely depends on the wide variance in blood perfusion rates of different tissues. the complex shape of the ventricles may also lead to a greater thermal diffusion surface area compared to the simple spherical tumor causing further reduction in the temperature."
"we have built the hyperthermia chamber system and the antenna with a foam substrate using simple inexpensive materials. preliminary proof of concept was achieved by placing another antenna at the location of the brain tumor and measuring the s 21 . in future work, experimental sar verification as well as temperature measurements in different tissues of a phantom head model would be conducted to test the effectiveness of the proposed hyperthermia chamber model."
"ziwei jin was born in jiaozuo, henan, china. she received the b.s. [cit] . she is currently pursuing the m.s. degree in computer technology with chongqing university. her research interests include social networks analysis, data mining, and graph embedding."
the rest of this paper is organized as follows: section ii introduces some related works; in section iii we describe our dynamic bipartite graph embedding approach in detail; section iv shows experiments and results; and section v gives the conclusion.
"from the dataset, we extract 9 features that are basic features for turnover prediction. these features can be divided into three categories: demographic features (gender, educational degree, etc.), current job features (industry type, start year, etc.), work experience features (years of working, number of previous turnovers), as shown in table 2 ."
"here, we choose deepwalk [cit] and node2vec [cit] graph embedding methods, which are also based on random walks and skip-gram models. we also include another deep learning-based graph embedding method, sdne [cit] ."
it is also worth noting that the sar value in the tumor was less than those computed in the two simpler head models due to the complexity of the head geometry and the presence of more brain structures.
"the focus of this study was to numerically design a non invasive hyperthermia applicator system capable of adequately heating deep seated as well as superficial brain tumors embedded in head models using the finite difference time domain method. the proposed hyperthermia system design is shown to be capable of effectively heating deep seated as well as superficial brain tumors using inexpensive, simple, and easy to fabricate constituents. to test the effectiveness of hyperthermia treatment, the specific absorption rate (sar) deposited in brain tissues must be high enough to produce sufficient temperature rise in brain tumors. the hyperthermia chamber system should be capable of rising the temperature of brain tumors to values above 42°c without harming surrounding healthy tissues. therefore, sar patterns and temperature distribution were computed and compared using two different antenna designs to irradiate three different head models."
"for each algorithm and each parameter setting, we repeat the experiment for 50 times and calculate the average and standard deviation for each metric to show the statistical significance of our results. table 4 shows the evaluation metrics with only the employee basic features, and table 5 shows the evaluation metrics after combining the dbge features. the subscripts (here referred to as k) in table 5 indicate that we use k-dimensional dbge features (after dimensionality reduction) together with basic features to train the corresponding leaning model."
"in addition, the hyperthermia applicator system should provide efficient power absorption distribution (sar) and selective heating efficacy in deep seated brain tumors. the sar is a measure of the rate at which electromagnetic energy is absorbed by brain tissues and is a vital quantity in assessing the effectiveness of hyperthermia. the temperature in brain tissues is usually computed by substituting the sar values into the pennes bioheat equation. knowledge of the temperature distribution in brain tissues is essential since the goal of hyperthermia is to raise the temperature of the tumor without affecting healthy brain tissues."
"our results also revealed that the sar patterns in brain tissues are not correlated in a simple manner to the temperature distribution in these tissues. comparing figures 10, 11, 12 and 13 with figures 14, 15, 16 and 17, we find that the temperature increase distribution does not coincide exactly with the sar pattern. in particular, although several hot spots are observed in the sar patterns, localized and smoother patterns are observed in the temperature distribution. these results are consistent with those of previous studies [cit] . the sar is known to be not very smooth due to the variation in the electrical conductivity between different brain tissues [cit] while the temperature rise distribution is rather smooth due to thermal transfer mechanisms between brain tissues."
"although much research has been done on graph embedding and many application scenarios have been found, to the best of our knowledge, graph embedding methods have not yet been applied to the employee turnover prediction problem."
we first compare the prediction performance of different machine learning models with and without dbge features to determine whether our proposed graph embedding method can solve the employee turnover problem more effectively.
higher sar values observed in the ventricles is due to the fact that these brain structures contain csf. using more tissues in the head model caused lower values of sar in the tumor compared to the previous head models.
"the temperature values were computed in the realistic 3d head model with its tumor placed at one of the focal points of the hyperthermia chamber model and the antenna with a foam substrate placed at the other focal point (figure 17a, b) . in this configuration, the brain tumor was placed at a superficial location inside the realistic 3d head model instead of being deeply seated within the brain. the~2:1 contrast between the tumor and the surrounding wm case was adopted. a localized temperature of 43.4°c was observed in the tumor while the temperature in the ventricles was less than 38.8°c. figure 15 temperature distribution (in°c) inside the deep seated tumor in the head model placed at one of the focal points of the hyperthermia chamber model and the antenna having a foam substrate placed at the other focal point for the 1:1 contrast case between the tumor and the surrounding tissue (control case) (a) axial view and (b) sagittal view. figure 16 temperature distribution (in°c) inside the deep seated tumor in the head model placed at the one of the focal points of the hyperthermia chamber model and the antenna having a foam substrate placed at the other focal point for the~2:1 contrast case between the tumor and the surrounding tissue (a) axial view and (b) sagittal view."
"to show the difference between horary random walk and traditional random walks, we give examples of random walks on dynamic ordinary graphs, static bipartite graphs and dynamic bipartite graphs, as shown in figure 3 in a dynamic ordinary graph, the walker follows strict chronological order while walking. when the walker arrives at a vertex v, it will check the unvisited adjacent edges of v and filter out those whose timestamp is smaller than the timestamp of the last walk and then visit the edge with the minimum timestamp. the walker stops when a length is achieved or when no available edges can be visited. one disadvantage of this random walk is that the generated sequences lack diversity and consequently lose much important information. an example sequence starting from v 1 6, v 7 as shown in figure 3(a), which ignores the fact that v 4 is connected to v 1 ."
"we first conduct comparative experiments to show that the features obtained using the dbge can be applied to the employee turnover prediction problem, and these features are highly beneficial."
"we evaluate our proposed approach through two sets of experiments. the first focuses on the employee turnover prediction problem, while the second considers a more general link prediction problem."
"we select common metrics to evaluate the effectiveness of the classification results: accuracy, precision, recall, f 1 -score, and auc. accuracy refers to the ratio of the correctly predicted samples to all samples, indicating the discriminating ability of a classifier. precision is the proportion of positive cases in all samples classified as positive. recall is the proportion of samples that are predicted to be positive in all samples that are actually positive. the f 1 -score is the harmonic average of accuracy and recall. the auc value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example [cit] . we define turnover as a positive sample, the confusion matrix is given in table 3, and the calculation of the metrics is as follows:"
"from figure 8 we can see that the results of line and deepwalk are not satisfactory, because the different categories of points are mixed together and are not well be distinguished. obviously, node2vec and dbge present good visualization results, because different categories of points can be well divided. compared to node2vec, dbge gives the best visualization results because the distances of the points with the same category in dbge are closer."
"the proposed focused deep brain hyperthermia model consists of a half ellipsoidal chamber with part of the bottom horizontal walls, coinciding with the major axis, partially covered (60 cm). a vertical cut is removed from the opposite side of the half ellipsoidal chamber leaving room for the head to be placed in the open uncovered part ( figure 1a ). the radiating device (antenna) is placed at one of the chambers focal points while the target (brain tumor) is placed at the other focal point. the exact placement of the antenna inside the chamber is depicted in figure 1b . the distance between the brain tumor placed at one of the focal points of the hyperthermia chamber model and the antenna placed at the other focal point was 30 cm."
"a return loss s 11 of −11 db was observed for the second antenna having a foam substrate when the antenna was placed at one of the foci of the hyperthermia chamber model at the desired frequency (see figure 4) . the 3d radiation patterns of the foam substrate antenna inside the hyperthermia system model with and without the presence of the head model were computed and depicted in figure 5 a and b at a point inside the proposed hyperthermia chamber. in the direction towards the head, radiation is reduced due to the power absorbed by the head. in the opposite direction there is even a slight increment in radiation, possibly due to partial reflection from the head. a similar radiation pattern was observed for the silicone substrate antenna."
"it can be seen that the label of an edge is mainly determined by two types of factors, i.e., the employee factors and the company factors, which are described by features. we further divide the features into basic features and networkbased features. the basic features can be modeled through employee and company basic information. for the networkbased features, traditional approaches mainly rely on a handcrafted manner which can be tedious and deficient. recent studies [cit] have shown that graph embedding can be a powerful tool for representation learning, so in this paper we utilize the graph embedding approach to learn the networkbased features."
"to evaluate our proposed approach, we conduct experiments on a real-world dataset collected from one of china's largest online professional social networks. the results show that features learned by dbge are beneficial to turnover prediction accuracy. we also compare our approach with other state-of-the-art graph embedding methods in a more general link prediction and visualization task on the public amazon transactions dataset and the taobao online user purchase behavior dataset. the results show that our proposed approach achieves better performance than the baseline methods."
"in a static bipartite graph, the edges between the two groups of vertices are static and the walker proceeds in exactly the same way as on a static normal graph. at each step, the walker randomly selects the next vertex from the neighbor vertices of the current vertex until the sequence reaches the maximum length. figure 3(b) depicts a possible sequence generated by a random walk, which contains two types of vertices that are alternately arranged. the major problem with this kind of random walk is that it cannot preserve the temporal information embedded in the dynamic interactions between vertices."
"by comparing the results in tables 4 and table 5, we can see that without consideration of graph embedding features, the gbdt algorithm exhibits the best prediction performance in acc, f1 and auc. by contrast, after combined with network-based features, the rf algorithm exhibits the best overall prediction performance in acc, f1 and auc. however, for both of the two learning models, their prediction performances are significantly improved after including the network-based features learned through our dbge approach, as compared to lr and nb. we think rf's good performance is mainly due to the fact that it is an ensemble learning algorithm that can handle nonlinear features very well. in addition, the randomness introduced by the rf algorithm can help it overcome the overfitting problem, so that it produces more accurate predictions."
"random walk is a commonly used method for learning network embedding features. it is usually initiated from a randomly selected seed vertex, and then the walker continues to jump from one vertex to a neighboring vertex following some probability distributions until some termination criteria are satisfied. after the random walk stops, the sequence of visited vertices can be generated for the walker (or seed vertex)."
"in this section, we describe the framework of our dbge approach. we first introduce some basic concepts about dynamic bipartite graph, and then give a formal definition of the turnover prediction problem, followed by the details of our algorithm design."
"the brain tumor was placed at a superficial location inside the realistic 3d head model instead of being deeply seated within the brain. this is analogous to a clinical situation where all the physician needs to do is to place the center of the brain tumor wherever it is located in the head at the other focal point of the hyperthermia system. the sar was computed in the realistic 3d head model with its superficial tumor placed at one of the focal points of the hyperthermia chamber model and the antenna with a foam substrate placed at the other focal point ( figure 13a, b) . in this configuration, the~2:1 contrast between the tumor and the surrounding wm case was adopted. a localized sar pattern was observed at the superficial tumor in both the axial view and the sagittal view. smaller values of sar (approximately 20% of the maximum sar value) were observed in the outer csf layer and the ventricles. similar sar patterns would be observed when using the silicone substrate antenna with the exception that lower values of sar would be present in the head. calculated when the brain tumor within the head was placed at one of the focal points of the hyperthermia chamber model system and the antenna placed at the other focal point. in the first configuration, the antenna having a silicone substrate was used to irradiate the head ( figure 14a, b). the~2:1 contrast between the tumor and the surrounding wm case was adopted. a localized temperature of 39.5°c was observed in the tumor. another observation worth noting is that the temperature distribution does not coincide exactly with the sar distribution in brain tissues since the highest temperature exists in the tumor in the sagittal plane while the temperature in the outer csf layer and ventricles was less than 38°c (see figure 10b, figure 14b ). in the second configuration, the antenna with a foam substrate was placed at the other focal point opposite to the head ( figure 15a, b) . in this configuration, the 1:1 contrast between the tumor and the surrounding wm case was adopted. a temperature of 41.8°c was observed in the deep seated tumor while the temperature of the surrounding healthy brain tissue was below 37.8°c. increasing the input power to the antenna from 50 w to 64 w caused the temperature in the tumor to increase to therapeutic values of 43.4°c while keeping the temperature of the surrounding healthy brain tissue below 38.5°c. when the~2:1 contrast between the tumor and the surrounding wm case was adopted, a localized temperature of 43.4°c was observed in the tumor while the temperature in the outer csf layer and ventricles was less than 39°c ( figure 16a, b) ."
"this method uses the cart regression tree as a weak classifier, and the next iteration based on the residuals of the last iteration, and iteratively reduces the sample loss."
"other network-based studies [cit] are mainly restricted to simple centrality analysis or handcrafted features of static networks. today's online professional social networks are highly dynamic and provide plentiful information, and one of the most important information is the personal historical job records which include the start and end timestamps of employment experiences together with company information. since historical job records are chronological, features extracted from static networks cannot preserve the temporal information embedded in one's dynamic work experience. figure 1. an example bipartite graph showing the dynamic interactions between employees and companies. each edge is associated with a timestamp indicating when the employee joined the company, and a label indicating whether the employee quit the job after a specific timespan."
"in the horary random walk, we only consider chronological order when walking from employee vertices to company vertices and use completely random walks when walking from company vertices back to employee vertices. this is because in the employee turnover problem, we only consider the voluntary turnover case, which means that the interactions between employees and companies in the bipartite graph are unidirectional. in the bipartite graph, each connection is initiated by an employee vertex and associated with a timestamp indicating when the employee joined the corresponding company. in contrast, for a company vertex, the timestamps of the different employees who have joined this company are relatively independent. as a result, when walking from the company vertices to the employee vertices, it is not necessary to follow the chronological order. by embedding the employee's historical work records with the dynamic bipartite graph, we can effectively learn the network-based features for the employee and company vertices. finally, we combine these learned features with basic employee and company features, and feed them into different classic machine learning models to solve the employee turnover prediction problem."
"this approach is a commonly used linear classification model. it uses the gradient descent method to iteratively find the optimal parameters of the linear model that minimize the loss function, and then outputs the probability value of the classification through the sigmoid function. finally, the classification result is obtained by comparison with the threshold."
"employee turnover problems have been widely considered because the resignation of talented employees may reduce a company's competitiveness. moreover, it often requires a large amount of time and resources for a company to look for appropriate replacements and train them, and this may hinder the normal development of the company. obviously, being able to foresee whether an employee is likely to leave the associate editor coordinating the review of this manuscript and approving it for publication was emre koyuncu . would enable the company to prepare and take proactive actions accordingly, so that the loss can be minimized. since involuntary turnover is mainly driven by the external environment and usually performed by a company's human resources department, it is out of our consideration in this paper. in this paper, we mainly focus on the voluntary turnover behavior of employees. specifically, we want to answer the following question: given that an employee u joined a company c at time t, will this employee quit his/her job within a specific timespan (e.g., one year)? it is straightforward to see that this problem is of vital importance to many companies."
"a bipartite graph is a special network structure. it has the characteristic that all of the vertices can be divided into two separate groups in such a way that vertices within the same group cannot be directly connected. based on a bipartite graph, a dynamic bipartite graph can be defined as follows:"
"the proposed hyperthermia system caused localized sar in the tumor for both cases of contrast between the tumor and the surrounding wm. although the temperature rise in the tumor was less for the 1:1 contrast case as compared to the temperature rise for the~2:1 contrast case ( figure 15 compared to figure 16 ), increasing the input power to the antenna compensated for the observed difference. thus, indicating the capability of the proposed system to cause heating of the brain tumor to therapeutic values despite the contrast in the electrical properties."
"to our knowledge, adequate localization and focusing of energy patterns and the calculation of the temperature distribution in a realistic 3d head model containing a deeply seated as well as a superficial brain tumor due to irradiation by a simple easy to fabricate antenna excited at a single frequency placed in a partial half ellipsoidal noninvasive air filled hyperthermia applicator is still lacking. in this study, we numerically designed a noninvasive hyperthermia applicator system composed of a patch antenna, a partial half ellipsoidal chamber and a head model containing a tumor. the fdtd method was used to compute both the sar patterns and the temperature distribution in three different head models. several improvement steps were performed on all of the applicator system configurations to adequately ensure sufficient and focused energy deposition and temperature distribution in the brain tumor. the system is capable of heating deep seated as well as superficial brain tumors by placing the center of the tumor at a pre-specified location (one of the foci) controlled by the operator of the proposed hyperthermia system. the use of a partial half ellipsoidal chamber ensures better head positioning and patients comfort compared to whole ellipsoidals."
the issue of employee turnover has been of widespread concern. here we only focus on data-driven studies. existing data-driven research can be divided into two main categories: machine learning methods [cit] and survival analysis methods [cit] .
"in this paper, we proposed an effective approach, called dbge, that learned low-dimensional vector representations for dynamic bipartite graphs to solve the employee turnover prediction problem. we first defined the horary random walk to obtain the sequence of vertices in chronological order, and then we employed the skip-gram model to obtain the low-dimensional representation of each vertex. we combined learned features with basic features and applied them to machine learning methods to solve the turnover prediction problem. experimental results on real-world datasets showed that our proposed method, which incorporated temporal information, was effective in both employee turnover prediction and link prediction. there are several directions for our future work. first, we have only used the interaction network between employees and companies; in the future, we will simultaneously consider the social network between employees. second, we can study the impact of other factors on employee turnover behavior, such as changes in a company's market value."
"the sar was computed in a simple spherical head model composed of skull, average brain tissue and a tumor (figure 7) . the antenna having a foam substrate was used to irradiate this simple head model. in this configuration, the~2:1 contrast between the tumor and the surrounding wm case was adopted. a centrally located region of high sar (107 mw/g) is observed (yellow color). this high sar region coincides with the tumor placed at the center of the spherical head model. a similar pattern was observed when using the antenna with a silicone substrate."
we compare our proposed dbge algorithm with other stateof-the-art graph embedding algorithms that do not consider temporal information to show the effectiveness of our approach.
"visualization is also a way to measure the quality of vectors learned by different graph embedding methods. we extract subset from taobao dataset which contains two categories of products. it consists of 988 purchasers and 131 sellers. we apply the t-sne tool [cit] to map the low dimensional features learned by different graph embedding methods to the 2d space. figure 8 show the visualization results of different graph embedding algorithms, where color of a point indicates the commodity preference of the purchaser.the same preference of purchasers should cluster together."
"when the~2:1 contrast between the tumor and the surrounding wm case was adopted, a similar sar pattern was observed in the tumor in both the axial view and the sagittal view ( figure 12a, b) . higher sar values are observed in the tumor compared to the 1:1 contrast case. increasing the input power to the foam antenna from 50 w to 64 w caused the sar value for the 1:1 contrast case in the tumor to increase to the same sar value observed for the~2:1 contrast case."
the focusing properties of the half ellipsoidal chamber were investigated in the presence of the realistic head model containing the tumor. the electric field distribution inside the proposed hyperthermia system is depicted in figure 9 . the center of the brain tumor was placed at one focal point and either antenna placed at the other focal point. penetration of the field is observed and focusing on the 2 cm brain tumor which coincides with the focal point is achieved. the radiated energy was mainly localized in the tumor as well as the high water content tissues in the brain (csf and the ventricles). it is worth noting that similar electric field patterns were observed in the hyperthermia system model with the realistic head model present due to irradiation by either antennas and for the two electrical properties contrast cases adopted in this study (the 1:1 contrast and the~2:1 contrast between the tumor and the surrounding wm).
"for the baseline machine learning models, we use the same model parameters (the default parameters provided by the package) before and after combining with the dbge features to ensure a fair comparison."
"it is also worth noting that, calculating the temperature increase with the pennes bioheat equation may have some limitations. in particular, the effect of vasculature which causes cooling of surrounding tissues was excluded in our study. however, in order to raise the temperature of the tumor to higher values using the proposed hyperthermia applicator, we only need to increase the input source power to the antenna which will result in higher sar values and higher temperatures in the brain tumor."
"the sar was computed in a more complicated head model composed of concentric spheres representing different brain tissues (skull, csf, gm and a tumor) which was placed at one of the foci of the hyperthermia chamber model (figure 8 ). the antenna having a foam substrate was used to irradiate this head model. in this configuration, the~2:1 contrast between the tumor and the surrounding wm case was adopted. the sar pattern shows a centrally located region of high sar within the tumor as well as in the outer thin csf layer. a similar pattern was observed when using the antenna with a silicone substrate. using more tissues in the head model caused different values of sar in the tumor (120 mw/g) compared to the simpler head model (107 mw/g). on the other hand, unwanted higher values of sar, compared to those in the tumor, were observed in the csf layer (293 mw/g). the presence of the high water content csf layer in the layered model caused better energy focusing off the chamber walls into the tumor. thus, indicating the importance of using more complicated head models to account for tissue heterogeneities in the brain."
"careful selection, design and tuning of the antenna used as a source of tissue irradiation in noninvasive hyperthermia applicators is necessary since the presence of the head comprised of lossy soft tissues in the vicinity of the antenna has a considerable effect on the characteristics and the performance of the antenna."
"as we have mentioned before, we use the pca algorithm to reduce the dimensionality of dbge features. in order to compare the influence of different dimensionalities on the prediction, we use the 1-dimensional to 9-dimensional features to conduct experiments and show the effects of these features on the improvement of prediction performance under different evaluation metrics, as shown in figure 6 . it can be clearly seen that the dgbe features combined with the rf model exhibit the best overall prediction performance improvement, as shown in figure (a), (c), (d) and (e). based on the results in figure 6, we choose different dbge dimensionalities for different machine learning models to ensure good overall performance."
"where ρ is the density of tissue, c is the specific heat capacity, t is the temperature of tissue, k is the thermal conductivity, q r is the regional heat delivered by the source (sar), q m is the power generated by metabolism, ρ bl is the density of blood, c bl is the specific heat capacity of blood, w bl is the blood perfusion, and t bl is the temperature of blood. b represents the term associated with blood flow and equals ρ bl c bl w bl ."
"in the majority of cases, our static approach based on code classification allows us to use the best static frequency/core configuration for the benchmark, whether optimizing for energy or for edp. there are, however, sporadic but clear cases of \"failure,\" such as dynprog-poly or adi-par, which correspond to codes with poor parallel scaling. this suggests our coarse approximation of parallel scaling can be improved, for example by more accurately computing the workload of the slowest thread and the expected parallel speedup."
"as illustrated in figure 8, there can be cases with stable phases in the computation, necessitating ideally different frequency/core configurations for ideal energy savings. algorithm 2 can be applied on any subpart of the program; in particular, it can be applied on different loop nests to obtain the estimated operational intensity for different program regions. if this intensity differs, then the classification of the region using the decision tree from section 5.2 may also change, leading to using different frequencies within the same program to be better suited to exploit its phases. as future work, we will investigate how to embed such mechanisms in our static analysis to detect program phases at compile-time."
"a wide-range of applications from social science to physics need to identify communities in complex networks that share certain characteristics at various scales and resolutions [cit] . challenges remain, however, to address both intensity and dynamicity of communities at large scale. we thus focus on metrics and algorithms whose complexity is no greater than o(n)."
"s the advancement in communication and computing technologies stimulates the progress of smart cities, trillions of sensors are deployed in every corner of a city, subsequently forming a huge sensor network. data collected by various sensors range from buildings, streets, and transportation systems, to natural spaces. these heterogeneous data respectively delineate the characteristics of a city. among these sensors, mobile sensing is becoming popular due to its mobility and pervasiveness. furthermore, it reflects human social behavior and shows the interaction between persons and a city."
"we use multiple k values to represent a community at multiple resolutions. for each social network dataset, we select three distinct k values so that 4, 8 and 16 percent of the vertices in that dataset have a degree of at least k. the higher the k value, the stronger or tightly knit the communities are. conversely, the lower the k value, the weaker or loosely connected the communities are. table iv lists the chosen k values. we first run base k-core construction algorithm to measure the baseline k-core construction time for each dataset and k value. then we run multi k-core construction algorithm, which is described in algorithms 3 and 4, for each dataset with all chosen k values at once to measure kcore construction for multiple k values. figure 3 shows the construction times for both algorithms. speedup achieved by multi k-core construction algorithm is upper bounded by the number of distinct values which is 3 in this case. we observe that, for larger datasets the algorithm achieved higher speedup due to the redundant computation saved."
"we provided a distributed implementation of the algorithms on top of apache hbase, leveraging its horizontal scaling, range-based data partitioning, and the newly introduced coprocessor framework. our implementation fully took advantage of distributed, parallel processing of the hbase coprocessors. building the graph data store and processing on hbase also benefits from the robustness of the platform and its future improvements."
"in this section, we first describe a naïve distributed algorithm that constructs a k-core subgraph, then we propose a novel algorithm to compute k-core graph for multiple k values simultaneously. table i summarizes notations used in our pseudocode."
"an interesting aspect of this compile-time approach is that it can be applied incrementally on each loop (nest) of the program to detect loop nests with significantly different behaviors. that is, this algorithm can be used to detect compute-bound and memory-bound application phases, thus allowing for individually fine-tuning the dvfs decision for each phase."
"here, we discuss the occurrences of phases in the programs we evaluated via a separate set of experiments conducted using the runtime system described in section 3. the objective is to show that, thanks to the stability of most affine kernels we evaluated (i.e., they contain only one phase), a purely static approach where we select a single frequency/core configuration for the entire kernel duration can achieve near-optimal results. we then discuss cases where phases occur and point to future work on using algorithm 2 to detect those phases analytically to enable the selection of different frequency/core configurations for different phases of the program."
"unlike many works that focused on the user side by mining their transaction records, browsing histories, and social connections, geo-conquesting concentrates on the marketer side. the following section discusses two different scales of geo-conquesting strategies by using the crowdsourced features in the previous section. one is from the perspective of mall management, and the other is for metropolitan planning."
"we plot in figure 8 the output of our adaptive runtime on several kernels. to better visualize phases, if any, we used a very low threshold for frequency change: as soon as there is a 1% energy-efficiency difference between two time quanta (set to 50ms), the runtime is allowed to increase/decrease frequency. benchmarks were set to using a very large problem size for better illustration. there is one point per time quanta, and we report both the current frequency (in green) and the \"instantaneous\" power for the quanta (in purple). we conducted this study on all 60 benchmarks on haswell, using four cores, and we isolated the most representative cases. the top charts show typical power trace examples for single-phase kernels: the power consumption is mostly stable, and the frequency typically oscillates between two of the frequency points (e.g., 1.2ghz or 1.6ghz), indicating that the optimal frequency may be between these points. we conducted this characterization for all 60 benchmarks, and, for 46 of them, there is a single phase. these includes gemm and trmm, for example. the bottom charts illustrate cases of multiple phases in the program. a total of 14 benchmarks show two or more phases (up to five), visualized as a stable change for a part of the program execution of the frequency used. the effect of program transformations to improve data locality can be visualized here: phases are not identical between both plots, in particular since data locality was changed by the polyhedral code transformation, and, therefore, the operational intensity was modified."
"we observe that, on average, our approach vastly outperforms performance and powersave in terms of both energy savings and edp improvements: we are, in fact, very close to the optimal situation for energy, with our approach being within 102% or less (i.e., less than 2% worse) of the truly minimal energy that can be achieved using the best configuration found individually for each binary tested and each processor. this result is even further exacerbated for edp, where our edp savings can be up to 50% better than using the performance governor. our predictor approach achieves always within less than 10% of the optimal edp. the penalty in execution times is also very good: the codes are slowed on average compared to their maximal speed by at most 11.6%; for the ivy bridge machines, where performance is the best configuration for energy efficiency for compute-bound benchmarks, our execution time increase is significantly smaller than our edp improvement over performance. in fact, such execution time degradation can be so small that, even if the system power is large compared to the cpu power, our dvfs approach still leads to significant overall energy savings."
dvfs is a well-known technique to adapt power consumption based on application demands and hardware state by modifying the frequency (and the associated voltage) at which a processor operates. previous works have typically focused on two aspects: how to reduce frequency without much of a wall-clock time penalty to reduce the power and energy used by a computation and how to increase the frequency on demand to improve completion time and optimize the overall energy-delay product.
"typical dvfs approaches fall into two categories: schemes that employ specific frequencies for default policies (e.g., powersave vs performance governors in linux) or schemes that observe the cpu execution and dynamically react to cpu load changes (e.g., the on-demand governor). although energy savings have been demonstrated with these approaches [cit], we observe several limitations. first, as we show in this article, the frequency/voltage that optimizes cpu energy can vary significantly across processors. even for the same compute-bound application, different processors have different energy-minimizing frequencies. second, optimizing energy for a parallel application is highly dependent on its parallel scaling, which in turn depends on the operating frequency, an aspect mostly ignored in previous work. third, dynamic schemes remain constrained to runtime inspection at specific time intervals, implying that shortrunning program phases (e.g., a few times longer than the sampling interval) will not see all the benefits of dynamic dvfs compared to using the best frequency for the phase from the start."
"according to a survey conducted by www.statisticbrain.com, the total number of worldwide cellular phone subscriptions was as high as 6.9 [cit] . the total app downloads for ios and android smartphones respectively reached 29 billion and 31 billion."
"the benefits of our approach over powersave are highlighted with architectures where the energy-minimizing frequency is not the minimal one, such as on ivy bridge and haswell. an average savings of 13% is achieved for the eight-core ivy bridge, coming from selecting higher frequencies to balance completion time and power consumed following the processor-specific characteristics. for almost all kernels, savings above 10% are achieved with our static approach. we also show savings comparable to the best possible ones for the vast majority of cases, thus demonstrating the viability of our compile-time categorization of codes. for cases with lower savings than best, the frequency/core selected was typically one step above the best one (e.g., we used 1.6ghz instead of the ideal 1.2ghz) and suggests that further improvements may be achieved using more categories for the applications. figure 5 summarizes the results for all architectures. for ivy bridge and p8, the energy difference of on-demand over powersave is characteristic of these machines, where actually maximal frequency leads to minimizing energy for compute-bound codes. this is exacerbated by the fact that the poly version of the benchmarks has been transformed for improved data locality and, for many, has become essentially compute-bound. our static analysis seamlessly captured these changes, with, for instance, gemm and seidel-2d moving from being categorized as memory-bound in their par version to compute-bound in their poly version as the result of transformations for locality, tiling, and simd vectorization."
"the rest of the paper is organized as follows. we first review prior work on community identification and k-core algorithms in section ii. section iii introduces the big data platform and programming framework. we define and introduce key k-core properties in section iv. section v de-scribes our distributed multi-k-core construction algorithms in naïve implementation and pruning techniques. section vi details our incremental maintenance algorithms for edge insertions and deletions. experimental results are reported and discussed in section vii. finally, section viii concludes the paper and discusses future work."
"-we demonstrate that some limitations of purely load-based approaches to dvfs can be addressed using a lightweight runtime approach optimizing for cpu energy efficiency. -we develop a compilation framework for the automatic selection of the frequency and number of cores to use for affine program regions by categorizing a region based on its approximated operational intensity and parallel scaling potential. -we provide extensive evaluation of our approach using 60 benchmarks and 5 multicore cpus, demonstrating significant energy savings and edp improvements over the powersave linux governor."
"second, the optimal cpu energy may be achieved at different frequencies depending on the number of cores used. this is seen in particular on haswell, where on one core 2ghz is the best frequency, but for two and four cores it is 1.6ghz. a similar situation is observed on sandy bridge, albeit the difference is very small. on the other hand, this is not observed for the ivy bridge cases."
"experimental results. figure 2 shows the energy savings, compared to powersave, of the on-demand linux frequency governor and of our runtime approach. we evaluate on 60 computation kernels as detailed in section 6.1 to enable comparison with our compiletime approach developed in later sections using openmp parallelization and executing on all cores of the target machines. we detail the two most interesting machines for this experiment: an eight-core ivy bridge machine, where the lowest energy can be achieved for high frequencies, and a four-core haswell, where the minimal frequency is not always minimizing energy, as shown in section 2."
"when the dl of a loop nest is lower than the cache size, it is then a good approximation of the number of cache misses: only cold misses will occur, one per line of cache accessed, because the reuse will be fully implemented without any data being evicted. conflict misses are ignored here, and, in the following, we will assume they do not dominate the miss behavior (e.g., arrays have been properly padded [cit] ). on the other hand, when the dl is larger than the cache size, dl does not allow us to determine an estimate of the number of misses: it depends on the schedule of operations and, in particular, on the reuse distance between references to the same array."
"4.1.2. approximating flops and getting oi. the last element needed to approximate the oi is the number of operations executed in the program so that we can divide it by the number of memory movements (obtained from the number of misses). this is straightforward in the polyhedral representation: we inspect the ast of each statement body, collect the number of operations not part of an array subscript expression, and multiply it by the number of points in the iteration domain of the statement. because this process requires explicit values for the parameters, we leverage the known values to obtain a numerical value for the flop count. the overall process to compute oi is outlined in algorithm 2."
"apache hbase is a non-relational, distributed data management system modeled after google's bigtable [cit] . hbase is developed as a part of the apache hadoop project and runs on top of hadoop distributed file system (hdfs). unlike conventional hadoop whose saved data becomes read-only, hbase supports random, fast insert, update and delete (iud) access. fig. 1 (a) depicts a simplified diagram of hbase with several key components relevant to this paper. an hbase cluster consists of master servers, which maintain hbase metadata, and region servers, which perform data operations. an hbase table, or htable, may grow large and get split into multiple hregions to be distributed across region servers. htable split operations are managed by hbase by default and can be controlled via api also. in the example of fig. 1(a), htable 1 has four regions managed by region servers 4, 7 and 10 respectively, while htable 2 has three"
"energy savings over powersave. the powersave linux governor uses all available cores and sets the frequency to the minimal one. this principle uses the idea that a frequency increase has a cubic effect on power increase but a linear effect in execution time decrease, so using the minimal frequency should minimize cpu energy. but, as we demonstrated in section 2, this may not lead to minimizing energy. furthermore, this approach essentially ignores the remaining system power consumption. in this work, we show how cpu energy savings can be achieved by increasing the frequency, which in turn typically leads to also reducing the kernel execution time compared to powersave. as a consequence, the system energy consumption is further reduced with our approach compared to powersave; we, however, do not report it and focus solely on cpu energy reports."
"to assess the quality of our framework, we needed to compute the optimal frequency/core configuration for each binary individually by collecting its energy and execution time for each frequency/core configuration evaluated on the target machine. these data were collected using intel pcm [intel b] for the intel chips, and ibm amester [cit] ] for the power8. the instrumentation was inserted around each kernel of interest. to obtain stable and sound energy measurement value, turbo-boost was turned off on the intel chips. the following process was repeated as needed to achieve an execution time of around 1 minute, and the average energy and time were obtained: (1) flush data caches, (2) start instruments, (3) execute kernels, (4) stop and collect energy and execution time"
"4.1.1. approximating data movement. the distinct lines (dl) model was designed originally to estimate the number of distinct cache lines, or tlb entries, accessed in a loop nest [cit] . it essentially represents the footprint of a computation in terms of cache lines accessed, thereby taking into account spatial locality. dl formulas are typically used to analytically find values for the loop bounds (e.g., tile sizes) to ensure the number of distinct lines accessed is below the cache capacity, therefore ensuring that data reuse is implemented in cache."
"in-store visits involve two factors. one is the directions of traffic flows, and the other is the flow capacity. the former dominates store types that are highly related to customer preferences when they move, as mentioned earlier in fig. 1 . the latter increases store visibility if the location of a store is right next to a major flow. therefore, monitoring flow directions and flow capacities in a hotspot network is important. flow directions can be easily manifested by computing in-degrees and out-degrees. however, flow capacities require a more complicated mechanism because both overflows and underflows in a route have influences on in-store visits and visibility. this relatively affects site rentals, product prices, and profits."
"different scales of geo-conquesting strategies, ranging from mall operations to metropolitan business targeting, are also presented in the discussion. with these features, intelligent marketing becomes feasible because store locationing is based on dynamic city characteristics instead of static demographics."
"this study examines crowdsourced features generated from mobile metatrails for geo-conquesting. to reflect the dynamics of a city, graph analysis is used for discovering hotspot networks, transition probabilities, and flow capacities. subsequently, affinity subnetworks and sequential visiting patterns are extracted by using rapid graph clustering. affinity subnetworks (i.e., combo stores) and sequential patterns allow marketers to analyze crowd sequential activities -between stores and even between subnetworks. such a discovery creates a weighting factor to in-store visits."
"in a manner analogous to data spaces, one can compute the dl of a particular loop iteration by using parametric slices of the domain. such a case will be denoted dl a ps l,α for array a and loop l with offset α, which represents dl for one iteration of loop l. the dl of a full loop l is noted as dl a l and is computed by using a parametric slice of the loops surrounding l, considering only the array references in l's body."
"we model interactions between pairs of objects, including structured metadata and rich, unstructured textual content, in a graph representation materialized as an adjacency list known as edge table. an edge table is stored and managed as an ordered collection of row records in an htable by apache hbase [cit] . since apache hbase is relatively new to the research community, we first describe its architectural foundation briefly to lay the context of its latest feature known as coprocessor, which our algorithms make use of for graph query processing."
"the intuition behind this theorem is that an edge deletion can at most decrease core number by one and thus an edge deleted from g ki may push some vertices from g ki to g ki−1 but not further down in the hierarchy. again, our algorithm exploits the property to minimize traversal."
"for sandybridge, we observe that minimizing frequency is a viable way to minimize energy, and for numerous benchmarks our approach selects the minimal frequency (therefore showing 0% savings over powersave), which was also found to be the best scenario by empirical evaluation of all frequency/core pairs (the best bars). still, a significant improvement is achieved using our approach for benchmarks which are either sequential or have poor scalability, such as dynprog-par, where we select a reduced number of cores."
"benchmarks considered. we use the polybench/c 3.2 benchmark suite [pol], a popular suite of 30 numerical computations written using affine loops and static control flow. it spans computations in numerous domains such as image processing, linear algebra, and more. for each benchmark, we considered two versions of the code: par was generated by applying a simple auto-parallelization without any other code transformation. it amounts to inserting openmp parallel for pragmas around the outer-most parallel loops in the code and vectorization pragmas around the inner-most parallel loops. this can very substantially improve the performance of the original codes, which are sequential by default, and substitute for the auto-parallelization schemes of the c back-end compilers used. we used the pocc source-to-source polyhedral compiler [poc] to generate the optimized c files. the second variant, poly, is the result of a complex automatic program transformation stage aimed at improving data locality via tiling, and it exposes coarse-grain (multicore) and fine-grain (simd) parallelism. it uses the pluto algorithm [cit] ] combined with several other optimizers of pocc implementing a model-driven prevectorization, unrolling, and parallelization. for these variants, the oi is typically significantly improved after transformation: temporal data locality is improved via loop tiling whenever possible. these two versions form a total of 60 benchmarks that we evaluated on."
"we remark that ds a is not necessarily a convex set but can still be manipulated with existing polyhedral libraries. for example, in figure 3,"
"crowd behavior can be further manifested when mobile crowdsensing is applied. based on communication types [cit], mobile sensing can be classified into two categories -direct and indirect sensing. the former involves direct communication between devices (e.g., device-to-device and machine-to-machine approaches) or direct communication between terminals and base stations (e.g., radio networks). signals during transmission can be directly used for power management and resource planning, subsequently utilized for estimating the number of cellular phone subscribers or user densities. the latter, indirect sensing, relies on intermedia, e.g., social websites and clouds, where the captured data via mobile sensors can be downloaded by analysts and further processed."
"sequential code. the sequential code we use as representative of sequential workloads is a simd-vectorizable benchmark, with balanced arithmetic intensity (i.e., there is non-negligible memory traffic). it was built from key code features of the durbin benchmark from polybench/c."
"in particular, as tiling improves the operational intensity, we see gemm moves from being memory-bound (no tiling) to compute-bound (after tiling). on the other hand, for jacobi-2d, tiling by itself did not improve the oi enough to make it go beyond the memory-bound cutoff point. this is expected because the best frequency/core configuration for this optimized kernel is indeed the best frequency for the bandwidth-bound codes: the program still suffers from bandwidth contention, even after the tiling we applied."
"the information on the maximum and the minimum flow capacity is useful when marketers select sites. their requirements for maximum and minimum capacities can be converted into upper and lower bounds, i.e., constraints, during profit optimization. assume that low flows bring few in-store visits and subsequently low rentals. there is a balancing relation between flows and site rentals. however, this is still a challenge of multiside profit optimization."
"graph data is stored in hbase and the algorithms are implemented as hbase coprocessors where distributed parallelism is applicable. table ii shows how notations in algorithms are interpreted in hbase implementation. our cluster consists of one master server and 13 slave servers, each of which is an intel cpu based blade running linux connected by a 10-gigabit ethernet. we use vanilla hbase environment running hadoop 1.0.3 and hbase 0.94 with data nodes and region servers co-located on the slave servers. we configured hbase with maximum 16 gb java heap space and hadoop with 16 gb heap to avoid long garbage collection in the java virtual machine. the hdfs (hadoop file system) replication factor is set at the default three replicas. there was no significant interference from other workloads on the cluster during the experiments."
"community identification and evolution in a complex network has applications spanning multiple disciplines ranging from social science to physics. in recent years, the rise of very large, rich social networks re-ignited interests to the problem at the big data scale that poses computation challenges to early work with algorithm complexity greater than o(n). in addition, many observed interactions with the community happen not just at one but multiple levels of intensity, which reflects in reality active to passive participants in a group."
"markov clustering was derived from flow simulation by stijn van dongen [cit] . it used the ideas of markov chains and random walks within a graph by iteratively computing transition matrices based on edge weights. the intuition behind markov clustering is that if a graph possesses a clustered structure, random walks between vertices lying in the same cluster are more likely than those between vertices which are located in different clusters [cit] . this finding is based on the equilibrium distribution of markov chains. let π represent the matrix of initial probabilities for all the vertices in a hotspot network g. by multiplying π by a transition-probability matrix t within finite times, the resulting product becomes stable."
"interesting challenges arise in intelligent marketing, for example, combo-site locationing, sales-force allocation theory, and multiside profit optimization. in the future, there will be systematic mathematical equations for modeling these challenges."
"in this work, we propose to address these limitations using two complementary strategies. first, we present a simple lightweight runtime which throttles frequency based on periodic measurements of the energy efficiency of the application. this approach can exploit the specific properties of the cpu power profile and is applicable to arbitrary programs. then, we develop a compile-time approach to select the best frequency but for program regions that can be analyzed using the polyhedral model [cit], which is typical of compute-intensive kernels/library calls such as blas operations [netlib; pol] or image processing filters [opencv; pol] . specifically, we develop static analysis to approximate the operational intensity (i.e., the ratio of operations executed per bytes of data transferred from the ram) and the parallel scaling (i.e., the execution time speedup as a function of the number of cores) of a program region in order to categorize that region (e.g., compute-bound, memory-bound, etc.) . the frequency and number of cores to use for each program category are chosen from the result of a one-time energy and energy-delay product profiling of the processor using microbenchmarks representative of different workload categories. our extensive evaluation demonstrates significant energy and edp savings over static (powersave) and dynamic (on-demand) schemes, thus validating our approach. we make the following contributions:"
"however, as we show in the next sections, further gains can be attained by addressing two inherent limitations of a runtime-based approach: (i) the \"wasted\" time to reach the optimal frequency for a regular computation chunk (i.e., a phase in the program) and (ii) the inability to adapt the number of cpu cores allocated to the chunk based on parallel scaling. we show how these limitations can be efficiently addressed for affine kernels."
"data space of a loop iteration. it is very convenient to be able to restrict the data space to a particular loop iteration, for instance, to compute the data space for one execution of the j loop (i.e., one iteration of the i loop). we will use such a mechanism to approximate cache misses later. a simple approach to achieve this is to compute a \"slice\" of interest of the iteration domain and use this sliced domain in the data space computation. a parametric slice [cit] ] along a set of dimensions (i.e., loops) is defined as follows: given a loop nest with a loop l of depth n surrounded by k − 1 loops, the parametric slice ps of loop l is a subset of z n defined as:"
"detailed results on haswell. table iv details similar results on a per-benchmark basis focusing exclusively on the haswell architecture and the gcc compiler. we selected this architecture because it is the most recent one in our testbed. on haswell, as shown in section 2, the optimal frequency for energy even for compute-bound codes is not the maximal (nor minimal) frequency. we report the best frequency/core configuration for each binary found by empirically evaluating all frequency/core cases in the design space-frequencies of 1.2ghz, 1.6ghz, 2.0ghz, 2.5ghz, 3.1ghz, and 3.5ghz-using 1, 2, or 4 cores. we report the one predicted by our approach, ourstatic, and compare the energy savings compared to powersave for both cases. a similar study is shown when optimizing for energy-delay product. averages (in % also) are reported on the last line. finally, the last column shows the increase in execution time by using ourstatic compared to using performance. it is obvious that, in many cases, optimizing cpu energy can be done without being detrimental to the best possible execution time or the increase remains moderate."
"we conclude our experimental evaluation with table v, which reports how many benchmarks belong to each category. this classification is done per binary and is independent of the compiler used. a benchmark may have more than one characteristic feature, which is why we use a decision tree to prioritize them, as described in section 5. we see that, for the simple parallelized versions, many codes are bandwidth-bound, and this decreases when using program transformations to improve data locality. our static analysis captures how a program is implemented (i.e., its result may change as a function of, for example, which loop transformations have been applied). this is confirmed by table vi, which zooms in on a few benchmarks, displaying how their features change depending on the structure of the code implementing the algorithm in the original benchmark (par versus poly, see section 6.1 for details)."
"for mall operators, as hotspot networks display frequently visited stores, the difference in rentals between stores with high and low in-store visits can be dynamically adjusted. when rentals decrease, the cost is reflected on the price of retail products, subsequently attracting more customers. there is a mutual interest among mall operators, storeowners, and customers."
"when the traversal order of different subnetworks is focused, an entire subnetwork is viewed as a vertex. namely, all the vertices are contracted to generate one vertex. the whole hotspot network g becomes a new graph, of which each vertex represents an affinity subnetwork. the approach for analyzing the order of subnetworks is similar to that for the order of hotspots. fig. 4 . example of affinity subnetworks. three subnetworks were discovered by graph mining when crowd dynamics were considered. each represents a combination of frequently visited hotspots. people prefer staying in the same subnetwork and visiting these places in combination."
we ran experiments to demonstrate the performance of our proposed multi k-core construction algorithm and the performance of our proposed k-core maintenance algorithms on dynamic graphs. we show that recomputing the k-core
"input: c: set of candidate edges, kj : target core value, output: c: the updated set of edges qualifying for k-core 1: changed ← true 2: while changed do 3:"
"the first feature we extract is whether the program is sequential or parallel. this is simply done by checking if the program has any openmp pragma in it; if not, then it is sequential. the second feature attempts to capture poor scalability of the code (i.e., if the performance improvement does not scale with the number of cores). for this feature, we again rely on the assumption that there are only openmp for pragmas to model the parallelization. we analyze the ast to form an estimate of the regularity of the parallel loop trip count, studying the parallel workload properties as detailed in algorithm 3."
"as shown in section 2, the frequency/number of cores configuration needs to be adapted as a function of the nature of the code executing. to find the best frequency/core configuration for a particular machine, we perform a profiling of four benchmarks on the target machine, running them on all frequency steps and core setups available and collecting the execution time and power using hardware counters. this enables us to build a profile of the energy and energy-delay product curves for each benchmark, which are the two metrics we optimize for. these benchmarks have been specifically built to exacerbate one of the features that is computed by our static analysis."
"unlike affinity subnetworks that concentrate on combo stores, this feature highlights the order of patterns. there are two types of sequential visiting patterns. one is the order of the hotspots in an affinity subnetwork, and the other is the order of subnetworks."
"-. regions stored in region servers 4 and 10. an hbase client can directly communicate with region servers to read and write data. an hregion is a single logical block of record data, in which row records are stored starting with a row key, followed by column families and their column values. hbase's coprocessor feature was introduced to selectively push computation to the server where user deployed code can operate on the data directly without communication overheads for performance benefit. the endpoint coprocessor (cp) is a user-deployed program, resembling database stored procedures, that runs natively in region servers. it can be invoked by an hbase client to execute at one or multiple target regions in parallel. results from the remote executions can be returned directly to the client, or inserted into other htables in hbase, as exemplified in our algorithms. fig. 1 (a) depicts common deployment scenarios for endpoint cp to access data. a cp may scan every row from the start to the end keys in the hregion or it may impose filters to retrieve a subset in selected rows and/or selected columns. note that the row keys are sorted alphanumerically in ascending order in the hregion and the scan results preserve the order of sorted keys. in addition to reading local data, a cp may be implemented to behave like an hbase client. through the scan, get, put and delete methods and their bulk processing variants, a cp can access other htables hosted in the hbase cluster."
energy efficiency is of increasing importance in a number of use cases ranging from battery-operated devices to data centers striving to lower energy costs. dynamic voltage and frequency scaling (dvfs) [cit] ] is a fundamental control mechanism in processors that enables a tradeoff between performance and energy.
"in this work, we propose to approximate the data movement between two levels of memory by approximating the number of cache misses at a certain level. we achieve this by (i) formulating dl in the polyhedral framework using the concepts presented earlier; (ii) extending it to also capture the data reuse between consecutive loop iterations at any loop level; and (iii) designing an algorithm that approximates the number of misses when the dl exceeds the cache size."
"in this paper, we propose a set of algorithms built on the k-core metric to identify and maintain a contentprojected community at multiple resolutions on an opensource big data platform, apache hbase. we formulate the community identification problem as first projecting a subgraph by content topic of the social network interaction, such as microblog or message, and then locating the \"dense\" areas in the subgraph which represent higher inter-vertex connectivity (or interactions in the case of a social network) at multiple resolutions. in the literature, there is a long list of subgraph density measures that may be suited in different application context. examples include cliques, quasi-cliques [cit], k-core, k-edge-connectivity [cit], etc. among these graph density measures, k-core stands out to be the least computationally expensive one that is still giving reasonable results. an o(n) algorithm is known to compute k-core decomposition in a graph with n edges [cit], where other measures have complexity growing super-linear or np-hard."
"bandwidth-bound code. bandwidth-bound programs are frequent, especially if no data locality optimization has been performed on the original benchmark. for this case, we created a benchmark that implements both spatial and temporal locality via tiling with a low arithmetic intensity. it is inspired by an iterative stencil (e.g., jacobi-2d). this benchmark is bandwidth-bound but with a relevant balance between computations and communications."
"compared with heatmaps that display frequently visited areas without showing connections between them, hotspot networks use transition probabilities to present user preferences and flows between places."
"to analyze the order of hotspots, firstly all the trails in an affinity subnetwork are extracted. then the system rearranges the vertices based on their latest timestamps in the trails. when more than one sequential pattern is generated, it means at least two types of orders exist in an affinity subnetwork."
"let an edge denote a road, and let a vertex represent a point of interest (i.e., a waypoint, a start point, or a finish point). according to graph theory, a walk is defined as a sequence of alternating vertices and edges. let us also define the distance between two directly connected vertices as one."
"based on a recent survey carried out by www.statisticbrain.com, more and more smartphone users prefer using apps for communication, so urban sensing via mobile apps becomes another way for collecting data. furthermore, the captured data can be forwarded and uploaded to cloud sides stealthily without interfering with user operations. closely examining the metadata crowdsourced from mobile users shows different demands of customers. this is beneficial for both sides -marketers and customers."
"k-core algorithms in sections v and vi are implemented in several hbase coprocessors to achieve maximal parallelism. take degree computation as an example. multiple instances of coprocessors scan the graph data table's local partitions in parallel and then insert vertices' degrees into another hbase table. when a non-local edge is to be deleted, a coprocessor instance issues the row delete message to the remote hbase region server, which deletes the edge. our algorithms are optimized to minimize the message exchanges by achieving as much processing in the local partition as possible."
"dvfs technology, which originated in the real-time and embedded system community [cit], is widely applied nowadays since energy use and power consumption have become the primary optimization metrics for the entire spectrum of computing, from handheld devices [cit] ] to desktops [cit] to clusters [cit] ."
"the algorithms are described in algorithms 3 and 4 for the client and server side, respectively. it first computes kcore graph for k 1 using the base algorithm. next, the client invokes distributed parallel processing compute core at the server side to compute core values for vertices with degree greater than or equal to k i and less than k i+1 . on the server side, it checks a vertex's degree count and decrements its neighbors' if their degree counts are greater than k i+1 . iterations continue until all the parallel execution reported vertices in g ki+1 have been identified."
"to evaluate the performance of maintenance algorithms 5 and 6, we first construct and materialize k-core graph for selected multiple k values and under three scenarios explained below we measure average maintenance times."
"figure 4 summarizes our performance results. the five bar charts provide the energy savings, on a per-benchmark basis, of our approach (ours) compared to using the powersave linux governor and the savings that can be achieved by using the best frequency/core configuration found empirically by testing all of them. figure 5 summarizes the average energy savings e and energy-delay product edp improvement across all 60 benchmarks, compared to powersave of the on-demand linux governor, our implementation of the cpumiser 2 runtime approach [cit] ], ours (static), and best (static) empirically found."
"for market analytics, indirect sensing is more convenient than direct sensing because the indirect mechanism avoids negotiations with wireless service providers and telecommunication companies. data can be accessed via dedicated apps or harvested from social websites."
"based on all the vertices and edges in this network, a matrix of transition probabilities is formed by calculating the in-degrees and the out-degrees of the vertices. notably, a user trajectory is a sequence of coordinates with timestamps. therefore, the system can compute the in-degree and the out-degree of a vertex. for example, assuming there are three vertices v 2 -v 4 adjacent to vertex v 1, the out-degrees from v 1 to v 2 -v 4 are respectively one, two, and three. thus, transition probabilities from v 1 to v 2 -v 4 are respectively 1/6, 2/6, and 3/6."
"algorithm 8 implements the theorem on the server side. edge deletion logic is similar to edge insertion case. upon receiving an edge deletion, it first finds out in which k-core graph this edges resides, say g ki . if it does not reside in any k-core, then the algorithm terminates. otherwise, update coreness cascaded algorithm described in algorithm 9 starts with the vertex with d g k i less than k i, moves it to the lower k-core graph g ki−1 . then it recursively traverses the neighbors whose degrees in g ki are now below k i . the algorithm accelerates k-core re-computing by knowing, at each iteration, which vertices have changed their degrees. for the majority of cases where an edge deletion impacts a small fraction of vertices in the k-core, we have found this improved algorithm to be very effective."
"the article is organized as follows: section 2 demonstrates the variability in optimizing for energy efficiency on cpus. section 3 presents our lightweight dvfs runtime approach. section 4 presents our compile-time framework to compute program features used to subsequently select the best frequency/core configuration for a program region, as detailed in section 5. sections 6 and 7 present extensive experimental results on 60 benchmarks and 5 multicore cpus. related work is presented in section 8 before concluding."
"at present, many graph clustering approaches have been proposed. among these approaches, spectral clustering, markov clustering, minimum cut, and k-means are most related to our case. notably, crowdsourced data can reach trillions. to process billions of vertices in a network, complexity is of prior concerns. algorithms that involve matrix decomposition like principal component analysis and singular value decomposition create too much computational time. thus, they are inappropriate in our case. in the following content, we use markov clustering as a case study because it is directly related to transition probabilities."
"as expected, the on-demand governor can significantly improve energy consumption on ivy bridge compared to powersave, but it can also be highly detrimental, in particular for more bandwidth-bound benchmarks. for haswell, on-demand is typically highly detrimental. in contrast, our proposed runtime approach is only rarely detrimental compared to powersave but, on average, significantly boosts energy savings for ivy bridge. it addresses the deficiencies of the two linux governors by looking at energy efficiency instead of simply cpu workload and offers a viable one-size-fits-all algorithm despite the processor-specific characteristics of the energy-minimizing frequency. note also that because powersave runs at the minimal frequency, our runtime approach can only equal or improve the overall execution time."
"the set of our proposed algorithms identify k-core subgraphs at multiple, fixed k values and maintain the identified subgraphs incrementally over dynamic changes. these distributed algorithms run on a multi-server cluster with shared nothing partitioned graph data, managed by apache hbase. the size of the social network graph and rich content is only limited by storage space and not by main memory. furthermore, identified communities at multi-resolution are also persisted and updated as changes come in. our algorithms thus enable practitioners to monitor changes in communities on different topics and resolutions in rich social network content simultaneously, which main-memory based algorithms cannot achieve."
"function stripmine performs a strip-mining of the loop so that the resulting outer loop has as many iterations as the maximal number of cores c available on the target machine, enabling us to analyze its inner loop l .inner that is the outer-most serial loop in a thread. we then perform parametric slicing to reason on the different trip counts this loop can have (e.g., for load-imbalanced cases, this loop may iterate 0 times). function count haszeroiteration counts the number of values of l for which the trip count of l .inner can be equal to 0; if it can be for more than half of the available cores, then this loop is considered to have poor scaling."
"as these data are generated along with trails, app types and their timestamps are labeled in the trajectories. we use the term -metatrails‖ to represent them. metatrails contain more geospatial and temporal characteristics that feature the preference of customers than gps trails do. different metatrails render various activities of customers. more importantly, metatrails are embedded with mobile geosocial networks."
"proposed approach. based on the preceding observations, we conclude that the best frequency to use to minimize cpu energy is per-processor, per-workload specific: one cannot be limited to looking at the cpu load (e.g., using on-demand) or using the minimal frequency (e.g., using powersave) to minimize cpu energy. we propose two approaches to address this problem: (i) a lightweight runtime that adapts the frequency based on the cpu energy changes dynamically during the application execution (see section 3); and (ii) a compile-time approach for static selection of the ideal frequency for each affine computation kernel within a full application based on (a) a new static analysis of the program's operational intensity and its potential for weak scaling across cores (see section 4) and (b) a characterization of the processor's power profile for a handful of extreme scenarios (e.g., compute-bound, bandwidth-bound, etc.) via microbenchmarking of a processor, as described in section 5."
"a hub in an affinity subnetwork, i.e., a vertex with the highest degree, is a pivotal place which people at adjacent hotspots have higher chances to visit. mining affinity subnetworks and hubs helps marketers discover combo stores and combo hotspots."
"mining the sequential patterns of mobile footprints is conducive to predicting crowd preferences and arranging store locations. nevertheless, it is difficult to analyze large-scale trails because of complexity. besides, sequential patterns with long duration are not practical for market planning. fortunately, affinity subnetworks reduce computational time since graph clustering breaks a large graph into small components."
"previous research has shown that the increase in execution time when throttling down the frequency can be limited to a very small quantity by performing careful dvfs, leveraging the fact that, on bandwidth-bound applications, the processor frequently stalls waiting for data transfer. therefore, processor frequency could be reduced without significant wall-clock time increase (if the latency of main memory operations is not affected by dvfs), resulting in energy savings [cit] . moreover, when considering the cpu energy alone (in isolation from the rest of the system), it could be intuitive to think that the lower the frequency, the lower the energy consumption: because power is often approximated to have a cubic relationship with frequency (assuming frequency and voltage have a linear relationship), using the minimal frequency (e.g., as with powersave) is expected to increase execution time linearly but decrease power in a nearly cubic way, thereby leading to minimal cpu energy. we now show that this is not always true."
"in table iii, we compare the same three approaches. note that the best approach here is the result of an empirical search for the best frequency/core configuration that maximizes edp. we additionally report the average wall-clock execution time increase using our predicted configuration versus using performance."
"for conquesting in a city, the main focus is on profitable regions of interest or points of interest. as large traffic flows increase visibility of stores, a hotspot network provides a good indicator for retail store locationing. moreover, affinity subnetworks can further store locationing analysis from individual sites to combo-site selection. advertising agencies can utilize affinity subnetworks to project related-product images onto customers because affinity subnetworks represent customer preferences. when geo-conquesting meets city planning, it becomes city marketing. city planners can improve public transportation and infrastructures by exploring affinity subnetworks. metropolitan branding will become more effective via dynamic crowdsourcing."
"with such a tremendous number of subscribers using a variety of apps, crowdsourced data from mobile sensing become a valuable resource for market planning. for marketers, user behavior reasoning is an important subject for targeting potential customers. analysis on mobile sensing data provides in the cyber age, metadata collected via mobile sensors facilitate geo-conquesting, for mobile sensing reveals customer activities and environmental conditions. marketers can benefit from these crowdsensed data by carefully investigating regional dynamics."
"for each textual statement in the program, the set of its runtime instances is captured with an integer set bounded by affine inequalities intersected with an affine integer lattice [cit] ] (i.e., the iteration domain of the statement). each point in this set represents a unique dynamic instance of the statement, such that the coordinates of the point correspond to the value the surrounding loop iterators take when this instance is executed. for instance, for statement r in figure 3, its iteration domain d r is:"
"data space. we first define the data space of an array a for a program (i.e., the set of data accessed by all references to a specific array) during the entire program execution. the data space is simply the union of the sets of data elements accessed through the various access functions referencing this array for each value of the surrounding loop iterators where the reference is executed. the polyhedral program representation enables the use of the image of a polyhedron (e.g., the iteration domain) by an affine function (e.g., the access function) to capture data spaces. the image of a polyhedron d by an affine function f is defined as the set"
"where v is the set of vertices, e is the set of edges, m [v, e] and c [v, e] are the structured metadata and unstructured content respectively. the paper simplified its description by including all vertices in the k-core computation while in practice, our system can be used to construct and maintain multiple k-core subgraphs on different metadata topics and context simultaneously. the problem of k-core subgraph identification is formally defined as follows:"
"on each target machine, we used the different frequencies made available by the linux os running on these machines. table i lists the frequency ranges. we used 5-6 frequency steps per machine, spaced evenly in the frequency range. we tested three core counts: one, half, and all available. so, a total of 78 configurations (15-18 per machine) was tested for each of the 60 binaries. we empirically found the best configuration (frequency/core) to optimize energy or energy-delay product for each benchmark when using a fixed configuration for the entire kernel execution. in the following, these are designated as the best configuration for a given benchmark and optimization metric."
"finally, we conclude by displaying our process to assign at compile-time a frequency to an affine program region. a program is classified using its oi and its parallelism scaling features to determine to which of the four preceding categories it is closest to. in particular, we found that ordering the decision by prioritizing the cases where the dominant effect has the highest impact delivers the best results."
"given the nature of the workloads evaluated, both conservative and on-demand use the maximal cpu frequency most of the time. consequently, the cpu energy compared to powersave increases on average because, for most benchmarks, maximal frequency is not the way to minimize cpu energy. this is especially true for sandybridge and haswell. in contrast, for ivy bridge, where running at maximal frequency is often good for energy optimization, the energy loss of these governors compared to powersaveis very small. this analysis holds true for edp results in figure 7 : significant edp savings over powersave are achieved by these governors, especially for ivy bridge. note in all cases that our static approach ours outperforms these governors, whether it was set to optimize cpu energy or to optimize energy-delay product."
"to make the situation more complex, in practice, the most energy-efficient frequency is also affected by how the execution time evolves as a function of frequency. when the execution time decreases at a slower rate than the frequency increases (e.g., the expected acceleration is not achieved), this shifts the optimal frequency toward lower values than for the compute-bound cases like dgemm/mkl. this is exemplified with a bandwidth-bound benchmark, as shown in the bottom row of figure 1 . j2d is a jacobi 2d code from polybench/c 3.2 which is parallelized naively among rows of the image and uses out-of-l3 data, too. it represents a bandwidth-bound case which is more realistic (e.g., less exacerbated) than the stream [cit] ] benchmark. 1 we see a systematic shift of the most energy-efficient frequency toward the left; that is, lower frequencies. in addition, due to bandwidth saturation effects, the code does not have good weak scaling: adding cores does not decrease the execution time linearly. it leads to higher energy consumption increase for the four-or eight-core cases than for single-core, when the frequency increases. this motivates the need for an approach that also considers the application characteristics to determine its most energy efficient frequency."
"the static analysis was also implemented in the pocc compiler in a fully automated way, as a new polyfeat module in pocc, to compute the features of the input source code to enable its categorization, as presented in the previous sections. each of the 60 c source code was separately analyzed, with the analysis taking at most a second to complete."
"1) in insertion scenario, 1000 randomly chosen edges are inserted into the graph. those random edges are selected from the graph and deleted before materialized k-core graph is constructed. 2) in deletion scenario, 1000 randomly chosen edges are deleted from the graph. 3) in mix scenario, insertion and deletion scenarios are run simultaneously where one insertion is followed by one deletion. we repeated these three scenarios with each dataset and measured their execution times. fig. 4 plots the speedup through our incremental maintenance algorithms over recomputing k-core from scratch, for 9 different datasets. the y-axis shows the speedup in log-scale. for insertion, deletion, mix scenarios and each dataset, the figure gives the speedup of incremental update approach with respect to from-scratch construction using the multi k-core construction algorithm. as the figure shows, three to five orders of magnitude speedup can be expected for edge insertion workload. similar speedup factors are also observed for mixed edge insertions and deletions with one to one ratio. higher speedup, more then five orders of magnitude was achieved for edge deletion only workload. note that storing a new edge in hbase without maintenance algorithm took 3 ms on the average."
"our work learned from the strength and limitation of these algorithms and platforms to make progress in the areas of distributed big graph data processing and incremental multiresolution maintenance. we implemented, tested and analyzed our algorithms on an open-source big-data processing framework. therefore, before getting to the details of our proposed algorithms, we first would like to briefly introduce in the next section the big data programming framework where our distributed k-core algorithms are implemented."
"in this work, we demonstrated inherent limitations to existing simple dvfs approaches due to processor-specific and application-specific effects that must be considered. we demonstrated that for a class of computations-namely, affine programs-we can develop a compile-time categorization of programs by approximating their oi and parallel scaling. this allows us to automatically select at compile-time the best frequency and number of cores to use to optimize cpu energy or the energy-delay product without the need for any application profiling or runtime monitoring. our evaluation on 60 benchmarks and five multicore cpus validated our approach, obtaining significant cpu energy savings and edp improvements over the powersave linux governor."
"a fundamental property of affine computations is to have only static control-flow and data-flow; that is, the code executed does not depend on the dataset value but only on the value of loop iterators and program constants. this regularity enables the design of key static analyses for these program regions, for instance, to characterize their operational intensity. we aim to substitute our runtime approach based on runtime oi inspection by a compile-time characterization of the program region using novel analyses we now develop."
"customer trails, as mentioned earlier, provide useful clues for market analysis. previous works [cit] usually focused on generation of heatmaps for displaying regions of interest in a city. however, valuable information is hidden in the crowdsourced trails, for instance, sequential visiting patterns, transition flows, and site combinations. to extract such hidden information that reflects customer behavior, this study investigates the dynamics behind crowdsourced trails, including hotspot networks, transitions, and affinity subnetworks by using graph analysis. figure 2 shows the overview. heterogeneous data transmission while different apps are running is used in the model. such information is another indicator of user dynamics, especially when the data are co-displayed with trajectories. typically, two types of data are formed while apps are running. one is the trails based on app types (e.g., instant messaging apps), and the other is crowdsensed multimedia. this study highlights the former type rather than the latter because crowdsensed multimedia involve pattern recognition, and this study focuses on graph analysis."
"we conducted an extensive characterization of the energy and energy-delay product of the same benchmarks as in the previous section, for a total of 60 benchmarks. each was compiled using gnu gcc-4.8.1 with -o3 [cit] -update5 compiler with -fast optimization, for a total of 120 different binaries evaluated. table ii reports the summary of experiments when optimizing for cpu energy minimization. for each 60 benchmarks, the average energy savings (e) compared to powersave is reported for three approaches. best is the best frequency/core configuration found by exhaustive search of all frequency/core configurations individually for each binary tested and each cpu tested. perf. is the performance linux governor, and ours is our static approach."
"for haswell, the empirical one-time characterization of the machine by microbenchmarking as described in section 5.1 gave the following four configurations to use when optimizing for energy: (1) 1.6ghz/4 cores for bandwidth-bound codes, (2) 2.0ghz/4 cores for compute-bound codes, (3) 1.2ghz/2 cores for codes with poor parallel scaling, and (4) 2.5ghz/1 core for sequential codes. the configurations found when optimizing for edp were (1) 2.5ghz/4 cores, (2) 3.5ghz/4 cores, (3) 2.0ghz/4 cores, and (4) 3.5ghz/1 core. the category to which a binary was classified by the decision tree discussed in section 5.2 can be inferred in table iv from the frequency/core pair reported in ourstatic."
"given a frequently visited spot r on a map, a tree is created by tracing all the routes (i.e., walks), of which the distance is one. next, removing all the vertices, of which the visiting frequencies are lower than a predefined threshold, generates a new pruning tree. iteratively selecting a vertex in this tree yields a network g."
candidate edges for g ki+1 . partial kcore in algorithm 7 then processes g candidate subgraph and returns the graph qualified for k i+1 core into g qualif ied .
"finally, when optimizing for edp, we observe consistent conclusions between the different schemes as when optimizing for energy, with our approach consistently outperforming competing schemes and being close to the best achievable in our framework. figure 6 presents a comparative study of the energy savings of various dvfs schemes. we report the results of cpumiser with different performance degradation parameters: 10%, 50%, and 100% respectively. we also tuned the linux on-demand (od) and conservative (co) governors. they are both dynamic schemes based on cpu usage and can be parameterized for different cpu loads. the conservative governor adapts the frequency gracefully, while on-demand goes to maximal frequency when the load threshold is reached. we vary the cpu load threshold from 45% to 95% by a step of 10% for both governors. we also report ours, the performance of our static approach as presented earlier, for comparison. figure 7 summarizes the energy-delay product savings over the powersave governor for the same dvfs schemes."
"an example of hotspot networks is shown in fig. 3 . this hotspot network displays frequently visited stores (i.e., red marks) in a mall, where the black lines are frequently visited routes."
"the decision process is identical whether we optimize for e or edp. only the frequency/core configuration selected changes between e and edp using the best configuration found by profiling for each metric independently. the decision process is as follows: 1) if the code is sequential, choose config_sequential; 2) else, if the code is bandwidth-bound (i.e., its oi is below threshold, found during profiling), choose config_bwbound 3) else, if the code has poor scaling, choose config_poorscale 4) else, choose config_computebound."
"the base algorithm is an adaptation of the bz algorithm to distributed processing for a fixed k value. as described in algorithms 1 and 2, the server side algorithm executes in parallel as hbase coprocessors to scan partitioned graph data in the local regions and delete those vertices with degrees less than k. the client side program monitors parallel execution and issues iterations until k-core is found. to compute k-core graph for multiple k values, this algorithm is called for each k value separately."
"furthermore, regardless of start points, the equilibrium distribution is the same. markov clustering employs two major operations -expansion and inflation -for graph clustering. the former tests connectivity between vertices when taking the hadamard product. the latter increases tightness of clusters. eventually, iterations result in separation of the network."
"after clustering, each cluster forms an affinity subnetwork, namely, a combination of frequently visited hotspots, as shown in fig. 4 . an affinity subnetwork indicates that people prefer visiting these places in combination. this is because markov clustering simulates people randomly walking in these hotspots based on their interest (i.e., transition probabilities) when sufficient crowdsourced trails are collected. therefore, strong connectivity is created among hotspots."
"for mall operation, how to select a group of stores and deploy them together is an important topic since an appropriate combination of stores creates a weighting factor in in-store visits. with the use of graph clustering and transition flows, the entire hotspot network is separated into several affinity subnetworks. each subnetwork represents combo stores. with such combo information, mall managers can preallocate space for lease. storeowners can join an affinity subnetwork and open a store that fits this subnetwork."
"at present, much effort has been devoted to the research on consumer purchasing behavior and patterns. nevertheless, few it approaches were proposed for maximizing the interest of enterprises. to this end, this study concentrates on the side of marketers. in the rest parts, several novel ideas are examined to deal with the challenge of geo-conquesting in smart cities."
"to the best of our knowledge, this paper is the first to propose a horizontally scaling solution on the big data platform for multi-resolution social network community identification and maintenance. by using k-core as the measure of community intensity, we proposed multi-k-core construction and incremental maintenance algorithms and ran experiments to demonstrate orders of magnitude speedup with the aggressive pruning and fairly low maintenance overhead in the majority of graph updates at relatively high k-valued cores. for the simplicity of the presentation, we left out the metadata and content associated with graph vertices and edges. in practice, a k-core subgraph is often associated with application context and semantic meaning. our efficient maintenance algorithms now enable many practical applications to keep many k-core materialized views up to date and ready for user exploration."
"we make several key observations from these data. first, the optimal cpu energy is not achieved for the minimal or maximal frequency in most cases. taking the case where all cores are used, on sb-4 and hsw-4, minimal energy is achieved at 1.6ghz (the minimal frequency for our sb setup), but it is achieved near the (but not at) maximal frequency for the two ivy bridge processors. the reason relates to the voltages used and, in particular, to the ratio of voltage changes versus the ratio of frequency changes for each machine. table i shows that, for the two ivy bridge cpus, the voltage range (from 0.97v to 1.09v) is much smaller than for the other two cpus. overly simplified power equations ignore key effects such as the relation between leakage current and temperature and frequency and voltage relations. a more realistic power equation [cit] captures the poole-frenkel effect, which relates leakage to temperature, and careful derivation of the evolution of the power equation as a function of changes in v, f, and temp demonstrates that the slope of increase of voltage versus frequency can influence the optimal frequency for energy efficiency. de [cit] developed an analogous characterization on a mobile processor, modeling the energy variation between frequency steps as a function of voltage and temperature and obtaining a curve similar to our result for haswell. they derived formulas to characterize the convex shape of the cpu energy efficiency for a fixed workload as a function of cmos characteristics including voltage and frequency increase relations. here, we observe across four different x86 intel processors four different cases for the most energy-efficient frequencies for compute-bound codes. a runtime approach focusing only on workload properties (e.g., the lack of stall cycles) and not taking into account these processor-specific effects would fail to select the optimal frequency for cpu energy minimization."
"algorithm for miss estimation. we are now equipped to build our procedure to estimate the misses. the idea is the following: we will recursively compute the dl of a loop (summing it for all arrays accessed by that loop) from the inner-most loops to the outer-most loops. for each loop l, its number of misses is estimated as either the product of its trip count and the number of misses of its loop body if the dl of this loop exceeds the cache size or as its dl if it is smaller than the cache size. for inner-most loops bodies, we set their number of misses to the dl of the loop body, regardless of its value. some additional treatment is done to capture the data reuse between consecutive iterations of a loop body to adjust the number of misses. we optimistically assume that if data are reused between two iterations, then they correspond to the part of the data that was loaded last at the previous iteration (e.g., if this set is smaller than the cache size, it is not evicted from the cache by other data of the previous iteration). see algorithm 1. for simplicity, we assume that the entire program is surrounded by a fake loop root having a single iteration, and we note that dl[] is a map, where dl [x] associates object x to an integer value."
to the other vertices in g k is greater than or equal to k. g k is the maximum subgraph in g with this property.
"compute-bound code. for this case, we use off-the-shelf dgemm/mkl, in the same setup as shown in section 2. we observed the near-perfect scaling of this code/problem size across the architectures tested, and, although being totally compute-bound by nature, it is an actual workload with heavy data traffic."
"parallel graph algorithms have a long history with high performance computing. most early studies, however, targeted static graphs [cit] . more recent work implemented graph algorithms on mapreduce framework [cit] and its open source implementation apache hadoop [cit] . however, the iterative nature of many graph algorithms soon prompted many to realize that static data is needlessly shuffled between mapreduce tasks [cit] . pregel [cit] thus proposed a new parallel graph programming framework following the bulk synchronous parallel (bsp) model and message passing constructs. two apache incubator projects, giraph [cit] and hama [cit], inspired by pregel, are looking to implement bsp on top of hadoop infrastructure."
"this feature is inspired by gps navigation systems, widely used in our daily lives. for a tourist attraction, it is usually a frequently visited spot, or a finish point. observations show that a waypoint or the start point on the route is sometimes another frequently visited spot. with sufficient data, there exists a frequently visited route between two hotspots. large-scale metatrails collected from crowds can substantiate such an observation. this feature is applicable in a city or a site. the following steps show how to build a hotspot network."
"runtime algorithm. the main idea of our runtime is to continuously inspect the changes in energy efficiency; that is, the energy consumption normalized by the number of instructions executed and the changes in operational intensity (oi) of the application. a decision to change frequency is made either because the oi has changed, indicating a change in the nature of the computation being performed, or because the energy efficiency has decreased compared to the last sample. within a single phase (i.e., a time segment with similar oi), we exploit the energy convexity rule: there is only one frequency maximizing energy efficiency [cit], as illustrated in the previous section. consequently, our algorithm to find this frequency simply performs a gradient descent in the space of frequencies. the algorithm is shown on the right."
code with poor scalability. the benchmark used here essentially consists of an openmp parallelization of a triangular loop with low trip count to ensure high load imbalance between threads. the computation performed in the loop body has balanced arithmetic intensity and is inspired from lu factorization.
"traffic flow management is another benefit brought by hotspot networks. mall operators can examine transition probabilities, crowd directions, and flow capacities to redeploy mall facilities and balance flows. compared with crowd flows or heatmaps in the unstructured open space, flows detection based on hotspot networks generate more concrete information on store connectivity. as various app-users generate different routes, electronic billboards or digital billboards can be set up in the hallway for displaying advertisements. additionally, advertising content can be proactively changed based on flow types for targeting customers. mall operators can dynamically charge advertising agencies according to flow amounts. when combined with sequential visiting patterns, deployment of stores and digital billboards can be reshaped to fit the flow directions, subsequently bringing in more customers."
"energy profiles of cpus. here, we discuss in detail the energy profiles of four off-theshelf intel x86 cpus running on a linux desktop. table i outlines their key characteristics. we report in figure 1 a series of plots obtained by actual power and time measurements using hardware counters available with intel pcm [intel b], on dgemm/mkl [intel a] for the top row. dgemm/mkl is run on a large out-of-l3 problem size (reported to achieve near peak compute performance on these machines), and its execution time scales nearly perfectly with the frequency and number of cores used: it captures a compute-bound scenario exploiting fully all the processor capabilities. the data plotted are the measured cpu energy for the computation where only one benchmark is running (no co-execution) and where turbo-boost and other hardware dvfs mechanism have been turned off to obtain deterministic data. each frequency was set prior to benchmarking using the userspace governor using the cpufreq package, implicitly changing voltage along with frequency change as per the cpu specification."
"the oi of a program, typically in flop per byte for scientific codes, is the ratio of operations executed per data moved to execute these operations. the oi may be computed during runtime using a data movement count from the transfers from ram to the lastlevel cache or another level of cache. in this work, we are interested in categorizing a program region (e.g., a loop nest) as either memory-bound or compute-bound: only a coarse approximation of the oi is needed. we also have the goal of developing a very fast analysis that can be applied on large source codes. indeed, the result of applying polyhedral transformations to a program may lead to a code of thousands of lines from an input loop nest of a few lines, as is the case for numerous benchmarks we evaluate in section 6."
"the transition matrix of a hotspot network represents the preference of mobile crowds when they move between places. a visiting pattern can be discovered by performing subgraph analysis. one feasible way is graph clustering. when graph clustering is applied to a hotspot network, hotspots are grouped together, subsequently forming several subnetworks. as the objective of graph clustering is to group vertices that present high connectivity, we denote the resulting subnetworks as -affinity subnetworks.‖ the affinity herein is used to describe crowd preferences. affinity subnetworks further the analysis for retail site selection from single sites to combo-site selection. combo sites take multiple effects and yield more profits than single sites."
"during polar decoding, a crc check may be performed by inputting a sequence of a decoded information bits into a crc generator and comparing the resultant p crc bits with the sequence of p decoded crc bits. if the two pbit sequences match, then the crc check is successful and no errors are detected. alternatively, the sequence of a + p decoded information and crc bits may be input into the crc generator, in order to obtain a p-bit syndrome. if the syndrome comprises p zero-valued bits, then the crc check is successful and no errors are detected. note that this process must be adjusted when using rnti scrambling, crc interleaving and crc-aided scl decoding, as discussed in sections iv-c, d and f."
"note that pucch supports information block lengths of up to 1706 bits, which is significantly higher than the longest 140-bit information blocks supported in the pdcch. this is because the uci's channel quality indicator (cqi) includes channel measurements, which requires more bits to convey than the signalling decisions carried by the dci."
"meanwhile, the presented far results were obtained by decoding random gaussian distributed llrs. each simulation was continued until 1000 block errors or false alarms were observed. the capacity bounds provided in bound [cit] . this characterizes the theoretical limit on the achievable bler for qpsk modulation over an awgn channel as a function of snr e s /n 0 and the block lengths a and g."
"as we mentioned before, since the standardization of polar codes in 3gpp nr, the interest in polar codes has increased significantly, as shown in figure 2 . however, the current limitation of polar codes is that they do not readily lend themselves to soft-decisions, which limits their benefits in the content of both powerful turbo equalizers and multiuserdetectors (muds). these problems are likely to inspire coding experts and the broader 6g-research community. another fertile area of future research is related to the conceptions of polar codes for enhancing quantum computers and other quantum systems [cit] ."
"the rest of this paper is organized as follows. section ii surveys the standards-related articles. section iii briefly reviews the uplink and downlink transport channels of 3gpp nr and provides encoding and decoding block diagrams for polar coding in the control channels. section iv details the operation of each component in these block diagrams. an end-to-end example of polar encoding for the uplink control channel is provided in section v. section vi characterizes the error correction and error detection performance of the polar codes used in the nr control channels. following this, section vii provides a summary of lessons learned and possible improvements for future iterations of the nr polar code design, as well as for future applications of polar codes beyond 5g. finally, section viii offers our conclusions. the skeleton structure of the paper is shown in figure 1 ."
"this paper has provided a tutorial and survey of the operation and performance of the polar codes used in the pucch, pbch and pdcch channels of 3gpp nr."
"the bler results are obtained using gray coded quadrature phase shift keying (qpsk) for communication over an additive white gaussian noise (awgn) channel, as a function of the signal to noise ratio (snr) e s /n 0 . this facilitates direct comparisons with the results that were presented in the numerous 3gpp technical documents that were considered during the specification of the 3gpp nr polar code. note that the only modulation schemes used by 3gpp nr for control information are bpsk and qpsk [cit] . in particular, bler vs. e s /n 0 is characterized for pbch in figure 28 for various scl decoder with list size l. furthermore, the snr e s /n 0 required for the pucch and pdcch polar codes to achieve a bler of 10 −3 is characterized in figures 29 and 30, as function of the information block length a, encoded block length g and the scl decoder with list size l."
"this section provides an overview of channel coding in the physical channels of nr. in section iii-a, we will introduce various nr physical channels, then in section iii-b and section iii-c, we will elaborate in particular on the uplink and downlink control channels, with the aid of block diagrams for their channel encoding and decoding."
"in the decoder, the same process is used to determine the positions of the frozen and pc bits. the redundant bits are used to aid the polar decoding core, as described in section iv-f. following this, the frozen and pc bits are removed, in order to obtain the decoded information and crc bits."
"each component in the block diagrams of figures 4 to 6 has a key feature, which is detailed in section iv and summarised as follows. code block segmentation splits long blocks in order to reduce the complexity, while the imposed crc attachment facilitates error detection and aids error correction. furthermore, the crc interleaver disperses the crc bits inserting them in between the information bits which in turn provides an early termination capability that reduces the complexity of pdcch blind decoding if the indications are that this decoding operation is likely to fail, despite continuing the computations. still referring to figures 4-6 polar coding provides the main error correction capability, while sub-block interleaving increases the polar code's error correction capability and the channel interleaver has the role of dispersing burst errors."
"pucch/pusch: figure 4 provides a block diagram for channel encoding and decoding in pucch/pusch. the encoding block diagram illustrates the conversion of an information block a into an encoded block g, including all intermediate operations defined in ts 38.212, namely code block segmentation, crc calculation and attachment, crc interleaving, frozen and pc bit insertion, polar coding core, sub-block interleaving, bit selection, channel interleaving and code block concatenation, as will be detailed in section iv. the corresponding decoding operations are illustrated in the decoding block diagram of figure 4 . the operations enclosed in solid lines in figure 4 are within the scope of this paper, while the operations enclosed in dashed lines are outside of the scope."
"crc calculation and attachment are defined in [37, subclause 5.1]. during polar encoding, a number of redundant crc bits are appended to each information block segment a . these redundant crc bits allow the polar decoder to perform error detection, as well as to aid its error correction [cit] . more specifically, the polar decoder can perform crc checks to determine if a codeword is free of errors, as well as to select a codeword from a list of decoding candidates, as it will be detailed in section iv-f."
"based on the lessons learned, as detailed in section vii, we would like to inspire the community to circumvent the above-mentioned limitations by conceiving powerful softdecision-aided polar coded turbo-transceivers, as well as wireless multimedia systems. numerous open questions have to be solved, such as the design of flawless polar coded audio and video systems, automatic repeat request (arq) solutions and quantum systems, just to name a few. given the complex design trade-off's and conflicting metrics such as the code-rate, code-length, coding-delay, complexity etc., the radical idea of determining the optimal pareto-front of polar codes arises. to elaborate a little further it is a challenging but extremely promising future research idea is to catalogue the set of 'best' polar codes, depending on the specific application considered. this set of codes would contain all the candidates, which exhibit for example a performance closest to capacity at a given code-rate, or codeword-length or complexity. viewing this radical new design approach from a difficult perspective, it would not be possible to approach capacity more closely without increasing the complexity or the codeword length, or without reducing the code-rate, for example."
"the gnodeb multiplexes dci intended for various connected ues into the pdcch. however, in order to reduce the control overhead, the gnodeb does not explicitly signal to the ues where they can find their intended dci within the time and frequency resources of the pdcch. instead, each ue performs a blind decoding process, in which it attempts the decoding of several hypothesised blocks having various combinations of information block length a, encoded block length g, dci type and location within the pdcch. as detailed in section iv-c, the pdcch polar code adopts an radio network temporary identifier (rnti) scrambling process in order to ensure that an incorrect hypothesis will lead to a failing crc with high probability, and that a passing crc can only be obtained with high probability for a correct hypothesis."
"it can be argued that there are four interleaving operations used in the nr polar code design, namely the crc interleaver of section iv-d (which is only used in pbch and pdcch), the pc and frozen bit insertion operation of section iv-e (which can be considered to be a form of interleaving), the sub-block interleaver of section iv-g and the channel interleaver of section iv-i (which is only used in pucch). however, these interleaving operations have complex implementations and required the introduction of buffers, i.e., increased hardware resources and impose extended latency. future applications of polar codes may with to merge or remove some of these interleavers."
"in the decoder, code block segmentation is reversed by concatenating the c number of decoded information block segments and removing the padding bit if one was used in the encoder."
"in nr, the physical uplink control channel is referred to as pucch, although some control information can also be carried in the physical uplink shared channel (pusch). likewise, the downlink control channels are referred to as pdcch and physical broadcast channel (pbch), where the later is used to broadcast control information to all ues connected to the gnodeb. the information and encoded block lengths that are supported for these channels are summarised in table vi. the control information transmitted in the control channels is used to coordinate the many ues connected to the gnodeb, in order to manage the transmission of data on the data channels and facilitate initial connections to the gnodeb [cit] . more specifically, pucch and pusch delivers uplink control information (uci), which comprises hybrid automatic repeat request acknowledgements (harq-ack), scheduling requests (srs), radio resource control (rrc) signalling messages and csi [cit] . meanwhile, pdcch delivers downlink control information (dci) [cit], which comprises transport format, resource allocation and harq information [cit] . the pbch conveys system information, which allows ues to connect to the gnodeb [cit] ."
"this section analyses the block error rate (bler) and far of the nr polar codes, in order to characterise their error correction and error detection performance, respectively. here, bler is defined as the fraction of transmitted blocks that are decoded erroneously [cit], while far is defined as the fraction of erroneously decoded blocks that nonetheless have a passing crc, in other words \"missed detection of an error event\" 2 [cit] ."
"2) pdcch: the channel encoding and decoding block diagrams of pdcch are provided in figure 6 . similarly to the other block diagrams, the operations enclosed in the solid lines in figure 6 are within the scope of this paper, while those enclosed in dashed lines are beyond our scope. the encoding block diagram illustrates the conversion of an information block a to an encoded block g, including dci bit sequence generation, crc initializing, crc scrambling, crc calculation and attachment, crc interleaving, frozen bit insertion, polar coding core, sub-block interleaving and bit selection, as defined will be detailed in section iv. the corresponding decoding operations are illustrated in the decoding block diagram of figure 6 ."
"t he 3rd generation partnership project (3gpp) represents a collaborative effort invested in the standardization of global network protocols, including the universal mobile telecommunications system (umts) [cit], which were the first examples of developing technical specifications for the 3rd generation (3g) of mobile telecommunication [cit] . the 3g concept fuelled the smart-phone revolution and high speed internet services, audio and video file transfers and other compelling services [cit] . the increasing demand for data, video and messaging traffic at higher throughput was led to manuscript the 4th generation (4g) 3gpp standard known as the long term evolution (lte), which is based on a transmission control protocol/internet protocol (tcp/ip) model [cit] for seamless integration with the internet. recent years have seen a rapidly growing population and increasing use of mobile devices with greater expectations of quality of service (qos) [cit], as well as the need for ultralow latency and ultra-reliable web connectivity applications of internet of things (iot) both in industry and in daily life [cit] . this motivates the development of the 5th generation (5g) of mobile communication systems."
"after determining the value of n and the corresponding nr polar code sequence q n −1 0, the positions of the k information and crc bits and the positions of the n−k frozen and pc bits are determined, as detailed in figure 17 . in particular, bit positions that corresponding to puncturing or shortening during rate matching are used for frozen bits. in general, frozen bits are inserted into the remaining lower reliability positions, while information and crc bits take the remaining higher reliability positions, as identified by the nr polar code sequence q n −1 0 . this process is dependent on the rate matching mode that is used, as discussed on section iv-h."
"the final step of polar code rate matching in nr is channel interleaving, as specified in [37, subclause 5.4.1.3]. however, channel interleaving is only used for pucch, while it is skipped for pdcch and pbch [cit] . the motivation for applying channel interleaving is that it improves the polar code's error correction capability when employing higherorder modulation schemes for pucch transmission over fading channels [cit] ."
"during crc checking in a receiving ue, the last 16 bits of the received crc are similarly scrambled using the ue's rnti. in this way, only the intended ue will be able to pass through the crc check. this avoids the requirement for the basestation to explicitly include 16-bit rnti of the destination ue in the transmitted dci."
"the nr physical layer is comprised of several physical channels, as described in table v . the uplink physical channels convey information from the ue such as a mobile handset, to a basestation, which is referred to as a nr basestation (gnodeb). meanwhile, the gnodeb transmits to the ue in the downlink physical channels [cit] . these physical channels correspond to particular sets of time-frequency resources [cit], and may be classified as data channels for the transmission of user information, or control channels for the transmission of control information [cit] ."
"the international telecommunication union radiocommunications (itu-r) has defined three 5g [cit] and beyond, namely enhanced mobile broadband (embb) for high-capacity and ultra-fast mobile communication, ultra-reliable and low latency communication (urllc) for mission critical applications such as vehicle-to-vehicle (v2v) and massive machine type communications (mmtc) for industrial and iot applications [cit] ."
"we commenced with an overview of the history of mobile communication and the motivation behind the introduction of nr. we discussed why polar codes have been selected for the nr control channels, with a summary of the 3gpp meeting outcomes that led to their specification. after that, the pucch, pbch and pdcch channels of 3gpp nr were briefly reviewed, complemented by a discussion of the encoding and decoding block diagrams of polar coding in these control channels. then, the operation and motivation of each component in these block diagrams was detailed, with the help of schematics, flow charts and examples. an end-to-end example of pucch polar encoding was provided to illustrate the step by step operation of these components. finally, the error correction and error detection performance of the nr polar codes was comprehensively characterized using our bler and far analysis."
"as in most standardised communication schemes, 3gpp nr uses channel coding to enable error correction and error detection in the presence of noise, fading and interference. during the specification of 3gpp nr, the channel coding of user data was considered separately from the channel coding of control information, such as channel state information (csi) and scheduling information. various channel coding techniques were compared [cit] and in particular, turbo codes [cit], low density parity check (ldpc) codes [cit] and polar codes [cit] were considered since they are all capacity [cit] . for these potential channel coding schemes, the ran1-86b meeting of 3gpp captured comparisons [cit] of the error correction capability, flexibility, hybrid automatic repeat request (harq) support, decoding latency, implementation complexity and other considerations. these observations are presented in table ii. [cit], ldpc codes were selected for the data channels of nr, while polar codes were selected for the control channels, replacing the turbo and tail-biting convolutional codes (tbcc) of lte, respectively. ldpc codes were selected for data channels because they can efficiently support multiple code rates, block lengths and harq, with better decoding latency, throughput and implementation complexity than other codes [cit] . in the case of the control channels, polar codes were selected, because they offer the best error correction capability at the short information block lengths that are used for control information [cit] ."
"the results revealed cortical modulation that reflects streaming and attention, but only in conditions that provided tonotopic differences between the tones; in conditions with only unresolved harmonics, and therefore no tonotopic differences, no significant modulation of cortical responses was observed. in addition, no brainstem or midbrain correlates of streaming or attention were observed in any condition."
"the plv spectra for the higher-frequency eeg responses demonstrate neural phase locking to the fundamental and harmonics of the tokens presented. as an example, fig. 4 shows phase locking in the unresolved singlestream conditions averaged over all 19 participants. in these conditions, the analyzed token was present in one condition (black traces) but absent in the other (red traces)."
"they were seated in clear view of a computer monitor, which indicated the target stream for the current block (high or low) and provided an immediate acknowledgment of each button press. however, no correct-answer feedback was provided. the participants were required to respond to the oddball tokens within the attended stream by pressing a button immediately after each detected oddball, and to suppress responses to oddballs in the unattended streams."
"we emphasize that although the metamer stimuli created for this experiment are specific to animal shapes, the same technique could be applied to any class of shape."
"this conditional independence assumption implies that the elements form a markov chain in which, given a sequence of elements, the location and orientation of the next element depends only upon the location and orientation of the element immediately preceding it in the chain. this markov property gives rise to an optimal substructure property that allows maximum probability contours to be identified in polynomial time despite the exponential size of the search space (figure 2c )."
"we emphasize that although the closed metamer stimuli created for this experiment are specific to animal shapes, the same technique could be applied to any class of shape, given a sufficient number of exemplars to learn the statistical model for local turning angles."
"human brain studies of illusory contour perception using event-related potential (erp; [cit] ), magnetoencephalography (meg; [cit] ) and trans-cranial magnetic stimulation (tms; [cit] ) methodologies point to an analogous cooperative recurrent computation involving early and late visual areas in ventral stream. similar feedback mechanisms have been proposed to account for the perceptual assignment of figure/ground relationships [cit] and border ownership [cit] ."
"in addition to closure, there are a number of other global shape attributes known to be perceptually important that might play a role in perceptual grouping. these include convexity [cit] and symmetry and parallelism [cit] . [cit] have also advanced a more general theory of shape detectability based on efficient coding. they suggest that shapes that are more likely to occur in our visual environment will be coded more concisely by the brain, and that these more compact encodings for common stimuli will lead to more efficient detection. our results are at least partially consistent with this theory, as the animal shapes we use here are surely more probable ecological stimuli than the metamers. it also seems plausible that our closed metamers are more ecological than our open metamers."
"pettet, mckee, and grzywacz (1998; [cit] ) found that introducing larger turning angles or inflections in a contour lowers detectability. this raises the question: could the enhanced detectability of closed animal shapes relative to open metamers be due to some difference in the average magnitude or sign of the turning angles? by construction, the magnitude of the turning angles for animals and metamers are identical, so certainly a difference in the average magnitude of the turning angles cannot explain our results. what about sign? while the signs of the turning angles for m 1 metamers match those for the animal contours, the signs of the turning angles were selected randomly for m 2 metamers, yet no effect on performance was observed. thus it seems that differences in the signs of the turning angles also cannot explain our results."
"data from the high-frequency band were also epoched into 1-min blocks, re-referenced, and baselined before responses to the single 50-ms tokens were isolated. individual tokens were baselined to the mean of the 75 ms preceding the token. each test session produced 796 high-f0 tokens and 398 low-f0 tokens for each block type, and combining data from the three sessions resulted in 2388 high-f0 tokens in each polarity and 1194 low-f0 tokens in each polarity for each experimental condition."
"nevertheless, experiment 2 leaves the factor of closure confounded with other potential nonlocal factors determining perceptual grouping. the relative improvement in detectability of the animal stimuli with increased target length could be due to closure but could also be due to other concomitant global shape cues that are progressively revealed. experiment 3 serves to more clearly separate these two possibilities by introducing a new class of closed metamer stimuli. local geometry (turning angles) of both open and closed metamers is by construction matched to each other and to the animal shapes. the closed metamers are also constrained to be closed but otherwise are maximum entropy. thus the sole difference between closed and open metamers is closure. the finding that closed metamers are more detectable than open metamers is thus clear evidence that closure does play a role in the detection of contours in clutter."
"the significance of the results reported here is that global cues beyond the local association field are seen to codetermine contour grouping. this evidence demands a revision to the standard model, a revision that allows for global shape properties to influence local grouping. these global shape and object proper-ties are coded deeper in the ventral stream, in area v4 and te/teo, for example [cit] and the responses of neurons in these areas depend upon input from the early visual areas coding local grouping relationships. we consider two alternative neural architectures that might account for the influence of global shape properties on perceptual grouping."
"to answer this question, we analyzed the number of inflections in target contours that were successfully detected (hits) versus the number in target contours that were not detected (misses). figure 12b shows the results. we see that there is no significant difference between the number of inflections in hit trials versus miss trials, for any of the stimulus conditions (p . 0.05 for matched-sample t tests over our 10 observers for all three stimulus conditions at all three target lengths). we thus conclude that a difference in the number of inflections does not explain the observed difference in detectability of animal and metamer stimuli."
"participants' behavioral responses during the eeg recordings were categorized into \"hit,\" \"false alarm,\" and \"random\" responses. hits were defined as button presses that occurred between 0.8 and 1.8 s after an oddball event in the target stream. false alarms were defined as button pushes that occurred between 0.8 and 1.8 s after an oddball event in the non-target stream, and random responses were all other button presses. these criteria were developed based on response timing during pilot blocks. random responses were relatively rare, averaging less than one every other block, and so were not analyzed further."
"property is commutative and so preserved in the metamer despite the shuffling. this regularity can be considered a ''convexity bias'' as it favors one sign of curvature over the other. as this convexity bias is a nonlocal property that could feasibly be a cue to perceptual grouping, we generated a second form of metamer without this bias by randomizing the sign of all of the turning angles. we refer to these two forms of metamer stimuli as m 1 and m 2, respectively. line segments were drawn in white on a midgray disk 208 in diameter, within a black rectangular screen."
"higher order inscribing polygons could also be used. for example, if a quintet of points is sampled, the result will be a two-dimensional family of pentagons with exactly the same side lengths. note, however, that in general a number of shape changes must be proposed before finding one that does not induce a selfintersection. we use the tetragon method because it involves the least computation, and is thus practical even when the proportion of valid proposals is small."
"although this markov assumption has been used by computer vision algorithms to extract global contours from complex natural images [cit], there are a number of reasons to be skeptical that it represents a complete model of human perceptual grouping. first, it has been noted that these algorithms can fail in some cases where the human visual system seems to have little trouble [cit] . second, the distribution of intrinsic distances between high-curvature events on natural contours has been shown to be incompatible with the markov assumption [cit] . third, the markov assumption does not accommodate important global topological constraints such as closure and avoidance of self-intersections [cit] . finally, we note that equation 2 implies that the whole is exactly equal to the sum of its parts: not a particularly gestalt assertion! for all of these reasons, it seems likely that the standard model is incomplete, and that the human visual system is somehow capable of combining these important local cues with additional global cues for the perceptual grouping of contours. the goal of this paper is to psychophysically test this hypothesis."
"in our first experiment we used a simple shuffling trick to achieve this, which produced first-order metamer contours with exactly the same local geometry as animal contours. the results (figure 8 ) clearly indicate that the animal shapes are easier to detect in oriented noise than the first-order controls. a series of control experiments ruled out trivial explanations based on differences in size, elongation, eccentricity, or self-intersections, pointing to the importance of more meaningful nonlocal regularities. a specific and somewhat controversial candidate is closure."
"one important acoustic feature that allows for perceptual segregation is the spectral content of sounds. segregation based on differences in spectral content occurs via the frequency-to-place mapping, or tonotopic organization, that is established in the cochlea and is maintained throughout the auditory pathways up to and including the auditory cortex. perceptual studies have shown that stream segregation can occur on the basis of tonotopic separation between alternating sounds [cit], and some physiological studies have reported correlates of perceptual streaming of pure-tone sequences, even in the absence of physical stimulus changes, in both subcortical [cit] and cortical [cit] responses."
the experiment was carried out in a sound-attenuating and electrically shielded booth. the participants were seated comfortably with a number pad on their laps and were instructed to attend selectively to either the high or low tones within each 60-s block and to press the button on their key pad every time an oddball occurred in the attended tones.
"the task proved difficult for some individuals, even with a 15-semitone separation between high-and low-f0 tokens, so a training/screening protocol was introduced to ensure that each listener could perform the task adequately prior to data collection. the first two blocks consisted of dichotic stimuli: the high-f0 (a) tones were presented to the right ear (attended in the first block), and the low-f0 (b) tones were presented to the left ear (attended in the second block). the results and strategies in the two dichotic blocks were discussed with each participant before they moved on to four blocks of diotically presented stimuli with the large f0 difference. participants completed two blocks with resolved harmonics (attend high and attend low) and two blocks with unresolved harmonics (attend high and attend low), and were provided with feedback. in order to pass a set of blocks, the participants were required to accurately respond to at least four of the six oddballs in the attended stream and to respond to two or fewer of the oddballs in the unattended stream in all four blocks. the participants were required to pass two consecutive sets of these four blocks in order to be included in the experiment. unlimited attempts at the four-block sets were allowed. the participants who failed to meet this criterion often attempted it over ten times before discontinuing the experiment and most commonly failed the attend-low condition with unresolved harmonics. those who met the criteria for the study generally did so within about four sets. they then completed a set of eight blocks that included both largeand small-f0 separation conditions with both resolved and unresolved harmonics before undertaking the three eeg test sessions."
"if we assume conditional independence between the cues relating pairs of elements, the likelihoods can be factored, so that figure 2 . (a) local geometric cues to contour grouping. smoothness properties of objects give rise to the classical gestalt cues of proximity and good continuation [cit] . for a pair of local oriented elements these can be expressed through a variable d encoding the separation of the elements and two angles (a,b) induced by a linear interpolation between the elements. (b) posterior probability, derived from ecological statistics, that a local oriented element (shown in black) should be grouped directly with a second observed element, as a function of its proximity, direction, and relative orientation [cit] . (c) local gestalt cues, together with a markov independence assumption, lead to an efficient framework for extracting extended contours. figure 1 . the problem of contour grouping. neurons in primary visual cortex (v1) code the local orientation of contours in the image, but without explicit representation of how they are linked together. our percept, on the other hand, is of coherent and complete objects, segmented from their background."
"the behavioral results show that listeners are able to detect oddballs accurately in one of the tone streams when the tone streams are presented in isolation, and when there is a large f0 difference between the attended and unattended stream. high levels of performance were observed regardless of whether the harmonics in the complex tones were spectrally resolved or not. this outcome confirms that listeners are able to segregate sequences into streams based on f0 differences, even when the harmonics are all unresolved, resulting in no tonotopic cues [cit] ."
"biologically important sounds, including speech, are rarely presented in isolation, but instead form mixtures with other sounds in the acoustic environment. the auditory system must therefore segregate the elements of a target sound from the complex background, and fuse those elements together into a perceptual stream that can be followed over time. following from early studies on cortical correlates of attention [cit], recent human studies have revealed strong attentional modulation effects on auditory streams, with responses to an attended target stream enhanced and/or the background suppressed, both for non-speech stimuli (e.g., [cit] and speech (e.g., [cit] . although the effects are clear, uncertainty remains regarding the underlying neural mechanisms that facilitate the streaming and attentional modulation of speech [cit] . some studies have suggested early influences of attention, extending down to the brainstem [cit] and even the cochlea [cit], but most studies have concluded that attentional modulation is not evident prior to auditory cortex. indeed, the earliest cortical responses, with latencies of 20-30 ms, responding to modulation rates around 40 hz, also show little evidence of attentional modulation (e.g., [cit] ) ."
"what is the minimum number of turning angles that must be changed in order to preserve the equilateral property? consider the triangle defined by a random triplet of points (generally not neighboring points) on the contour. note that moving any of these three points relative to each other must entail a length change in at least one of the triangle sides, as the side lengths uniquely determine the triangle. but now consider a tetragon defined by a quartet of points on the contour."
"construction of open metamers that match the local pairwise geometry of animal shapes was a simple matter, requiring only a shuffle of pairwise geometric properties. creating simple (nonintersecting) and closed versions of these metamers is more complicated, as the constraints of nonintersection and closure introduce nonlocal dependencies between the placement of the oriented elements comprising the stimulus."
"some earlier studies have suggested that neural correlates of streaming may emerge in subcortical structures [cit], and one study has reported modulation of the subcortical frequency following response (ffr) by perceived streaming (whether one or two streams were perceived in a repeating aba triplet sequence) [cit] . however, in that study, the effect was only observed for the second a tone of each triplet, whereas cortical correlates have tended to be observed for the b tone in each sequence [cit] . in general, reports of attentional effects on subcortical responses have been rare (e.g., [cit], and a review of the earlier literature concluded that there was little or no evidence supporting the attentional modulation of the efr [cit] . most recently, a study found attentional modulation of the efr to frequencies less than 100 hz, but not to frequencies around 200 hz [cit] ."
"1 thresholds are somewhat lower than for experiments 1 and 2. this may be due to the fact that targets in experiments 1 and 2 were slightly less eccentric on average than in experiment 3, due to an overly conservative rule used to ensure targets remained within the 208 stimulus window. also, in experiments 1 and 2 the stimulus region was defined by a midgray disk within the black background of the screen, whereas in experiment 3 the entire background was black. the absence of a clear boundary for the stimulus region may have created more spatial uncertainty for the observer."
"it is important to note, however, that the number of inflections in the contour depends not only on the signs of the turning angles but also upon their sequencing. for the animal contours the signs of the turning angles at adjacent elements are correlated, and this could be expected to produce fewer inflections on average than for metamers, for which turning angles at adjacent elements are statistically independent. figure 12a shows the mean probability of an inflection occurring at any given element in our animal and metamer stimuli. we see that indeed for longer target lengths, the metamers contain slightly more inflections than the animal stimuli, as predicted. could this explain why animals are more easily detected?"
"our results using f0s between 100 and 231 hz showed a strong response to the f0 of tones in the sequence and its harmonics. in general, the plvs in response to tones containing only unresolved harmonics were larger than the plvs in response to tones containing resolved harmonics. thus, the change in plv amplitude from resolved to unresolved harmonics is opposite to the change in perceptual pitch strength or accuracy found in many previous studies [cit] . the dissociation between plv and pitch strength is consistent with the idea that the efr reflects stimulus periodicity but not the perception of pitch [cit] . despite the robust highfrequency efrs in our study, no effects of attention were observed in any condition. thus, our results provide no evidence for the emergence of auditory stream-based attentional effects in subcortical structures."
"the goal of this experiment is to compare detection for partial fragmented animal shapes as a function of their closure, compared with metamer shapes matched in target length (number of elements). because the results of experiment 1 suggest that the convexity bias that distinguishes our two types of metamer stimuli has very little impact on detectability, we elected to employ only the m 1 metamer stimulus for this comparison."
"a yes/no design was employed. each trial sequence consisted of a fixation display (0.5 s), stimulus display (160 ms in experiments 1 and 2, 200 ms in experiment 3), and then the fixation display again until response ( figure 6 ). a target shape appeared within the stimulus window with 0.5 probability. the observer pressed the left arrow key on a keyboard to indicate target absent, and the right arrow key to indicate target present. feedback was provided in the form of a tone for incorrect responses. the number of distractor elements at criterion (75% correct) performance was estimated using an adaptive psychometric procedure (quest; [cit] ) . each experiment was broken into three 100-trial blocks per condition; blocks were counterbalanced across conditions. quest was reinitialized for each block, yielding three independent threshold estimates for each condition."
"the envelope phase-locking value (plv; [cit] was computed for all electrodes, frequencies, and conditions. the plv is the magnitude of the unit vector in each frequency bin after averaging across presentations. if the eeg waveform were completely coherent with the stimulus, and hence completely repeatable, the plv would be 1 for each frequency bin. for random eeg responses, the angle or direction of the unit vector at each frequency would be random on each presentation, and so the magnitude of the vector after averaging tends to 0. for a given condition, epoch lists from the three sessions were first concatenated. a bootstrapping procedure was used to estimate plvs based on 400 trials of 75 random draws from the positive and negative event lists. for each draw, a first-order slepian window was applied before computing the fourier transform. an average of the unit phase vectors was calculated across all draws. the plv was averaged across all 32 channels, and peak values were extracted at key frequencies. the slow (3.3 hz), fast (6.7 hz), and combined (13.3 hz) presentation rates of the tone sequences were analyzed in the low-frequency band, and the tone f0s (100, 106, and 238 hz) were analyzed in the highfrequency band. a schematic showing the procedure by which the plvs were extracted, along with a heatmap of the distribution of the plv peak values at 3.3 hz, is shown in fig. 2. fig. 2. (color online) schematic diagram of the neural envelope-following responses to the low-and highfrequency components within the eeg signal (red and black lines, respectively). the heat map shows the distribution across the scalp of the amplitude of the peak efr component at the low efr frequency of 3.3 hz."
"in these experiments, the shape targets subtended roughly 2.58 visual arc and were randomly oriented and positioned within a 208 diameter circular stimulus window. the task was made challenging by embedding the targets within a field of randomly positioned and oriented distractor segments. the lengths of the distractor segments were drawn randomly and uniformly from the lengths of the visible target segments when the target was present, and from a randomly selected target when the target was absent. antialiased segments were drawn white on a midgray background. figure 6 shows a sample target-present stimulus display."
"these control analyses appear to rule out explanations for the superior detectability of animal shapes over metamers based upon size, length, eccentricity, self-intersection, and number of inflections. however, one important difference between animal and metamer stimuli that we have not analyzed is closure. closure has been suggested as potentially important both as a cue for contour grouping [cit] and as a bridge from one-dimensional to two-dimensional shape [cit], and thus could plausibly contribute to the detectability of our animal stimuli. our next two experiments will explicitly assess the potential role of closure in the perceptual organization of fragmented shapes."
"what are the brain mechanisms underlying these gestalt principles? physiological experiments first in cat [cit] and later in nonhuman primate [cit] have revealed a striking bias in the long-range horizontal connections between orientation columns in primary visual cortex that serves to connect neurons coding local orientation signals that can be smoothly interpolated, consistent with the classical gestalt principles of proximity and good continuation. psychophysical experiments suggest that these connections serve to facilitate geometrically consistent orientation responses in the human striate cortex [cit] ."
"in summary, prior work suggests that, despite the clear role of closure in the formation of 2d shape percepts [cit], 1994 [cit] ), its role as a global cue for detection of contours in clutter may be minimal, with evidence to the contrary attributed to local geometric covariates, in particular the magnitude and sign of turning angles."
"each of the gap segments in a polygon forms two angles with respect to the adjacent visible segments. let the gap length and two angles identified with gap segment i be denoted d i, a i, and b i, respectively ( figure 4 ). note that the visible and gap segments varied slightly in length."
"a second possibility is that higher visual areas receive grouping hypotheses from early visual areas and then provide feedback to these early visual areas to support hypotheses that are consistent with global evidence while suppressing hypotheses that are inconsistent with global evidence. this kind of model extends the function of v1 beyond a transient initial stage of local processing to include a more sustained role in representing high-resolution geometric detail [cit] . anatomically, substantial feedback connections from higher to lower visual areas in ventral stream are known to exist [cit], and the feedback of global shape information to inform local grouping in early visual cortex could be one of their functions. under this second model, one would expect to find a trace of this global influence in the later phase of response of the early visual neurons coding local contour geometry."
"our experiments assessed human ability to detect these animal shape targets in noise, compared with various forms of local metamer targets that have the same first-order statistical geometric properties between neighboring elements but lack the global regularities of natural shapes ( figure 5 ). we detail the method for constructing these metamer stimuli within the appropriate experiment-specific methodology sections below."
"the study of these perceptual organization principles has a long history stretching back to the early days of gestalt psychology [cit] . two key gestalt principles relevant to contour grouping are proximity and good continuation. we will refer to these grouping cues as local cues because they can be defined for pairs of neighboring contour elements. in particular, proximity can be encoded in terms of the distance d ij between elements i and j, and good continuation can be coded in terms of the angles a ij, b ij induced by a linear interpolation between the elements (figure 2a )."
"the stimuli were filtered into either a low spectral region (300-1800 hz) so that the complexes contained some spectrally resolved harmonics, or a high spectral region (3000-4500 hz) so that no harmonics were spectrally resolved (the lowest harmonic number within the pass band was always greater than 12); see fig. 1 ."
"real-world scenes can be highly cluttered, often causing an object to fragment into many pieces before it hits our retinae. perceptual grouping is the process of reassociating these fragments to generate the percept of a whole object. here we consider in particular the problem of grouping the fragments of the bounding contour based on their geometric relationships."
"the tones were presented at an overall root-mean-square (rms) level of 70 db sound pressure level (spl) in either positive or negative polarity, and were embedded in spectrally notched threshold-equalizing noise (ten) at 50 db spl per erb, to reduce potential off-frequency listening and audible distortion products [cit] . the ten was generated from 50 hz to 6 khz with a spectral notch between 250 hz and 2 khz for the resolved-harmonics conditions and a spectral notch between 2.7 and 5.25 khz for the unresolved-harmonics conditions. the noise was generated as a 1-s circular token, repeated to create a 70-s source sound. for each block, 61 s of noise was randomly sampled and added to the triplet stimuli with a 1-s onset lead. level oddballs in both the a-and b-tone sequences were used to monitor stream segregation and attention. each sequence (a and b tones) contained six oddball tokens per block that were presented at a level 6 db higher than the regular tokens. oddballs were prevented from occurring in the first or last triplets of the sequence, and were restricted from occurring within the same triplet in both the a-and b-tone sequences and from being separated by fewer than two regular tokens within a given sequence."
"to determine an approximately equilateral polygonal model, we searched the exponential space of possible partitions to find the partition minimizing the variance in side length, using an expansion moves algorithm [cit] ). the resulting polygons were approximately equilateral, with a mean standard deviation in length of 2.0%."
"the eeg signal in the 70-1000-hz band sometimes contained an unexpected artifact at harmonics of 50 hz, with increasing magnitude with harmonic number, even in the absence of 100 hz stimuli. given the significance of 100 hz as an experimental frequency, a denoising source separation (dss) routine was applied to reduce the role of any artifact in the analysis. using the noisetools matlab toolbox, the epoched data (time â channel â trial) were orthogonalized through principal component analysis (pca), the components were normalized, and the data were rotated and projected on the orthogonalized reference axes to remove the first two components (time-shift pca) (s€ arel€ [cit], 2008) . the resultant data no longer exhibited visible artifacts at multiples of 50 hz."
"there were 10 observers in our first experiment. each performed 3 blocks of 100 trials each for 3 target lengths (5, 10, 20) and 3 target conditions (animal, m 1, m 2 ), for a total of 2,700 trials per observer. one of the observers was an author; the others were naïve to the goals of the experiment. all observers had normal or corrected-to-normal vision. figure 8 shows the results. mean noise thresholds over observers for the three conditions are shown on the left, and pairwise contrasts between conditions are shown on the right. we see that results for the two metamer conditions are nearly identical: the convexity bias induced by retaining the signs of the turning angles does not seem to impact target detection. however, there is a clear difference between these metamer conditions and the animal shape condition: thresholds are significantly higher for the animal condition than for the metamer conditions at higher target lengths (table 1) . since the information present in the pairwise relationships between successive elements is identical for animal and metamer stimuli, this result indicates that observers are making use of information in the animal targets beyond these local cues."
"as mentioned in the introduction, there have been numerous reports of attentional modulation of auditory cortical responses in the past, using eeg (e.g., [cit] ), meg [cit], functional magnetic resonance imaging (fmri) [cit], and electrocorticography (ecog) [cit] . neural correlates of streaming for pure tones have been reported previously in humans [cit] and other species [cit] . neural correlates of streaming have also been reported based on higher-level features, such as pitch in humans and spatial separation in both humans [cit] and other species [cit] . however, none of these studies on higher-level features explicitly examined the role or correlates of attention in their tasks. thus, to our knowledge, this is the first study to examine the correlations of attention to specific sound dimensions in a streaming paradigm. our results suggest that the tonotopic, but not the periodicity, dimension produces measurable attention-based modulation of the cortical efrs. far right panels show the results with only one stream present. as shown in fig. 6, the effect of attention was significant in the two-stream conditions, but only for the condition with the large f0 separation, and then only in the condition with resolved harmonics (***p 0.001, **p 0.01)."
"with three experiments we have tested the hypothesis that nonlocal cues play a role in the perceptual grouping of contours. rather than narrowing focus on a specific cue, we employed natural animal shapes that afford many nonlocal cues, on the assumption that the brain is adapted to take advantage of cues available in ecologically important stimuli. critical to our method is the design of control stimuli that match the local cues afforded by our animal stimuli, but none of the global cues."
"orientation and turning angles (local curvature) are equated for our animal and metamer stimuli, suggesting that the greatest differentiation in neural response to our target shapes would be seen in the later phase of pit response, when selectivity for multifragment configurations emerges. however, it is unclear whether the simple local facilitatory circuit proposed here would be sufficient to resolve local grouping ambiguities in cluttered displays like the ones we use here."
"these primate and human studies have all focused on the perceptual organization of shape. however, recent human fmri evidence suggests that feedback modulation may also extend to more semantic object category information [cit] . this is consistent with psychophysical studies that show an influence of object category on figure/ground organization [cit] ."
"there were 13 observers. one of the observers was an author; the others were naïve to the goals of the experiment. all observers had normal or corrected-tonormal vision. figure 15 shows the results of this experiment. we see that as the target length is increased to 20 elements and beyond, a dissociation between the three conditions emerges: animal target are significantly more easily detected than either form of metamer, but at the same time the closed metamers are significantly more easily detected than open metamers (table 3) . these results indicate that closure is certainly one nonlocal cue that aids detection, but it is not the only nonlocal cue: there must be additional nonlocal regularities of the animal targets that play a role in the perceptual organization of shape."
"in summary, recent anatomical, physiological and behavioral evidence suggests that cortical feedback of global shape information from higher visual areas to lower visual areas may play an important role in the perceptual grouping of contours, and our psychophysical results are consistent with this hypothesis. that said, we suspect that perceptual object formation is the outcome of a cooperative computation involving diverse feedforward, lateral, and feedback connections between a number of early and late visual areas. a computation that is progressive, generating quick approximate solutions that are refined over time, would have adaptive advantage in supporting rapid real-time vision, and such mechanisms could be probed through future behavioral and physiological experiments that jointly manipulate local and global grouping cues as well as stimulus dynamics."
"the stimuli were 1 [cit], showing that the tones in the low spectral condition produced individual peaks in the excitation pattern and so can be considered spectrally resolved, whereas the tones in the high spectral condition did not produce clear individual spectral peaks and so can be considered spectrally unresolved. (b) schematic diagram of the tone sequence, as presented. the higher-f0 a tones (red) are interleaved with the b tones (black) to form a pattern of repeating triplets. certain tones, selected at random, are increased in level by 6 db to produce oddballs. (c) schematic diagram of the tone sequence, as perceived when the a and b tones form separate perceptual streams. the task of the participants was to attend to just one of the streams and report oddballs in only that stream. sequence, where the a tone was higher in f0 than the b tone. each tone was 50 ms long, including 10-ms raisedcosine onset and offset ramps. tones within a triplet were separated by 25-ms gaps, and each triplet was separated by a gap of 100 ms (as if every second b tone was silenced within an alternating abababa sequence). the repetition rate of the higher a tones was therefore 6.67 hz, and the repetition rate of the lower b tones (and the entire triplet pattern) was 3.33 hz. each harmonic complex tone was generated in sine phase and bandpass filtered into a fixed spectral region. the f0 of the b tone was always 100 hz and the f0 of the a tone was either 1 semitone higher (106 hz) or 15 semitones higher (238 hz). with a 1-semitone f0 difference, termed the small-separation condition, the two tones were likely to form a single perceptual stream, eliciting the percept of a galloping rhythm; with a 15-semitone f0 difference, termed the large-separation condition, the two tones were most likely to form two separate perceptual streams of isochronous tones [cit] . control conditions were also tested, in which only the a tones (at 238 hz), or only the b tones (at 100 hz) were presented; these were termed the single-stream conditions."
"the stimuli were based upon 264 blue-screened images of animal objects taken from the hemera photo-objects dataset. we tried to select shapes that were recognizable as animals, as opposed to more generic shapes (e.g., snakes). the original images had maximum dimension (either height or width) of 1,024 pixels; the other dimension varied depending upon the shape of the animal. the alpha channel for each image signaled figure/ ground for the blue-screened animal object. contours were extracted by tracing points with nonzero gradient magnitude in the alpha channel using moore neighborhood tracing [cit], and each contour was then represented as a high resolution polygon with this sequence of points as vertices. the number of vertices n ranged from 1,506-4,668 over shapes. see figure 3 for examples."
"what are the brain mechanisms underlying the integration of local and global cues for contour grouping? neurons in early visual cortex have highly localized, oriented receptive fields well suited to coding the local edges of object boundaries [cit] . the standard view is that perceptual grouping of these edges into contours relies upon longrange horizontal connections between orientation columns in primary visual cortex [cit] ."
"ten observers participated in experiment 2. five of these observers had already participated in experiment 1 (including one author); the other five were new. all had normal or corrected-to-normal vision, and all but the author were naïve with respect to the goals of the experiment. figure 13 shows the results. noise thresholds are consistently higher for the partial animal targets relative to the partial metamer shapes, and this detectability advantage becomes statistically significant for higher target lengths (table 2 ). these results suggest that the detectability advantage for animal targets is not dependent upon perfect closure: seventy-five percent closure is sufficient to show a statistically significant advantage. this is consistent with prior work suggesting that perceptual closure, unlike mathematical closure, is a continuous property of contours, rather than a binary, all-or-nothing feature [cit], 1994 . this experiment therefore leaves us with uncertainty about why the animal targets are easier to detect: is it perceptual closure, or some other nonlocal property of these animal shapes? our final experiment will address this question."
"this kind of computational progression is consistent with some studies that examine the dynamics of neural tuning in intermediate and higher areas of the object pathway. [cit] studied the dynamics of contour shape selectivity in posterior inferotemporal cortex (pit). they found that early pit responses were selective for individual straight and curved contour fragments, but that tuning evolved toward nonlinear selectivity to multifragment configurations of contours over a time course of about 60 ms. [cit] extended this analysis to neurons in area v4. analogously to pit, they found that v4 neural response evolved over a time course of about 50 ms from selectivity to contour orientation to selectivity for curvature. they posited an intracortical facilitatory circuit between similarly tuned v4 neurons that would generate a sustained selectivity for curvature."
"despite strong responses to the stimuli, no attentional modulation of high-frequency efrs was observed, consistent with no modulation of phase-locked brainstem or midbrain responses. low-frequency efr provided correlates of attention to streams that are likely to be cortical in nature, but only in conditions where some tonotopic differences existed between the alternating tones in the sequences. our findings should not be interpreted as evidence against a neural representation of non-tonotopic auditory streaming; instead, it may be that the population-based eeg recordings are not sensitive to the potentially smaller neural populations that respond selectively to higher-level features, such as pitch. as an example, a search for pitch-sensitive neurons in the auditory cortex of marmosets found only a relatively small number of such units in a relatively constrained region of auditory cortex [cit], whereas large areas of primary auditory cortex are known to reflect tonotopic organization in response to both simple [cit] and complex [cit] sounds. taken together with earlier findings, our results suggest that eeg measures can provide correlates of auditory streaming and attention, but that such population-based correlates may be sensitive primarily to low-level tonotopic differences, and not to higher-level features, such as pitch, that extend beyond simple tonotopy."
"we also assessed whether there were systematic differences in the eccentricity of the stimuli. figure 11a shows the mean eccentricity of the target shape centroids as a function of target length. note that on average, the animal stimuli tend to be more eccentric than the metamer stimuli. this is because targets were positioned randomly to lie entirely within the central 208 of the display. since the animal stimuli tended to be more compact, they tended to assume more eccentric locations. since more eccentric presentation would be expected to reduce performance, these differences are not likely to account for the superior detectability of animal targets."
"each of these simpler stimuli was formed by partitioning the sequence of high-resolution vertices representing a shape into a sequence of 120 disjoint, contiguous subsequences, and then representing the shape by vertices located at the 120 arc-length mean locations for each of these subsequences."
"finite impulse response (fir) filters with linear phase response using 800 taps were applied to the eeg recordings to separate low-frequency (0.5-30 hz) from higher-frequency (70-1000 hz) responses. data from the low-frequency band were first epoched into 1-min events corresponding to presentation blocks. the 1-min eeg blocks were re-referenced to averaged mastoid electrodes and baselined to the 25-ms period prior to stimulus onset. the re-referenced and baselined 1-min blocks were further segmented into eight-triplet events with 50% (four triplets) overlap, resulting in 96 epochs of 2.4 s, each baselined to the preceding triplet (300 ms). combining data from three sessions resulted in 288 events in positive polarity and 288 in negative polarity for each experimental condition."
"there is growing evidence for this feedback hypothesis. perceptual learning and attention have been found to dynamically influence contour grouping mechanisms in early visual cortex [cit], and v1 neurons can adapt dynamically to different nonlocal shape properties depending on the immediate task [cit] . both of these effects are thought to be the result of feedback from higher shape-selective visual areas. in more recent physiological experiments involving simultaneous recording from neurons in macaque areas v1 and v4, selectivity for global contour information is first seen in v4, emerging in v1 roughly 40 ms later [cit] . these results point to a v1-v4 recurrent network underlying the global perceptual integration of contours."
"in this case, there is a one-dimensional family of tetragons with exactly the same sequence of side lengths. let us express the original contour as a concatenation of four fragments, each completed by one of the four tetragon segments ( figure a1 ). imagine continuously rotating through this one-dimensional family, keeping each contour fragment rigidly attached to its associated tetragon segment, but allowing it to rotate relative to the adjacent fragments. since the sides of the tetragon remain fixed in length, all of the sides of the associated contour fragments will also remain fixed in length; only four of the turning angles will change, and these four changes must sum to 0. in this way, an isoperimetric shape change preserving the equilateral nature of the contour has been induced by a change to just four of the turning angles. note that perceptually the change in shape can be dramatic if the sampled quartet of points is widely spaced."
"a different pattern of results was observed in the lowerfrequency eeg band, phase-locked to the stimulus repetition rates, which likely reflects cortical activity [cit] . in contrast to the high-frequency efr, a robust effect of attention was observed here, but only in the case of the large f0 separation with the spectrally resolved harmonics. the lack of an attentional effect with the small f0 separations is consistent with the behavioral results: the f0 difference was too small to induce sequential streaming between the two alternating tones, resulting in near-chance performance in the behavioral task, which in turn indicated an inability to attend selectively to one or other sequence. however, the lack of an attentional effect on the cortical plv with the unresolved harmonics and large f0 separation represents a dissociation between the cortical and the behavioral results. it therefore appears that the eeg correlates of streaming and attention in our paradigm are limited to conditions where there are tonotopic differences between the aand b-tone sequences."
"during the sessions with eeg, simultaneous low-and high-frequency efrs were recorded to the 1-min-long presentations of the stimuli. the low-frequency efrs corresponded to the presentation rates of the a and b tones (6.67 and 3.33 hz, respectively) and the individual tones (13.3 hz, including the \"missing\" b tone after each triplet), reflecting cortical activity. the high-frequency efrs corresponded to the f0s of the tones (between 100 and 238 hz) and are thought to reflect primarily subcortical activity [cit] . the stimuli were generated using matlab (the mathworks, natick, ma) and were played to participants via a tucker davis technologies (alachua, fl) real-time processor with headphone buffer and er1 insert earphones (etymotic research, elk grove village, il). the eeg measurements were acquired using a biosemi (amsterdam, netherlands) active electrode system with a sampling rate of 4096 hz and 32 channels, referenced to averaged mastoid electrodes. two blocks of each of the 24 conditions (resolved or unresolved harmonics; small or large f0 difference or single sequence; attend high or low; positive or negative stimulus polarity) were tested in each of the three sessions, resulting in 48 blocks per session, presented in random order. the participants were allowed to self-initiate each block, allowing for rest as needed between blocks. all participants completed each eeg session in 90 min or less."
"the creation of closed metamer stimuli depends upon a fair sampling from the space s n of all simple (nonintersecting) polygons of length n. [cit] . in particular, we devised an isoperimetric procedure for incrementally changing the turning angles of a shape while not altering any of the segment lengths, so that the polygon remains equilateral. further, the resulting shape must not selfintersect. note that almost all angle updates will not satisfy these requirements. for example, changing only one turning angle will generally require the length of at least one segment to change."
"one possible account is that early visual cortex passes multiple, competing local grouping hypotheses forward to intermediate and higher visual areas, which through a combination of feedforward computation and lateral interactions bring to bear more global factors to select a unique solution that is globally consistent with the visual input. under this model, no trace of this global computation would necessarily be evident in early visual cortex."
"our discussion so far has focused on the so-called object pathway in ventral stream, but posterior areas of parietal cortex are known to be involved in the processing of object configural information [cit], and could also be involved in the detection of contours in clutter. [cit] have assessed to what degree the presence of a contour in a cluttered display can be decoded from visual areas in the object pathway but also from areas in parietal cortex. they employed a standard static form of the field, hayes, and hess stimulus, but also an interesting dynamic form, in which the stimulus is revealed over time through a narrow slit. for smaller slit widths, the elements constituting the collinear contour appeared sequentially, moving either up or down the slit, depending on the orientation of the contour and the direction of motion (left or right). under these conditions, not only do the elements not appear simultaneously, their relative geometry when they do appear is not colinear. as a consequence the stimulus is unlikely to strongly activate long-range horizontal connections in v1, which are thought to be primarily selective for simultaneously presented, colinear stimuli."
"has it not already been tested? one would think so. particularly in the last 25 years there have been a number of excellent quantitative psychophysical studies on the perceptual determinants of contour grouping. [cit], in which a sequence of oriented elements following a curvilinear path is embedded in uniformly distributed and randomly oriented distractor elements. the observer's task is to detect the path, and properties such as the spacing, orientation jitter, and photometric appearance of the elements can be varied to identify their influence on the underlying grouping mechanisms (e.g., [cit] . however, in these studies the target contours are typically constructed to afford only local cues to grouping-the placement and orientation of each element on the path depends only upon the previous element. it is thus impossible to infer anything about global factors in the perceptual grouping of contours from these experiments."
"what is the nature of the nonlocal differences between animal and metamer stimuli responsible for the observed differences in detectability? there are many possibilities; we first seek to determine whether simple factors such as target size, elongation, eccentricity, self-intersections, or number of inflections could explain our results."
"as for open metamers, closed metamers have slightly more inflections than animal shapes, but the number of inflections did not seem to influence thresholds. this figure 14 . experiment 3. the probability density of turning angles for our closed metamer shapes is a close match to the turning angle density for the original animal shapes. was verified by a four-way anova analysis of the number of inflections in the target for target-present trials with observer, target shape, target length, and response (hit or miss) as factors (table 4) . whereas both target shape and target length are significant predictors of the number of inflections, observer and response are not."
"the temporal dynamics in v4 and pit are consistent with a feed-forward progression of progressively more complex shape tuning. v4 neural selectivity for orientation peaks at roughly 100 ms poststimulus, and selectivity for curvature peaks at roughly 150 ms. in pit, neural selectivity for individual straight and curved contour fragments peaks at roughly 150 ms poststimulus, while selectivity for multifragment configurations peaks at roughly 200 ms."
"token-specific phase locking to the f0 was also observed in conditions where the stimuli were identical (both high and low tokens present) and differentiated only by which stream the participants were instructed to attend to. in these cases, peak magnitudes were similar between conditions. peaks were extracted from these spectra at points corresponding to the high f0 (238 hz in large-separation and single-stream conditions, 106 hz in small-separation conditions) and low f0 (100 hz) in each condition; the values of these peaks are shown in fig. 5, with the individual values as lines and the mean values as symbols."
"our novel metamer stimuli provide an opportunity to more carefully test for a role of closure in contour detection while controlling carefully for local geometry. in experiment 2, we compared the detectability of fragments of our animal stimuli ranging from 50% closure to 100% closure against length-matched local metamer controls. the results show that the superior detectability of the animal shapes does not depend upon perfect closure: seventy-five percent closure was sufficient to generate a statistically significant advantage. these results are consistent with prior work [cit] showing that perceptual closure is a graded phenomenon, increasing smoothly in influence as the degree of closure increases. however, they are not consistent with a more simplistic explanation of the effects of closure in terms of proximity and good continuation cues [cit], as these are the same for the partial animal shapes and the metamer stimuli."
"for our first experiment, we used a simple procedure to construct metamer stimuli that have exactly the same first-order (local) geometric properties as our animal shapes, but none of the global regularities. the process is illustrated in figure 7a . to construct a metamer, a fragmented animal shape was first selected, and lists of the lengths l i of the visible segments and of the gap geometry (d i, a i, b i ) were extracted, starting from a randomly selected visible segment on the animal shape. these lists were then randomly and independently permuted. finally, the fragmented metamer shape was constructed by sequentially rendering the sequence of visible segment lengths and gap geometries from the permuted lists. since the individual gap geometries were copied faithfully, this transcription preserves the local pairwise grouping relationships between visible elements. however, since the order of these gap geometries is randomized, higher order nonlocal structure has been shuffled away. figure 7b shows the result for an example animal shape."
"while the shape of these contour fragments can be arbitrary, approximating them as piecewise linear allows the problem to be modeled as the grouping of local orientation signals, coded by neurons in primary visual cortex [cit], to form a sequence or chain that describes the entire contour ( figure 1 ). note the combinatorial nature of the problem: given n oriented elements in the image, there are n!/(2(n -k)!) different sequences of length k to consider. this is far too many to evaluate for most contours in complex natural scenes-the brain must employ some prior knowledge or principles to limit the search."
"they found that the closed contours were more easily detected than the open contours, replicating the kovacs & julesz result. in order to isolate the underlying cause, they [cit] ) also compared the detectability of a circular closed contour with constant turning angles and no inflections to the detectability of a noncircular closed contour with no inflections but nonconstant turning angles (a ''bean''), and to a noncircular closed contour with two inflections and nonconstant turning angles (a ''moon''). they found that variation in turning angle (curvature) and the presence of inflections reduced detection performance. in a critical control, they showed that when closed and open contours were better matched for the presence of inflections, the advantage of closure disappeared. [cit] have also presented evidence that small enhancements in detectability found for closed contours may derive from probability summation over local configurations of elements rather than a global closure mechanism."
"d-stem is developed in the matlab (the mathworks, inc. 2010) language and it is available at https://code.google.com/p/d-stem/. the modeling capabilities of d-stem are detailed in this paper by introducing three case studies of increasing complexity. the reader can download the d-stem_v4.7.11_full.zip archive -either from the journal web page or from the link above -including source code and demo folders with replication materials. please follow the instructions given in the readme.txt file of the demo folder to reproduce the case studies. d-stem requires the statistics toolbox, the optimization toolbox and the mapping toolbox (the mathworks, inc. 2010) ."
"choosing option number five from the dstem_demo.m script, the case study of section 6 is reproduced and model estimation is carried out in distributed manner. to avoid the complication of setting up a distributed environment, the dstem_demo.m script starts a second matlab process in which the demo_runslave script is executed and that, in turn, executes the daemon.m script. the original matlab process executes the demo_section7_2.m script which differs from the demo_section6.m script with respect to the following lines of code. the timeout input argument is the time in seconds that the master waits when listening for the slave nodes. it is worth knowing that the content of nfs shared folders on unix distributed environments is not always updated in real time. if the user cannot change the updating time of nfs folders, then the timeout input argument must be increased in order to ensure that master and slaves can always read the files written in the shared folder."
"this paper introduces the d-stem (distributed space time expectation maximization) software as a statistical tool for the analysis of environmental space-time data sets and the prediction, uncertainty included, of the observed variables."
"along with the information about the type of the spatial correlation function (exponential in this case), the obj_stem_data object is needed to create the obj_stem_par object of class 'stem_par'."
"the above remote sensing data can be considered a special case of block or pixel data which are denoted here by y b (b, t), where b ⊂ d is the generic grid pixel. with this notation, the model of equation 6 is extended by introducing a new equation for remote sensing data and a downscaling link term into the ground data equation as follows:"
"the variance-covariance matrix of the estimated model parameters and the observed data loglikelihood are evaluated after model estimation using the methods set_varcov and set_logl of the class stem_model. all the relevant information about model estimation can be found in the internal object stem_em_result which can be accessed as a property of the obj_stem_model object. after model estimation, the obj_stem_model object is saved in the subfolder output of the demo folder."
"regardint the software side, some time consuming procedures such as the estimation of the variance-covariance matrix of the model parameters and kriging could also be implemented in a distributed manner. furthermore, the handling of the model parameters could be improved by introducing constraints on the parameter vectors and matrices, widening the range of models that can be estimated."
"the rest of the paper is organized as follows. section 2 describes the capabilities of the software in terms of data modeling and data handling in general terms. section 3 introduces the software classes at the basis of d-stem. sections 4, 5 and 6 illustrate software usage and capabilities considering the three case studies implemented in the above mentioned demo, which are based, respectively, on univariate, multivariate and data fusion models. section 7 describes three options for handling large data sets and in particular tapering, distributed computing and software configuration to reduce the computational burden. conclusions are given in section 8."
"the second aspect is related to the dimension of the grid and memory usage. if the grid is large and/or very dense, the number of pixels can be high and the loading coefficients may require a lot of memory when loaded. in order to avoid memory problems, the loading coefficients (related to the non-masked pixels) can be saved on disk within different blocks. kriging is then executed block by block without the need of loading the entire data set of loading coefficients. on the other hand, when pixels are low in number, the user can implement kriging providing all the coefficients at once. all the details about the two approaches are found within the help of the class 'stem_krig'. in this paper, the first approach is considered as more complex in terms of data structure."
"the latent spatial random variables are modeled as gaussian random fields with a matérn correlation function and, in the case of multiple variables, the spatial cross-correlation is modeled through the linear coregionalization model (lcm). on the other hand, time is assumed to be discrete and it is modeled through latent temporal random variables with markovian dynamics."
"the parametric statistical model implemented in d-stem is based on latent space-time random variables and space-time varying coefficients. the varying coefficients can be either observed covariates or the loadings derived from some basis functions. the model, thus, can reach a high level of flexibility and it is suitable for modeling variables over large geographic regions."
"in order to improve the numerical stability of the model estimation algorithm, observations and loading coefficients are standardized using the standardize method of the class 'stem_data' as follows."
"depending on the coordinate system of the data, two options are available, namely d ⊂ 2 or d ⊂ s 2, where s 2 is the sphere in 3 ."
3. software structure d-stem is based on the object oriented paradigm. data handling and analysis are thus performed by creating objects from the d-stem classes and by calling the appropriate methods. the following list describes the classes that the end user should manage.
"the kriging result is saved in the obj_stem_krig_result object and the plot method can be used to display the result on a map. for example, the estimated no 2 concentration and the respective standard deviation for april 10, 2009 are depicted in figure 2 . note that, since w(s, t) and x land (s) are interacted, the spatial pattern of the standard deviation does not reflect the monitoring network, that is, the standard deviation is not necessarily lower near the monitoring stations."
"it is assumed that observations and covariates are stored in matlab format files. in general, the user has to take care of loading the data from external sources and formatting them as requested by the class constructors."
details on the mathematical structure of the model at the basis of d-stem are given in the following sections while model estimation formulas and their derivation can be found in fassò [cit] and references therein.
"the constructor of the class 'stem_grid' requires to specify some information about the grid and in particular the unit of measure (degrees, kilometers or meters), the configuration of the spatial locations (sparse or regular) and the grid type (points or pixels)."
"with transition matrix g assumed to have eigenvalues smaller than 1 in absolute value and innovations η(t) ∼ n p (0, σ η ). eventually the variables w j (s, t) are zero-mean and unitvariance independent gaussian processes uncorrelated over time but correlated over space with matérn spatial covariance function"
"pixel variables are considered as secondary information useful to improve the mapping of the point variables. the resulting maps are displayed in figure 4 . the observed pixel data are characterized by large areas of missing data but the latent variable w b (b, t) allows to reconstruct the missing data and to filter the observed data corrupted by noise. moreover, since w b (b, t) is used to model both point data and pixel data, the reconstruction of the missing pixel data benefits from the observed point data."
"if the observations have been log-transformed and/or standardized, the back_transform argument allows to produce the kriging output in the original unit of measure. if it is not necessary to estimate the variance of the prediction, the no_varcov argument can be set to 1 saving computing time. finally, note that the directory where the blocks are stored is provided as well as the size (number of grid pixels) of each block."
"d-stem is the evolution of the r [cit] package stem [cit] ) which provides space-time data modeling capabilities by means of hierarchical space-time models within the frequentist paradigm. excluding the many packages for spatial data, only few r packages can handle space-time data and even fewer are suitable for multivariate spacetime data. the r package sptimer [cit] b,a) implements space-time models similar to those implemented by stem but model estimation is performed within the bayesian setting. compared to the packages stem and sptimer, d-stem allows to estimate a larger class of univariate and multivariate hierarchical space-time models and it is optimized for large data sets. the gstat package [cit] can deal with multivariate space-time data but data interpolation is based on variogram modeling. when modeling environmental space-time variables, d-stem is an alternative to the r package inla [cit] which implements the integrated nested laplace approximation (inla) and the stochastic partial differential equation (spde) modeling approaches. although d-stem and the inla package are based on hierarchical models and latent variables, the space-time models they implement overlap only partially and the user may benefit from using them both depending on the specific application [cit] ."
"the following lines of code describe how the obj_stem_em_options object is created in order to avoid the evaluation of the log-likelihood at each iteration. the output of the model estimation is similar to the output reported in section 4 and only the relevant part is reported hereafter. ******************************** * model estimation results * ******************************** * tapering is not enabled due to the different exit condition, the model parameters estimated without computing the log-likelihood at each iteration differ from those estimated in section 4. in particular, the θ parameter decreased from 31.42 to 22.45 km. nonetheless, the observed data log-likelihood, evaluated after model estimation, is only slightly higher (−13797.357 vs. −13889.064) . this is due to the fact that, using a non-large data set as in this case study, the θ parameter of equation 5 is poorly identifiable and it monotonically changes from one iteration to the next even if the observed data log-likelihood does not change significantly."
"in this paper, the use of d-stem has been illustrated for three different case studies involving a univariate model, a bivariate model and a data fusion model. the model at the basis of d-stem is general enough to accommodate many environmental data sets, nonetheless, both model and software can be extended with respect to many aspects. from the modeling point of view, additional spatial correlation functions could be introduced as well as more flexible \"coregionalization models\" [cit] . moreover, markov random fields could be introduced in order to model pixel data. indeed, gaussian random fields easily handle data sets with extensive missing data but they are more computationally expensive even under tapering. finally, it could be useful to extend the model to accommodate for time-varying grids and irregularly spaced sampling times."
"d-stem has been tested by the authors in various real-data applications. at the urban scale, it has been used for assessing the space-time impact of traffic policies in milan city (fassò 2013) . at the country scale, it has been used for evaluating multi-variable air quality indexes and for assessing the airborne pollutant exposure distribution in scotland (finazzi, scott, and fassò 2013) . at the continental scale, considering a large data set of both ground level and remote sensing data, it has been used for air quality dynamic mapping over europe ."
"the result of model estimation consists of the values of the estimated parameters, their variance-covariance matrix and the observed data log-likelihood. moreover, cross-validation mean squared error can be obtained for each variable following a 2-fold cross-validation approach. the estimated model is eventually used to dynamically map each variable at high spatial resolution over the geographic region."
******************************** * model estimation results * ******************************** * tapering is not enabled figure 3 shows the components of the estimated z(t) related to both the variables. the graphs are obtained by calling the method plot of the class 'stem_kalmansmoother_result'. daily concentration maps of both variables can be obtained as in the previous section.
"the width φ is a property of the grids as all the variance-covariance matrices are directly derived from the distance matrices which, in order to reduce memory usage, are also created as sparse matrices. also note that φ can be different for point and pixel data."
"the output of model estimation is similar to the output reported in section 6 and only the relevant part is reported hereafter. ******************************** * model estimation results * ******************************** * tapering is enabled. looking at the estimation result, it can be noted that the estimated θ parameters, as well as their standard deviations, are higher compared to those reported in section 6. moreover, the observed data log-likelihood is lower. the tapering approach, thus, reduces computing time but may produce biased estimates of the θ parameters and/or estimates with a larger uncertainty. finally, note that tapering is intended for large data sets. the case studies discussed in this paper are based on medium-size data sets so that the actual computing time may be higher when tapering is enabled. the same consideration applies to the two following strategies."
"as in section 4.2, the suffix \"_p\" refers to ground data because it is necessary to differentiate then from the remote data of section 6. now, the obj_stem_par object is created in the following way. model estimation is thus obtained as in the previous section and it gives the following results."
"although the computing time of each iteration is reduced, thus, a possible drawback is that the total number of iterations required to estimate the model might be higher. the user should be careful when adopting this strategy as, if poor identifiability of θ is not detected, the em algorithm might not converge or it might take more iterations than necessary. in the above example, the em algorithm takes 78 iterations to converge with respect to the 32 iterations required when the log-likelihood is evaluated at each iteration. again, this strategy to reduce computing time is intended for large data sets."
note that the theta_p parameter must be provided in kilometers regardless of the unit of measure of the grid. the matrix v_p describes the cross-correlation between multiple variables and it is equal to 1 for the univariate case.
"in order to estimate the model in a distributed manner, a script that calls the daemon.m function must first be executed on each slave node possibly in batch mode. the function requires as input argument the path of the shared folder to use. the master runs the usual main script but the name of the shared folder, as well as additional parameters, must be given as properties of the obj_stem_em_options object."
"remote sensing data may represent a valuable data source for estimating the ground level variables after statistical calibration based on the available ground level data. in doing this, a change of support problem (cosp) must be solved [cit] . depending on the aim of the data analysis, the cosp can also be considered as a data fusion or downscaling problem. applications are not restricted to air quality remote sensing but include the case of physical model outputs and the case of environmental areal data in general."
"the demo_section7_1.m script, which can be executed by choosing option number four from the dstem_demo.m script, implements the same case study of section 6 with tapering enabled. in particular, tapering can be enabled by providing the width φ (expressed in kilometers) of the wendland function to the constructor of the class 'stem_gridlist' as follows."
"similarly, the loading coefficients are loaded into the same ground structure. note that all the loading coefficients are supposed to be observed without error and/or missing data for each day and each spatial location. the loading coefficients related to β are directly obtained from the covariates as detailed below."
"thanks to the separability between space and time, most of the matrix algebra operations at the basis of the em algorithm only involve the data of a single time step. moreover, each time step is independent from the previous and the next time steps. the kalman filter itself is implemented in such a way that some operations executed at time t are independent from the result of the operations at time t − 1."
"the computing nodes can be any number of heterogeneous machines connected through a local area network (lan) and no additional parallel and/or distributed software libraries are required. all the nodes must be able to read and write to a common shared folder and each node must run at least one matlab process. one node, usually the fastest or the one with the highest quantity of ram, is designated to be the master while all the other nodes are considered slaves. the number of slaves can change during model estimation but the master node must always run."
"in many applications, the observations of a variable must be calibrated using the observations of a second variable or a given variable is observed using more than one instrument and/or technique. for instance, remote sensing data are often calibrated using ground level data. d-stem allows to jointly solve the calibration and the data fusion problems. in particular, point data and pixel/block data can be handled in a multivariate setting."
"although not mandatory, the temporary data structure ground will be used to pass the data to the class constructors. in the following lines of code, data related to the no 2 concentration are loaded into the structure ground along with the variable name. note that the term \"ground\", referring to the monitoring network data, is used to contrast them with \"remote\" sensing data of section 6."
"model parameters are estimated following the maximum likelihood approach by means of the expectation maximization (em) algorithm. when large data sets are considered, the tapering approach can be used in order to obtain sparse variance-covariance matrices reducing the computing time. if a computer cluster is available, model estimation can be performed in a distributed manner exploiting all the available cpu as well as cpu cores."
the next step is to create an object of class 'stem_grid' and to add it in the following way to the obj_stem_gridlist_p object of class 'stem_gridlist'.
"the em algorithm requires the model parameters to be initialized to some starting values. the estimation result may depend on the starting values and they must be chosen carefully. in its current version, d-stem can automatically provide starting values only for the β parameter vector. the following lines of code describe the initialization of the model parameters."
"the tapering approach consists of adopting a sparse variance-covariance matrix characterized by a high percentage of zero elements (possibly higher than 90%). the idea behind tapering is that spatial locations at great distance should not exhibit spatial correlation. hence, tapering forces to zero the covariances of observations at distances higher than a threshold in such a way that the positive definiteness of the variance-covariance matrix is preserved."
-'stem_misc' -the class provides miscellaneous methods used by the mother classes. all the methods of the class 'stem_misc' are static which implies that they can be called without creating an object of class 'stem_misc'.
"this paragraph describes the demo_section6.m script which can be executed choosing option number three from the dstem_demo.m script. only the code related to the pixel variables and the downscaler are discussed here, the reader being referred to the previous section 5.2."
"the understanding of complex environmental phenomena usually requires the analysis of multiple variables observed over space and time, resulting in possibly large and complex data sets. when multivariate space-time data sets are considered, it is common to rely on statistical spatio-temporal models able to exploit the correlation across variables and to provide spacetime predictions over the geographic region of interest [cit] ."
the third and fourth input arguments are empty as they are not required for this case study and will be discussed in section 6. a custom map of the geographic region can be loaded from a shape file and passed as input argument to the constructor. a map of the world country boundaries is provided along with the case study data.
"the model in equations 9 and 10 [cit], with the main difference that d-stem handles multivariate data and use gaussian processes instead of gaussian markov random fields. in fact gaussian processes, thanks to the em algorithm, are more suitable for handling extensive missing data which often arise in remote sensing."
"at this point, the obj_stem_varset_p object of class 'stem_varset' can be created using the class constructor as follows. the empty input arguments relate to the pixel data which, in this case study, are not considered."
"multivariate space-time data sets are challenging as, in general, each variable can be observed at different spatial locations and missing data are the rule rather than the exception. d-stem is able to handle heterotopic data sets where each variable is observed at possibly different sets of spatial locations. the sets of spatial locations or the grids of pixels are assumed to be time invariant. as a consequence, the single observation is considered to be missing if it is not observed at a given time step. missing data, however, are automatically handled without the need of data imputation or interpolation."
d-stem is constantly updated and improved and new versions are released on https:// code.google.com/p/d-stem/. google code runs a project hosting service that provides revision control and an issue tracker. the users of d-stem are welcome to notify bugs and to submit extensions or improvements of the code.
"the estimated model is eventually used to map the no 2 concentration over the geographic region following the kriging approach. since the loading coefficients of this case study consist of a set of covariates, the same covariates must be available for the entire region as a regular grid with the proper spatial resolution."
"at this point, model estimation can be performed by calling the method em_estimate of the class 'stem_model' which requires as input argument an object of class 'stem_em_options'."
"this paragraph describes the relevant lines of code of the demo_section5.m script. the script can be executed choosing option number two from the dstem_demo.m script. since demo_section4.m of section 4 and demo_section5.m are similar, only the differences induced by the multivariate setting are detailed here."
the estimate of the latent temporal variable z(t) is stored in the stem_kalmansmoother_result object which is a property of the stem_em_result object. the graph of figure 1 shows the estimated temporal variable and it has been obtained calling the method plot of class 'stem_kalmansmoother_result'.
this paragraph describes the relevant lines of code of the demo_section4.m script related to the case study previously introduced. the script can be executed choosing option number one from the dstem_demo.m script.
"by default, d-stem compares the estimated parameters and the observed data log-likelihood between two consecutive em iterations. if the relative norm of the difference between the parameter vectors or between the log-likelihoods is lower than the tolerance specified by the property exit_toll of class 'stem_em_options' (see section 4.2), the em algorithm stops. in the case of large data sets, computing the log-likelihood at each em iteration is time consuming. in order to speed up model estimation, the evaluation of the log-likelihood can be avoided and the exit condition is only based on the model parameters."
"the temporal information of the observed data is provided as follows and it is used when model output is displayed. note that both date and time must be provided regardless of the temporal granularity of the data (hourly, daily, etc.)."
"the demo_section7_3.m script, which can be executed choosing option number six from the dstem_demo.m script, implements the same case study as in section 4 but model estimation is carried out without computing the observed data log-likelihood at each iteration."
"the rest of the paper is organized as follows. section 2 surveys work related to the fog computing paradigm and real-time task assignment problem. in section 3, we propose the reinforcement learning model and how to craft the states of the system. section 4 introduces the evolution strategies as a learning algorithm for the proposed model. section 5 shows the experiment results for proving the efficiency of the proposed model and exploring the effectiveness of the parameters of the model. section 6 concludes the study."
"both observed features and latent variables are involved. in this section, we will describe the general philosophy of feature design in the context of the consumer behavior model, and the specific features used in each purchase stage for each dataset."
"in real cases, the complete product price history may be unavailable. given the transaction logs, we can only observe the prices of those products sold at a certain timestamp. however, as we claimed in the previous section, prices of unsold products ought to be included in the model as well, which requires us to attempt complete price history recovery. specifically, we applied a simple 'hot deck' method [cit] for imputing these missing prices, where the transactions are sorted by timestamps and the last observed price of the same product is carried forward to the current missing price. note that this approach can be implemented efficiently but may generate biased values if people rarely buy products at their original price. thus we claim that developing stronger approaches to recover the complete price histories could be another important problem which can potentially be explored as future research."
"subjects. eight brachial plexus root avulsion patients (subject 1-3, 5-9) and one amputee (subject 4) (nine men; mean age 50.8 years, range 38-58 years) participated in this study. all subjects were right-handed. their clinical profiles are listed in table 1 . all subjects were informed of the purpose and possible consequences of this study, and written informed consent was obtained. all experimental procedures were performed in accordance with protocols approved by the ethics committee of osaka university hospital (# 12107)."
"to evaluate the selectivity of onset detection in the closed-loop session, the session was divided into two sections depending on the instruction relative to the current state of the prosthetic hand: \"same-state\" in which instruction and current state were the same (no need to move the prosthetic hand) and \"different-state\" in which they differed (need to move the prosthetic hand). each section was classified by the detection of onset within the section (true positive: different-state section with onset detection; false positive: same-state section with onset detection; false negative: different-state section without onset detection; true negative: same-state section without onset detection; for an illustration of this evaluation method, see supplementary fig. s3 ). the recorded times of onset detections were corrected by subtracting 70 ms, the system delay required to detect onset. selectivity of the onset detection in the closed-loop session was tested using a one-tailed fisher's exact test based on the classified sections. the decoding performance of movement types in the closed-loop session was evaluated by the number of correctly inferred movement types at all onset detections during which the patients intended to move their affected hands (different-state sections). selectivity of the movement types in the closed-loop session was tested using a one-tailed fisher's exact test."
"in this section, we introduce a generalized feature-based matrix factorization approach, which can be adjusted and applied in different purchase prediction stages. the basic notation used in this paper is provided in table 1 ."
"action selection function in the rl model is a trainable machine learning (ml) function that reinforces its ability in action selection through rewards. among various ml functions that are applied to the rl model, the neural network (nn) is the most popular [cit] . since the nn is a universal approximation function, it can fit well to various types of rl problems [cit] . additionally, a combination of the rl and nn has also shown the ability to surpass human level in many applications such as the game of go [cit] . therefore, nn is chosen to be the action selection function in the proposed rl model."
"we consider two real-world datasets of supermarket transactions. msr-grocery is a new dataset of convenience store transactions from a grocery store in the seattle area; since this dataset is proprietary, we also evaluate our method on the public dunnhumby dataset to ensure the reproducibility and extensibility of our results. note that both datasets contain instances of variability in the price of a given product due to promotions, making them an ideal platform to study the effect of price variability on consumer behavior."
"classification accuracy in the open-loop session. meg signals from 84 parietal sensors were converted to smf by averaging over 500 ms and normalizing to a z-score from a 50-s period at the beginning of the session in the same way as the online-acquired features. the powers of three frequency bands (alpha: 8− 13 hz; beta: 13− 30 hz; high-γ : 80− 150 hz) were calculated by the fast fourier transform from the same signal used to obtain the smf, and likewise converted to frequency-domain features. classification accuracy to infer movement type was estimated with decoding features that were calculated at each 500-ms time window starting at − 500 and continuing to 500 ms relative to the execution cue, shifting by 100 ms. nested cross-validation was adopted for classification to avoid overestimation of the decoding accuracy 49 . each test dataset was classified by a decoder trained with the rbf kernel svm using decoding features at a certain time window and the hyperparameters gamma and cost of the svm (see supplementary fig. s2 ). the time window and hyperparameters were selected only based on the training dataset; thus, they were always selected independently from the test dataset."
"on the other hand, price sensitivity has been richly studied in the areas of economics and marketing, from classic demand systems [cit] to customized promotion models [cit] . demand systems are used to explore the relationship between product prices and quantities sold. in this context, price sensitivity is measured by the 'price elasticity' value obtained from a demand system, which is defined as the unit change of purchase quantity (or probability) given a unit fluctuation in price [cit] . in practice, elasticity-based consumer segments are considered and separate demand models are constructed for different consumer segments. such segments can be regarded as useful signals for retailers and manufacturers to identify consumer groups to target. however, there are two limitations in current demand systems: 1) the data volumes involved are typically limited in terms of the number of products, categories and shopping trips 1 and 2) classic demand models are not able to be updated efficiently."
"the rewards given by the testing are compared to the greedy algorithm. the greedy-based result is calculated as follows. an average reward is received after a process that is similar to testing; however, we apply the greedy method to the last 200 tasks (figure 5b ). the process is repeated 100 times and the average reward over 100 times is the reward of the system with the greedy method. table 3 is a summary of the parameters utilized in the experiments. the middle column lists all possible values of a parameter, whereas the role of the value in the last column is not only the initial value of the parameter but also an anchor when we explore the effectiveness of other parameters. for instance, when the number of iot devices changes in a range, the number of fog servers is set to 5. in the case where the initial value of a parameter is fixed, the parameter does not change its value throughout the experiments. a fixed value of a parameter is the optimal value that makes the best contribution to the results and is discovered by grid search. based on the values of parameters in table 3, 11 experiments were conducted. a summary of the results of the experiments is listed in table 4 . in each part of the table, values corresponding to a parameter are highlighted to indicate that we are changing the value of the parameter to discover its effect on the final results. for this reason, other parameters are set to initial values. the only exception is the number of iot devices and the number of fog servers. since the parameters are important in the design of the system, we explore their roles with two different experiments. in experiment 1, the number of fog servers and iot devices are set to five and 100, respectively. we scrutinize the experiment by plotting the result of 10,000 iterations as shown in figure 6a . in this figure, the thin blue plot denotes the rewards in all iterations; the dashed red line is the greedy-based results, i.e., the reward when we apply greedy method. figure 6b has the same presentation but the results of the first 1000 iterations only are shown. in the experiment, the proposed model overcomes the greedy method after a few iterations more than the first 200 iterations and peaks near the 60,000-th iteration (a magnified area). more specifically, table 4 indicates that the proposed model outperforms the greedy method by 15.309%. in experiment 2, the number of iot devices increases to 200. thereby, the average task uploading rate is 19.95 gigacycles per second. to maintain the ratio of the average task uploading rate and the total capability of the servers at 0.96 (as in experiment 1), we increase the total capability to 20.8 ghz."
"the long-term latency optimization of real-time task assignment is one of the most critical problems in the fog computing. the problem is difficult due to its high complexity; therefore, conventional optimization techniques do not work well. in this study, we resolve the problem with an rl model and apply the es algorithm to optimize the model. the experiments show that the proposed model overcomes the greedy approach approximately 16.1% in terms of long-term latency for task execution. moreover, the es algorithm avoids the incorrect convergence to local optima which appears in most of the existing optimization methods based on the gradient. additionally, the algorithm is embarrassingly parallel in implementation. hence, it can speed up the learning process, particularly in practical frameworks and applications such as omega, mesos, kubernetes, and aneka [cit] . to the best of our knowledge, the algorithm is applied to fog computing for the first time. this study also opens a new direction of the real-time task assignment in fog computing based on the rl and neuroevolution approach."
"we evaluate the proposed nested feature-based matrix factorization framework for consumer preference prediction and price sensitivity estimation on two real-world grocery store transaction datasets. for consumer preferences, we evaluate the proposed fmf model's ability to make satisfying purchase predictions in terms of category purchase incidence, product choice and purchase quantity estimation. in addition, we provide analysis of the price elasticity estimations and discuss the economic insights behind these observations."
"finally, we apply two sets of fmf-based methods for all of these three prediction stages: 1) l-fmf-b, c-fmfb-mle and q-fmf-b are three fmf baselines where all features in table 3 except for product prices are included and the mle optimization criterion is applied; 2) l-fmf-p, c-fmf-p-mle and q-fmf-p are three full fmf models where product prices are added back. comparing these two sets of baselines, the importance of product prices can be"
"reinforcement learning (rl) is a class of machine learning, besides supervised learning and unsupervised learning [cit] . the objective of an rl problem is automation and control of a system for adapting to an unknown environment [9, [cit] . in our problem, the training environment is the system consisting of fog servers and their buffers. it is worth noting that the environment is consistent, e.g., for each condition, it expresses a unique state. in other words, a state is representative of the environment at the moment we observe it. at the center of the rl model is the action selection function, briefly, the action function (i.e., task assignment module in the central scheduler). the function selects actions based on the states of the system. each time the system conducts an action, the condition of the environment changes and a new state is expressed. a reward is also assigned to the system for indicating its adaptation. thereby, the objective of the model is to maximize the rewards received, e.g., maximize the adaptation of the system to the environment. in turn, the action function has to reinforce itself to enhance its ability in choosing actions efficiently by harnessing the rewards. when a new state is expressed, the learning loop continues and the action selection function is continuously reinforced. in the proposed model, the action selection function is a trainable neural network [cit] and the learning rule of the function follows the algorithm mentioned in section 4."
"ii,u (t) will decrease as the associated preference prediction increases. for two products i and i, we have the cross elasticity (how a price change for i affects the sales of i )"
"characteristic activations of the cortical potentials were observed in the sensorimotor cortex during the attempted movements of affected hands. some invasive studies using ecog or intracortical signals have reported that, in the sensorimotor cortex, the characteristic features of movement-specific activities such as evoked activity of alpha, beta and high-γ frequency ranges are preserved even in severely paralysed patients 2, 6, 31 . this is the first report in which a non-invasive whole brain study showed significant modulation of cortical currents depending on movement type. the characteristic time course of the smf and the cortical distribution of the escp estimated from meg signals suggest that the smfs originated from the movement-related cortical potentials (mrcps) 29, 32, 33, which gradually increased prior to the movement onset, peaked during the movement, and were predominantly observed in the sensorimotor cortex contralateral to the movements. our results show that even attempted movements of affected hands elicited significant mrcps in the sensorimotor cortex contralateral to the hand and specifically depended on the attempted movement type. actually, the requested movement type for the affected hand was inferred without significant difference in accuracy compared to that for the intact hand; however, even a small decrement in accuracy might be due to the lower amplitude of the mrcfs during the attempts to move the affected hand compared to that during actual movement."
"i,u (t) can be composed of both implicit parameters and explicit features. 6 this equation can be derived based on the fact that d(log(x)) ≈ dx/x."
"specifically, we first consider whether a consumer will make a purchase from a particular category in a certain shopping trip, 2 which can be regarded as a binary prediction problem. if so, we model their purchase from this category following a multinomial distribution. third, we determine what quantity of the product will be purchased, which leads to a numeric prediction problem. this combination of binary, categorical, and numeric prediction is quite different from that used by traditional recommender systems, requiring new approaches to be developed. in particular, we develop a nested framework and extend state-of-the-art feature-based matrix factorization models to include price as a factor; this framework is embedded in the above prediction tasks with different link functions. we evaluate our model on two real-world grocery shopping datasets where our experiments reveal that the proposed framework is capable of providing high-quality preference predictions and personalized price sensitivity estimates."
"dunnhumby category purchase freq., last purchase quant., day-of-week, storeid, household demographics, prices of all products intercept msr-grocery category purchase freq., last purchase quant., day-of-week, prices of all products intercept (a) category purchase"
"to clearly demonstrate the problem, we consider an extreme case in figure 2 when there are only two fog servers: fog1 and fog2, and three tasks are uploaded in order. in addition, the buffer in fog servers are assumed to operate with first-come-first-serve (fifo) policy. the heart of the system is a task assignment module, a.k.a task scheduler, for deciding servers where the tasks are to be executed. at t 1, task1 is uploaded and assigned to fog1. the dark bar represents a time span needed by fog1 to process task1. computational latency of the system is considered to be the maximum latency among fog servers to completely process the uploaded tasks. at t 2, task2 is uploaded and the module chooses fog2 for processing the task. the module makes decision by using the greedy method, which minimizes the latency of the system at the moment when a task is uploaded. it is worth noting that at t 2, a part of task1 is completed by fog1 and the remaining task is in the buffer of the server; this is indicated by a bar with a dashed dotted choke. adopting the greedy policy, task3 is uploaded and assigned to fog1 since the current buffer of fog2 is larger than fog1's at t 3 . consequently, the buffer of fog1 contains both the remaining tasks of task1 and the new task of task3, whereas the buffer of task2 contains the remaining task of task2. at that moment, system latency is caused by fog1."
"while fog computing deployment provides substantial benefit over cloud computing, it exposes a critical challenge in terms of task assignment problem [cit] . if tasks are not assigned to suitable servers, some servers may suffer from a burden in processing while others with rich resources relax [cit] . particularly, the imbalance in resource utilization is heightened in scenarios where a large number of iot devices are present. consequently, efficient task assignment techniques in real-time are inevitable for fog networks, especially over a long-term period to achieve system stability."
"we conducted 11 independent experiments, which are, i.e., at the beginning of an experiment, buffers of the servers are empty, and the result of the experiment does not affect other experiments. from the task uploaded dataset in section 5.1.1, we divide 1 million tasks for testing and the rest for training the rl model. (since our objective to the real-time task assignment is to optimize long-term latency, when we mention n tasks, it implies n consecutive tasks). each experiment includes 10,000 iterations, where each iteration includes training and testing phases. in the training phase (figure 5a ), 600 tasks are randomly chosen for each iteration. the first 500 tasks are included in the chosen tasks replayed and assigned to the servers using the greedy method. (when n consecutive tasks are replayed, they exactly follow the uploading time, i.e., the time spans between task uploading times do not change). the reason we apply the greedy method to the first 500 tasks is to make sure that the buffers are in a normal status. if the buffers are empty, the latency of the system is low and it does not exactly express long-term latency of the system. we store the status of the buffers at the moment right before the task 501 is uploaded (which is the first task of the next 100 consecutive tasks). we generate 10 children of the model following the method in section 4. for each child, the initial state is a combination of the stored buffer and the first task in the 100 consecutive tasks, which craft a state as mentioned in section 3.2. with each task assignment, the child receives a reward (which is the inverse of the latency at that moment). the average reward after 100 task assignments is the reward of the child. after all the children receive rewards, we update the system following the algorithm 1. as a consequence of the training, the rl model maximizes the reward of the system over every 100 consecutive tasks."
"to overcome the aforementioned issues, we proposed a real-time task assignment approach, which leverages reinforcement learning (rl) [cit] with evolution strategies (es) training method [cit] for long-term latency minimization in fog computing. in this approach, a central scheduler that performs task assignment among fog servers considers the fog computing infrastructure as a trainable neural network (nn) [cit] . in the nn, fog computing resources, remaining tasks in the buffers of fog servers, and demand of the offloaded task make up various states of the system. the number of system states is extremely huge; therefore, the es algorithm has been utilized for learning operations for obtaining a fast optimization of long-term latency minimization as a training reward."
"where ω(t) and ψ are the sets of the iot tasks and fog servers, respectively. therefore, the long-term latency minimization function (f ) is given by"
"finally, we let θcate, θ prod, θquant denote the sets of parameters involved in category purchase incidence, product choice and purchase quantity prediction respectively."
(c) purchase quantity table 3 : specific features applied in fmf on the dunnhumby and msr-grocery datasets. notice that coefficients for the item intercept and user intercept indicate consumer bias and product bias respectively.
"suppose product prices are involved in previous fmf models by logarithmic transformations, and pi(t) is defined as the price of product i at timestamp t. due to the linear representation of fmf, for a product i, we can represent the previous preference scores s ·,u captures consumer u's category loyalty or product loyalty which is independent of the product's price and the environment of the shopping trip."
"based on the results in table 5, price elasticity for category purchase prediction is limited. this indicates that it is hard to drive consumers' desire to purchase items from a table 5 : summary of self price elasticity estimation. particular category by a single product promotion (at least for grocery shopping). compared with category and quantity prediction, product choice is the most price sensitive stage (in terms of elasticity) in the decision making process, while price still serves as an important, but less significant, feature for quantity prediction (especially on the dunnhumby dataset). we also notice that consumers in the dunnhumby dataset are more price-sensitive than those in the msr-grocery dataset in the product choice and purchase quantity stages. one possible reason is that the msrgrocery dataset is collected from a convenience store, where people usually have certain targets in mind and are less likely to seek a large inventory of products. on the other hand, the dunnhumby dataset is composed of household-level shopping transactions where consumers may be more likely to redeem promotions and purchase more products."
"evaluated. in addition to mle, we adopt another method c-fmf-p-bpr for product choice where the bpr criterion (11) is used to optimize the personalized product ranking (i.e., the auc * ) directly."
"the number of servers in this experiment is 10, and the frequency of their processors' servers ranges from 1.56 ghz to 2.47 ghz. the improvement in experiment 2 is 16.101%, which is much higher than the improvement in experiment 1. the reason for the significant increase in the improvement with 10 servers, is that the rl model has more options for assigning the tasks to be uploaded, i.e., the model can find the better solution for the task assignment problem in general. however, an increase in the number of fog servers impacts the training time of the model. the reason is that the model has to calculate a probability that a server is chosen for task assignment among 10 servers, which takes more time than choosing a server among five servers in experiment 1. more specifically, average runtimes per iteration of experiments 2 and 1 are 1.667 s and 1.496 s, respectively, i.e., tasks in experiment 2 require 11% more time for each iteration than those in experiment 1. figure 7 illustrates a comparison of results in experiments 1 (exp1) and 2 (exp2). in the second experiment, the rl needs more time to reach the greedy-based results than in experiment 1. in fact, the model only reaches the greedy-based result after 1000-th iteration, which is five times compared to the 200-th iteration in the model in experiment 2. the result expresses that the complexity of the problem of task assignment in fog computing significantly increases in the case when the number of iot devices increases. thereby, the number of fog servers and the total capability of a server correspondingly increase. consequently, the rl model takes more time to find the solution to the problem. on the other hand, the rl in experiment 2 has an optimal solution to the problem after 5000-th iteration, which is not much different than experiment 1 that has the optimal solution after 6000-th iteration. in other words, the rl in experiment 2 incurs a burden with an increase in the number of iot devices in the system; however, it has the ability to find the optimal solution for the task assignment problem as well. from the experiments, it can be confidently shown that the rl approach works well with various fog computing systems. we explore the effectiveness of the number of training tasks in figure 8 . the blue line indicates the improvement of the proposed model in the experiments 3, 1, 4, and 5 compared to the greedy-based result, and the red line is the average run time per iteration of the experiments. the number of training tasks n indicates the number of consecutive tasks that the rl model needs to find the optimal solution (for task assignments). in other words, long-term latency optimization implies optimization in n consecutive tasks. therefore, in case the number of training tasks is large, the rl model can optimize better. in contrast, the model may take more time to find the optimal solution, and in the worst case, it cannot find the optimal solution with 10,000 iterations. figure 8 expresses the analysis, in which the result does not improve in case the number of training tasks is over 100, although the training time increases significantly. conclusively, this parameter should be set at 100 for attaining the best result in the task assignment problem. the effectiveness of the number of children generated in each iteration by the es algorithm is explored in figure 9 . it is apparent that the runtime linearly grows corresponding to the number of children, since for each child, the rl has to run the same training and testing procedure. on the other hand, the improvements of the rl model peak for the population of 15 and decrease in the case of a larger population. since the es algorithm explores a solution in a nearby area, it needs enough samples to make a move toward the optimal area. this is the reason why the improvement grows corresponding to the number of children in the range of five to 15. on the other hand, too many samples do not help to improve the results, since it increases the risk of the nn getting stuck in the local optima, i.e., the model cannot find the best solution to the problem. number of children."
"in the testing phase, we randomly choose 700 consecutive tasks in the testing set. task assignment method for the first 500 tasks is greedy, which is similar to the training phase. however, for the next 200 tasks, for each task, we observe the state of the system, and the trained rl model chooses a server for task assignment based on the state. the average reward over 200 task assignments is the reward of the testing (figure 5b) . we repeat the testing five times and the average reward over five times is the reward of the system after the iteration. each time the reward of the system after an iteration is higher than the maximum reward of the system in the previous iterations, we store the weight matrices of the rl model and discard the previously stored matrices. after 10,000 iterations, the rl model with the stored weight matrices expresses the model that gives the maximum reward (algorithm 1)."
"notice that an advantage of the nested fmf framework is that these three elasticities are additive. if we consider the price elasticity for the whole shopping trip, since"
"besides showing the overall preference prediction performance and the price elasticity distribution across the 104 categories in the dunnhumby and 55 categories in the msrgrocery dataset, we provide detailed explorations of the most price sensitive category from the dunnhumby dataset in the product choice stage: 'bacon (economy).' a summary of product prices in this category is included in table 7, where price variabilities can be observed for all products except product 10. we also include the total quantity sold for each product in table 7, where we notice that products with moderate prices are more popular than others. preference vs. representative features. we find that the estimated coefficient on 'bacon (economy)' for category frequency in the category purchase stage is 0.28, which indicates that a consumer's previous category purchase frequency is still positively related to the category preference. also the estimated coefficient for last purchase quantity is −0.22, which means if consumers purchased a substantial volume of economy bacon products in their previous shopping trips, they may avoid making the same category purchase in their current shopping trip. for the product choice stage and the purchase quantity stage, we find that the estimated coefficients for product frequency and average purchase quantity are 0.28 and 0.20, which indicates these two features are positively correlated with preferences as well."
"therefore the major goal of this study is to construct an interpretable framework to model consumer preferences and price sensitivities at scale, by connecting well-developed techniques in recommender systems and well-established behavioral economic theories."
"we propose the evolution strategies as a learning method for the reinforcement learning model for optimizing the server selection function, i.e., the trainable neural network. the algorithm has low computational complexity and simplicity in implementation. additionally, the algorithm is remarkably parallel due to the independence in evaluation of its children. therefore, it is suitable for modern computers with parallel cpus. 3."
"evaluate parent nn 14: end return the highest performing parent nn in summary, by adding random noise to the copies of the nn, the es algorithm generates a population of networks in the nearby area of the nn. for each iteration, the nn moves toward the area that offers high rewards (positive gain) and avoids the area that offers negative gain. over several iterations, the algorithm seeks the area that offers the best reward, e.g., the optimization of the rl model for the task assignment problem."
we prove by comprehensive experiments that the proposed model is scalable when the system escalates the number of iot devices or the number of fog servers. the model attains 15.3% higher reward than the greedy method in a system with 100 iot devices and five fog servers; and 16.1% with 200 iot devices and 10 fog servers.
"literature reviews [cit] revealed a great contribution of research communities for improving fog computing performances in terms of latency, energy consumption, resource utilization, service availability, their variants, and hybrid solutions [2, [cit] . in the latency minimization objective, the state-of-the-art solutions, however, mainly considered the fog computing on the basis of time intervals. meanwhile, the effective real-time operations that require immediate reaction immediately after the tasks arrive at the fog computing infrastructure have not yet been significantly taken into account."
"the movement intention was also inferred by nested cross-validation, optimizing the hyperparameters with combined decoding features of 11 time windows from − 2500 to − 1000 ms as resting and those from − 500 to 1000 ms as features during intention. [cit] b using the rbf kernel svm."
"preferences & price sensitivities. product preferences are reflected by purchase incidence or purchase quantity in a consumer's shopping history. from item-based collaborative filtering [cit] to matrix factorization techniques [cit], various methods for consumer preference matching have been developed in the field of recommender systems. however, there are few studies where price is considered as a factor, let alone the relationship between preferences and price sensitivity."
"particularly for product choice, consumer purchase behavior is a kind of implicit feedback, in the sense that not purchasing a particular product does not necessarily indicate that a consumer dislikes it. thus rather than predicting if a product is selected via mle, we can instead optimize a criterion that says purchased products are simply 'more preferred' than non-purchased ones. this type of optimization criterion is captured by bayesian personalized ranking (bpr) [cit], a state-of-the-art technique that approximately optimizes the area under the curve in terms of product rankings, i.e.,"
"for each device, the size and complexity of each uploaded task are similar and do not change during its lifetime. the sizes of tasks are in the range [cit] kbits and their complexities are in the range [cit] cycles/bit. consequently, the requirement for a task ranges from 10,000 cycles to 10 megacycles. since we run the experiments with python 3.6, the minimum time scale in the experiments is 1 µs. this means if the uploading times of two tasks are not the same, then their discrepancy is at least 1 µs. it is worth noting that the minimum timescale is practical, particularly in factories."
and n * indicates the total number of successful product transactions in the given category. one advantage of this measure is that the mae is more robust to outliers than the root mean squared error (rmse).
"three-stage purchase decision model. different from modeling user preferences as a whole process, we follow the three-stage framework from recent customized promotion studies [cit] . we notice that in real-world grocery shopping scenarios, products can be categorized either based on an existing commodity hierarchy or by clustering their associated characteristics (e.g. text descriptions). each category should consist of some kind of products where consumers' purchase decisions share similar patterns. for example, one category might be 'organic milk' and two products in this category could be 'horizon organic whole milk' and 'organic valley whole milk.' as shown in figure 1, we assume that for a given category, consumers' purchase decisions can be decomposed into three stages: 1) category purchase incidence, 2) product choice, and 3) purchase quantity. in a complete purchase decision-making process, stages are heterogeneous and consumers may behave quite differently across them. given the fact that there are more than ten thousand distinct products in a typical grocery store [cit], this three-stage model is more efficient compared with a flat model without fine-grained product categorization [cit], since it can be constructed in parallel across product categories and explicitly interpreted across different purchase stages."
"the system is simulated using python 3.6. in the experiments, we suppose that the buffers of the servers are unlimited. computation latency, e.g., a time span for a server to execute remaining tasks in its buffer, has the minimum scale in µs, and follows the smallest scale of time in python 3.6. it is worth noting that in the literature, we only consider computation latency in fog servers. transmission delay and queue delay are not covers in the study. the simulation runs on a single computer with a cpu core i7-4770 3.4 ghz. the computer has 16 gb memory and no gpu is used in the simulation."
"by contrast, the greedy method is commonly used while considering real-time processing. the function of this method is illustrated in figure 1 . at t 0 (figure 1a), there are some servers in processing. when a task is uploaded at t 1 (figure 1b ), some tasks have been done and there are tasks remaining from the previous task list in the buffers. this method determines the demand of a task and the status of buffers for selecting a server with the objective of minimizing latency at that moment. in figure 1, server fog3 has the lowest latency in processing the remaining tasks and the new task, e.g., the new task is assigned to the server. although the greedy method is more realistic compared to time interval approach, it is not functional toward long-term latency optimization. section 3.1 shows an extreme case that the greedy method does not achieve the minimal latency in the long-term, which proves that the method is not sufficient for real-time task assignment. in summary, almost all related works lack the truly real-time consideration and a flexible task arrival adaptation. therefore, an effective solution, which aims at resolving these problems, is crucial for the fog computing system."
"in contrast to the similar activation in contralateral sensorimotor cortex, our results also showed that the ipsilateral sensorimotor cortex was differently activated during movements of intact and paralyzed hands. a previous study using transcranial magnetic stimulation suggested that there was cortical reorganization in the ipsilateral sensory motor cortex after amputation 26 . our results are consistent with asymmetric brain activations for the intact and paralyzed hands. notably, the current estimation technique from meg signals demonstrated its potential for imaging brain activity related to the reorganization. classification analysis using the estimated cortical currents would reveal cortical reorganization in the ipsilateral sensorimotor cortex in terms of neural information."
"since the es algorithm seeks the gain in the nearby area during each iteration, it is important to control the deviation noise added to the children. if the area of a child is very near the root nn, the network may be stuck in the small area. however, if a child is too far from the root nn, we may skip the area that could be the solution to the problem. in the experiments in section 5, the deviation of the children is searched by a practical method. it is worth noting that the es algorithm does not depend on the derivative of the reward function, hence, it is not stuck in the local optima as the back-propagation algorithm which is based on gradients. on the other hand, each child functions independently from the others; therefore, the computation of the es is parallel. this makes the es algorithm work efficiently in a modern computer, which has many parallel cpus, whereas the deep rl models with backpropagation algorithm can only update in a single cpu environment."
"to extend our study, future works should consider an implementation of the proposed approach to real-world data sets such as telehealth big data and smart city on a testbed system. another possible extension of this study is to use es algorithm for simultaneously optimizing multiple utilities of the system in order to provide a balance between latency and energy consumption [cit] . in addition, transmission latency when uploading task to fog servers or responding from servers to iot devices should be taken into account."
"based on the system described above, we define a real-time task assignment problem as selecting a fog server for assigning a task to minimize the computation latency of the system during its operational time, which is referred to as long-term latency optimization. to mathematically express this problem, let x ij denote the case when the i-th task is assigned to the j-th fog server. the latency minimization function at timeslot t is defined by"
"as discussed, we assume that purchase decisions can be predicted in three stages: category purchase incidence, product choice, and purchase quantity. in this section, we propose a nested framework to holistically model the interdependence of these three stages, adopting the above fmf model as a building block in each stage."
we propose a reinforcement learning model for the real-time task assignment in fog networks with the objective of minimizing long-term latency. the method for crafting states of the system is novel and is an important contribution to the success of the model.
"thus if we focus on the category c, a consumer's preferences can be represented by the joint probability of buying a certain quantity of a particular product in category c, i.e.,"
"task. open-loop session. subjects were instructed to grasp with or open the intact hand or to attempt these movements with the affected hand once at the time of the execution cues given visually and aurally every 5.5s, 40 times for each movement type (fig. 1a ). to reduce motion artefacts, the subjects were instructed to perform the attempt or actual movement without moving any other body part. the type of movement to perform was presented visually with either the japanese word for \"grasp\" or \"open. \" after the movement type instruction, four execution cues were given to the subject. the order of the movement type instructions was randomized."
"offline evaluation of onset detection. the algorithm of the real-time decoder was adapted to the smfs in the open-loop session using ten-fold cross-validation to evaluate timing of the onset detection. the real-time decoder was trained with a training dataset, and the time of the first onset detection was pinpointed in each trial within the test dataset. the search started at − 2000 ms relative to the time at which the classification accuracy of the movement types peaked within the training dataset, and continued until 1000 ms. during the searched period, the smfs were tested at an interval of 200 ms."
"we experiment task assignment in the fog computing of a factory, where iot devices are frequent but have noise interference. the system includes 200 iot devices and 10 fog servers with different capabilities of task processing. although the complexity of real-time task assignment problem is high, the proposed reinforcement learning model with evolution strategies algorithm reaches the objective of optimizing long-term latency and has 16.1% higher reward than the greedy method, which is the baseline in real-time task assignment. the contributions of this study are as follows."
"closed-loop session. visual feedback was given to subjects with a screen fixed in front of them, showing a picture of the prosthetic hand in real-time and the instruction monitor (fig. 1b) . the instruction monitor displayed either the japanese word for \"grasp\" or \"open\" alternately every 7s for a total of 22 instructions. subjects were told to control the prosthetic hand by following the instruction (grasp or open), using the same attempts to move their affected hand as in the open-loop session."
"we introduce the concept of 'price elasticity' to model the product price sensitivity, which is a popular measure in economics and can be defined as the responsiveness of a product's purchase quantity (or probability) to changes in its price ('self elasticity') or another product's price ('cross elasticity') [cit] . self elasticity values are usually negative. larger absolute values of elasticity indicate higher price sensitivity, which means if the product price drops, its purchase probability or purchase quantity will increase accordingly. since products within a category are often the same kind of commodities (and likely to be substitutes), the cross elasticity values in the product choice stage are usually positive, which indicates that if the product price drops, purchase probabilities of other products within the same category will decrease."
"to determine the signal source of the smf, the cortical currents were estimated by vbmeg and time-averaged to obtain an estimated scp (escp) in the same way as the smf. as shown in fig. 3a, when a patient attempted to move his completely paralysed right hand, escps were clearly activated in the left sensorimotor cortex (contralateral to the tested hand) and depended on the movement type, similar to the activation during movement of his intact left hand (fig. 3b ). the differences in the escps between the two types of movements were evaluated by one-way analysis of variance (anova). the f-values, colour-coded on the reconstructed surface of the normalized brain, also show that the escp in the contralateral sensorimotor cortex varied significantly between the two movement types, similar to those during the actual hand movements (fig. 3c ). notably, a significant f-value in the contralateral sensorimotor cortex was observed in all subjects for movements of their intact hand, and in eight of nine subjects (except subject 9) for attempted movements of their affected hands. these results suggest that, even during attempted movements of affected hands, the motor representations by cortical currents are preserved in the contralateral sensorimotor cortex, and are similar to those during actual movements of intact hands. movement decoding. the motor information about movement type and movement intention was evaluated by decoding analysis. classification accuracies were compared between smfs and powers of the alpha, beta, and high-γ bands. movement type and intention features were extracted from the 84 parietal meg sensors ( fig. 1b ) and the analysis was performed using a nested cross-validation technique 30 and a support vector machine (svm, see methods)."
"baselines. consumers' previous category purchase frequencies, product purchase frequencies and average purchase quantities can be adopted as three simple baselines -catefreq, prodfreq and avgquant for category purchase, product choice and purchase quantity predictions."
"analysis of offline data. cortical current estimation by vbmeg. a reconstruction of the cortical surface was constructed based on mr structural images using freesurfer image analysis 48 . using the vbmeg, we estimated 4004 single-current dipoles that were equidistantly distributed on and perpendicular to the cortical surface. the method calculated an inverse filter to estimate the cortical current for each dipole from the selected meg sensor signals. the hyperparameters m 0 and γ 0 were set to 100 and 10, respectively. the inverse filter was estimated using meg signals from 0 to 1000 ms relative to the execution cue, with a baseline of the current variance estimated from the signals from − 1500 to − 500 ms. the filter was then applied to sensor signals in each trial to calculate cortical currents."
"to optimize the rl model following the rewards, we update the nn to enhance the ability of the model in choosing actions for task assignments. backpropagation is the most popular algorithm for updating the nn. the algorithm calculates the derivatives of the objective function given by weights of the nn and updates the network toward maximizing the objective. however, in our problem, if we update the nn following the current action but not future actions, we cannot attain long-term optimization. some backpropagation-based paradigms are proposed for optimizing the long-term reward (e.g., deep q-learning [cit] ). however, in practice, such paradigms only work well if the reward received from the environment is either 1 or 0 (which means winning or losing the game). the following drawbacks hinder the algorithms' success in the real-time task assignment problem since the reward from the environment (e.g., the inversed latency of the system) is arbitrarily floating values. neuroevolution (ne), i.e., neural network evolution, which is inspired by biological evolution, is another approach for training neural networks [cit] . in nature, evolution begins when parents produce an offspring with random deviation. among the children, those who fit the environment have better opportunity to survive and reproduce their genomes. as a result of the selection, the next generation enhances the fitness to the environment. the concept of ne is similar to evolution in nature. given an nn, for each of its iterations, a new generation is produced from the nn, which includes derivations of the nn. the children that have the highest rewards are chosen and the nn is updated based on the rewards. the method to update the nn is conducted in evolution strategies (es), which is the most well-known algorithm that applies the ne approach [cit] . algorithm 1 describes the process for updating the nn by es. for each iteration, m children of the nn are produced by adding gaussian noise to each weight in the network. each child nn plays a role as task assignment module in the rl model with n consecutive tasks and receives an average reward over n actions. since an average reward is the feedback of the system to the actions chosen by the child, it is also the fitness of the child to the environment. we calculate the mean reward of m children and differences of the rewards of children with the mean reward, which is also the gain of the children over the root network. if a child has a gain, it has better fitness than the root network and should be encouraged to contribute more to the next generation. following that idea, the root nn is updated by adding weights of children to its weights toward the gain of the children."
"however, the greedy method is not suitable for long-term latency optimization. in the example above, if focusing on long-term latency, we can propose a better solution to the task assignment problem as shown in table 2b . in the table, when task2 is assigned to fog2, system latency is 7 ms, which is an increase from the latency of 6.5 ms in case of the greedy method. however, task3 is assigned to fog1; this causes the latency of the system to be 7 ms. as a result of the change in task assignment, system latency right after the moment 2 ms reduces (7 ms compared to 8 ms when we apply the greedy method). intuitively, given the state of a system, which includes task demand (size and complexity) and status of server buffers, the action of assigning a task to a server changes the state of the system at that moment, and a reward is returned, e.g., an inverse of latency. since the tasks that are uploaded are not totally random but frequently with noise, there should also exist a reward pattern when the determined system states are given. therefore, in this study, we utilize reinforcement learning for exploiting the pattern of the pair state-reward to minimize the latency of the system in the real-time task assignment."
"in table 6, we provide details of the eight most pricesensitive categories in each purchase stage. from table 6b, we notice that consumers tend to select the most inexpensive products when shopping for meat (bacon, pork rolls, tuna), eggs, drinks (water, juice, coffee), cereal and snacks (potato chips, candy). in addition, from table 6c we observe that consumers are more likely to stock products which have relatively long shelf lives (e.g. frozen food, soft drinks) if appropriate promotions are offered. some featured categories (categories with promotions and located in designated areas) in the msr-grocery dataset appear in table 6a and table 6b, which indicates that a combination of promotions and advertisements may help to affect consumers' purchase decisions."
"movement-related activity during the open-loop session. the characteristic activation of meg signals was observed similarly when subjects attempted to move their affected hands and when they actually moved their intact hands in the open-loop session. figure 2a shows a representative mean contour map of the to begin, one of the movement types, grasp or open, was presented on the screen in front of the patient, followed by two \"timing cues\" and an \"execution cue\" at an interval of 1 s. the patient then attempted to move the affected hand as instructed at the timing of the \"execution cue. \" each movement type was repeated four times. (b) system overview of the real-time prosthetic hand control. meg signals from 84 parietal sensors, denoted by red dots, were acquired in real-time and analysed on a single computer. the prosthetic hand was controlled according to decoders that inferred the timing of movement intention and types of performed movements. the patient controlled the prosthetic hand by watching the screen representing the prosthetic hand and following the instructions for movements."
"the meg-based neuroprosthesis examined here is the first non-invasive bmi controlled by severely paralysed patients by combining motor information about both movement type and intention. our previous study using ecog revealed that motor information about movement intention was preserved even among severely paralysed patients, whereas motor information about movement type varied widely 6 . most of the previously reported non-invasive bmis controlled the prosthesis based on preserved motor information about movement intention extracted using alpha and beta frequency powers in the sensorimotor cortex [cit] . however, as shown in our results, these features are not good at distinguishing multiple movement types and can only discriminate a moving (movement imaging) state from a resting state. by contrast, our meg-controlled neuroprosthetic hand inferred not only movement timing, but also movement type. it enabled the paralysed patients more natural and sophisticated control of the prosthesis to perform multiple movements. moreover, as the amount of motor information varies from patient to patient 6, the proposed bmi can be adapted for individual patients based on how much motor information can be derived from their brain signals, thereby maximising performance by inferring movement timing in those patients without movement-type information."
"in the real-time task assignment problem, we define a system to be a fog network that includes iot devices that generate tasks, fog servers with various capabilities in processing a task, and a task assignment module that chooses servers where the tasks are executed. the tasks are uploaded in real-time in the system, which means that a time interval between two consecutive tasks is in the range [0, ∞]. each fog server has a buffer for remaining tasks with unlimited capacity. for the sake of simplicity, we assume that an iot device always generates tasks with the same size and complexity. in this study, we consider a scenario in a factory where tasks uploaded by iot devices are frequent but occasionally have an interference of random noise. as there are many devices that continuously upload tasks, traffic reaching to fog servers is extremely noisy and the complexity of the problem is high. table 1 lists the explanation of terms used in the study. it is observed that the total latency consists of propagation, execution, and buffering. to adapt to various networking environments, we consider the iot task arrival as a random and independent process on the communication channels between the iot devices and fog servers. in other words, the propagation latency is omitted in the scope of this study. let s i, c i, τ i denote a three-dimensional characteristic vector of the i-th task, where s i, c i, and τ i are the size, complexity, and latency threshold of the task, respectively. in addition, let f j and b j denote the cpu frequency and current buffer size of the j-th fog server, respectively. assume that the i-th task is assigned to the j-th fog server, and then the latency of i-th task is given by"
"we systematically studied the problem of modeling consumer preferences and price sensitivities, and proposed a nested feature-based matrix factorization framework to support personalized and scalable recommendation and demand systems. we verified that the proposed model is capable of providing high quality preference predictions and specific price elasticity can be appropriately estimated for each shopping trip. by applying the proposed framework on two realworld datasets, we provided economic insights which may benefit both data mining and economics communities. par- ticularly, we noticed that price affects product choice but has limited effects on category purchase or product quantity, which means coupons are primarily effective \"within category\". grocery shopping behavior is particularly explored in this study but the nested multi-stage framework and the relationship between preference and price sensitivities can be translated to other domains (e.g. clothes shopping, online advertising). price sensitivity in large-scale systems is an important problem and a number of possible topics can be explored along this trajectory. for example, temporally-aware models could be developed to allow long-term purchase patterns to be carefully studied. cross elasticity has been introduced but not completely explored in this work; this could be studied in detail in future work where not only product substitution but product complementarity could be modeled. in addition, since the straightforward imputation method we applied to recover price history will be problematic if people rarely buy products at their original price, another possible direction could be to develop more sophisticated approaches for price history recovery by combining preference prediction and missing price inference. in the context of hybrid recommender and demand systems, we have so far only studied consumer behavior in this work, but the optimization strategies could be adapted to generate personalized coupons."
"this joint probability can be regarded as a product of three conditional probabilities which represent the preferences in previous purchase stages. by adopting different link functions in the previous fmf formulation, these three preferences can be estimated by logistic, categorical, and quantity-based fmf models."
"we collect task uploading in one hour and save the tasks in a dataset along the uploading time. the number of tasks uploaded is over 12 million. the average number of tasks uploaded in one second is 2700, and the average task rate per second (which is a sum of requirements of all tasks in one second) is 7.68 gigacycles."
"in this study, we applied an meg-based neuroprosthesis to eight severely paralysed patients suffering from brachial plexus root avulsion and to one amputee (table 1), to evaluate the cortical currents representing the attempted movements of their affected hands and the performances in controlling the neuroprosthesis. meg signals were recorded during attempts to grasp with or open their affected hand, and during actual execution by their intact hand (open-loop session, fig. 1a ). the obtained signals were converted into smfs and some frequency powers to elucidate the appropriate features for controlling the prosthetic hand and a signal source reconstruction technique (variational bayesian multimodal encephalography; vbmeg) 28 was used to reveal the activated brain areas. moreover, five patients underwent another measurement with real-time meg to control a neuroprosthetic hand under a closed-loop condition using smfs evoked by the attempted movements of their affected hand (fig. 1b) ."
"to control the neuroprosthetic hand, we used an onset detection algorithm 14 to infer the timing of movement intention using the smfs (see methods). the accuracy of the algorithm was evaluated for the meg signals from − 2000 to 1000 ms of the open-loop session (see methods). the initial time of the inferred movement onset was evaluated in each trial by the nested cross-validation technique (see methods). the algorithm successfully inferred the movement onset to be around the peak of the classification accuracy of movement type (0 ms, fig. 5 ). notably, the detected time of movement onset peaked at − 200 ms for both affected and intact hands. moreover, the onset was selectively inferred within ± 500 ms in 63.0 ± 15.6% (mean ± sd) and 63.1 ± 16.0% of trials, for affected and intact hands, respectively. thus, the proposed algorithm succeeded in detecting the point in time at which the classification accuracy of the performed movement type was high."
"modeling consumer preferences and price sensitivities at scale is useful in both online and offline shopping worlds: matching shoppers with the most desired products can help improve overall satisfaction, while providing appropriate promotions may lead to increased basket sizes (and revenue). grocery shopping is one of the most frequent and regular shopping patterns in an individual or household's day-to-day activities. as a result, incredible volumes of data including transaction logs, product meta-data, and consumer demographics, can be collected from a number of offline (e.g. wal- figure 1 : general workflow of the proposed three-stage purchase decision model. mart, kroger, whole foods) and online (e.g. amazonfresh, walmart.com) grocery stores and supermarkets."
"evaluation methodology. note that the number of purchase incidences for each category is usually much smaller than the total of those for the remaining categories in the complete transaction logs. therefore we apply the area under the curve (auc) metric to evaluate the performance of category purchase prediction, which is suited to imbalanced binary prediction tasks [cit] . for product choice, in real-world recommender systems, one is often interested in providing satisfactory ranked lists instead of simply predicting incidence. thus we directly adopt the auc * defined in (10), which measures if the selected product is preferred to those products that were not selected in each shopping trip."
"user random coefficient, item latent factors µu(t) probability of user u selecting a category η i,u (t) conditional prob. of user u purchasing product î q i,u (t) u's conditional expected quantity of product i observed item/user-specific effects and latent item-user interactions."
"next we consider price elasticity estimation. table 5 shows summary results (median, mean and standard deviation) of the elasticity distribution across all shopping trips in each purchase stage. elasticity for product choice is calculated from c-fmf-p-bpr."
"fog computing was developed to act as an intermediate between a remote cloud computing environment and internet of things (iot) devices. it is a novel architecture that extends the cloud to the edge of the network [cit] . in fog computing, latency-sensitive tasks can be executed at the fog servers, near the devices, while delay-tolerant and computationally intensive applications can be offloaded to the cloud. fog computing also provides additional advantages such as the ability of processing applications at specific locations. owing to these advantages, the fog computing infrastructure is increasingly utilized for handling real-time iot services and applications [cit] ."
"demand systems and price sensitivity have been an ongoing focus of economists [cit] . three-stage purchase decision decomposition (i.e., category purchase; product choice; purchase quantity), such as we consider here, has been explored in several studies [cit] . customized promotion techniques have been recently proposed for offline and online shopping behavior [cit] where individual purchase behavior is considered and optimal promotions are derived. however, these are not completely personalized demand systems and consumer segmentation is required beforehand. in addition, none of these models is considered in the context of large-scale predictive systems."
"where h and η are the number of children and the learning rate (how fast should we update the weights of the nn), respectively. it is worth noting that since the gain of a child is a difference between the mean reward of m children and its reward, the gain can be negative. in that case, equation (12) discourages the child to contribute to the reproduction of the next generation."
"using an algorithm we developed previously 14, a prosthetic hand was controlled by neuromagnetic activity evoked by the patient's attempts to move their affected hands. [cit] (mathworks, natwick, ma, usa) was used to calculate decoding features and to control the prosthetic hand in real-time. in the open-loop session, meg signals from 84 parietal sensors (fig. 1b) were averaged over 500 ms and were z-scored using the mean and standard deviations estimated from the initial 50s of the session to acquire the smf. the smf was calculated with its time window beginning at − 2000 and continuing to 1000 ms with respect to the time of the execution cue, shifting by 100 ms. to control the prosthesis in the subsequent closed-loop sessions, a real-time decoder was trained using the smfs from the previous open-loop session. in the closed-loop session, the real-time decoder estimated the confidence values of movement intention via the radial basis function (rbf) kernel svm in libsvm toolbox 45 and the gaussian process regression 46 in the gpml toolbox 47, using the latest smf every 200 ms (see supplementary fig. s1 ). we used an onset detection algorithm combining these two confidence values to detect the timing of the patient's intention to move their affected hands. in the algorithm, an onset was detected when both of the confidence values exceeded their respective thresholds, which were set manually. to avoid multiple detections in a single intention, onsets detected within 1.5s from the first detection were ignored. at the time of the detected onset, the attempted movement type (grasp or open) was inferred by another rbf kernel svm, and the prosthesis was controlled to form the hand shape of the inferred movement type. the prosthetic hand used in this study was developed by dr. hiroshi yokoi to imitate the human distal upper limb. ten servo motors controlled the joints in each finger, which had 2 degrees of freedom, using flexible wires in a coordinated manner to form a grasping or opening hand shape. the overall delay from the meg system to the visual feedback of the prosthetic hand was around 830 ms in total: real-time data acquisition, ~20 μ s; data processing, ~70 ms; the time window for the smf, 500 ms; prosthetic hand control, ~150 ms; projection to screen in meg room including video recording, ~110 ms."
"paralysis might affect the cortical potentials recorded over the motor cortex and, thus, the performance of a brain machine interface (bmi). previous studies revealed that cortical reorganization took place in both hemispheres in accordance with the degree of motor dysfunction [cit], and that cortical potentials of paralysed patients were affected in amplitude and latency from the onset of movement 27 . moreover, it has been shown that the ecog of paralysed patients deteriorates in response to different movements, resulting in decreased accuracy of decoding 6 . thus, the cortical potentials of severely paralysed patients should be non-invasively evaluated before the invasive treatment to predict the performance of neuroprostheses. in particular, the information represented by the scp and frequency features should be estimated over the whole brain to evaluate the effect of cortical reorganization after paralysis."
"moreover, the performance of our system might be improved, however, by refining the hardware and training the patients to use the prosthetic hand. previous studies reported that the delay of visual feedback deteriorated tracking performance [cit] . in our system, an intention required a delay of approximately 830 ms to control the prosthesis. the performance of the real-time control might be increased by improving the processing speed of the system. in addition, the results of this study indicated that some frequency features also contain information about movements. by improving the speed, the system may be able to handle these features, whose calculations are time-consuming. moreover, previous studies revealed that the performance of the bmi was improved by training with feedback 8, 41 . by combining minimisation of the delay and training, the performance of the prosthetic hand control is expected to improve. the performance of our meg-based neuroprosthesis might reflect a patient's ability to control an invasive bmi using the scps in the sensorimotor cortex. the source localization analysis showed that the observed smfs had characteristics of the mrcfs and originated from the scps in the sensorimotor cortex, which are used in the ecog-based bmi 6, 42 . moreover, previous studies revealed that appropriate feedback using a real-time bmi was able to enhance cortical function 43, 44 and to improve the bmi performance itself 8, 36 . by using signals common to both the invasive and the non-invasive bmi, the proposed non-invasive bmi might possibly be used to evaluate and train a patient's individual ability to control an invasive bmi."
"recommender & demand systems. the general workflow of the type of hybrid, large-scale recommendation and demand system we are considering is shown in figure 1 . we feed large transaction logs including product prices, metadata, and consumer information to our behavioral model which generates feedback in the form of purchase predictions. on top of this model, we apply different optimization rules to provide user-specific results. for example, personalized ranked lists can be provided by matching preferences, or customized promotion strategies can be provided based on estimated price elasticity. or hybrid personalized coupon lists can be provided by combining preference-matching and price-matching criteria. to achieve these goals, we need to consider consumer preferences in concert with price sensitivities in our behavioral model."
"preference matching has been richly studied in the area of recommender systems, where two kinds of approaches of interest have been developed: 1) content-based approaches [cit], where explicit user profiles or item information are used as features, and 2) collaborative filtering approaches where preference predictions mainly rely on users' previous behavior [cit] . by combining multiple techinques, hybrid recommender systems can be developed to handle a variety of complex scenarios [cit] . matrix factorization techniques have been widely applied for recommender systems due to their accuracy and scalability [cit] . of particular interest, feature-based matrix factorization techniques have been proposed [cit] and efficient tools (e.g. svdfeature, libfm) have been developed [cit] . such ideas have been included in a recently proposed generalized linear mixed model (glmix) [cit], which has been deployed in the linkedin job recommender system with a scalable parallel block-wise coordinate descent algorithm. we build upon glmix and adapt it to fit different prediction settings, such as multi-class classification."
"to train an ml model, we define an objective function for measuring how well the model is performing in a problem and optimize the ml model based on the function. given the rl to solve the task assignment problem, our objective is choosing an action for minimizing the long-term latency of the system. however, the rl is defined to train for maximizing rewards from the system. consequently, a reward is an inverse of the system latency. more precisely, a reward from the system after choosing an action a(t) is defined as follows."
"to move their intact hand, and in another open-loop session to attempt movements of their affected hand. five subjects (subjects 1-5) joined one closed-loop session after the open-loop session for the affected hand to perform the online control of a prosthetic hand. in the closed-loop sessions, a real-time decoder, which was trained with decoding features calculated from the previous open-loop session, controlled the prosthetic hand. to control the prosthetic hand, the subjects were instructed to perform the same movements performed in the open-loop sessions. in the beginning of each closed-loop session, the experimenter modulated the thresholds of the real-time decoder to detect movement intention. then, the subjects controlled the prosthesis with a fixed threshold to evaluate the performance of the prosthesis. to avoid fatigue, each subject participated in only one closed-loop session for evaluation, and did not have enough time to be trained with the prosthetic hand control. figure 1b shows an overall schematic of the system. the subjects lay in a supine position with a cushion placed under their elbow to reduce artefacts caused by shoulder movements. a projection screen fixed in front of their face presented visual stimuli using a presentation system and a liquid crystal projector. neuromagnetic brain activity was measured by a 160-channel whole-head meg housed in a magnetically shielded room. the meg signals were sampled at 1000 hz with an online low-pass filter at 200 hz and acquired online by fpga daq boards after passing through an optical isolation circuit. subjects were instructed not to move their head to avoid motion artefacts. the head position was measured by five marker coils attached to the subject's face to estimate cortical currents before each session."
"here y (t) is the time-aware label matrix, where each element yi,u(t) indicates the label for an item i and a user u at timestamp t. yi,u(t) could be a binary label when predicting category purchase or product choice, or a numeric label when predicting purchase quantity. by applying a link function link (·) (e.g. the logit function, or logarithm function), we can transform the original label matrix into a numeric matrix l(t) and decompose l(t) as a product of φ(t) and ψ(t). here φ(t) and ψ(t) capture both explicit features and latent factors from items and users. specifically for each element li,u(t) in l(t), we have"
"we previously developed and tested on healthy subjects a novel neuroprosthetic hand that used real-time meg signals for movement 14 . here, we tested this neuroprosthetic hand on severely paralysed patients and an amputee to evaluate their ability to control the prosthesis through the slow components of the meg signals. we demonstrated that the slow components conveyed enough information about the affected hands to infer the timing and type of the attempted movements and to control a neuroprosthetic hand in real-time. moreover, the slow components of the meg signals appear to reflect the slow components of the cortical potentials in the sensorimotor cortex related to motor information. although further studies are required, these results suggested that this meg-based bmi might estimate the ability of paralysed patients to control an invasive neuroprosthesis using the slow cortical potentials of the ecog signals."
"in figure 10, we explore the effectiveness of the number of hidden nodes in the nn to the final result. since a neural network is a universal approximation function, an nn with more nodes approximates a function better than an nn with fewer nodes. moreover, if the number of nodes in the nn is too small, the result is not stable enough. however, if the number of nodes is large, it takes much more time to train the network, and in the worst case, the nn cannot reach the optimal solution after 10,000 iterations. the figure clearly expresses the aforementioned analysis. the result fluctuates if the number of nodes is low. the network peaks at 1024 nodes and performs poorly in case the number of nodes is too large. it is worth noting that the number of hidden nodes only slightly affects the running time. in fact, if the number of hidden nodes in the nn is low, it does not affect the running time at all."
"since the three purchase stages are heterogeneous, we assume θcate, θ prod, θquant are separate parameter sets. models for each stage can then be inferred independently. the proposed framework inherits the scalability of matrix factorization techniques, where efficient algorithms such as stochastic gradient descent can be applied [cit] . we optimize all terms following the principle of maximum likelihood estimation (mle). for a given category, we have the following likelihood functions for category purchase, product choice and purchase quantity:"
"dunnhumby product purchase freq., product price, price*freq., price*day-of-week, price*storeid intercept, product price, product info. (brand, manufacturer, size description) intercept, product price, household demographics msr-grocery product purchase freq., product price, price*freq., price*day-of-week intercept, product price, product info. (package size, size description) intercept, product price"
"preference vs. price sensitivity. recall we claimed that if the variances of price-associated coefficients in (13), (14), (16) are limited, then consumers with high preference scores will be relatively insensitive to price changes as far as cat- egory and product choice is concerned, but they tend to be price sensitive with respect to purchase quantity. again taking 'bacon (economy)' as an example, in figure 3 we show the relationship between preferences and price sensitivities in different purchase stages. we notice that all elasticity values are negative, which is consistent with the intuition that purchase probability will increase if product price drops. here absolute price elasticity values are generally negatively correlated with preferences in category purchase and product choice, but positively correlated with purchase quantity, which indeed verifies our previous arguments about the relationship between preference and price sensitivity. in figure 3b, we notice that 'low-preference' consumers have larger variations in price sensitivity than 'high-preference' consumers. this is possibly because highpreference consumers' preferences dominate purchase decisions (i.e., 1−ηi,u(t) is close to zero in (14) ) and they tend to purchase a product no matter its price. on the other hand, if a product is not preferred by a consumer, this could be either because the price is too high to trigger a purchase, or because the consumer simply dislikes the product. in figure 3c, we observe that those consumers with strong preferences are not the most price-sensitive consumers. this observation is consistent with the intuition that aggressive buyers are more likely to exhaust the potential of purchase quantity due to budget limits so that it would be difficult to increase their purchase quantities by adjusting price."
"we also consider standard logistic regression (l-reg) for category purchase where all the global features in table 3 are included. for product choice, matrix factorization as in (3) (mf-mle) is applied to fit the multi-class classification setting."
"personalized product-specific price sensitivity. among the 11 products in the example category 'bacon (economy)' there are 448 consumers who have purchased products in this category. we randomly select 10 consumers and calculate their average price elasticity for each 'bacon (economy)' product in terms of category purchase, product choice and purchase quantity decisions. heatmaps of the results are shown in figure 2 . we notice that within the 'bacon (economy)' category, different consumers and products may have significantly different price sensitivities in each of the three stages, though the personalized elasticity is not obvious in figure 2a since the user-specific price coefficient is not considered in the first stage. by setting appropriate thresholds for price elasticity, we can easily uncover those price sensitive consumer-product pairs in figure 2 and customize promotion strategies accordingly. in addition, we find that in figure 2a, consumers are more sensitive to the prices of products 2-6 in the category purchase stage, which indeed are popular products as we observed in table 7 . this implies that while it is hard to increase the possibility of category purchase incidence, promotions on popular products will be more effective than others in terms of category purchase."
our goal in this paper is to study the problem of modeling consumer preferences and price sensitivities from large-scale grocery shopping data in order to support personalized and scalable recommendation and demand-forecasting systems.
"it is important to note that this method is a classical downscaling method and not a grid-refinement technique, which means that with meco(2) for instance over germany we calculate the contributions three times, once in each model instance (emac, cm50, and cm12). by comparing the results of the different model instances the impact of the model resolution (and the model itself) can be investigated."
"for the specific model set-up involving the global model emac and the regional model cosmo-clm/messy our results show that simulated differences in ozone contribu-tions on a continental scale (e.g. europe) are rather small. the largest differences in the contribution of anthropogenic emission sources were up to 10 % for the contribution of land transport emissions to ground-level ozone. however, the contribution of stratospheric ozone to ground-level ozone calculated by emac and cosmo differs by up to 30 %. one main reason for this large difference in the contributions of stratospheric ozone between the two models is the existence of enhanced vertical mixing and larger convective up-and downdrafts in cosmo-clm/messy compared to emac. taking the comparison with the measurements into account, the vertical mixing in cosmo-clm/messy and the enhanced stratospheric contribution are likely too large. on the regional scale, the differences between the contributions of anthropogenic emission sources simulated by cosmo-clm/messy and emac are much larger. here, we observed differences of up to 20 % for the contributions of land transport emissions to ground-level ozone. this difference is mainly caused by the coarse land-sea mask used in the global model instance, leading to emissions of land transport emissions over sea, different ozone dry deposition, and missing biogenic emissions. taking the results of the same model instance (cm50) into account, the largest influence on the results is caused by different emissions inventories. locally, however, coarsely resolved emission inventories and differences between the biogenic emissions can also lead to differences of up to 20 %. in addition, we showed how the differences in the source apportionment results between different model instances can help to explain model biases and the physical/chemical mechanisms causing these biases."
"where n is the number of data points, o 3 mod the simulated, and o 3 meas the measured ozone concentrations. the normalised mean bias error (mb) is defined as"
"the differences, which cannot be attributed directly to the resolution of the anthropogenic emission inventory, are caused by a variety of other model factors which cannot be disentangled in detail. the most important factor in this context is the enhanced vertical mixing in cm50 compared to emac, mainly in the boundary layer; it is also due to stronger convective up-and downdraft mass fluxes in cm50 compared to emac. the enhanced vertical mixing transports higher amounts of ozone from the free troposphere into the boundary layer, leading to higher ozone mixing ratios in the boundary layer. in addition, ozone precursors are transported more efficiently from the boundary layer into the free troposphere. further, differences in the land use classes between emac and cm50 lead to differences in the calculated dry deposition velocities, which affects also ozone mixing ratios near the surface [cit] . figure 7 shows the absolute and relative contributions of different emission sources to the european ozone column up to 850 hpa as simulated by emac and cm50 for the ref simulation (see table s1 in the supplement for a detailed definition of the tagging categories). the largest absolute and relative ozone contributors are the anthropogenic non-traffic and the biogenic categories, both with contributions of more than 1 du, corresponding to more than 15 %. both model instances simulate similar absolute ozone contributions from the categories anthropogenic non-traffic (≈ 1.0 du), land transport (≈ 0.7 du), shipping (≈ 0.5 du), and biomass burning (≈ 0.4 du). for the biogenic category, cm50 calculates slightly larger absolute contributions compared to emac (see sect. 4.2), but the differences are small compared to the temporal variability in the contributions. further, cm50 calculates larger absolute contributions from the lightning and stratosphere categories. this mainly affects the categories land transport, anthropogenic non-traffic, shipping, and biomass burning, where emac simulates 0.1 to around 1 percentage point larger relative contributions compared to cm50. at the same time, the increased vertical mixing in cm50 leads to an increase in the relative contributions from the categories stratosphere, lightning, and aviation compared to emac. here, the differences are in the range of 0.1 to around 1.5 percentage points."
"in the present study, we are focusing on the question: are contributions of emissions to ozone a matter of scale? to answer this question we compare the influences of the model, the model resolution, the emission resolution, and the emission inventory on the results of ozone contribution analyses. for this we apply the meco(n) model system which combines a global and a regional model by means of an online nesting technique. by applying the identical tagging diagnostics (source apportionment method) in the regional and global model with consistent boundary conditions, we are able to compare the results of model instances with different resolutions to investigate the influence of the model and emission inventory resolutions onto the diagnosed ozone contributions. such analyses are important for quantifying uncertainties of ozone source apportionment studies, which arise due to limitations of the model and/or computational resources."
"to evaluate the simulated ozone mixing ratios in the free troposphere, the model results are compared to ozone sonde data (see sect. s4 in the supplement for a list of considered stations). in total, 510 [cit] . to compare the ozone sonde data with the model results, the vertical ozone profiles simulated by the model were sampled online at every time step of the model at the location where the ozone sonde was launched. drifts of the ozone sonde by winds are not taken into account. for every launched ozone sonde, we averaged the simulated vertical profiles in time over the measurement period (usually some hours). these vertical profiles of simulated ozone mixing ratios are compared to the measurements of the ozone sonde data. as the main focus of this comparison is the free troposphere, we restrict this analysis to all data in the pressure range of 600 to 200 hpa."
"the largest differences in the relative contribution of o tra 3 to ground-level ozone are simulated around the mediterranean area. the differences over the mediterranean sea (2 percentage points or more, corresponding to more than 10 %) can partly be attributed to the coarse resolution of the emissions in emac compared to cm50. the coarse resolution leads to an artificial increase in p o 3 (see sect. 3.1), which in turn leads to an increase in the contribution from o tra 3 (and other anthropogenic categories). accordingly, the results of cm50 from the et42 simulation show regionally up to 3 nmol mol −1 and 3 percentage points larger contributions of land transport emissions to ozone than the results from the ref simulation (see also fig. s7 in the supplement). however, the large differences over southern italy and sicily between cm50 and emac especially cannot be attributed to the coarse resolution of the emissions. here, emac simulates the largest contribution (up to 17 %) in the european region (particularly around the naples region with large land transport emissions), while cm50 simulates contributions of around 13 %. on the coarse emac grid most parts of southern italy are considered as sea, especially affecting the dry deposition calculation in emac, as dry deposition of ozone is lower over sea than over land. therefore, the coarse resolution of the land-sea mask in emac compared to cm50 leads to an artificial underestimation of the ozone dry deposition in emac. in addition, the coarse land-sea mask leads to differences in the calculation of biogenic emissions. in particular, over sicily emac simulates no biogenic emissions (including soil no x ) while cm50 simulates large emissions there (see fig. s17 in the supplement). accordingly, soil no x and anthropogenic no x do not compete in this area in emac, and ozone is mostly formed from anthropogenic emissions. compared to these artificial peaks simulated by emac around naples and over sicily, cm50 shows the largest contribution (up to 15 %) around the po valley. in this region, large amounts of emissions by land transport take place, and ozone production is enhanced by stable and sunny weather conditions. the differences between emac and cm50 around the naples region are even larger (up to 6 percentage points; see fig. s6 in the supplement) for the extreme values (95th percentile) than for the mean values which were discussed so far. accordingly, extreme values are even more strongly deteriorated than the mean values by the coarse land-sea mask problems discussed above."
analysis and graphics for the data used were performed using the ncar command language (version 6.4.0) software developed by ucar ncar cisl tdd and available online at this site: https://doi.org/10.5065/d6wd3xh5 [cit] .
"in the simulation ebio, the biogenic c 5 h 8 and soil no x emissions as calculated by emac are transformed down and applied at the resolution of emac in cm50. by comparing the results from cm50 of the simulations ref and ebio, the effect of the differently simulated biogenic emissions can be analysed. these differences in the biogenic emissions are caused by different meteorological conditions simulated by emac and cm50."
"clearly, this study is only a first step in quantifying the driving sources of uncertainties and, particularly, the role of the model and emission inventory resolutions on the results of ozone contribution studies. especially, as some processes like vertical diffusion or vertical transport can heavily alter the model results, follow-up studies need to take into account more (and different) models to better quantify the uncertainties due to differences in the meteorology simulated by different models. in addition, the two analysed anthropogenic emission inventories clearly do not reflect the whole spectrum of different emission estimates. further, our analyses only focused on differences near the origin of the emissions. an increased resolution leads to a more realistic chemistry within the plumes downwind of the emission hotspots. this can affect the long-range transport from different precursors and might influence regions far away from the emission region. especially, calculations of radiative forcings are very sensitive to ozone near the tropopause. in a coarsely resolved model, the overestimated absolute contributions might lead to a biased radiative forcing. this effect, however, is difficult to quantify and would require very finely resolved global chemistry-climate models or two-way nesting capabilities, which feed back information about the contributions from the fine grid back to the coarse grid. for a next step, a further increase in the model and emission resolution should be envisaged. even if we found only small differences between the 50 and 12 km resolutions this step would be important, as even with a 12 km grid resolution emissions are diluted over large areas. a finer resolution could strongly reduce the dilution . such an analysis, however, is hindered by two aspects. first, consistent emission inventories (anthropogenic and natural) with a resolution of 1 km over areas that are large enough to compare models on a regional and global scale must be available. second, requirements with respect to the computational time of chemistry-climate models with ≈ 1 km resolution over large computational domains are very demanding, hindering the detailed quantification of the differences caused by the resolution over long integration periods."
"so far, the results indicate that with respect to average values on a continental scale the differences caused by the resolutions of the model/emission inventory are rather small. [cit], which reported only a small influence of the global redistribution of megacity emissions (which can be seen as a locally decreased emission resolution) on the global ozone budget."
"-first, our study shows that average continental contributions of anthropogenic emissions are quite robust with respect to the model and the model resolution used. this means that global models at coarse resolution can be used to perform ozone source apportionment in the global context."
"as an example of the generalised tagging method we consider the production of ozone from the reaction of no with an organic peroxy radical (ro 2 ) which yields no 2 and an organic oxy radical (ro),"
"to summarise and quantify these differences in more detail, fig. 11 shows the (a) absolute and (b) relative contributions of o tra 3 to ground-level ozone averaged over the cm50 domain, as well as for the geographical regions defined in the prudence project [cit] . the results of emac are not analysed for these geographical regions because, due to the coarse resolution, some regions would only consist of a few grid points. figure 11 also shows that on the scale of smaller regions the absolute and the relative contribution of o tra 3 to groundlevel ozone is only slightly influenced by the coarse resolution of anthropogenic emission inventories (et42) as well as having a different geographical location and resolution of biogenic emissions (ebio). this holds not only for the mean o tra 3 contributions, but also for the extreme values expressed by the 95th percentile. further, the simulated differences in the biogenic and shipping categories, which are more affected by the differences in the emission inventories in the two simulations, are also rather small (see figs. s13 and s14 compared to the differences between the contributions of o tra 3 in the ref, et42, and ebio, the differences caused by a changed emission inventory (eveu) are larger. in the mediterranean region, the mean and 95th percentile of the contributions of o tra 3 increase by 1 nmol mol −1 and 2 percentage points, respectively. in the alps region, the increase in the mean and 95th percentile of the contributions is up to 1.3 nmol mol −1 and 3 percentage points, respectively. similarly, for the contribution of shipping emissions the differences are largest with the changed emission inventory (up to 1.5 nmol mol −1 and 1 percentage point). accordingly, changes in the resolution of the emission inventory or the biogenic emissions can affect the contribution from anthropogenic categories (such as land transport and shipping). however, on the regional scale the main drivers of uncertainties are clearly the anthropogenic emissions and differences caused by the model resolution and/or model differences. for example we found regional differences (see sect. 4.1) in the contribution of o tra 3 to ground-level o 3 between emac and cm50 of up to 20 % around the naples region, which in this case can mainly be attributed to the coarse land-sea mask used in emac, leading to land transport emissions to occur over the sea."
"-third, our results clearly indicate how large the spread between models is, with respect to ste. the importance of stratospheric ozone, both in the global and regional model, corroborates the necessity for tracing the contributions of stratospheric ozone to ground-level ozone explicitly by the source apportionment methods. however, only a few currently available methods used on the regional scale account for this process."
"cm12 simulates a lower relative contribution of o tra 3 to ground-level o 3 over germany than cm50 (see fig. 9 ). the difference is largest in southern germany; however it is mostly below 0.5 percentage point (corresponding to less than 5 %). the differences between the mean and 95th percentile (see fig. s12 in the supplement) of the contributions of o tra 3 between cm12 and cm50 are much smaller compared to the differences caused by different anthropogenic emissions inventories (e.g. the differences between the results of the ref and eveu simulation). accordingly, the differences in emission inventories dominate over differences caused by the resolution of emission inventories and models when comparing the results of cm50 and cm12."
"due to the messy infrastructure, the same diagnostics or chemical process descriptions are applied in all of the model instances. following the modular structure of messy, each diagnostic or process description is coded as a so-called submodel. the applied submodels are listed in table 1 . besides the name of the submodel and their reference, a short description provides general information on the process or diagnostic represented by the respective submodel. most importantly, the same kinetic solver (mecca; [cit] ) and same tagging submodel are applied in each instance."
"author contributions. mm performed the simulations, analysed the data, and drafted the manuscript. ak and pj developed the model system.vg developed the tagging method. rs drafted the study. all authors contributed to the interpretation of the results and to the text."
"abstract. anthropogenic and natural emissions influence the tropospheric ozone budget, thereby affecting air quality and climate. to study the influence of different emission sources on the ozone budget, often source apportionment studies with a tagged tracer approach are performed. studies investigating air quality issues usually rely on regional models with a fine spatial resolution, while studies focusing on climaterelated questions often use coarsely resolved global models. it is well known that simulated ozone mixing ratios depend on the resolution of the model and the resolution of the emission inventory. whether the contributions simulated using source apportionment approaches also depend on the model resolution, however, is still unclear. therefore, this study attempts for the first time to analyse the impact of the model, the model resolution, and the emission inventory resolution on simulated ozone contributions using a diagnostic tagging method. the differences in the ozone contributions caused by these factors are compared with differences that arise from the usage of different emission inventories. to do so, we apply the meco(n) (messy-fied echam and cosmo models nested n times) model system which couples online a global chemistry-climate model with a regional chemistry-climate model equipped with a tagging scheme for source apportionment. the results of the global model (at 300 km horizontal resolution) are compared with the results of the regional model at 50 km (europe) and 12 km (germany) resolutions. besides model-specific differences and biases that are discussed in detail, our results have important implications for other modelling studies and modellers applying source apportionment methods. first, contributions from anthropogenic emissions averaged over the continen-"
"the results of the model evaluation, however, are not very helpful in judging which of the two emission inventories are more realistic. although eveu shows a smaller ozone bias compared to ref, caused by reduced precursor emissions, it is unclear if lower anthropogenic non-traffic emissions in the veu compared to mac emission inventories are realistic."
"in general, we conclude that regional differences in the relative and absolute contribution of o soi 3 caused by inter-model differences, emission resolution, and different geographical distribution are up to 15 %. averaged over europe the differences are lower (10 %). again, these differences are lower than for example the differences of approximately 30 % of the observed contributions from the stratosphere between the results of emac and cm50."
"in general, cm50 simulates larger ozone mixing ratios than emac over the continent (see fig. 3 ). this ozone bias in the case of cm50 compared to emac is caused neither by the finer resolution of the emissions nor by the different biogenic emissions compared to emac, because the positive ozone bias for cm50 compared to emac is also apparent in the results of et42 and ebio. only over the mediterranean sea, lower ozone values are simulated by cm50 compared to emac. these lower ozone mixing ratios can be partly attributed to the coarser resolution of the emissions in emac compared to cm50, as the difference is lower in the et42 simulation (fig. 3b) . the simulated ozone mixing ratios in cm50 are up to 7.5 nmol mol −1 larger [cit] in et42 compared to ref. averaged over the area of the mediter-ranean sea the increase in ozone is around 3 nmol mol −1 . the application of the soil no x and biogenic emissions calculated by emac in cm50 (ebio) leads to an increase in the ozone mixing ratios of 1 to 3 nmol mol −1 . the differences are largest over south-eastern europe, the mediterranean sea, and the iberian peninsula (fig. 3c) . overall, the differences in the results of cm50 between ref, ebio, and et42 are small compared to the bias between emac and cm50. in particular, the positive ozone bias over serbia and bulgaria cannot be attributed to different biogenic emissions or the coarser resolution of the emission inventories in emac compared to cm50. figure 4 shows scatter plots comparing observed and simulated ozone monthly mean concentrations at all considered stations of the emep network. the simulated concentrations in all model instances and simulations lie, with one outlier, within a factor of 2 of the measurements. as already discussed, the simulated ozone concentrations at most stations show a positive ozone bias. the simulated ozone concentrations are lower than the measured ozone concentrations only at a few stations . the ozone bias is very similar in all cm50 simulations; ebio and et42 show almost the same bias as ref. only the simulation eveu shows a slightly lower positive ozone bias. accordingly, the change in the anthropogenic emission inventory has a larger impact on the model results than the influence of the emission inventory resolution and the geographical distribution of the biogenic emissions."
"more details about the meco(n) model system are presented in a set of publications including a chemical and meteorological evaluation (kerkweg and jöckel, 2012a, b; [cit] . [cit] . therefore, we only present the most important details of the model set-up. the complete namelist set-up is part of the supplement."
"the values which we have discussed so far, however, are averages on the continental scale. on the regional scale the differences can be much larger. geographical distributions of the differences in the absolute and relative contributions as simulated by emac and cm50 are given in the supplement ( figs. s3 and s4) . exemplarily, we want to focus on the categories land transport, an important anthropogenic emission source, and biogenic emissions. as discussed in sect. 2, the biogenic emissions are calculated online by both model instances and depend on the meteorology and surface properties. while the total emissions are comparable, the geographical distribution and the area-averaged contribution differ (see supplement fig. s17 and tables s2 to s10). as disparity between online-simulated emissions is a typical intermodel difference, a detailed investigation of the influence of these differences is of interest."
"the area-averaged values indicate that the inter-model differences between cm50 and emac, as discussed in detail in sect. 3, have a larger influence on the calculated contributions than the change in the anthropogenic emission inventory. the impact of the coarsely resolved emission inventory on the area-averaged values is rather small. in general, the difference in the average contributions of o tra 3 simulated by the two model instances (emac and cm50), as well as simulated by cm50 for the four different simulations, is ≈ 10 % at maximum. in comparison, the differences in the contributions to ground-level o 3 between emac and cm50 from the lightning and stratosphere categories are much larger, ≈ 20 % and ≈ 30 %, respectively."
"the further increase in resolution from 50 km (cm50) to 12 km (cm12) impacts ozone and the contributions of ozone only slightly (see fig. s11 in the supplement). in general, we note a decrease in the absolute ozone values, as well as the absolute contributions of anthropogenic emissions (including the land transport category) near the hotspot regions (e.g. rhine-ruhr, munich, and frankfurt), if the model resolution is increased (ref simulation) . the increase in the resolution of the emission inventory (eveu simulation) intensifies this effect; i.e. near the hotspots ozone values and absolute contributions of o tra 3 decrease further. in southern and eastern germany, however, the ozone values increase. as a comparison of the contributions of the individual tagging categories shows, this is mainly caused by an increase in the contributions from stratospheric ozone and the ch 4 category. the increase in stratospheric ozone is partly caused by the enhanced topography in cm12 compared to cm50 as well as larger convective up-and downdraft mass fluxes in cm12 compared to cm50. the larger contribution of ozone from the ch 4 category (meaning more ozone formed by reactions involving ch 4 oxidation products) is consistent with the finding of a larger tropospheric oxidation capacity (i.e. lower methane lifetime) in cm12 compared to cm50 [cit] ."
"in a next step, the difference between the ozone production simulated by emac and cm50 is analysed (for the ref simulation) . for this, we consider the net ozone production (p o 3 ), which is calculated as follows:"
"for our comparison we focus on the period june-august (jja) when the ozone production is largest. further, we compare the results on the coarsest grid, to analyse if the finer resolution leads to any added value compared to the coarse resolution."
"regionally, the differences in the relative contribution of o tra 3 to ground-level ozone (see fig. 8 ) can be larger than the area-averaged differences. in general, both model instances simulate a comparable distribution, with the largest relative contribution of o tra 3 in the mediterranean region and contributions of around 8 % over the western atlantic. these values are larger (10 %-18 %) over the continent than over the sea. cm50 simulates a 0.5-1 percentage point lower relative contribution compared to emac. as discussed before, this is partly caused by stronger vertical mixing and reduced ozone production (p o 3 ) in cm50 compared to emac. with increasing altitude the differences between emac and cm50 decrease (see fig. s5 in the supplement)."
"(2) p r1 is the production rate of o 3 by reaction (r1). no y and nmhc are the mixing ratios of the corresponding tagged families, while species marked with \"tag\" represent quantities tagged for a specific category (e.g. stratosphere or land transport). the denominator represents the sum of the mixing ratios over all categories of the respective tagged family/species. accordingly, the tagging scheme takes into account the specific reaction rates from the full chemistry scheme. further, the fractional apportionment is inherent to the applied tagging method, as due to the combinatorial ansatz every regarded chemical reaction is decomposed into all possible combinations of reacting tagged species."
(2) the resolutions of these models differ; and (3) emac and cosmo-clm/messy calculate different soil no x and biogenic c 5 h 8 emissions. the last item is due to the dependencies on meteorology and different soil types in emac and cosmo-clm/messy.
"-second, our results also show that on the regional scale, the differences caused either by different models or by model resolution can be larger. these effects arise mainly near hotspot regions like the po valley or near major shipping routes in the mediterranean sea. however, especially in these areas, contribution analyses of anthropogenic emissions are very important, and spurious effects, such as artificially increased ozone levels and contributions caused by the coarse resolution of models and/or emission inventories should be avoided. hence, for regional analyses finely resolved models and emission inventories are required."
"this paper is organised as follows. first, sect. 2 gives an overview of the model system and discusses the investigation strategy and the performed simulations. in sect. 3 we present a brief evaluation of the model results compared against ground-level and ozone sonde observations as well as a comparison of the ozone production rates simulated by emac and cosmo-clm/messy (sect. 3.1). in sect. 4 the differences of the ozone contributions caused by differences of model and emission inventory resolutions are analysed in detail. we provide a more detailed quantification of the differences in specific regions and a further discussion in sect. 5. depicted is the topography of the continents (in metres) at the resolution of the corresponding model instance. outside the cm50 domain the topography of emac is displayed. shown is the entire computational domain including the relaxation area. the dashed red square indicates the region analysed in sect. 4. [cit] . are applied, leading to highly consistent chemical boundary conditions. therefore, there is no need for lumping (i.e. treating different chemical species with similar chemical formulas as one species), scaling of boundary conditions for specific chemical species, or taking boundary conditions from different models."
"apart from the many model specific findings of this study, its results have important implications for other modelling studies and modellers who are applying source apportionment methods. these implications are as follows:"
"with respect to the geographical distribution (fig. 10 the differences between emac and cm50 are only partly caused by the different geographical distribution of the biogenic emissions in emac compared to cm50. when applying the biogenic emissions as calculated by emac in cm50 (ebio simulation), the relative and absolute contributions of o soi 3 increase mainly in the mediterranean area, by up to 2 percentage points and 3 nmol mol −1, respectively (see figs. s9 and s10 in the supplement). the characteristic dipole pattern, with lower contributions of o soi 3 in southeastern europe and higher contributions in southern europe and northern africa, in cm50 compared to emac is similar. this pattern can partly be attributed to the coarse resolution of the shipping emissions in emac, leading to a positive ozone bias in the mediterranean sea (see sect. 3). the dipole pattern, however, is caused neither by the coarse resolution of the emissions nor by the different biogenic emissions, but rather mainly by the differences between the meteorology simulated by emac and cm50."
"code and data availability. the modular earth submodel system (messy) is continuously being further developed and applied by a consortium of institutions. the usage of messy and access to the source code is licensed to all affiliates of institutions which are members of the messy consortium. institutions can become a member of the messy consortium by signing the messy memorandum of understanding. more information, including on how to become a licensee for the required third-party software, can be found on the messy consortium website (http://www. messy-interface.org, last access: 20 [cit] ). the code presented here was based on messy version 2.50 and is available in the official release (version 2.51). the namelist set-up used for the simulations is part of the supplement. the data used for the figs. 6 to 11 are part of the supplement."
"in this section, a comparative analysis of binned and svrbased pitch curves concludes that the support vector regression fits the data well between a cut in and rated wind speeds, but that the binned approach to fitting works well across the entire range of wind speed, as shown in fig. 9 . however, the binned pitch curve obtained from extensive measured data requires an extended measurement period to limit the uncertainty associated with the calculated pitch curve and is far too slow to be used directly for condition monitoring where any changes in operation need to be identified quickly. support vector regression is not limited in this way and hence can detect anomalies quickly. thus, the support vector regression is preferred for detecting damage at an early stage."
coregistration of the patient's mri with the meg sensor space is typically performed through the digitization information of the head and head position indicator that generates artificial electric currents in the sensor array. this process estimates the relationship between the source and the sensor spaces (figure 3) .
"the power curve is widely used to assess the performance of a wind turbine; it signifies the nonlinear relationship between power production and hub height wind speed [cit] . iec-61400-12-2 [cit] prescribes the method known as 'binning' to calculate the power curve. the \"method of bins\" is a data reduction technique used to normalize the data to construct the measured power curve. this binned power curve includes the effect of the site turbulence and all other effects reflecting onsite conditions [cit] ."
"the cross-validation of five folds is used to find the best value for kernel scale and to prevent overfitting [cit] . the scada datasets described in \"scada data for wind turbine performance curves\" were randomly shuffled and split into training and testing datasets for training and svr model validations purposes, respectively."
"(2) mne solution does not consider the anatomical connection through white matter tracts. tractography using diffusion tensor imaging will be useful in combination with mne maps . (3) there is no established way to determine the threshold objectively, although such a threshold greatly affects the appearance of source maps. several studies have introduced thresholds which are determined by a quantitative procedure [cit],b) . further investigation will be necessary to understand how the threshold should be determined."
"single equivalent current dipole (ecd) analysis has been widely used for source localization of epileptic spikes for decades [cit] . this model assumes that a single dipole source generates all the neuromagnetic fields recorded on the sensors, and is considered physiologically plausible when a limited area of the cortex is synchronously activated. in the analysis, the measured magnetic fields at a given latency are modeled by the best-fitting single dipole. ecds are typically calculated by using a standard iterative least-square algorithm [cit], and several indicators of their reliability are also calculated, such as goodness of fit (gof) and correlation coefficient. these indicators reflect the concordance between the magnetic fields calculated from the ecd and the actual measurement meg data. the current dipole moment is represented by the magnitude of the ecd. these indicators and other metrics are used for selecting adequate sources and discarding inadequate ecds by setting a threshold. adequate ecds are mapped on the patient's mri, demonstrating the distribution of ecds (figure 1) . previous studies have validated ecd analysis in temporal lobe epilepsy [cit] and frontal lobe epilepsy [cit] . several studies have shown spatial concordance of ecd distribution and interictal spiking area on intracranial eeg (ieeg) [cit] ."
"an important distinction between ecd and mne is that distributed source maps generally show the source localization of one single spike whereas single dipole maps project many dipoles obtained from different spikes. thus, the single dipole method provides a viewpoint of a spike population, such as \"clustered\" or \"scattered.\" distributed source maps do not have such mapping procedures that are widely used. therefore, combined use of single dipole maps and distributed source maps is necessary in the current settings of spike analysis. development of new mapping techniques, which overview numerous distributed source maps, will be useful for analyzing many spike populations."
"calculating intracranial sources from meg obtained outside the brain, an example of the inverse problem, is mathematically non-unique and ill-posed. certain assumptions are necessary for providing the proper source modeling. thus, many procedures of source analysis have been proposed, such as single dipole, multidipole, and distributed source models, which are also applied for the source analysis of scalp electroencephalography (eeg) [cit] ."
"in this article, we review the source analysis methods, describe the techniques of the distributed source analysis, and discuss the feasibility of this method in evaluation of epileptic spikes."
"this paper has proposed an svm-based regression model for estimating the blade pitch angle curve. the estimated svr pitch curve follows the standard variations, though due to the lack of data points in above rated wind speed, its accuracy suffers. this highlights how the quality and quantity of data points significantly affects the svr model prediction accuracy. svr is then compared with the conventional approach based on a binned pitch curve together with individual bin probability distributions to identify operational anomalies. this comparative study yielded significant results. the svr blade pitch curve closely follows the binned pitch curve, but above rated wind speed, there are fewer scada data values available and, as a result, the svr curve is less well determined with some mismatch with the binned pitch curve. the major issue associated with wind turbine condition monitoring is to detect a fault or failure as soon as possible and with limited computational time and processing power so that catastrophic damage due to failure can be prevented with a cost-effective approach. the comparative analysis illustrates the strengths and weaknesses of these techniques in context to anomaly detection and model uncertainty. this should support a wind farm operator in selecting the best method for wind turbine condition monitoring. the future work is to develop and appropriate uncertainty analysis for the svr blade pitch curve and then use it for developing a practical fault detection svr algorithm."
"supervisory control and data acquisition (scada) systems record the operational status of wind turbines and are essential for reliable performance optimization. performance monitoring based on available scada data is also a cost-effective approach to turbine condition appraisal, as confirmed by various literature reviews that highlight the feasibility of identifying turbine health status using scada data, and the high potential of further enhancing the health monitoring function through sophisticated data analysis."
"one of the possible benefits is to obtain accurate spike localization. distributed source analysis likely provides more reasonable solution than a single dipole model, although there is a localization error still observed in various distributed models [cit] . previous studies have shown that mne (and its derivative, dspm) provides more accurate source localization than single dipoles, by comparing with single photon emission tomography [cit] ), surgical outcome [cit] ), and ieeg [cit] ) in a small group of patients. by using the distributed source analysis, meg may contribute to the presurgical evaluation of epilepsy more effectively."
"minimum norm estimate constrains source activities to the cortical surface images. the cortical surface is reconstructed from anatomical t1 mri data, and the reconstruction is the first step of mri processing. [cit] ) and brainstorm [cit], provide cortical surface reconstructions. the source space is created by using the cortical surface, deploying grid spacing with numerous cortical patches [cit] . unit current dipoles are distributed in the source space, and the boundary elemental method (bem) creates a head model for calculating the activation of these dipoles (hämäläinen [cit] . a single-layer bem model is generally used, since the neuromagnetic signals are not affected by the tissue conductivity (hämäläinen [cit] ) ."
"a typical blade pitch curve described the nonlinear relationship between turbine pitch angle and wind speed and shown in fig. 2 . below the rated wind speed, the blade pitch angle is set to maximize power production. beyond"
"recently, the wind turbine condition monitoring studies have mostly focused on the power curve for evaluating performance. however, this cannot reflect the complete turbine operation since the operational behavior of the wind turbines is profoundly influenced by a parameter such as a rotor power, torque, and pitch angle. valid assessments of these parameters improve the power performance of a wind turbine. in this study, the blade pitch angle impact on wind turbine performance is analyzed using the blade pitch curve that reveals the nonlinear relationship between pitch angle and the hub height wind speed that can be useful for analyzing wind turbine performance and the detection of faults."
the appropriate kernel makes svr algorithm faster and involves computations in higher dimensional space. the gaussian kernel is used in this study and mathematically is expressed as:
"where v i is the normalized and averaged wind speed in bin i, v n,i,j is the normalized wind speed of data sets j in bin i, b i is the normalized and averaged pitch angle in bin i, b n,i,j is the normalized pitch angle of data set j in bin i, and n i is the number of 10 min average data sets in bin i. figure 8 shows the reference binned blade pitch curve together with error bars. type b uncertainties would be difficult to treat in a consistent manner without greater knowledge of the instrumentation used. therefore, in this paper, we used the statistical spread evident in the binned data. the two standard deviations (i.e., 95% confidence intervals) of"
"magnetoencephalography (meg) is an important, non-invasive diagnostic tool, which acquires neuromagnetic fields generated in the brain with high spatial and temporal resolution. clinical usefulness of meg, especially in presurgical evaluation of epilepsy, is well documented in recent reviews [cit] . currently, clinical applications of meg are divided into two categories: (1) spontaneous brain activity analysis, including epileptic spike mapping, most often for determining an irritative zone, (2) mapping of eloquent cortex, such as primary motor cortex and language area for avoiding postsurgical functional deficits [cit] ."
"based on the svr theory outlined above, a blade pitch curve model is proposed using a gaussian kernel function and then compared with the measured blade pitch curve and is shown in fig. 6 . figure 6 suggest that the support vector regression can fit the wind turbine pitch curve smoothly, however at higher wind speed, its accuracy suffers because of unavailability of sufficient data points. furthermore, the accuracy of a support vector regression model depends on the quantity and quality of the data as well as the appropriate"
"thus, the brain and the sensor array are co-registered (f), and magnetic fields on the sensor space provides activation maps on the source space (g) that consists of the cortical surface (h)."
"another area under active investigation is the surgical implications of distributed source maps. several studies have demonstrated that meg affects the planning ieeg placement and interpretation [cit], and correlates with surgical outcomes [cit] by using a single dipole model. on the other hand, a recent study has shown that these propagation patterns are highly correlated with surgical outcomes in patients with temporal lobe epilepsy by using mne [cit] b) . comparison of these techniques regarding with surgical outcomes is now becoming better understood."
"the iec 61400-12 standard describes the data reduction technique known as binning, typically using 0.5 m/s wide wind speed intervals. the power curve is a smooth curve drawn through these points, but in actuality is only defined precisely at the points themselves. in this study, binning methods are applied to calculate blade pitch curve using the following equations:"
"spatiotemporal distributed source analysis is highly useful for understanding epileptic spikes. it provides more accurate source localization than a single dipole model in some situations, and may be informative in presurgical evaluation of epilepsy. however, further observations are necessary for establishing its usefulness in clinical practice."
"wind farms equipped with supervisory control and data acquisition (scada) systems provide data essential for reliable performance optimization [cit] . performance monitoring based on the available scada data is also a cost-effective approach to turbine condition appraisal, as confirmed by various literature reviews [cit] that highlight the feasibility of identifying turbine health status using scada data, and the vast potential of further enhancing the health monitoring function through sophisticated data analysis. scada-based monitoring of the condition of internal components of a wind turbine can be used to optimize maintenance activities and thus reduce o&m costs and increase reliability and production time; see [cit] ."
where α n and α n * are the nonnegative multipliers for each observation x n . the obtained biased value for this study is 1.70 which is added into the svr model to predict the blade pitch curve of wind turbine accurately.
"single dipole method has been potentially investigated and its benefits and limitations are well understood, and now several laboratories are doing similar studies with mne and other distributed source solutions. recent studies have validated mne source distribution with ieeg. [cit] demonstrated that mne analysis of frontotemporal meg spikes accurately represents spike propagation as observed in ieeg. frontoparietal and temporoparietal propagation patterns are also consistent between meg and ieeg in a series of cases [cit] . such validation will be highly desirable in other propagation involving various regions and in patients with various types of epilepsy."
"comparison between measured and svm fitted blade pitch curve fitting method used. furthermore, the estimated blade pitch angle as a function of time plotted and compared with the observed pitch angle, as in fig. 7, suggests that svr can estimate the pitch angle efficiently. blade pitch failure may lead wind turbines to underperform, and an svr model can help identify such failures through uncertainty analysis."
"the forward solution, which models the magnetic signals generated by unit current dipoles, is obtained by using the coregistration (hämäläinen [cit] . inverse solution is calculated based on the forward solution, mapping the strength of each unit dipole [cit] . source activation is projected on the cortical surface by applying a certain threshold, showing the source strength with different colors (figure 4) . the strength and extent of activation change along with the time course (figure 4) ."
"this paper proposed a novel support vector regression (svr) approach to estimate wind turbine blade pitch curve and its application in anomaly detection for condition monitoring. the binning method is a benchmark data reduction approach for the wind industries, but its application is generally limited to the power curve. in this study, the binning method is applied to calculate the blade pitch curve. finally, a comparative analysis of the binned blade pitch curve and support vector regression blade pitch curve is undertaken regarding fitting uncertainty and identifies the advantages and disadvantages of the svr model. this paper is structured as follows: the introduction is the first section. the next section describes the wind turbine performance curves and air density corrections. the following section describes the scada dataset and its pre-processing. the next section outlines the methodologies and this section is further divided into subsections explaining support vector regression (svr) and the binning approach to wind turbine blade pitch curve modeling. the next section presents the comparative analysis of proposed models and the last section concludes the paper."
"following the iec standard (61400-12-1) [cit], air density correction should be applied to a pitch-regulated wind turbine in which a corrected wind speed v c is calculated using eqs. (2) and (3) as shown below and where v c and v m are the corrected and measured wind speed in m/s and the corrected air density is calculated by eq. (2) where b is atmospheric pressure in mbar and t the temperature in kelvin for which 10 min average values obtained from scada data are used. the corrected wind speed (v c ) from eq. (3) is then used to calculate the power curve normally by binning."
"source localization of meg spikes is frequently performed in clinical practice for identifying an irritative zone [cit], providing spatial information of spike activities at the sensor level. source analysis of meg typically incorporates anatomical information derived from each individual's magnetic resonance imaging (mri), and calculates the sources of neural activities by applying a certain mathematical model to the measured magnetic fields. these cortical and subcortical sources are visualized on the mri or mri-based anatomical atlas, and provide current dipole distribution maps."
"nonparametric models are data driven, and their structure is not specified a priori but is obtained exclusively from the data [cit] . commonly used nonparametric models in wind turbine condition monitoring are support vector machine (svm), copulas, gaussian process (gp) and other data derived models; see [cit] . scada data record a large number of measurements which make nonparametric models appropriate."
"the two approaches, namely binning and support vector regression used to build effective blade pitch angle curves for wind turbine condition monitoring, are described as follows."
"measured power values are used to calculate the error bars which are used to measure the uncertainty associated with each bin of the blade pitch curve. however, binning is not necessarily the most effective way to generate a pitch angle curve from wind speed and pitch angle data, since there is compromise being made for accuracy while choosing a bin width of 0.5 m/sec. within each bin, the measured power will depend strongly and non-linearly on wind speed, and a wide bin would result in a systematic bias; on the other hand, in practice, there must be a sufficient number of data points in each bin to be statistical significance [cit] . the comparative analysis of binning and svr models is described in the upcoming sections where the advantages and disadvantages of the individual methods are outlined."
"in this chapter, lssa algorithm is applied to optimize the dce function of glcm algorithm. in order to better verify the image segmentation ability of glcm-lssa algorithm, it is compared with the optimized glcm algorithm of woa, pso, fpa and ba. the color image has three color channels. in this paper, the images of the three channels are segmented, and then the three result images are fused to obtain the final segmentation result graph. firstly, the segmentation effect and precision of glcm-lssa algorithm are analyzed when the threshold value is increased. then the segmentation ability, statistical analysis and stability analysis of the proposed lssa algorithm and other optimization algorithms in glcm image segmentation are analyzed. finally, the berkeley image library is tested and analyzed. all parameters of the comparison optimization algorithm are shown in table 1."
"in this experiment, for further showing the merits of glcm-lssa method, comparison is performed with other classical image segmentation algorithms, such as random walk (rw) [cit], a level set approach to image segmentation (lsa) [cit] and multi-scale convolutional neural network (mnn) [cit] . each image segmentation method has also been evaluated using a well-known benchmark-the berkley segmentation data set (bsds300) with 300 distinct images. table 8 shows the average results of pri, bde, gce, voi and cpu time of ground truth results of the bsds300 data set."
"the results displayed in table 8, that the proposed technique outperforms all other compared algorithms. the glcm-lssa technique has obtained results close to the ground truth images. it can be seen from the table that the numerical value of glcm-lssa algorithm is the best, indicating that its segmentation result is the closest to groundtruth and the segmentation effect is the best. and the lsa and mnn algorithm segmentation effect is the most check. it can be seen from the cpu time of each image segmentation algorithm that the time of glcm-lssa algorithm is the shortest, indicating that the algorithm has good robustness. it can be seen from the analysis above, glcm-lssa algorithm can effectively solve the problem of image segmentation and the cpu time is short."
"in order to search m optimal threshold values [t 1, t 2, · · ·, t m ] for a given image, we try to maximize the objective function:"
"kapur's entropy method finds the optimal thresholding values by maximizing the entropy of each distinctive class or the sum of entropies based on information theory. since it has superior performance, kapur's entropy method have drawn the attentions of many researchers and been widely used for image segmentation problem [cit] . let there is n pixels and l gray levels in a given image, then the probability of each gray level i is the relative occurrence frequency of the gray level i, normalized by the total number of gray levels eq.9:"
"where, m, n is the size of the image, i is the original image, andî is the segmented image. the feature similarity (fsim) is used to estimate the structural similarity of the original image and the segmented image [cit] . we define fsim as:"
"the evaluation of image segmentation result graph is very important, so this paper selected psnr and fsim as the evaluation index of test image. the parameter of the peak signal to noise ratio (psnr) is used to compute the peak signal to noise ratio between the original image and the segmented image [cit] . the psnr index is calculated as:"
"in this paper, chapter 2 describes the mathematical model and principle of each basic algorithm. chapter 3 proposes the improved glcm-lssa, which is improved on ssa by lf. the lssa algorithm optimized the novel diagonal class entropy(dce) function of glcm. in chapter 4, standard function is carried out on the improved lssa algorithm, and the optimization ability of the lssa algorithm is analyzed through the experimental results. in chapter 5, the glcm-lssa algorithm is used to segment natural color images, satellite images and berkeley images. in order to verify the algorithm is of excellent performance in image segmentation, psnr, fsim, pri, voi, gce, bde and cpu time are used."
"where represents the entire image, and s l (x) indicates the similarity between the segmented images obtained through multilevel thresholding task and input image. the fsim parameter of color rgb image is defined as:"
"as a scope of further research, we will continue to study and improve the optimization ability of the lssa algorithm to solve more complex optimization problems. meanwhile, we will apply the glcm-lssa algorithm to solve more complex image segmentation problems, such as medical image segmentation and plant phenotype image segmentation."
"based on the natural optimization algorithm, the results of each run are not the same. therefore, in order to analyze the stability of the proposed algorithm based on glcm-lssa, we use the value of standard deviation (std). the std can be intuitive to the operation stability of the algorithm, and the lower the value of the algorithm, the stronger the robustness of the algorithm. table 5 shows the std values of each algorithm after 30 runs. it can be seen from the table that the stability of lssa algorithm is the strongest, especially when dealing with the segmentation of satellite images, its stability is obviously better than other comparison algorithms, indicating that glcm-lssa algorithm has a good segmentation ability, and can find the optimal threshold of image better, more accurately and more stable."
"minimum value of this function by lssa, so as to find the optimal multi-threshold value of image. the image with high segmentation precision can be obtained by the optimal multithreshold segmentation. the flowchart of the glcm-lssa can be seen from fig.3 . the pseudo code of the glcm-lssa algorithm is given below:"
"glcm is a second-order statistical method that computes the frequency of pixel pairs having same gray-levels in an image and applies additional knowledge obtained using spatial pixel relations [cit] . co-occurrence matrix embeds distribution of gray-scale transitions using edge information. since, most of the information required for computing threshold values are embedded in glcm, it emerges as a simple yet effective technique."
"from fig 5-12, the visual results show that this method achieves a good segmentation effect by accurately identifying the complex target and background in each level of satellite image segmentation. the image segmentation effect in fig. 5(b, c, g ) and fig. 6(h, r) is poor, and the contour segmentation in satellite images is not clear. as the number of thresholds increases, the image segmentation quality can be enhanced from fig. 5 and fig 6. the lssa algorithm in this paper has the best segmentation effect. it can be seen from fig9-fig.12, lssa algorithm for natural color image segmentation effect is best, woa and ba algorithm is essentially the same as a result, pso algorithm segmentation results figure effect is the worst, under segmentation phenomenon exists, the target area segmentation effect is not obvious, and the existence chromatism, the best threshold segmentation results are local optimal phenomenon. obviously, from fig. 13 and 14, the fsim value and psnr value of glcm-lssa algorithm are better than other algorithms."
"the results displayed in pri indicate better segmentation performance. while lower values of bde, gce, and voi show better segmentation. it can be seen from the table that the numerical value of glcm-lssa algorithm is the best, indicating that its segmentation result is the closest to groundtruth and the segmentation effect is the best. and the pso and fpa algorithm segmentation effect is the most check. it can be seen from the table that the value of the multi-threshold kapur algorithm optimized by lssa algorithm is superior to other optimization algorithms, indicating that the optimization ability of lssa algorithm is relatively excellent and it can effectively improve the segmentation accuracy of image segmentation algorithm. according to the comparison between the results of glcm-lssa and kapur-lssa algorithms, the segmentation accuracy of glcm-lssa algorithm is higher and its stability is better than other comparison algorithms. therefore, glcm-lssa algorithm can effectively solve the problem of image segmentation."
"the general framework of ssa as follows: levy's flight was firstly proposed by levy and then described in detail by benoit mandelbrot. in fact, levy flight is a random step that describes the levy distribution [cit] . numerous studies have shown that the behavior of many animals and insects are a classic feature of levy's flight. levy flight is a"
"the ssa can solve the problem of low dimensional single mode optimization with simple and efficient solution. however, when dealing with high dimensional and complex image processing problems, traditional ssa is not very satisfactory. in order to improve the global search capability of ssa, an improved optimization algorithm of ssa is proposed in this paper. levy flight can maximize the diversity of search domains, so that the algorithm can efficiently search the location of food sources and achieve local optimization. the levy flight can help ssa get better optimization results, therefore to salp leader position update formula optimization, can be used to express the following mathematical formula:"
glcm for computing the dce for each level of thresholding. the optimum thresholds are obtained when dce is minimized. we introduce here the theoretical formulation for multilevel thresholding using dce.
"special random step method, as shown in fig.2, which is a simulation of the flight path. its step length is always small, but occasionally it will also appear large pulsation."
"c. salp swarm algorithm salps belong to the family of salpidae and have transparent barrel-shaped body. their tissues are highly similar to jelly fishes [cit] . they also move very similar to jelly fish, in which the water is pumped through body as propulsion to move forward. in deep oceans, salps often form a swarm called salp chain. this chain is illustrated in fig.1 . the main reason of this behavior is not very clear yet, but some researchers believe that this is done for achieving better locomotion using rapid coordinated changes and foraging. to mathematically model the salp chains, the population is firstly divided into two groups: leader and followers. the leader is the salp at the front of chain, whereas the rest of salps are considered as followers. as the name of these salps implies, the leader guides swarm and the followers follow each other."
"in this section, we compare the multi-threshold glcm algorithm with the multi-threshold kapur algorithm, and respectively apply different optimization algorithms to optimize the two multi-threshold methods. each multilevel image thresholding method has also been evaluated using a well-known benchmark-the berkley segmentation data set (bsds300) with 300 distinct images. the 300 images from the berkeley segmentation data set (bsds 300) available at https://www2. eecs.berkeley.edu/research/projects/cs/vision/grouping/ segbench/bsds300/html/dataset/images.html. this section uses an extensive comparative study on berkeley database by using performance metrics like probability rand index (pri), variation of information (voi), global consistency error (gce), and boundary displacement error (bde) [cit] . table 7 shows the average results of pri, bde, gce and voi of ground truth results of the bsds300 data set."
"we statistically analyze the experimental results to better observe the differences between algorithms. we use wilcoxon rank sum test [cit], a nonparametric statistical test that checks whether one of two independent samples is larger than the other. we calculate the p-value of fsim of lssa algorithm and woa, fpa, pso and ba algorithm. the experimental statistical results are shown in table 6. it can be seen from the table6 that the glcm-lssa algorithm is obviously better than the comparison algorithm in the statistical sense."
"where x 1 j shows the position of the first salp (leader) in the jth dimension, f j is the position of the food source in the jth dimension, ub j indicates the upper bound of jth dimension, lb j indicates the lower bound of jth dimension, c 1, c 2 and c 3 are random numbers. eq.15 shows that the leader only updates its position with respect to the food source. the coefficient c 1 is the most important parameter in ssa because it balances exploration and exploitation defined as follows:"
"levy flight can significantly improve the ssa's global search ability to avoid getting into local optimal values. this method not only improves the search intensity of ssa, but also improves the diversity of the algorithm. the optimization algorithm ensures that the algorithm can find the optimal value and avoid getting into local optimum, and the algorithm has better global searching ability by increasing the diversity."
"in this section, the glcm-lssa is described in detail. the improved lssa algorithm has simple structure and strong optimization ability. therefore, the lssa algorithm is applied to optimize the threshold selection of multi-threshold glcm algorithm. in the glcm-lssa, as the fitness function of lssa, dce value of glcm is used to find the"
"this paper proposes an improved salp swarm algorithm to optimize multi-threshold glcm image segmentation method. we use levy flight to improve the ssa algorithm, the method can balance the exploration and exploitation. the lssa algorithm is used to optimize glcm multi-threshold image segmentation method. in order to verify the proposed algorithm is of excellent performance in image segmentation, psnr, fsim, pri, voi, gce, bde and cpu time methods are used. through the experiment and analysis of color natural image, satellite image and berkeley image, the experiment proves that glcm-lssa algorithm has better image segmentation effect. and then, we compare glcm-lssa with kapur-lssa algorithm and classic image segmentation. the experimental results show that glcm-lssa algorithm can obtain better segmentation results and robustness are better. therefore, glcm-lssa algorithm has good image segmentation ability and can better handle complex image segmentation tasks."
"the position of salps is defined in dimensional search space where n is the number of variables of a given problem. therefore, the position of all salps are stored in a twodimensional matrix called x. it is also assumed that there is a food source called f in the search space as the swarm's target."
