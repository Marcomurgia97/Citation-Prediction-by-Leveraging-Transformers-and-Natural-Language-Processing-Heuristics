text
"current cell factory construction mainly focuses on enhancing precursor supply, enzyme engineering, and flux balancing. however, the cofactors that are essential for enzyme activity have always been ignored. in particular, many catalytic enzymes of natural product biosynthesis require the specific cofactors, but heterologous hosts such yeasts have insufficient cofactor supply. although some strategies have been developed for cofactor engineering, we still know very little about the regulation mechanism of cellular cofactor equilibrium. on the other hand, the variation of cofactor biosynthesis and regulation among different organisms poses a huge challenge for the cofactor engineering in cell factories. thus, it is essential to illuminate the cellular cofactor biosynthesis, distribution and transportation in yeast for cofactor regulation. artificial intelligence (ai) provides an opportunity for speeding up dbtl cycles by computer automatic design and optimization. the computer-aided design (cad) systems have greatly improved the design quality and documentation communications, and created shareable databases for manufacturing. recently, deep neural networks were trained from published reactions in organic chemistry, which enabled the discovery of retrosynthetic routes and resulted in faster and more selective synthesis than traditional methods [cit] . some attempts have also been made in synthetic biology. an automated dbtl pipeline succeeded in improving the production of flavonoid (2s)-pinocembrin in e. coli [cit] . another computational platform clustercad formalized a paradigm for the design of pks (type i modular polyketide synthase) chimeras and streamlined the process of designing experiments to test strategies for engineering pks variants [cit] . however, the complexity and nonlinearity of biological systems makes it challenging in precise modeling and operation and thus needs novel and robust computational tools. the current protein design platforms rely on the homology of existing proteins at different resolutions and accurately predicting the structure of a novel protein is still challenging so far [cit], not to mention the design of complex biological systems. building an interoperable and reliable standard database is a prerequisite in biological engineering, which will be beneficial for rapid integration of data, continuous self-renewing of biological dbtl cycles, and automated design of metabolic pathways and molecules."
"step 1. generate initial seed for hrf we start by generating an initial seed for the hrf. the hrf represents the timecourse of the bold response and is assumed to be the same, up to a scale factor, for each condition. to generate the initial seed, we take a canonical hrf representing the response to a brief (0.1 s) stimulus and convolve this hrf with the appropriate square-wave function to predict the response for the condition duration used in the experiment."
"standard glm (e.g., datasets 1, 16). this highlights the important point that more nuisance regressors is not always better: using too many nuisance regressors can result in overfitting and poor task estimates."
"biosensor is an ideal strategy for high-throughput screening of desired homologous proteins or mutants [cit], which requires a certain correlation between biosensor and the enzyme activity with an easily observed signal, such as color, fluorescence, or growth state ( figure 3c )."
"as an alternative to estimating the hrf, glmdenoise can also accept a fixed, pre-determined hrf as an input. an implicit assumption of glmdenoise is that a single hrf can describe the responses of all voxels in a dataset. however, the hrf may vary in shape across brain regions [cit] . it is possible to adapt glmdenoise to account for hrf variations, but this is outside of the scope of the present paper."
"we designed a denoising procedure based on fsl's melodic (fsl 5.0, http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/melodic), a utility that implements independent components analysis (ica) of fmri data [cit] . for our purposes, it is essential that an ica-based denoising procedure automatically determines which components are signal (taskrelated) and which are noise (task-unrelated). this is not only to avoid subjectivity and to promote reproducibility, but also for practical reasons, as there are many runs and datasets in the dnb. moreover, given the event-related designs used in the datasets, it may be quite difficult to tell by inspection whether a given component represents signal or noise. the ica-based denoising procedure is as follows. (1) we detrend the data by projecting out polynomials from each run (the maximum polynomial degree is the same as used in glmdenoise). however, the mean of each run is preserved. (2) for each run, we run melodic, which produces a set of component timecourses. (3) for each timecourse, we calculate the amount of variance explained by the task-related regressors corresponding to that run. for comparison, we repeat this calculation for 1,000 randomly shuffled versions of the timecourse. (4) if the amount of variance explained in the actual timecourse is more than c standard deviations away from the mean of the shuffled results (in the positive direction), the timecourse is marked as signal. otherwise, the timecourse is marked as noise. (5) after processing all component timecourses, we use fsl's fsl_regfilt utility to remove the identified noise components from the data. (6) the denoised data are then analyzed using standard glm."
"a few notes regarding the setup of the glmdenoise model. first, data from multiple runs are analyzed in ensemble. thus, a single beta weight is obtained for a condition that occurs across multiple runs. second, each run obtains its own set of noise regressors. third, the same hrf and the same set of polynomial and noise regressors are used for all voxels within a given dataset. each voxel, however, obtains its own set of beta weights and its own weights on the polynomial and noise regressors. finally, due to the use of cross-validation across runs (see section quantification of accuracy), glmdenoise requires at least two runs of data and assumes that conditions are repeated across runs. if only one run of data is collected, the run must be split into multiple segments before using glmdenoise. we note, however, that there may be practical limitations to this splitting strategy. for example, because a separate set of noise regressors is constructed for each run, if the number of volumes in each run becomes too small, the inclusion of noise regressors may quickly result in overfitting. the code implementing glmdenoise (see section code) provides diagnostic figures that make it easy to assess the efficacy of glmdenoise on any particular data preparation. procedure. the procedure consists of selecting voxels that are unrelated to the experiment (cross-validated r 2 less than 0%), performing principal components analysis (pca) on the time-series of these voxels to derive noise regressors, and using cross-validation to determine the number of regressors to enter into the model."
"network prediction. the high confidence dataset referred to as \"golden set\" of npinter v3.0 28 is downloaded from npinter website (http://www.bioinfo.org/npinter/) for the network analysis. this set of npinter v3.0 contains all known functional ncrna interactions, verified and curated from literature. we extracted only those entries having \"ncrna-protein binding\" and wherever sequence information is available. the resulting entries were then segregated according to the source organism. the corresponding protein and rna sequence data is downloaded from \"the blast database in npinter v3.0\" section of npinter. all the rpis involving xist and lincrna from mus musculus were used for creating the xist and linc rna networks. telomeric proteins interaction network (telopin) database consists of data on telomeric protein interactions with telomeric repeat-containing rna (terra). these interactions are downloaded from the telopin database 29 (http://songyanglab.sysu.edu.cn/telopin/). all the datasets used in the current study are available at xrpi webpage."
"in s. cerevisiae, erg20p catalyzes consecutive reactions for the biosynthesis of gpp (c10) and fpp (c15) by consecutively condensing ipp and dmapp (c5 units). to improve the monoterpene and diterpene production, erg20p was engineered into a double-mutant gpp synthase (f96w-n127w) [cit] ) and a single-mutant ggpp synthase (f96c or y95a) [cit], through protein engineering, when compared with previously identified crystal model of the avian fpps [cit] . similarly, erg20p was specifically engineered to a non-natural c11 terpene synthase by a single-residue switch, which enabled the biosynthesis of a serious of non-natural c11 terpenes in s. cerevisiae [cit] . with the development of structural biology, more and more crystal structures of catalytic enzyme have been resolved, which laid the foundation for the homology model construction and directed evolution. knowledge-driven or rational protein engineering can provide more efficient enzymes for construction of robust yeast cell factories [cit] ."
"where p j is the rpi interaction propensity of amino acid j, i j is the total number of amino acids of type j at the binding interface and i n is the total number of amino acids that are interacting at the binding interface in all of the rpis in the dataset. n j is the total number of non-interacting amino acids of type j at the binding interface and n n is the total number of non-interacting amino acids at the binding interface in all of the rpis in the dataset. this resulted in formation of four distinct groups (supplementary table 2a ). as expected positively charged amino acids (arginine, lysine, and histidine) showed the highest propensity and are grouped in one class. negatively charged amino acids aspartic acid and glutamic acid have shown very low propensity but aspartic acid has the same propensity as the other non-polar hydrophobic amino acids proline, methionine, phenylalanine, alanine, valine, leucine, and isoleucine. though both are negatively charged, aspartic acid shows higher propensity than glutamic acid due to interactions involving the backbone atoms rather than the side chain and the smaller side chain may provide the extra flexibility. as a result, aspartic acid is grouped along with the hydrophobic amino acids proline, methionine, phenylalanine, alanine, valine, leucine, and isoleucine. cysteine and glutamic acid having the lowest propensity and are grouped into one class. the remaining amino acids, asparagine, tryptophan, serine, glutamine, tyrosine, glycine, and threonine make up the final group. since each class of amino acids share similar physicochemical properties and are mostly conservative substitutions in a protein, the information content is not lost in this reductionist approach. the same analysis for rna nucleotides showed similar propensities for all the four residues and hence a four-mer representation of the nucleotides is used. the modified residues are considered as belonging to their parent nucleotide class (supplementary table 2b )."
candidate denoising methods. any denoising method that conforms to the prescribed application programming interface (api) can be evaluated in the dnb architecture. note that the cross-validation used in the dnb is distinct from any internal resampling scheme that might be used by a denoising method (such as the cross-validation used within glmdenoise).
"snr is a metric whose units are easier to interpret than crossvalidated r 2 values. to better understand the magnitude of the improvements provided by glmdenoise, we quantified snr in beta weight estimates ( figure 3b ). the results demonstrate that glmdenoise substantially improves snr compared to standard glm (figures 4c,d) . we find that the amount of snr improvement is fairly constant across snr levels. for example, a voxel with a base snr of 4 under standard glm typically experiences an increase in snr to 5 under glmdenoise, just as a voxel with a base snr of 8 typically experiences an increase in snr to 9."
"we have constructed 3 different predictive models; one is trained on only ribosomal rpis (rpi2435), another on non-ribosomal rpis (rpi390), and a comprehensive model that is trained on the complete dataset of 2825 rpis (rpi2825) derived from the high-resolution x-ray crystallography structures deposited in the protein data bank. in developing xrpi, we elucidated rpi specific knowledge about the optimum binding interface (msu) and rpi specific amino acid/nucleotide interaction propensities which are then used to represent the rpi to ml algorithms. the rpi specific properties elucidated in the current study will definitely expand the knowledge of rna protein interactions in general. we believe that using domain-specific knowledge (either readily available or needs to be derived as is the case here) greatly enhances the performance of ml-based prediction models. to the best of our knowledge, xrpi is the first of its kind sequence alone based prediction model that uses domain-specific knowledge. the use of xgboost algorithm that has the ability to control overfitting and over-specialization also enabled superior performance of xrpi. this is particularly important in the context of biological sequence data where models perform well during the cross-validation step but fail to generalize to external testing datasets. comparative analysis of xrpi with the existing rpi prediction tools has revealed the superior performance of xrpi in predicting rna protein complexes involving both long noncoding rna and smaller telomere complex rnas."
"step 3. compute cross-validated r 2 for each voxel now that the hrf has been determined, we quantify accuracy of the glm using leave-one-run-out cross-validation. this involves fitting the model to all runs except one, using the fitted model to predict the left-out run, and repeating this process for each run. model predictions are concatenated across the left-out runs and then compared to the data using r 2 (details provided in section quantification of accuracy)."
"omnibus is a denoising method that combines several of the methods described above. if physiological data are available, the method involves removing the retroicor and rvhrcor regressors (see section retroicor/rvhrcor) and then analyzing the data using standard glm, including global signal (see section global signal) and motion parameters (see section motion regressors) as nuisance regressors. if physiological data are not available, the method involves analyzing the data using standard glm, including global signal and motion parameters as nuisance regressors."
"gc was carried on perkin elmer, using rtx-5 (0.32 mm, 30 m) capillary column, and connected to fid detector, carrier gas was nitrogen with a flow rate of 15 ml/min. the detector temperature was set at 225˚c while the injector temperature was set at 200˚c. initial column temperature was set 50˚c and held for 3 min. a gradient temperature a ramp of 20˚c /min was applied to a final temperature 210˚c that was maintained during the final 10 minutes."
"although modular co-culture has some unique advantages over monoculture, there are still some major obstacles to overcome for large-scale industrial applications. the biggest obstacle is coordinating multiple microbial groups into stable synthetic consortia. the strategies of commensalism-based or mutualistic synthetic consortia might be a way out [cit] . however, little is known about the specific principles of cooperation and competition in these mutualistic models. when these interactions scale to higher dimensions, the behaviors of complex ecosystems are even more elusive [cit] . in addition, achieving a balance of metabolic ratios among different modules and efficient delivery of intermediates may also be obstacles for the application of co-culture strategy."
"the (s)-reticuline-overproducing strain provides the chassis for the synthesis of versatile bias in s. cerevisiaec ( 2018; [cit] ) . it is worthy to mention that up to 30 genes from various species were functionally expressed in s. cerevisiae for bia biosynthesis, which demonstrated the high potential and capacity of yeast cell factories for natural product biosyntheses . however, the yield of alkaloids in yeast remains low, which is commonly believed to be caused by insufficient core precursor (s)-reticuline [cit] ."
"glmdenoise (scrambled) is identical to glmdenoise except that the phase spectrum of each noise regressor is scrambled before noise regressors are entered into the model. this manipulation serves as a control: presumably, the precise temporal information contained in the noise regressors is critical for accurate characterization of noise in the time-series data. phase-scrambled noise regressors should therefore fail to provide substantial denoising benefits."
these databases provide two kinds of networks: 1. single core networks with a star topology consisting of either a single rna interacting with multiple proteins or a single protein interacting with multiple rna and 2. partial mesh networks with multiple interconnected rna and protein components.
"to select the number of regressors to use, we first identify voxels that are likely to be related to the experiment. this is done by selecting all voxels that achieve a cross-validated r 2 greater than 0% under any of the numbers of noise regressors. we then compute the median cross-validated r 2 across these voxels for each number of regressors. we select the minimum number of regressors necessary to achieve a performance improvement that is within 5% of the maximum performance improvement (see figure 2c ). this slightly conservative selection strategy avoids potential overfitting and reduces susceptibility to random performance fluctuations. for example, a performance curve might generally peak at around 4 noise regressors but due to chance have a spike in performance at 10 noise regressors. the strategy also avoids unnecessary use of noise regressors in cases where performance curves have relatively flat plateaus (e.g., see dataset 11 in figure 2c )."
"non-linear biosynthetic pathways, such as diverging and converging pathways, are particularly challenging in pathway optimization in mono-culture system, because they require a delicate balancing between all interconnected pathway modules . furthermore, functional expression of multiple genes from various species may require different cellular environments, which is difficult in a single organism. the emergence of modular co-culture engineering offers a feasible approach for modularizing and balancing complicated biosynthetic pathways ( figure 5f ), whose application in production of natural products has just been comprehensively reviewed . the optimized multiplestrain culture of same species such as e. coli showed 38-fold higher production of rosmarinic acid over the mono-culture system . construction of chimeric s. cerevisiae-e.coli co-cultures enabled high-level production of oxygenated terpenoids, by taking advantage of the high-efficiency terpene biosynthesis in e. coli and suitable environment of s. cerevisiae for p450 expression . engineering co-culture methylotrophic yeasts k. phaffii (two strains) enabled 50% to 70% higher production of polyketide drug monacolin j and lovastatin than that from mono-culture by relieving metabolic stress [cit] c) . stepwise fermentation is consequently culturing multiple strains that harbor divided modules of complicated biosynthetic pathways. the opiate biosynthetic pathway was divided into four modules and distributed into four e. coli strains [cit], and stepwise culture of four engineered strains produced 2.1 mg/l thebaine from glycerol, corresponding to a 300-fold increase from yeast system [cit] . this stepwise culture system can avoid tetrahydropapaveroline (thp) degradation by o-diphenolase activity of tyrosinase [cit] ."
"the consistency and size of the cross-validation improvements suggest that including noise regressors improves accuracy of glm estimates. however, technically, the selection of the number of regressors and the selection of the noise pool are parameters of the fitting process, and these must be evaluated to obtain a strict test of accuracy."
both 10 g from the herbal tea sekem ® as well as 10 g of lab-prepared imitated compositional mixture of the same locally available herbal constituents were extracted with dichloromethane.
"cofactor b12 is very important for glycerol dehydratase activity in 1,3-propanediol (1,3-pdo) production, which becomes inactive (b12-inact) when tightly bound to the dehydratase after the reaction with glycerol [cit] . a protein partner (pduo), glycerol dehydratase reactivase, facilitates the dissociation of the b12-inact from glycerol dehydratase. also, b12-inact is reconverted to b12-act with the participation of flavoprotein and atp [cit] . thus, engineering rapid conversion of b12-inact into b12-act, in addition to increased b12 level, is very beneficial for enhancing glycerol dehydratase activity [cit] ."
"to enhance the accuracy and confidence of predicting rpis with sequence information alone we have developed xrpi, an ml-based method using the extreme gradient boosting classifier, xgboost, with features based on data-driven parameters, such as the amino acid interaction propensities and minimum structural unit (window size). firstly, we have extracted biologically significant features from the rpi structural dataset itself. we have calculated the rpi specific amino acid interaction propensities and have scanned the entire rpi to obtain the length of the optimum binding interface. interestingly, both the amino acid propensities as well as the optimum binding interface are different for rpi as compared to ppi. though xgboost has been applied in several data science challenges, its application in the field of computational biology has been limited 26 . we have optimized xgboost to maximize its applicability and performance in dealing with biological systems. to overcome overfitting of the data, we have carefully designed a negative dataset consisting of pairs that are not similar to any known interacting pairs. comparison with existing methods has shown that xrpi is the best performing method on npinter, a database of curated rpi sourced from high throughput data and literature 27, 28, with 97.8% accuracy as well as telopin, a network of rpi involved in telomere function 29, with 99.4%. our results indicate that xrpi does consistently well both in predicting rpi wherein the input sequence information is either large (npinter) or very sparse (telopin). another advantage of xrpi is its computational efficiency allowing processing of large batches in seconds. we believe that the robustness of xrpi will greatly enhance the prediction ability of rpi and the associated rpi networks involving not only the long ncrna but also microrna and shorter non-coding rna."
"the above-mentioned protocol was used to assess the amount of cymbopogon in a commercial herbal preparation (sekem renal herbal tea). the percentage of cymbopogon in the herbal formula was measured by calculating normalized peak areas ratio, the markers were evident but in much lesser amount in the marketed sample as the normalized peak areas ratio for the marketed product was calculated as 5% of the herbal tea. despite the fact that it is stated in the inserted pamphlet to contain 20%, halfa bar (chromatogram 2)."
"plant material.the aerial parts of cymbopogon proximus, collected from luxor, egypt, were air-dried in shade before processing. it was identified by the department of botany, faculty of science, alexandria university. other plant samples were purchased from four different herbal stores (a -d)."
"a denoising technique should produce estimates of taskrelated bold responses that accurately generalize to novel measurements; hence, we use cross-validation to evaluate the effectiveness of glmdenoise [cit] ) . we find that the method consistently improves cross-validation accuracy of bold response estimates compared to a standard glm analysis. furthermore, glmdenoise yields substantial improvements in snr, which we quantify as the maximum response amplitude observed for a voxel, divided by the error (uncertainty) on this amplitude."
"a stimulating exercise is visualized on how to analyze, assess or identify botanical preparations in the absence of unique or distinct chemical markers. nevertheless, the intricate chemical compositional profile of the plant's metabolic pool, per se, constitutes a specific phenotypic identity and a quality fingerprint. most of the current qc practices focus on using one or few key active components or specific markers in the analysis while keeping a blind eye to the synergy of the whole chemical configuration. analytical approaches that can simultaneously track down subtle changes over a wide array of variables will be more competent of accurately describing the sample. such comprehensive analytical approach can conceive an intuitive resolution on how to evaluate natural products from a holistic perspective. chemometric tools are superbly useful in simultaneously monitoring elements of change in the sample's makeup; hence, it can equally be exploited to monitor plant-to-plant metabolic divergence or unexpected chemical signature change by adulteration [cit] . multivariate data analysis (mvda) has proven to be exceptionally efficient in extracting hidden information through cross-examining all the data's array of variables. on the other hand, conventional methods handle observations from a univariate standpoint as the key data processor and consequently they only expose limited information [cit] . cymbopogon proximus, family gramineae, locally known as halfa-bar, is an aromatic densely-tufted grass growing wildly and widely in upper egypt. the herb is highly reputed in folkloric medicine as an effective diuretic, renal or abdominal antispasmodic agent, and for relieving bronchial asthma as well. the herb exerts its unique pharmacological action through relaxation of the smooth muscle fibers without abolishing the propulsive movement of the tissue, thus, it is traditionally used in the expulsion of renal and ureteric calculi [cit] . the sesquiterpene cryptomeridiol (proximadiol) has been previously reported to be responsible for its antispasmodic activity [cit] . the plant itself or admixed with other herbs like ammi visnaga, as sekem ® tea, or khellagon ® capsules, is marketed for renal ailments. proximol ® tablets or effervescent granules are labeled to contain standardized c. proximus extract. a colorimetric method has been reported to quantitatively assess proximadiol in c. proximus after separation from the extract by preparative tlc [cit] . in addition, the amount of cryptomeridiol in the petroleum ether or ether extracts was assessed by glc [cit] ."
"directed evolution at the cellular level emphasizes ''collaboration'' between scientists and microorganisms to achieve design goals, harnessing the initiative of the yeast rather than relying entirely on biologists' deliberate choices [cit] . this global-directed evolution strategy often requires yeast to adapt to challenging environments by genomic flexibility and dna recombination that are very difficult to realize through rational engineering. similar to protein-directed evolution, cellular-directed evolution requires efficient mutagenesis and high-throughput screening. adaptive laboratory evolution of s. cerevisiae toward enhanced resistance of periodic hydrogen peroxide shocking improved carotenoid production by exploiting the antioxidant properties of carotenoids [cit] . similarly, this ''hydrogen peroxide-shocking'' strategy also improved the production of antioxidant 3 0 -hydroxygenistein in recombinant k. phaffii . incorporating the synthetic chromosome rearrangement and modification by loxp-mediated evolution into designed yeast cells with loxp sites enabled generation of genotype diversity for improved production of carotenoids, violacein, and penicillin in yeast [cit] . further high-throughput sequencing can map genotypephenotype relationships, which is helpful for the construction of robust cell factories by reverse engineering . together with random mutations, directed evolution can be combined with rational designs to form a loop, which in turn provides the targets for future rational design and regulation."
"we used the dnb to assess the cross-validation accuracy of a variety of denoising methods, and in figure 6 we summarize performance by computing the median cross-validated r 2 value achieved by each method on each dataset. the median is computed across voxels in each dataset that satisfy three conditions: (1) the voxel is contained within the brain mask calculated by fsl's brain extraction tool [cit], (2) the voxel has a cross-validated r 2 value greater than 0% under at least one of the denoising methods being compared, and (3) the voxel satisfies condition 2 after slight spatial smoothing of the r 2 volumes (3d gaussian filter, fwhm equal to 1.5 times the voxel size in each dimension). the purpose of conditions 1 and 3 is to ignore random speckles in the r 2 volumes (false positives) and to help focus the performance summary on brain regions with task-related signals. comprehensive results showing all voxels in all datasets are available at the dnb web site."
"step 5. calculate noise regressors using pca on time-series of voxels in the noise pool for each run, we extract the time-series of the voxels in the noise pool, project out the polynomial regressors from each time-series, normalize each time-series to unit length, and perform principal components analysis (pca) [cit] b) . the resulting principal components constitute candidate noise regressors."
"possible reasons for the superior performance of xrpi over the other methods can be attributed to: (1) the rpi specific domain knowledge used in developing the prediction models and (2) the discriminatory power (positive vs. negative rpi) of xgboost compared to other ml tools. when we have used the ppi based domain knowledge in generating the rpi prediction models using the current datasets, we have seen lesser prediction accuracies compared to the xrpi (supplementary table 3 ). furthermore, analysis of the feature discrimination utilized by xrpi in predicting rpis has revealed a positive correlation (0.78) between the protein msus chosen by xrpi to the msus often seen at the binding interface of the rpis. in other words, xrpi is learning to discriminate based on biologically relevant features (http://universe.bits-pilani.ac.in/goa/aduri/xrpi). another plausible explanation for the better performance of xrpi lies in the domain-specific knowledge provided to the ml."
"alternatively, hca (hierarchical cluster analysis) was explored to examine the capacity of the uv spectra of the extract to reveal authenticity of the commercial c. proximus samples. hence, four c. proximus samples were purshased from 4 different herb stores, designated as a, b, c and d, and were similarly treated with the same extraction protocol as the authentic sample but in three triplicates. then, separately, each solvent-extract for the commercial samples-along with the 10 replicates for the equivalent selective solvent-extracts for the authentic sample of c. proximus numbered from 1 to 10-, were scrutinized by hca. figures 3-4 clearly reveals that, regardeless of the solvent used, each of the ten replicates were well grouped together into one cluster and distinctably separate from the commercial samples, as an example the dendogram of light petroleum and alcohol extracts are shown in figure 3 . nevertheless, some solvent-extracts were found to be, even, equivaluable in distinguishing the source of the sample (as belonging to a specific supplier). hence, it is clearly obvious that using any solvent for the extraction can conclusively distinguish between the available samples."
"previously studies found that plant th had low activity in s. cerevisiae [cit], which limited the alkaloid biosynthesis in yeast. the main reason is the deficiency of its cofactor bh4 in yeast [cit] . thus, bh4 biosynthetic pathway was reconstituted and enabled the significant noscapine biosynthesis in s. cerevisiae . we also found that reconstruction of bacterial nadph-dependent electron transfer systems, fd/fnr (ferredoxin-ferredoxin reductase) and fld/fnr (putidaredoxin-putidaredoxin reductase), significantly improved the performance of bacterial alkane and 1-alkene biosynthetic pathways in yeast [cit] ."
"the bootstrapping procedure described here occurs at the end of the glmdenoise procedure (after the selection of the noise pool, the selection of the number of noise regressors, etc.). in contrast, a stricter bootstrapping procedure would resample the data prior to the glmdenoise procedure (such that the entire glmdenoise procedure is applied to each bootstrap sample). it is possible that because of the late application of bootstrapping, estimates of standard error provided by glmdenoise may be somewhat optimistic (too small). one strategy for addressing this issue is to split the data prior to the application of glmdenoise. for example, glmdenoise could be independently applied to two halves of a given dataset (e.g., odd runs, even runs). such an approach may be especially useful for classification-based studies that require strict separation between training and testing data. finally, note that depending on the analysis workflow in which glmdenoise is embedded, estimates of standard error may not be necessary. for example, if beta weight estimates are intended to be used in a second-level analysis, the bootstrapping procedure can be simply omitted from glmdenoise."
"flavin is an active cofactor involved in maintaining redox homeostasis in mitochondria [cit], endoplasmic reticulum (er) [cit], and nucleus [cit] . flavin adenine dinucleotide (fad)-dependent monooxygenase (hpab) enabled higher-level production of caffeic acid from p-ca than plant-specific cytochrome p450-dependent monooxygenase [cit] b; [cit] ) . it is expected that this pathway will consume a large amount of cytoplasmic fadh 2 and disrupt intracellular fadh 2 /fad homeostasis, and enhancing fad(h 2 ) level will further improve caffeic acid production in yeast."
"the blood oxygenation level dependent (bold) signal measured in functional magnetic resonance imaging (fmri) arises from multiple sources. the portion of the bold signal arising from neural activity is generally of scientific interest. other portions of the bold signal reflect various physiological and instrumental factors, and these are typically unwanted and considered to be noise. being able to separate signal from noise has clear value for scientific experiments."
motion regressors is identical to standard glm except that motion parameter estimates from the spm5 motion correction procedure are included as additional nuisance regressors [cit] . there are six additional regressors for each run (corresponding to three translation parameters and three rotation parameters).
"non-conventional yeast refers to more than 2,000 identified yeast species except for two model yeasts, s. cerevisiae and schizosaccharomyces pombe [cit] . several non-conventional yeasts have attracted more interest in metabolic engineering applications because of their unique advantages and characteristics [cit] . in contrast to the crabtree yeast s. cerevisiae, many con-conventional yeasts are crabtree negative, have much higher biosynthetic capacity, and use low-cost or marketsurplus carbon sources, such as methanol or glycerol [cit] . k. phaffii has been known as a powerful protein expression system because of its capacity to perform complex post-translational modifications [cit] . ogataea polymorpha and ogataea thermomethanolica are thermo-tolerant yeast and have high potential for industrial application along with saving cooling cost [cit] . these characteristics and advantages of non-conventional yeasts make them more suitable as hosts than s. cerevisiae for specific production [cit] . it should be emphasized that fungi such as aspergillus are much more suitable as cell factories for heterologous production of complicated antibiotics that are derived from fungi [cit] . however, lack of genetic tools and inefficient homologous recombination (hr) make it challenging in engineering these microbes [cit] ."
"the prediction ability of the calibration model for the determination of the class identity of the extract was tested using external test sets a & b. furthermore, the score scatter plot described in figure 1, clearly reveals that different extracts categorically associated in proximity to each others, which reflects a sensible resemblance in content-constituents. meanwhile, the 2d plot failed to clearly express the resolution between the clusters, but, instead the 3d representation (figure 2) have successfully corrected the aberration in the 2d data representation and displayed the distant allocation of the clusters."
"in vitro testing. guinea pig ileum was used to evaluate the antihistaminic activity of the different extracts. contractions were expressed as a percentage inhibition of the response to histamine dose. silica gel (10 -40 µ, mesh size), e. merck, darmstadt, were used for column chromatography menthol, thymol, borneol, camphor, eugenol, and cineol, are bdh laboratory reagents. reference solutions were prepared in concentrations 10 mg/ml for the solid compounds or 0.3 ml/ml for cineol and 0.1 ml/ml eugenol in methanol."
"in case of telopin database, where the rna sequence is a short repeat hexamer sequence, xrpi390 (99.4%) does better than both xrpi2825 (88.4%) and rpiseq-svm (90.1%). in contrast, rpiseq-rf performed poorly with accuracy less than random guessing (48.6%). rpi-pred, where structural information is incorporated in the model building, has an accuracy of only 74.3% and lncpro which also uses structural information performed very poorly (table 3) . when the long non-coding rna containing rna protein complexes of npinter are considered, the performances of the models are different compared to the telopin dataset (table 4) . again xrpi has higher prediction accuracy than the existing methods, albeit with xrpi2825 doing better than xrpi390 in this case. rpiseq-rf (95.8%) outperforms rpiseq-svm (88.4%) and rpi-pred has an accuracy of 96.8% and able to correctly predict all rpis in drosophila melanogaster and saccharomyces cerevisiae. lncpro again performs very poorly with an accuracy of 55.5%. one possible explanation for rpiseq rf and svm models performing differently on different datasets may be due to the lack of regularization mechanisms in the random forest algorithm which may lead to the underperformance of these models in rpi containing sparse data as is the case with telopin having a short hexamer repeat of rna. though rpiseq does perform well on the external datasets depending on the model (svm vs. rf), the confidence of predictions is very low 14 . rpi-pred performs reasonably well on npinter dataset but the difficulty in obtaining the rna secondary structure and protein structural blocks is a major hindrance in using this tool. on the other hand, lncpro doesn't perform much better than random guessing in our test. surprisingly, ipminer 16 fails to correctly predict any of the rpis either in the npinter or telopin datasets (see supplementary notes) ."
"fractional solvents-extractives solutions (successive). aqueous successive extracts prepared as 0.15714 mg/ml solutions, alcoholic successive extracts (0.0625 mg/ml), ethylacetate successive extract, (0.045 mg/ml) and dichloromethane extracts as (0.142857 mg/ml) solutions."
"the dried light petroleum extract (24 gm, oily yellowish green residue) of air-dried powdered plant material (600 g) was chromatographed on 480 gm (10 -40 µ silica gel) using negative pressure. light petroleum was used for elution and the polarity was raised by increasing concentrations of ethylacetate. 110 mg of proximadiol (cryptomeridiol) was separated from the fraction eluted with 60% ethylacetate in light petroleum. its identification was based on its different physical and spectral properties in comparison with the reported ones."
"nongenetic variation is naturally inherent and will lead to suboptimal performance at the ensemble level due to the subpopulations of low-performance variants consuming nutrients without efficiently synthesizing products. it was found that a minority (15%) of the total cell population produced more than half of the total free fatty acid in an engineered e. coli, which indicated that most cells performed very weakly . to our knowledge, there is no report on engineering population quality control in yeast for production of natural products, which might be attributed to the challenge in coupling the cellular fitness with natural product biosynthesis, although genetic circuits such as dual feedback loops have been shown to suppress cellular heterogeneity in yeast [cit] ."
"an alternative approach for improving sensitivity is to incorporate nuisance regressors into a general linear model (glm) analysis of fmri data [cit] . in this approach, a linear model is specified that includes not only task-related regressors describing the effects of experimental events but also nuisance regressors describing likely sources of noise. if the nuisance regressors successfully capture some of the noise, then this may improve estimates of the task-related components of the bold signal. however, denoising via nuisance regressors depends critically on the selection of regressors: if the regressors are inaccurate or fail to capture a significant portion of the noise, they may have little effect or even worsen task-related estimates."
"in our tests, we found that xrpi consistently performs better than any of the existing tools. in cross-validation tests on the rpi2241 and rpi369 datasets, xrpi performs better than all other methods. since this evaluation needs to be done via cross validation, we could not use the stand alone lncpro software provided by the authors to do this comparison (table 2) ."
"the secondary performance metric in the dnb is snr. to quantify snr, we examine the stability of beta weight estimates across the cross-validation iterations ( figure 3b) . the final beta weight estimate is computed as the mean across iterations, and the standard error is computed as the standard deviation across iterations, multiplied by the square-root of n − 1 where n is the number of iterations (this is the jackknife estimate of standard"
we evaluated two versions of this denoising procedure. retroicor involves removing only the retroicor regressors (a set of eight regressors that model the effects of cardiac pulsation and respiratory motion). retroicor + rvhrcor involves removing both the retroicor regressors and the rvhrcor regressors (a set of four regressors that model the effects of heart rate and respiratory variations).
"a third reason to prefer glmdenoise is that it is selfcalibrating. as our results have shown, the accuracy of task estimates can decrease if too many noise regressors are used (e.g.,"
"functional mri data were collected at the lucas center at stanford university and the stanford center for cognitive and neurobiological imaging (cni) using 3t mr scanners and t2 * -weighted, single-shot, gradient-echo spiral-trajectory (lucas center) and echo-planar imaging (cni) pulse sequences. experiments involved presentation of visual stimuli while bold responses were measured in visual cortex. subjects maintained central fixation on a small target throughout the experiments. in some datasets (datasets 14-21; 8 out of 21 datasets), physiological data were recorded using a pulse oximeter and a respiratory belt attached to the subject."
"the study in hand is designed to explore the potential of using chemometric approaches to analyze a family of data sets derived from the uv spectra (uv assisted-pca and pls analyses) for a collection of systematically generated c. proximus solvent extracts. the extracts were assembled in such a manner to suit correlating chemically communicative groups pertinent to the nature of the extract. the collected spectral data can be used for predicting the membership of samples to pre-defined chemical profile (solvent-extractives). gc and de-tlc techniques were also introduced as simple and selective methods for routine analysis. moreover, the plant's antihistaminic activity on guinea pig ileum was investigated to substantiate its traditional application in asthma."
"machine learning algorithms. we used the implementation provided in the xgboost python library which is optimized for distributed systems. the xgboost model was trained using 200 estimators and a learning rate (η) of 0.25 with each tree having a maximum depth of eight. the l1 regularization parameter was set using cross-validation to 1.12 and the l2 regularization parameter was set to 18.51 before training. the sub-sample ratio was set to 0.9 and the loss function to be optimized was the \"binary:logistic\" function, otherwise known as the log-loss function, which is well suited for binary classification tasks. the optimized xgboost models were trained on a 16 core cpu to speed up the learning process. parameter optimization and evaluation of the models is done using 10-fold nested cross-validation (supplementary notes and supplementary fig. 1 )."
"data availability. xrpi is freely available at http://universe.bits-pilani.ac.in/goa/aduri/xrpi. all the datasets used in the current study, as well as the comparative analysis metrics, are also available at xrpi website. since user-friendly and publicly accessible web-servers represent the future direction for developing practically more useful models [cit], we have developed a webserver that provides xrpi as an online service. a standalone linux program has also been provided so that users can process multiple rna-protein sequence pairs efficiently on their local systems. the links for the web server as well as the binary files are freely available at the xrpi website http://universe.bits-pilani.ac.in/goa/aduri/xrpi."
"construction of metabolon is beneficial for substrate channeling by preventing unstable intermediates or relieving toxicity of some intermediates [cit] . there are two ways to construct a metabolon: protein fusion ( figure 5c ) and protein scaffold ( figure 5d ). protein fusion is the most convenient strategy for enhancing substrate channeling [cit], which, however, may destroy the enzyme structure and is always limited to two enzymes ( figure 5c ). protein scaffold can gather several enzymes into proximity by affinity binding, which has been successfully applied in s. cerevisiae for improving resveratrol production [cit] and also xylose utilization with diminished the accumulation of by-product xylitol by co-localizing corresponding enzymes [cit] ."
"an implicit assumption in glmdenoise is that all time-series modulations that correlate with experimental conditions are signals of interest. however, a problematic scenario is one in which head motion is reliably correlated with the experimental conditions. in such a scenario, we may find that a glm fitted to the data may produce spurious task estimates that cross-validate well. the problem of task-correlated motion is not specifically addressed by glmdenoise. in cases of task-correlated motion, independent metrics of head motion (e.g., motion parameters) may be valuable for disentangling motion-related effects from task-related effects."
"other than the budding yeast s. cerevisiae, some other yeasts have also been harnessed for the production of terpenoids. yarrowia lipolytica is a non-conventional oleaginous yeast, which is supposed to be a valuable host for the production of terpenoids due to its own endogenous mva pathway and high acetyl-coa biosynthesis with high oil production capacity [cit] c) . indeed, y. lipolytica has been engineered for producing a plethora of bioactive terprnoids such as ginsenoside and nootkatone [cit] . the methylotrophic yeast komagataella phaffii (previously named as pichia pastoris) has also been engineered for the production of lycopene [cit] ) and nootkatone [cit] . however, the limited genetic tools hindered the extensive metabolic rewiring, which resulted in much lower production than that in s. cerevisiae (table 1) ."
"in parallel, successive solvent extracts were prepared by extracting 10 grams of the plant powdered material successively with light petroleum followed by dichloromethane, ethylacetate, alcohol and finally water, once more this process was repeated ten times in order to obtain ten replicates per each solvent."
"we previously reprogrammed s. cerevisiae metabolism from ethanol fermentation toward lipogenesis. ethanol fermentation (crabtree effect) is mainly attributed to the high nadh/nad + from efficient glycolysis, and ethanol biosynthesis can re-oxidize cytosolic nadh. enhancing fatty acid biosynthesis required high level of nadph, and blocking ethanol fermentation disturbed the cellular redox hemostasis. redirecting nadh to nadph by engineered transhydrogenase and pentose phosphate pathway, and down-regulating glycolytic flux for reduced nadh generation, resulted a crabtree-negative yeast for high-level production of fatty acids . it is worthy to mention that crabtree effect can be relieved by enhancing the respiration of cytosolic nadh via external mitochondrial nadh dehydrogenases (nde1/2) [cit] . many natural products involve p450-catalyzed tailoring steps, which require nadph for electron transfer. for example, noscapine biosynthesis involves five p450 enzymes [cit], which would consume large amount of nadph for high-level production of noscapine and disturb the balance of nadph/nadp + . thus the nadph was enhanced by overexpressing nadh kinase, three isocitrate reductases, prephenate dehydrogenase, and nadp-dependent glycerol dehydrogenase (gcy1p), leading to significant improvement of noscapine production by using glycerol as carbon source . 3-hydroxypropionic acid (3-hp) biosynthesis from acetyl-coa requires extensive nadph-dependent reduction, which deprives the cellular reducing power. coupling the glycolysis with nadph regeneration by expressing an nadp + -dependent g3p dehydrogenase, enabled a 2-fold improvement of 3-hp in s. cerevisiae . nadph is the driving force for fatty acid biosynthesis; thus extensive redox engineering was implemented for converting nadh to nadph in y. lipolytica, which achieved a new redox equilibrium state and enabled a high-level lipid production of 90 g/l [cit] ."
"quantitative analysis by tlc was carried out using sorbifil tlc videodensitometer. a densitometric calibration curve was constructed by spotting serial dilutions of pro- ximadiol standard and revealing the spots with anisaldehyde sr (figure 7) . proximadiol can be evaluated quantitatively by extracting the powdered herb with light petroleum or optionally by dichloromethane. the amount of proximadiol in the proximol ® effervescent granules dichloromethane extract was determined according to the peak area from the calibration curve and was found 4 mg/50 g powdered drug which is in accordance to the stated label. while, the amount of proximadiol in the root extract was found 10.5 mg % while that of the aerial parts 5.6 mg %."
"other than gene excavation from multi-species genomes, knowledge-driven or rational protein engineering is a more targeted strategy to improve enzyme specificity and efficiency with the aid of protein crystal structure and computer simulation ( figure 3b ). furthermore, protein engineering can help to create efficient enzymes for specific reactions without available enzymes, by engineering the promiscuous enzymes from similar reaction."
"step 2. estimate hrf by iterative linear fitting using the hrf determined in step 1 as a starting point, we estimate the optimal hrf (i.e., the hrf that best fits the data). this is accomplished using an iterative fitting strategy [cit] b) : first, the hrf is fixed, and beta weights and polynomial weights are estimated using ordinary least-squares (ols). then, the beta weights are fixed, and the hrf and polynomial weights are re-estimated using ols [cit] . next, the hrf is fixed, and beta weights and polynomial weights are re-estimated using ols. this procedure is repeated until convergence of parameter estimates (defined as when the r 2 between the previous and current hrf estimate is greater than 99%). after convergence, a post-hoc scaling of the hrf and beta weights is applied such that the peak value in the hrf is 1."
"bias biosynthetic pathway can be divided into two parts ( figure 2 ): the production of the key branchpoint metabolite (s)-reticuline and the downstream modification . although high-level production of (s)-reticuline has been achieved in escherichia coli [cit], it is still challenging in the efficient biosynthesis of (s)-reticuline in s. cerevisiae due to the difficulties in functional expression of bacterial tyrosinase that specifically catalyzes tyrosine hydroxylation toward l-3,4-dihydroxyphenylalanine (l-dopa) [cit] . instead of tyrosinase, tyrosine hydroxylases (ths) and cytochrome p450 hydroxylase cyp76ad1 can serve as alternative options for l-dopa production in s. cerevisiae [cit] . although th has high specificity toward l-dopa biosynthesis, it is heavily allosterically inhibited by substrate tyrosine and requires cofactor 5,6,7,8-tetrahydrobiopterin (bh4) [cit] . a feedback inhibition-resistant mutant th* and reconstruction of bh4 biosynthetic module (please refer to the section reconstruction of cofactor biosynthesis) enabled efficient biosynthesis of (s)-reticuline. similarly, expression of a cyp76ad1 mutant with abolished side activity enhanced l-dopa biosynthesis in s. cerevisiae [cit] ."
"some enzymes often require special cofactors to achieve high catalytic activities. heterologous expression of enzymes derived from plant and other species in yeast may suffer from insufficient or deficient cofactor supply, due to the different cellular environment between yeast and their natural hosts. thus, reconstituting the cofactor biosynthesis should be helpful for maintaining the high enzyme activity for efficient natural product biosynthesis in yeast ( figure 4c )."
"besides superior empirical performance, there are several theoretical reasons to prefer glmdenoise over other denoising methods. one is that the noise regressors derived in glmdenoise are general and can encompass many different types of noise, including motion-related noise, physiological noise, neural noise, etc. thus, glmdenoise has the potential to correct for multiple noise sources. second is that because noise regressors are derived from the data, glmdenoise relies on minimal assumptions regarding the nature of the noise. for example, the denoising strategy of including motion parameters as regressors assumes that motionrelated noise can be described as a linear function of the motion parameters. in contrast, glmdenoise is agnostic with respect to how motion-related noise is manifested in the data (the noise can bear a complex non-linear relationship to motion parameters)."
"a second, pls-2, calibration set was constructed by using 21 different alcoholic extract samples, in concentration ranges from 0.03 -1.2 mg/ml. prediction ability was assessed with test set t containing 6 samples. these samples are composed of two new concentration samples from the alcoholic extract and four artificially reconstituted extracts. these were produced initially by successively fractionating 1 gm dried alcoholic extract of c. proximus with the following solvents; light petroleum, dichloromethane, ethylacetate, ethanol and water. their compositional ratios were found to be 3%, 13%, 22%, 51% and 11%, respectively. the reconstituted extracts were created by adding these solvent-extractives in altered ratios (different from its natural existence) to formulate the modified or mutilated mixtures. mixture 1 consisted of 1% light petroleum fraction, 33% dichloromethane fraction, 33% ethylacetate fraction, 16% alcoholic fraction and 17% aqueous fraction. mixture 2 consisted of 10% dichloromethane fraction, 40% ethylacetate fraction, 20% alcoholic fraction and 30% aqueous fraction, mixture 3 consisted only of 77.3% alcoholic fraction and 16.7% aqueous fraction. mixture 4 consisted of 100 % alcoholic fraction of the alcoholic extract."
"the negative dataset for the training is prepared as follows: the rna protein complexes having only one rna and one protein chain are classified as monomeric complexes and all others are considered as multimeric complexes. a large set of potential \"negative\" rna-protein interaction pairs are generated from the monomeric complexes (by pairing rna from one monomeric complex with proteins from all other monomeric complexes except, from the complex that the rna is part of) and the multimeric complexes (here, if rna chains r1 and r2 are interacting with protein chains p1 and p2 respectively in a multimeric complex, r1 is paired with p2 and r2 is paired with p1). for a pair in this generated set to be considered as truly negative: (1) it should not be in the positive rpi list; (2) the protein in a negative rpi must have less than 30% sequence similarity to all the proteins that the rna of the given negative rpi is paired with in the positive rpi set (for example, if r1 and p1 are paired in the negative rpi, p1 should have less than 30% sequence similarity with all the proteins that are paired with r1 in the positive rpi set), similar check is done for the rna; and (3) in the negative rpi set itself, all the proteins/ rna that are paired with the same rna/protein should not have more than 30% sequence similarity. this has resulted in 7561 unique negative rna-protein interaction pairs, of which we have selected a subset of 2825 in order to balance the positive and negative datasets for training. this subset is chosen in such a way that the ratio of unique protein chains and unique rna chains is roughly the same in both the positive set and the negative set."
"in addition to the cofactor levels, the cellular equilibrium states (the ratio between reducing and oxidation forms) affect the efficiency of the corresponding enzyme and even cellular redox hemostasis. the nad + / nadh, nadp + /nadph, fmn/fmnh 2, and fad/fadh 2 cofactor pairs are important components of the electron transport chain and involved in at least 800 biochemical reactions . thus, engineering the cellular redox balance is important for increasing the biosynthesis of target compounds and robustness of cell factories ( figure 4e )."
"glmdenoise (no exclusion) is identical to glmdenoise except that there is no exclusion of voxels with task-related signals from the noise pool (specifically, in step 4 of the glmdenoise fitting procedure, the exclusion of voxels based on cross-validated r 2 is omitted). this manipulation provides insight into whether the exclusion of task-related signals matters. in theory, if substantial task-related signals are present in the noise pool, the noise regressors that are derived will contain a mixture of both signal and noise and will therefore be less effective at denoising the task-related estimates."
"because the physiological regressors computed in these techniques are sensitive to slice timing, we apply the techniques as part of the data pre-processing stream. the procedure is as follows: (1) retroicor and rvhrcor regressors are computed from the physiological data (code available at https://github.com/cni/ nims/blob/master/nimsdata/nimsphysio.py). (2) the physiological regressors are orthogonalized with respect to polynomials of up to degree 2 (constant, linear, quadratic) . (3) the first five volumes of each run are discarded. (4) the physiological regressors and polynomials are fit to the time-series data from each voxel. (5) the component of the fit that is attributed to the physiological regressors is subtracted from the data. (6) the data undergo the remaining pre-processing steps (slice time correction, spatial undistortion, motion correction). (7) finally, the data are analyzed using standard glm."
"in this section we describe the denoising methods that we tested using the dnb. to ensure that methods differed only in their beta weight estimates, the hrf estimate for a given dataset was fixed and used across all methods. thus, differences in crossvalidation performance can be directly attributed to differences in the quality of beta weight estimates."
"in this review, we first summarize the achievements made so far in the synthesis of natural products in yeast. then we introduce the current advances in synthetic biology tools and metabolic engineering strategies at three dimensions (point, line, and plane), which are consistent with the general steps in building a cell factory. point, line, and plane refer to individual optimization of engineering blocks (e.g., enzymes and cofactors), dynamic optimization of metabolic fluxes, and global optimization of cell robustness, respectively ( figure 1) . in particular, we here discuss in detail the cofactor supply issue for retaining high activities of key enzymes other than their expression level. meanwhile, we also present some perspectives to address the problems possibly encountered in the future. in the yeast cell factory. here, we briefly introduce the yeast-based production of three types of most representative natural products: terpenoids, phenylpropanoids, and alkaloids ( figure 2 )."
"selective solvent extracts which were prepared by extracting five weights (each 10.0 g) of the powdered au-thentic plant with 75 ml of each of the following five solvents, light petroleum, dichloromethane, ethylacetate, alcohol and water. aqueous extraction was carried out using water at ambient temperature and boiling water). the extracts were filtered and the solvents were evaporated under reduced pressure. the extraction process for each solvent is repeated ten times, using fresh material each time, to generate ten replicate extracts for each solvent."
"natural products, either directly or as inspiration, account for more than 50% of all small-molecule pharmaceutical agents in current clinical use [cit], natural products and derivatives possess about 38% of all us food and drug administration-approved new molecular entities. however, the limited availability always becomes problematic when a bioactive natural product comes to a promising pharmaceutical [cit] . metabolic engineering endows cells with the ability of overproduction of new products, which provides a feasible approach for supplying these precious molecules [cit] ."
"in parallel, the same analysis was repeated on the ten sample replicates for the successive extracts (five solvents) of the authentic samples (numbered from 1-10), along with the equivalent three replicates from the commercial extracts. figure 5 reveals that the ten replicates were again well grouped as one cluster separate from the commercial ones. furthermore, the alcoholic and aqueous successive procedure can be considered good discriminating routines as they succedded, to great extent, to identify the source of sample. this suggests that deffating the plant prior to the analysis can be considered a wise step to diminish the effect of the fluctuating volatile content due to the evaporation processing steps."
"initially, the first multivariate calibration project was created, using simca-p (soft independent modeling of class analogy), to construct a mathematical model that relates the sample's uv-absorbances to the type of the extract (whether selective or successive) and consequently, predict its affiliate fit-in class. all of the uv absorbance data (average of three runs) of the 100 extracts (4 different succesive solvent x 10 replicates and 6 different slective solvents x 10 replicates) produced by different solvent extraction of c. proximus were employed to construct the training set of the pca models. r 2 x was calculated as 0.992 (close to 1 indicates an excellent model for assigning the solvent extraction mode). in addition, the model was electronically cross-validated."
"step 6. enter noise regressors into model; evaluate using cross-validation we refit the model to the data, systematically varying the number of noise regressors included in the model. for example, for two noise regressors, the model includes two additional regressors for each run, specifically, the two principal components that account for the maximum variance in the time-series of the voxels in the noise pool. fitting is performed using leave-one-run-out crossvalidation (as in step 3). this produces cross-validated r 2 values for each number of noise regressors. note that the noise regressors will, in general, have some correlation with the task-related regressors in the model. indeed, the only way that beta weight estimates will change (thereby producing changes in cross-validation performance) is if there is some correlation between the noise and task regressors. by entering noise regressors into the model, variance in the data that was previously erroneously attributed to task regressors should now be captured by the noise regressors and lead to better beta weight estimates."
"the escalating interest in herbal therapies and its expansive involvement in the health sector is not surprising and is undoubtfully capturing positive reception. foliage and plant parts like roots, leaves, flowers, etc. have been continually used to promote health or treat diseases and are typically marketed as herbal remedies or phytopharmaceuticals. the challenge facing herbal analysis often revolves around what tests to be performed and which meaningful product specifications should be determined as a part of an operative quality control routine. univariate measurements, which, indeed, are straightforward direct properties of a sample, can be a valuable source of information. for example, certificates of analysis issued by most herbal products manufacturers use solvent-extractive values as helpful univariate measurements for determining compliance with the established or declared values. meanwhile, chemical markers are playing a crucial role in the evaluation of herbal preparations and are routinely used in conventional identification, authenticity, and standardization procedures [cit] ."
"single core networks with a star topology. terra interactome. terra, an essential noncoding rna (ncrna) is involved in several regulatory processes such as telomere length control, the formation of telomeric heterochromatin, and telomere motion. terra performs these functions by interacting with specific proteins and recruiting them to the telomeres 32 . the telopin database currently has terra networks of two organismshomo sapiens and mus musculus which we use to construct the networks. at the heart of this star network is the repetitive terra sequence -uuaggg. our method trained using rpi2825 successfully predicted 124 out of 140 potential terra-protein interactions in humans and 36 out of 41 in mouse giving a prediction accuracy of 88.4% (fig. 2a) . interestingly rpi390 trained model predicted all but one of the rpis accurately (table 3) ."
"specific biosynthesis can be conducted by different pathways (even different enzymes) among various species due to evolution diversity. there will be low efficiency when wholesale adopting inherent biosynthetic pathway from single species into a heterologous host. alternatively, combinatorial optimization by using alternative steps or isoenzymes from multi-species genomes may result in efficient biosynthetic pathways [cit] ( figure 3a ). for example, non-natural ipa pathway was reconstructed for isoprenoid biosynthesis by combining multi-species genes from staphylococcus aureus, myxococcus xanthus, clostridium beijerinckii, and e. coli. this novel ipa pathway is more energy efficient than native mva and mep pathways, resulting in the production of 0.6 g/l monoterpenoids from glycerol [cit] . as mentioned above, integration of four non-native metabolic reactions in s. cerevisiae improved acetyl-coa supply and enabled high-level production of farnesene with reduced atp requirement and carbon loss [cit] . a shorter and more efficient pathway catalyzed by hpab (4-hydroxyphenylacetate 3-monooxygenase) and hpac (nadph-flavin oxidoreductase) from bacteria was constructed to replace the native plant pathway in e. coli and s. cerevisiae, which significantly improved conversion of coumaric acid toward caffeic acid [cit] b; [cit] ) ."
"the main parameters of glmdenoise are as follows: (1) the number of polynomial regressors included in the model. the default is to include polynomials of degrees 0 through round(l/2) for each run where l is the duration in minutes of the run. (2) the number of voxels considered when fitting the hrf. the default is 50. (3) the maximum number of principal components to evaluate. the default is 20, which is typically sufficient to cover the peaks of the cross-validation curves obtained in our datasets (see figure 2c) . (4) the number of bootstraps to perform for the final model fit. the default is 100, which provides reasonably accurate results with modest computational time and memory requirements. the default parameter values described here were used for all of the datasets in this paper. however, parameter values can be adjusted by the user if desired."
"glmdenoise is a fast, automated denoising technique for taskbased fmri that requires no extra data. glmdenoise can be used to extract greater amounts of information from existing sets of data, reduce data collection time, or increase the number of conditions sampled in an experiment. we have shown that glmdenoise outperforms a number of other denoising methods on a variety of datasets. to facilitate usage of this new tool, we make freely available matlab code implementing glmdenoise. in addition, we provide the dnb, which facilitates direct, quantitative comparisons between denoising methods."
"accuracy is quantified at several points in the fitting of the glmdenoise model (see section fitting procedure) as well as in the validation of the model [see section the denoise benchmark (dnb)]. to quantify accuracy, we use a cross-validation strategy in which a model is fit to a subset of the runs and the fitted model is used to predict the left-out runs. the model predictions include components of the data related to the experiment (hrf and beta weights) but do not include components of the data unrelated to the experiment (polynomial and noise regressors). the reason that these latter components are omitted is that the effects of these components cannot be predicted. (signal drift, captured by polynomials, varies from run to run, and we do not expect the same signal drift to occur in different runs. as for the effects of the noise regressors, these effects can only be determined by peeking at the left-out data). because of cross-validation and because the accuracy metric includes only experiment-related components, the accuracy values obtained through this procedure are lower than those that would be obtained in a typical glm analysis. the goodness-of-fit between model predictions and data is quantified using the coefficient of determination (r 2 ):"
"xrpi trained on rpi2825 (xrpi2825) is the best performing method on npinter with 97.8% accuracy and comes in a close second on telopin with 88.4%. it is the only method that does consistently well on both datasets, in spite of vastly differing rna compositions. the lack of long rna sequences in the rpi390 (average length of rna in rpi390 is 44 residues) and the predominance of ribosomal rna in the comprehensive dataset rpi2825 (with the average length of rna being 1768) might be the possible reasons for the better performance of rpi2825 in predicting long noncoding rna protein interactions represented in the npinter database, whereas telopin interactions involve a repeat hexamer sequence where rpi390 is expected to do better. we provide the xrpi with the models trained on both the rpi2825 and rpi390 datasets allowing the user to choose based on the size and nature of the rpi to be predicted."
"precision, expressed as the standard deviation, obtained by repetitive four injections of the dichlormethane extract, was calculated as 0.08 and relative standard deviation as 5.7%. accuracy is assessed by calculating percentage recovery using minimum three concentration levels and three replicates of each concentration. percentage recovery was calculated by injecting five samples of different concentrations of dichloromethane extracts (30, 45, 60, 70 and 120 mg/ml). the normalized total areas was measured and used to construct the calibration curve. three additional samples with concentrations (35, 75 and 115 mg/ml) were injected in triplicate injections and the average percentage recovery was calculated and found 88% -90%."
"the dnb (http://kendrickkay.net/dnb/) is a public database and architecture for evaluating denoising methods for task-based fmri. the dnb consists of the 21 datasets described in this paper, a code framework that enables automatic evaluation of a candidate denoising method, and implementations of several different denoising methods (detailed in section denoising methods). we used the dnb to compare the performance of glmdenoise to that of other denoising methods."
"xist interactome. one of the x-chromosomes in the mammalian females is rendered inactive/silenced to achieve dosage compensation by the expression of a long noncoding rna referred to as x inactive specific transcript, xist, in a process often referred to as x-chromosome inactivation (xci) 33 . xist recruits several rna binding proteins in a cascade of events to perform its function. the current method is successful in predicting 71 out of 74 xist interactions with the proteins (fig. 2b) . 8 . owing to their controlled interactions with regulatory proteins, these rnas have been implicated in chromosome remodeling and gene expression 34, pluripotency and differentiation 35, and immune response 36 . we constructed the lincrna interactome from m. musculus from the lincrna rpi derived from npinter golden dataset. our method successfully predicted 142 out of 147 (with an average confidence of prediction being 81%) possible interactions involving lincrna (fig. 2c )."
"omnibus on dataset 1 in figure 6) . moreover, even a policy of using a small, fixed number of noise regressors does not guarantee good results (e.g., motion regressors on dataset 17 in figure 6 ). the reason for this variability in results is that the efficacy of including noise regressors depends on the magnitude of the noise, the magnitude of the task-related signals, and the amount of correlation between the noise and task-related signals, all of which may depend on the subject and the experiment. glmdenoise addresses this issue by using cross-validation to determine the proper number of noise regressors to use on each given dataset. this adaptive, data-driven approach ensures that glmdenoise consistently improves (or at least preserves) the quality of task estimates."
"prediction accuracy. the current prediction models are trained on rpi2825, rpi2435, and rpi390 using the amino acid and nucleotide classification described above and with the msu as the window size. the performance of the models is evaluated using a ten-fold nested cross-validation. the prediction results for the three datasets are presented in table 1 . the averaged confusion matrix, showing the false positive and false negative values besides the true positive and true negative values is provided in supplementary fig. 3 . the prediction accuracies are 94.3%, 95.0%, and 87.1% for the comprehensive dataset rpi2825, ribosomal dataset rpi2435, and non-ribosomal dataset rpi390 respectively."
"sample dilution (pls study). the initial calibration set, of pls-1, is constructed using uv-absorbances (average of three runs) were recorded for different concentrations from the following extracts:"
kendrick n. kay conducted the experiments and analyzed the data. ariel rokem and jonathan winawer provided conceptual guidance. ariel rokem provided software and programming advice. robert f. dougherty aided in the collection and analysis of physiological data. kendrick n. kay and brian a. wandell wrote the manuscript. all authors discussed the results and edited the manuscript.
"even the synthesis of (s)-reticuline from glucose was already achieved in e. coli [cit], non-functional th limited the synthesis of bias in s. cerevisiae [cit] . a small-molecule biosensor was developed by setting a negative correlation between the activity of th and yellow by-product betaxanthin, which achieved high-throughput screening of th mutants for the efficient synthesis of upstream intermediate l-dopa and a 7.4-fold improvement of dopamine production with the best mutant [cit] . transcription factor-based biosensor is much more commonly used and can directly respond to molecules with conformational change of transcription factors [cit] . using a bacterial transcription factor fapr and its corresponding operator fapo, a malonyl-coa biosensor was constructed in s. cerevisiae to gauge and improve intracellular malonyl-coa levels [cit] b) . the yeast-based biosensor was also successfully used in the screening of itaconic acid [cit] ) and short-/medium-chain fatty acids producer [cit] ."
"the eukaryotic cell metabolism is compartmentalized in sub-organelles, whose membranes are impermeable to various cofactors. thus, it is necessary to reconstitute cofactor biosynthesis to drive the corresponding cofactor-dependent biosynthetic pathways in specific sub-organelles even if there is cofactor localization in other sub-organelles ( figure 4c ). pdh complex consists of three catalytic subunits, pdh (e1), dihydrolipoyl transacetylase (e2), and dihydrolipoyl dehydrogenase (e3), which catalyze the efficient conversion of pyruvate to acetyl-coa in yeast mitochondria. reconstruction of pdh into cytosol is an ideal strategy for enhancing cytosolic acetyl-coa supply without accumulation of by-product ethanol, which would be beneficial for biosynthesis of acetyl-coa-derived molecules such as terpenoids and polyketides. however, it is challenging in cytosol reconstruction of pdh, because the biosynthesis of cofactor lipoic acid for e2 subunit activation localizes in mitochondria. thus, external lipoic acid supplementation and reconstruction of cytosolic lipoic acid biosynthetic pathways are prerequisites for functional assembly of cytosolic pdh [cit] ."
"finally, we tested a couple of variants of glmdenoise as control cases. glmdenoise (scrambled) is a variant of glmdenoise where the phase spectra of the noise regressors are scrambled before entering the model. this method performs about as poorly as standard glm, confirming that the specific timecourses of the noise regressors are essential to achieve denoising. glmdenoise (no exclusion) is a variant of glmdenoise where all voxels are allowed to enter the noise pool, even if they have high crossvalidated r 2 values. this method performs substantially worse than glmdenoise in many datasets (e.g., datasets 8, 18), demonstrating the importance of the exclusion step: the noise regressors will be most effective at accounting for noise if they do not also contain task-related signals in them. this is consistent with the finding that glmdenoise (no exclusion) performs about as well as glmdenoise in datasets where bold activations are weak (e.g., datasets 11, 13)."
"the glmdenoise model consists of several components (figure 1) : an hrf characterizing the shape of the timecourse of the bold response, beta weights specifying the amplitude of the bold response to each condition, polynomial regressors characterizing the baseline signal level (which typically drifts) in each run [cit], and noise regressors capturing widespread bold fluctuations unrelated to the experiment [cit] b) . some model components (hrf and beta weights) describe effects related to the experiment, while other model components (polynomial and noise regressors) describe effects unrelated to the experiment. the number of polynomial regressors included in the model is set by a simple heuristic: for each run, we include polynomials of degrees 0 through round(l/2) where l is the duration in minutes of the run (thus, higher degree polynomials are used for longer runs). formally, let t be the number of time points in a run, r be the number of runs, c be the number of conditions, l be the number of time points in the hrf, p be the number of polynomial regressors per run, and g be the number of noise regressors per run. the time-series data for a voxel are modeled as"
"standard glm is identical to glmdenoise except that no noise regressors are used. (thus, only steps 1, 2, and 8 of the glmdenoise fitting procedure are used.) this method provides a measure of baseline performance."
feature synthesis. the features have been generated using the standard k-mer representation. the amino acids have been classified into four categories and the nucleotides retain their individual monomers. this classification was a result of the interaction propensity calculations performed on data obtained from experimentally determined crystal structures (see methods). the interface size for interactions or the window size (k) for the feature generation is defined as a minimal structural unit (msu) equal to five. the resulting k-mer representations of the rna and protein sequences are concatenated to form a single feature vector which is then fed into the xgboost model (fig. 1) .
"xrpi is developed based on xgboost, an optimized variant of the gradient boosting machine, using data-driven parameters from high-resolution structures of rna protein complexes. in this method, the amino acids are classified into four classes based on their respective interaction propensities in rpi. each of the four monomers of rna is considered separately along with any modified base belonging to the same class as the parent nucleotide. to account for the nearest neighbor effects, that control the structural context of the protein and rna interactions, an interface size of five is considered for both protein and rna."
"despite the availability of versatile active enzyme elements and cofactors, it is still challenging in heterologous reconstruction of the efficient biosynthesis pathways, which is influenced by the complex interactions between metabolites and enzymes as well as the severely disturbing endogenous cellular metabolism. several strategies, including relieving feedback inhibition, dynamic pathway regulation, and metabolon construction, were developed for improving the biosynthesis in yeast cell factories ( figure 5 )."
"when all extracts (selective and successive) were analyzed by gc, four main peaks at t r 11.2, 14.1, 15.0 and 17.7 min were detected and were found to be in common in all the gc chromatograms. hence, cineol was used as an internal standard in all the experimented extracts and their relative t r were found to be 1.34, 1.69, 1.83, 2.13 respectively and the normalized total areas (a s /a st ) of these four peaks were calculated and are listed in table 1 . obviously, the normalized total peak areas for the resolved peaks are at maximum in the light petroleum and dichloromethane extracts, in a lesser amount in ethylacetate extract, minimum in alcoholic extract and barely observed in the aqueous extract (chromatogram 1)."
"optogenetic regulation is an easily operational strategy for dynamic control of biosynthetic pathways, as light can be applied and removed easily without complex media changes . a blue-lightresponsive circuit was constructed by using a blue light transcription factor el222 from the marine bacterium erythrobacter litoralis [cit], for dynamically activating cell growth upon blue light exposure and product biosynthesis in the dark . this optogenetic regulation strategy successfully separated isobutanol biosynthesis from cell growth and improved the production of isobutanol by more than three times, which relieved the toxicity of isobutanol . although this strategy is easy operating, the permeability of light should be addressed for industrial high-density fermentation."
the prediction ability of the calibration model for the determination of the content of the extract was tested using 3 samples of different known identity and content.
"mias are a diverse family of complex nitrogen-containing plant-derived metabolites, which were composed of a secoiridoid moiety and an indole moiety with very diverse structures. reconstruction of a seven-step pathway in s. cerevisiae enabled the production of anti-cancer drug precursor vindoline from tabersonine [cit] . reconstruction of a plant-derived pathway with 15 genes, along with deletion of side pathways and enhancement of precursor supply, enabled for the first time de novo the biosynthesis of strictosidine in yeast [cit] ."
"examining the cross-validation accuracy of glmdenoise and standard glm, we find that glmdenoise consistently improves accuracy, though the exact magnitude of the improvement varies across datasets (figures 4a,b) . notice that improvements are found even for voxels for which standard glm produces a crossvalidated r 2 value less than 0%. even though these voxels are initially deemed unrelated to the experiment and are used to construct the noise pool, the denoising process is still able to improve the glm estimates for these voxels (in some cases producing cross-validated r 2 values greater than 0%)."
"the dnb is designed such that different denoising methods all conform to the same api. in essence, the api specifies that a denoising method should accept as input an fmri dataset (a set of 3d volumes over time and a design matrix) and should return as output an estimate of task-related responses. any denoising method that conforms to this api can be automatically evaluated by the dnb. the dnb allows direct comparison of different methods on the same datasets, and thus provides a means to adjudicate between methods. the dnb framework bears some similarity to the npairs (non-parametric prediction, activation, influence and reproducibility resampling) framework [cit] ."
"recently, the crispr-cas9 system has emerged as a powerful gene editing tool for metabolic engineering and synthetic biology, which has worked well in organisms including model microorganism s. cerevisiae and e. coli. a vast number of cas9-mediated genetic tools enabled efficient and rapid genetic editing in s. cerevisiae [cit] . furthermore, numerous genetic manipulation toolboxes were developed for the construction of heterologous pathways, including promoters [cit], terminators [cit], and neutral sites [cit] . these genetic toolboxes make s. cerevisiae the most attractive cell factory for the production of natural products. as mentioned above, most of non-conventional yeasts display a stronger preference for non-homologous end joining (nhej) when exogenous dna is introduced, which hampers the precise genetic editing in metabolic engineering [cit] . although this problem has been alleviated to some extent through the regulation of the hr-or nhej-related proteins such as rad52p [cit], ku70p [cit] c), and ku80p [cit], the hr efficiency is still far less for precise genetic engineering, in particular multiple dna fragment assembly. besides efficient genetic editing, non-conventional yeast systems require an expanded synthetic biology toolbox (such as highly controllable promoters and enough terminators without homologous sequences) for pathway construction."
"heterologous biosynthetic pathways may bring enormous stress on the cell fitness, such as the accumulation of toxic intermediates, the competition of carbon sources, and reducing power. dynamic regulation of biosynthetic pathways may relieve the stresses by separating the cell growth and product biosynthesis. a dynamic control system comprises two elements: a sensor that perceives external environment change and an effector that can respond to the sensor [cit] (figure 5b )."
"there are two important characteristics of this cross-validation framework. first, the held-out data that a denoising method attempts to predict are raw data that have undergone only minimal pre-processing (see section data pre-processing). these data have not been denoised because applying a denoising method to these data would pre-suppose the validity of the denoising method. second, when using the dnb to evaluate glmdenoise, there are actually two levels of cross-validation involved: an outer cross-validation used to evaluate the accuracy of the entire glmdenoise procedure ( figure 3a ) and an inner cross-validation used within glmdenoise ( figure 3c) . in practice, when applying glmdenoise to a dataset, only the inner cross-validation is performed."
"terpenoids represent the largest class of natural products, which can be resources for pharmaceuticals, fragrances, and biofuels. the high-level production of artemisinic acid, the precursor of antimalarial artemisinin, showed the high potential of yeast saccharomyces cerevisiae as a terpenoid cell factory [cit] . the terpenoids have conserved skeletons with different amounts of isoprene units (c 5 h 8 ), whose biosynthesis has been extensively reviewed elsewhere [cit] . we here classified the current progress in engineering yeast for overproduction of terpenoids into two parts, including (1) engineering central metabolism for sufficient supply of the key shared precursor acetyl-coa and pyrophosphate precursors: isopentenyl pyrophosphate/dimethylallyl pyrophosphate (ipp/dmapp), geranyl pyrophosphate (gpp) for monoterpene, farnesyl pyrophosphate (fpp) for sesquiterpene, geranylgeranyl pyrophosphate (ggpp) for diterpene, etc., and (2) engineering downstream decorating pathways for diverse functional terpenoids."
"in addition to the pathway engineering and flux rewiring, enhancing the robustness of microbial cell factories is essential for industrial application with improved resistance to harsh industrial conditions and toxicity of natural products [cit] . engineering cellular tolerance against harsh conditions has been discussed in detail in previous reviews by us and others [cit] . here we emphasize the global regulation of cellular performance for natural biosynthesis by directed evolution and population quality control."
"at each step where beta weights are estimated, the 50 bestfit voxels are determined, and these voxels are the ones fit in the subsequent step where the hrf is estimated. excluding poorly-fit voxels in the hrf-estimation step is essential for obtaining good hrf estimates. in datasets with extremely low snr, poor hrf estimates might be obtained. to compensate for such cases, we adopt the heuristic that if the r 2 between the initial hrf and the fitted hrf is less than 50%, we simply use the initial hrf as the hrf estimate."
"accordingly, constructing a pls-2 model, has produced r 2 x cumulative of 0.981 and r 2 y 0.928. mixtures 1 to 4 were classified as not belonging to the model according to their dmodx values, while the two concentration samples of the same alcoholic extract were in agreement with the model, which was also capable of predicting their concentrations."
"global signal is identical to standard glm except that for each run, one additional nuisance regressor is used: a regressor that is computed by taking the mean of each functional volume [cit] )."
"the primary performance metric in the dnb is crossvalidation accuracy, whereby a denoising method is evaluated on how well estimated task-related responses predict held-out data. this is implemented through a leave-one-run-out crossvalidation procedure ( figure 3a) . first, we leave out one run from the dataset and apply a denoising method to the remaining runs. (in datasets where conditions are grouped into run sets, the resampling procedure involves leaving out run sets instead of individual runs). we then use the estimate of task-related responses to predict the left-out run. this process is repeated for each run, the predictions are aggregated across left-out runs, and the overall accuracy of the predictions is quantified using r 2 (see section quantification of accuracy). to make minimal assumptions regarding signal drift, polynomials only up to degree 1 (i.e., constant and linear terms) are projected out from the predicted and measured time-series before computing r 2 ."
"the noise pool consists of voxels that are used to derive noise regressors. we select voxels for the noise pool according to two criteria. first, the cross-validated r 2 value determined in step 3 must be less than 0%. this criterion helps prevent voxels related to the experiment from entering the noise pool. second, the mean signal intensity must be above a minimum threshold (specifically, one-half of the 99th percentile of mean signal intensity values across voxels). this criterion helps prevent voxels outside the brain from entering the noise pool. we could also accomplish this goal using an actual brain mask; we prefer intensity-based thresholding as it is simple and robust."
"a summary figure shows the median cross-validation accuracy of each denoising method on each dataset ( figure 6a ). the datasets with physiological data are located in the bottom row. cross-validation accuracy varies substantially across datasets, reflecting the fact that different experiments produce different levels of bold activations (i.e., some visual stimuli drive responses better than others). within individual datasets, however, the pattern of performance across denoising methods is relatively consistent. to see this more clearly, we normalize the pattern of results found for each dataset and average across datasets ( figure 6b) . the best-performing method is glmdenoise."
"in summary, the yeast cells are ideal workhorses for overproduction of natural products, and advanced synthetic biology and systems biology can accelerate the rising of yeast cell factory for industrial application."
"we collected 21 datasets from 12 experienced fmri subjects (8 males). informed written consent was obtained from all subjects, and the stanford university institutional review board approved the experimental protocol. each dataset consisted of one scan session, and each scan session consisted of multiple runs, each typically lasting about 5 min."
"menthol, thymol, borneol, camphor, eugenol and cineol were experimented under the adopted gc conditions to explore the suitability of any of them as an internal standard. the t r of cineol (8.3 min) was found to be appropriate and not interfering with sample peaks. hence, 0.2 ml of its solution was added to all extracts as the appropriate internal reference."
"the canonical hrf used in glmdenoise was determined by fitting the double-gamma function implemented in spm to experimental measurements of the hrf, and can be obtained using the following line of code: [0; spm_hrf(0.1,[6.68 14.66 1.82 3.15 3.08 0 48.9])] (note that the first time point corresponds to stimulus onset)."
"tas, containing common tropane ring, are a group of more than 200 specialized metabolites naturally produced in most solanaceous plants, which are widely used for treating nerve agent poisoning, gastrointestinal spasms, cardiac arrhythmia, and symptoms of parkinson disease. with the characterized enzyme from a chinese medical plant anisodus acutangulus, the six-step biosynthetic pathway was reconstructed for de novo biosynthesis of tropine and pseudotropine (about 0.1 mg/l) in s. cerevisiae [cit] . almost at the same time, the yeast central metabolism was extensively rewired for improving precursor supply and relieving the side pathway competition, which enabled much higher tropine production of 6 mg/l. with this chassis platform, the non-natural ta cinnamoyltropine was de novo synthesized by coupling biosynthetic modules from diverse plant lineages, which may accelerate the drug discovery pipeline [cit] ."
"two aspects of the glmdenoise fitting procedure are illustrated for further clarification. one is that glmdenoise includes estimation of the hrf from the data (figures 2a,b) . this is useful since the hrf may vary across subjects [cit] . another aspect is that cross-validation is used to determine the appropriate number of noise regressors to include in the model. including noise regressors typically improves crossvalidation performance, indicating that it is beneficial to use www.frontiersin.org"
"comparison of xrpi with existing methods. we have compared the performance of xrpi with the existing rpi prediction tools rpiseq 14, rpi-pred 17, and lncpro 19 . for making reliable comparisons, we have trained our models on rpi2241 and rpi369 datasets provided by rpiseq (table 2) . instead, we could have used the current rpi2825 and rpi390 datasets but that would require retraining the rpiseq and rpi-pred models on the newer datasets. xrpi and rpiseq are the only methods that are based only on the sequence information; the other methods used here required obtaining secondary structural information for either or both rna and proteins. please refer to the supplementary notes for a detailed description of the comparative study. we have also compared the performances of these methods on external datasets from telopin and npinter."
"one way to improve ml-based prediction models of biological interactions is to apply novel ml techniques and/ or through domain knowledge based representation of biological information. here, we propose xrpi, an rna protein interaction prediction tool using sequence information alone as input, based on xgboost, a boosting ml algorithm that hasn't been explored in biological systems extensively, and features obtained from understanding the rpi at a molecular level."
"the performance of the other denoising methods provides insight into the types of noise that glmdenoise may remove. global signal (a method that includes the mean of each functional volume as a nuisance regressor), motion regressors (a method that includes six motion parameters as nuisance regressors), and retroicor/rvhrcor (a method that includes nuisance regressors constructed from physiological data) all produce some improvement in cross-validation accuracy. this suggests that the noise removed by glmdenoise may include global bold modulations, residual effects of head motion, and physiological noise. interestingly, omnibus (a method that combines the global, motion, and physiological regressors) often performs well but is inconsistent, sometimes performing even worse than overall results. to summarize performance across datasets, we normalize the pattern of results from each dataset such that standard glm corresponds to 0 and the best-performing method corresponds to 1. we then compute the mean of this pattern across datasets (error bars indicate standard error of the mean). as an alternative performance summary, we count the number of datasets for which a given method achieves the best or nearly the best performance (specifically, the number of datasets for which the median performance of a method either is the best or provides at least 95% of the performance improvement provided by the best method). the number of datasets (out of 21 total datasets) is indicated in the legend."
"all selective and successive solvent extracts-for all replicates-were dissolved in methanol and stock solutions (10%) for each different extract were prepared. these solutions were used to generate appropriate dilutions that would produce suitable uv absorbances. selective solvents-extractives solutions. hot water extracts prepared in concentrations of 0.18333 mg/ml, aqueous extract (ambient temperature) as 0.18333 mg/ml solutions, alcoholic extracts (0.0625 mg/ml), ethylacetate extracts (0.076923 mg/ml), dichloromethane extracts (0.125 mg/ml), and light petroleum extracts as (0.25 mg/ml) solutions."
"the carbon source sensor is extensively used for the construction of dynamic pathway, such as gal and hxt systems. the promoters of galactose metabolism genes, such as gal1p and gal10p, are activated by galactose and repressed by glucose in s. cerevisiae [cit] . with deletion of gal80, these gal promoters are constitutively transcribed at low glucose level and can be used for the construction of biosynthetic pathways responding to glucose level, which can divide fermentation process into cell growth and product biosynthesis phases. the gal system has derived a variety of dynamic regulation strategies to meet different needs, which provides a tool for further refined control of metabolic engineering [cit] . the promoter of hexose transporters (hxt1p, hxt6p, and hxt7p) were used to dynamically regulate by-product biosynthesis pathways [cit] and fatty alcohol transporter with relieved yeast cell stresses."
"60 grams of proximol ® effervescent granules, (containing 18.6 mg c. proximus extract standardized to contain 8 mg proximadiol/100 g granules, in addition to hexamine and piperazine citrate) was dissolved in 300 ml of water, gradually added, and then the aqueous extract is extracted three times with dichloromethane. the organic layer was evaporated to dryness, the residue was reserved to be used for tlc, gc, and chemometric test set c analysis."
"the following is a description of the steps involved in applying glmdenoise to a given dataset. note that the steps are applied in sequence (there is no nesting of steps) and that each step involves all of the runs that are made available to glmdenoise. in a later section [section the denoise benchmark (dnb)] we describe a testing framework in which runs are held out before calling glmdenoise, thereby isolating glmdenoise from the held-out data."
"the 2-c-methyl-d-erythritol-4-phosphate (mep) pathway synthesizes ipp by condensation of the glycolysis intermediates glyceraldehyde-3-phosphate (g3p) and pyruvate, which has higher theoretical yield of ipp than that of the yeast mva pathway (1 versus 0.66 mol ipp per mol glucose) [cit] . however, the lack of cytosolic [4fe-4s] clusters for functional mep key enzymes 1-hydroxy-2-methyl-butenyl-4diphosphate (hmbpp) synthase (ispg) and hmbpp reductase (isph), makes it challenging to reconstruct fully functional mep pathway in yeast cytosol [cit] . alternatively, non-natural isoprenoid alcohol (ipa) pathway [cit] and isopentenol utilization pathway [cit] monoterpene overproduction in s. cerevisiae is always challenging, because there is no specific gpp synthase. in s. cerevisiae, gpp is synthesized by a bifunctional enzyme erg20p (farnesyl diphosphate synthase [fpps] ) and is rapidly utilized toward fpp by erg20p, which limits the c10 precursor pool. enzyme engineering created erg20p mutants for specific gpp synthase activity [cit], which, however, hampered cell growth as the normal erg20p is essential for the biosynthesis of essential cellular components. construction of an orthogonal neryl diphosphate biosynthetic pathway enabled high-level production of monoterpenes without retarding cell growth [cit] . recently, a plethora of c11 terpenes were synthesized in s. cerevisiae by construction of chimeric pathway harboring gpp methyltransferase and monoterpene synthase mutants that prefer c11 pyrophosphate [cit] . this non-natural terpene biosynthesis can expand chemical space and diversity of natural products for pharmaceutical development [cit] ."
"in task-based fmri, researchers seek to identify signals that are related to an experimental manipulation, such as a sensory stimulus, motor act, or cognitive process. this is challenging due to the presence of many sources of noise (e.g., physiological noise, instrumental noise) in the bold signal. to improve sensitivity to task-related signals, a simple and effective approach is to use block experimental designs [cit] . in block designs, experimental conditions have long durations (e.g., 12 s). this elicits (or is likely to elicit) sustained neural activity and leads to a large bold response. however, in many circumstances, block designs conflict with the experimental goals, and researchers must use event-related designs where conditions are brief (e.g., 1 s). for example, event-related designs may be necessary to avoid adaptation and anticipatory effects [cit], to sample many conditions [cit] b; [cit] ), to examine the temporal dynamics of the bold response to a single event [cit], or to match the duration of the stimulus to a psychophysical threshold [cit] )."
"in consent, the compositional constitution of any extract is expected to vary once the original sample used for extraction is subjected to any sort of tampering by exhaustion or adulteration. however, the plant's monograph specifications, in this regard, propose adopting or specifying the weight of the \"solvent extractive\" in some organic solvent as a preliminary indicator in these cases. alas, these weights are non-informative values that do not actually reflect the chemical nature of the compounds in the sample. however, once a more informative property, such as an ir or uv absorption characteristics, is related to the sample, a better understanding of the chemical compositional pattern of such extract can be reliably achieved."
"biotin acts as a cofactor for biotin-dependent carboxylases involved in essential carboxylation and decarboxylation reactions [cit] . most yeasts are deficient in de novo biosynthesis of biotin or have insufficient level for fast growth and product formation [cit] and thus require external addition of this expensive vitamin in cultures. laboratory evolution rendered full biotin prototrophy of s. cerevisiae, and genome sequencing revealed causal mutations for biotin deficiency in s. cerevisiae [cit], which should be helpful for the construction of biotin-dependent biosynthetic pathways in yeast."
"we designed an architecture that enables automatic evaluation of a candidate denoising method. (a) cross-validation accuracy. leave-one-run-out cross-validation is used to quantify the accuracy of the denoising method. in each iteration of this procedure, the denoising method is trained on all runs except one and is asked to predict the task-related signal in the left-out run. predictions are aggregated across the left-out runs, and the accuracy of the predictions is quantified using coefficient of"
"the datasets described in this paper were collected from relatively well-behaved subjects and are free of gross imaging artifacts. to denoise datasets containing extreme head motion and/or substantial image artifacts, strategies beyond glmdenoise may be necessary. in particular, ica-based denoising approaches might prove valuable for these types of datasets. this may explain, for example, why ica provided substantial improvements on one of our datasets (dataset 16). an interesting future direction is to incorporate datasets with severe artifacts into the dnb."
"glmdenoise is a variant of the glm that is commonly used in fmri analysis [cit] . the glmdenoise model consists of an hrf and beta weights, which describe effects related to the experiment and are of primary interest, as well as polynomial and noise regressors, which describe nuisance effects ( figure 1a) . to determine the noise regressors, an initial model without noise regressors is used to identify voxels unrelated to the experimental paradigm (the noise pool) and pca is performed on the time-series of these voxels ( figure 1b) . the basic idea of using pca to derive noise regressors has been presented previously [cit] b) ."
"rpi at an organism level. we were able to predict the rpi networks for all four organisms in the npinter golden dataset. using our method, which has correctly predicted 97.8% (95.1% for rpi390 model) of the rpi present in s. cerevisiae, we were able to build the rpi network which revealed multiple hubs made of both proteins and rna (fig. 2d and table 4 ). for h. sapiens, our method could correctly predict 1533 out of 1560 rpis (98.3% for the rpi2825 and 95.1% for the rpi390 datasets) with confidence levels of 88%. for d. melanogaster and m. musculus, the prediction accuracies are 94.7% and 94.9% respectively (table 4) ."
"natural product biosynthesis always involves complex multi-step pathways, and reconstruction of yeast cell factory often encounters some common challenges, such as unsuitable physicochemical environments (ph and redox potential), insufficient supply of essential materials (substrates and cofactors), occurrence of undesired side reactions, toxicity of intermediates, and incompatibility with endogenous metabolism. several spatialized metabolic engineering strategies including metabolon construction (protein level), sub-organelle engineering (subcellular level), and co-culture engineering (cell level) have been developed to address these problems."
"with regard to enhancing pyrophosphate precursor supply, the limiting steps of mevalonate (mva) pathway were enhanced by overexpression of the catalyzing enzymes or corresponding mutants with higher activity. for example, a truncated thmg1 or a variant hmg2 k6r was overexpressed for enhancing the activity of rate-limiting 3-hydroxy-3-methylglutaryl-coenzyme reductase (hmg-r) [cit] . furthermore, overexpression of erg10 (encoding acetoacetyl-coa thiolase), mk/erg12 (encoding mevalonate kinase), and mvd1 (encoding mevalonate pyrophosphate decarboxylase) were shown to be helpful in enhancing mva flux for terpene biosynthesis [cit] ."
"some cofactors do not work alone and require activation through covalent binding to the protein (figure 4f) . as mentioned above, lipoic acid is necessary for functional reconstruction of pdh in yeast cytosol, but it cannot directly activate e2 subunit of pdh. its active form lipoamide functions as an acyl carrier for e2 activation by covalent attachment of the lipoic acid to the e-amino group of a specific lysine residue of e2 [cit] . therefore, in addition to reconstruction of cytosolic lipoic acid biosynthesis, a de novo synthetic lipoylation machinery is also necessary for functional reconstitution of pdh in the yeast cytosol [cit] ."
"these studies clearly demonstrated that the construction of chimeric pathways, by integration of multiple species genes, is beneficial for efficient biosynthesis of molecules of interest, which, however, strongly relies on the identification of efficient functional genes from the avalanche gene databases. fortunately, extensive bioinformatic tools and high-throughput experimental data were developed to elucidate biological function from multi-species genomes [cit], which should facilitate the identification of efficient enzymes for construction of biosynthetic pathways."
"we are currently applying the data driven parameter strategy outlined here to develop models for predicting other biomolecular interactions such as dna-protein and protein-protein interactions. we also hope to enhance the quality of predictions from xrpi by incorporating structural information in our feature set. creating a structure-aware model for location specific rna-protein interaction prediction is another research area that we are currently exploring. incorporating additional training data from newer rpi databases such as raid 42 is one more approach which might improve the predictive power of xrpi. in the future, we plan to combine the domain knowledge and graph theoretical approaches to further the performance of xrpi."
"finally, the improvements in snr provided by glmdenoise can be visualized using activation maps (figure 5) . notice that the maps produced by glmdenoise have improved statistical power compared to those produced by standard glm. of course, the depicted maps represent just a small subset of the full set of results (provided in figure 4) ."
"it is well known that the amino acid interaction propensities for ppi are different compared to rpi and dna-protein interactions 21, 22, 37 . to calculate the amino acid interaction propensities in rpi, we have defined the binding interface of an rpi using a 5 å distance cutoff between the rna and protein chains. once the binding interface(s) is identified, we have calculated the rpi interaction propensity of an amino acid, to interact with the rna as given by equation 1."
"the feedback inhibition mentioned here refers specifically to product inhibition at the enzyme level, not at the transcription level ( figure 5a ). introduction of feedback-insensitive amino acid biosynthetic genes (aro4 k229l and aro7 g141s ) improved cellular aromatic compound levels by 200-fold in e. coli and s. cerevisiae [cit] . characterizing and expressing a non-feedback acetyl-coa synthetase mutant from salmonella enterica increased the production of acetyl-coa/malonyl-coa-derived products in yeast by enhancing acetyl-coa supply [cit] . this strategy is generally based on a clear understanding of the enzyme structure and inhibition mechanism. otherwise, biosensor-based enzyme engineering may be helpful for screening feedback-insensitive mutants (as mentioned in the section biosensor for high-throughput screening)."
"in folkloric use, c. proximus is included in the herbal mixtures used in the treatment of bronchial asthma. [cit], the alcoholic solution of the oleoresin was given orally to guinea pigs. it was found that it gradually develops a protective action against bronchial spasms produced by histamine sprays. since then no further trials concerning the antihistaminic properties were performed. the alcoholic extract produces the maximum inhibition (100%), followed by aqueous extract (80%), ethylacetate (38%), dichloromethane (12%) and light petroleum (8%)."
"finally, to facilitate comparison of glmdenoise to other denoising methods, we present the denoise benchmark (dnb). the dnb, available at http://kendrickkay.net/dnb/, is a public database and architecture for comparing denoising methods. the premise behind the dnb is to provide an application programming interface (api) for denoising methods; when this api is satisfied, the accuracy of a denoising method is evaluated using an automatic cross-validation procedure. the dnb consists of the datasets described in this paper, code that performs the crossvalidation procedure, and implementations of several denoising methods. using the dnb, we find that glmdenoise outperforms a number of other denoising techniques, including the use of motion parameters as noise regressors, ica-based denoising, and retroicor/rvhrcor."
"to date, more than 45,000 genomes are available in public databases, providing abundant resources for gene discovery. even many functional genes and secondary metabolic pathways have been identified through the combination of computational mining and experimental confirmation [cit], it is still challenging to identify suitable enzymes for the construction of biosynthetic pathways in specific hosts. heterologous protein expression in yeast often results in reduced activity or undesired catalytic properties, because of incorrect folding, low expression levels, unsuitable microenvironments or feedback inhibition, etc. [cit] . here, we will discuss the advanced approaches to get suitable enzymes and regulation components for efficient biosynthetic pathways."
"we used the dnb to compare the performance of glmdenoise to that of several popular denoising methods. to allow comparison with methods that require physiological data (e.g., retroicor/rvhrcor), the dnb includes several datasets that were acquired with concurrent physiological monitoring (pulse oximeter and respiratory belt). the dnb is publicly available (including data and code implementations of the denoising methods), and we welcome efforts to implement and test other methods."
"the glmdenoise technique is designed to be applied to fmri data following general pre-processing steps. for the datasets in this paper, we performed the following pre-processing steps: the first five volumes of each run were discarded to allow longitudinal magnetization to reach steady-state; differences in slice acquisition times were corrected using sinc interpolation; measurements of the static magnetic field (b 0 ) performed in each scan session (except datasets 16, 17, 20, 21) were used to correct volumes for spatial distortion; and motion correction was performed using spm5 (http://www.fil.ion.ucl.ac.uk/spm/). in general, we recommend that pre-processing include, at a minimum, discarding of the initial few volumes (if not already discarded by the scanner) and performing corrections for slice timing and motion."
"xrpi surpasses the current state of the art in predicting rpi by using a superior classifier whose performance is demonstrated through evaluation on external datasets. hence xrpi is an accurate and robust tool that can be used for reliably predicting rpis with a high measure of confidence, especially in cases where the sequence information is limiting (eg. telomere network). we anticipate that the method proposed will greatly aid in predicting rpi involving not only long non-coding rna (lincrna) but also micrornas."
"we perform a final fit of the model using the number of noise regressors determined in step 7. to obtain error bars, we bootstrap the model 100 times. in each iteration, the model is fit to a bootstrap sample drawn from all available runs (e.g., if there are 10 runs, the model is fit to 10 runs drawn with replacement from the runs). the median across bootstrap results is used as the final model estimate, and one-half of the 68% range of bootstrap results is used as the estimate of standard error. (the idea behind the bootstrap is to use the data themselves as an estimate of the underlying population distribution and to use resampling to estimate confidence intervals). finally, beta weights are converted to units of percent bold signal change by dividing by the mean signal intensity in each voxel. voxels passing a minimum threshold are identified (voxels with cross-validated r 2 greater than 0% under any number of noise regressors), and median cross-validated r 2 values are calculated. the minimum number of regressors necessary to achieve within 5% of the maximum performance is selected. the performance curves are generally u-shaped, indicating that noise regressors help but too many noise regressors hurt performance."
"to rigorously evaluate glmdenoise, we designed an architecture that subjects the entire glmdenoise method to a crossvalidation procedure (thus, there are two levels of cross-validation involved). the architecture is termed the dnb (figure 3) . the architecture allows different analysis procedures to be tested, including other denoising methods (see section comparison with other denoising methods). here we focus on comparing the performance of glmdenoise to that of an analysis that omits the noise regressors, termed standard glm."
"other tree-based ensemble methods such as random forests, adaboost, and the traditional gradient boosted trees, in terms of both speed and accuracy 31 . to test the validity of choosing xgboost over other tree-based boosting algorithms we constructed prediction models using these algorithms and conducted two tests: (1) with the same parameters as the xgboost classifier, such as the learning rate and the number of trees/iterations, wherever applicable, and (2) with each algorithm's parameters being independently optimized. although all the algorithms (with the exception of adaboost) have comparable performance on rpi2825 and npinter, they fail to perform well on the telopin dataset with the gbtree and adaboost performing no better than random guessing. this suggests that these boosting methods have learned patterns specific to the rpi2825 dataset and are unable to generalize their predictions to the telopin dataset, consisting of a central rna molecule made of a repetitive hexamer sequence. the comparative analysis clearly showed the advantage of xgboost both in the cross-validation as well as on external databases (supplementary table 4 ). it is clear that in the case of biological sequence data, overfitting can happen even when model parameters are chosen through cross-validation making the traditional tree-based algorithms to fail. in contrast, xgboost is a robust classifier that provides strong regularization methods to prevent overfitting and over-specialization which makes it an ideal candidate as a classifier for rpi prediction."
"authenticity and purity are mandatory key drivers of acceptance for herbal products to consistently meet the approval of regulatory bodies or consumer-defined quality. these requirements are strategic attributes that can be monitored by different chromatographic and spectroscopic techniques. accordingly, uv spectroscopy and chemometric analysis were utilized to assess the quality of cymbopogon proximus. hca, pca or pls, can definitely detect the slightest change in the composition of the plant extract, and again it can easily distinguish any exhaustion or adulteration attempt that might have been encountered during processing the plant material. this routine has proven to be valuable in showing that the uv spectra of the different extracts were successful in predicting the solvent-class identity for the ten positive validation samples. meanwhile, gc can be used to quantitatively evaluate cymbopogon proximus by extracting the powdered herb with dichloromethane with using cineol as internal standard."
"we believe in the importance of developing methods that are precisely described and readily applicable to actual studies [cit] . in line with these values, we make available matlab code implementing glmdenoise at http://kendrickkay. net/glmdenoise/. the code takes a design matrix and fmri time-series and returns estimates of the hemodynamic response function (hrf) and bold response amplitudes (beta weights). the code also returns the original time-series with nuisance components removed; this allows the code to be incorporated into existing analysis workflows (i.e., the user can choose to ignore the glm estimates and treat the code as a pre-processing step prior to subsequent data analysis). since the fitting process consists of large-scale matrix operations applied to many voxels simultaneously, the code is memory-intensive but fast (an entire dataset can be processed in less than 15 min)."
"subsequently, gc/ms analysis of the dichloromethane extract has helped in the identification of piperitone (peak 1). however, peaks 2 and 3 were proposed by the software library of ms data, as nerolidol and β-eudesmol, respectively. finally, peak 4 identity was additionally secured as proximadiol by spiking the injected samples with reference proximadiol."
random behavior is to randomly select a new state next in its visual field and then move a step in the direction. it is actually a default behavior.
"the data is generated after the sensor nodes monitor the environment or detect the target, and then it is transmitted to the storage node and the metadata generated for it. then, metadata is sent to center ring node that is the nearest to the storage node and is diffused and synchronized at the other nodes within the center ring, so the data map be formed. it is only in this period where the user cannot find the data in the network when it sends out the query sentence to the network. however, the probability is very low. not considering this case, one important problem of uwsn is ensuring a minimum time interval from the user sending a data query sentence to it receiving specific application data. another important problem is ensuring the network energy consumption is as little as possible. due to the communication energy consumption for data transmission being much higher than the energy consumption for data processing into the node, the time of data processing in node is much less than the time of data transmission. hence, the delay in data query mainly includes the delay in the query sentence and specific application data transmission. at the same time, the total energy consumption of the network mainly includes energy consumption on transmission of data storage, diffusion and synchronization of metadata, as well as query sentence and specific application data."
"recent research has thrown serious doubt upon both the practical usefulness and theoretical basis of dcf. its practical usefulness is suspect because it involves choosing a discount rate, an interest rate that will apply for the duration. in practice, people applying dcf choose unrealistically high interest rates, making investment in long-term projects excessively difficult [cit] . its theoretical basis is suspect because the single constant interest rate averages out the effect of periods of very high or (as now) very low interest rates. this would be correct if the outcome was linearly related to the interest rate, but it isn't [cit] )."
provided source code for training machine learning models on provided training dataset and making predictions on new perovskite compounds. users can supply their own perovskite dataset for training or testing.
"basic artificial fish swarm algorithm (bafsa) [cit] . according to the characteristics of the fish swarm and its animal autonomous model, it simulates behavior of fish to achieve the purpose of the group global optimization by each individual in the local optimization. the main fish behaviors are the following: foraging, huddling, following, and being random. in almost all the cases, the bafsa is easy to avoid falling into local optimum and obtain the global optimum. although the algorithm with the merits of strong robustness and good convergence performance is not sensitive to initial value and parameter selection, it has the weakness with search efficiency such as the poor ability to keep balance of exploration and development, late blind search, slow arithmetic speed, and low accuracy of optimization results."
"the center ring nodes k are determined by the data storage node location, user location and so on. from the basic framework of dagm, we will analyze the building process of the center ring structure and data map, as well as methods of data storage, metadata diffusion and synchronization, and data query and specific application data transmission."
"the new methods of distribution network programming, genetic algorithm (ga) [cit], ant colony algorithm (aca) [cit], particle swarm algorithm (psa) [cit], and chaos algorithm (ca), have advantages and disadvantages, respectively. the robustness of the ga is strong in addition to its ability to be trapped in a local minimum. initial particles generated by ca have property of ergodicity but slower convergence speed. therefore, how to effectively combine the merits of different intelligent algorithms to improve the performance among search algorithm is a worth studying direction [cit] ."
"where is voltage of node, min is the minimum, and max is the maximum of the node voltage, respectively. dg is active power and dg is reactive power of dg, respectively; min dg is the minimum and max dg is the maximum of the active power of dg, whilst min dg is minimum of the reactive power of dg and max dg is the maximum. is the current of the node, min is the minimum, and max is the maximum of node current separately."
"for uwsns, in the process of monitoring ocean environment and detecting underwater targets, first of all sensor nodes are distributed in a wide range for underwater monitoring and collection of the environmental information and target information. then, this is stored by an appropriate mechanism. for the collection and processing of the distributed data, it can be divided into two types: non-real-time data (such as environmental information and data) and real-time data (such as military target and rescue target information). at the same time, in a specific scenario such as search and detection of underwater targets for rescue, the observation node should reduce the times and distances of active communication as far as possible on collecting the distributed data in the network. this is in order to lower the background noise in the uwsn. underwater acoustic communication is mainly used in uwsns, and is characterized by low bandwidth, high delay and higher bit error rate, showing great differences on comparison with sensor networks based on radio communication from the perspective of transmission reliability, transmission delay, energy consumption on storage, and query data. the above has brought about new challenges for uwsn data storage, discovery and query [cit] . the main challenges are as follows:"
"to address the challenges mentioned above, the goal of our work is to provide data access mechanism for storage and query real time data with less query time and energy consumption as possible. at the same time, this mechanism can support data discovery and large volume data aggregation for uwsns. for this aim, a data access based on a guide map (dagm) for the underwater wireless sensor networks was proposed in this paper, which is based on the metadata [cit] and ring structure [cit] . in dagm, according to the shortest average user data query path of the network, a center ring structure composed of nodes was established for storing the metadata, which describes the abstracts of data content and storage location. for prolonging the network lifetime, the scope of network is divided into different areas. the sensor nodes in an area are formed into a cluster. when data is generated in the sensor node, the storage node is determined by the geographic harsh table (ght) [cit] . the data is transmitted from the sensor node (data generation source) to the storage node being stored and the metadata generated for it, and metadata is sent to the center ring node that is the nearest to the storage node. after that, metadata is diffused and synchronized at the other nodes within the center ring, so every center ring node can store all metadata in a set for the network, and the data map can be formed. in the process of data query, when there are query data in any node (observation nodes, also called sink nodes are query users generally), the center ring node that is the nearest to it should be selected to deal with the query sentence. if there is no data meeting the user requirements in the map data, the query result \"query failed\" is given to the user; if there is existing data that the user requires, data transmission routing will be generated according to the storage location abstract described by the metadata, and the routing and specific data requests will be sent to the storage node. at the same time, the query result \"ready to receive data\" and data transmission routing will be returned to the query user. lastly, specific application data transmission could be completed by the user and the storage node according to data transmission routing."
this repository contains source code for training/testing machine learning models on the provided dataset. this package can provide predictions of stability of new perovskite compounds using the machine learning approaches developed in this work. the code repository is available online at https://github.com/uw-cmg/perovskite-oxide-stability-prediction.
"preying is a kind of the basic behavior of artificial fish, which move to the direction of food with high concentration. the af-preying behavior is described as follows:"
"t_i(x + y √ 2). combined with the above analysis, the distance sum d er from the nodes in the external area er to the center ring node is shown as equation (2):"
"based on the costs incurred during our experiment after adjustment (see experimental results above), table 5 shows an estimate of the cloud service costs that would have been incurred by this configuration of a median lockss box. compare with table 3 ."
1. dft calculated dataset: [cit] perovskites. table: the 82 elemental properties of all elements in the periodic table. 3. shannon radius: [cit] perovskites which were used to calculate geometric descriptors unique to the perovskite structure.
"work, discussed in future work below, is under way to build economic models of long-term data storage. based on an initial model, and using interest rates from the last 20 years, figure 1 compares the endowment in current dollars to fund storing 117tb for 100 years at varying rates of annual price drop for s3 and 3 raid6 copies in backblaze's storage pods. both alternatives provide 3 geographically separate replicas in storage protected against more than one simultaneous disk failure:"
"whether this architecture would deliver adequate performance would depend on the performance of the underlying service in computing the object's hashes. what is striking, however, is that using s3 directly is not significantly cheaper than using ebs backed by s3 with minimal over-provisioning. the compression and de-duplication capabilities of the mechanism for preserving snapshots of ebs volumes in s3 are remarkably effective at reducing the cost of doing so. thus, given the performance questions surrounding using s3 directly, and the need for an api enhancement to address them, there is no compelling reason to pursue this architecture. instead, the next step would rather be to ease the task of adding storage to the ebs volumes, so that this would be done frequently and thus the costs of over-provisioning minimized."
"data storage time t dstr in dagm consists of three parts. the first part is that the sensor nodes generate data and transmit it to the storage node to complete storing time t dtra . another part is that the storage nodes generate metadata and metadata is transmitted to the nearest center ring node time t mdtra . the last part is the time of metadata completing diffusion and synchronization in the center ring, recorded as t mdsyn . hence, we can obtain equation (6):"
"(a − i). 4 ir-2-4 represents the area of center node in the vertical (or horizontal) direction (excluding the center point). d ir-2-4 represents the distance sum from those nodes in this area to the center ring nodes. as a result, we can find that"
were not used and 791 descriptors that were used for feature selection) generated from the elemental properties. 5. top features selected: the descriptors selected during feature selection as optimal (top 70 from the 791 descriptors) for use the regression model and the classification model.
"we adjusted the results from table 2 to model a production lockss box having already ingested its content and using a reserved instance. to illustrate the range of costs implied by different amounts of over-provisioning, we modelled both minimal over-provisioning (table 3) and over-provisioning matching that of the median lockss box (table 4 ). the projected total 3-year cost in the minimal overprovisioning case would be about $8,800, and in the matching over-provisioning case about $18,100. we would expect actual costs between these two, and probably closer to the minimum. current purchase cost for the matching over-provisioning case is about $1,500. thus non-hardware costs for the local case would have to be over 5/6 (minimal) or 11/12 (matching) of the 3-year tco for cloud to be cheaper. table 1 . price history of the base tier of some leading cloud storage services. table 1 shows the history of the prices charged by several major storage services. it shows that most have dropped less than 10%/yr. this is in stark contrast with the 30-year history of raw disk prices, which have dropped at least 30%/yr, as predicted by kryder's law [cit] )."
"the center ring nodes k are determined by the data storage node location, user location and so on. from the basic framework of dagm, we will analyze the building process of the center ring structure and data map, as well as methods of data storage, metadata diffusion and synchronization, and data query and specific application data transmission."
"these methods are realized through the geographic hash functions or other mechanisms to complete data storage, and show good performance advantages with respect to data storage, query and acquisition in special backgrounds. however, the disadvantage is that the data query does not achieve the shortest average query path."
"for the basic ideas of dagm, it cannot only carry the storage and query real time data with less query time and energy consumption as possible, but also support data discovery, large volume data aggregation and a lot of data requests from one user for uwsns. the data storage, metadata diffusion, data query and data transmission of dagm are as shown in figure 1 ."
"some of these libraries asked whether they could use ``affordable cloud storage'' for their lockss boxes. we first describe the relevant features of typical cloud services, then discuss the various possible technical architectures by which a lockss box could use such a service, ruling some out for performance reasons. applying the typical service's charging model to the remaining architectures rules others out for economic reasons. we then describe an experiment in which we implemented the most cost-effective architecture and ran a production lockss box preserving gln content in amazon's cloud service for several months. we report on the pricing history of cloud storage, using it and the costs from the experiment to compare the projected future costs of storing content preserved by lockss boxes in cloud and local storage. we conclude with discussion of future work, some of which is already under way."
"the procedure of dagm is shown in figure 2b . according the basic ideas of dagm, firstly, the center ring that is composed of different nodes is established. when any node in the center ring receives a piece of metadata, they will merge it into the metadata set that has been stored in this node, and diffuse the metadata to the other node in the center ring along double directions. when all nodes in center ring perform same process, the metadata synchronization is finished. thus, the data map is formed. when a center ring node receives a data query sentence, the query sentence is split into meta-query sentences. then, the meta-query will be compared with the metadata in this node. if data is found in the network, the center ring nodes will establish data transmission routing, and return the query result \"ready to receive data\" and data transmission routing to user. at the same time, the routing and specific data requests will be sent to the storage node. if there is no data that meeting the requirement in the network, the result \"query failed\" is returned to the user."
"in the 21st century, there has been increasing concern with respect to the development of the world economy and the military, and the ocean has become an important base for humans to maintain their survival and the sustainable development of society. worldwide, there has been more and more attention given to maritime rights and interests. there has therefore been a boom in global, modern ocean technology research, specifically with respect to marine resource exploitation and ocean energy utilization, representing a new technological revolution across the globe [cit] . with the rapid development of sensor and wireless network technologies, underwater wireless sensor networks (uwsns) have gradually become the focus of new research interest because of their low cost, high reliability, and potential for a wide range of applications."
"due to the different times and different application backgrounds, the requirements for data granularity and qos are different, so in the user query sentence, the data granularity, qos, and so on must be specified. when the center ring node receives the query sentence, it will find the metadata set according to the specific requirement. then, it can obtain specific application data from the corresponding data storage node. finally, the data will be integrated. the form of integration is a hash map table of multiple values. in the hash map table, a key may correspond to multiple values, and each value is a structure body (including the attribute value, the sensor node identification number, storage time, and so on). according to the specific qos, the data in hash map table is divided into different level and granularity, and sent to the user according to the requirement. all the data-meeting requirements are sent to the user when the user requires the accurate data. when the user requires the summary data, summary data (such as average, max, min) will be generated and returned to user. the user query process is described as algorithm 4:"
"we chose amazon for our experiment because amazon has dominated the market for cloud services, with an estimated market share above 90%, and remains the price leader in storage (see table 1 ). applying the amazon charging model to our preferred architecture, the following costs could be incurred:"
"in uwsn, energy consumption mainly includes the consumption on data processing and consumption on data transmission, as usually the processing energy consumption is much less than the transmission energy consumption. transmission energy consumption includes data-sending energy consumption (e send ) and data-receiving energy consumption (e recieve ). in general, the data-receiving energy consumption is a constant, and data-sending energy consumption has a connection with communication distance, and transmission channel characteristics (bandwidth, error rate, etc.). thus, the energy consumption is shown in equation (8):"
"in the process of data query, every sensor node can be considered as a user, so the user is randomly located in the internal network. therefore, the question of user query data is transformed into any node in the internal network querying data to center ring node. the nodes in the network always query data from their nearest center ring node."
"based on the above analysis, the core problem of the dagm is to determine the location of the center ring nodes based on the minimum delay from the sent query sentence to received specific application data by the user, and minimizing the energy consumption of the network. the data transmission in the network is completed through multiple hops. assuming the number of hops on sending query sentence and on querying results returning is same in each node, k denotes the center ring node; i presents other nodes exception the center ring node; and d(k, i) shows the number of hops from node i to the center ring node k that is the nearest from i. thus, the distance sum (the hops sum, that is, the path length) from all nodes to their nearest center ring nodes is,"
"our experiments suggest that, with some relatively simple additions to the lockss daemon, this enhancement would make the following architecture economically feasible: a lockss daemon running in a local machine with storage in an object storage service."
"the file systems it mounts from ebs contain the preserved content. because ebs is not reliable over the long term, a snapshot of each entire ebs file system is preserved in s3. this snapshot is updated at regular intervals. note that only content changed since the previous snapshot is transferred in this process."
"because the quality of preying and following behavior has close relation with vision and step of artificial fish. the results in the literature [cit] show that the larger range of vision is, the stronger global search ability of artificial fish is and faster convergence speed is. on the contrary, the smaller range of vision is, the stronger local search ability of artificial fish is. whilst the larger size of step is, the faster convergence speed is; however, it will turn out to be an oscillation phenomenon. on the other hand, the smaller size of step is, the faster convergence speed is, but there will be the high precision of the solution. consequently, the dynamic adjustments of artificial fish vision and step size are as follows:"
"e elec represents the energy consumed by the circuit. f (d) is a function of the energy consumption on the transmission data in a specific transmission channel, mainly related to the distance. m indicates the time of retransmission."
"if there is no application data that meets the query sentence in the network, there is no data storage. in the process of data query, there is also no transmission of the specific application data. the results of the experiment performance are as shown in figure 7 ."
"the 30-year history of raw disk costs shows a drop of at least 30% per year. the history of cloud storage costs from commercial providers shows that they drop at most 3% per year. until there is a radical change in one or other of these cost curves it clear that cloud storage is not even close to cost-competitive with local disk storage for long-term preservation purposes in general, and lockss boxes in particular."
"when compared to wireless sensor networks based on radio communication, uwsns are very different in terms of network composition, deployment and networking patterns. this is because of differences in utilization objectives, the environment, the scope of work and the patterns of communication. among them, uwsns used for deep ocean marine exploration and monitoring are composed of floating or submerged floating sensor nodes, mobile nodes (unmanned underwater vehicles or uuvs and autonomous underwater vehicles or auvs) and observation nodes (ships), to form a network system. the sensor nodes carry out real-time monitoring and collect a variety of data distributed in the monitoring region. by way of underwater acoustic communication [cit] or up-water radio communication, they send the specific application data (such as target detection information and ocean environmental information) to the mobile node or observation node. the mobile nodes (uuvs, auvs) can carry sensor nodes, which have the function of redeploying the sensor node and reconfiguring the network, as well as monitoring, communication, data collection, and processing. the observation node is responsible for deployment of sensor nodes, configuring the network, and collecting and processing the application data distributed in the network effectively as well."
"with the emphasis on effective safeguarding of national marine rights and interests, the upsurge in marine economic development, and the significant progress in wireless sensor networks, underwater wireless sensor networks (uwsns) are receiving increasing attention. underwater acoustic communication is mainly used for uwsns, which are characterized by low bandwidth, high delay and a higher bit error rates. for specific applications, there is a type of uwsn that can monitor real-time events and application data in the deployment area. it is necessary for the observation node to quickly obtain the events and specific application data it is concerned with in cases such as underwater target monitoring and emergency rescue. it is challenging to store data efficiently generated by sensor nodes, and for specific application data and events to be accessed as quickly as possible by the user while extending the lifetime of the network as much as possible."
"the second section gives the system model and problem description of the work in this paper. the construction of data storage and query, architecture of the data map, and the mode of data storage and query will be discussed in section 3. in section 4, the performance of dagm with respect to data access and energy consumption is discussed in detail. simulation experiments and their results are given in section 5. in section 6, we provide recent research advances in the data storage and query of the wireless sensor network. the final section presents the conclusions of the paper."
"with each node queries user data from the network, it will send the query sentence to the nearest center ring node from it. data query sentences include data types, granularity, qos etc., at first, the query sentence is split into meta-query (mq) sentences in accordance with the metadata where the center ring node receives the data query sentence. then, the meta-query is compared with the metadata that is stored in this node. if it is found that the data exists in the network, data transmission routing is generated, and the query result \"ready to receive data\" and data transmission routing are returned to the user. in the meantime, the specific data requests and data transmission routing are sent to the data storage node. if there is no data in the network, the query will be ended and the result \"query failed\" is returned to the user. the processing of data query and data transmission routing generation are described as algorithm 2:"
algorithm 4: user query data on dagm user m generates data query sentences dq; generate transmission routing tr and send dq to center ring node nearest k; create data receive handle drh; center ring node processing dq (look at section 4.1); if(receive dtr and ready to receive data from center ring node) receive data and insert it to drh; else if(receive query failed from center ring node) release drh; end if.
"the number of nodes in area ir-2 is obtained when the number of all nodes in ir is subtracted from the number of nodes in ir-1. the distance of the nodes in the opposite side along the horizontal or vertical direction is respectively 1, 2, 3, and so on. because the size of the ir-1 will affect the size of the ir-2, it is necessary to discuss the distance sum from nodes in ir-2 to the center ring nodes according to the size of the ir-1 region. figure 5a shows the case where center node does not have a location in ir-1: 1 ir-2-1 is below the diagonal line in ir-2, and d ir-2-1 represents the shortest distance sum from the nodes (excluding the nodes on diagonal line) in this area to the center ring nodes. then,"
"1 for ir-2-1, the number of nodes in the horizontal and vertical direction is a − b + 1, and the distance sum is ir-1-2 represents the area of center node in vertical (or horizontal) direction (excluding the center point) and belongs to the area ir-1. the number of nodes in this area is 2b − a. the distance sum from those nodes to the center ring node is"
"thus, an integrity check has no option but to retrieve the entire object from the object storage service and compute its hash anew. this is slow and, if the system is running outside the compute service, expensive."
we ran this box using a separate amazon account created for the experiment and monitored the costs incurred by the account every week for 14 weeks. the results are shown in table 2 (see appendix).
"the n ir-2-3 is the number of nodes (excluding the center point) on the diagonal line nl. the shortest distance sum from them to the center ring node is recorded as d ir-2-3. then, we find that"
"despite the fact that qafsa can reach high precision and convergence speed, it may get some inferior solutions while producing better one. the fish diversity is not very rich and the convergence speed can be further improved. therefore, the iqafsa given in this paper is encoded by quantum bits and updated by quantum revolving gate. make it perform preying behavior and following behavior. and swap quantum probability amplitude of artificial fish having poor results can realize the variation, and then the optimal solution is obtained [cit] . at the early stage of the quantum artificial fish optimization, each time the artificial fish has achieved an update, it will turn into the implementation of the tailgating behavior of afsa, which can improve the convergence speed of artificial fish. at the late stage of the artificial fish quantum optimization, each time the artificial fish has realized an update, it will turn into the implementation of the preying behavior of afsa, which can improve the accuracy of optimization. having been updated by quantum revolving gate, preying behavior, and following behavior, for those artificial fish having poor optimization results, it can enrich the diversity of the artificial fish by swapping the quantum bit probability amplitude value, namely, (, ), and reversing the individual evolutionary direction as a whole, which can prevent the individual evolution falling into a local optimum. finally, the variation of artificial fish is brought about. here a point that should be stressed is that the pauli-x gate is used below to realize mutation operation:"
we eliminated week 1 because there were some startup effects. the anomalous storage cost drop in weeks 8 and 12 of the following tables was caused by a glitch in amazon's accounting system that resulted in missing accounting records.
"artificial fish complete update and obtain the optimal value mainly through the following four behaviors: being random, preying, swarming, and following in the process of iterative calculation."
"in this paper, the data access based on a guide map (dagm) method for uwsns is proposed. in dagm, metadata is used to describe the data content abstract and storage location abstract. the center ring is based on the shortest average query path of the network, and thus, the metadata is stored in the center ring nodes, then forming a data map of the network. in the process of data query, dagm chooses the nearest center ring from the user to deal with the query sentence, so that the user can quickly access the specific application data and events that it is concerned with. the results of the simulated experiment show that dagm provides quicker user access for the specific application data and events, and reduces the total energy consumption of the network while effectively prolonging the lifetime of the network."
"we implemented an amazon machine instance (ami) containing our recommended configuration for a lockss box. it is backed by s3 and configured by a bootbucket which specifies these system parameters: aws access key, aws secret access key, s3 bucket backing the image, name of .tar.gz archive in the s3 bucket containing the lockss configuration, and a comma delineated list of volume ids to automatically attach and mount from ebs on start-up."
"flow. dg installed on the radial power distribution network can be simplified to a pv node, pq node, or pi node. in this paper, dg is considered as a pq node which has constant power factor. the characteristic of dg makes it an optional and alternative power supply normally so that it is usually installed on the location of the load. assume that placements of all dg units are in the neighborhood of the load instead of line in the present paper. so, the basic configuration of the distribution network after installing dg is shown in figure 15 . according to the comparison of active powers from the load and dg, there are three kinds of situations between load and distributed network when it comes to active power flow. to distributed network, namely, to form reverse power flow."
"data query includes a query strategy and a data-receiving strategy. under the different times and in different application backgrounds, users require different data granularity and quality of service (qos) in the network. sometimes they require original precise data in network, while at times they only require integration data, with types including average, minimum, maximum, and so on. therefore, the data query sentence includes data types, granularity, qos, etc., which can decrease redundant data that reduce the delay and energy consumption on communication. in remote data transmission of underwater environment, where the energy consumption is huge, bandwidth is low and error rate is high, so data is transmitted in network to the user by multiple hops according to data transmission routing."
"in future work, we will mainly focus on how to improve the storage efficiency, decrease the query time and energy consumption of networks under practical underwater conditions. in addition, our approach can support data aggregation and data discovery. then we will adaptively improve the data map and metadata mechanism on dagm for them, and provide a data access method with full functions and performances efficiency for uwsns. [cit] have proposed a new way to look at cognitive access in underwater acoustic communication, and consider it a new research field. we will also improve our approach for this theme in the near future."
"the optimal configuration of the distribution networks with dg units is a nonlinear optimization problem. in this paper, the objective function of the nonlinear model is formulated with the aim of finding the minimum power loss. thus, on the condition that the constraints below are considered, the objective function of the reconstruction is defined as"
"(1) efficient uwsn data access includes storage, discovery and query. networks of different types generate different types of data, and the data on the sensor node needs to be collected and stored efficiently. at the same time, the query sentences from the users need to be dealt with as far as possible in order to return different granularity data (especially in military application and emergency rescue) to the users with the smallest delay possible. (2) there is the problem of energy consumption in the process of data storage and query. energy consumption is huge due to the long distances and long underwater data transmission times, and this will shorten the network service life while increasing background noise in uwsn. hence, in the processing of data storage and query, energy consumption of data transmission needs to be reduced and balanced as much as possible for the purpose of prolonging the network lifetime."
"1) when it comes to the query time, the value of performance indexes on ght do not change compared with figure 6, because the query data processing is not changed in scenario 1. in this case, all storage nodes must be checked out, and then the result that the network has no application data will be given. thus, the query time does not change in ght. in dagm, if the center ring node does not find the content abstracts and storage location abstracts of the data that form query sentence through the metadata, it directly returns the result of query failed. therefore, the data query time becomes much shorter (shown on the left side of figure 7a, from 108 s to 12 s in state 1). 2) when it comes to energy consumption, in dagm, there are no the routing and specific data requests sent to data storage node, and there are also no specific application data transmitted to the user. therefore, the energy consumption of the network is of decreased amplitude (in state 1, shown in right side figures 6b,c and 7, global query and partial query drops from 578 j to 144 j). in ght, the energy consumption of the network does not decline. 3) in three different states, the minimum change trend of the time performance and total energy consumption is similar in scenario 1. node acknowledges the fact that whether it lies within the sub-region of interest or not. sensory data are routed to the sink node leveraging the parent-child relation of sensor nodes specified in the routing tree. so the qpm complements the querying data function which interest in partial, rather than the whole, network region. in the experiment, the routing protocol data and sensory data transmission mechanism were adopted in our technique, and the calculation of energy consumption on data delivery was also adopted in our approach. the experimental results are shown in figure 8 . as seen in the experimental results shown in figure 8, when it comes to data storage time and data query time, for dagm and qpm in setup 1, setup 2 and setup 3, the node deployment in the network just increases the distance between nodes, and then the communication time between nodes increases, so the data storage time and query time increase linearly. however, the energy consumption of the network becomes dramatically larger with the distance between sensor nodes becoming larger, which is mainly due to the cubic relationship between energy consumption and the communication distance. from this perspective, the method of this paper is in line with the general facts."
"when it comes to the query time, the value of performance indexes on ght do not change compared with figure 6, because the query data processing is not changed in scenario 1."
"this file contains additional details related to our machine learning models which were not provided in the main text. in particular, the supplemental information provides results on training and testing five regression models (using the same data and descriptors as the regression of e hull in main text) to predict the formation energies of perovskite oxides. the supplemental information contains 4 sections: models/apis used in this work, best parameters for each model investigated, regression of formation energy, learning curve with the best model for stability classification."
"we configured a lockss box using this ami to preserve a sample of open access content from the lockss gln, so that it participated fully in the gln although with less content (82gb) than the median gln lockss box (1.58tb). snapshots of the ebs file system containing this content were preserved in s3 at 12-hourly intervals."
"the artificial fish is encoded with quantum bits in qafsa. it mainly uses update strategy of quantum revolving gate for self-renewal to get new artificial fish. compared with bafsa, the diversity of its fish becomes richer. despite the small scale of the fish, it does not affect the convergence of the algorithm. at the same time, the algorithm has faster convergence speed as well as higher accuracy."
"this would have two potential advantages. first, it would eliminate the need to pay to store the data twice, once in ebs (for performance) and once in s3 (for reliability), as we did in our experiment. second, it would eliminate the need for overprovisioning storage to amortize the cost of adding storage over a reasonable amount of increased data."
"the lockss 1 program at the stanford university libraries pioneered the concepts of distributed digital preservation about 14 years ago. by building a peer-to-peer ntwork of lockss boxes in which libraries could collect and preserve content published on the web. about 150 libraries currently run lockss boxes preserving e-journals and e-books in the global lockss network (gln), and databases, government documents, special collections and other content in private lockss networks (plns). these boxes are typically modest pc servers with substantial amounts of local disk."
"therefore, the center ring node k is selected from the nodes with the minimum distance sum (the shortest path) among all nodes t. thus, we can obtain equation (1):"
"as seen in the experimental results shown in figure 8, when it comes to data storage time and data query time, for dagm and qpm in setup 1, setup 2 and setup 3, the node deployment in the network just increases the distance between nodes, and then the communication time between nodes increases, so the data storage time and query time increase linearly. however, the energy consumption of the network becomes dramatically larger with the distance between sensor nodes becoming larger, which is mainly due to the cubic relationship between energy consumption and the communication distance. from this perspective, the method of this paper is in line with the general facts. if there is no application data that meets the query sentence in the network, there is no data storage. in the process of data query, there is also no transmission of the specific application data. the results of the experiment performance are as shown in figure 7 ."
algorithm 3: data storage on dagm sensor node i generates data td; node i finds the storage node k through ght; node i generates data transmission routing dtr based on gpsr; node i sends td to storage node k based on dtr; if(td exist in database of node k)
"(a) when it comes to storage time performance, in dagm the senor node generates data and sends it to the storage node. after the storage node stores the data, it also generates the corresponding metadata, and then the metadata is sent to the center ring node and is diffused and synchronized."
"the procedure of dagm is shown in figure 2b . according the basic ideas of dagm, firstly, the center ring that is composed of different nodes is established. when any node in the center ring figure 2a shows the data map that is the core of dagm. the data map includes building the center ring for storing metadata, for metadata diffusion, and for synchronization, as well as data query sentence processing, data transmission routing generation, and so on."
"nowadays, as new applications of wireless sensor network need data processing with temporal constraints in their tasks, one of the new challenges for wireless sensor networks is the handling of data access [cit] . there are three research aspects in wireless sensor network data access: data storage [cit], data aggregation [cit] and data query [cit] . these prior studies have looked at maximizing the energy efficiency in order to increase the lifetime of wireless sensor network, or focus on the time performance on data access of the wireless sensor network from a single aspect. as we acknowledge, the energy efficiency and time performance of wireless sensor networks are the result of a compromise between the data storage schemes, data aggregation methods, and data query strategies. our study systematically investigates the data storage schemes and data query strategies through balancing the performance between access time and energy consumption of the underwater wireless sensor network."
"this comparison is somewhat unfair to s3. amazon has used the decrease in storage costs to implement a tiered pricing model; over time larger and larger tiers with lower prices have been introduced. the price of the largest tier, now 5pb, has dropped about 10% per year; prices of each tier once introduced have been stable or dropped slowly. [cit] price war."
"delays in data query can be divided into two parts: communication delay tra_t and processing delay (that is, the delay of store-and-forward intermediate nodes) pro_t. the communication delay tra_t mainly depends on the communication distance and the times of awaiting retransmission. if network connectivity is confirmed, the time for awaiting retransmission is a stable value. therefore, the question of minimum delay of any position querying data in the network can be transferred as the question that calculates the minimum hops sum of communication (or the minimum distance sum of communication) from arbitrary nodes to the nearest center ring node in the network."
"data storage is the first step in data processing in the wireless sensor network. according to the different data storage strategies, storage can be divided into centralized storage, local storage, and distributed storage [cit] . as for the analytical modeling of storage schemes and query strategies to increase the lifetime of the network and decrease time delays with respect to data access, there are two interesting categories studied: tree-based and grid-based protocols. the tree-based and in terms of the nodes located in it, when each lattice consists of multiple sensor nodes, only one node is working, while the others are sleeping. each node knows the situation of the deployment, such as the scope of network, neighbor nodes, and so on. (3) each node can be considered as a data query user. they can send a data query sentence to the nearest center ring node at any time, and will not leave the location out of the communication range before the end of data query."
"in order to prolong the lifetime of the network and to reduce the time the user needs to obtain the data, center ring nodes are selected by balance and optimization between data query time and the energy consumption."
"combined with the above analysis, the distance sum d ir from the nodes in the internal area to the center ring nodes is shown as equation (3): figure 5b shows the case where ir-1 contains the center node:"
"when the sensor nodes generate data, the location of the storage node is determined based on ght according to the network area partition. according to the system model in the second section, the location of the k-th storage node is shown as equation (5):"
"the location of the center ring node for metadata will affect the data access performance, as well as the energy consumption. the center ring nodes are selected according to balance and optimization between data query time and the energy consumption of the network. this section discusses the data storage and query time performance, and the network energy consumption."
t sque represents the interval from user sending query sentence to the center ring node receiving it. t rqn represents the interval from center ring node sending query result to the user receiving it.
"the shortest distance sum from all nodes in ir-1 to the center ring nodes is recorded as d ir-1, and the shortest distance sum from all nodes in ir-2 to the center ring nodes is recorded as d ir-2 ."
"in order to decrease the delay of data access and the total energy consumption in uwsn, the framework and flow of the dagm is shown in figure 2 . the framework is divided into three parts, that are data storage, data map and data query. figure 2a shows the data map that is the core of dagm. the data map includes building the center ring for storing metadata, for metadata diffusion, and for synchronization, as well as data query sentence processing, data transmission routing generation, and so on."
a simple enhancement to the object storage api would solve this problem. a client could supply a nonce with the head request which the object storage service would prepend to the object before hashing it [cit] . in this way any of a range of integrity check technologies [cit] could force the object storage service to prove that it currently contains a good copy of the object without the need to retrieve it.
", where m represents the number of center ring nodes. therefore, the center ring node k is selected from the nodes with the minimum distance sum (the shortest path) among all nodes t. thus, we can obtain equation (1):"
"however, the ght only saves the data and does not generate metadata; there is no process of diffusing and synchronizing the metadata. therefore, the time of dagm in the data storage is more relative than that of ght (shown in figure 6a, state 1 is 33 s to 20 s; state 2 and state 3 are both 13 s to 7 s). at the same time, in the global and partial queries, the experimental data storage times have the same results. (b) in terms of the data query time, this is the interval from the user sending a query sentence to the user receiving specific application data or a query result. in dagm, the corresponding storage node is purposely located through the data map, so whether it is the global query or a partial query, the storage node will be checked out first. then, the storage node will send the specific application data through data transmission routing to the user. thus, the experimental data query times show the same results for a global query and a partial query. however, in ght, the query sentence is sent to all storage nodes through ght routing in the network. the specific application data that is required returns along the same routing. hence, data query delay of dagm is much better than that of ght ( . this is because the distance between senor nodes is small and there are more connectivity paths between nodes, but the time of data access in state 1 is generally a little more than in states 2 and 3. the best of time performance is in state 2, because of its data transmission with fewer hops. however, because of the large communication distance, energy consumption is also the greatest."
"this makes the possible technical architectures by which lockss boxes could use cloud storage irrelevant. nevertheless, we have implemented and tested the most costeffective architecture for a lockss box in the current amazon environment, and have made this implementation available for use by anyone who disagrees with our conclusions."
"specifications table subject provide detailed information on the feature selection and model training results, making it easy for other researchers to study feature importance, and to understand and reproduce the results in the manuscript."
"in this case, all storage nodes must be checked out, and then the result that the network has no application data will be given. thus, the query time does not change in ght. in dagm, if the center ring node does not find the content abstracts and storage location abstracts of the data that form query sentence through the metadata, it directly returns the result of query failed. therefore, the data query time becomes much shorter (shown on the left side of figure 7a, from 108 s to 12 s in state 1). 2) when it comes to energy consumption, in dagm, there are no the routing and specific data requests sent to data storage node, and there are also no specific application data transmitted to the user. therefore, the energy consumption of the network is of decreased amplitude (in state 1, shown in right side figure 6b,c and figure 7, global query and partial query drops from 578 j to 144 j). in ght, the energy consumption of the network does not decline."
it is important to understand the interfaces between these components. their performance characteristics determine the technical viability of the various system architectures possible for lockss boxes in the cloud:
"in this paper, quantum computing is introduced on basis of artificial fish algorithm, and the improved quantum artificial fish algorithm is put forward. in the early stage of the artificial fish optimization, following behavior and preying behavior of artificial fish are added in the late stage, which improve convergence speed and optimization precision and enhance the diversity of population by implementation of the mutation after each iteration. simulation results show that compared with gafsa, bafsa, and qafsa, iqafsa is more suitable for solving high dimensional and complex nonconvex programming. so the algorithm is more convergent. finally, iqafsa is applied to optimal configuration of dg in distributed network system and simulation results indicate that iqafsa has great validity and feasibility. the theory research of iqafsa is still in its infancy. in the future, using more test functions with higher dimension to verify performance of the proposed method is necessary. convergence and stability of the algorithm remains to be proven so that more detailed work and further research from the theory are needed to expand."
"step 4. on the basis of the above adjustment strategy, firstly use the quantum revolving gate ( ) and the corresponding rotation angle adjustment policy to update quantum bit of artificial fish ( + 1), and perform preying behavior and following behavior for artificial fish through the fish evolution process. finally, new artificial fish is gotten."
"quantum computing that is different from the traditional calculation model of classical physics has incomparable advantages such as quantum super parallelism and exponential storage capacity [cit] . the combination of quantum computing and intelligent optimization algorithm injected new life into the intelligent optimization computing source, by using quantum computing in a new mode of representing and processing information, and so forth [cit] . intelligent optimization algorithm can be designed from another angel to enrich the theory of intelligent computing and improve the traditional method of intelligent search performance as a whole."
"in the process of the experiment, we assume there are no retransmissions in data storage, metadata diffusion, query data and specific application data transmission. the delay of processing data in the sensor node is less than the delay of the data transmitted, therefore, it is neglected. in the process of data query, there are two kinds of different query methods, which include global query and partial query in the network. in the partial query, one area is randomly selected in the network. in the case of existing application data meeting the requirement, the experiment results of the three methods are shown in figure 6 . in particular, in dagm the storage time not only includes the time that the sensor node sends the data to the storage node to be stored, but also includes the time where the storage node sends the metadata to the center ring and the diffusion is completed."
"the update of the quantum bit needs to make use of quantum gates which include the xor gate, the controlled xor gate, hadamard transform gate, and the revolving gate. the quantum revolving gate [cit] was adopted in this paper to change its phase by interference and the basic state probability amplitude. the revolving gate is described as follows:"
"(1) the dagm system model is proposed so that the user (observation node) in an uwsn can access the specific application data and events it is concerned with as much as possible. the most challenging and difficult problems of dagm have been analyzed; (2) the framework of dagm is established. according to the shortest average query path of the network, the center ring structure composed of nodes is established for storing the metadata, and then the data map of the network is formed. in the meantime, data storage and query mechanisms are established to aid user access-specific application data and events quickly; (3) data storage delay, data query delay, and a network energy consumption calculation model of dagm are established, providing a method for the analysis of dagm performance."
"the total energy consumption of the network e total includes data storage energy consumption e dstro, metadata diffusion energy consumption e mdsyn, data query energy consumption (e dque ) and specific application data transmission energy consumption (e dtra ). therefore, we can obtain equation (9):"
"in order to verify the feasibility and effectiveness of iqafsa, four typical function simulations are proposed compared with qafsa, bafsa, and gafsa in the literature [cit] :"
"represent the lattice id numbers of nodes. the data transmission routing is calculated out by gpsr routing protocol when the sensor node gets the location of the storage node. then, the data is transmitted to the storage node. in processing data transmission, each node will send data to the neighbor node that is the nearest from the storage node. when a node's next hop is itself, this node can be considered a storage node, which stores data locally and generates metadata. the metadata is composed of data content abstract (ca) and data storage location abstract (sa). the ca (vn, vd, sn, gt) indicates the data attribute name, attribute values, generation node identification number and generation time, respectively; and sa (gn, vn, sl) indicates lattice id numbers of storage node, data attribute names and storage node identification number, respectively. after that, the metadata is sent to the center ring node that is the nearest from the storage node. when the metadata is transmitted to the center ring node, the transmission is stopped. the data storage process is described as algorithm 3:"
"in the process of data storage, metadata diffusion and data query, the delay includes node processing data time and data packet (dp) transmission delay. the node processing data time mainly includes delay in store-and-forwarding of the data, which is much smaller than the data packet transmission delay. data packet transmission delay is the sum of the sending time and transmission time. data transmission delay is mainly determined by transmission bandwidth b t, so the greater the bandwidth is, the less sending time is spent; transmission time depends on the velocity v t and distance r t of the transmission medium in signal carriers. for the data packet in p bytes, m represents the number of retransmissions, and the transmission delay is"
"we repeat our earlier [cit] proposal for a simple extension to the current s3 api that would greatly improve the suitability of s3 and similar services, such as private cloud implementations, for digital preservation."
"the experimental lockss box was empty at the start, and was ingesting content during the experiment. during week 11 it finished ingesting, as reflected in the decrease in bandwidth charges, primarily for writing the new data to ebs and the snapshots to s3."
"in order to decrease the delay of data access and the total energy consumption in uwsn, the framework and flow of the dagm is shown in figure 2 . the framework is divided into three parts, that are data storage, data map and data query. storage, diffusion and synchronization of metadata, as well as query sentence and specific application data. based on the above analysis, the core problem of the dagm is to determine the location of the center ring nodes based on the minimum delay from the sent query sentence to received specific application data by the user, and minimizing the energy consumption of the network. the data transmission in the network is completed through multiple hops. assuming the number of hops on sending query sentence and on querying results returning is same in each node, k denotes the center ring node; i presents other nodes exception the center ring node; and d(k, i) shows the number of hops from node i to the center ring node k that is the nearest from i. thus, the distance sum (the hops sum, that is, the path length) from all nodes to their nearest center ring nodes is,"
"comparing the economics of local storage, which has both purchase and running costs, with cloud storage, which has only running costs, involves comparing expenditures through time. the standard technique for doing so is discounted cash flow (dcf), which allows a series of incomes or expenditures through time to be reduced to a net present value subject to an assumed constant interest rate."
"t smq represents the interval from center ring node sending specific data requests and data transmission routing to the data storage node. t dt represents the interval from the storage node-transmitted specific application data to the user who receives it. generally, t smq is much less than t dt ."
"a lockss box consists of some storage holding the preserved content in a repository, and a daemon, software running in a computer that accesses the repository to perform the technical preservation functions described in the oais reference model [cit] including ingest, dissemination. integrity checking and management as part of a peer-to-peer network [cit] ."
"an important purpose of uwsn is to make the user (such as the observation node) obtain the required data as fast as possible, and to prolong the lifetime of the network. in dagm, there are three stages from the sensor nodes generating data to the user getting data, that are: generation and storage of data, diffusion and synchronization of metadata, and query and receipt of data. in dagm, to make the user query and receive data of different granularity and quality as fast as possible, at first, when sensor nodes generate data, the uwsn will not only store data, but it will also generate metadata and diffuse it to the center ring. secondly, in the user query data, the uwsn not only can send the query sentence to the nearest center ring node, but it can also transmit the data with a data transmission routing strategy. this can not only reduce the delay in data access, but also save network energy and prolong the network lifetime. when it comes to the time performance of network, it contains two parts. one part is the time of data storage and the metadata diffusion, and the other is the interval from user sent out the data query sentences to who receives the specific application data. similarly, the network energy consumption also includes two parts. one is the energy consumption of data storage and metadata diffusion, and the other is energy consumption of the data query and specific application data transmission."
"thus, for figure 5b, the shortest distance sum d ir from the nodes in the internal area ir to the center ring nodes is shown in equation (4):"
"distributed generation (dg) is small-scale and radial generating facilities, which are placed in the vicinity of the load, delivering electricity to consumers independently. compared with the traditional and centralized power, dg has many advantages [cit] such as being near to the center of the users and having high energy efficiency and lower cost of construct. studies show that dg grid connection has significant impact on the distributed network, including power flow, voltage profile, system losses, and reliability, the extend of which has something to do with the location of dg intimately [cit] . on the one hand, the suitable installation position and capacity can improve the voltage quality effectively and reduce the active loss; on the other hand, unsuitable configuration turns out to be just the opposite wish and threat to the safe and stable operation of the power network. there are many approaches for deciding the optimum sizing and sitting of dg units in distribution systems. some of them rely on conventional optimization methods and others use artificial intelligence-based optimization methods [cit] ."
"a study of access patterns to data in digital archives [cit] showed that the majority of read operations were for integrity checking. this is also true of lockss boxes. checking the integrity of content in current cloud storage systems is problematic. although it is possible to use the http head operation to ask the service for the checksum of an object, this is not in fact useful as an integrity check [cit] . although one might hope that the service would re-compute the hash for each head request, it need not do so. the service could respond with a correct hash to this request by remembering the hash it computed when the object was created without ever storing the object."
"for the combination of workloads (1, 1), we observe from fig. 4 (b) that it has the lowest event and switch number. however, this workload combination has high contention on cpu resource. thus, the poor throughput performance shown in fig.4 (a) should come from other factors. from fig. 4 (c), we see that the combination (1, 1) processes the i/o communication more efficiently than the combination of (100, 100), it has three times more pages exchange during each execution when the workload rate is high. before the workload rate reaches 80%, the workload combination of (1,1) has higher pages exchange per execution compared to that of combination (1, 100). thus, we can infer that the throughput interference of combination (1, 1) may be caused by the fast i/o processing between guest domains and driver domain. this conclusion can be further validated using the experimental results shown in fig. 4 (f) and fig. 4 (i) . we see that the cpu waiting time of dom0 and guest domains are both the longest in comparison, approximately achieving 17% and 34% respectively, i.e., the cpu run queues are crowed waiting for more vcpus to be put into execution state. all the vms in the combination of (1, 1), including dom0, are acquiring cpu resource. the credit scheduler used in our experiments is configured with equal weight for all vms, i.e., all vms should be dispatched with the same cpu share, thus we could see that the cpu utilization of dom0 and of guest domains have similar trend and share (100% / 3 ≈ 33%) when they all demand for cpu resource. to achieve high throughput, all the vms in (1, 1) combination need to perform fast i/o page flipping, which results in higher interference in cpu scheduling, and the lower throughput performance shown in fig. 4 (a) compared to (1,100) ."
"according to george boole [cit], boolean algebra is an algebraic structure with a set of values, two binary operations \"+\" and \".\" over the values in the set and proof of huntington [cit] postulates. later on, shannon [cit] introduced a two-valued version of the algebra to represent properties of the switching circuits. in a twovalued algebra, the set of values include elements 1 and 0 and the two binary operations are conjunction (logical and) and disjunction (logical or). in addition, there is a unary operation called negation or complement (not)."
"boolean functions or logic circuits described as boolean functions are normally manipulated through the error-prone pen-and-paper method using the basic theorems and postulates of boolean algebra. to the best of our knowledge, neither the algebra nor the mathematical manipulation process has been formally defined in an interactive theorem prover and hence their correctness cannot be checked mechanically."
"the advent of the information age, also commonly known as the digital age, has made a profound impact on health sciences. vast amounts of datasets now flow through the different stages of healthcare organizations, and there is a major requirement to extract knowledge and employ it to improve these centres in all respects."
"to get simplified logic circuit, the number of literals and terms must be reduced in the description of the circuit. e algebraic manipulation method is one of the most popular methods used in textbooks [cit] for this purpose. is kind of manipulation is informal and hence is error prone. e formal structure including the postulates and basic theorems defined in previous sections can be applied, as in the informal algebraic manipulation, using coq proof assistant. e main advantage of mechanizing algebraic manipulation in proof assistant is that proof scripts can be mechanically read, checked, and maintained."
"e wolframalpha computational engine [cit] translates logical function as input to a truth table and different minimal forms. moreover, the tool generates a venn diagram and logic circuit for the input function. e wolframalpha engine has been included as boolean algebra calculator by the company tutorvista. another tool [cit] was developed to minimize boolean functions using karnaugh maps [cit] . e function can be entered as a sequence of notations or as a truth table (up to six variables)."
"uavs or drones are like flying computers with linux or windows operating system, flight controllers, main boards, memory units, and thousands of lines of programmable code. formal methods, and in particular our framework, can be applied to the analysis of uavs in different ways. as mentioned, it is a complex computer system, and hence formal techniques can be used to verify the operating system, protocols, logic components, memory units, and any algorithm used in the uav system. as arithmetic and logic components (e.g., adder and comparator) and memory (e.g., rom) are the main components of any typical computer (including uavs), we apply our formal framework to analyse a memory and an adder circuit. in this section, functional equivalence and reversibility properties of few combinational circuits are proven. furthermore, it is also proven that all the described circuits hold the basic reliability property called type-safety. e formal model of the combinational circuit in listing 16 has been simplified following the well-known simplification method k-map to circuit in listing 18; however, it is not guaranteed that the circuits are functionally equivalent. in other words, there is no mathematical guarantee that the k-map simplification process has not altered the original behaviour of the circuit. e initial design built from the truth table specification serves as the \"golden\" or \"reference\" design. to prove the simplified circuit is functionally equivalent to the \"golden\" design, the individual functions are proven to be equivalent in eorem 1 (this theorem corresponds to a set of eight lemmas equiv_oo′ 0-equi-v_oo′ 7 in coq script.). functional equivalence of the two circuits is checked in eorem 2. e theorem states that the circuit before simplification is equivalent to the circuit after simplification, which guarantees that the simplification process preserves the functionality. proof: is theorem is proven using case analysis on the boolean variables used. e coq proof of this theorem is listed at our github repository at https://github.com/ wilstef/booleanalgebra. eorem 2 (functional equivalence of circuits): e combinational circuit c described in listing 16 and its simplified version c s in listing 18 are functionally equivalent. more formally, c ≈ c s, where c s � s kmap (c) and s kmap is k-map simplification operation."
"e solver gets number of minterm indices (separated by spaces) and returns a simplified function. all these tools manipulate circuits at gate level; however, none of them have formal foundation and hence the manipulation of circuit designs cannot be proven correct."
"hardware, software, and communication networks altogether make systems operating in the cyberspace such as unmanned aerial vehicles (uavs). uavs operating in an uncertain, potentially hazardous, remote, and dynamic environment are extremely important but challenging to be reliable, robust, and secure. ese flying vehicles are currently being used in mission-critical [cit], industry-oriented [cit], and internet of ings (iot) [cit] applications, to name a few. to meet the high standards of safety and reliability of uav-based mission-critical [cit] or iot [cit] applications, the uavs in such systems must be studied and analysed using rigorous and formal techniques. failure or unauthentic use of software and hardware systems of air vehicles can lead to human losses [cit] and strategic losses [cit] . uav-based human environment can be manipulated and controlled by a remote attacker using attacks such as sensor input spoofing attack [cit] . military uavs have been and are currently the favourite target of attackers to gain cyber power [cit], which has recently led towards a drone war [cit] ."
"e boolean algebra defined in section 3 is extended and tailored towards combinational circuit definitions. to begin with, the set of notations in listing 3 is extended with symbol ⊕ (listing 4) for xor operation with same precedence as. (operation prod). e formal definitions of logic operations sum, prod, xor, and not model the basic logic components or, and, xor, and not gates, respectively. after formally modelling these basic logic components, they are combined together to form combinational circuits. a combinational circuit is defined as a list of boolean (functions) values (listing 5)."
"let dom 0, dom 1 … dom n be the virtual machines running on the same host, where dom 0 is the driver domain. suppose that dom i is serving workloads i, we define the maximum throughput of dom i as t i . we use b i to denote the maximum throughput of dom i in basecase scenario where n equals 1, i.e., only driver domain and one guest domain are hosted on the physical machine. then, the combined normalized throughput scores is defined as follows: (table i ). to present the results clearly, we denote each combination of the two workloads running on vm1 and vm2 in a tuple of two elements, with the first element x denotes xkb file retrieving from vm1, and the second element y is the ykb file retrieving from vm2. for example, the expression (1, 30) says that vm1 is severing 1 kb file workload and vm2 is severing 30 kb file workload. the expression (1, idle) refers to the case where vm1 is serving 1kb file workload and vm2 is idle. from fig. 3, we observe some interesting facts regarding performance interferences. first, the combination of workloads (1, 100) achieves the best performance with its combined throughput score of 1.58. given that 1 kb workload is cpu bounded and 100 kb workload is network bounded, this combination clearly incurs the least resource contention compared to other combinations. similarly, the workload combinations of (1, 50) is the next best pairing for the similar reason. the workload combinations of (30, 100) and (50, 100) offer better combined throughput than the worst combination of workloads (1, 1) and (1, 10), which incurs highest resource contention. our observations are consistent with the xen i/o architecture presented in previous section ii. current xen i/o mode uses the privileged driver domain to provide better security isolation and fault isolation among vms. although this mode could prevent buggy drivers and malicious attacks successfully, the driver domain may easily becomes the bottleneck: when a vm wishes to get accesses to underlying hardware or communicate with others, all the events have to go through driver domain and hypervisor layer. this is supposed to cause control or communication interferences among vms. the multiplexing/demultiplexing of bridge and i/o channel may incur memory page management interferences, such as packets lost for high latency, fragmentations and increased data coping overhead. in addition, we also believe that the default credit scheduler may have some impacts on the overall performance of running multiple vms on the same hardware platform. this set of experiments also indicates that although performance interference of network i/o applications in virtualized cloud environments is unavoidable given the xen i/o architecture and the inherent resource sharing principle across vms. in-depth understanding of the number of key factors that cause certain types of resource contentions and thus performance interferences is critical for both cloud service providers and cloud consumers."
"when (model of) a logic circuit is transformed or mathematically manipulated (most often for optimization purposes), it must be guaranteed that the transformation does not alter the desired behaviour of the circuit. in this paper, a formal framework for describing and verifying boolean functions and logic circuits at gate level was defined in the coq theorem prover. to demonstrate the significance of the framework, basic theorems of the boolean algebra and the duality principle were proven. furthermore, multiple basic hardware components were described in the formal notations and functional equivalence and reversibility properties were verified. our formal developments can be used to describe other logic components used in critical systems and can be formally proven correct using the coq theorem prover tool. as a future work, we plan to build a translator to automatically translate circuit designs as boolean functions in a natural style to the formalized (in coq) boolean algebra. furthermore, to complete a formal electronic design flow, we intend to build a logic synthesis tool for translation from formal register-transfer level representation of the circuit (e.g., in veriformal, [cit] ) to gate-level representation as boolean functions. saud university, saudi arabia, for partially funding this"
"first, we design a bcd adder using binary adders without look-ahead carry generator. e 4-bit parallel full adder fa4bit is defined as a function in listing 19. it applies the full-adder function fa to the 4-bit pairs of the two 4-bit numbers and input carry (inputs d-a, d-a and e), and returns a 5 bit tuple. e first bit in the tuple is the final carry and the next four bits are the sum bits. e binary adder fa4bit is used to implement the bcd adder as shown in listing 20. e let-expressions on lines 2, 3, and 4 calculate the result of the first binary adder, output carry, and second binary adder, respectively. e additional circuitry between the two binary adders is needed to generate the output carry of the bcd adder and to adjust the sum of the first binary adder to bcd sum."
"formal verification using proof assistant is very tedious and requires expertise; however, many researchers have recently used proof assistants to build formal languages and frameworks for hardware verification [cit] . proof assistants include benefits of both manual and automated theorem provers and are more powerful and expressive [cit] . our framework allows defining boolean functions and combinational circuits in a natural style similar to that used in many popular textbooks [cit] and most of the proofs can be carried out easily by applying coq's destruct tactic."
"to operate on numbers, we define a recursive function add (lines 5-9) to add values of type nat. e function returns the second argument m if the first argument is o; otherwise, it returns succ (add n′ m). a lemma add_n_o, where add n o � n holds for any value of n, is stated and proven in listing 1 (lines [cit] . e proof of this lemma has been carried out by applying induction on the construction of the first argument n. while carrying out the proof, the coq tool was interactively guided by giving proof commands called tactics (lines [cit] . many other properties of the add function, such as commutative and associative properties, may also be stated and proven mechanically using the coq proof assistant."
"the second class of methods includes classification systems which are most useful for the early diagnosis of diseases for which there is currently no definitive diagnostic test. computer-aided screening for alzheimer's disease based on neuropsychological rating scales is carried out by a hybrid approach which combines rough sets, genetic algorithms, and bayesian networks, and an expert system for multicriteria decision support in psychotic disorders is proposed. clinical decision support systems are not limited to diagnosis, as noted in the osteoporosis case, where the system can also recommend treatments and assess the fracture risk."
"to design fault-tolerant and secure air vehicles, conventional design and testing methods must be augmented with more robust and reliable tools and techniques called formal methods [cit] . formal methods have been successfully used to verify collision avoidance between uavs [cit], certify uavs within civil airspace [cit], and design resilient uav systems [cit] . as the security and reliability of uavs depend upon the security and reliability of individual components, their correctness must be ensured in the design phase. in addition to uavs, the formal framework presented in this paper is equally applicable to any other computer system such as sensor networks [cit] and hardware components, such as flash memory, of such systems [cit] . in this paper, we address the formal specification and verification of hardware (logic) components, such as memory units and adders, designed at the logic gates level using boolean algebra. is makes our formal model extremely important in the domain of critical system design in general and in uavs in particular."
"i. introduction virtualization technology [cit] offers many advantages in current cloud computing environments by providing physical resources sharing, fault isolation and live migration. virtualization allows diverse applications to run in the isolated environments through creating multiple virtual machines (vms) on a shared hardware platform, and managing resource sharing across vms by virtual machine monitor (vmm) technology [cit] . although vmms (hypervisors) have the abilities to slice resources and allocate the shares to different vms, our measurement study shows that applications running on one vm may still affect the performance of applications running on its neighbor vms. in fact, the level of interferences mainly depends on the degree of the competition that the concurrent applications running in separate vms may have in terms of shared resources. we argue that the in-depth understanding of the possible performance interferences among different vms running on a shared hardware platform is critical for effective management of virtualized cloud, and an open challenge in current virtualization research and development."
"mathematical proofs carried out in a human-assisted theorem prover are more organized because of the following reasons: (i) in mechanized theorem prover (such as coq), the proofs can be divided into modules (to make proof handling easy); (ii) lemmas/theorems already proven can be easily invoked and applied; (iii) the coq language of tactics (ltac) can be used to combine complex set of proof commands (tactics) in a single tactic; (iv) the proofs can be read and checked by the computer using a proof checker; (v) a new proof can be opened inside an existing unfinished proof; (vi) large proof scripts can be checked for unproved lemmas just by using a single coq command; and so on. in summary, the proofs in proof assistants such as coq are more organized as compared to pen-and-paper proof methods and the correctness of the proof scripts can be checked by the computer."
"the goal of this special issue is to offer a broad view of this exciting field, the ever-growing importance of which is driven by the increasing availability of data and computational power."
"in order to provide the in-depth understanding of the detailed factors that cause these different levels of throughput interferences, we conduct the rest of the experiments and the results are shown in the remaining figures and plots in fig. 4 . first, let us examine at the combination of two networkintensive workloads of (100, 100). fig. 4 (b) shows the numbers of switches per second when varying the workload rates from 10% to 100%. note that the number of events follows exactly same trend as switches, though the concrete values are different in scale. at 50% workload rate, the combination (100, 100) reaches the maximum switch number, and it starts to drop when the workload rate increases to 60% and it remains almost flat even when the workload rate increases from 60% to 100%. comparing with other two combinations of workloads, (100, 100) has at least twice higher switch cost than (1,100) and (1, 1) under the peak switch costs. this implies that a main source of overhead and throughput interference for the combination (100, 100) may come from the high event and switch costs in vmm and dom0. fig. 4 (c) shows the exchanged memory pages during one cpu execution time. we observe that as the workload rate is getting higher, the combination of (100, 100) has less than four pages per cpu execution duration and it experiences the worst efficiency in i/o pages exchange. practically, heavy event and switch costs lead to a lot more interrupts that need to be processed, resulting in few i/o pages exchange during each execution cycle. fig. 4 (d) measures the dom0 cpu utilization with varying workload rates for three combinations of workloads. we observe that dom0 is busy for being scheduled for cpu processing on each interrupt. in fig. 4 (d), (100, 100) combination uses relatively higher but more stable cpu usage around 35% for the workload rate at 50% or higher. fig. 4 (e) and fig.4 (h) present the block time of dom0 and block time of two guest domains respectively. for (100,100) combination, the block time of dom0 is around 30% for workload rate of 50% or higher, and the block time of guest domains are around 48%, and both are relatively high compared to other two workload combinations. this indicates that vms are frequently blocked for i/o events and waiting for next scheduling. fig. 4 (f) and fig.4 (i) measure the waiting time of dom0 and of guest domains. we observe that the combination of (100,100) has the lowest waiting time in both dom0 and guest domains. this is mainly due to the high blocking time, as it reveals that the cpu run queues are not crowd and could serve the vms much faster. in summary, we conclude that due to heavy event and switch costs in (100, 100), vmm and dom0 are quite busy to do notifications in event channel, resulting in the fact that driver domain needs more cpu resource and guest domains are free and are waiting for i/o events (see fig. 4 (g) )."
"in addition to the combined throughput ratio scores, we also measure the virtualization specific system-level characteristics using the following eight performance metrics to better understand the performance interference of running multiple network i/o workloads in isolated vm environments on a single hardware platform. these system-level metrics can be best utilized to analyze the resource contentions of network i/o workloads and reveal the intrinsic factors that may have been induced performance interference observed. using xen hypervisor system monitor tools, we collect eight system-level characteristics from i/o workloads. these system-level characteristics could reveal the underlying details of i/o performance interference and make vms behaviors understandable. in our design of evaluation, the following eight different workload characteristics are collected:"
"in this set of experiments, we set up our testbed with one vm (vm1) running one of the six selected network i/o workloads of 1 kb, 4 kb, 30 kb, 50 kb, 70 kb and 100 kb for all eight metrics outlined in section iii. table ii shows the results. the value of each i/o workload characteristics is measured at 100% workload rate for the given workload type. comparing with network-intensive workloads of 30 kb, 50 kb, 70 kb, and 100 kb files, the cpu-intensive workloads of 1 kb and 4 kb files have at least 30% and 60% lower event and switch costs respectively because the network i/o processing is more efficient in these cases. concretely, for 1 kb and 4 kb workloads, we can see from table ii, driver domain has to wait about 2.5 times longer on the cpu run queue for being scheduled into the execution state and the guest domain (vm1) has 30 times longer waiting time. however, they are infrequently blocked for acquiring more cpu resource, especially in the guest domain the block time is less than 6%. we notice that the borderline network-intensive 10 kb workload has the most efficient i/o processing ability with 7.28 pages per execution while the event and switch numbers are 15% larger than cpu-intensive workloads. interesting to note is that initially, the i/o execution is getting more and more efficient as file size increases. however, with file size of workload grows larger and larger, more and more packets need to be delivered for each request. the event and switch number are increasing gradually as observed in table ii . note that the vmm events per second are also related to request rate of the workload. though it drops slightly for the workload of file size 30-100 kb, the overall event number of network-intensive workloads is still higher than cpu-intensive ones. with increasing file size of shorter workloads (1 kb, 4 kb and 10 kb), vms are blocked more and more frequently. finally, i/o execution starts to decline when the file size is greater than 10 kb. the network i/o workloads that exhibit cpu bound are now transformed to network bounded as the file size of the workloads exceeding 10 kb and the contention for network resource is growing higher as the file size of the workload increases. in our experiments, the 100 kb workload shows the highest demand for the network resource. these basic systemlevel characteristics in our basecase scenario can help us to compare and understand better the combination of different workloads and the multiple factors that may cause different levels of performance interferences with respect to both throughput and net i/o."
proof: is theorem is proven by applying (rewriting) the eight lemmas already proven in eorem 1. e coq proof of this theorem is listed at our github repository at https://github.com/wilstef/booleanalgebra.
"to demonstrate that our formal framework can also be used to verify properties other than equivalence, we formally verify reversibility of a simple circuit description. to this end, we prove that the circuit circuit defined in listing 24 is reversible. e theorem reversible_circuit states that, for all different (2-bit) inputs, the outputs are different. additionally, this theorem also states that the numbers of inputs and outputs are the same. in this theorem, the lists [w; x] and [y; z] on line 4 in listing 24 model the inputs."
"last but not least, computational intelligence techniques are also useful tools for probing the improvement of drug manufacturing processes, as demonstrated in another proposal where neural networks are applied to drug tablet visual tracking for high speed mass production."
"one of the best-known application fields of these techniques is computer-aided diagnosis (cad). at this point, two types of methods must be distinguished: those which aim 2 computational and mathematical methods in medicine at providing clinicians with enhanced diagnostic procedures that can lead to improved human decisions, and those that intend to provide a second opinion; that is, given a set of signs, they can indicate or confirm one or more possible disease processes."
"in this paper, we have present our experimental study on the performance interference in parallel processing of cpu and network intensive workloads in the xen virtual machine monitors (vmms). we conduct extensive experiments to measure the performance interference among vms running network i/o workloads that are either cpu bound or network bound. based on our experiments and observations, we conclude with four key findings that are critical to effective management of virtualized cloud environments for both cloud service providers and cloud consumers. first, running networkintensive workloads in isolated environments on a shared hardware platform can lead to high overheads due to extensive context switches and events in driver domain and vmm. second, co-locating cpu-intensive workloads in isolated environments on a shared hardware platform can incurs high cpu contention due to the demand for fast memory pages exchanges in i/o channel. third, running cpu-intensive workloads and network-intensive workloads in conjunction incurs the least resource contention, delivering higher aggregate performance. last but not the least, identifying factors that impact the total demand of the exchanged memory pages is critical to the in-depth understanding of the interference overheads in i/o channel in the driver domain and vmm."
"(iv) e performance of the coq theorem prover over our model is evaluated. e rest of the paper is organized as follows. in the next section, the significance of interactive theorem proving is highlighted. e tool coq theorem prover and boolean"
"in this subsection, we focus on studying performance interference of running multiple network i/o workloads in isolated vm environments on a shared hardware platform, our testbed setup. we focus on understanding the impact of running different combinations of workloads of different file sizes on the aggregate throughput."
"e duality principle states that expressions derived from postulates by interchanging the operators (+ and .) and identity elements (0 and 1) are valid. after defining the interchange operations over the boolean expressions, the duality principle can now be stated as a lemma as shown in listing 11. e lemma states that if a part of theorem holds (two arbitrary expressions are equal), then it implies that the second part of the theorem can be derived by changing the identity elements and operators. to check that the duality property can be used in proofs, given the first part of commutative property (x + y � y + x), the second part of commutative property (x · y � y · x) is proven (listing 12) by applying the duality property."
"ere is no guarantee that the principle of duality indeed holds, the mathematical manipulation carried out is correct, or the logic circuits (described as boolean functions) optimized for size, efficiency, and cost-effectiveness using k-map [cit] or tabulation [cit] methods are behaviourally symmetric."
"automated tools, such as model checkers, sometimes stuck due to memory or state explosion problems [cit] and never return. to evaluate the performance of coq tool running over our framework, we tested the coq proof checker to check proof scripts of functional equivalence of boolean functions with multiple boolean variables. e results in figure 3 show that the coq proof checker takes around 12 seconds to check proof scripts for functional equivalence with functions up to 45 variables (over a billion input cases). e formal model developed in coq provides a formal foundation for defining and reasoning about boolean functions and logic circuits using the calculus of constructions behind the coq theorem prover. e following is a list of the major advantages of our formal model: and (models of ) logic circuits can be proven interactively using the coq theorem prover. (iv) all the proofs carried out can be mechanically checked by the coq proof checker using computer."
"other diagnostic enhancement procedures are not related to image processing. in this issue, the most relevant criteria to diagnose guillain-barré syndrome are studied by the partitions around medoids clustering algorithm, which is able to manage both categorical and numerical data since it only requires the distance matrix amongst the samples. the reactivity of blood pressure (bp) to talking episodes is predicted by a hybrid system composed of several soft computing approaches, specifically neural networks, an adaptive neurofuzzy inference system, and support vector machines (svms). this increases the precision of bp measures for clinical and research purposes. screening for prediabetes is carried out by means of a combination of neural networks and svms, which is aimed at early intervention to avoid the serious complications associated with the disease. validation procedures for electroencephalographic (eeg) signal processing by principal component analysis (pca) are also proposed, which is a valuable tool for many neurological studies. eeg signals are also analyzed by mixed-norm regularization for sensor selection in brain computer interfaces. finally, the problem of privacy preservation in self-helped medical diagnosis is addressed, where secure two-party computation in wireless sensor networks is ensured in order to develop a system where the patient inserts a health card into an automated teller machine and obtains a diagnostic report."
"e list \"f 1 x y::f 2 x y::nil\" represents a combinational circuit with two inputs x and y and two outputs defined as two boolean functions. to evaluate each boolean function in the list to a boolean value, an evaluation function eval_cir is defined in listing 6. e function takes a combinational circuit as list of boolean functions and returns a list of boolean values (listing 7). e final requirement of boolean algebra is to formalize the six huntington postulates [cit] and prove the set of all basic theorems. formal definitions of huntington postulates are included in the coq script available from our repository. furthermore, the formal definitions of operation sum, prod, and not are sufficient to reason about huntington postulates and all the basic theorems except principle of duality (discussed in section 5). as the derivation in principle of duality requires operating on the boolean expressions, the principle cannot be defined in the current setting. e existing definitions of expressions do not differentiate between variables and values (identity elements), which is the basic requirement of operation in the duality. to do this, a type exp for boolean expressions is defined as shown in listing 8. e first three constructors (line 2-4) correspond to the three logical operations sum, prod, and not, respectively."
"this special issue presents a compilation of research papers describing novel recent strategies and developments on the use of computational intelligence techniques in medicine. it will be of interest to researchers interested in industry, academics, and also to postgraduate students interested in the latest advances and developments in the field of computational intelligence in medicine. its contents are summarized next."
"in this section, we provide a brief background of xen which is the virtualization platform we use in our measurement study. then we describe the experimental setup, including measurement method and i/o workloads."
"is semiautomatic itp approach requires expertise and skills; however, it has been used in the past for investigating large case studies [cit] . in automated theorem proving, it is hard to get insights when a proof attempt fails, while itp forces the designer to pay attention to even the minor details. is results in understanding the system under study more precisely. e conventional mathematical proofs on pen and paper (informal) are error-prone and difficult to manage large and complex proofs. computer-aided verification, on the other hand, is an effective, efficient, and rigorous way of formal specification and verification. automated theorem provers are widely used in industry as they automatically create proofs without requiring human effort; however, they face problems such as state explosion [cit] . human-assisted theorem provers, on the other hand, require human support to carry out mathematical proofs. a proof engineer guides the theorem prover by providing proof commands and interactively creating proofs. coq is one such popular interactive proof assistant considered for defining our formal framework."
"to generate a bcd adder with look-ahead carry generator, we must first design binary parallel adder with look-ahead carry generator. formal definition of a 4-bit binary parallel adder has been listed in listing 21. e definitions c2-c5 (lines 1-7) calculate the four output carries of the four full adders in the binary adder. unlike the adder fa4bit, every internal output carry is independent of the output carry of the previous full adder. is allows every full adder to produce output sum simultaneously without waiting for the output carry from the previous adder. e definitions s1-s4 figure 2 : block diagrams of two versions of a bcd adder. e bcd adder on the left is with look-ahead carry generator and the one on the right is without look-ahead carry generator [cit] . calculate the four sum bits in terms of the look-ahead carry bits c2-c5 and initial carry e. finally, the 4-bit binary parallel adder with look-ahead carry generator is defined as a function fa4bitla (lines [cit] . as in bcdadder, two instances of binary parallel adder fa4bitla are combined together with the additional circuitry for output carry to form a bcd adder. e formal definition of 4-bit bcd adder with look-ahead carry generator has been shown in listing 22. readers are advised to refer to books [cit] for further details about bcd adder and look-ahead carry generator. e formal definitions bcdadderla and bcdadder model 4-bit bcd adders with and without look-ahead carries, respectively. eir functional equivalence is stated in theorem check_equiv_bcd_adder in listing 23. e theorem states that both adders produce equal outputs for all equal inputs. is theorem is proven using case analysis on the input variables. e formal proof of eorem check_equiv_bcd_adder demonstrates that our framework can effectively be used to check equivalence of combinational circuits other than memory circuits."
"e look-ahead carry generator is a small-scale integrated circuit added in front of the 4-bit adder to make it fast. e transformation avoids propagation delay; however, no formal proof that the functional properties of the adder are preserved is provided. in this section, we formally prove that two implementations of the bcd adder (figure 2), one with and the other without look-ahead carry generator circuit, are functionally equivalent."
"in order to formally reason about boolean algebra and verify the correctness of digital components described at gate level as boolean functions, we define a formal model of the boolean algebra using the calculus of construction in theorem prover coq. e formal model of boolean algebra defined enables one to define and prove all the basic theorems as well as the principle of duality. we extend our formal model of the boolean algebra to represent combinational circuits. to assess the efficacy of our formal model, the proof facility of coq is used to carry out proof of correctness of gate-level combinational circuits and reason about boolean algebra. among other numerous advantages (section 2) of computer-aided verification using interactive theorem prover are the following: (a) all the formal definitions and proofs can be defined in the computer, and (b) the correctness of the proofs can be automatically checked by the computer [cit] . e formal approach for checking correctness of boolean functions and digital circuits is described in figure 1 . formal models of the digital circuit under verification and the properties of interest as theorems are fed into an itp engine (coq system, in our case) and a formal proof that the (model of the) circuit holds the properties is carried out interactively. a proof engineer guides the tool by providing it proof commands, which becomes part of the proof script if accepted by the tool. e correctness of the proof script is automatically checked using the tool. e formal verification in coq using the proposed formal model is demonstrated in section 6 by proving equivalence, reversibility, and type safety properties of multiple circuits. in this context, the major contributions of this paper are the following:"
"in this group of experiments, we use the representative cpu bounded workload of 1 kb file and the representative network bounded workload of 100 kb file as the main workload combination for our analysis. the reason we choose this combination is primarily because this combination exhibits the best combined throughput score in table i . this motivates us to understand the key factors that impact on their high combined throughput score. fig. 4 shows the set of experiments we have conducted for throughput interference analysis on three combinations of workloads running concurrently on the setup of two vms: (1, 1), (1, 100) and (100, 100). we use the tuple (1,100) to denote the workload of 1 kb running on vm1 and the workload of 100 kb running on vm2. also we use the notation of (1, 100)_1kb to denote the measurement of workload 1 kb on vm1 in the graphs. we vary the workload rates in x-axis and measure the different performance metrics and plot them on yaxis. to get a clear overview of curves on the same scale, the rates of different workloads in fig. 4 were converted into the percentage of maximum achieved throughput in basecase. for example, for 100% workload rate of 1 kb workload in fig. 4, [cit] req/sec in table i is used. similarly, for 100 kb workload, the 100% workload rate in fig. 4 means the maximum request rate of 112 req/sec is used recall fig. 3, we have shown that the combinations of workloads that compete for the same resource, either cpu or network, stand a good chance of performance degradation compared to the basecase in table i . however, two workloads that exhibit distinct resource need, such as one cpu bound and the other network bound, running together gets better performance compared to the one vm basecase. the following analysis will provide us some in-depth understanding of the key factors that lead to different performance interferences among different combinations of applications running concurrently in multiple vms fig. 4 (a) illustrates the throughput performance of the three workloads combinations: (1, 1), (1, 100), (100, 100). we observe that 1 kb workload in (1, 100) combination reaches around 1560 req/sec, which is twice the achieved maximum 1 kb workload throughput of 790 req/sec in (1, 1) combination, even though it is still 18% [cit] req/sec, the maximum throughput in basecase. for (1, 100) combination, 100 kb workload on vm2 achieves the throughput of 85 req/sec at 100% workload rate, which is 76% of maximum throughput of 112 req/sec in basecase and 52% higher than the throughput of 56 req/sec for 100 kb workload in the combination of (100, 100). note that the combination of (100,100) causes high network resource contention and saturates the host at 50% workload rate (see the enhanced chart embedded in fig. 4) ."
"to demonstrate that our formal framework is fit for mechanically checking correctness of algebraic manipulation, proof of the theorem absorption_sum has been listed in listing 13. is proof is carried out by applying (using the rewrite tactic) the postulates and theorems."
"among the most recent tools is 32x8 [cit] which has been built for logic circuit simplification. it accepts a function (up to eight variables) in the form of a truth table and returns a karnaugh map, boolean function (as sum of product or product of sums), truth table, and logic circuit for the input. lean and marxel developed a solver, qmsolver [cit], based on quine-mccluskey algorithm for simplification of boolean functions."
"e rules for the first operation + (sum) are defined as function sum on lines 3-7 in listing 2. using pattern matching, the function sum gets two values of type bool and returns value false if both input values are false; otherwise, it returns true."
"management and planning represent the other sections of healthcare organizations that can also benefit from the applications of computational intelligence. particle swarm optimization can be employed to enhance blood assignment in blood banks, whilst social network simulation can help to develop effective preventative and interventional strategies for aids epidemics."
"boolean algebra is one of the most widely used mathematical techniques to model and analyse logic circuits. in the electronic design flow, the initial circuit design is often described at system level in high-level languages (e.g., matlab and c) and translated to register-transfer logic (rtl) representation in a description language (e.g., verilog and vhdl) using high-level synthesis (hardware compilers) tools. e rtl representation of the circuit is transformed to gate-level representation (often as boolean functions) using logic synthesis tools, which is finally fabricated to produce physical layout of the circuit. e gate-level representation described as boolean functions is commonly used in techniques and frameworks [cit] and in classrooms for the design and analysis of simple logic circuits [cit] ."
"e rules for the second binary operation . (product) are defined in function prod (lines 9-13, listing 2). is function returns true only if the two input values are true; otherwise, it returns false. a unary operation. complement on elements of boolean set is defined in the function not on lines 15-19 in the listing. given one value as the input, not returns the other value. among these operations, the operation ¬ has the highest and + has the lowest precedence."
ere are a number of other tools developed specifically for boolean functions manipulation. [cit] developed an online database of boolean functions. eir tool can be used to check different properties of a boolean function and convert between different representations.
"intelligent computer systems provide support to health professionals involved both in the medical and managerial contexts. amongst these systems, computational intelligence approaches have gained increasing popularity given their ability to cope with large amounts of clinical data and uncertain information."
"arithmetic component, such as adder, is an integral part of the arithmetic and logic unit (processor) of any microprocessor-based system. to design energy-efficient and fast components, their design description (normally in a description language) is often transformed and mathematically manipulated. however, it must be ensured that such transformations do not alter the intended behaviour of these components. is surety is even more necessary when the (1) lemma abroption_prod: ∀ x y, x * (x + y) � x."
"is proof wireless communications and mobile computing script is a mechanized version of the informal proof of eorem 6 (a) in the textbook [cit] . proof of the second part abroption_prod of the same theorem is believed to hold by the duality principle. is principle has been proven to hold in the above section. every postulate or basic theorem has two parts, where one is the dual of the other. is is interesting to show that, given stepby-step proof of one theorem, proof of the other (dual) part can be carried out by applying the dual of postulate/theorem applied at the corresponding step. e proof of the theorem abroption_prod (dual of theorem absorption_sum) has been listed in listing 14. e postulates/theorems applied on lines 5-9 in both proofs are the dual of each other."
"using boolean functions in the two-valued boolean algebra, logic circuits can be modelled. e function f � x · y models a logic circuit where the and gate represents the product operation and the not gate represents the complement operation. when logic circuits are represented with functions, they can be manipulated and reasoned about using tools and techniques developed for boolean algebra. for example, the algebraic manipulation steps using postulates can be written down in and checked using coq tool (see section 5)."
"by analyzing the performance interferences in the combinations of (100, 100) and (1, 1), we understand that the frequently i/o memory pages exchange of (1, 1) leads to cpu contention among vms, and the combination of (100, 100) incurs higher event and switch overheads in vmm and dom0, leads to high level of network contention. in comparison, the combination of (1, 100) founds the balance to achieve higher throughput with increasing workload rates. concretely, comparing with the combination (1, 1), dom0 and vm1 in (1,100) experience blocked state infrequently and shorter waiting time to be allocated more cpu resource, finally 50% lower in vm1 (see fig. 4 (e) and fig. 4 (h) ). comparing with (100, 100), in (1,100) combination, vm2 is blocked frequently and waiting longer to reduce the event and switch overhead (fig. 4 (f) and fig. 4 (i) ). finally, the i/o page exchanges per execution becomes more efficiently under high workload rate (fig. 4 (c) ) and dom0 is better utilized."
"many methods belonging to the first type correspond to image processing algorithms. indeed, the automated segmentation of magnetic resonance images (mri) of the brain is employed to determine pathological regions and to plan image-guided surgery. on the other hand, highresolution computed tomography has become a standard tool to diagnose diffuse lung diseases, and sparse representations can help to identify abnormal patterns. segmentation is also useful at the microscopic scale, where intracellular calcium variation can be measured by a marked controlled watershed transform. additionally, the emergence of a number of differing medical imaging techniques brings our attention to the problem of fusing these images. an approach based on the nonsubsampled contourlet transform, which is a directional multiresolution image representation method, is also proposed."
"shannon's two-valued version of the boolean algebra is defined in coq theorem prover as described in listing 2 [cit] . e first part of boolean algebra, a set of two elements, is represented with coq type bool defined using the coq keyword inductive as shown in listing 2 (line 1). e two values of the boolean algebra defined are true and false."
"e formal definitions in section 4 enable one to formally reason about both boolean algebra and combinational circuits described as boolean functions. is section contains proof of duality principle and demonstrates with examples that our formal setting mechanizes the informal algebraic manipulation used in textbooks on digital logic design. furthermore, all the basic theorems and postulates have been defined and proven in coq and given in the coq script available from our repository."
"despite achieving better request throughput and net i/o in the combination of (1, 100), we notice that 1 kb workload in this combination gets \"priority\" treatment while leaving 100 kb workload blocked more often and waiting longer. to understand this phenomenon, it is worthwhile to discuss the details of cpu run queues in credit scheduler. the default credit scheduler is configured with equal weight for each vm and cap is 0, which means it is working in the work-conserving mode attempting to share the processor resources fairly. all the vcpus in the cpu queues are served in the first-in, first-out manner. additional, when a vm receives an interrupt while it is idle, the vm enters the particular boost state which has a higher priority to be inserted into the head of run queue for the first cpu execution. this mechanism prevents the long waiting time for the latest active vm by preempting the current running vm. however, the even priority shares of processor usage remains a problem for network-intensive workloads here. consider the combination of (1, 100), because of the file size of 1 kb is the shortest, it could finish each request and enters the idle state faster (most infrequently blocked in fig. 4 (h) ). finally, hypervisor makes the decision to put the vm1 in the head of run queue frequently. this makes the 1 kb workload have higher priority. thus, the effects of credit scheduler should be considered in virtualized cloud environments as it is making positive contributions to cpu-intensive workloads while treating network-intensive workloads unfairly with higher processing latency, leading to poor net i/o performance."
"when a network packet is received by the nic (rx), the nic will raise an interrupt to the upper layer. before the interrupt reaches the driver domain, hypervisor (vmm) handles the interrupt first. hypervisor will determine whether or not the driver domain has the access to the real hardware. upon receiving the interrupt, the privileged driver domain starts to process the network packet. it first removes the packet from nic and sends the packet to the software ethernet bridge. then ethernet bridge de-multiplexes the packet and delivers it to the appropriate netback interface. netback raises a hypercall to hypervisor, requesting an unused memory page and hypervisor notifies the corresponding guest domain to release a page to keep the overall memory allocation balanced. netback and netfront exchange the page descriptors by page-remapping mechanism over i/o descriptor ring (later the data copy is performed). finally, guest domain receives the packet as if it comes directly from nic. a similar but reverse procedure is applied to send a packet using the send path (tx), except that no explicit memory page exchange is involved, only the ownership of physical page is transferred instead of the real page. nic supports direct memory access (dma) technique handles the target guest memory page directly. we will see that three address remappings and two memory allocation/deallocation operations are used for per packet receive and only two remappings are required for each packet transmit [cit] ."
"computational intelligence techniques have been successfully used in many \"real-world\" applications in a variety of engineering problems. they can also be used in the medical research domain."
"virtualization is becoming widely used in cloud environments. although a fair number of research projects have dedicated to measuring, scheduling, and resource management of virtual machines, there still lacks of in-depth understanding of the performance factors that can impact the efficiency and effectiveness of resource multiplexing and resource scheduling among virtual machines."
"in this paper, we proposed a deep learning model to identify channel conditions by combining cnn and rnn. in the proposed cnnlstm model, the cnn captured the feature from frequency-domain characteristics of csis and then lstms extracted the temporal feature from rssi and the output of cnn. in addition, a cnnlstm model with the absolute value of csis was proposed to reduce the complexity with slightly better performance than the conventional models. the proposed methods were verified under indoor environments for wlans and achieved higher accuracy than the conventional lstm model in classifying los and nlos. in future work, we would like to investigate the performance of the proposed cnnlstm models in outdoor environments to expand the range of applications of the algorithm."
"generally the accuracy of the presented algorithm is higher than other used methods. the average accuracy of the proposed algorithm is 95.63% in traffic video. and, the proposed method can increase the tracking accuracy at about 3.34%. fig . 6 indicates the resulting pictures of one target tracing via different methods in ai car and traffic video."
"the mean shift algorithm is carried out by iteratively computing new values of (, c c x y ), which is the mass center of the search window. the algorithm can be stopped in the case where 0th is zero, it shows no difference in the window of zero intensity. the position and dimensions in the search window are rescaled until convergence. fig. 1 shows the processing of the generation of the roi region using the camshift algorithm in ai car. table 1 shows the camshift algorithm steps. step 1.set the region of interest (roi) of the probability distribution image to the entire image."
"camshift basically climbs the gradient of a background probability distribution calculated from regulated color histogram to seek the neighbourhood peak in a search region. the mean location of the object is found by computing 0th, 1st and 2nd order picture moments: a) compute the 0th moment"
"we consider a general baseband volterra non-linear system with memory to model the channel [cit] . the volterra baseband model, while being general, it is particularly applicable to satellite channels [cit] . using kronecker products, we formulate the general base band volterra function in a compact way and referring to fig. 1, the noiseless channel output is expressed as,"
"where n(k) is complex gaussian noise for the kth subcarrier with zero-mean and variance of n 0 [cit] . in ieee 802.11 wlans, rssi is provided for upper layer information. at each transmission, the rssi is used as an indication of the received power level. rssis for the los condition are concentrated at a high value, while rssis for the nlos condition are distributed over a wide range [cit] ."
"the cfr block performs clipping of the original signal before sig nal predistortion. the challenge of signal clipping is to understand if and to what extent it is convenient to clip the signal. in fact, clipping too much results in a loss of the signal while not clipping would nat urally enhance papr. in particular, referring to (4), it is necessary to estimate the parameter 'y towards improving the system perfor mance."
"in the experiments, the results shows the comparison of mean processing time between the proposed algorithm and meanshift. in case of ai car, it takes 0.052 s to process one frame, and spends 0.088 s on processing one frame. due to extracting feature points, it is slower than the algorithm based on color. however, it is enough time for real-time tracking. table 3 indicates the difference of the accuracy between the algorithms. to estimate the tracing accuracy, the function is shown as follows: ( ) total frames miss frames accuracy % 100 total frames"
"whereĥ p [k] is the csi for the pth packet transmission, and r(.) and i(.) are the real and imaginary parts of the complex value, respectively. the lth convolutional layer convolves the input regions locally using n f filter kernels, where each filter uses the same kernel to extract the local features of the input region. the output of a convolution operation at the lth layer for one filter is determined by"
"cd is a ld x (2kd + l) d matrix selecting the relevant product terms from the dth degree kronecker product vector for a complete, non redundant volterra representation."
"referring to the fig. 1, we consider a general baseband multicarrier signal u( n) input to the cascade of cfr and predistortion (dpd) blocks prior to transmission. the channel is assumed to be a non linear function with memory."
"in this section, we consider a system model and experimental data for commodity wlans, where a receiver obtains the rssi at each transmission and estimates the frequency-domain csi of the subcarriers."
"for performance comparison with the previous result, we exploited data collected at seoul national university [cit] . figure 1 shows the layout of the measurement site, which can be considered a typical indoor office environment. for measurement campaigns, two laptops equipped with qualcomm atheros network interface cards (nics) were used to capture both rssi and csi. the height of the transmitter and the receiver were fixed at 1.2 m. a person holding the receiver walked around the highlighted area shown in figure 1 to collect data while recording the labels of the collected data: los if there was no obstacle between the transmitter and the receiver or nlos if the direct path was blocked by the person holding the receiver or other obstacles, e.g., walls and doors."
"communication systems are based on the assumption that the trans mitter can deliver the signal to the receiver with the required level of energy. power amplification has the key purpose of enhancing the signal power sufficient to compensate for the channel losses and im pairments and achieve the target signal to noise ratio at the receiver. however, power amplification is inherently a non-linear operation that introduces distortion of the signal. this distortion is due to the natural saturation effect present in the high power amplifier (hpa) that, depending on the amplitude of input signal distribution and the required power efficiency, can generate significant distortions. fur ther, the severity of the interference generated is magnified when the non-linearity of the hpa is combined with channel memory effects. a simple mechanism to avoid such distortions is to operate the hpa in the linear region far from the saturation region. however, operat ing the amplifier in the linear region drastically reduces the power ef ficiency and the resulting signal output power. the back-off needed for such an operation depends on the peak to average power ratio (papr) of the input signal. papr refers to the ratio between the peak input power and the average input signal power. further, the papr is significant for multiple carrier signals being amplified by a single hpa."
"where n is the batch size of model, c(g) is the cost of the gth input and output pair that measures how accurately the model predicts the label that corresponds to the input. among many choices of the loss function used in optimizing our model, we adopt the binary cross-entropy function, expressed by"
"in this paper, we present vss algorithm based on multiple feature to achieve the exact tracing of moving targets. so as to reduce the processing time to identify the object, it make use of the color feature of the tracked object to change the search space from a whole frame to the region of roi only. the point feature is used to match the same object between the roi and original object image to prohibit missing the object. furthermore, the four matched feature points which have maximum or minimum axis value, generates a rectangle window to obtain an accurate tracking. through the experiments, the proposed algorithm shows more effective and robust in tracking applications."
"a novel design method for combined cfr and dpd has been pro posed. this includes an automatic method to determine the opti mal clipping amplitude for a general non-linear channel with mem ory together with a reformulated and generalized direct estimation method for predistortion. while the reformulated direct estimation itself shows to provide gain with respect to the state of the art, the combination of cfr and dpd is shown to provide the best perfor mance."
"on the basis of its feature values, the methods of target tracking can be divided as color-based method, modelbased method, and boundary-based method. as the simple implementation to facilitate fast calculation, the color-based method is widely exploited in object tracking. classical color-based algorithms are mean-shift [cit] and camshift [cit], which is used for the color probability distribution of object tracking. however, a drawback of this algorithm is vulnerable to different illumination changed and background with similar colors. thus, two or more methods are combined to achieve more robust tracking effects [cit] ."
"where h 0 and h 1 are null and alternative hypothesis, respectively, and α denotes the decision threshold. we assume that, los detection rate is a true positive rate (tpr) corresponding to the portion of correct decisions among all measurements under the los condition. similarly, nlos detection rate is a true negative rate (tnr) corresponding to the portion of correct decisions over all measurements. these statistical values depend on the decisions."
"the model was trained by truncated backpropagation through time [cit] with adam optimization [cit] with an initial learning rate of 0.001. on the dataset, we used a minibatch [cit] with a size of 128 for high efficiency. after each batch, the gradients were averaged and updated. we employed the early stopping method to stop the training when the validation accuracy becomes stagnant and does not increase after 10 epochs. we adopted the dropout method with a probability of 0.5 after the cnn layers and lstm layer for regularization to avoid over-fitting problems [cit] ."
"the predistortion function acts on the output of the clipping func tion to yield the channel input x(n). the predistorter per-se is a non-linear function with memory accomodating various polynomial functions including memory polynomials [cit], generalized memory polynomials [cit] and volterra expansion [cit] . the output of the pre distorter is computed as,"
"given the crest factor reduction and predistortion techniques pro posed in sections 2.2 and 2.3, we define a transmitter architecture in which the cfr block precedes the dpd block. in this section, we derive optimization algorithms to determine the optimal cfr and predistortion parameters towards reducing the error at the receiver. fig. 2 illustrates the considered transmitter architecture including the two iterative optimization processes. the target of the predistortion function is to reduce the non-linear distortion at the channel output. this is achieved by optimizing the parameters w. the general estimation paradigm is shown in fig. 2 ."
"the proposed algorithm in this paper combines color feature with point feature, which is an effective enhancement for meanshift using sift algorithm. the key to the method is that extracts the color feature from the detected object, and reduces the search space of extracting the feature points [cit] . this method may change the global search into the local search so as to decrease the processing time, and settle the similar color background problem and enhance the property of target tracking [cit] . the major results of this paper are as follows: this approach grounded on color and point features is presented for object tracking to obtain accurate tracking, and the proposed algorithm improves the classical algorithms of meanshift and sift in tracking and performs effectively in complex real environments."
"step 5.for the following frame, center the search window at the mean location found in step 4 and set the window size to a function of the zeroth moment. go to step 3."
"visual object tracking in complicated environments is an important topic within the field of computer vision and is widely used in various applications such as vehicle navigation [cit], intelligent surveillance, traffic monitoring, and robot vision [cit] . the target tracking indicates tracing the process of targets when they shift over a set of pictures. an excellent tracking approach should be able to work well in many real circumstances, such as background clutters, object occlusions, and different illuminations [cit] ."
"the process starts with random initialization of all the model parameters. during the training phase, the weight update takes place after a whole sequence has been propagated forward through the network. the error signals are calculated with respect to the mean of cross entropy losses cost function. the loss function was chosen as the natural cost function for the sigmoid output layer with the aim of maximizing the likelihood of classifying the input data correctly."
"the camshift algorithm is derived from the original mean shift algorithm based on color for tracking. the meanshift algorithm, which repeatedly moves a mass center to the average of data points in its neighborhood, is a statistical approach. it is effective and robust to use for the conditions such as clustering, tracking, and probability density estimations. however, mean shift algorithm based on static probability distributions, is not updated unless the object obviously changes in shape, color or size, while camshift employs continuously adaptive probability distributions."
"further, depending on the channel characteristics and the param eter setting, the convergence property of the proposed algorithms might vary. in general, we observed poorer convergence when the channel exhibits longer memory effects."
"in the cnn segment, we used hybrid hyper-parameters settings for the cnns with high numbers, l of convolution layers to extract the implicit features of data. we applied the cnn models with a different number of n (l)"
"as the fig. 6 shows, a red rectangle is the proposed algorithm, and its size denotes the window of tracking. the proposed algorithm offer more accurate tracking information. the size of the red rectangle is up to the size of the detected object. the object's size change a bigger one, while the red rectangle enlarge its size to the big, and vice versa. it is clear that the window created by the proposed algorithm adapted to the size of the target properly. fig. 6 the images of single target tracking result using the proposed algorithm in ai car and traffic video."
"once every parameter in the proposed cnnlstm model is adjusted appropriately, the model can identify the channel condition based on the following simple hypothesis test"
"r and b (l) are the weight and bias elements located at (r) on the kernel, respectively, in the lth convolutional layer. in addition, a(.) represents a non-linearity activation function, that is typically given by the sigmoid, softsign, hyperbolic tangent (tanh) and rectified linear unit (relu), etc. [cit] . without zero-padding, the output size is calculated as"
"the cascaded predistortion and channel blocks (kindly refer to figs. 1, 2) result in a non-linear system with memory that can be modeled using a volterra system as (here limited to the third degree),"
"scale invariant feature transform was presented by lowe, for extracting highly feature point from images [cit] . the major steps of sift are detecting scale space extrema, localizing keypoints, assigning orientation and calculating descriptor [cit] . in this paper, we make use of detected feature points to implement reliable matching of the same target between different pictures and generate a window for the tracking."
"further, in the case when both cfr and dpd algorithms are ap plied, they can be run alternately or concurrently. however, when running concurrently the final performance is slightly penalized due to the inter dependency in the target error. for the sake of perfor mance optimization, we alternate them in three phases: we first per form a dpd estimation followed by the cfr estimation and a sub sequent dpd estimation. we noticed that further iterations do not improve significantly the performance."
"the proposed vss algorithm involves three steps. the first step is seeking the roi of target by the meanshift approach based on color feature. the second is the extraction of sift points within the detected roi. the last step is the recognition and tracing of the moving target. at last, the presented tracking method looks for rapidly the most accurate target by decreasing the search space. during camshift tracking, a histogram of color is easily be computed, and the size of the search window can be updated. due to noise and illumination, it is sensitive to some resembled colored background regions and objects that disturb the process of tracking. thus, to offset the drawback, the vss algorithm will generates a variable search space with candidate points for the accurate tracing of moving targets. a flowchart of the vss algorithm is shown in fig. 4 . the roi of the object is set as the finding of the maximization via meanshift algorithm in each frame. the size of roi is a variable region according to the size of the object. the next step involves that gaussian pyramid pictures are made in the region, dog pyramid pictures are made to gaussian pyramid pictures. in the dog pyramid pictures, maxima and minima candidate key points are found. using the sift, feature points are extracted within the roi only though screening. it should be used to implement stable matching of the same target between the roi and original object image. we then take four feature points among correct matched points which have maximum or minimum axis value, and generate a rectangle window with them. the region of the rectangle represents the target that we are tracking."
"to minimize the loss function, many variants of the gradient-descent method such as adagrad, adadelta, and adam optimizers have been studied. these optimizers adaptively change the learning rate to properly minimize the loss function. in this study, we applied the adam optimization algorithm to train our proposed cnnlstm model as the adam optimizer is straight-forward and saves memory and computational resources."
"step 2 extract the robust feature with sift step 3 match feature points to make sure the identified object in the roi is the same in the original object image step 4 make a rectangle with four correct-matched points in roi, which have maximum or minimum axis value step 5 stop tracing unless the number of correct matched points is under four if not, repeat from steps 1 to 4 fig. 5 shows the process of the generation of the vss. in fig. 5, 5b is the roi that is found by color feature and represented by the green rectangle. to compensate for the weakness that it is sensitive to illumination and similar color background, we extract robust matching points within the roi. this method reduce the calculation time greatly because of changing the search space from the whole frame to the roi. it set four features into min_x, min_y, max_x and max_y, which generate a red rectangle in the roi. therefore, the tracking window then changes from the roi to the region in red rectangle. producing a variable tracking window in each picture can enhance the accuracy of the target tracking property. the presented approach is executed in visual c++ and opencv. table ii denotes the detailed information for each video sequence. two video sequences are used in the experiments, ai car and traffic videos. in order to recognize the object handily, we create the original object image that is the standard image of the ideal object region."
the first stage is seeking throughout all scales and locations in image. it is invariant to scale and orientation that potential interest points extracted in a difference -ofgaussian image are [cit] . the function of scale space in an picture is shown as follows:
"generally, a wireless sensor node contains the following essential components: sensors, a microcontroller, a transceiver, external memory, and a power source [cit] . the role of the sensors is to obtain data from the environment and to convert it into data for postprocessing, i.e., digital data the algorithm can operate. the microcontroller then processes the raw data acquired and stores the results of the processing in an external memory. the data saved in the external memory may be later accessed by the user. the role of the transceiver is to share the data between the nodes and the base station. the power unit provides energy, and it can be a battery, a capacitor, or both. other modules can be added to the system if required. figure 3 shows the basic elements of wireless sensors [cit] ."
"the previous section provided an overview of the main components of a wireless sensor. the current section offers recommendations of particular components that can be used for the strain gauge, taking into account their technical properties and their market availability. the components are shown in figure 4, and their cost is presented in table 1 . table 2 shows a cost comparison between the proposed and existing architecture."
"the ar headset is equipped with a clicker accessory connected via bluetooth, which enables the user to make physical commands instead of using hand gestures or voice commands."
this section describes the experimental setup that is applied in order to validate the developed method. the first part provides an overview of the simplified cantilever beam model that was chosen for the experiment. the second part describes the placement of the sensor as well as the instrumentation that was necessary for the method. this section develops an analysis and provides an assessment of strains that were collected from the experiments to evaluate the performance of the developed method.
"the proposed ar techniques improve the inspectors' capacity to perform informed maintenance decisions, to efficiently evaluate on-site damage and potential risk, to determine the growth of defects that occur over time, and to provide high-resolution documentations of performed inspections, which reduces the variability found in manual inspections. however, due to the high cost of ar equipment, these solutions are rather expensive to implement. in the experiment reported in this paper, the researchers made use of a more economical data acquisition system (daq), which reduced the overall cost of the application."
"as shown in figure 8, a cantilever ruler was utilized as the test specimen to conduct the experiment, the material and sectional properties of the ruler are presented in table 3, and its dimensions are displayed in table 4 ."
"it must be noted though that the method developed in this paper has some limitations (for a general discussion of the limitations of augmented reality, see [cit] ). for example, there are technological challenges that need to be addressed in future work, such as the limited battery life, the insufficient data storage and processing capacity of mobile devices, and the high cost of these appliances. furthermore, there are physiological limitations concerned with the user's interaction with ar technology. they are related to the insufficient understanding and knowledge of natural body movements during the application of ar technology, as well as the restricted scope of vision offered to the user of ar headsets."
"to support affordability and accessibility, the researchers used open-source software. the major exception is unity 3d, for which a license must be obtained. although the features of unity are free to use, they are not open source. it is possible to use cocos 2d-x or the godot engine as alternatives. the software components are shown in figure 7 ."
"critical civil infrastructure needs to cope with serious demands, related to both internal and external factors. all components of the infrastructure deteriorate due to natural processes such as aging and the normal wear and tear that comes with utilization. structures may experience unexpected and accelerated damage because of sudden environmental events, such as earthquakes and hurricanes. furthermore, human-related events may also cause damages, such as variability in service loads or unanticipated overloading. in order to determine whether a structure has experienced damage, engineers use mechatronic sensors, which provide additional information concerning the precise damage location and severity. for damage detection to be accurate, sensors need to provide automatic, high quality data. moreover, the data must be readily available to inspectors working outside the field where it is collected. although most wireless sensors provide the necessary quantitative information and allow engineers to collect data in the field, it is often necessary to perform subsequent data processing in the office or at the decision-making headquarters [cit] . such an approach to data collection and analysis delays decision making. in general, the industry relies on inspectors' analyses for critical decision-making. if wireless sensors could provide direct information to inspectors, the industry decisions would be improved by the data obtained."
"this paper introduced a new automatic strain gauge platform, which collects data with a smart low-cost sensor to measure the strain of different applications. in the experiment presented in this paper, the platform allowed real-time visualization of data coming from the strain gauge in the ar headset. in the context of the research presented in this paper, the application of the ar headset was shown to ensure automatic, effective communication from a smart, low-cost sensor strain gauge to a database in real time in a reliable way. inspectors can use the ar headset to observe augmented data and quickly compare it across time and space, which in turn leads to the prioritization of infrastructure management decisions following accurate observations. the opportunities afforded by augmented reality for industry and academia will help to visualize the design-sense-build experiment. the two main contributions of this paper in the field of sensing technology and mechatronics are the low-cost fabrication of a smart strain gauge sensor and the real-time visualization of data provided by the ar headset. by demonstrating the framework for strain gage sensing, this experiment encourages students and professional engineers to apply the sensors as a crucial component of the design. a design-sense-build framework results in a groundbreaking shift in the civil engineering, mechatronics, and construction industry. the cost comparison shows a difference in price of an order of magnitude $153.45 compared to $14,257. finally, this paper demonstrates that building a smart low-cost sensor that can be rapidly deployable on-site may result in more efficient use of time and labor, therefore increasing overall efficiency."
"the researchers installed node.js and mysql on the server (i), which are the necessary software to connect databases to any application. mysql is one of the most popular open-source databases in the world because of its accessibility and efficiency. virtually every popular programming language, including java or php, provides access and enables operations with mysql. node.js is an open-code, which executes javascript codes outside the display program. node.js represents a \"javascript everywhere\" paradigm, which allows the user to apply javascript for server-side scripting and to write command-line tools. node.js is supported on macos, linux, microsoft windows 7, [cit] (and later versions). it is mainly applied to develop network programs such as web servers, but it also allows creating web servers and networking tools that use javascript. mysql is a free, opencode relational database management system (rdbms) that follows the gnu general public license (gnu gpl.) it is written in c and c++, and it can be used on many system platforms. mysql can be built and manually installed from source code. when special customizations are not needed, it can also be installed from a binary package. since the researchers work with a database, it is possible to visualize storage data from previous measurements."
"for the purposes concerned with reliable communication and data transfer between wsns and databases, ar permits engineers to directly interact with the real world. furthermore, as is mentioned earlier in the paper, wireless sensor networks (wsn) usually provide the necessary quantitative information to engineers. however, it is also important to be able to present the collected data to inspectors in a useful and transparent way. sensor networks consist of many sensor nodes, coupled with wireless communication and sensing hardware. they are deployed in an area under investigation, collectively measure a phenomenon, and then process received data before transferring it to the base station. ar improves access and the analysis of the data by making it possible to visualize information coming from the mechatronic sensors in a graphic form in real time, in a reliable way, to inspectors working outside the field in this way inspectors can observe augmented data and compare it across time and space, which leads to the prioritization of infrastructure management decisions based on accurate observations."
"ar can be applied to all senses, including vision, touch, and hearing, so it is normally implemented through a headset or a handset. in the experiment reported in this paper, the researchers used an augmented reality headset \"hololens from microsoft.\" the headset is equipped with a variety of mechatronic sensors, such as microphones, rgb/depth imagers, and inertial measurement units (imu); communication interfaces such as bluetooth, and peripherals such as stereo sound. interaction with ar headsets occurs via voice commands, gesture recognition, and tracking the user's gaze. due to these interaction possibilities, ar headsets are useful for industrial applications, as they allow leaving the hands free for performing manual work."
this chip is used for communication between the strain gauge and the arduino. it can transfer data with precision. it is a 24-bit analog to digital converter (adc).
"the ar headset is equipped with a holographic processing unit, which handles one trillion calculations per second, and has two gigabytes (gb) of ram. moreover, it has 64 gb of internal flash storage and provides wi-fi support and bluetooth. the battery life is up to two weeks of standby mode or 3 hours of active usage."
"the application of ar raises a number of issues that need to be considered. for example, ar functionality must match the user's needs. the additional information provided by ar must be appropriate in scope for the user to be able to process it and comprehend it. in view of this, it is necessary to take into account the properties and limitations of human perception so as to avoid human errors and prevent creating confusion. the properties of human perception may impose requirements on the size of objects provided on display and the availability of interactive information. other limitations may be related to human learnability and the amount of information that can be presented to the user at a given time."
"in the experiment, the researchers used the augmented reality headset the \"hololens from microsoft.\" the ar headset was used to visualize the data coming from the strain gauge. this headset collects data via its image sensors. it also contains an imu; four sensors; an ambient light sensor; a depth camera; a photographic video camera of 2.4-megapixels; and four microphones [cit] . the process of capturing data by the ar headset involves four light-sensitive grayscale cameras which track visual features in front as well as alongside the edge of the smart glasses. the sensors operate in conjunction with a depth camera that measures depth through infrared light by applying the timeof-flight method. this method is more accurate and effective than the ones used by standard cameras, which are more sensitive to ambient light. moreover, when the ar headset is used in windows research mode, inspectors can access an even wider spectrum of data collected by the sensors. for example, they can obtain access to computer vision algorithms, such as simultaneous localization and mapping (slam) for spatial mapping and motion as well as the audio and video streams. it is also possible to route the output provided by the smart glasses wirelessly to a computer or cloud environment for subsequent more intensive data processing."
the server is a computer which manages access to the sensor data through the ar device in a network. the server used in this experiment is a microsoft surface computer.
"computers, systems, and microcontrollers can be interconnected with a module referred to as the xbee series 1 module, providing support to point-to-point and multi-point networks, which can be converted to a mesh network point. according to the specifications included in the digi xbee documentation [cit], xbee series 1 has the range of 300 feet outdoors within line-of-sight and 100 feet indoors, and it is equipped with an excellent wire antenna. the setup is easy, and it is not necessary to perform any prior configuration. the xbee series 1 can be applied as a replacement for wired serial connections used previously. the researchers chose to use the xbee module as it ensures continuous communication between the sensor platform and any computational platform. in our research, we wanted to communicate the sensor with the ar headset."
"this paper shows that the communication and the data transfer between wireless sensor networks (wsn) and databases can be significantly improved through the application of augmented reality (ar). ar is a tool which overlays the known attributes of an object with the corresponding position on the computer screen. once ar captures input from the mechatronic devices, the ar application recognizes the target, processes the images, and augments it with audiovisual data, creating an illusion that helps the user to analyze the real-world situations in more detail. in this way, ar allows integration of reality with virtual representations provided by a computer in real time. these additional synthetic overlays supply data that may be unavailable otherwise, but it may also display additional contextual information [cit] . two other advantages of ar include the possibility of having real-time interactions and access to accurate three-dimensional (3d) representation of structures."
the observed maximum and minimum strain values for loading of 697.9 g of weight are summarized in table 6 . note: this experiment is done with the calibration factor as -375. the error was calculated using the equation given in equation (2).
"a strong advantage of the application of the ar headset for data collection is the produced hologram effect which mixes physical and virtual objects. the hologram effect is understood as the extra information visualized through the ar device which the viewer obtains in addition to the information received from the real world. this effect allows inspectors to visualize and interact with the obtained data as well as with the real existing physical objects in the actual environment. the ar headset works with very powerful computing technologies that produce quick visualization of complex data and reliable task analysis. moreover, the ar headset has the additional advantage of enabling a computing infrastructure, which permits inspectors to implement both data analysis and physical operations at the same physical locations without a context switch. in this way, inspectors from different locations can perform a collaborative analysis of infrastructure in an identical environment at the same time. such an analysis can be carried out in different situations, both in disaster scenarios when visualization of critical infrastructures is necessary to ensure a quick and efficient emergency response, as well as during the monitoring of infrastructure in routine operation."
"the sensing platform is a wireless (ws) structural health monitoring (shm) system. wireless shm systems have substantial advantages when compared to their conventional wired counterparts. first of all, a major advantage of ws-based shm systems is their reliability and that they enable autonomous, efficient monitoring that takes place in real time in any location [cit] . for this reason, ws-based shm systems are used to monitor large structures that are strategic and critical public infrastructure, such as bridges, mines, pipelines, dams, oil rigs, and wind turbines [cit] . a second major advantage of ws-based shm systems is related to their economic performance: in comparison to traditional, wiring-based communication systems, wireless communication is very cost-effective due to lower labor costs [cit] . the commercial advantage of ws-based shm systems permits an application of a denser network of wireless sensor nodes, which in consequence provides a more accurate and reliable level of shm monitoring."
"this section introduces the principles of estimating strain from a strain gauge. since strains directly relate to the structural condition of bridges, they can also be used to determine bridge serviceability. as shown in figure 9, the strain is calculated at a specified point where the strain gauge is attached. this value is derived utilizing the strain-stress formula and deflection due to point load. as the equation is only valid for the elastic behavior of a ruler, some discrepancy with the experimental data is expected."
"1. a device used for capturing images (for example, a charge-coupled device (ccd), a stereo, or a depth-sensing camera). 2. a display that is used for projecting the virtual information on the images that were obtained by the capturing device. there are two types of display technologies:"
"the strain gauge 3/120 cly41-2l-0.5m has a measuring grid length of 3 mm, made of ferritic steel (10.8 ppm/k) with a nominal resistance of 120 ohms, employs two wires with a connection cable of 0.5 m, produced by hbm with headquarters in germany."
"ar is a complex technology that involves the application of novel solutions that mix actual and virtual realities. the technology supplements actual realities with computer-generated sensory inputs, such as sound, video, and graphics. due to the novelty of ar applications and solutions, it is not yet clear how ar may affect the human-machine interface (in particular, what new humanrelated challenges may emerge) and what design principles should be at work for the effective application of ar technologies (see [cit] for a discussion of human factor considerations for the application of ar in an operational railway environment)."
"this research uses the arduino uno r3. it is an atmega328p-based microcontroller platform. it is an open-source platform that is utilized for various electronic applications and can be applied to many projects. it can be powered through a usb or connected through a barrel jack. it contains a circuit board that is programmable physically and through coding, and it features 14 digital input/output pins. these pins have two purposes: they can collect information from the sensors (measuring temperature, acceleration, strain, light), and they can control actuators. another component of the pins includes six pwm outputs. there is an integrated circuit (ic) which essentially runs the arduino board. in summary, it is a flexible and efficient microcontroller; it provides an easy to use interface and ensures a reliable connection between systems [cit] . since it can interact with various types of equipment, it is a very useful component for this experiment."
"the ar headset (iii) was connected to the server that contains the web page connected to the internet. in this way, it was possible to project the information from the sensors in the ar headset. the projection of the web page contained in the server in the ar headset was possible because of the windows 10 microsoft edge application. microsoft edge is a web browser first released for windows 10 [cit], developed by microsoft. it is built with microsoft edge html and chakra engines. it is integrated with microsoft's online platforms and provides voice control, search functionality, and dynamic information search in the address bar. edge shows superior power efficiency in comparison with most other major web browsers."
"activating elements triggering the virtual information display. they include gps positions, images, sensor values from accelerometers, qr (quick response) markers, compasses, gyroscopes, altimeters, and thermal sensors."
"(2) figure 11 presents a comparison of the errors obtained in the experimental measurements compared to the values of the theoretical measurements. the experimental strain is also displayed using the ar device and is shown in figure 12 . the researchers used php and html to graph the information contained in the database from the sensors, whereas the web page was developed using php and html. as can be seen from this video capture, the inspector can visualize real-time changes in the strain that inform structural properties. according to the structural inspection community, the application of this technology can be used to observe live load testing quantitatively in the field. the future direction of this research includes field application of this new framework."
the hx711 allows for a digital interface which is connected to the arduino shield. the dt (data output) and sck (serial clock input) are connected to the arduino along with the ground and 5 v power (vcc).
"ar covers a number of technical components that apply electronic devices to observe, directly or indirectly, physical environments in the real world and to combine them with virtual elements."
"this is a sensor that consists of a resistor (metallic grid) which measures displacement due to different types of forces. this gage converts forces that are applied and records the displacement due to that force. this gage is affixed to a metal ruler for this experiment. as load is added to the free end of the ruler, the metallic ruler bends. as the metal bends, the resistor changes its resistance value, which in turn changes the output voltage. this millivoltage is then amplified by the hx11 converter to be correlated to displacement."
"the sensor (ii) programming was performed in the arduino ide platform. arduino ide is an open-code software that runs on different operative systems. the environment is written in java and based on open-source software, including processing. this software can be used with any arduino board, which facilitates writing the code and uploading it to the arduino compatible boards, but also to other vendor development boards when third party cores are used, which makes it a very flexible solution."
"the research team used php and html to graph the information contained in the database from the sensors. a web page was developed using php and html. this paper shows a first approach to a data visualization from only one sensor. however, in the future, it can easily be expanded to a mesh of sensors, which is of interest to the research and industry application domain of this new interface. in the event of a human using the ar headset with a mesh of sensors, which sensor data to visualize will need to be chosen. in order to distinguish various sensors using the ar headset, it is possible to paste a quick response (qr) code to each sensor. each qr code is linked to an id associated with each sensor. under this scenario, the user will select one sensor and scan it with the qr code reader. then, the ar headset will identify the id being scanned at that moment with their corresponding sensor. with this qr-id pairing, it will be possible to filter the data from the database and display only the data corresponding to the sensor chosen by the user."
"to the left of the hx711 adc, there are four input connections: ch a and ch e, one positive and one negative for each channel"
"the theoretical strain was calculated on the basis of the formula obtained in equation (1). this value is based on the load imposed ( ), section property ( ), material property ( ), and the relative location of the sensor ( ), the theoretical strain and the experimental strain are presented in table 5 and plotted in figure 10 ."
"in this work, we present a method which uses estimated confidences in the context of photoacoustic signal quantification to increase the accuracy of the quantification algorithms. in theory, the proposed method is independent of the underlying qpai method, as it uses a deep learning model to observe the errors resulting from the quantification method in order to provide confidence estimates. while the application of our method to other state-of-the-art qpai methods is the subject of future work, we aimed to show the general applicability of our method by also incorporating a naïve fluence compensation method into the experiments. our results suggest that using a method to estimate confidence information to refine a region of interest for subsequent computations might be a valuable tool for increasing the robustness of qpai methods and could be easily integrated in future qpai research."
"surviving from the previous stages, the small number of windows have been quite challenging, among which face and non-face windows are more difficult to be distinguished. considering that multiple models running in parallel tend to introduce more false alarms, it is desirable to prominent divergence exists in appearances of multiview faces, which is mainly due to the unaligned features, i.e. features are extracted at positions that are not semantically consistent. for example, the central region of a frontal face covers the nose, while that of a profile face is part of the cheek, as shown in figure 2 . to address this issue, we adopt shape-indexed features extracted at semantically consistent positions as the input of the fine mlp cascade classifier. as shown in figure 5, four semantic positions are selected, corresponding to the facial landmarks of left and right eye center, nose tip and mouth center. for profile faces, the invisible eye is assumed to be at the same position as the other eye. the sift (scale-invariant feature transform) [cit] feature is computed at each semantic position on candidate windows, and they are robust to large face variations such as pose, translation, etc."
"another widely used face detection dataset is the afw [cit] . this set contains 205 images from flickr with 468 faces. it is a small set, yet is challenging, since faces appears in cluttered backgrounds and with large variations in viewpoints."
"although carriers do not typically write their code they can use state-of-the art server and network virtualization to achieve similar goals of flexibility, elasticity, and resiliency. virtual machines (vms) are used for porting existing functional code to server executable images, capturing the standard carrier (3gpp etc.) interfaces as well as proprietary operating systems. the"
"funnel structure vs parallel structure to demonstrate the effectiveness of the proposed funnel structure employing a unified model to handle candidate windows coming from different classifiers, we compare the parallel and the funnel structure on frontal and half profile faces in the coarse mlp cascade stage. specifically, for the parallel structure, we train three mlps, one for each of the three views, which follows the corresponding fast lab cascade. for the funnel structure, only one mlp is trained for frontal, left half profile and right half profile faces. the parallel structure obtains a recall of 94.41% with 297.06 windows per image, while the funnel structure reaches a higher recall of 94.43% with only 268.10 windows per image. this demonstrates that a unified model can effectively control the false positives with less sacrifice of recall. shape-indexed feature to verify the effectiveness of the shape-indexed feature, we train two types of two-stage fine mlp cascade classifiers with mean shape and refined shape respectively, which are used to extract shape-indexed feature. namely, one mlp cascade uses sift extracted according to mean shape as input at both stages, while the other uses sift extracted with refined and thus more accurate shapes as input at the second stage."
"most works mentioned above focus on designing an effective detector for generic faces without considerations for specific scenarios such as multi-view face detection. in order to handle faces in different views, a straightforward solution is to use multiple face detectors in parallel [cit], one for each view, as shown in figure 1a . the parallel structure requires each candidate window to be classified by all models, resulting in an increase of the overall computational cost and false alarm rate. to alleviate this issue, each model needs to be elaborately trained and tuned for better discrimination between face and non-face windows, ensuring faster and more accurate removal of nonface windows."
"we also evaluate the coarse shape predictions on afw. figure 7 compares the predicted shape with the mean shape. with only two stages of refinement, the predicted shapes achieve significant improvement over the mean shape, leading to more semantically consistent shape-indexed features. when followed by an alignment model, the predicted shape from our fust detector can be directly used as a good initialization, which is more preferable than only bounding boxes of detected faces. figure 8 gives several examples of predicted shapes on faces in different views."
"today's network operators are increasingly weighted down by complexities as they work to accommodate the relentless demand for new services and more bandwidth. a recent network function virtualization (nfv) industry white paper laments a growing and increasingly diverse population of proprietary appliances that make service additions and upgrades more and more difficult. the type of appliances referred to by the nfv paper are typically turn-key in-line systems that maintain real-time state of subscriber mobility, voice and media calls, security, contextual content management, etc."
"discussion compared with cnn based methods, the proposed funnel structure is a general framework of organizing multiple models, adopting a divide-and-conquer strategy to handle multi-view faces. the mlps used with the framework can also be replaced by cnns. one other aspect that makes our fust detector different is that hand-crafted shape-indexed feature is adopted based on explicit consideration for semantically consistent feature representation. by contrast, cnn learns the feature representation merely from data without considering the semantic consistency."
"the most popular dataset for evaluating face detectors is the fddb [cit] . it contains 5, 171 labeled faces from 2, 845 news images. fddb is challenging in the sense that the labeled faces appear with great variations in view, skin color, facial expression, illumination, occlusion, resolution, etc."
"method for confidence estimation. our approach to confidence estimation can be applied to any qpai method designed to convert an input image i (p 0 or raw time-series data) into an image of optical absorption i µ a . in order to not restrict the qpai method to a certain class (e.g., a deep learning-based method), we made the design decision to base the confidence quantification method on an external observing method. for this, we use a neural network, which is presented tuples of input image i and absorption quantification error e µ a in the training phase. when applying the method to a previously unseen image i, the following steps are performed (cf. figure 1) . figure 1 . visualization of the proposed method for confidence estimation using an observing neural network as an error model. the estimator generates an output for a given input and the error model is used to obtain an estimate of the quantification error from the same input data. the region of interest (roi), which is based on the aleatoric uncertainty i aleatoric extracted from the input data, can then be refined using the error estimates i epistemic of the error model as a confidence threshold (ct)."
"the fast lab cascade classifiers aim to quickly remove most non-face windows while retaining a high call of face windows. the following coarse mlp cascade classifiers further roughly refine the candidate windows at a low cost. finally the unified fine mlp cascade classifiers accurately determine faces with the expressive shape-indexed features. in addition, it also predicts landmark positions which are beneficial for subsequent alignment."
"to provide a more effective framework for multi-view face detection, we design a novel funnel-structured cascade (fust) multi-view face detector, which enjoys both high accuracy and fast speed. the fust detector, as shown in figure 3, features a funnel-like structure, being wider on the top and narrower at the bottom, which is evidently different from previous ones. at early stages from the top, multiple fast but coarse classifiers run in parallel to rapidly remove a large proportion of non-face windows. each of the parallel classifiers is trained specifically for faces within a small range of views, so they are able to ensure a high recall of multi-view faces. by contrast, at subsequent stages, fewer classifiers, which are slightly more time-consuming but with higher discriminative capability, are employed to verify the remaining candidate windows. gathering the small number of windows surviving from previous stages, at the last stages at the bottom, a unified multilayer perceptron (mlp) cascade with shape-indexed features is leveraged to output the final face detection results. from top to bottom, the number of models used decreases while the model complexity and discriminative capability increase, forming a coarse-to-fine framework for multi-view face detection. compared with previous multi-view face detectors, the proposed fust detector is superior in that a more effective framework is used to organize multiple models. the contribution of our work compared to existing literature is listed as below."
"to further evaluate the performance of the fust detector on multi-view face detection, we compare it with the state-of-the-art methods on fddb and afw as shown in figure 10 . methods being compared include cascadestructured detectors such as joint cascade [cit], acf [cit], surf cascade [cit], and head hunter [cit], dpm-based detectors such as fastest dpm [cit], and tsm [cit], and deepnetwork-based detectors such as ddfd [cit], cascade cnn [cit], ccf [cit], and facenessnet [cit] ."
"although the lab feature is quite computationally efficient, it is less expressive and has difficulty modeling the complicated variations of multi-view faces for a high recall of face windows. therefore, we adopt a divide-andconquer strategy by dividing the difficult multi-view face detection problem into multiple easier single-view face detection problems. specifically, multiple lab cascade classifiers, one for each view, are leveraged in parallel and the final candidate face windows are the union of surviving windows from all of them."
"additionally, predicting shapes has made the detector alignment-aware in the sense that an alignment model can be initialized with landmark coordinates directly instead of bounding boxes of detected faces."
"more efficiently, the multiple models for multi-view face detection can be organized in a pyramid [cit] or tree structure [cit], as shown in figure 1b and 1c, forming a coarse-tofine classification scheme. in such structures, the root classifier performs the binary classification of face vs. non-face, and then at subsequent layers, faces are divided into multiple sub-categories with respect to views in a finer granularity, each of which is handled by an independent model. the pyramid structure is actually a compressed parallel structure with shared nodes in higher layers or a stack of parallel structures with different view partitions. therefore the pyramid-structured detectors suffer from similar problems that parallel-structured ones are faced with. the treestructured detectors are different in that branching schemes are adopted to avoid evaluating all classifiers at each layer, but this can easily lead to missing detections with incorrect branching. to relax the dependence on accurate branching, [cit] designs a vector boosting algorithm to allow multiple branching."
"the benefits of replacing proprietary-systems with virtualization are clear. multi-core-threaded computing architectures transformed industry-standard hardware into highly concurrent machines, enabling cots hardware to take on functions traditionally delivered using proprietary systems. however if complex carrier functions are to be virtualized this inevitably means (a) significant component based unbundling of both capacity and functionality locked today in monolithic systems, and (b) a method to dynamically assemble discrete functional components to elastic end-to-end services. such \"scatter-gather\" rearrangement of carrier functionality needs to work on commodity hardware and changing demand patterns. dynamic component based models are already heavily used today by internet providers, however mostly using software designed from the ground up along these lines. mapping of users to functions to compute resources is based on common map-reduce infrastructure embedded in every module, library, and utility function call."
"in order to derive quantitative information from initial pressure p 0 reconstructions of photoacoustic images, one has to account for the light fluence and solve the optical inverse problem. most methods model the distribution of optical absorption coefficients by iteratively updating the distribution after computing the solution of a forward model (cf., e.g., [cit] ) with inclusion of the acoustic inverse problem [cit] . alternatively, in multispectral photoacoustic imaging applications, the functional parameters are approximated directly by using a variety of spectral unmixing techniques (cf., e.g., [cit] ). recently, machine learning-based methods for quantitative pai (qpai) have been proposed. these encompass end-to-end deep learning on 2d images [cit] or the estimation of voxel point estimates with context encoding qpai (ce-qpai) [cit], which incorporates the 3d p 0 context around each voxel into a single feature vector that is used to learn the fluence at that particular voxel. some of the listed approaches to qpai have been shown to work in ideal in silico conditions or on specific datasets. at the same time, they have proven difficult to use in clinical applications, which can be attributed to a lack of robustness caused by a priori assumptions that are made regarding, e.g., illumination, probe design, calibration factors, or scattering properties [cit] . developing tools to estimate systematic errors and gain information on the quantification of uncertainties in pai could thus be of great benefit and could be utilized to improve quantification accuracy."
"for evaluation of the detection accuracy, we apply the officially provided tool to our detection results on fddb to obtain the rocs, and draw precision-recall curve for the results on afw, following most existing works."
"uncertainty quantification and compensation is an essential research objective in computer sciences and has been studied extensively in various fields, including image-guided navigation (cf., e.g., [cit] ), multi-modal image registration (cf., e.g., [cit] ), and lesion detection [cit] . current approaches to obtaining confidence intervals for neural network estimates include, e.g., dropout sampling (cf., e.g., [cit] ), probabilistic inference (cf., e.g., [cit] ), sampling from latent variables (cf., e.g., [cit] ), or using ensembles of estimators (cf., e.g., [cit] ). the exploration of such uncertainty quantification methods in the field of pai, however, has only just started (cf., e.g., [cit] )."
"for validation in the context of qpai, we applied this methodology to different pa signal quantification algorithms to investigate whether the approach is applicable in a general manner. we hypothesize that an estimated error metric is indicative of the actual absorption quantification error and that we can consequently improve on µ a estimations by evaluating on an roi that is further narrowed down with a confidence threshold (ct)."
"in a recent publication [cit], we showed a method for uncertainty quantification for the ce-qpai method. a key result was that the practice of evaluating pa images over a purely input noise-based (aleatoric) region of interest (roi) can be improved when also taking into account model-based (epistemic) uncertainty. to achieve this, we combined both sources of uncertainty into a joint uncertainty metric and used this to create an roi mask on which to compute the statistics. a limitation to that approach could be seen in the fact that we used an uncertainty model specially tailored toward the ce-qpai method. to overcome this bottleneck, we expand on our prior work in this contribution and present a method that yields confidence estimates by observing the performance of an arbitrary qpai algorithm and uses the estimates to refine an roi that was defined based on aleatoric uncertainty."
"considering the appearance divergence of multi-view faces from the perspective of feature representation, the intra-class variations are mainly due to features extracted at positions with inconsistent semantics. for instance, in figure 2, three faces in different views are shown and the window at the same positions on different faces contains completely distinct semantics, resulting in features describing eye, nose and cheek respectively. thus there does not exist a good correspondence between representations of faces in different views. [cit] compares densely extracted features with shape-indexed features and finds the latter to be more discriminative. by using features at aligned landmarks, faces in different views can be more compactly rep- resented and better distinguished from non-face regions."
"the dataset simulated for the experiments was specifically designed such that out-of-plane fluence effects cannot occur, as the in silico phantoms contain only straight tubular vessel structures that run orthogonal to the imaging plane. additionally, other a priori assumptions of the parameter space were made, such as a constant background absorption, an overall constant scattering coefficient, and a fixed illumination geometry. due to the homogeneous nature of the background structure, the errors observed in our study are highly specialized to our dataset. this is especially apparent with the direct estimation method, as here, e µ a is never greater than 0.2%. as such, we focus on reporting the errors in the roi, as only reporting the results of the entire images would be misleading. in order for the method to generalize to more complex or in vitro datasets and yield similar µ a and confidence estimation results, more elaborate and diverse datasets would need to be simulated. nevertheless, the experiments demonstrate that applying an roi threshold based on the estimation of the quantification error can lead to an increase in accuracy for a given dataset regardless of the underlying qpai method."
"the powerful cnn models have achieved good results in face detection task [cit], so we also compare mlp with cnn under the proposed funnelstructured cascade framework. two commonly used cnn models are considered in the comparison, i.e. lenet [cit] and alexnet [cit], and they serve as replacements for the final performance curves on fddb are given in figure 9 . as is shown, the mlp cascade outperforms lenet by a large margin and also performs better than the 8-layer alexnet. this is most likely because the semantically consistent shapeindexed features are more effective than the learned convolutional features. considering the result that the mlp with hand-crafted features has the ability to defeat deep cnn models, it implies that a well-designed model with considerations for the problem can be better than an off-the-shelf cnn."
"quantification of epistemic confidence: i is converted into an image i epistemic reflecting the epistemic confidence. for this purpose, we use the external model to estimate the quantification error e µ a of the qpai algorithm."
"to evaluate the proposed fust detector for multi-view face detection, as well as to analyse the detector in various aspects, extensive experiments are performed on two challenging face datasets."
"we use 1 stage with a total of 150 lab features for the fast lab cascade, and 3 stages for the coarse mlp cascade, which exploit 2, 4 and 6 surf features respectively. surf features are extracted based on local patches, which will cover redundant information if there is considerable overlap between them. therefore a large step of"
"compared with multi-view face detectors like surf cascade, acf, and head hunter, which all employ a parallel structure, our fust detector performs better on fddb, indicating the superiority of our funnel structure. with as few as 100 false positives, the fust detector achieves a high recall of 85%, which is quite favorable in practical applications. compared with the impressive deep-network-based methods, we achieve comparable performance with that of cascade cnn. however, as stated in section 3.2, our fust detector enjoys a more favorable speed, taking only 50ms to detect a vga image with a single thread on cpu. by contrast, cascade cnn costs 110ms on cpu. on afw dataset, our pr curve is comparable to or better than most methods, further demonstrating that our fust detector is favorable for multi-view face detection."
"surf features are more expressive than lab features, but are still computationally efficient benefited from the integral image trick. therefore face windows can be better differentiated from non-face windows with low time cost. furthermore, mlp is used with surf feature for window classification, which can better model the non-linear variations of multi-view faces and diverse non-face patterns with the equipped nonlinear activation functions."
"from a qpai perspective, end-to-end deep learning-based inversion of pa data is feasible in specific contexts and for specific in silico datasets, as shown previously [cit] and in this work. however, pa signal quantification cannot be regarded as solved in a general manner. one of the main reasons is the large gap between simulated in silico data and in vivo recordings. in order for deep learning to tackle this problem, either highly sophisticated unsupervised domain adaptation methods have to be developed, or a large number of labeled correspondences between the simulation domain and real recorded images need to be provided, which is not currently feasible due to the lack of methodology to reliably measure ground truth optical properties in in vivo settings. however, with the promising progress in pa image reconstruction from limited-view geometries with deep learning techniques (cf., e.g., [cit] ), it might be possible to start bridging the gap and to improve on the current methods for qpai."
"we first conduct a detailed analysis of the proposed fust detector to evaluate its performance from various perspectives. specifically, we compare different view partitions, verify the effectiveness of shape-indexed features, assess the accuracy of shape predictions, and compare the final mlp cascade with two widely used cnn models. different view partitions at the beginning, we adopt a divide-and-conquer strategy to treat faces in different views with separate lab cascade classifiers. this makes it possible for such simple classifiers to reject a large proportion of non-faces windows, while retaining a high overall recall of faces. to explore the impact of different view partitions, we compare two typical partition schemes: (1) five-view partition, i.e. left full profile, left half profile, near frontal, right half profile, and right full profile; (2) two-view partition, i.e. near frontal, profile. note that in the second two-view partition scheme, left and right profile faces are mixed together, and half profile faces are mixed with frontal ones. to supplement the training set with more half profile face images, we also use some images from celeba dataset [cit] . the recall of faces with the two schemes are presented in table 1 . here we manually partition the fddb into two subsets of profile and frontal faces to evaluate on them separately. the former contains 527 profile faces from 428 images, and the latter, i.e. the frontal face subset, contains the rest faces including both near frontal and some half profile faces. as can be seen, the recall of faces with the five-view partition, especially the recall of profile faces, are higher than that with the two-view partition when both scheme remove over 99% of candidate windows. as expected, the finer partition allows classifiers to cover more variations within each view of faces, and is beneficial for obtaining higher recall. this demonstrates the effectiveness of using a reasonably wide top in the proposed funnel structure."
"the authors would like to thank david zimmerer for letting us tap into his well of knowledge on deep learning, clemens hentschke and the sidt group for their support on the state of the art in non deep learning uncertainty quantification, niklas holzwarth for proofreading the manuscript, and the itcf of the dkfz for the provision of their computing cluster for data simulation."
"where x i is the feature vector of the i-th training sample and y i the corresponding label as either 1 or 0, representing whether the sample is a face or not. the problem in eq. (4) can be easily solved by using gradient descent under the back propagation framework [cit] . we employ multiple coarse mlps to construct an attentional cascade, in which the number of features used and the size of the network gradually increase stage by stage. the surf features used at each stage is selected by using group sparse [cit] . since the mlp cascade classifiers have stronger ability to model face and non-face variations, windows passing through multiple lab cascade classifiers can be handled together by one model, i.e. one mlp cascade can connect to multiple lab cascade classifiers."
"an overview of the framework of fust detector is presented in figure 3 . specifically, the fust detector consists of three coarse-to-fine stages in consideration of both detection accuracy and computational cost, i.e. fast lab cascade classifier, coarse mlp cascade classifier, and fine mlp cascade classifier. an input image is scanned according to the sliding window paradigm, and each window goes through the detector stage by stage."
"in this paper, we have proposed a novel multi-view face detection framework, i.e. the funnel-structured cascade (fust), which has a coarse-to-fine flavor and is alignmentaware. the proposed fust detector operates in a gathering style, with the early stages of multiple parallel models reaching a high recall of faces at low cost and the final unified mlp cascade well reducing false alarms. as evaluated on two challenging datasets, the fust detector has shown good performance, and the speed of the detector is also quite favorable. in addition, the alignment-awareness nature of our fust detector can be leveraged to achieve a good initial shape for subsequent alignment models with minor cost."
"the diagrams illustrates how rfc6830 lisp based sdn overlays can implement such interconnect for virtualized functions by surrounding data-center spines or national backbones. such solutions have been deployed in production tier1 carriers over the past 18 months, dynamically connecting mobility, content and signaling functions to 10s of millions of users."
output generation: a threshold over i aleatoric yields a binary image with an roi representing confident pixels in i µ a according to the input signal intensity. we then proceed to narrow down the roi by applying a confidence threshold (ct) which removes the n% least confident pixels according to i epistemic .
"after the stages of lab cascade, most of the non-face windows have been discarded, and the remaining ones are too hard for the simple lab feature to handle. therefore, on subsequent stages, the candidate windows are further verified by more sophisticated classifiers, i.e. mlp with surf (speeded-up robust feature) [cit] . to avoid imposing too much computational cost, small networks are exploited to perform a better but still coarse examination."
"the rest of the paper is organized as follows. section 2 describes the proposed fust detector in detail, explaining the design of different stages from top to bottom. section 3 presents the experimental results on two challenging face detection datasets together with analysis on the structure and shape prediction. the final section 4 concludes the paper and discusses the future work."
"to further investigate the potential of our fust detector on fddb, we trained a new detector fust-wf with a more diverse dataset wider face [cit] . wider face dataset covers much more face variations, which is beneficial for obtaining higher performance. since wider face does not provide landmark annotations for faces, we only trained one stage for the unified mlp cascade with mean shape. as shown in figure 10, fust-wf achieves obvious performance boost, further demonstrating the effectiveness of the funnelstructure design. with higher quality and more data, the fust detector can continue to improve."
"to recap, in order to virtualize carrier functions we break monolithic systems to components running on virtual machines and we assemble these dynamically using sdn flow-mapping. we scale sdn control to be able to keep track of the potentially hundreds of millions of subscriber flows and their affinityassociation to virtualized function instances. to do that we extend sdn control model to decouple not just control and forwarding, but also the control and forwarding topologies. this is done using overlays and results in the below sdn-nfv design characteristics:"
"(1), a window will be rejected if and only if it is classified as negative by all lab cascade classifiers. using multiple models will cost more time, but all models can share the same lab feature map for feature extraction. therefore more models add only minor cost and the overall speed is still very fast as a high recall is reached. besides the high recall, the parallel structure also allows more flexibility in view partitions. since it does not suffer from missing detections caused by incorrect branching as in tree structure, a rough rather than an accurate view partition is enough. in other words, degenerated partitions with incorrect view labeling of faces has minor influences on the overall recall of all lab cascade classifiers. it is even applicable for automatic view partition from clustering or that based on other factors."
"we hypothesized that our confidence metric is indicative of e µ a . in the experiments, we showed that a deep learning model is able to learn a representation of the errors of the quantification method leading to error improvements of 10-50% in region-of-interest structures and yielding up to 5-fold improvements in background structures. furthermore, figure 5 shows that the absorption estimation error does not decrease monotonously, especially for the qpai methods that yield more accurate results. one reason for this might be that the confidence estimates are not correlated to the quantification error and that low confidences might still correspond to low errors. one has to point out the worse performance of the quantification methods when applied directly to raw time-series data. one reason for this might be that the addition of the acoustic inverse problem and the inclusion of a realistic noise model greatly increased the complexity of the problem and reduced the amount of information in the data due to, e.g., limited-view artifacts. at the same time, we did not increase the number of training samples or change the methodology to account for this."
"mlp is a type of neural network consisting of an input layer, an output layer, and one or more hidden layers in between. an n-layer mlp f (·) can be formulated as"
"hypervisors overlay priority control rings around standard cpus, enabling multiple vm images to run concurrently on servers, reducing their span to the compute resources within the server each vm can use, enabling sharing with minimal cross interference. virtualization overlay applies to the network as well. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. having unbundled functions to small virtual units (scatter) we turn to their dynamic assembly, global coordination, and scaling. for this task (gather) we turn to network virtualization and software defined networks (sdn). in order to perform this role sdn needs to map the right traffic flows, in the right sequence, to the right virtual functions. in order for that to happen, we must first enhance the sdn control model to overcome built-in nonlinear scale limitations (similar ones to out-of-band circuit switching). we add to the open-flow interface, decoupling control and forwarding.. an overlay interface, decoupling control and forwarding topologies. the overlay needs for global-lookup are realized using a mapping database, built-in and distributed by the underlying ip network the resulting solution is a virtual network that functions as a flow-mapping-fabric connecting virtualized network functions; balancing load among instances of classes, preserving affinity of subscriber traffic to associated functional instances, protecting states, and assembling end-to-end services."
"operators want to streamline their infrastructures through nfv and this predicates a consolidation of network functions onto industry-standard servers, switches, and storage hardware located in data and distribution-centers. the benefits of such consolidation and cots migration include significant cost cutting, operational efficiency, as well as accelerated revenue generating innovation."
"with the more expressive shape-indexed features, larger mlps with higher capacity of nonlinearity are used to perform finer discrimination between face and non-face windows. moreover, different from previous ones, the larger mlps predict both class label, indicating whether a candidate window is a face, and shape simultaneously. an extra term of shape prediction errors is added to the objective function in eq. (4). the new optimization problem is the following"
"photoacoustic imaging (pai) has been shown to have various medical applications and to potentially benefit patient care [cit] . it is a non-invasive modality that offers the ability to measure optical tissue properties, especially optical absorption µ a, both locally resolved and centimeters deep in tissue. knowledge of these properties allows for deriving functional tissue parameters, such as blood oxygenation so 2, which is a biomarker for tumors and other diseases [cit] . the photoacoustic (pa) signal is a measure of the pressure waves arising from the initial pressure distribution p 0, which depends mainly on µ a, the grüneisen coefficient γ, and the light fluence φ, which is shaped by the optical properties of the imaged tissue [cit] . because of this dependence, the measured p 0 is only a qualitative indicator of the underlying µ a, because even if the initial pressure distribution could be recovered perfectly, estimation of the light fluence is an ill-posed inverse problem that has not conclusively been solved [cit] ."
"along with the steady progress of face detection, there have been mainly three categories of face detectors with different highlights. the most classic are those following the boosted cascade framework [cit], originating in the seminal work of viola and jones [cit] . these detectors are quite computationally efficient, benefited from the attentional cascade and fast feature extraction. then to explicitly deal with large appearance variations, deformable part models (dpm) [cit] are introduced to simultaneously model global and local face features [cit], providing an intuitive way to cover intra-class variations and thus being more robust to deformations due to pose, facial expressions, etc. dpm has established a reputation for its promising results on challenging datasets, but detection with dpm is time-consuming, inspiring researches on speeding up techniques [cit] . recently, detectors based on neural networks, e.g. convolutional neural networks (cnn) [cit], have attracted much attention and achieved magnificent accuracy on the challenging fddb dataset [cit], as they enjoy the natural advantage of strong capability in non-linear feature learning. the weakness of cnn-based detectors is their high computational cost due to intensive convolution and complex nonlinear operations."
"fast and accurate detection of human faces is greatly demanded in various applications. while current detectors can easily detect frontal faces, they become less satisfactory when confronted with complex situations, e.g. to detect faces viewed from various angles, in low resolution, with occlusion, etc. especially, the multi-view face detection is quite challenging, because faces can be captured almost from any angle -even exceeding 90"
"this section gives an overview of the confidence estimation approach, the experiments, and the used dataset and briefly introduces the different qpai methods that are being used."
"fixing previous stages, we compare the two types of fine mlp cascades on fddb. the performance curves are presented in figure 6 . as expected, using more accurate shapes brings performance gain, demonstrating the effectiveness of shape-indexed features for multi-view faces. shape-indexed features from two faces have good semantic consistence, thus reducing intra-class variations and increasing inter-class distinctions. this makes it easier to distinguish face from non-face windows."
"approximate entropy is a recently formulated family of parameters and statistics quantifying regularity (orderliness) in serial data [cit] . it has been used mainly in the analysis of heart rate variability [cit], endocrine hormone release pulsatility [cit], estimating regularity in epileptic seizure time series data [cit] and estimating the depth of anesthesia [cit] . approximate entropy assigns a non-negative number to a time series, with larger values corresponding to more complexity or irregularity in the data. eeg signal represents regular and uniform pattern during synchronized cooperative function of cortical cells. this pattern results to low entropy values. in contrast, concentric functions and higher levels of brain activity lead to high values of entropy. shannon entropy h is defined as:"
location heuristic psychophysical study shows that human attention favors central regions [cit] . so pixels close to a natural image center could be salient in many cases. our location heuristic is thus written as
"our method utilizes information from multiple image layers, gaining special benefit. single-layer saliency computation does not work similarly well. to validate it, we takes i in eq. (4) in different layers, as well as the average of them, as the saliency values and evaluate how they work respectively when applied to our cssd image data. the precision-recall curves are plotted in fig. 11 . result from layer l 1 is the worst since it contains many small structures. results in the other two layer with larger-scale regions perform better, but still contain various problems related to scale determination. the result by naively averaging the three single-layer maps is also worse than our final one produced by optimal inference."
"global methods mostly consider color statistics. zhai and shah [cit] introduced image histograms to calculate color saliency. to deal with rgb color, [cit] provided an approximate by subtracting the average color from the low-pass filtered input. [cit] extended the histogram to 3d color space. these methods find pixels/regions with colors much different from the dominant one, but do not consider spatial locations. to compensate the lost spatial information, [cit] measured the variance of spatial distribution for each color. global methods have their difficulty in distinguishing among similar colors in both foreground and background. a few recent methods exploit background smoothness [cit] . note that assuming background is smooth could be invalid for many natural images, as explained in section 1."
"cue maps reveal saliency in different scales and could be quite different. at the bottom level, small regions are produced while top layers contain large-scale structures. due to possible diversity, none of the single layer information is guaranteed to be perfect. also, it is hard to determine which layer is the best by heuristics."
"1 is the i th output of layer 1, μ a i ( x ) and μ b i -2 ( x ) are type a and type b arbitrary fuzzy membership functions of nodes i and i -2, respectively. in the second and third layer, the nodes are fixed nodes. they are labeled m and n respectively, indicating they perform as a simple multiplier. the outputs of these layers can be represented as:"
"where a is state transition matrix, b is observation probability matrix, π is initial condition probability matrix, o represents the sequence of feature vectors. in this context the superscript a, denotes the first stream parameters and superscript b denotes the second stream parameters."
"our method is as follows. first, three image layers of different scales are extracted from the input. saliency cues are computed for each layer. they are finally fused into one single map using a graphical model. these steps are described in sections 3.1 -3.3 respectively. the framework is illustrated in fig. 2 ."
"and h 1, h 2 and e are real matrices that define how a and c elements are affected due to f k variations. these matrices are estimated using a separate recursive least square estimation [cit] ."
"training of the above mentioned multistream model could be done by synchronous and asynchronous methods. in this section the multistream approach is based on pseudo-synchronous coupled hidden markov model. as it will be shown, it is an interesting option for multimodal continuous speech identification because of, 1) synchronous multimodal continuous speech identification, 2) consideration of asynchrony between streams. some resynchronization points are defined at the beginning and end of bci segments including phonemes or words."
"although images from msra-1000 [cit] have a large variety in their content, background structures are primarily simple and smooth. to represent more general situations that natural images fall into, we construct a complex scene saliency dataset (cssd) with 200 images. they contain diversified patterns in both foreground and background. ground truth masks are produced by 5 helpers. these images are collected from bsd300 [cit], voc dataset [cit] and internet. our dataset is now publicly available."
"for each layer we extract, saliency cues are applied to find important pixels from the perspectives of color, position and size. we present two cues that are particularly useful."
"fuzzy theory provides the basis for fuzzy inference systems (fis) which is a useful tool for classifications of data, static and dynamic process modeling and identification, decision making, classification, and control of processes. these characteristics could be used in different kinds of fis and be applied to brain computer interface (bci) systems which are discussed in this chapter."
"usually nonlinear kernels provide classification hyper-planes that cannot be achieved by linear weighting [cit] . using appropriate kernels, svm offered an efficient tool for flexible classification with a highly nonlinear decision boundary. rbf kernel was used in this section which is defined as:"
"fractal dimension emphasizes the geometric property of basin of attraction. these dimension show geometrical property of attractors and is also computed very fast [cit] . our goal was to associate each 5-second segment data as a trial to its corresponding class. to do this, features were extracted from each 1 second segment with 50% overlap, and sequence of 9 extracted features were considered as the feature vector of a 5-second segment, which was to be modeled and classified. in higuchi's algorithm, k new time series are constructed from the signal x ( 1 ), x ( 2 ), x ( 3 ), …, x ( n ) under study is [cit] :"
a feature extraction algorithm based on the kalman estimation discussed. this estimator minimizes the error covariance and therefore is an optimum estimator if appropriate initial condition is selected. kalman estimator is used for adaptive estimation of dynamic parameters of eeg. the estimator reduces the error variance adaptively and after a period of time a unique estimation is achieved.
"in which f i is the firing rate, p i is the x scale, q i is the y scale, and r i is the bias for i th node. in the fifth layer, there is only one single fixed node that performs the summation of all incoming signals:"
where β l controls the layer confidence ands based on our encompassment definition and the segment generation procedure. e s is defined on them as
"aiming to solve this notorious and universal problem, we propose a hierarchical model, to analyze saliency cues from multiple levels of structure, and then integrate them to infer the final saliency map. our model finds foundation from studies in psychology [cit], which show the selection process in human attention system operates from more than one levels, and the interaction between levels is more complex than a feed-forward scheme. with our multi-level analysis and hierarchical inference, the model is able to deal with salient small-scale structure, so that salient objects are labeled more uniformly."
"use of multimodal approaches provides improvement in robustness of classification under disturbances. this could extend the margin of security for bci systems. integration of two sets of features namely set a and set b could be done by various methods. several integration models have been proposed in the literature that can be divided into early integration (ei) and late integration (li). in the ei model, information is integrated in the feature space to form a feature vector combined of both features. classification is based on this composite feature vector. the model is based on assumption of conditional dependence between different modes and therefore is more general than the li model. in the li model, the modules are pre-classified independently of each other. the final classification is based on the fusion of both modules by evaluating their joint occurrence. this method is based on assumption of conditional independency of both data streams. it is generally accepted that the auditory system performs partial identification in independent channels, whereas, bci classification seems to be based on early integration which assumes conditional dependence between both modules [cit] . this theory is based on li method and models pseudo-synchronization between modules to account some temporal dependency between them. it is a compromise of ei and li schemes. the bimodal signal is considered as an observation vector consists of two sets of features. optimum classifier which is based on bayesian decision theory could be obtained using the maximum a posteriori probability function [cit] :"
"the parameters are nonlinear which could be seen in equations 3 and 4. the system identification is based on linear arma model that means the parameters are computed well only around the operating point of the system. the operating point of system is time-varying and therefore the parameters vary with time. therefore, the feature vector parameters are computed at each frame during train and test of hmm."
"the energy consists of two parts. data term e d (s l i ) is to gather separate saliency confidence, and hence is defined, for every node, as"
"the state-space representation of a linear system is much more flexible and useful than the transfer function form because it includes both time-dependent and time-independent systems and also encompasses stochastic and deterministic systems. furthermore, it is possible to evaluate precisely concepts of observability and controllability, which are useful in determining whether the desired unknown parameters of a system can be estimated from the given observations for instance."
local contrast image regions contrasting their surroundings are general eye-catching [cit] . we define the local contrast saliency cue for r i in an image with a total of n regions as a weighed sum of color difference from other regions:
the support vector machine (svm) method has been used extensively for classification of eeg signals [cit] . it is shown that eeg signal has separable intrinsic vectors which could be used in svm classifier. svm classifiers use discriminant hyper-planes for classification. the selected hyper-planes are those that maximize the margin of classification edges. the distance from the nearest training points usually measured based on a non-linear kernel to map the problem to a linear solvation space [cit] . a radial basis function (rbf) kernel based svm is proposed here that lagrangian optimization is performed using an adjustable anfis algorithm. it will be shown that due to conceptual nature of bci for patients this proposed method leads to adjustable soft decision classification.
the svm has been used extensively for classification of eeg signals. it is shown that eeg signal has separable intrinsic vectors which could be used in svm classifier. svm classifiers use discriminant hyper-planes for classification. the selected hyper-planes are those that maximize the margin of classification edges. the distance from the nearest training points usually measured based on a non-linear kernel to map the problem to a linear solvation space. a rbf kernel based svm is proposed here that lagrangian optimization is performed using an adjustable anfis algorithm. it will be shown that this method leads to adjustable soft decision classification.
"once the optimal consequent parameters are found, the backward pass starts immediately. the gradient descent method (backward pass) was used to adjust optimally the premise parameters corresponding to the fuzzy sets in the input domain. the output of the anfis was calculated by employing the consequent parameters found in the forward pass. the output error was used to adapt the premise parameters by means of a standard back propagation algorithm. it has been shown that this hybrid algorithm is highly efficient in training the anfis [cit] ."
"the task of the learning algorithm for this architecture is to tune all the above mentioned modifiable parameters to make the anfis output match the training data. when the premise parameters of the membership function are fixed, the output of the anfis model can be written as:"
use of parametric model provides a time variant estimate of state equations of system at the working point. it is in accordance to the highly variable nature of eeg signals. in fact the modeling of nonlinear and time variant signal using a feature vector with limited dimensions provides a filtering on data. the abstract characteristics of signal would be extracted that leads to lower variance compared to the frequency feature extraction methods. dimension of feature vector is a critical point to avoid over learning for high values of features or lake of convergence due to small size of feature vector.
"through the feature space constructed using approximate entropy and fractal dimension in addition to conventional spectral features, different stages of eeg signals can be recognized from each other expressly. these methods provide better performance in datasets with lower dimension. the rbf kernel of svm modified by anfis can be a strong base for improved methods in the field of bci cognition and analysis for therapeutic applications."
"bci with dynamic parameters was studied in this section. the algorithm adopted is based on pseudo-synchronous hidden markov chains to model the asynchrony between events. the proposed combination of arma model and kalman filtering for feature extraction resulted to the best identification rates compared to usual methods. complimentary effect of video information and dynamic parameters on bci was studied. effectiveness of the proposed identification system was more beneficial in low signal to noise ratios which reveals the robustness of the algorithm adopted in this study. owing to high rate of information, the voice of speakers has significant effect on the identification rate; however, it reduces rapidly due to environmental noise. it was also shown that a specific combination weight of voice and video information provides optimum identification rate which is dependent on the signal to noise ratio and provides low dependency on the environmental noise. the phonetic content of spoken phrases was evaluated and the phonemes were sorted based on their influence on bci rate. identification rate of proposed model based system was compared to other parameter extraction methods including kalman filtering, neural network, anfis system and auto regressive moving average. the combination of proposed model with kalman filter led to the best identification performance. combination of feature vectors content has a great roll on identification rate, therefore, more efficient methods rather than pseudo-synchronized hidden markov chain could be used for better achievements. application of feature extraction methods like sample entropy, fractal dimension and nonlinear model based approaches have shown appropriate performance in bci processing and could result to better identification rates in this area. lip shape extraction is a critical point in this identification method and more robust algorithms provide accurate and precise results."
"these examples are not special, and exhibit one common problem -that is, when objects contain salient smallscale patterns, saliency could generally be misled by their complexity. given texture existing in many natural images, this problem cannot be escaped. it easily turns extracting salient objects to finding cluttered fragments of local details, complicating detection and making results not usable in, for example, object recognition [cit], where connected regions with reasonable sizes are favored."
"for a node corresponding to region i in layer l l, we define a saliency variable s l i . set s contains all of them. we minimize the following energy function"
"follow previous settings, we also quantitatively compare our method with several others with publicly avail- able implementation. we plot the precision-recall and fscore curves in fig. 10(a)-(b) . the difference between our method and others is clear, manifesting the importance to capture hierarchical saliency in a computationally feasible framework."
"the combination of proposed model with kalman filter can lead to the best identification performance. combination of different feature sets has a great roll on classification rate, therefore, more efficient methods rather than pseudo-synchronized hidden markov chain could be used for better achievements. application of feature extraction methods like sample entropy, fractal dimension and nonlinear model based approaches have shown appropriate performance and could result to better identification rates in this area."
"saliency detection, which is closely related to selective processing in human visual system [cit], aims to locate important regions or objects in images. it gains much attention recently [cit] . knowing where important regions are broadly benefits applications, including classification [cit], retrieval [cit] and object co-segmentation [cit], for optimally allocating computation."
"in addition, contributions in this paper also include 1) a new measure of region scales, which is compatible with human perception on object scales, and 2) construction of a new scene dataset, which contains challenging natural images for saliency detection. our method yields much improvement over others on the new dataset as well as other benchmarking data."
"stemming from psychological science [cit], the commonly adopted saliency definition is based on how pixels/regions stand out and is dependent of what kind of visual stimuli human respond to most. by defining pixel/region uniqueness in either local or global context, existing methods can be classified to two streams. local methods [cit] rely on pixel/region difference in the vicinity, while global methods [cit] rely mainly on color uniqueness in terms of global statistics."
"in the following sections, the features which are used to compose feature vectors are discussed. these features provide various views of eeg signal which can be used in anfis classification system."
"generally, lyapunov exponents can be extracted from observed signals in two different ways [cit] . the first method is based on the idea of following the time evolution of nearby points in the state space [cit] . this method provides an estimation of the largest lyapunov exponent only. the second method is based on the estimation of local jacobi matrices and is capable of estimating all the lyapunov exponents [cit] . vectors of all the lyapunov exponents for particular systems are often called their lyapunov spectra. this method was used for lyapunov vector extraction in this section. an optimized size of 7 is considered for the vector which led to the best mean classification rate on the support vector machine (svm) classifier."
"multi-layer fusion by naively averaging all maps is not a good choice, considering possibly complex background and/or foreground. on the other hand, in our region merging steps, a segment is guaranteed to be encompassed by the corresponding ones in upper levels. we therefore resort to hierarchical inference based on a tree-structure graphical model. an example is shown in fig. 2(e), where nodes represent regions in their corresponding layers. for instance, the blue node j corresponds to the region marked in blue in (d). it contains two segments in the lower level and thus introduces two children nodes. the root maps to the entire image in the coarsest representation."
"visual comparison of results are shown in fig. 9 . on these difficult examples, our method can still produce reasonable saliency maps. more results are available online."
"the first kind of fis is designed based on the ability of fuzzy logic to model human perception. these fis elaborates fuzzy rules originates from expert knowledge and they are called fuzzy expert system. expert knowledge was also used prior to fis to construct expert systems for simulation purposes. these expert systems were based on boolean algebra and were not well defined to adapt to regressive intrinsic of underlying process phenomena. despite that, fuzzy logic allows the rules to be gradually introduced into expert simulators due to input in a knowledge based manner. it also depicts the limitations of human knowledge, particularly the ambiguities in formalizing interactions in complex processes. this type of fis offers high semantic degree and good generalization ability. unfortunately, the complexity of large systems may lead to high ambiguities and insufficient accuracies which lead to poor performances [cit] ."
"to minimize eq. (5), exact inference can be achieved via belief propagation [cit] . it can reach the global optimum due to the tree model. the propagation procedure includes bottom-up and top-down steps. the bottom-up step updates variables s once results are obtained in the top layer, a top-down step is performed. in each layer, since there is already a minimum energy representation obtained in the previous step, we optimize it to get new saliency values. after all variables s l j are updated in a top-down fashion, we obtain the final saliency map in l"
in this method a third order ar model is used that directly estimates eeg parameters without using any intrinsic model. such linear methods could be used to estimate nonlinear systems in piece wise linear manner.
"in fact, solving the three layer hierarchical model via belief propagation is equivalent to applying a weighted average to all single-layer saliency cue maps. our method differs from naive multi-layer fusion by selecting weights optimally for each region in hierarchical inference instead of global weighting. the proposed solution theoretically and empirically performs better than simply averaging all layers, while not scarifying much computation efficiency."
"an overview of our hierarchical framework. we extract three image layers from the input, and then compute saliency cues from each of these layers. they are finally fed into a hierarchical model to get the final results. ing between expression capability and structure complexity. the layer number is fixed to 3 in our experiments. in the bottom level, finest details such as flower are retained, while in the top level large-scale structures are produced."
"there are methods for constructing fuzzy structures by using rule-based inference. these methods extract the rules directly from data and can be considered as rule generation approaches. generation of the rules includes preliminary rule generation and rule adaptation according to input and output data. automatic rule generation methods were applied to simple systems with limited number of variables. these simple systems do not need to optimize the rule base. it is different for complex systems. the number of generated rules becomes enormous and the description of rules becomes more complex due to the number of variables. the simulator will be easier to interpret if it is defined by the most influential variables. also, the system behavior will be more comprehensive when the number of rules becomes smaller. therefore, variable selection and rule reduction are two important subcategories of the rule generation process which is called structure optimization. a fis has many more parameters that can also be optimized including membership functions and rule conclusions. a thorough study in these fields and their respective advantages and considerations are provided in the following sections."
"a classification method base on anfis adapted svm proposed in this section. it showed that non-linear features improve classification rate as an effective component. through the feature space constructed using approximate entropy and fractal dimension in addition to conventional spectral features, different stages of eeg signals can be recognized from each other expressly. the successful implementations of anfis-svm for eeg signal classification was reported in this context. the results confirmed that this method provides better performance in datasets with lower dimension. this performance achieved for rbf kernel of svm modified by anfis. this section can be a strong base for improved methods in the field of bci cognition and analysis for therapeutic applications."
a hybrid algorithm combining the least squares method and the gradient descent method was adopted to identify the optimal values of these parameters [cit] . the hybrid algorithm is composed of a forward pass and a backward pass. the least squares method (forward pass) was used to optimize the consequent parameters with the premise parameters fixed.
"this is a linear combination of the modifiable consequent parameters p 1, q 1, r 1, p 2, q 2 and r 2 . when the premise parameters are fixed the least squares method is used easily to identify the optimal values of these parameters after adjustment of anfis weights using svm. when the premise parameters are not fixed, the search space becomes larger and the convergence of the training becomes slower."
"1 . an example is shown in fig. 6 where separate layers in (b)-(d) miss out either large-or small-scale structures. our result in (e) contains information from all scales, making the saliency map better than any of the single-layer ones."
"which are the so-called normalized firing strengths. in the fourth layer, the nodes are adaptive nodes. the output of each node in this layer is simply the product of the normalized firing strength and a first order polynomial for the first order sugeno model. the outputs of this layer are given by:"
the estimate and process errors are considered independent additive white gaussian noises. in practice these are time varying processes which are considered stationary for simplicity.
"another class of modeling tools is based on adaptive knowledge based learning from data. this category includes supervised learning and outputs of observations when the training data is provided. a numerical performance index can be defined in such simulators which are usually based on the mean square error. neural networks have become very popular and efficient in this field. their main characteristic is the numerical accuracy while they also provide a qualitative black box behavior. the first self-learning fis was proposed by sugeno that provided a way to design the second kind of fis [cit] . in this case, even if the fuzzy rules are expressed in the form of expert rules, a loss of semantic occurs because of direct generation of weights from data. these types of simulators are usually named adaptive network fuzzy inference system (anfis)."
"we have tackled a fundamental problem that small-scale structures would adversely affect salient detection. this problem is ubiquitous in natural images due to common texture. in order to obtain a uniformly high-response saliency map, we propose a hierarchical framework that infers importance values from three image layers in different scales. our proposed method achieves high performance and broadens the feasibility to apply saliency detection to more applications handling different natural images."
"some resynchronization points are defined at the beginning and end of bci segments including phonemes or words. combination of the independent likelihoods is done by multiplying the segment likelihoods from the two streams, thus assuming conditional independence of the streams according to:"
"albeit many methods have been proposed, a few commonly noticeable and critically influencing issues still endure. they are related to complexity of patterns in natural images. a few examples are shown in fig. 1 . for the first two examples, the boards containing characters are salient foreground objects. but the results in (b), produced by a previous local method, only highlight a few edges that scatter in the image. the global method results in (c) also cannot clearly distinguish among regions. similar challenge arises when the background is with complex patterns, as shown in the last example of fig. 1 . the yellow flowers lying on grass stand out. but they are actually part of the background when viewing the picture as a whole, confusing saliency detection."
"prior work does not consider the situation that locally smooth regions could be inside a salient object and globally salient color, contrarily, could be from the background. these difficulties boil down to the same type of problems and indicate that saliency is ambiguous in one single scale. as image structures exhibit different characteristics when varying resolutions, they should be treated differently to embody diversity. our hierarchical framework is a unified one to address these issues."
some promising methods for feature extraction and classification of eeg signal are described in this chapter. the aim of these methods is to overcome the ambiguities encountering bci applications.
"the epnn possesses the ability to perform a neural pruning function to simplify its topological morphology. the neural pruning technique represents omitting the extra nodes and weights by learning and training the neural network [cit] . in the epnn, the pruning function can eliminate unnecessary synapses and dendrites and then form a unique neural structure for a given problem. the function contains two parts: synaptic pruning and dendritic pruning. synaptic pruning: when the synaptic layer that receives the input from the axon is in the constant-1 connection case, the synaptic output is always 1. because of the multiplication operation, the result of any arbitrary value multiplying 1 will equal itself in the dendrite layer. it is evident that the synaptic input in the constant-1 connection has minimal impact on the output of the dendrite layer. therefore, this type of synaptic input can be neglected entirely."
"5) in addition, when the effect of maintenance is considered, the actual corrected life expectancy was 43.57 year, far higher than the value of the calculated 23.016 year when the maintenance effect is not considered. this also reflects that after the overhaul, the insulation fault of the transformer has been dealt with in time, the operation conditions have been improved, and the life expectancy loss has also been reduced. besides, after considering the effect of oil filtering, the failure rate of the transformer operating to sixth years is 0.005%, much lower than the 0.1% before the correction. at this point, the corrected failure rate curve becomes a dynamic staircase-like piecewise curve, which shows that the failure rate of the transformer is updated with the operating state of the transformer. hence, compared with usual method, the model developed in this paper can diagnose the internal fault of transformer more accurately and the evaluation values can better track and reflect the actual operating state of the transformer, thus the effectiveness and feasibility of the proposed model can be verified."
"the remainder of the paper is organized as follows. the method of transformer reliability assessment is briefly summarized in section 1. the hst based static ageing failure model is established in section 2. in section 3, the grey target theory based dynamic correction model is developed combing with the dga data analysis. practical case study is carried out in section 4. at last, section 5 concludes the paper."
"eq. (1) is adequate to illustrate various equipment failure rates by carefully varying the system parameters, namely β and η, thus it has a strong compatibility, here, several examples are given as follows:"
"subspace learning has been extensively studied and widely used in many real-world applications, such as face recognition, object recognition, and visualization. the basic idea of subspace learning methods is to project high-dimensional samples into a low-dimensional subspace, in which some specific properties could be satisfied. according to the usage of class labels, subspace learning methods are mainly divided into three categories: 1) unsupervised methods; 2) supervised methods; and 3) semisupervised methods. in this paper, we only focus on the supervised ones."
"it is assumed that the transformer has already been overhauled once since it was put into operation. here, this main transformer is assumed to have been conducted a heavy oil filtering maintenance. hence, the corresponding dga data reference values of the transformer are shown in table 4, from which the life expectancy loss of the transformer before maintenance can be obtained."
"to evaluate the performance of the classification methods, each dataset is separated randomly into two subsets: a training set and a testing set. the training subset is used to set up the classification model, and the testing dataset is adopted to test the model's accuracy. the splitting strategy is significantly relevant to achieve reliable model evaluation because the case data are usually very scarce. according to prior experiments, a larger training set results in a better classifier [cit] . in this simulation, 50% of the samples are chosen randomly for training, while the remaining 50% is for testing to guarantee high test accuracy. the average value of the classification accuracy rate over 30 runs is regarded as the overall classification performance."
"unfortunately, few studies have been conducted to investigate the transformer reliability assessment and condition analysis, especially for the transformer oil-paper insulation system (topis), as the core part of a transformer, which normally determines the lifetime and directly influences the normal transformer operation. hence, it is very crucial to prevent the transformer from premature insulation failures through a timely reliability assessment and control. at present, the majority of topis researches are just carried out about the rules of ageing characteristic parameters while the issues of reliability assessment has not yet been fully addressed. w. j. mcnutt [cit] developed a thermal ageing model for the oil-paper insulation of transformer in order to ensure a reliable transformer operation. however, this model is inadequate to achieve dynamic corrections due to the use of one-dimensional input. based on a hybrid weibull model, a thorough analysis of interrelated characteristic parameters was undertaken to properly select the characteristic parameters [cit], e.g., degree of polymerization (dp), volume fraction of furaldehyde (vff). however, these characteristic parameters are relatively difficult to be found [cit] and the model feasibility was merely validated by small-scale laboratory experiments. for example, at present, most power-supply enterprises/bureaux in china do not meet the requirement of dp testing, and they also rarely test the furfural content. this situation has made it very difficult to obtain the characteristic parameters of furfural. in addition, the preventive test regulation stipulates that the testing cycle of furfural is 6 years. during the period, the manual operations, such as the transformer oil filtering and transformer maintenance, have great influence on the furfural content, resulting in a relatively low accuracy of the model, as well as the difficulty of its promotion."
"we evaluate the computational cost of different methods when increasing the sample size. taking coil database as an example, the training times are shown in fig. 10 . since pca, lda, npe, and lsda have similar computational complexity, and fddl has a similar complexity to dlrd, we only compare against latlrr, pca, and dlrd. in fig. 10, we observe that the linear subspace method pca has the lowest training time. our approach and latlrr have similar training time, which is much less than the time cost of dlrd. moreover, the test time of our approach is even less than that of pca, due to the fact that our approach can achieve the best recognition rates with only a few features."
"we theoretically analyze the convexity of supervised regularization termf (p, z ) with respect to z, which is critical to ensure that our model is solvable using alm algorithms. in particular, to guarantee the convexity of (6), we provide theorem 1."
"is the kth state parameter sequence for the equipment state monitoring, and then the standard state mode 0 ω is set up as follows. first, make"
"in our future work, we will develop a divide-and-conquer version of srrs approach to make it scalable to larger data sets, and we would also like to design the dictionary learning algorithms to further enhance the classification performance."
"morphology. in addition to the neural pruning function, the other function worth emphasizing is that the simplified structure of the epnn can form an approximate logic circuit applicable to hardware implementations. figures 16 and 17 present further simplified logic circuits of the structural morphologies. we use an analog-to-digital converter, which can be regarded as a \"comparator\", to compare the input with the threshold . once the input is less than the threshold, the \"comparator\" will output 0; otherwise, it will output 1. using the logic circuits, we can classify the companies into bankrupt and not-bankrupt on both the qualitative bankruptcy dataset and distress dataset. the accuracies of the logic circuits are shown in table 16 . clearly, the test accuracies of the logic circuits do not decrease and are higher than those of the epnn. note that the logic circuits in figures 16 and 17 are selected randomly from an arbitrarily chosen experiment, and they are not unique to each problem. forming a logic circuit can further increase the classification speed of the epnn, thereby creating a more powerful method for the prediction of financial bankruptcy."
", where tr(k ) is the trace of matrix k . s b (p t az) and s w (p t az) are the betweenclass and within-class scatter matrices"
"in this study, we propose an epnn model with a dendritic structure as a global optimization algorithm called the jade algorithm [cit] . with respect to the epnn, the axons of the other neurons transmit the input signals to the synaptic layer; then, the interaction of the synaptic signals transfers to every branch of the dendrites. next, the interactions are collected and sent to the membrane layer and then transformed to the soma body. in addition, the neuronal pruning function can remove extra synapses and dendrites and simultaneously achieve high accuracy. specifically, during the training process, the superfluous inputs and dendrites are eliminated, while the useful and necessary ones are retained. then, the neuronal pruning function can produce a simplified dendritic morphology without a loss of classification accuracy. furthermore, the simplified topological morphology can operate similarly to a logic circuit composed merely of comparators and logic not, and, and or gates. thus, applying epnn to bankruptcy analysis can easily be implemented in hardware. to the best of our knowledge, we note that if achieved through hardware implementation, this technique will achieve the highest computation speed when compared with other methods. this demonstrates an excellent adoption possibility for financial institutions. jade is a state-of-theart variant of the differential evolution algorithm and uses a self-adaptive mechanism to select suitable parameters for each optimization problem. this imbues jade with a better balance between exploration and exploitation compared to other heuristic algorithms [cit] . in training the epnn, jade can avoid local minima and speed up the training process during the optimization process. thus, jade allows the epnn to obtain satisfactory results and produce an effective logic circuit for each bankruptcy prediction problem."
"anns are flexible and nonparametric modelling tools capable of performing any complicated function mapping with arbitrarily required accuracies [cit] . among the diverse types of anns, mlp is one of the simplest and most widely applied models, in which the hidden layer determines the mapping relationships between input and output layers and the relationships between neurons stored as the weights of the connecting links [cit] . the mlp's learning algorithm implements a gradient search to minimize the squared error between the realized and desired outputs. this type of threelayer mlp is a commonly adopted ann structure for binary classification problems such as bankruptcy prediction [cit] . although the characteristics of ann ensembles, such as efficiency, robustness, and adaptability, make them a valuable tool for classification, decision support, financial analysis, and credit scoring, it should be noted that some researchers have shown that the ensembles of multiple neural network classifiers are not always superior to a single best neural network classifier [cit] . hence, we focus on applying a single neural network model to bankruptcy prediction."
"in this section, an srrs approach is proposed. we first formulate our approach as a regularized rank-minimization problem. to solve this problem, we develop an efficient optimization algorithm. the theoretical analysis on convexity is also provided."
"second, since class information is crucial to classification problems, we design a supervised regularization term f (p, z ) based on the idea of fisher criterion [cit], that is,"
"is called index mode sequence. in general, the polarity of the index has three categories as maximum polarity, minimum polarity, and medium polarity, which are represented as polmax, polmin and polmed, and then the selection of standard evaluation model elements is based on as"
a membrane layer accumulates the linear summation of the dendritic signals from the upper dendrite layer. it is similar to the logic or operation in the binary cases. this logic or operation generates a 1 when at least one of the variables is 1. its equation is given below:
"according to the hst based static ageing failure model and grey target theory based dynamic correction model combing with the dga method, the final framework of the whole reliability assessment model of the topis is illustrated in figure 4 ."
"hence, a comparative analysis of the original failure rate of the topis and the one after adding a dynamic correction have been made, as shown in figure 13 and figure 14, respectively. particularly, the failure rate of the topis without any correction is static and approximates to an exponential distribution, which can reflect the relationship between the failure rate (%) and the operation age (year) of the transformer to some extent. however, the other one after adding the dynamic corrections is a stair-stepping segmental curve, in which the transformer failure rate will be updated timely with the changes of operating conditions, thus it is a dynamic reliability assessment curve. this curve can better diagnose the internal fault of the transformer, compared with the traditional reliability assessment methods, thus ensuring that the value of the evaluation can well track and reflect the actual operation state of the transformer."
"the feret database contains 2200 face images collected from 200 subjects, and each subject has 11 images. these images were captured under various poses and expressions. in this experiment, we randomly select the images from 50 individuals. fig. 6(b) shows the images of one individual we randomly select five images of each individual as training samples, and the remaining samples are regarded as test samples. table iv lists the average recognition rates of all compared methods over 20 runs. it reflects that our approach can improve the recognition results over the existing methods. interestingly, the pca can outperform some supervised subspace methods on this database. a likely reason for this is that large pose changes of one individual produce large intraclass variations, which highly influence the performance of supervised methods."
"3) after the above dynamic corrections were introduced, the corrected failure rate curve was transformed into a dynamic ladder form, which would ensure that the failure rate and life prediction curves of the transformer are more consistent with the actual state of the transformer, thus the overall reliability assessment model can be properly adjusted according to the operating conditions of the evaluated objective, and a higher accuracy can be achieved. 4) at last, a practical case study in china southern power grid has been carried out. in this case, the actual data of a main transformer with 110 kv from the jiangmen power-supply bureau were analysed to verify the validity of the built model. here, according to the static ageing failure model, the failure rate of this transformer is only 0.01%, showed extremely low, and the corresponding life expectancy is 64.12 year, after it was put into operation for 12 years, this is mainly due to the load of the transformer is controlled at about 40%. after introducing dynamic corrections, the corrected life expectancy is 59.46 year via the dga data analysis, as well as its corresponding corrected failure rate curve can be obtained. owing to the good dga data, the corrected failure rate is only 0.014%. this value is very low, showing that this transformer is in a good operating state and its detection data are very normal, which has been verified via verifying to the operation and maintenance personnel. this also verifies the correctness of the evaluation method proposed in this paper."
"it can be seen from table 3 that the dga data are continuous and no maintenances are carried out on the winding insulation system in the transformer, e.g., the disintegration maintenances, and the oil separations. as there is an accumulation in the dga data, it's necessary to correct the reliability model (the base model) using the latest dga data."
"as discussed previously, dga contains a large amount of data, which can be easily obtained by offline or online monitoring approach, thus it becomes an effective characteristic quantity to study the internal characteristics of the transformer. based on dga, the topis is selected as the evaluation objective, and in this section, a practical case study is carried out with the ultimate transformer reliability assessment model developed in previous section. in this case study, the dynamic corrections are performed based on the actual dga data from the jiangmen power supply bureau in china southern power grid. here, a conclusive failure rate curve with dynamic corrections is obtained, which can effectively track the actual operation status of the transformer, and accurately reflect its actual reliability level and the ageing process. the case study is discussed as follows."
"the total average results are also summarized in table i . we can see that our approach and latlrr have lower deviations than other methods, which demonstrates good scalability. when the images are corrupted, all traditional subspace methods have difficulty obtaining reasonable results. however, three low-rank modeling-based methods achieve remarkable performance. in most cases, our srrs approach achieves the best recognition results. moreover, we utilize other levels of corruption, such as 20%, 30%, 40%, and 50% on coil-20 database, and report the results in fig. 4 . it shows that our srrs approach consistently outperforms other methods."
"here, the hydrogen (h2), methane (ch4), ethane (c2h6), ethylene (c2h4) and acetylene (c2h2) in dga are selected as condition assessment indices of the oil-paper insulation of the transformer. the relationship between these gases and internal failures are demonstrated in table 1 . note that these gases are chosen as characteristic gases because the power supply enterprises do not have the condition of testing the dp at the present stage, and besides, the tests on the content of furfural (a kind of a chemical) are rarely conducted, such that it is very difficult to obtain the characteristic parameters. moreover, the operation of transformer oil filtration and maintenance has a great influence on the content of furfural, which affects the accuracy of the model and thus causes the model to be difficult to popularize. in contrast, dga data is large and easy to obtain, i.e., it can be obtained via both off-line mode and on-line monitoring mode, such that the dga data for condition assessment has unique advantages."
"where z * is the nuclear norm of a matrix (i.e., the sum of singular values of the matrix) [cit] . we also notice that the second termf (p, z ) in (4) is not convex to z because of the term −tr(s b ), so we add an elastic term to ensure the convexitŷ"
"where is the set that records all successful crossover rates at generation . the initial value of is set as 0.5; then, it is updated by the following equation at the end of each generation:"
"2) in the traditional method, the characteristic parameters such as dp and vff are used to reflect the reliability of transformer oil-paper insulation, which leads to some defects for the furfural, such as content is easily affected by external factors, difficult to be obtained, a long testing cycle, the model accuracy is not high. to avoid this, the dga data, as the primary characteristic parameters, were employed in the grey target theory based model for dynamic corrections, such that the corresponding relation between the fault degree of topis and the life expectancy of transformer was dynamically adjusted, the corrected life expectancy and life loss was obtained, and then the equivalent hst was achieved, and finally the dynamic correction of the static model can be realized. besides, the age reduction factor was introduced, thus the model after the maintenance of topis was considered so as to realize a more reliable diagnosis on transformer internal faults."
the structural description of the mlp and epnn is shown in table 6 . both models have nearly equal parameter numbers for the two datasets.
"a dendrite layer denotes a typical nonlinear interaction of the synaptic signals on each branch of dendrites. the multiplication operation plays a vital role in the process of transporting and disposing the neural information [cit] . thus, the nonlinearity calculation of the synaptic layer can be implemented by a typical multiplication operation instead of summation. the interaction of a dendritic branch is equivalent to a logic and operation. the operation of the input variables will generate a 1 when and only when all input variables equal 1 simultaneously. the corresponding equation of the dendrite layer is defined as follows:"
"as stated in the inexact alm algorithm, we also need to update the lagrange multipliers y and r, and the parameter μ after optimizing the variables p, j, z, and e."
"in order to calculate the winding hst and reflect the actual load-bearing ability of the power transformer, as well as obtain the load rate curve, the following aspects are considered as follows."
"meanwhile, in the pnn and epnn, the synaptic input number is, and the dendritic branch number is ℎ. the dimension number of the pnn and epnn can be calculated in the following equation:"
"the most relevant method in the literature is low-rank transfer subspace learning (ltsl) [cit], which incorporates low-rank constraint in subspace learning. however, there are significant differences between ltsl and our approach. first, the ltsl is a transfer learning method that seeks a common subspace for two domains, whereas our approach lies in supervised learning. second, the ltsl employs low-rank constraint in low-dimensional subspace in order to transfer knowledge across two domains. in our approach, the low-rank constraint is enforced in the high-dimensional feature space in order to preserve more information."
"in work [cit], the life distributions of 154 transformers were analyzed statistically. the empirical values of the arrhenius equation using gaussian distribution can be determined. the simulation is performed in matlab and the relationship between winding hst and life expectancy is presented in figure 8, from which the life expectancy can reach 64.12 (year) when the winding hst is 43.42 ℃. substitute the obtained temperature and related parameters into (1), the failure rate can be calculated as"
"besides, in the analysis of transformer reliability, the research status single still in the initial stage, the method of which is single and its theory is under development but not yet perfected. for examples, [cit] presented a minimum cut set solution of the fault tree for the failure rate analysis of transformer; r.j. [cit] established a reliability assessment model for power transformers via markov process. however, most of the current methods are based on statistical laws, and employed rarely considering the individual difference of the transformer which is a complex aggregation composed of multiple systems."
"2.4. soma layer. the output of the membrane layer is transmitted to the soma layer. once the membrane potential exceeds the threshold, the neuron fires. a sigmoid operation is used to describe the function of the soma layer:"
"where l is the expected life of insulation material, here it represents the expected life of the winding insulation system; coefficients b and c are related to the insulation material type and activation energy from the resistance-to-high temperature tests; and t is the thermal temperature of winding, here it refers to the winding hst, which has been obtained in section 2.3."
"while, after the maintenance, the internal insulation fault of the transformer has been handled in time, thus the operating conditions of the topis have been improved and the loss of life expectancy will be reduced. this can be verified from figure 12, which shows the failure rate of the transformer when considering the effect of oil filtering. from this figure, the actual lifetime correction value z″ and the corresponding hst heq are obtained as 43.57 years and 71.50 ℃, respectively. hence, it can be concluded from this figure that when the effect of oil filtering is considered, the failure rate of the transformer in the sixth year is 0.005%, much lower than the 0.1% before the correction, that is, the failure rate is reduced by 95% at this point. the same method can be used to obtain the fault correction value after the transformer is overhauled, which involves the oil-paper insulation system."
"3) sunglasses + scarf: moreover, we consider the case where images contain both sunglasses and scarf. we select all the seven neutral images and two corrupted images (one with sunglasses and the other with scarf) at session 1 for training. for each individual, there are 17 test images in total. table v shows the recognition rates of compared methods in three different scenarios. we can observe that lrdl obtains the best result in the sunglasses case, and our approach obtains the best results in the scarf case and the mixed case. fig. 9(d) shows that our approach can correctly recover the clean images from the occluded face images. therefore, we can train robust classifiers from the recovered image set az."
"when the transformer is in operation, the maintenance of topis is carried out based on its operational condition, which is an interruption for status continuity of the transformer. after that, the life expectancy of the transformer can be extended, with a reduction of the equivalent hst."
"1) according to the ieee std c57.91 [cit] and iec std 60076-7 equipment practice, and combining with the weibull distribution and arrhenius reaction law, an hst-based ageing failure model as a static model was developed to describe the ageing process of transformer, in which the winding hst and transformer failure rate can be obtained, as well as the life expectancy of transformer can be further calculated. compared to the traditional method of reliability assessment of transformer based on statistical theory, this static model has been developed which takes the individual differences of transformers into consideration, such that the failure rate level of the topis can be better reflected and the result is more reliable."
"next, to make a fair comparison, the performances of the epnn, mlp, and pnn should be compared with an approximately equal number of thresholds and weights. in addition, the learning rate of the pnn and mlp trained by using the back-error propagation algorithm is set to 0.01. the number of dendrites in the pnn should be defined in this simulation, as should the number of hidden layers and neurons of the mlp. the mlp's parameter number depends mainly on the number of neurons in the hidden layer. thus, we denote mlp as a -dimensional vector, which consists of weights and biases. the dimension number can be calculated as follows:"
"large oil-immersed power transformers are crucial links between the generators of a power system and the transmission lines and between lines of different voltage levels [cit] . in general, oil-immersed power transformers can significantly influence the operation safety and maintenance reliability of the power system [cit] . a transformer failure (e.g., oil-transformer ageing [cit] ) may result in a catastrophic and irreversible internal damage to the whole power system [cit] . thus, failure rate analysis of large oil-immersed power transformers is a very important aspect of reliability of power system and of great significance to a secure and efficient power transmission in power industry."
"as discussed above, the low-rank modeling has shown impressive performance in various applications [cit] . however, only a few of those methods can take advantages of class label information during low-rank learning, which is a key for classification purpose. on the other hand, although the conventional subspace learning approaches usually obtain good performance for classification tasks, they have strong assumptions on the data distribution, and therefore, they are sensitive to the noisy data. the learned subspace has limited discriminability. can we leverage the advantages of both supervised subspace learning and low-rank modeling for classification?"
"4.6.1. the ultimate synaptic and dendritic morphology. as mentioned above, the epnn can implement synaptic pruning and dendritic pruning during the training process. thus, superfluous synapses and dendrites can be removed, and then, a simplified and distinct topological morphology is formed for each problem. figure 8 shows the particular dendritic structure of the epnn on the qualitative bankruptcy dataset after learning. the unnecessary dendrites (branch 2, branch 4 and branch 9) of the pnn are presented in figure 9, and superfluous synaptic layers are provided in figure 10 . finally, the simplified structural morphology is described in figure 11 . it can be observed that 7 dendritic branches and 4 features are reserved in the structure. this means that the features 1 and 2 are not crucial for the epnn and have no contribution to solving the qualitative bankruptcy dataset problem. in addition, figure 12 illustrates the unique dendritic morphology of the epnn on the distress dataset after learning. figure 13 shows that all ineffective dendrites are removed, and figure 14 rules out all ineffective synaptic layers. thus, the final structural morphology is presented in figure 15 . only four branches of the dendrites are remaining, and the feature 6 is removed. as summarized in table 15, it can be observed that synaptic pruning and dendritic pruning mechanism can largely simplify the structure of the epnn. thus, it is able to speed up the bankruptcy prediction analysis by the simplified epnn obviously."
"where and are the desired output and the actual output, respectively; represents the number of instances applied for training; and denotes the number of simulation runs."
"the above static model or the base model can only be used to simulate the failure rate of topis when a transformer is in the ideal ageing process. however, some unpredictable factors, e.g., the partial discharge, high-temperature, and overheat, may occur in the actual operation of the transformer. as a result, an accurate system modelling is required to take these uncertainties into account, such that a more satisfactory prediction can be ensured."
"kinship verification is a recently investigated research topic, which aims at determining kin relationships from photos. it is still a very challenging task due to large variations in different human faces. we also evaluate the performance of our approach and related methods on kinship verification. we conduct the kinship verification experiments on the ub kinface database version 2 [cit] . this database contains 600 face images that can be separated into 200 groups, and each group consists of children, young parents, and old parents. fig. 6(c) shows example images in the kinface database, in which three columns (from left to right) represent the images of children, young parents, and old parents, respectively. given two images of faces, our task is to determine whether they are an accurate child-parent pair."
"in practice, the mlem chooses proper parameters of the weibull distribution model to generate a maximum probability of occurrence of the samples, which values are used as point estimations of unknown parameters."
"the experimental results show that, compared with the traditional subspace learning methods, our approach is robust to noise and large variations. the reason is that the low-rank property helps us obtain a better estimate of the underlying distribution of samples from the recovered images, and then, our approach learns a robust and discriminative subspace. the resulting performance is better than the compared low-rank modeling and dictionary learning methods. fig. 8 shows why our approach performs so well by visualizing lrr coefficients of latlrr and our approach. in particular, we show the coefficients for representing two pairs of samples. one pair, samples a and b, is selected from the same class, while the other pair, samples a and c, is selected from different classes. fig. 8 (a) and (b) shows that, in latlrr, samples from the same class contribute more in the representation, as the coefficients within the same class are a little larger than those in other classes. in some sense, latlrr could discover the subspace membership of samples. compared with latlrr, fig. 8 (c) and (d) shows that the coefficients of the same class are higher than others, which implies our approach can clearly reveal the subspace structure. since we incorporate supervised regularization in our model, the lrrs as well as the resulting subspace learnt by our approach should be more discriminative than that of the latlrr. furthermore, fig. 9 shows the corrupted images, the recovered images, and the noisy part on five object and face databases. it shows that, although training images (i.e., x) have large pose variations and corruptions, the recovered images az are very similar to each other, which helps us learn a robust subspace for classification."
where diag(k ) is to construct a block diagonal matrix with each block on the diagonal being matrix k . the convexity of f (t) depends on whether its hessian matrix ∇ 2 f (t) is positive definite or not. ∇ 2 f (t) will be positive definite if matrix s is positive definite
"since there are many proposed methods which are adopted to classify qualitative bankruptcy dataset in the relative literatures, we summarized the classification performances and compared them with that of the epnn. specifically, table 11 presents some single classification methods and table 12 demonstrates some hybrid classification methods, respectively. from table 11, it can be observed that the accuracy rate of the epnn is only slightly less than rbfbased svm, ant-miner, and random forest. as table 12 shows, compared with other hybrid classification methods, the average accuracy rate of the epnn is only slightly lower than the hybrid logistic regression-naive bayes. thus, it can be concluded that although the epnn adopted 50%-50% trainto-test ratio, it has still presented a competitive performance on the qualitative bankruptcy dataset. and it is worth mentioning that hybrid classification methods are not always superior to single classification methods based on the above experimental results."
"where z is the original life expectancy solved by the static model (i.e., the base model). the life loss δz′ after correction is defined as"
"in this process, first, it can be calculated that the life expectancy of the transformer before overhaul has been reduced to 23.016 years, as shown in figure 11, from this failure rate curve without considering the effect of oil filtering, the equivalent hst can be obtained as 130.66℃. hence, it can be analyzed from this curve that if the effect of maintenance is not taken into account, the failure rate in the sixth year has reduced 0.1%, which shows that the internal insulation situation has begun to deteriorate and the failure rate is rising rapidly."
"in order to achieve the goal mentioned above, an enormous variety of approaches have been proposed to accurately evaluate the health status of transformer life, from transformer condition assessment and maintenance, transformer reliability analysis, to fault diagnosis. in the past years, on one hand, plenty of elegant methods have been put forwarded for condition assessment of transformer, such as evidential reasoning [cit], association rule and variable weight coefficients [cit], genetic-based neural networks (gnn), statistical learning technique [cit], fuzzy logic [cit], sweep frequency response analysis (sfra) [cit], and modern machine learning techniques [cit] . these methods can help predictive maintenance programmes to offer a low cost and highly flexible solution for fault prediction [cit] . however, there are still some defects in these methods; for instance, the mentioned fuzzy logic method has some limitations, such as the sample data are required to possess completeness by the fuzzy rule table, the fuzzy membership function is difficult to be determined accurately, thus it has an indirect effect on the comprehensiveness of the diagnosis. this is mainly due to the currently indeterminate relationship between the transformer fault phenomenon, fault cause, fault mechanism and fault classification. for another example, for the mentioned gnn method, the assessment performance of which is excessively dependent on the completeness of training samples. in addition, it has a single utilization and representation form for the knowledge, thus the effect of fault identification is easily fluctuated. here, s. [cit] presented the status and current trends of different diagnostic techniques of power transformers, including the dga, partial discharge (pd), international electrotechnical commission (iec), ultra-high frequency (uhf), frequency response analysis (fra), polarization and depolarization currents (pdc), and frequency domain spectroscopy (fds). among them, the pdc measurements, as a diagnosis tool, is difficult to be employed to obtain the ageing condition of transformer cellulose insulation due to the variation in transformer insulation geometry [cit] ."
"the supervised regularization termf (p, z ) is convex with respect to z . we will provide the theoretical analysis to prove it in section iii-b."
"lemma 1 tells us the smallest eigenvalue of matrix (b + c) is greater than or equal to the sum of the smallest eigenvalues of b and c. in our problem, we need to make s positive definite, which means the smallest eigenvalue of s should be greater than 0. thus, we employ lemma 1 to evaluate the (12) . the minimal eigenvalues of −h b and h t are −1 and 0, so we should ensure"
"after obtaining the optimal solution p * and z *, we project both training samples and test samples onto p *, and then utilize nn classifier to predict the label vector of test samples. the complete procedures of our srrs approach are summarized in algorithm 2."
"where tp, tn, fp, and fn represent true positive, true negative, false positive, and false negative, respectively. true positive (tp) means that the company is detected as healthy, and the teacher target label is healthy as well. true negative (tn) represents that the input and the teacher target label are detected as unhealthy simultaneously. false positive (fp) shows that the input is detected as healthy, whereas the teacher target label is unhealthy. false negative (fn) denotes that input is detected unhealthy, and the teacher target label shows the opposite,"
"where m i is the mean sample of the i th class inx, m is the overall mean sample ofx, andx i j is the j th sample in the i th class ofx. by using fisher criterion, the projected samples from different classes should be far apart, while projected samples from the same class should be close to each other. furthermore, [cit] pointed out that this trace-ratio problem can be converted into a trace difference problem."
"the failure rate curve of topis can be dynamically corrected based on actual dga data. the dga testing data of the main transformer (#2) in substation are shown in table 3, which are obtained from the maintenance department of jiangmen power supply bureau."
"yun fu (s'07-m'08-sm'11) received the b.eng. degree in information engineering and the m.eng. degree in pattern recognition and intelligence systems from xi'an jiaotong university, xi'an, china, and the m.s. degree in statistics and the ph.d. degree in electrical and computer engineering from the university of illinois at urbana-champaign, champaign, il, usa."
"to evaluate the robustness to noise of the different methods, we randomly choose a percentage (from 10% to 50%) pixels and replace their values by random numbers that are uniformly distributed on [cit] . fig. 7 shows that, in noisy scenarios, low-rank modeling-based methods (latlrr, dlrd, and our approach) consistently obtain better performance than other methods. in particular, our srrs approach can get the best performance."
"dendritic pruning: when the synaptic layer that receives the input signal is in the constant-0 connection case, the output is always 0. consequently, the output of the adjacent dendrite layer becomes 0 because the result of any value multiplying 0 equals 0. this implies that this entire dendrite layer should be omitted because it has minimal influence on the result of the soma layer."
"6) this paper also provides a scientific guidance for the decision-making of the planning, replacement, maintenance and technical transformation of the primary equipment in the power grid, as well as the routine maintenance operation of equipment and fault treatment analysis, and improves the efficiency of field work. in addition, the research results have strong applicability, which effectively reduces the difficulty of obtaining the characteristic parameters and improves the accuracy of the model, thus it is more conducive to the popularization and application of the model. electric arc in oil and oil-paper h2, c2h2, co, co2 ch4, c2h4, c2h6"
"in general, most performance evaluation metrics attempt to estimate how well the learned model predicts the correct class of new input samples; however, different metrics yield different orderings of model performances [cit] . the classification accuracy has been by far the most frequently adopted indicator of performance in the literature [cit] . sensitivity and specificity can be highlighted as straightforward indices. the auc does not implicitly assume equal misclassification costs, and it corresponds to the most preferred score calculated as the empirical probability that a randomly chosen positive observation ranks above a randomly chosen negative sample [cit] . hence, the overall accuracy rate, sensitivity, specificity, and auc are used to construct the performance evaluation system. table 7 demonstrates that the result of a classifier can be measured by a 2-dimensional contingency matrix. the accuracy rate is the critical indicator in evaluating the classification algorithms; another indicator of accuracy analysis is related to the possibility of misclassifying bankruptcies. the classification accuracy rate is measured by the following equation:"
"first, due to the fact that n samples belong to c different classes and n c, these samples should be drawn from c different subspaces, and therefore, the coefficient matrix z is expected to be low rank. in other words, the coefficient vectors corresponding to samples from the same class should be highly correlated."
"actually, pnn suffers from the curse of dimensionality. when the dimension increases largely, any small change of the weights on one dendritic branch will produce a great disparity of its final results because of the multiplication operation. this is the main limitation of epnn. thus it needs us to propose more powerful optimization algorithms to figure it out. conventional classifiers use bp to adjust the weights and threshold. however, bp suffers an inherent local minimum trapping problem and has difficulties in achieving the globally best values of its weights and thresholds. this disadvantage of bp has largely limited the computational capabilities of our neural mode. to improve the performance of the epnn, we adopt jade to train the model. jade has been regarded as one of a few \"important variants of differential evolution (de)\" [cit] . the vast popularity of de algorithms has led to an increasing interest in developing their variants [cit] . it is well known that the performances of many metaheuristic methods are influenced by the choice of their control parameters [cit] . jade can use a selfadaptive mechanism to select suitable parameters and for different optimization problems and implement a \"de/current-to-best\" mutation strategy with an optional external archive. the experimental results verified that jade obtains a better balance between exploration and exploitation during the evolutionary process and is superior to other optimization algorithms in terms of solution quality and convergence rate [cit] . jade follows the general procedure of an evolutionary algorithm. after initialization, de executes a loop of evolutionary operations: mutation, crossover, and selection. in addition, jade dynamically updates control parameters as the evolutionary search proceeds."
"the performance of our srrs approach is evaluated on six benchmark data sets, including object data sets [cit], face data sets [cit], and kinface data set [cit] . we compare our approach with related methods on the robustness to different types of noise, including pixel corruption and large pose/illumination variations. our code is publicly available. 1"
"low-rank modeling has attracted a lot of attention recently, which can recover the underlying structure of data [cit] . it is an extension of sr. when data are drawn from a fig. 1 . framework of the proposed approach. we jointly remove noise from data x and learn robust subspace p. the corrupted samples are mixed in the original space, but they are well separated in the learned subspace."
"the weibull distribution model, as a kind of monotonous failure rate distribution model, is adopted in this paper to investigate the failure rate of topis after the winding hst is obtained due to the merit of a closer representation of stochastic events, such as the lifetime and reliability of the product, as well as multiple deformation modes of failure rate [cit], and it is defined by"
"parameter adaptation: better controlling the parameter values can result in individuals that have greater possibility to survive, and hence, these values should be retained in the next generation. at each generation, the crossover rate is formed independently according to a normal distribution of mean and standard deviation 0.1 and then normalized to the range [cit], which can be described as follows:"
"there are no missing values in the qualitative bankruptcy dataset, but all the attributes are nominal. this dataset includes 250 samples, and each sample possesses 6 features. the 6 features are all represented by 3 labels: \"p\", \"a\", and \"n\". we convert the qualitative features into the values 1, 2, and 3, respectively."
"2) learn low-rank representations z on fixed subspace: here, we show how to update j k+1, z k+1, and e k+1 when fixing p k+1 . after dropping the irrelevant terms with respect to j, (15) can be rewritten as"
"2) scarf: we utilize the corrupted training images due to the occlusion of scarf. using a similar training/test setting as above, we have 8 training images and 12 test images for each individual. the scarf covers about 40% of the face image."
"selection: the selection operation compares the parent vector, with the trial vector, according to their fitness values (⋅), and it chooses the better vector for the next generation. for example, if given a minimization problem, the selected vector is generated by the following equation:"
"in this process, combined with dga data, the life expectancy of the transformer is dynamically corrected via the grey target theory based dynamic correction model, in which the equivalent hst is obtained, thus the effects of local overheating or arc overheating on the insulation are quantized by the equivalent hst. here, the corrected expected lifetime z′ is calculated as 59.46 years, under which the equivalent hst heq is 48.53 ℃, thus the corrected failure rate of the transformer can be obtained, as shown in figure 10, where the three lines of deep blue, red and light blue represent the failure rate curves of the transformer before, during and after correction, respectively. due to the good dga data of the transformer, it can be seen from figure 10 that the failure rate of the transformer after correction is only 0.014%. this value is also very low, which is 0.004% higher than the calculated value 0.01% before the correction. the value of the two is close, which shows that the operation state of the transformer is also good after the correction. this is also can be verified by analysing various pre-test data of the transformer and inquiring about the operation and maintenance personnel, who give the results as follows: this transformer is in good operating state and its various testing data are normal, thus verifying the effectiveness of the dynamic correction method proposed in this calculation stage."
"weibull distribution based failure rate function li the correction coefficients t time in year θhd the average hst for a whole day, ℃ f(t) probability density function θhy the yearly average hst, ℃ r(t) degree of reliability z′ the lifetime of topis after corrections, year f(t) failure distribution function heq the equivalent hst, ℃ r ratio of load loss to no-load loss at a rated load τto,r time constant of transformer oil at rated load, h s complex frequency"
"after discretization of the winding hst rise in each hour, as shown in figure 7, the average hst for a whole day is calculated as"
"in our previous research, a pnn model, in which the particular locations and types of synapses on the dendrite branches are formulated via learning, is proposed, and useless and superfluous synaptic and dendritic connections are eliminated. thus, the efficiency of the model is enhanced [cit] . similar to most other anns, pnn adopts the backpropagation (bp) algorithm as its learning method. however, learning algorithms are widely considered to have significant influences on the performances of anns [cit] ."
"nomenclature sets γ(ω0, ωi) the degree of each mode close to the standard state mode f failure data set c truncated data set z″ the life expectancy of the transformer after maintenance, year x data sequence ω″ recognition sequence z′eq the corrected life loss before the last maintenance, year"
"in this paper, the grey target theory in the grey-system theory has been used, which contains the grey-assessment and grey decision-making theory. the gist of grey target theory is at first to set a grey target under the condition of no standard modes, in which find the bull's-eye via the grey-system theory; and then, compare the model of each index with the standard model; and finally, determine the assessment grade via the grad division. concretely, the grey target theory used in this paper is briefly introduced as follows."
"where is a positive constant in the interval [cit] and ( ) represents the arithmetic mean of the agents in . similarly, the mutation factor is also independently generated according to a cauchy distribution with location"
"where the sensitivity and specificity are called the true positive rate and true negative rate, respectively. sensitivity measures the percentage of real positives that are identified correctly. this metric shows how successfully a classifier can identify regular records, which means that the companies are healthy in terms of bankruptcy analysis. therefore, financial institutions can achieve correct and efficient analysis by adopting a classifier with a higher sensitivity. specificity represents how successfully a classifier can distinguish abnormal records; i.e., it is the proportion of true negatives. hence, a higher specificity can help financial institutions reduce the possibility of misclassifying healthy companies. auc represents the ratio of companies that are not in danger of bankruptcy. in other words, a score of 100% indicates that two classes can be correctly discriminated by the classifier, whereas a score of 50% indicates that the classifier has an insignificant ability to classify companies correctly. in addition to comparing different classification algorithms, the convergence performances of the two models, epnn and mlp, are compared. when the mean squared error (mse) achieves a predetermined minimum value, the learning tends to be completed. the training error is calculated as shown in (21),"
"we derive the solution to the projection vectors in p k one-by-one. to obtain the i th column in p k [denoted as p k(:,i) ], we rewrite (16) as"
"the novel contribution of this paper can be summarized as follows: a new model is developed for the reliability assessment of the large oil-immersed power transformers, in order to achieve accurate evaluation of the reliability level of the operation state of topis, which is of great significance to guard against a premature insulation fault. in this model, a novel concept of dynamic correction is introduced, which can provide a new way for the reliability assessment of the power transformer. the validity of the model has been verified via the practical case study. addressed concretely, the built model contains an hst-based ageing failure model, called the base/static model, as well as a grey target theory based correction model, called the dynamic/correction model. in the process of building the whole model, the statistical techniques based traditional reliability assessment mode that only takes into consideration the macro level and neglects the micro level is not used, which enables the evaluation results to give a good consideration to the individual differences of the power transformer, thus the result is more reliable. besides, the model is built with a structure of the base model plus the dynamic correction model, such that the introduction of dynamic correction makes the entire reliability evaluation model can be adjusted according to operation state of the evaluation object, thus the credibility is higher. moreover, the model built in this paper selects the dga data as a source of information for dynamic correction instead of the characteristic parameters reflecting the reliability of transformer oil-paper insulation, such as dp [cit] and vff. this is because the operations of the power transformer, e.g., oil filtering and maintenance, have a great impact on furfural's concentration. instead, the dga data contains a large amount and can be obtained offline or online, thus it has a unique advantage, reduces the difficulty of obtaining the characteristic parameters, improves the accuracy of the built model, and is more conducive to the promotion and use of the model. in addition, the grey target theory is employed to process these dga data, which can dynamically correct the base model so as to ensure the evaluation better tracking the actual reliability level of transformer and accurately reflect its ageing process. this has been verified in this paper via the analysis of the actual data of jiangmen power-supply bureau in china southern power grid and the results of the practical case study show that the built model can well track the operational status of transformer. on the whole, the hst and grey target theory based dynamic correction dga model built in this paper can solve some issues of the traditional reliability analysis model caused by the difficulty of obtaining the characteristic parameters (e.g., dp and vff), such as lower accuracy and difficult popularization and application, provide a good guidance for the operation and maintenance personnel of the power-supply bureau for the daily maintenance of primary equipment and the fault treatment analysis, as well as suggestions for the adjustment of the maintenance cycle of various equipment, thus the research results have strong applicability and it can improve the efficiency of field work, save human resources and working hours, ensure the safety operation of the power grid when a large number of new energy resources integrated, and finally provide scientific guidance for the planning, replacement, maintenance and technical transformation of the primary equipment of the power grid."
"after the three stages of calculation above, the final corrected failure rate of the topis can be obtained ( figure 14) . that is, this curve is obtained by adding the dynamic corrections above on the base failure rate curve (figure 13 ) of the transformer (i.e., the original failure rate curve obtained from the base model)."
"artificial intelligence algorithms, such as neural network methods, are being widely applied in bankruptcy analysis. in this paper, we introduce a more realistic neural model called the epnn to facilitate bankruptcy analysis. this technique adopts the jade algorithm to train the model to obtain satisfactory classification performances. in contrast with the pnn and mlp, the proposed epnn performs the best in terms of the average accuracy and auc on both benchmark and application-oriented datasets, namely, the qualitative bankruptcy dataset and the distress dataset. in addition, compared with other classification methods such as knn, rbf, rf, dt, svm, and da, the epnn also provides competitive and satisfactory classification performances. note that the neuronal pruning mechanism is an important aspect of the epnn. after synaptic pruning and dendritic pruning, the number of input features in both datasets is reduced, and the structure of the neural network is simplified. moreover, the simplified structural morphology can form a logic circuit which can also be employed as a powerful tool to solve bankruptcy prediction problems. thus, the contribution of this paper can be summarized from three aspects: first, we provide a comprehensive study by comparing different classification models in terms of bankruptcy prediction problems. although many novel algorithms are continually emerging, a large proportion of approaches still only focus on the bankruptcy prediction model's ability to improve the classification accuracy. compared with some other models, the epnn possesses a certain advantage with respect to average accuracy and auc. second, the epnn can implement synaptic and dendritic pruning to realize pattern extraction and reconstruct a more compact neuronal morphology. the epnn has a large initial neuronal topology, which makes it not very sensitive to its initial conditions, but it can utilize neuronal pruning after learning, which increases the efficiency of the neural network, speeds up the convergence, avoids becoming trapped in local minima, and reduces the operation time and computational cost. third, the simplified models can be replaced by logic circuits, which can increase the classification accuracy and be easily implemented on the hardware. therefore, these findings provide details and offer insight into technical development for understanding and tracing the operating mechanisms and construction of single neurons. in addition, the results also imply that the proposed epnn classifier possesses an excellent potential to be applied in other binary classification problems. the epnn makes it possible to draw standard profiles of the failing companies and provide a theoretical contribution to the phenomenon of bankruptcy. it is believed that the epnn will be suitable for not only bankruptcy prediction but also other fields of application within the scope of financial analysis such as performance analysis."
"in addition, because both mlp and pnn are adopted as the competitors in our experiment, several other parameters for these algorithms are considered cautiously. table 5 lists the relevant parameters."
"furthermore, the set contains all the successful mutation factors in generation . the initial value of of the cauchy distribution is set to 0.5, and then, they are updated at the end of each generation by the following equation:"
"the remainder of this paper is constructed as follows. section 2 presents an overview of the related theories in bankruptcy analysis. section 3 introduces the proposed epnn model in detail. moreover, the epnn's learning algorithm jade is described. section 4 presents the experimental results obtained using the epnn and makes a comparison with other algorithms by adopting the qualitative bankruptcy dataset and distress dataset. section 5 concludes this paper."
"in order to dynamically correct the life prediction curves of the topis obtained in traditional methods, a novel concept of dynamic correction is introduced to the reliability assessment of the large oil-immersed power transformers for the first time, based on which, with the topis as the target of evaluation and hst as the core point in this paper, a modelling framework with a combination of the hst-based static ageing failure model and grey target theory based dynamic correction model was developed and comprehensively compared, which provides a new way for the internal diagnosis of topis and accurately tracking the operation status of power transformer, and has been verified to have strong effectiveness and practicality when it is compared to the traditional transformer reliability assessment method in which the characteristic parameters such as dp and vff. the main contributions can be summarized as follows:"
"comparatively, the dga contains a large amount of data and is easier to be obtained thus it is more appropriate to be adopted as the characteristic parameter for reliability assessment of transformer. additionally, it can be calculated either offline or online via various monitoring approaches, these merits lead to a more proper reliability assessment and internal performance analysis of transformer. hence, a dynamic modelling method for the reliability assessment of the topis is proposed in this paper, in which the topis is taken as the evaluation objective, and the hot spot temperature (hst) of transformer winding is treated as the core point. the model in this paper is developed based on the weibull distribution, arrhenius reaction law and grey target theory, including a static ageing failure model and a dynamic correction model. the former is employed to calculate the winding hst and the failure rate of the topis, in order to obtain the life expectancy of the transformer. the latter is used to dynamically correct the static ageing failure model, in which the corrected life expectancy and life loss can be calculated, in order to build the relation between status grade and life expectancy of topis. besides, the influence of the transformer after overhaul is considered, in order to correct the calculated life expectancy, thus the corresponding equivalent hst can be obtained and the original static model can be corrected."
"he is currently an interdisciplinary faculty member with the college of engineering and the college of computer and information science, northeastern university, boston, ma, usa. his current research interests include interdisciplinary research in machine learning and computational intelligence, social media analytics, human-computer interaction, and cyber-physical systems."
"in addition to avoiding misleading and contradictory conclusions, four key components are carefully defined to allow one to draw well-founded conclusions from the experimental results. first, the research has adopted both benchmark and application-oriented databases, namely, a qualitative bankruptcy dataset from the uci machine learning database repository and a distress dataset from the kaggle dataset. second, in the simulation, the two datasets are separated into a training set and a testing set at proportions of 50% each. third, the average accuracy, sensitivity, specificity, convergence speed, and auc are used as the evaluation metric framework; such metrics can be used to effectively and efficiently analyse the possibility of bankruptcy. fourth, a nonparametric test called the wilcoxon rank-sum test has been adopted to allow us to claim that the observed result differences in performance are statistically significant and not simply caused by random splitting effects. to conclude, our main contributions are clarified as follows: first, a novel epnn model is proposed in this paper which can adopt synaptic and dendritic pruning to simplify its neuron morphology during the training process. second, the simplified model of epnn can be completely replaced by logic circuits which be easily implemented on hardware. the logic circuits can maintain high classification accuracy and obtain extremely high computation speed, simultaneously. last but not least, comprehensive comparison experiments have been implemented to demonstrate that the epnn outperforms the mlp, pnn, and other commonly used classifiers on the bankruptcy prediction problems."
where lk is the equivalent constant load of each time period. nc is the number of equivalent time periods within a cycle period. tk is the length of the set time period.
"we also performed a significance test, mcnemar's test, for the results shown in table i, in order to demonstrate the statistical significance of our approach compared with several of the most representative state-of-the-art methods. we use a significance level of 0.05. in another word, the performance difference between two methods is statistically significant, if the estimated p-value is lower than 0.05. table ii shows the table ii p-value between srrs and other methods on the coil object database. the asterisk * indicates that the difference between method a p-values of comparing srrs with other methods. from this table, the following conclusions can be reached."
"various quantitative statistical approaches have been adopted to improve bankruptcy forecasting models. discriminant analysis is adopted to classify observations between good and bad payers [cit], and logistic regression is adapted to determine the default probability of the borrowers [cit] . however, it is argued that these popular models are inaccurate [cit] . hence, several machine learning tools are explored to assess bankruptcy risk using computer technology. because bankruptcy risk analysis is similar to pattern recognition tasks, most methods can be adapted to classify the creditworthiness of potential clients of financial institutions [cit] . among them, anns achieve outstanding performances in applications such as predicting financial crises [cit], scoring credit [cit], and building up credit analysis models [cit] . the 2 [cit] s [cit] . prior studies revealed that anns are powerful for use in pattern recognition because of their nonlinear and nonparametric adaptive-learning properties [cit] . this imbues anns with obvious advantages over conventional statistical algorithms and inductive learning methods, especially in comparison with discriminant analysis and logistic regression [cit] . hence, researchers have put a major emphasis on the application of anns in finance and accounting."
"generally, data preprocessing is an initial and basic step of data analysis. because artificial neural networks require that every data sample be expressed as a real number vector, we need to change the nominal attributes of the data samples into numerical values before inputting them into the classifier [cit] ."
"the average yearly temperature is selected as the temperature reference, which is the average value of 57 [cit] . based on these monthly average temperatures, the average temperatures of 12 months (i.e., from january to december) are 13.9, 15.4, 18.1, 23.2, 25.8, 27.6, 28.6, 29.2, 28.3, 25.5, 21.4 and 17.9 ℃, respectively."
"an example of a synaptic and dendritic pruning procedure is presented in figure 3 . the neural structure is composed of four synaptic inputs, two dendrite branches, one membrane layer, and one soma layer as shown in figure 3 (a). the connection case of input 1 is a in branch 1; this synaptic layer can be omitted according to the mechanism of synaptic pruning. in addition, the connection case of input 3 is é in branch 2; branch 2 can be completely deleted based on the dendritic pruning mechanism. the unnecessary synaptic inputs and dendritic branches, which are shown with dotted lines in figure 3 (b), should be removed. finally, the simplified dendritic morphology can be obtained, as in figure 3 (c). only the synaptic layer on branch 1 remains in the structure because only the input 2 can affect the final output of the soma."
"bankruptcy dataset, as shown in table 8, the proposed epnn obtains an average testing accuracy of 99.57%, which is higher than the 98.11% obtained by the pnn and the 94.59% obtained by mlp. in addition, the statistical results also show that the epnn achieves significantly better performances than the pnn and mlp. moreover, the epnn also performs better than the pnn and mlp in terms of sensitivity and specificity. a comparatively higher sensitivity value indicates the powerful capability of the epnn in identifying the companies that are healthy. higher specificity values represent the epnn's ability to not misclassify an unhealthy company. furthermore, the convergence rate of the three models, epnn, pnn, and mlp, are also compared in our experiments in figure 4 . as observed, the epnn achieves the highest convergence rate compared to the pnn and mlp. moreover, figure 5 shows the roc of the epnn, the pnn, and mlp. the corresponding auc value of the epnn is larger than that of the pnn and of mlp. it is emphasized that the epnn is superior to the pnn and mlp in solving the qualitative bankruptcy dataset problem. in addition, the performances' comparisons between the epnn and other commonly used classifiers are presented in table 10 . it is clear that the epnn also shows its superiority in the average accuracy rate on qualitative bankruptcy dataset."
"under normal circumstances, no characteristic gases are produced when the transformer is under operation. however, if a local overheating or a high-temperature electric arc emerges in the transformer, these characteristic gases will be rapidly generated and gradually dissolved into the transformer oil. this indicates an internal failure would occur in the transformer and may significantly shorten the life expectancy of topis, which has an exponential relationship with its winding hst [cit] . therefore, the equivalent hst can be obtained via the corrections of transformer life expectancy, such that the impacts of the transformer failures, such as the local overheating and electric arc overheat on the transformer insulation can be quantified via the equivalent hst."
"the transformer health grade is closely related to transformer life expectancy, i.e., when the transformer runs well, and then the life expectancy of the topis will be extended; meanwhile when the transformer is in bad health, and then the deterioration of the topis will be accelerated and the life expectancy will be shortened. hence, in order to develop such relationship between transformer health grade and transformer life expectancy, a life correction model of the transformer needs to be proposed after consulting the model framework of deterioration degree."
"however, as discussed previously, among these diagnosis techniques above, the fuzzy logic approach has some limitations, thus its fuzzy rule table and fuzzy membership function may indirectly affect the comprehensiveness of the diagnosis results; the ann diagnosis technique is excessively dependent on the completeness of training samples, and its knowledge utilization and representation forms are single, thus its recognition effect is easily affected by external factors, and it is difficult to be employed in high-accuracy transformer fault diagnosis. hence, this method can be combined with other intelligent algorithms, which is also one of the future development directions of transformer fault diagnosis. besides the above methods, there are some intelligent algorithms employed in transformer fault diagnosis, such as the artificial immune algorithm (aia), dynamic clustering (dc), wavelet analysis (wa), bayesian network, and information integration technique. nevertheless, most of the current methods of intelligent diagnosis are only used to diagnose separately according to the fault types, without considering some internal relations between various faults. in addition, some intelligent algorithms are not very mature and only at the exploratory and experimental stage, so it is bound to affect the results of fault diagnosis. moreover, in the actual operation of the transformer, there are many incomplete dga data samples, which are difficult to be employed for intelligent diagnosis. hence, in the future, multiple intelligent algorithms can be combined to complement each other to form a compound network, which will be beneficial to balance the relationship between local search and global search, thus avoiding falling into local optimum. in addition, the transformer detection means can be improved, and the feature data can be extracted by using reasonable detection methods, and then these data can be used to properly match the dga data in order to find the best transformer fault diagnosis method."
"without loss of generality, the average winding hst of other sampling days can be similarly calculated. thus, the average of temperatures in these 36 groups of sampling days can be regarded as the yearly average hst, namely"
"we build up the epnn, which has a dendritic structure and which is trained by jade, to achieve a high bankruptcy classification accuracy. the morphological architecture of the epnn is shown in figure 1 . the network has four layers, namely, a synaptic layer, a dendritic layer, a membrane layer, and a soma layer. the inputs 1∼ from the axons of the prior neurons enter the synaptic layer; then, the interactions of the synaptic signals occur on each branch of dendrites. after that, the interactions are collected and sent to the membrane layer; finally, they are sent to the soma body. during the training process, the necessary inputs and useful dendrites are held, whereas the unnecessary ones are filtered out. the cell would be motivated and would then send an output signal to other neurons through the axon terminal when the input of the soma exceeds its threshold. the morphological architecture of the epnn model is presented below in detail."
"in this case study, as discussed earlier, the reliability assessment of the topis can be divided into three procedures. among them, the first procedure is to employ the hst-based normal ageing model of transformer to calculate the winding hst and solve the failure rate of the topis. in this process, a comprehensive consideration of two dimensions of load and ambient temperature are implemented. first, calculate the transformer hst, i.e., the hst rise in each hour period; then, based on the main load rate curve (figure 6 (28), the failure rate of the transformer can be obtained, which shows that the failure rate of this transformer is only 0.01% when it has been put into operation for 12 years, this is mainly due to the load of this transformer is controlled within 40%. at this point, it shows that this transformer has a good operation state, thus this curve can well reflect the fault level of the topis. however, some unpredictable factors such as partial discharge and high temperature overheating have broken the continuity of normal ageing after the transformer is put into operation for a number of years, leading to insulation deterioration of the transformer, as well as the increase of the failure rate, thus the assessment accuracy of the base model will be extremely reduced. in addition, the possible defects of the transformer in the design will also have a greater impact on the failure rate of the transformer. hence, the failure rate curve must be corrected according to the actual condition of the transformer, that is, the second calculation process is needed."
"the bp algorithm and its variations [cit] are considered rather inefficient because of their obvious drawbacks such as their slow convergence [cit], sensitivity to initialization [cit], and a tendency to become trapped in local minima [cit] . specifically, first, during the learning process, the error often remains large because the learning algorithm leads the anns to local minima instead of the global minimum. this problem is quite common in gradient-based learning approaches. second, the convergence of the bp algorithm is strongly dependent on the initial values of the learning rate and momentum. the unsuitable values for these variables may even lead to divergence. third, the learning time increases substantially when the dataset becomes larger [cit] . many researchers have focused on making improvements to resolve these shortcomings of bp, but each method has its disadvantages [cit] . these disadvantages make them unreliable for risk classification applications and inspire us to adopt other algorithms to train the neural model to avoid the computational inefficiency and local minimum problems."
"method (train-to-test ratios) average accuracy (%) [cit] logistic regression (2/3-1/3) 97.2 [cit] rotation forest (2/3-1/3) 97.4 [cit] naive bayes (2/3-1/3) 98.3 [cit] rbf-based svm (2/3-1/3) 99.6 e. k. [cit] nearest neighborhood (50%-50%) 97.6 j. uthayakumar et. al. (2017) [cit] ant-miner (10 fold cross validation) 100 j. uthayakumar et. al. (2017) [cit] logistic regression (10 fold cross validation) 99.2 j. uthayakumar et. al. (2017) [cit] mlp (10 fold [cit] hybrid logistic regression-naive bayes (90%-10%)) 99.64 [cit] neural network model with robust logistic regression (50%-50%) 69.44 [cit] neural network model with inductive learning algorithm (50%-50%) 89.7 [cit] neural network model with genetic algorithm (50%-50%) 94 [cit] neural network model with neural networks without dropout (50%-50%) 90.3 [cit] neural network model with svm (50%-50%) 98.67 [cit] neural network model with decision tree (50%-50%) 99.33 j. uthayakumar et. al. (2018) [cit] genetic ant colony algorithm (not mentioned) 91.32 j. uthayakumar et. al. (2018) [cit] fitness although not all the sensitivity and specificity values of the epnn are larger than those of the pnn and mlp, the pnn performs worse on sensitivity, and mlp is the worst on specificity. the epnn achieves better performances on both sensitivity and specificity. the convergence curves of the epnn, the pnn, and mlp are compared in figure 6 . this figure shows that the epnn achieves the highest convergence rate compared to the pnn and mlp. in addition, figure 7 presents the roc of the epnn, pnn, and mlp. in addition, the corresponding auc value of the epnn is larger than that of the pnn and mlp. this implies that, compared with the pnn and mlp, the epnn is a more effective classifier on the distress dataset. besides, the classification performance of the epnn is compared with knn, rbf, rf, dt, svm, and da, and the corresponding results are presented in table 14 . as table 14 illustrated, epnn performs better than all the other classification methods except rf. since there are no other classification methods applied to classify distress dataset in the literature, horizontal comparison can not be fulfilled for this dataset."
"there are many factors that can affect the oil-paper insulation property of the transformer, which cause the insulation ageing. among them, the factors that influence the transformer internal temperature, especially the winding hst [cit], are the core ones to the topis [cit] . at present, the exact position of transformer hot spot is very difficult to be determined while most transformers haven't been installed with temperature sensors; in particular, future installation of such sensors is relatively difficult to be realized. to address this issue, so far, thermal characteristic based model has been widely adopted to study the thermal operation of transformer. in addition, ieee std. c57.91 [cit] and iec 60076-7−power transformers−part 7 [cit] recommended an empirical model to calculate transformer winding hst. here, the former one defined the empirical equations to calculate the top-oil temperature and winding hst of the transformer while the latter one assumed that the temperature rise curves of the winding distribution and oil distribution are two parallel straight lines, as shown in figure 1 . it can be found that the winding hst is obtained by the ambient temperature, top-oil and bottom-oil temperatures, and temperature gradient of winding hot spots with respect to the oil temperature. note that the hst calculation method in ieee standard [cit] was employed in this paper to solve the winding hst."
"as the approaching degree q based on the dga testing data is calculated as 0.5705, which lies in the third grade, the corresponding life expectancy is calculated as eq 0.5705 0.33 1 64.12 23.016 (year) 1 0.33"
"in step 3 of algorithm 2, we project the recovered clean training images x z onto the subspace p. ideally, we would also like to project the clean test images onto p for classification. however, it is usually not practical to obtain clean test images in real applications. in this paper, to show the robustness of p for noisy data, we directly project noisy images onto p. to enhance the classification performance, one could apply some image denoising techniques before projecting noisy test data onto p."
"in this paper, a novel lsl approach, srrs, is proposed for feature extraction and classification. the proposed approach iteratively learns robust subspaces from a low-rank learning model, and naturally incorporates discriminative information. the convexity of the supervised regularization term has been theoretically proved. the experimental results on six benchmark data sets demonstrate the effectiveness of our approach compared with the state-of-the-art subspace methods and lowrank learning methods. moreover, when the data contain considerable noise or variations, our approach can improve the classification performance."
"the ageing of winding insulation system for the oil-immersed power transformer is unidirectional and irreversible. in fact, the mechanical property, dielectric strength and resistance will be gradually degenerated along with the time, which might result in a transformer failure or even an end of transformer life. hence, after obtaining the winding hst namely θh (℃) via calculation, the expected life of the winding insulation system can be obtained. here, the relationship between temperature and chemical reaction rate of insulation materials can be written as"
"the original distress dataset is an extensive dataset; it includes 422 companies, and each company behaves differently in different time series. moreover, this dataset is imbalanced and skewed; there are 136 financially distressed companies against 286 healthy ones, 136 firm-year observations are financially distressed, while 3546 firm-year observations are healthy. to make the structure of the distress dataset under observation be similar to the qualitative bankruptcy dataset, we perform some preprocessing. first, all the distressed companies are chosen from time series period 1 to period 14, and the total number of distressed companies is 126. in each time series period, 15 healthy companies are selected randomly, and the number of healthy companies is 210. thus, there are 336 samples remaining in the newly generated dataset. because each company presents 83 features, this dataset remains relatively large. we have adopted the minimal-redundancy-maximal-relevance (mrmr) criterion to generate the feature selection. the mrmr criterion offers an excellent way to maximize the dependence of the results on the input features by combining the max-relevance criterion with the min-redundancy criterion. moreover, mrmr can not only enhance the appropriate feature selection but also achieve high classification accuracy and high computation speeds [cit] . the max-relevance mechanism of mrmr is defined as follows:"
"besides, in this case study, this transformer has been overhauled once from putting into operation to now. considering that the continuity of the operating state of the transformer is interrupted after the overhaul, the corresponding equivalent hst is also reduced. therefore, it is necessary to conduct a second dynamic correction to the life expectancy of the transformer, that is, the third calculation process is needed."
"abstract-this paper presents a novel deep learning architecture to classify structured objects in datasets with a large number of visually similar categories. we model sequences of images as linear-chain crfs, and jointly learn the parameters from both local-visual features and neighboring classes. the visual features are computed by convolutional layers, and the class embeddings are learned by factorizing the crf pairwise potential matrix. this forms a highly nonlinear objective function which is trained by optimizing a local likelihood approximation with batch-normalization. this model overcomes the difficulties of existing crf methods to learn the contextual relationships thoroughly when there is a large number of classes and the data is sparse. the performance of the proposed method is illustrated on a huge dataset that contains images of retail-store product displays, taken in varying settings and viewpoints, and shows significantly improved results compared to linear crf modeling and unnormalized likelihood optimization."
"few studies have applied deep learning features or detection results to context models: [cit] explored several techniques to learn structured models jointly with deep features that form mrf potentials. chu and cai [cit] evaluated fig. 2 . scene example with the relevant classes presented on the left. for some of the products in this scene, classification based on local appearance alone would be extremely difficult even for expert humans. the object on the top right, for instance, is partially occluded, and visually distorted by reflections, illuminations, and focus. additionally, it is facing backwards, making its flavor undetermined. when viewed separately, the volume cannot be determined either. shelf-level classification exploits the information extracted from the other items and their spatial relations, including a-priori knowledge of structure statistics, to jointly determine the shelf classes."
the crf model defined above can be trained in a supervised manner by maximizing the log-likelihood of all sequences in the training dataset [cit] :
"the benchmark contains 24,024 images, 76,081 sequences, and 460,121 objects, each labeled as one of 972 different classes. sequence lengths can vary from 2 to 32, and are typically between 4-12. we split the dataset into 80% training and 20% testing."
"one major advantage of memms over crfs (and hmms) is that training can be considerably more efficient. unlike crfs, in memms the parameters of the maximum-entropy distributions used for the transition probabilities can be estimated for each transition distribution separately. when applying mmem for inference it suffers from the label bias problem [cit] which may lead to a drop in performance in some applications. here, however, memm objective is used only as a local approximation to learn the parameter set of the linear-chain crf model whereas the test time inference uses a global normalization of crf modeling and thus avoids the label bias problem. the objective function is now defined as the conditional probability of the current-object class, given the class of the left-side neighbor object:"
"in our model, we found it advantageous to standardize the input features of the softmax layer. they are composed of the visual features of the cnn h t and the learned neighbor embeddings ry t−1 (see fig. 4 ). the standardization of the feature vector (ry t−1, h t ) is important in order to avoid inherent bias between the local-visual and contextual information. the goal is to encourage each feature of the softmax input to have standard mean and variance. since we use a pretrained cnn we can standardize the visual features by an offline pre-processing stage. in contrast, the embeddings are jointly learned with the softmax layer and hence we use the batch-normalization [cit] method to learn their mini-batch normalization during the training process. in fact, since the input of the embedding layer is a one-hot vector, the batchnormalization process directly standardize each feature in the embedding space."
"1) combining deep class embedding into a crf formulation that enables handling datasets with huge number of classes. 2) an approximated-likelihood training procedure that is both computational efficient and, unlike exact crf likelihood, we can easily incorporate batch-normalization into the training procedure. the rest of the paper is organized as follows. in the next section we describe a crf model with class embedding formulation and present the learning and inference algorithms. section 3 contains a detailed data description and comparative experimental results and finally conclusions are given in section 4."
"class-embedding crf: this is the main model described in this study, where the crf is enhanced into a much richer, but non-convex model by extending the pairwise weight matrix as defined in eq. (5). we implemented both global and local training procedures, and studied several alternatives of embedding structures and likelihood approximations as elaborated below. in all cases, local likelihood approximations are extremely faster to train, but most of the methods for either local or global training provided similar or worse results in performance, compared to the linear crf model. the sole variant which remarkably improved performance is the objective structure in which all the output embedding features are standardized by batch-normalization. in this case, the local approximate likelihood method has two major advantages over global maximum likelihood: global optimization of lc-crf is not only much more time-consuming, but also lacks the ability to apply a straightforward batch normalization strategy, since the activations are shared in multiple locations in each sample in the mini-batch."
"many groups of classes belong to the same archetype, and only differ in terms of minor details such as volume, flavor, nutrient-content etc. they often share similar visual features, which makes appearance-based classification very difficult (fig. 2) . on the other hand, the object layout behavior is very coherent: it is dictated by the supplier planograms (specified product layouts) and extracted from the image realograms (observed product layouts). although realograms are non-deterministic by nature, consistent semantic patterns are frequently spotted. class transition behavior may be discovered, revealing tendencies of pairs to appear as left-toright neighbors, and individual classes to appear multiple times successively (fig. 3) . the unique challenges we face in our task are derived from the large number of visually similar classes, which co-occur in distinct structures in large-scale images. since the images capture arbitrary subsections of the shelf displays, the visual appearances of object sequence in a shelf vary unpredictably in terms of their relative positions and occasional unnoticed or absent elements. nevertheless, the co-occurrence data statistics remains stable in most cases, which justifies stationarity and markovity assumptions for the structure modeling."
"this study addresses the problem of classifying a sequence of objects based on their visual appearance and their relative locations. our dataset contains photos of retail-store product displays, taken in varying settings and viewpoints. we need to identify the class of each product at the front of the shelves. the dataset is exclusively characterized by having a distinct geometric object structure -sequences of shelves, a large number of classes, and very subtle visual differences between groups of classes -some classes only differ in sizes or minor design details. the unique challenges in this task involve handling the large number of possible classes, and the fact that the classes are not clearly distinctive by their appearance but rather by their context. for example, products with identical appearance but with different container volumes are considered different classes (see examples at fig.1, 2) ."
"the objective function is no longer linear or concave with respect to the network parameters, but deep learning training techniques have been shown to yield good results for nonconvex optimization tasks [cit] . this simply means that we need to apply the deep learning approach not only for the input image representations, but also for the neighboring transition parameters."
"because the object local appearance may not suffice for accurate categorization, additional information needs to be considered. in real world images, contextual data provides useful information about spatial and semantic relationships between objects. modeling a joint visual-contextual classifier is nontrivial in that some contextual cues are very informative, whereas others are irrelevant, or even misleading. most deep learning detectors classify each detected object individually without taking the contextual information into account."
"as a byproduct of the classification model we also obtain a low-dimensional embedding of the different classes. each column of the neighbor embedding matrix r is vector representation of the corresponding class. a common similarity metric is the cosine of the angle between the vectors. we can measure the distance between classes by the cosine of their vector representation. fig. 5 shows several examples of an object class and its most similar classes. we can see that this similarity does not reflect visual appearance similarity, e.g. in the second example the similar classes have very different colors. this situation was extensively studied for the linguistic problem of word embedding. the goal of word embedding algorithms is to represent similar words by similar vectors. it is often useful to distinguish two kinds of similarity or association between words [cit] . two words have firstorder co-occurrence if they are typically nearby each other (e.g. wrote is a first-order associate of book or poem). two words have second-order co-occurrence if they have similar neighbors (e.g. wrote is a second-order associate of words like said or remarked). second-order word similarity is thus expected to capture a semantic meaning and measure the extent to which the two classes are replaceable based on their tendencies to appear in similar contexts. in fig. 5 and 6 we show that object class embedding captures secondorder information. proximity here corresponds to the mutual tendency to have similar neighbors. we can see in the figures that similar classes, although look visually different, represent products of similar container-types, volumes and brands."
"however, the optimization is relatively slow for a large number of classes, because its complexity is quadratic in the number of possible classes. in order to speed up the training process, we can estimate the parameters locally, by optimizing an approximate objective function. a local approximation of the likelihood would require samples of individual objects and their immediate neighbors rather than entire sequences. linear-chain crfs were originally introduced as an improvement on the maximum entropy markov model (memm) [cit], which is essentially a markov model in which the transition distributions are given by a logistic regression model. the main difference between crfs and memms is that a memm uses per-state exponential models for the conditional probabilities of next-states given the current-state, whereas the crf has a single exponential model for the joint probability of the entire sequence of labels given the observation-sequence. crf and memm can be written using the same set of parameters. the memm directed graphical modeling in our case is:"
"where i goes over the sequences and t goes over the objects in the sequence, h i,t is the object cnn-based representation, y i,t is the true class label and p is as defined at (8) . note that the computational complexity of the memm likelihood (9) is linear in the number of classes unlike the crf likelihood whose computational complexity is quadratic. this surrogate likelihood function whose samples are pairs of objects and corresponding neighboring labels can be used at train time to accelerate the training process. because the model is stationary and conditionally independent of indirect neighbors, breaking the samples from sequences into adjacent pairs of direct neighbors does not necessarily eliminate significant contextual information. rather, when learning the non-convex objective of class-embedding crf, it may enrich the training dataset, improve the stochastic nature of the sgd optimization process, simplify and improve regularization techniques such as batch-normalization, and help prevent overfitting since there are many more object samples than sequence samples, and the mini-batches are composed of adjacent pairs of objects taken from random training samples. in contrast, restricting the minibatches to contain full sequences, would decrease the model's freedom to discover better solutions for the objective of pairwise transition parameters. in fact, as we empirically show in the next section, optimizing the local approximate likelihood with object-level batch-normalization yields better results than optimizing the unnormalized global lc-crf likelihood. in the appendix we review standard likelihood approximation strategies for efficient crf training and show that the training method we are using in this study can be viewed as a simplified version of the piecewise-pseudolikelihood approximation [cit] ."
"we also tried an alternative (or complementary) clustering approach, in which we grouped the classes into clusters that maximize the mutual information between consecutively visited groups [cit] . the pairwise potential was then defined by pairwise statistics of the class clusters. distinct clusters of classes were identified, but we did not manage to harness this information for the task of non-hierarchic class identification."
"the backwards term of the pwpl (16) is independent of the crf input and hence the memm-like objective function is theoretically very related to pwpl. thus, the approximated likelihood we are using for training, that is based on the memm model, can be viewed as a simplified version of the piecewise-pseudolikelihood objective (15) that was found to be the preferred likelihood approximation for language processing tasks [cit] ."
"our dataset contains photos of retail-store displays, taken in supermarkets and grocery-stores. the images capture arbitrary subsections of the displays, in varying settings and viewpoints. the objects are the inventory items positioned at the front of the displays, and the classes are their stock-keeping-unit (sku) unique identifiers. each object is annotated by its class label and bounding-box coordinates. the objects in each image are grouped into shelves -sequences of horizontal layouts, sorted from left to right."
"note that computing the piecewise likelihood is quadratic in the number of classes. piecewise pseudolikelihood (pwpl) is the standard pseudolikelihood applied to the node-split graph. its computation is efficient because the objective function is simply the sum of local conditional probabilities. sutton and mccallum [cit] showed that in many cases the pwpl has better accuracy than standard pseudolikelihood, and in some scenarios it has nearly equivalent performance to piecewise approximation and even to global maximum likelihood. in our case, applying pseudolikelihood approach on the piecewise objective (13) would give us the pwpl form:"
"in this study we provide a crf based method that explicitly learns the embedding of classes with respect to their neighbor's visual features. this is achieved by factorizing the crf pairwise potential matrix to impose the desired structure of class embedding in a low-dimensional space. our model learns the factorized parameters, and yields a joint contextual-visual embedding of the classes. to efficiently train the network, we introduce a pairwise softmax architecture which optimizes a local approximation of the likelihood. since the factorized loss function is not convex, we exploit the simplicity of the local approximation architecture to include batch-norm related regularization for the object samples, and attain dramatic improvement not only in training time but also in the overall performance of the trained model. at test time dynamics programming techniques are used for efficient exact inference of classes."
"we also tried to replace bn by standardization of the onehot input vectors at the input of the embedding layer, but this approach does not affect the output of the embedding layer as bn does, and did not achieve improvement in performance. hence, we favor the pairwise softmax architecture with the memm-like objective (10) and a bn layer between the embedding output and the softmax input. in addition, we tried increasing the model's non-linearity by adding another fully connected layer and nonlinear relu between the one-hot vector input and the fully connected embedding layer. we also tried learning the embedding in a higher dimensional space. those enhancement, however, did not improve performance, and turned out to be redundant."
"object recognition is one of the fundamental problems in computer vision. it involves finding and identifying objects in images, and plays an important role in many real-world applications such as advanced driver assistance systems, military target detection, diagnosis with medical images, video surveillance, and identity recognition."
"our study is motivated by images of store shelves where a large number of objects with many possible classes appear in a single image and we want to classify the object using both visual and context information. a preprossessing detection stage is applied to extract the detection bounding boxes, crop their image patches and organize them according to their locations on the shelves. the input data used for our task are the sequences of images: each image captures an individual product, and the images are organized in sequences ordered according to their relative positions on the shelves. let x i,t denote the image in position t of sequence i, and y i,t the corresponding class label. for notational simplicity, we omit the index i when referring to an individual sequence."
"in optimization, feature standardization or whitening is a common procedure that has been shown to reduce convergence rates [cit] . in deep neural networks, whitening the inputs to each layer may also prevent converging into poor local optima. however, training a deep neural network is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers, and need to continuously adapt to the new distribution. the batch-normalization (bn) [cit] method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch."
"log-linear crf: this method learns the log-linear parameters of the linear-chain crf (3). we implemented both global and local approximate likelihood training methods and tried both l 1 and l 2 regularizations for the pairwise potential matrix. we also applied standardization on the one-hot input vectors. the results in all cases were comparable, and provided noticeable improvement over the baseline contextless classifier. the local training procedure is much more efficient, because its time complexity is linear in the number of classes, whereas the global training procedure is quadratic in the number of classes. since the number of classes is 972 and the training dataset is large, this significantly affects training time even when applying extensive gpu parallelization to compute the partition function. in our experiments, the local training method was about 25 times faster than the global training method, provided the same amount of gpu memory, and as we empirically found, its performance is nearly identical to the globally trained network."
"the performance of a joint crf model on faster r-cnn [cit] detection results, using an a-priori statistical summary for the pairwise potentials. korzeniowski and widmer [cit] introduced a two-stage learning model for musical chord recognition: one network learns a single-frame representation, and the other learns the potentials of a linear-chain crf model, using the frame-representations as the crf input. the aforementioned crf models use pairwise potentials to represent object-pair interaction. they allocate a different parameter for each class pair. this approach, which ignores class similarities, may be sufficient in small sets of distinct classes. however, it is not suitable for a large class-set that contains visually similar classes. our dataset, which includes many visually similar categories, and nearly a thousand classes and a million possible pairwise transitions overall, requires more advanced learning mechanism. in most previous object recognition studies the visual information was dominant. in our task context information also has a significant contribution."
"we introduced a novel technique to learn deep contextual and visual features for fine-grained structured prediction of object categories, and tested it on a dataset that contains spatial sequences of objects, and a large number of visually similar classes."
"the same problem appears in other known methods of local likelihood approximation: close variants of our local training model are the piecewise, pseudolikelihood piecewisepseudolikelihood (pwpl) methods (see details in appendix a). applying embedding-batch-norm to the pseudolikelihood or pwpl methods would once-again require parameter tying between the softamx inputs and the softmax weights. however, the pwpl in our case can be reduced to the from of a forward term which is equivalent to the memm-like objective (8) and an additive backwards term which is independent of the crf input. hence, the memm-like objective function is theoretically very related to pwpl."
"where the vector y i contains the ground-truth labels of the i th sequence, h(x i ) contains the corresponding object feature vectors of the sequence of observations, i goes over the sequences in the training data, and the loss function is defined at (1) with the potentials (5). since the underlying graph is loop-free, it is tractable to compute the likelihood function and its gradient using the forward-backward algorithm [cit] ."
"crf was originally applied to language processing tasks such as part of speech (pos) tagging and named entity recognition (ner) [cit] . in most applications of crf to either language or image understanding, there are no more than a few dozen different classes. in our dataset, we have hundreds of classes. the pairwise transition between two classes has nearly a million possible states, whereas the crf function (3) has a log-linear form, and contains a single parameter per transition ordered-pair. in order to properly learn and generalize the massive variety of possible neighboring patterns, we enforce a structure on the pairwise potential matrix: the goal is to learn neighboring-class embedding in a feature vector space. for this purpose, we define a low-dimensional decomposition of the pairwise potential matrix p as the product of the left-side neighbor embedding matrix r and the class embedding matrix q:"
"standard classification approaches use a cnn to predict each object-level observation individually, implicitly assuming independence between object samples. in order to include context in the classification process, we model the sequences as a crf."
"at test time, global classification is applied to the linearchain crf. dynamic programming algorithms may be used for efficient and exact inference as follows: the viterbi algorithm training data: feature sequences x 1, ..., x n with corresponding label sequences y 1, ..., y n . training algorithm:"
"our model clearly outperforms all the other tested models. this architecture appears to be the most straightforward generalization of a context-less classifier to be context-dependent when both the input and the context data require a large learning capacity: the network learns deep feature vectors for neighboring classes, analogously to the learned deep input representations. the markovity and stationarity assumptions make it sufficient to train with individual objects as samples to enrich the training data diversity, allow for a simple embedding batch normalization, and boost the non-convex optimization process both in terms of time and performance."
in order to validate the performance of the proposed method we implemented several alternative methods. all the methods are based on the same context-less cnn local information. they differ in the way they learn the object context information from the training dataset and the way they integrate the context model with the local cnn soft-decision. below is a list of the context models we implemented.
"shelf-level classification may not be sufficient in situation such as the one depicted in fig. 7, where it may help identify a few probable shelf-classification possibilities, but no clear decision on the best one. this can be resolved by defining additional spatial-relationships in the graph, such as top-bottom edges, or hyperedges between shelves, and learning their embeddings. a scene-level classification may 6 . a visualiztion of the embedded classes in the 2 nd order similarity space, created by t-sne to reduce the 32d space into a 2d space. it can be seen that classes are clustered according to shelf \"semantic\" (rather than visual) similarty and relations. fig. 7 . example of a case where shelf-level classifications are not sufficient, and additional spatial relations need to be included in the model. the arrows point at the thumbnails of the ground-truth labels of the objects on each shelf. be inferred using a belief-propagation, variational methods or mcmc methods. additional improvement may be achieved by integrating the bounding-box dimensions as part of the model input, applying end-to-end joint training of the cnn and the crf, and using recurrent layers to model relations between object proposals and labels, learn sequence embedding, and perform training and inference in loopy graphs."
"to find the labels of the object sequence. finds the most probable sequence label assignment, and the forward-backward algorithm extracts the marginal probability of each item, by summation over all possible assignments [cit] . note that although the training was done by local likelihood approximation, and we assumed that the predecessor label is known, at test time we apply the global normalization over all possible object sequences. the proposed training and inference methods are summarized in table ii-c. fig. 4 shows an illustration of the training architecture."
"exp(bn (ry t−1 ) qy t + h t u y t + b y t ) (10) a major advantage of the approximate-likelihood we are using is that, unlike sequential models such as crf, here it is very simple and effective to apply embedding batch normalization for each neighboring-label sample."
"the columns of q are low-dimensional embeddings of the target classes, and the columns of r are embeddings of the classes of the left-side object. assigning the matrix factorization (4) to the crf potential function (3) we get:"
"context has been used to improve performance for image understanding tasks in various ways [cit] . graphical models have been widely applied to visual and auditory analysis tasks, by jointly modeling local features, and contextual relations. tasks addressed by these models include image segmentation and object recognition [cit], as well as speech [cit], music [cit], text [cit] and video analysis [cit] ."
"in our approach, several sma wires with different characteristics have been evaluated in terms of power consumption, cooling time, and dimensions, mounted in a simple configuration (only one sma wire) or in parallel (in the case where various sma wires are needed for high force). in this last case, various sma wires can be mounted in parallel in the same bowden cable or separately, each one in a bowden cable. in both cases, when the sma wires are in parallel configuration, an important characteristic is the tension of each sma wire. although the terminal units are used, these do not guarantee the same tension in each sma wire. compared to the case where only one sma wire is controlled, in parallel configuration, various sma wires (a sum of non-linear models with different force tensions) are controlled with a single controller. the different force tensions between sma wires implicate that one sma wire supports more force than other wires, and when these force differences are significant, it causes the actuator to break."
"in the development of new systems a compact mechanical design is often required, with lightweight, noiseless operation. all of this is closely related with the actuator, and in this context new solutions are being sought in emerging actuation systems such actuators based on sma."
depending on the area of the signals which doing with the axes system observe that the actuator with bowden cable is little great compared without bowden cable.
"the displacement results and pwm control signal for the simple and double actuator are compared in the test bench. due to the power electronic restrictions of the test bench presented in section 2.2 the tests are done with 0.25 mm sma wire diameter. the control parameters are the same presented in section 2.4 and the length of the wires for the simple actuator is 0.2 m. for the double actuator, the wire dimensions are calculated in function of equation (6) . the recuperation force, in function of the wire diameter, is generated with 0.5 kg weight. the actuator position response to a step signal reference is presented in figure 13 . in the heating stage, the actuator's response is relatively similar, with the actuators presenting overshoot. compared with the response of the actuators based on 0.51 mm sma wire presented in the previous sections, overshoot is presented, representing a small electrical power consumption and the necessity to recalibrate the bpid controller parameters. in the first half of actuator cooling stage, the double actuator presents a small delay in the last part, due to entry in the martensite zone before the simple actuator."
"the sma wires are fixed in the actuator terminal and to the other extremity to the bowden cable. in each bowden cable up to five sma wires (limited according to the bowden diameter) can be mounted, but there must be always the same number of smas in each bowden cable, for the force equilibrium. in the heating stage, the sma wires actuate over the mobile part of the actuator, producing the displacement of the output wire (nylon wire) over the pulleys. the actuator presents six pulleys, each one with a middle channel from the output wire pass. the distance between the pulley walls is less than the output wire diameter, avoiding going to and from the channel and produce jams. the output pulleys of the mobile part coincide perpendicularly with the output pulleys of the fixed part. according to this, each pulley multiplied the displacement by two. changing the number of pulleys, the multiplication rate easy can be modified. the current configuration permits mounting 10 sma wires which give a total force around 356 n. the output actuator force with this configuration is 59.3 n."
"another interesting configuration is the actuator with bowden cable which contains multiples sma wires. in this case, it is necessary to decide between two configurations: multiple sma wires inside a single ptfe tube and bowden cable or for each sma wire independently a ptfe tube with bowden cable. the first configuration, multiple sma wires in parallel configuration in only one ptfe tube and bowden cable presents some advantages compared with the second configuration:"
"different mechanical configurations have been presented to increase the electrical efficiency and the total displacement work. the brake gear 1:6 actuator offers the possibility of configuration of the multiplication ratio, and maintains the actuator in a fixed position with a low energy consumption (with the brake aid)."
"the corresponding control signals, pwm currents, are given in figure 11b . although the peaks at the beginning are similar for both configurations (and the energy consumption, which is proportional to the pwm signal), the option with a single bowden cable has a lower energy consumption during the displacement stage."
"the pwm control signals of both actuators is presented in figure 14 . as can be seen, the pwm controller signal generated by the double actuator presents a relatively high peak compared with the pwm signals of the simple actuator. these high peaks can be cancelled by recalibrating the bpid controller."
"stepper motors are good for the position control at low velocity; instead, brushless dc motors are indicated where a high velocity is needed; in general, these are used together with the gearboxes to reduce the velocity and increase the torque."
"to heat the sma wire, which is necessary to activate its shape-memory effect and thus contract it, a controlled electric current is passed through the wire. this electric current is provided by a high-fidelity commutation circuitry, driving an extremely low resistance with a mosfet transistor activated with a pulse width-modulated (pwm) signal from the microcontroller."
"in the previous works, a sma has been studied, modelling his hysteresis behavior with models based on prandtl-ishlinskii and bouc-wen, and the actuator with this controller such a black-box model based on narx and hammerstein-wiener models [cit] ."
"this section presents comparison and analysis between different configurations of sma-based actuators: with and without bowden cable, high and low temperature, and independent or multiple sma wires in the same bowden cable. three novel mechanical designs are proposed in sections 3.4-3.6. with these new mechanical configurations, the total displacement or the energy consumption can be improved in some specific applications."
"as can be seen in figure 11a, the configuration with three bowden cables has a slower response in the heating stage. this is caused by its better heat dissipation. this effect can also be observed in the cooling stage, where this configuration has a lower recovery time. after some cycles, the difference between both configurations in the cooling stage is more evident due to the heat accumulated in the bowden cable."
"from figure 14 it is difficult to say if the simple actuator is electrically more efficient than the double actuator, due to the big differences between the generated signals by the controller. the simple actuator pwm signal presents an amplitude which oscillates in a small range during the actuation, compared with the double actuator. in this sense the average of the pwm control signals was calculated, which is 2.0696% for the simple actuator and 2.2283% for the double actuator, representing a relatively similar power consumption. however, this result depends on the bpid parameters, the desired reference and the actuation force. in this case, the pwm signal average is not reliable. also, this signal presented in figure 14 is not adequate for the actuator control; a signal of this type shortens the actuator life (in general, a soft signal can be used). in certain cases, it is probable that double actuators have less power consumption than simple actuators, considering that the need to heat is less in the simple actuator to reach the desired reference."
"compared to the literature, this paper, in addition to the new configurations such as double actuator or brake actuator, presents an analysis of different configurations of sma actuators, with the goal to provide the reader with the opportunity to choose the adequate configuration of sma actuators according to the application in which it will be integrated. compared with other actuators which multiply the sma displacement, the actuator mechanical design is proposed for large forces (more than 100 n from various wires in parallel), with a compact and flexible design (less the multiplication mechanism part)."
"in the majority of applications, the actuators based on sma have a simple design: the two extremities of the sma wire are crimped, one to the fixed point and another one to the system which will be moved, but different approaches to mechanical configuration design have been proposed to improve the force and total displacement of these:"
"when the sma is proposed to be used as actuator in any robotic application, important factors must to be considered to select the most suitable configuration:"
"in practice, the control signal has a great importance which directly affects the dynamics of the system and, implicitly, its lifespan. a control signal that easily exceeds the system input or shows oscillating pulses between high values can cause severe damage to the system. the control signal responses of the two actuators with the same bpid controller parameters in front of the same reference (from figure 7) can be seen in figure 9 . figure 9 presents the control signals of the two actuators following a step reference from 0 to 10,000 sensor units and maintaining this position for 20 s. the control signal is approximately the same with a little difference."
"the control signal is directly proportional to the energy consumption, which means the actuator with the bowden cable has a little more energy consumption compared to the actuator without bowden cable."
"focusing on robotics, one of the limitations in the development of wearable robotic devices lies in the development of lightweight actuators. thanks to their flexibility, high force-to-weight ratio, and small volume, sma-based actuators can be considered a good actuation solution for wearable and soft robotic applications and especially for rehabilitation devices. they can be considered an alternative to conventional actuators such as dc and ac motors for robotics applications where a high force at low velocity is needed, or for pneumatic muscles if a low weight, low size, and noiseless operation is required."
"one of the actual limitations of the sma-based actuators is the electrical power consumption when this is actuated by the joule effect. the proposed actuator presents a brake which maintains a certain position, blocking the output wire. the brake can be seen in the figure 17b . this is composed of a fixed part, attached to the actuator and electrically isolated, and a mobile part which is actuated with a sma wire with a length of 0.025 m. the fixed part has two holes which, when the mobile part is actuated, are aligned with the hole of the mobile part and passes the output wire. without actuation, the mobile part recuperates the position (closing the pass of the output wire) with the aid of a compressing spring. in function of this when the mobile part is not actuated by the output wire is blocked, maintaining the current position and reducing the electrical power consumption to 0. other advantages are the possibility of cooling the actuators when maintaining a fixed position and to control the output position of the actuator in the initial shape recuperation stage with the aid of the brake."
"to test different configurations of the actuators, a test bench was built. the test bench allows multiple configurations for testing the sma actuators: the sma wire with a hanging mass attached to the non-fixed end, and the sma wire with a bias spring attached to the non-fixed end, tests various wires in parallel configurations, adding external cooling system such air-flow and an agonistic-antagonistic actuation with two sma actuators. a schematic diagram of the test bench part used in this work can be seen in the figure 3, where the sma actuator in the lineal movement can be tested. in figure 3, 1 represents the points where the sma actuators are attached, 2 represents the sma actuator, 3 is the position sensor which measures the displacement of the movable part, 4 is a weight which will be moved by the actuator, and 5 is the movable part. the method to fix the sma wire or sma-based flexible actuator (with the bowden cable) is as follows: one of the ends of the sma wire is fixed to the structure of the test bench and the other sma ends is crimped to a movable part of the test bench. over this movable part a magnetic strip on its top is fixed such as a part of the position sensor. when the wire contracts, the movable part is displaced, and the position sensor measures the displacement of the magnetic strip. in the movable part, a high-resistance polyethylene wire, dyneema, from the caperlan company was fixed, from where the dead weight or spring will be attached."
"the simple actuator is based on sma wire from the dynalloy inc. company, which is introduced in the ptfe tube and everything in a bowden cable. the diameter wire used in this experiment was 0.51 mm. in figure 7 the response of the sma actuator with the bowden cable and the response of the actuator without the bowden cable is presented, when the reference pattern is represented by a step reference with the amplitude of 10,000 units of sensor. with the same bpid controller parameters, the actuator responses are similar. the actuator without bowden cable in the heating stage presents a little overshoot compared with the second case when the sma wire is introduced in the bowden cable. this occurs due to the better heat dissipation effect when the sma wire is in contact with the bowden and ptfe tube. as a consequence of this effect, the actuator in the second case presents a slow response. the same effect is identified in the cooling stage, when the actuator \"recuperates\" the initial form. in this case, the bowden cable helps the cooling stage of the sma wire ( figure 8 ). figure 7 to highlight the behavior of the two actuators in the heating and cooling stage. as can be seen, the bowden cable affect in the cooling stage accelerates the actuator \"recuperation\", and the heat is transmitted from the sma wire to the bowden cable, which aids the dissipation. this is observed in the first recuperation stage when the difference between the ambient environment and bowden cable temperature is more elevated, and the heat exchange is more accelerated. in the second stage, when the temperature of the bowden cable is nearer the ambient temperature, the sma actuator without bowden cable presents a better \"recuperation\", the heat exchange going directly with the ambient environment. the bowden cable in this stage maintains the temperature and slows the last stage of cooling for the sma wire."
"where the p double is the power absorbed by the double actuator and the u is the power supply voltage. from equations (8) and (9), it can be seen that the power absorbed by the double actuator is 2.0799 times greater than the simple actuator, but it obtains 1.9225 more displacement."
"the bowden cable configuration offers the possibility of flexibility for the actuator which easy can be integrated in applications such as robotics and soft robotics, medicine, aerospace, etc. also, the bowden cable offers the possibility to unite various sma wires in parallel configuration to increase the actuation force (the sma wires in the same bowden cable is more electrically efficient and it is easier to adjust the same tension, but the cooling stage is slower)."
"corresponding with equations (5) and (6), with the same actuator length the double actuator presents a displacement of 1.9225 times more. on the other hand, the disadvantage of using this type of actuator compared with the simple actuator is to fix the extremity to a fixed structure (in the simple actuator the extremity of the sma wire is fixed to the bowden cable)."
"actuators are one of the most important part from one actuated system. the most common actuators in mechatronics systems are dc motors in mobile systems due to the possibility to aliment them from the battery and due the easy ability to control them, and ac motors are preferred when the system must be easy to connect to the electrical grid (most common in the static devices). on the other hand, pneumatic systems which offer a faster response but cannot carry high force and need an air compressor, or hydraulic actuators which offer a high force but provide a rather slower system using fluids and need a compressor, were integrated in various systems."
"the sma-based actuators can be considered a good alternative to the conventional actuators in applications where lightweight, noiseless operation, low speed (without gearboxes), and relatively low cost is necessary. the analysis and different mechanical configuration presented in section 3 offer the possibility to choose the adequate configuration of the actuator based on sma, which can be better adapted to a specific application."
"in recent years, shape memory alloy (sma) materials have been considered a promising technology for the development of non-conventional actuators oriented on some specific applications. sma-based actuators present characteristics which make them suitable to be integrated in a large variety of applications: they have a high force-to-weight ratio and noiseless operation, present a low volume (smas can generate about 150 times higher force compared with hydraulic actuators and 400 times higher force compared with magnetic actuators, at the same volume), and are a relatively low-cost solution compared with another actuators. in recent years, smas have been used in a widespread variety of applications [cit], among them aerospace [cit], the automotive industry [cit], medical applications [cit], and robotics [cit] ."
"where i(z) is the pwm duty cycle, k p is the proportional gain, k d is the derivative gain and k i is the integral gain. e(z) is the error between the reference and the output. the gains of the bpid controller were experimentally set by changing the gain values and observing the actuator response (trial and error method). the gains of the bpid are shown in table 1 ."
"different configurations of sma-based actuators have been proposed, tested, and analyzed with the objective to determine the most effective configuration such an alternative solutions to the conventional actuators to be integrated in different applications."
"to test the gearbox, three sma wires with the diameter of 0.51 mm were mounted in each input pulley, and a nylon wire to the output pulley. the response of the actuator to a sine reference and the pwm control signal can be seen in the figure 16 . the actuator response with a gearbox 1:4 in the cooling stage presents a delayed response compared with other configurations. this delayed response is due to the cooling deformation force applied to the sma actuator, and the movement range of the sma wires (around the martensite zone). to avoid these inconveniences, the initial position can be recuperated with more weight in the cooling stage and for the last inconvenience a possible solution is to mount long-length wires, avoiding working in the martensite zone."
"the same test with the two configurations was realized with real actuators. the results can been seen in figure 11 where (a) presents the results of the two configurations (three sma wires in three different bowden cables and three sma wires in only one bowden cable, and (b) presents the pwm signals of the actuators. the two configurations were tested in a test bench capable of simulating the elbow joint of the human body for a person of 70 kg. the initial position of the elbow joint is 20 degrees, and the reference follows a sinusoidal pattern with a 30-degree amplitude and a 30-degree bias."
"the brake gear 1:6 offers the possibility to multiply the displacement of sma actuators six times, but it can be easily configured for different relations (less-with the actual components, higher-some components need to be fabricated). this gear presents one brake, which can be activated to maintain a fixed position or to control the initial shape recovering stage (after the sma wires are cooled) with low power-supply consumption. figure 17 presents the proposed actuator with the brake. in figure 17 the brake actuator with the output displacement multiplied 1:6 times is presented. in the left part is a front view of the actuator, where: 1-the terminal part where the smas wires are fixed, 2-the terminal part for the bowden tube (this part is mobile), 3-the brake, 4-a mobile axis which presents in the middle a hole from where the output cable passes, 5-is one sma wire which actuates the brake, 6-the axis displacement for the mobile part, 7-the output cable, and 8-the point where the output cable is fixed. the output cable gives the multiplied movement."
"according to the results presented in section 3, the sma-based actuators can be a good candidate to replace conventional actuators in certain applications. these present a good relation force/weight, noiseless operation, have a low frequency of work (no gears are necessary) and are relatively low cost compared with other conventional actuators."
"to calculate the exact power consumption, a current sensor can be placed in series with the actuator. the pwm signal is only trying to estimate the power consumption (the pwm signal is proportional to the current), but this estimation is not reliable due to the signal fluctuating. the pwm signals of the two actuators are compared in the figure 6 . the pwm signal area of the low-temperature wire is less than the area of the high-temperature wire, which implicate a lower energy consumption. this is largely due to the necessary temperature activation (lower) obtained with the joule effect."
"the material and the mechanical design is an important factor when a sma-based actuator is developed for a certain application. these characteristics influence the work frequency, electrical consumption, and the total movement length; for example a sma wire with high-temperature activation presents a faster work frequency but the power consumption is more elevated compared with a low-temperature activation sma actuator. furthermore, the mechanical design gives the possibility of flexibility to the actuator or to multiplicate the displacement (such as in the case of the double actuator and gear presented in section 3), which is a good option for applications such biomedicine, the automotive industry, robotics, and soft robotics, etc. on the other hand, the majority of smas based on niti alloy only contracts 4% of the total length, which offers a limited displacement. with the objective to increase this displacement and the total force, different mechanical designs can be addressed such as presented in section 3. some of the principal characteristics according to the results will be detailed further:"
3. it is expected that number of generated association rules would not be high (data set contains 256 instances) due to computation of confidence for each rule and selection of rules with the highest confidence.
"the usage of weka system, or a similar system, is of great help because it generates association rules (hypotheses) automatically. furthermore, weka system generates association rules for which the factor of confidence is high. this method can be used in any case of data, but weka system requires that attributes with numerical values do not participate in generating the association rule mining, so they are ignored. this is not a great handicap because linguistic terms are frequently used in queries and surveys. the final conclusion is: the usage of weka system in order to generated association rules automatically is of great help because the hypotheses of 942 d. radosav, e. brtka, v. brtka little importance are avoided."
"on the other hand, the weka gui chooser provides a starting point for launching wekas main gui applications and supporting tools. the weka system can be used to start the particular dm applications:"
rosetta system and weka are particularly suitable for data analysis in the field of education because they offer a selection of dm techniques and are relatively easy to use.
"the data sample, in form of ms excel document, consists of a total of 256 instances (students). it is important to mention that the data are not collected with the aim to be analyzed by dm techniques. the survey was conducted on students from the territory of the republic of serbia and the territory of bosnia and herzegovina. the computer technology that is used in this region is mostly out of date but is sufficient for elementary usage in education. description of the data, presented in table 1, shows the names of attributes and their possible values. attribute names and associated values comply with the form of survey."
"the rule 2 can be interpreted as follows. two facts: the fact that student uses home computer for learning and the fact that student likes the subjects of computer science, are associated with opinion that computer skills will be helpful in the future. rule number 3 associates the type of software that is most frequently used by student (games, educational software) with the opinion that computer skills will be helpful in the future."
"the paper is organized as follows: section two gives the short description of various dm techniques used in the domain of education. one of dm techniques is chosen to be used for the analysis of the real-life data. it is explained why this dm technique is the most suitable in this particular case. section three contains data description, as well as the methodology description. section four lists the results obtained by application of the dm technique, as well as the interpretation of these results. finally, section five contains conclusions and remarks about applicability of dm techniques in the domain of education."
1. collection of the data. the cms system is used and the collected data are stored in database. this step can be executed by a questionnaire or some other data collection technique instead of cms usage.
"3. application of suitable dm technique. the dm technique is applied to build the model that discovers new rules, patterns and knowledge. to execute this step, either a general or a specific data mining tool, or a commercial or a free data mining tool can be used."
the previous rule x ⇒ y is interpreted as follows: the student who has a computer at home and uses the internet is associated with the opinion that his/her computer skills will help him/her in the future.
"various techniques can be used on this small data set: first of all, there are statistical techniques, for example students distribution that is used when estimating the mean of a normally distributed population when the data set is small; then there are techniques for inferring decision rules (based on pawlaks rough sets theory, decision trees, etc.), even neural networks can be trained on small data set [cit] . in addition, the data sample which is described in table 1 can be analyzed by various dm techniques or dm systems. the association rule mining is adopted as the most suitable dm technique."
4. it is expected that association rules have a great value when inferred from data set in education domain because association rules can be treated as a hypothesis.
"the opinion that computer skills will be helpful in the future is mostly associated with: believing that student gained enough knowledge to work independently on a computer, the usage of the computer for learning and aspiration to use distant learning system at home to review learning materials previously used in school. attribute a14 (the student likes the subject of computer science) is most frequently associated with opinion that computer skills will be helpful in the future."
"the previous concepts are explained in next simple example. for a data set given in table 2 it is possible to infer some association rules, as well as the confidence and support parameters."
"the analysis of generated association rules provides insight into the dependence of the monitored parameters. each rule is accompanied by a factor of confidence that takes a value in the range [cit] . ten association rules have been generated, see table 3 ."
rosetta system allows data to be loaded from ms excel table; the format of the loaded data can also be csv (comma separated values). rosetta system performs the extraction of the if...then rules. the data can be collected by various methods; the format of the collected data does not have to be specially adapted to dm techniques implemented in rosetta.
"however, this paper lists the basic dm techniques and some tools that allow the application of these techniques in domain of education, but cms and their application is not the topic of this paper, although the importance of cms is evident. this paper deals with some special dm techniques when applied to data collected in the domain of education. the application of dm techniques results in some rules or patterns that can be used as feedback to cms. rather than investigation of the connection between cms and dm system, this paper deals with dm techniques when applied to data in the domain of education and gives some conclusions and remarks about the importance of inferred knowledge. special contribution is the analysis of the results of dm technique when applied to real-life data collected from the region of serbia and bosnia and herzegovina."
"the application of association rule mining allows automatic generation of hypotheses and factors of confidence that are related to them. considering rule number one (see table 3 ) the hypothesis is: if a student wants more educational computer software to be used in school in order to improve learning then the student thinks that his/her computer skills will help him/her in the future. other rules may also be interpreted as a hypothesis. obviously the factor of confidence has a great impact on confirmation of hypotheses. at the technical faculty \"mihajlo pupin\" in zrenjanin, serbia, extensive research is underway, investigating the possibilities of applying dm techniques to data from the domain of education, extracted from a survey conducted in the wider balkan region. the fact that it is not obligatory that the data are collected with the aim to be analyzed by dm techniques, offers an excellent chance to assess real possibilities in the actual practice. in future, this leads to identification of the advantages of dm techniques over standard statistical techniques. so far, there have been identified the following advantages of application of the dm techniques to data from the domain of education:"
"however, rule number nine associate the students that are not satisfied with the usage of e-mail with the opinion that computer skills will be helpful in the future. in fact, this may be in accordance with rule number eight: the usage of distant learning system at home to review learning materials previously used in school is associated with the opinion that computer skills will be helpful in the future. so, in students opinion, distant learning system makes e-mail service obsolete in a way."
"efforts that have been invested in the integration of cms and data mining (dm) systems are evident. this integration often means adding dm modules to the existing cms [cit], but it is possible to approach to cms and dm system integration through serial connection [cit] . serial connection, in this case, means collecting data with cms, and then processing collected data by dm system. the results of dm analysis are fed back to cms in order to improve their effectiveness."
"the experiment was conducted on data set by software system weka in order to generate association rules. after loading, the data are ready for pre-processing and application of dm techniques. weka system requires that the attributes with numerical values do not participate in the association rule mining, so they are ignored. association rules generated by weka system (apriori algorithm is used) are shown in table 3 . there are 10 rules generated. the if part of every rule is followed by support measure, as is the case with the then part of each rule. the confidence for each rule is given in the separate column."
"in the past few years e-learning techniques have significantly improved as the result of progress and increased use of the internet. the \"desktop\" e-learning systems, in many cases, have been replaced by systems that operate using the internet. some of the web-based systems allow the determination of preferences for each participant in the process and adjustment of activities in accordance with the profile of participants [cit] . recently, techniques from the domain of data mining have been incorporated into systems for e-learning. the changes that are constantly taking place in terms of rapid technical and technological developments affect society as a whole. the educational system is experiencing changes in terms of modernization and globalization. however, the educational system that is \"inert\" is not suitable for rapid change and modernization. the educational processes in serbia and some other countries in the region of western balkans are changing so that the \"reproduction\" style of learning is replaced with the style that prefers \"understanding\" of the learning content and usage of the acquired knowledge. the theories of learning that are used are no longer associative and behavioral, but have become constructive and cognitive. students are required to improve the style of self-training and their skills. in these processes the information capacity is an important factor in development, especially the internet and the resources that are available through world wide web. the usage of the internet by some course management system (cms) is not unusual occurrence in serbia, but cannot be said that such systems are widely present."
"in particular, it is necessary to apply and elaborate in detail each of these steps depending on the data to be analyzed. some of the systems for data mining that are used to analyze data from different domains are:"
"crest, loughborough university the adoption of information and communication technology (ict) based centralized volt-var control (vvc) leads to an optimal operation of a distribution feeder. however, it also poses a challenge that an adversary can tamper with the metered data and thus can render the vvc action ineffective. distribution system state estimation (dsse) acts as a backbone of centralized vvc. distributed energy resources (der) injection measurements constitute leverage measurements from a dsse point of view. this paper proposes two solutions as a volt var optimization-distribution system state estimation (vvo-dsse) malicious attack mitigating strategy when the der injection measurements are compromised. the first solution is based on local voltage regulation controller setpoints. the other solution effectively employs historical data or forecast information. the concept is based on a cumulant based probabilistic optimal power flow with the objective of minimizing the expectation of total power losses. the effectiveness of the approach is performed on the 95-bus uk generic distribution system (ukgds) and validated against monte carlo simulations."
"by association rule 1 attribute a11 (does the student want more educational computer software to be used in school in order to improve learning) is associated with attribute a17 (does the student think that his/her computer skills will help him/her in the future). the factor of confidence for this rule is 0.97, that rule is guaranteed to a great extent. if the value of attribute a11 is \"yes\" then, by this association rule, the value of attribute a17 is also \"yes\". a possible conclusion which can be drawn is that most of the students think that usage of computer technology in school generates skills and knowledge that could be used in the future. this students opinion is a good indicator of the importance of the usage of computer technology and educational software in teaching process. it is evident that technology usage in future is closely related to computer technology software and methods which are used at present. other association rules can be interpreted in analogous way."
"there are several methods to approach multicriteria optimization problems such as simulated annealing and evolutionary algorithms; here data envelopment analysis (dea) [cit] ) is used. dea has been previously used in our research group to solve multicriteria optimization problems in manufacturing (cabrera-ríos, [cit] ) . the main motivations for using dea to solve the multiple criteria optimization problems are: (1) dea uses linear programming, which is the simplest optimization problem, (2) dea can be carried out using easily available software, like ms excel, and (3) once an efficient solution is identified by dea, one can be sure that it is indeed an efficient solution."
"we have experimental results from the two aforementioned areas. echo park lake (fig. 2) is a small man made lake in los angeles, ca. apart from the shoreline, the lakes obstacles include floating islands and a small dock. the second area is king harbor (fig. 3), a marina in redondo beach, ca. in the marina we have floating docks, docked and moving boats and buoys. the two experiments were done independently and no data was shared between them. the data processing and obstacle map generation for these experiments was done offline after the data gathering."
"in this session, the semantic role labeling of document is conducted by using framenet, the measurement method of semantic similarity of document by comparing the semantic role information in the sentence is described. the document similarity measurement process using semantic role labeling is composed of pre-process, semantic role labeling process, and semantic similarity measurement process. fig. 2 is a structure for document similarity measurement method by using framenet."
"the experimental platform used is the autonomous surface vehicle (asv) designed by the university of southern california's robotic embedded systems laboratory (fig. 1) . the asv is an ocean science qboat-i hull, 2.1 m long and 0.7 m wide at the widest section. the asv is actuated by two electrical motors and a rudder and is capable of speeds up to 1.6 m/s. the asv has an on-board computer, gps, an imu and a compass. the asv is controlled by software built using the open-source framework robot operating system (ros)."
"as previously stated, we have split the area of interest into 1 x 1 m sites. from overhead imagery, we extract features for each of these sites. we want the features to capture some statistics about the color, texture and structure of the site. based on literature in texture classification and classification of aerial imagery [cit], we have chosen the following set of features: image pixel values in two color spaces (hsv and cielab), gabor energy features and entropy within sites."
"for the trials described in this paper, the asv was equipped with an imagenex 881l profiling sonar mounted facing forward scanning in a plane parallel to the water surface. the 881l is a single-beam mechanically scanned multi-frequency sonar with a full scale range from 1 m to 100 m. along with the sonar measurements, gps position and compass heading was also recorded."
"the document similarity measurement is to measure similarity of documents by comparing the document as a test object and the base document while it is generally conducted in a way of selecting a candidate sentence from sets of documents to compare by using guide words in sentences, and measuring the similarity of documents by similarity measurement between the two sentences. the proposed method is to tag words in sentences with the relevant semantic role and to focus on the similarity measurement between the semantic role of two sentences while arguments of similar semantic in sentences with similar contents can appear in semantic role since argument with semantic relationship from predicate are distributed in sentences. comparing similarity based on the semantic role in the sentence can reduce time and cost to be consumed for similarity measurement of sentences, and it has a merit to detect paraphrasing type which includes the same semantic though the sentence is altered. the process for tagging semantic role in this study used sem afor, open source api developed for semantic analysis of documents. the result of semantic role labeling conducted by the sem afor system creates json file being tagged with frame-argument information. the table 1 and table 2 intuitively illustrates the semantic role information of sentences. since the two sentences of the table 1 and table 2 have the same semantic yet paraphrased in different form, we can identify that they have the common semantic role information. the similarity of the two sentences is calculated by greedy string tiling similarity, [cit] which is a comparison method utilizing substring. the call of, \"pig away!\" and the dash of bairn in the pursuit, at last make such a soprano that both attend grey and the much-try andrew make disorder to the vicar. in this method, the number of semantic role labeling is defined as characteristic information instead of the number of substring while the similarity of the two sentences is calculated based on the ratio of characteristic information found commonly between the two sentences. the similarity is calculated as the formula 1."
"the semantic role labeling analyzes the sentence components as predicate-argument structure based on sentence structure analysis, and determines and tags the semantic role in a sentence for each component of sentence [cit] . the semantic role labeling identifies argument information required for completing a sentence by predicates centering of natural language sentences, and determines the relationships between predicates and corresponding arguments. by mapping the semantic role arguments with acting agent, experiencing agent, object, and etc., semantic role labeling can be conducted [cit] . the predicate-argument structure of sentence is an important component to represent the semantic of sentence while predicate of particular semantic needs essential argument information. this will result in using common predicate-argument information for sentences of similar semantic. the conduct of semantic role labeling can be divided into the following 4 stages: pi(predicate identification) stage, pc(predicate classification) stage, ai(argument identification) stage, and ac(argument classification) stage [cit] . firstly the pi stage, where predicate is identified through sentence structure analysis on input sentence, and the pc stage, where predicate is ambiguity of predicate can be resolved, are conducted, then the ai stage, where information required for making a sentence with predicates with particular semantic in the sentence is identified, and the ac stage, where semantic role of acknowledged argument is determined, are conducted [cit] . in this study, the semantic role labeling is utilized to improve detection performance on similar sentences being much altered by paraphrasing sentences, which are difficult to detect with conventional similarity measurement methods."
"2) probability computation: the effective noise variance ofñ k in (4) depends on the amplitude ratio used at time instant k, which can be computed as:"
"the field of 3d face recognition (3dfr) is quite new but advancing quite rapidly. at the algorithmic level, the techniques vary depending on the modes of model representation (or registration) [cit], feature extraction [cit] and matching [cit] . a good set of survey papers [cit] provide varied systems on generic 3dfr. these cover a range of techniques starting from imaging, representation, matching, both grey scale as well as colour images. feature extraction has recently gained a lot of prominence as it dictates the performance of a recognition system. in this section, we consider a review of current techniques that is related to 3d facial feature extraction."
"in our previous work [cit] we explored the feasibility of using a forward mounted profiling sonar on an asv for obstacle detection. we investigated the effective range and ability of this setup and found it to be a useful option for obstacle detection for asvs. during this previous research, we have collected large amounts of forward facing profiling sonar data (with gps coordinates and compass heading for each measurement). we collected two sets of data, one in echo park lake, los angeles and the other in king harbor, redondo beach, both of which are utilized in this paper."
"as in all transceivers, the soft-decision based m-dapsk (ma,mp) block is placed before the tc decoder, as shown in fig. 1 . the kth received symbol may then be written as:"
gabor filters are a family of filters frequently used for edge detection [cit] and texture classification [cit] . the real component of the gabor filter is formulated as follows:
"the classification process was run on images from google maps and bing maps and the results were then smoothed individually and finally combined as described above. two image sources are used in order to get the added dimension of time, but because the two image sources are quite different in terms of color and illumination we actually train a classifier independently for each of them instead of using the same one for both as was suggested in the previous section. we have calculated the error rates of the classification by comparing the results to a manually labeled images."
"to further illustrate the application of the proposed methodology, we analyze the molding of a disposable camera. two scenarios are discussed, the first one involves two performance measures and the second one, three. in both cases we ran the optimization method using two initial does, a ccd and a lhd. two controllable variables are used in all cases. here, it is desirable that the molded part preserves the designed symmetry and dimensions. the part to be analyzed is the front plate of the camera shown in figure 13 ."
"in optimization problems (4) and (5), the values of shrinkage (δa 3 ) and (δb 3 ) are estimated via moldflow tm . the constraints represent the experimental region of the controllable variables: mold temperature and packing time. in order to optimize problem (5) we ran the optimization algorithm with the same does as in problem (4) (see figures 15 and 16 as reference). figure 22 shows the evaluation of the pms at each design point. figures 23a and 23b show the incumbent efficient frontier at each iteration of the algorithm, when a ccd and a lhd were used. figure 24 shows the final efficient solutions of both cases. table 6 gives the values of the final efficient solutions. once again the final efficient solutions when using a ccd outperformed the ones obtained when using an lhd. regarding the number of simulations, the algorithm required 33 runs when started with the ccd and 37 runs with the lhd. in this case each simulation run takes 70 seconds + setup time. simulations of bigger and more complex parts can take hours or days to evaluate a single run. from the set of efficient solutions the decision maker can select the best alternative depending on the particular application."
the asv was driven manually around the two areas to collect sonar measurements. the sonar data was processed as described above and a small part of it selected for use as labels for the classification process. figure 4 shows the labels generated by the sonar overlaid on top of aerial images of the areas.
x partitioning the database into sub-databases and performing matching to reduce eer and increase m1 (score). this is particularly a useful criterion in handling large databases.
"to combine the results from the two classifications we simply take the intersection of the obstacles from both predicted maps and label those sites as obstacles. the difference between the two maps will be labeled as transient/suspect obstacle and finally the intersection of the non-obstacles will be labeled as non-obstacles. if we assume the labels to be non-obstacle, l n, obstacle, l o, and transient/suspect, l t, the resulting label would be:"
x providing generalization results whereby the data is partitioned into learning and testing and are mutually exclusive. this performance evaluation will determine the ability to work with unseen data. x providing individual model analysis for the student db.
"x the average face model performs better than the individual models in respect of m1(score) but at the compromise of eer. x higher order signature combinations as features are useful in reducing the eer in general and increasing the matching score. x not all features are useful especially d135 signature which acts as an outlier to the roc. x uni-modal signatures, especially the vertical signatures perform very well across the board of testing."
"with frgc dataset, both average and individual face models are derived and tested separately. the average models were built based on varying sample sizes. the individual models are useful when there are insufficient samples for the subjects as in the case of the frvt dataset where the number of samples/subject is one for some part of the database (table i ). the within-class distance is larger in the former case compared to the latter as it is a fuzzy representation encompassing the average information from all of the samples of a subject. therefore, with the average model representation, it is not expected to produce a 100% match score between the query and the target images even for validation tests. however, this does not imply that it is a poor representation as it allows an implicit modelling of imprecision within the dataset."
the rest of the paper is organized as follows: section ii provides a literature review of 3dfr systems from a feature extraction perspective. section iii details the proposed system. section iv describes the experimental setup and reports on various performance measures and relative performances of the angular features extracted. section v provides a conclusion and recommendations for further work.
"for future work we plan to address the multiple label problem and incorporate repeated sensor measurements (and a measure of confidence in the measurements) into the estimation process. furthermore, we plan to frame the problem in a probabilistic framework. we also plan to address problems with occlusions such as bridges and trees. in this work, we have made training data available to the classification process based on location, i.e. we have made the training dataset consist of all labels in a certain region. we plan to make the data sequentially available to the classifier and look at the progression of the classification results. finally, we plan to formulate this as an active classification problem and plan a path for the asv with the objective of improving the classifier."
"in this paper, the author chooses to determine the 3d profile called signatures along several possible angles of planar intersections with the 3d image to accommodate such illumination and small 2d pose variations. for a chosen angle, a set of 3d signatures are derived along the y-axis at fixed sampling points. variations in angles include 0º, 45º, 90º, 135º and their combinations of 0º+45º, 135º+45º, 135º+45º+0º, 90º+45º, 90º+135º, 90º+135º+0º, 90º+135º+45º, 135º+0º. from these signatures, statistical moments are determined as feature sets. performance evaluations through roc for the above modalities have been carried."
"the use of simulation for selecting injection molding processing conditions has been the subject of much research in the past [cit] . specialists usually generate a limited number of solutions from which one is finally selected. nevertheless, this does not guarantee having found the optimal solution. therefore, there is a lot of potential to be exploited in the adequate and efficient selection of optimization techniques for the design of manufacturing processes through computer simulations. such potential explains the relatively recent and rapidly increasing interest in simulation optimization (so) or optimization via simulation (ovs) as a field on its own. the objective of a so method is to provide a structure to determine the values of the controllable variables that op- villarreal-marroquín, cabrera-ríos, and castro timize an objective function defined as a combination of the simulation model's outputs (performance measures) [cit] ). an optimization routine uses the calculated values of the objective function along with previous evaluations to select a new set of input values; this is continued until a preselected convergence criterion is satisfied [cit] ."
"in this section, experimental results based on rocs for rank vs cumulative match are determined. using the notations defined in setions ia and ib, performance evaluation is carried out:"
the problem setup is a conventional mrf image denoising problem where the observations come from our classifier but instead of having homogeneous edge weights we generate the weights based on the similarity of the features of the neighboring sites. we set this up as an energy minimization problem [cit] where we assign energy to the edges and nodes. the total energy to minimize is:
"the proposed accuracy rate and recall factor are calculated by the formula 2 and 3. the 's' is the range of plagiarism which is 'correct group' to assess the proposed method while 'r' is the range of plagiarism found by the proposed method. the 'r' is the number of common semantic role information from the two documents extracted by the proposed method whereas the 's' is the number of semantic role within the plagiarism range, 'r∩s' is calculated by the number of semantic role within the plagiarism range among the number of common semantic role information from the two documents. the table 6 illustrates accuracy rate and recall factor measured on the similarity between documents based on the tagged semantic role labeling information through the extended framenet."
"we perform the classification process on the aerial images captured at different time instances. the added dimension of time allows us to address non-static obstacles as well as help with addressing noise such as shadows cast from trees and structures that might be predicted to be obstacles by our classifier. our resulting obstacle map will therefore have three classes for each cell: obstacle, transient/suspect obstacle and non-obstacle."
"as the volume of information is exponentially increased along with the fact that the various information including newspaper articles, books, and academic papers has been digitalized and existed online leading to easy and convenient access to the information, the social problems such as unauthorized use and plagiarism has been emerged at the same time due to indiscreet sharing [cit] . plagiaris m in this era of information has adversely affect desirable distribution and utilization, thus the researches on measurement of document similarity to more efficiently detect plagiarism. the measurement methods of document similarity can be classified into 2 categories, for instance, comparing morphological similarity and comparing semantic similarity. for comparing morphological similarity, some of the representative methods include n-gram method, in which extracting and comparing adjacent 'n' number of words in a sentence, comparing substring in a sentence, and vsm (vector space m odel), in which similarity is determined by measuring the distance between vectors after putting a document in a vector space [cit] . however in this morphological similarity comparison, it has a drawback that similarity measurement does not consider for the case when the original document has been altered such as substituting, paraphrasing, and rewriting words since it only compares words in two subject documents [cit] . the semantic similarity measurement method was proposed to improve this kind of issues. the semantic similarity measurement method is a method using knowledge base and thesaurus, in which semantic relationships among words are defined in hierarchy information. it may be useful for only measuring similarity of words in a sentence considering semantic similarity with substituting or changing to similar words, but it cannot comprehend the structural information in a sentence where the subject words are at. therefore, it does have limitation on detecting types of plagiarism which evolves in various forms including paraphrasing sentences [cit] . m ost people think that copying the expression from other writings is plagiarism but paraphrasing, which borrows and alters the expression, is not. despite the fact that expression is borrowed and changed, it would be plagiarism if the entire message and structure is the same. this study is to propose the methods of measuring similarity between permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for third-party components of this work must be honored. documents utilizing srl(semantic role labeling), which is one of the semantic analysis methods based on structural information of sentence in order to measure similarity of documents similar in semantic being composed of much altered from the original such as paraphrasing sentences."
"we gratefully acknowledge help with deployments and hardware provided by jnaneshwar das, carl oberg, arvind pereira, ryan smith. this work was supported in part by the onr antidote muri project (grant n00014-09-1-1031), the noaa merhab program (grant na05nos4781228), nsf as part of the center for embedded network sensing (cens) (grant ccr-0120778), and nsf grant cns-1035866."
"this study proposed the semantic similarity measurement method through structure analysis of sentences using semantic role labeling whereas the semantic role labeling is the method of document analysis to determine the role of argument information which is required for completing a sentence being composed of predicate centering on the predicate of sentence. the semantic role labeling information acquired through this process as a characteristic of document is used for the similarity measurement. it is a method with especially not only semantic of words but also structure of document is considered leading to identify more improved performance than the conventional similarity measurement method on paraphrased documents. as a future study, we plan to research further on similarity measurement methods being applied by expansion measures being able to enrich semantic role information of existing framenet."
frvt data consisting of 275 subjects with varying sample sizes leading to a total of 943 images. the database consists only of frontal images. the images vary in illumination and scaling.
"research in 3d face recognition systems is becoming increasingly popular due to the development of more affordable 3d image acquisition systems and the availability of 3d face databases. such systems have made quite a progress in solving problems of localisation, pose and illumination variances. however, these problems still continue to exist. with security applications such as border crossing, it is difficult to acquire idealistic images without being constrained and intrusive at capture points."
"in the classification process we ignored the spatial relationship between neighboring sites. in environments we are interested in, a certain class almost always forms sizable continuous patches on the map, i.e. there are rarely spots of the size of a few sites. when we get such small spots we would like the feature-set to give a strong support for that labeling. spots with low support that are still misclassified, due to insufficient training data set coverage, artifacts introduced by image stitching and image noise, are something we want to remove. in order to do that we will incorporate the classification results along with the feature set in to a markov random field (mrf) framework. mrfs are one of the tools of the trade in computer vision and allow one to represent spatial dependencies between sites/pixels."
"contributions in the area of ovs have primarily focused on discrete-event simulations and not on continuous or physics-based simulations like the ones used to model polymer processes. one reason is that the computational time required to run physics-based simulation models is usually very long. consequently, having an iterative algorithm that requires many simulation runs to find the processing conditions to optimize different performance measures simultaneously is not computationally practical. as a result, metamodeling-based techniques [cit] have played an important role on the analysis of physics-based simulation models. metamodels are mathematical representations of real phenomena based on a limited number of measurements. these measurements can take the form of the outputs of a simulation model, overcoming the need to run the simulation many times. the evaluation time of a metamodel is considered to be much shorter than that of the simulation software. in such cases, new solutions can be estimated rather inexpensively."
a student database captured from a stereo vision systems [cit] consisting of the 100 students as subjects with 10 canonical views per subject (fixed sample sizes) under a controlled illumination environment. small variations in pose were allowed. the canonical views span 180º and therefore an approximate 18º separation between two consecutive samples.
"for this paper, the student database has two datasets based on the camera lens used namely 7.5 mm and 12.5 mm. each partition contains 100 subjects with 10 canonical views per subject with a total of 1000 images in each dataset. the frvt database consists of frontal images of 275 subjects with varying samples/subject as shown by the frequency distribution in table i . the size of the database is 943. both the student and frvt databases were manually cropped and resized to an image size of 128x128 pixels. the student db was acquired in an illumination controlled environment; hence did not require further normalization. the frvt database required illumination normalisation using the standard histogram equalisation technique available in matlab. thus the dbs were normalised with respect to scaling and illumination (fig.4) . the rest of the steps are common for both databases."
"1) echo park lake: the resulting obstacle maps are displayed in fig. 5 . one can see that the lake shore is correctly classified as obstacle as well as the floating islands and the fountain. there is some confusion as to if trees extending over the lake and their shadows are obstacles, but the combined classification in fig. 6 improves on that somewhat. in this figure it can clearly seen how this allows us to mark the shadows cast by trees at the south side of the lake as transient/suspect obstacles and not as obstacles as they were originally classified by one of the classifiers. one thing to notice is the bridge at the north end of the lake that is classified as an obstacle. even if there was labeling support at that location, our method would still be hard pressed to classify the bridge as not being an obstacle. the classification error rates are displayed in table i."
"polymers have been increasingly replacing metallic components in many applications such as the manufacture of automobiles, aircrafts, toys, appliances, office equipment, among others. this is because they are very versatile materials. nowadays, many consumer products such as computer and automobile components rely on the technology and production of polymer companies. thus, it is important to design reliable processes to ensure low cost and high quality products."
"the approach consists of four parts. first the collection and processing of the sonar data, next the processing and feature extraction of the overhead imagery, then combining the two in a binary classification framework and predicting the remainder of the map and finally smoothing and combining the results from the earlier classifications and generating a map with three classes: obstacle, transient/suspect obstacle, non-obstacle. a path planner working with the resulting map would plan paths through the non-obstacle space while avoiding obstacles. the planner would treat the transient obstacles as something it might actually be able to traverse through, but with caution and have some reactive sensing/avoidance handle the obstacles if needed."
"in injection molding (im), for instance, processing conditions such as melt temperature, mold temperature, pack/hold pressure and duration, and cooling time have to be properly set to ensure the quality of the molded components. often, these conditions are set by process engineers based on prior experience, resin supplier's recommendations, and/or reference handbooks. these conditions are usually further adjusted by trial and error on the shop floor. this approach is highly dependent on the experience of molding operators and can be costly and time consuming, especially with new resins and/or new applications [cit] . however, with recent advances in numerical modeling and computer simulation techniques, a large effort has been made in developing computer simulation tools to help improve and facilitate the modeling of plastic parts."
in a previous work [cit] ) our group introduced a single objective optimization via simulation method whose objective was to find the best process conditions using a small number of simulation runs. the algorithm was tested using different global optimization test functions with satisfactory results. it was also tested using several small discrete event simulations as well as continuous simulation models.
"in 3d profile generation techniques, only one angle of planar intersection with the 3d image is typically considered. such techniques deal with variations in pose by normalising the image to a standard pose as a preprocessing step. additionally, automatic illumination normalisation techniques do not reach an optimal performance with uniformity across a database. this has been experimented on the frvt database on 4 different illumination normalisation techniques namely global equalization, parabola equalization, double equalization & wavelet-based equalization [cit] as indicated by legends g,p,d and w respectively in fig.1 showing part results of 65 images. the double equalisation algorithm is the only one that had managed to achieve an sse of zero at some points, but not across the board. there is a residue despite normalisation. face recognition technologies have to cope and perform under such noisy environments."
"the optimization process was repeated starting with a space filling latin hypercube design. the lhd consists of 9 points (same number of points that the ccd) which are shown in figure 9 . figure 10 shows the values of the performance measures (f 1 and f 2 ) at each design point. the optimization proceeded as in the previous case (section 3.1). figure 11 shows the incumbent efficient frontier at each iteration of the method. the method stopped at iteration 3 because the r 2 of both metamodels was 100%. the final efficient solutions are shown in table 4 . a comparison of the real efficient solutions and the efficient solutions found by the optimization algorithm on both cases is given in figure 12, the actual values are given in tables 2 to 4. as we can see when we run the optimization algorithm using a ccd as initial doe, the method was able to find almost all the real efficient solutions (3 are equal and one is very close). on the other hand, when using a lhd the method identified closely only one solution. therefore, the initial doe plays an important role on the course of the optimization. regarding the total number of simulations required by the method, both cases used the same number of simulations, 18. while space filling designs, such as the lhd used here, are commonly used on the analysis of deterministic computer experiments [cit], we believe ccds do also a good job in combination with regression models for the optimization of deterministic simulation models. in the future we will analyze the performance of both does in combination with gaussian models. the goal is to have a precise idea of which combination of experimental design and metamodel works best to identify the efficient solutions of multicriteria injection molding problems with the minimum number of simulation runs."
"it is important to note that the sonar measurements will in great likelihood produce inconsistent labels since each site is usually scanned multiple times by the sonar; the first scan could be from a distance, over which a particular obstacle is not visible, and then, once closer, another scan is performed in which the obstacle appears. we use a simple, and perhaps naive, strategy for resolving these labeling inconsistencies: detected obstacle always takes precedence over non-obstacle, i.e. once we've detected an obstacle in a site, we mark that site having an obstacle, no matter if we get a non-obstacle measurement later."
"it's the aim of this paper to evaluate the performance of the system by using these directional signatures as features as a function of the model representations. then, the dimensionality of the feature set is given by x* y * ∂ * ∆ * μ * θ the dimension of the feature sets for the above model representations for a directional signature along x or y axis is given in table ii . (∂, ∆) pair are constants for directional signatures along (0º, 90º) respectively. however, in the directions of 45º and 135º, the value of ∆, i.e. the length of the signature along the diagonals will be larger compared to x and y axes. further, θ, the number of directional combinations proportionally increase the feature set dimension."
"in this section, a 3dfr system architecture using facial directional signatures is outlined. the generic block diagram for the system is shown in fig.4 ."
"in this session, the semantic role labeling on experimental data for similarity measurement is conducted by using the framenet. for the experiment, the proposed method and the conventional similarity measurement method are applied on 500 [cit] corpus (simulated_paraphrase category). the table 3 is the result of measurement of similarity by substring comparison method on the experimental data set whereas the table 4 is the result of the result of measurement of similarity on the experimental data set by the semantic role labeling information."
"our group is in the process of comparing the results with actual experiments as well as investigating the use of different metamodeling techniques. in addition, we are evaluating how to map the efficient frontier into process windows, which can be used by engineers to better interpret the efficient solutions."
"where p is the set of all sites within the image, n is the set of all pairs of interacting sites (i.e. all pairs of sites that are in each other's neighborhood), d i (·) is the data penalty function and v i,j (·) is the interaction potential between neighboring sites and c ′ i is the labeling for site i. the site neighborhood size was chosen after testing to be 4 (8-neighborhood did not deliver substantial improvements)."
"the symbol-to-bit soft-demapper probability formulas of the 64-dapsk (4,16) scheme were derived and its softdecision aided performance was investigated in the context of a tc scheme. the 64-dapsk-tc scheme outperforms the identical-throughput 64-dpsk-tc scheme by about 4.2 db at a ber of 10 −5, when communicating over correlated rayleigh fading channels having a normalised doppler frequency of 0.01. finally, the achievable throughput characteristics of the m-dapsk (4,mp) modem-family were presented. our future research will improve the resilience of these schemes against high doppler frequencies with the aid of multiplesymbol differential detection (msdd) [cit] . we will also conceive low-complexity msdd schemes with the aid of sphere decoding [cit] ."
"with the db images, signatures were derived at the intersections of facial surface with evenly spaced vertical planes. the signatures act as profile curves at sample points along the y-axis (90º) of the image. for convenience, a fixed set of 40 signatures is derived for each image. similarly, other directional signatures are also derived, as shown in (fig.4-5) . the 3d signatures appear as a compressed image due the effect of sampling in 3d. sampling takes place at points of intersection of a stack of planar surfaces oriented in a particular angle with the images."
"because none of the stopping criteria were met, the simulated solutions (squared points in figure 6 ) are added to the set of available points to build a new metamodel for each pm, and the main iteration is repeated."
"this manuscript presents a multicriteria optimization via simulation method which integrates design of experiments and metamodeling techniques to reduce the number of simulation runs needed to solve the multicriteria problem. the method is first introduced in section 2 and illustrated with a simple test case in section 3. finally, in section 4, it is applied to a real injection molding case."
"common so methodologies are mainly characterized by the use of one objective function representing a performance measure (pm), as the one our group presented earlier at the winter simulation conference [cit] . however, in injection molding, we are typically interested in optimizing a set of performance measures as opposed to a single one. and, as previously mentioned, the controllable variables in general have conflicting effects on the performance measures. therefore the best solution for one performance measure is usually not the best for some other performance measure. thus it is not the best approach to obtain a single solution but rather a set of solutions corresponding to the best compromises (efficient solutions). the efficient solutions are the solutions for which none of the performance measures can be improved without deteriorating another. this last task falls into the emerging area of optimization via simulation with multiple performance measures."
the research leading to these results has received funding from the european union's seventh framework programme ([fp7/2007 ([fp7/ -2013 ) under grant agreement no [cit] . the financial support of the epsrc uk in the framework of the iu-atc and the china-uk project in 4g wireless communications is also gratefully acknowledged.
"this paper is organised as follows. in section ii, the softdecision demodulation of m-dapsk (ma,mp) aided tc will be presented. our results will be discussed in section iii and our conclusions are offered in section iv."
"in this work, a 3d face recognition system with higher order signatures was evaluated with two databases and two model configurations the following are noted:"
we discretize the map to a grid with 1 x 1 m sites. the features for each cell are generated from the collection of pixels within each site and the labeling and classification is done in this 1 x 1 m grid.
"the classification process utilizes the real adaboost boosting algorithm [cit], a generalization of the basic adaboost algorithm by freund and schapire [cit] . other classifiers do work as well, but we have chosen adaboost for its simplicity, performance and quick run times. there have also been developed online variants of the algorithm [cit], in which we are interested for the further development of this work."
"the sentence segmentation, stopword removal, and lemmatization are conducted in the pre-process. the sentence segmentation is a process to segment a document into single sentences to conduct morpheme analysis or sentence structure analysis as a pre-process. the proposed method in this study is to conduct the sentence segmentation process segmenting the input document into sentence units since it conducts semantic role labeling based on the sentence structure analysis for similarity measurement of documents. in this study for stopword removal, it is intended to increase the processing speed of the system by utilizing pre-defined stopwords and removing all stopwords in the text. the lemmatization is a process to find lemma for altered words of sentences in various forms. the lemmatization is appraised as an efficient method to resolve the problem with the fact that the same semantic words in an altered form are identified as different information in the document analysis [cit] . in this study, we intend to improve the performance of sentence structure analysis by lemmatization."
"the resulting obstacle maps are displayed in fig. 7 . the two maps agree for the most part. we can see that all the docks are correctly classified as obstacles and the boats in the as well. the two maps were then combined into one in fig. 8 . here we can clearly see the benefit of using multiple images as this environment is packed with dynamic obstacles, such as the moving boat coming in to the harbor, two kayaks and of course the different boats parked in the berths. the classification error rates are displayed in table i. in this paper we have developed a method for obstacle detection from an overhead image using labels generated from a forward looking sonar attached to an asv. the results indicate that this is a viable way to generate an obstacle map on the fly for use in path planning or velocity planning."
"the proposed 3dfr system deals with databases namely the student database (db 1 ) and frvt v1 database (db 2 ), samples of which are shown in fig.3 . the student db is a disparity map derived from a stereo-vision pair of left and right images. the shape channels were used in case of frvt dataset [cit] ."
"this paper introduced an optimization via simulation methodology for multicriteria problems. the methodology combines design of experiments and metamodeling techniques to reduce the number of simulation evaluations. this makes it attractive for cases where simulations take long time to run, like the ones used to analyze the injection molding process. an example using global optimization test functions was used to illustrate the methodology. to observe how the final efficient frontier is affected by the selection of the initial design of experiments, two different initial does were evaluated, a ccd and a lhd. an application to injection molding is also presented. two variations of this case were presented; one involving 2 pms and a one with 3 pm. two different initial does were also applied in these cases. in all the cases dea was used to find the efficient solutions."
"the sonar data processing is a modified version of our previous work [cit] . a sonar measurement consists of a vector where each element represents echo strength measured from a certain distance. the vector elements are spread equally over the set range of the sonar, which can be set from 1-100 m. in our experience, the effective range when mounted forward is 30-40 m. the sonar data processing extracts potential obstacles from the measurements. the sonar processing pipeline is capable of extracting multiple obstacles from a single measurement, but now we discard all data past the first obstacle found in each measurement, if any. this prevents mis-labeling of anything that is beyond the first obstacle due to reflections. to account for errors in obstacle location, caused by asv localization error as well as compass error, we draw a 1 m radius circle around each obstacle we find and label each site within that circle as an obstacle."
"where h k is the rayleigh fading channel's coefficient, while n k represents the awgn having a variance of n0/2 per dimension. assuming a slowly rayleigh fading channel, where we have h k ≈ h k−1, we can rewrite (3) using (1) as:"
"the rgb color model, in which digital images are usually represented, is unsuitable here because of it's vulnerability to changes in lighting. instead we use the hsv color representation which is better suited for perception of the images. for further perceptual uniformity, we also use the cielab color space too. for each channel in these two color models we take the average intensity value as well as the standard deviation of the intensity values for each site and use that as a feature."
"our training corpus contains 2.9m sentence pairs with 80.9m chinese words and 86.4m english words from ldc data 1 . we used nist mt03 as our development set, nist mt06 as our development test set and mt08 as our final test set. we ran giza++ on the training corpus in both chinese-to-english and english-to-chinese directions and applied the \"grow-diag-final\" refinement rule [cit] to obtain word alignments. we used the sri language modeling toolkit 2 [cit] to train our language models. mert [cit] ) was adopted to tune feature weights of the decoder. we used the case-insensitive bleu 3 as our evaluation metric. in order to alleviate the instability of mert, [cit] to perform three runs of mert and reported average bleu scores over the three runs for all our experiments."
"in this paper, we present the redefine polymorphic asic architecture 1, 15 as a suitable platform for flexible cryptoaccelerators. the overall architecture of the redefine platform is illustrated in fig. 1 . in redefine, specialised hardware units are replaced by more basic hardware units that can be dynamically recomposed to provide different functionalities required to accelerate higher-level applications. applications described in a high level language, namely c, are broken down into application substructures (called hyperops) that are then mapped onto a set of basic processing elements (compute elements (ces) in redefine) interconnected through a noc. the redefine platform includes its own compiler 1, 2, which performs this decomposition in an extremely efficient, hardware-aware manner. since, application synthesis in redefine is from a high level specification in c, new applications as well as application enhancements decided upon after deployment can be easily realised within the redefine framework, by simply creating a new software (functional) description for it."
"unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. in contrast, a large amount of unlabeled data are easily available. therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? or can we exploit these data to refine nonterminals for smt?"
"finally, the client determines a cloaking region based on a particular privacy level which will dictate the number of geographic grid cells to include inside the cloaking region. suppose that the client chooses a privacy level such that the cloaking region consists of four geographic grid cells. the user's true location is in one of these grid cells. inside of the geographic grid cell, there is a set of variable-sized vhc cells according to the distribution of the pois in the geographic grid cell. the user's area of interest, in which pois will be searched, will be the single current vhc cell found inside the geographic grid cell. the number of pois per vhc cell is known, and in our case, it is 60. thus, the user will initiate a request that will reference the cloaking region, as well as the specific vhc cell in which the user is located or interested in. the user will receive a set of 60 pois that are found in his or her current vhc cell only. the server will only know that the location of interest is somewhere within the cloaking region defined by the geographic grid cells."
"we have developed a hybrid solution that consists of pir to achieve query privacy in the context of a location-based service, and a cloaking technique to reduce the computational cost of pir to a feasible level. our technique essentially describes how the user creates a cloaking region around his or her true location, and performs a pir query on the contents of the cloaking region only. the benefits are numerous: the user's location is kept hidden from the server to an acceptable degree regardless of the number of other users in the area; there is no intermediary server that is responsible for cloaking and that would need to be trusted; and the computational cost of the cryptographic algorithms employed is still practical. we ensure that the user downloads only the pois that are of interest to the smartphone, keeping wireless traffic to a minimum to reduce costs and conserve the battery. we describe our solution in this section."
"the geographic grid is fixed. the initial grid cell dimensions are configured based on the maximum size of each vhc cell, but once established, will not need to change. both the client and server must have the same knowledge of the geographic grid. it can be distributed offline (along with the software for the user's smartphone). a simple approach to determining grid cell dimensions is to use a geographic coordinate system such as degrees-minutes-seconds (dms) [cit] . for instance, each grid cell may be two latitude degrees in length, which roughly equates to 200 km at the 30 degree latitude. a population of tens of thousands to millions of users may typically inhabit and stay within the bounds of a grid cell that is 200 km 2 in size, leading to excellent privacy. cells of larger size will afford province-and state-level privacy if desired."
"4. the client sends the cloaking region to the server. also, the client identifies which portion of the cloaking region contains the area of interest, in a way that is hidden from the server."
"the general purpose method of performing reduction is repeated subtractions (or equivalent), which is too slow to be of use in practice, especially for the finite field sizes of cryptographic importance. to circumvent this problem, fast reduction methods have been developed 3, 9 . however, these schemes assume the modulus of the finite field to be a priori decided, and this is where the flexibility of the crypto system wrt different field sizes becomes limited. in redefine, it has been found possible to obtain a 'general purpose' implementation of the fast reduction methods, that can scale to different moduli defined at runtime. the complete method is explained in a related publication 4 . some of the sections are reproduced here to illustrate the essential concepts."
"the proposed semantic nonterminal refinement model estimates the semantic similarity between a phrase p and nonterminal x. the phrase p and nonterminal x will have a high similarity score in the representation space if they are semantically similar. the higher semantic similarity scores are, the more compatible nonterminals are with corresponding phrases."
"finally, we integrated both the source-and targetside semantic nonterminal refinement models into the baseline system. in this experiment, we adopted nonlinear projection to obtain target semantic vector representations for target phrases. these two models collectively achieve a gain of up to 1.16 bleu points over the baseline and 0.41 bleu points over syn-mis model on average, which is shown in table 3 ."
"note that we make no effort to hide the user's identity from the location-based service. we assume that it is acceptable to reveal the user's identity for the purpose of routing the response to a location-based request, and for offering a customized lbs experience. a user that also wishes to hide his or her identity to some extent may wish to make use of an onion router, such as tor [cit] . however, we note that there are application domains where the protection of a user's location using our proposed technique is superior to anonymizing the user's identity. for example, it is easy to try to identify a user who made a query with a particular geographical coordinate, simply by looking up the user who lives at the corresponding residential address and assuming the request did not originate elsewhere. on the other hand, our proposed technique hides query contents from the lbs, and leaves no useful clues for determining the user's current location."
"in the opposite extreme case, the user may choose a minimum level of privacy, for maximum performance benefit. in this case, the user will not utilize the geographic grid to establish the boundary of the cloaking region. instead, the user will pick the current vhc cell that he or she occupies as the actual cloaking region. the server will then infer that the user's location must be within that individual vhc cell. all the pois for that vhc cell will be returned, but the user's exact location within the vhc cell will still be unknown to the server."
"the problem of colluding servers is mitigated by practical business concerns. realistically, a single poi database would be maintained by an organization that is independent of the lbs providers that a user may query. for instance, lbs providers such as google and microsoft may contract the use of a poi source such as the yellow pages, an organization that is responsible for its own content that it updates regularly. however, google and microsoft would be responsible for the content's distribution to end users as well as integration of partners through banner ads and promotions. since the lbs providers are operating in the same or similar line of business where they compete to win users and deliver their own advertising models to reap economic benefits, there is no real incentive to collude in order to break the privacy of any user. in this model, it it conceivable that a user would perform a location-based query and would invoke it on the multiple lbs providers concurrently, and combine the results, without fear of the queries divulging the user's precise location. additionally, individual service agreements can foreclose any chance of collusion with a third party on legal grounds. users then enjoy greater confidence in usage of the service, and the lbs providers in turn can capitalize on revenue generation opportunities such as pay-per-use subscriptions and revenue-sharing ad opportunities."
"one can assume that the programming phase of the redefine-based cryptosystem is distinct from the actual usage of the cryptosystem, as the crypto algorithms mapped onto redefine are not likely to change frequently. supporting non-streaming applications in the context of fig. 5 is trivial. for streaming applications, it would be beneficial to make an application stay on the computation fabric for an indefinite amount of time, until end of streaming input is signaled by some external entity. it turns out, that due to the ability to program the redefine-based cryptosystem in c, this feature can be obtained again without changing any hardware, by introducing the while (1) loop. a sample code snippet for a streaming application is shown in listing 2."
"in order to address this issue, researchers either use syntactic labels to annotate nonterminal xs [cit], or employ syntactic information * corresponding author from parse trees to refine nonterminals with realvalued vectors [cit] . in addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals [cit] . all these efforts focus on incorporating linguistic knowledge into hierarchical translation rules."
"for many users, this constitutes an unacceptable violation of privacy, and efforts should be made to avoid it. as location technology becomes commonplace, users will become increasingly aware of and concerned about location privacy. not only are privacy and personal safety important considerations, but recent advances in mobile advertising have even opened the possibility of location-based spam. [cit], the energy and commerce joint subcommittee of the u.s. house of representatives held a joint hearing on the implications of location-based services on the privacy of consumers 1 . our challenge has been to design a system whereby a user can retrieve useful poi information without having to disclose his or her exact location to a third party such as the lbs server. the user should also not have to reveal what particular pois were searched for and found, as each poi record typically includes precise location coordinates. thus, the server will be unable to infer the user's current location or likely destination, or accumulate a history of requests made for profiling purposes. generally speaking, a user will typically be comfortable with a certain degree of privacy, meaning that the user could be expected to be anywhere within a certain geographic area, such as a city or neighbourhood without fear of discovery."
"we measured query roundtrip times for the c++ prototype on a machine with a 2.91 ghz dual-core amd cpu, 3gb ram, and running ubuntu linux. since the percy++ pir uses replicated databases, we set the number of databases to 2 [cit] . fig. 3 shows query roundtrip times and levels of privacy for queries returning various numbers of pois. the number of pois returned for each query is equivalent to the number of pois in a vhc cell. similarly, the number of pois returned by a query is equivalent of the number of blocks (in bytes), that a traditional pir query returns. a block of 10 pois is equivalent to 2560 bytes of data (each poi consists of 256 bytes)."
"we also installed the client for the java prototype on a g1 android smartphone from t-mobile, which features a qualcomm arm processor running at 528 mhz, and includes 192 mb ddr sdram, and 256 mb flash memory. although our locked smartphone was capable of running on t-mobile's 3g network in the u.s., it did not support the 3g frequency bands in operation in canada. we ran our tests using the rogers edge network, which is slower by up to a factor of ten. we created an android application with a user interface that allows the user to specify the server address and query parameters such as the size of the cloaking region and the size of the portion of the cloaking region to fetch, all in bits (see fig 4) ."
"the approach that we propose entails two phases. first, there is a pre-processing phase in which the system is set up for use. the pre-processing operation must be carried out whenever significant changes are made to the poi database on the server. in practice, it can occur every few months during a period of low usage on the server such as nighttime maintenance activities. second, there is an execution phase, in which the lbs server responds to queries for pois from users. at a high level, the pre-processing phase consists of the following steps:"
"only the lowest layer of operations, namely the finite field operations is accelerated by using cfus. thus, we use two cfus, one for gf (2 m ) squaring of a 32-term polynomial and one for polynomial multiplication of two 16-term polynomials with binary coefficients. these cfus can be used in a scalable manner, i.e., for different field sizes, the higher level application only needs to operate an appropriate number of these cfus. it is the reduction operation that poses a problem with respect to the flexibility of the cryptosystem, and hence, this issue is considered in greater detail in subsequent sections."
"next, consider the case of a mobile user who is physically moving between vhc cells. as long as the user does not move outside of the cloaking region, then the same cloaking region will be transmitted for all queries, and the user will not be subject to a correlation attack, even if the user moves between vhc cells. the same is true if the user moves between geographic grid cells, but still within the same cloaking region defined by the user's level of privacy."
"for an scfg rule, we can learn semantic vectors for nonterminals on both the source and target side. due to the space limitation, we introduce the procedure of learning nonterminal vectors on the source side. semantic vectors on the target side can be learned analogically."
"we incorporate the proposed model as a new feature into the hierarchical phrase-based translation system. specifically, two features are added into the baseline system: 1. source-side semantic similarity between source phrases and nonterminals 2. target-side semantic similarity between target phrases and nonterminals"
"in an extreme case, the user may choose a maximum level of privacy, despite the computational costs entailed. in this case, the user will define a cloaking region that includes the entire geographic region that can be queried; e.g., all of north america. the server would execute its query on all the rows of its database, and so there will be a significant computation cost. however, because only the coordinates of the cloaking region are sent in our protocol, and because only the pois for the user's current vhc cell will be returned, there will be no additional bandwidth costs in this scenario. the level of privacy will be absolute, in that no information about the user's location will be leaked to the server."
"this nonterminal s is different from x. we therefore treat it as a special case in the computation of semantic similarity. in this work, we explore two approaches to compute similarity: one based on cosine similarity and the other based on euclidean distance."
"advanced encryption standard (aes) and elliptic curve cryptography (ecc) algorithms are accelerated on the the highest levels of security can be achieved through the use of more than one type of cryptographic algorithm for each security function. in this paper, the redefine polymorphic architecture is presented as an architecture framework that can optimally support a varied set of crypto algorithms without losing high performance. the presented solution is capable of accelerating the advanced encryption standard (aes) and elliptic curve cryptography (ecc) cryptographic protocols, while still supporting different flavors of these algorithms as well as different underlying finite field sizes. the compelling feature of this cryptosystem is the ability to provide acceleration support for new field sizes as well as new (possibly proprietary) cryptographic algorithms decided upon after the cryptosystem is deployed."
"further, the redefine framework allows the customisation of the basic processing units within the architecture, in order to support special instructions accelerating the common low-level operations occurring in all the applications. processing units thus added are called custom functional units (cfus). this makes it possible to integrate asic-like speed of execution, with the flexibility coming from being able to describe applications in c, which is an ideal combination for unified accelerator for different existing as well as proprietary cryptographic algorithms."
"learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years [cit] c; [cit] ) . these representations have been used successfully in various nlp tasks. however, there is no attempt to learn semantic representations for nonterminals from unlabeled data. in this paper we propose a framework to learn semantic representations for nonterminal xs in translation rules. our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal xs during hierarchical rule extraction. we propose a weighted mean value and a minimum distance method to obtain nonterminal representations from representations of their phrasal substitutions. we further build a semantic nonterminal refinement model with semantic representations of nonterminals to compute similarities between phrasal substitutions and nonterminals. in doing so, we want to enhance phrasal substitution and translation rule selection during decoding."
"our work could be extended by improving on our general scenario where the user retrieves all of the pois that belong to the vhc cell of interest. it is possible that the user will not find a suitable poi within this set, and will wish to search further in neighbouring vhc cells. this may be the case when the result set does not contain a desired poi. therefore, the user may wish to expand the search by searching in a broader geographical area."
"the first step in the pre-processing phase is to represent a geographic area such as the united states and canada on a two-dimensional plane using a map projection method such as the commonly used miller cylindrical projection [cit] . once that is done, the user's location of interest may be found on this plane. it is necessary to obscure the user's location by creating a cloaking area around the user's true position or area of interest. pois will be found anywhere by the lbs server within this cloaking region. the region must be sufficiently large in order to achieve sufficient privacy for the user, but at the same time it must be sufficiently small to minimize the amount of computation required on the user's mobile device to process the query results, as well as to constrain the amount of wireless data traffic required to transport them."
"3. a collection of pois is saved in a database such that each row corresponds to one poi. 4 . each cell of the grid is mapped to a portion of the database, i.e., a particular set of database rows (each containing a poi)."
"as shown in fig 5, we introduced a set of two-input and gates in each stage of the migf multiplier to enable two-word shift operations. in a w-bit instance of the multiplier, two sets of w two input and gates are introduced. the first set of w two input and gates are used for masking the irreducible polynomial input to the multiplier to zero. the second set of w two input and gates are used for enabling two-word shift operation. this increase in hardware complexity is compensated by the significant reduction in the number of operations brought about by using this multiplier as a hardware assist for reduction. the proposed reduction method also leads to a reduction in the basic operation count for finite field reduction, but the associated material is not reproduced here due to space constraints. table 1 details the performance obtained on the redefine-based cryptosystem for the aes and ecc kernels. these results were obtained by running the application on a cycle accurate simulator of the redefine platform described using bluespec system verilog (bsv). the operating frequency of the redefine-based cryptosystem is assumed to be 400 mhz, deriving from synthesis results of the verilog descriptions of the component modules using synopsys design compiler, using 90 nm technology libraries. the results in table 1 are comparable to the performance of individual accelerators for the respective algorithms available in the market. as another point of comparison, the openssl implementation of the ecc operation (random base point) achieves around 350 operations/ second on a 450 mhz ultrasparc 2 processor 14 . while the speeds achieved are comparable to those listed in table 1, the power dissipation is significantly higher due to the higher clock frequency. however, there is scope for achieving even more throughput by utilising the resources available in the redefine architecture framework more fully. for instance, all the ces available in the computation fabric can be dedicated to aes, in which case one would obtain an ultra-high throughput aes engine with multi-gigabit per second performance."
"the device must store a copy of the vhc map in local non-volatile memory, but the storage requirements are very reasonable. the current geographic grid cell encapsulating the user can be derived from the user's current latitude and longitude coordinate, if the mapping convention is known. a single coordinate for the intersection point of each vhc cell inside (i.e. one of its corners) can then be recorded. hence, a single coordinate would suffice to store each vhc cell in device memory. for quick lookup and to minimize storage requirements, the coordinates of all vhc cells only in the current geographic cell could be stored. assuming that the smallest vhc cell size is 1 km 2 in size, then the worst case is that 40,000 coordinates will need to be stored to account for all vhc's. two bytes will be sufficient to store each vhc coordinate, because the origin of the geographic grid cell is known, so that the total cost will be approximately 80,000 bytes to store all vhc cells. this is the worst theoretical case; in practice, small vhc cells will only be encountered in very dense metropolitan areas, and they will not occupy an entire geographic cell."
we propose a novel hybrid lbs technique that integrates location cloaking and private information retrieval. we have also implemented and evaluated our proposal to determine its practicality on resource-constrained hardware. the results show that users can achieve a good compromise between privacy and computational efficiency with our technique unlike all other existing lbs proposals.
"this paper shows how all the challenges associated with constructing a crypto-accelerator capable of accelerating even 'run-time' defined crypto applications can be met by using the redefine architecture framework as the overall platform for appropriately dividing applications into software and hardware portions. specifically, a procedure to perform efficient and programmable finite field reduction is discussed. there are quite a few avenues for future work, among which two are listed here:"
"our approach of retrieving the points of interest within a single cell, contained within a cloaking region, is much less expensive than the naive approach of requiring the user to download the entire contents of the cloaking region. at the same time, privacy is not compromised. because either the user or the network provider must ultimately pay for the cost of the wireless data that is sent, the delays associated with the transfer of wireless traffic are significant, and memory on the device is constrained, it is a great benefit that traffic is kept to a minimum in our scheme."
"in this paper, we have proposed an algorithm for private information retrieval that achieves a good compromise between user location privacy and computational efficiency. we have implemented and evaluated our algorithm and shown that it is practical on resource-constrained hardware."
"to generate the results given in table 1, we have used the montgomery algorithm for the random base point case, and the fixed base comb method for the fixed base point case 14 ."
"we have presented a framework to refine nonterminal x in hierarchical translation rules with semantic representations. the semantic vectors are derived from vector representations of phrasal substitutions, which are automatically learned using an unsupervised rae. as the semantic nonterminal refinement model is capable of selecting more semantically similar translation rules, it achieves statistically significant improvements over the baseline on chinese-to-english translation. experiment results have shown that"
"our approach of using a variable-sized cloaking region divided into vhc cells results in greater location privacy than the traditional approach of a single cloaking region, while at the same time decreasing wireless data traffic usage from an amount proportional to the size of the cloaking region to an amount proportional to the size of a vhc cell. it also allows the user to dynamically choose various levels of privacy. although increasing the size of the cloaking region does result in higher computation in processing the query, we believe that this tradeoff is very reasonable, given that the processing power of today's smartphones is still less of a concern than the speed and cost of wireless network connectivity."
"several techniques allow pois to be mapped to a cloaking region. one technique is quad-tree mapping [cit], but it has the disadvantage (from its use in casper [cit] ) of forming an unnecessarily large cloaking region which can impair performance [cit] . another technique is called vhc (various-size-grid hilbert curve) mapping [cit], which suits our purpose. in particular, it solves the problem of the density of pois varying by geographic area. if the density of pois is significantly higher for a given region (such as a city), then a higher data traffic cost will result if the size of the cloaking region remains constant, and the query will be much slower. if on the other hand, the density becomes significantly lower (such as in a sparsely populated region like the countryside), then the result size may be so minimal that the server may guess the user's likely destination with a high degree of confidence, leading to loss of privacy. vhc solves this problem by creating variable-sized regions that can be used for cloaking, based on the density of the pois in the geographic area."
the above output representation p can be used as a child vector to construct the representation for a larger subphrase. this process is repeated until a binary tree covering the whole input phrase is generated.
"some of the drawbacks of the above approaches are addressed with distributed k-anonymity [cit], where homomorphic encryption is used to allow network operators to collaborate and inform a user whether kanonymity holds for his or her current area without the operators learning any additional information. in this case, the user can ensure that k-anonymity is achieved."
"the approach presented in this paper is sufficiently generic to allow an application to rely on any existing computational or information-theoretic pir (single-server, multi-server or hardware-assisted). we present an approach for preventing collusion between servers in the case of information-theoretic pir. this concept is important to our discussion because information-theoretic pir makes the usual assumption that the replicated pir servers that implement it do not collude to violate the privacy of the users."
"a malicious server can return pois that are not found in the cloaking region, but the user can filter the results out based on their coordinates. if the server returns false pois, the user will not be able to verify and detect them. this is unlikely, as the server would need to replace all the rows in the portion of the database (corresponding to the cloaking region) with false information. if the server returns an empty result instead of the actual content in its database portion, then the user may expand the search by increasing the level of privacy and increasing the size of the cloaking region or by exploring neighbouring vhc cells within the cloaking region, but the server will fail to learn anything additional about the user's location."
"similar to the center of gravity, the semantic vector v x learned by this method acts as a semantic centroid for all vectors of phrases that are substituted by x. nonterminals in different hierarchical translation rules will have different semantic centroids. these centroids will help translation model capture semantic diversity to a certain degree."
"our basic scenario entails a mobile device user who operates a smartphone with location technology and wireless data transfer capability. the user searches for nearby pois (i.e., nearest neighbour) by first constructing and sending a query to a known lbs server over the wireless network. the lbs server retrieves the query, performs a search of its poi database, and returns a set of results to the user containing all pois found in the specified region. our protocol must meet the following requirements:"
"thus, our cloaking technique provides a way of reducing the search space of the poi database by employing multiple levels of database segmentation. the cloaking region itself is described as a single, or multiple, geographic grid cell or cells. inside each geographic grid cell are found one or multiple vhc cells, the number depending on the poi density. the user's true location is inside one of these vhc cells, and the user retrieves poi's corresponding to that vhc cell only. as far as the lbs server is concerned, though, the user could be located anywhere within the larger geographic grid cell."
"keywords: redefine, advanced encryption standard, aes, elliptic curve cryptography, ecc, cryptographic protocols redefine architecture framework. these two algorithms are chosen since they are instances of two distinct types of cryptographic algorithms-aes involves a large number of bitwise operations, while ecc requires computations over finite fields. we show how redefine can accelerate finite-field computations of arbitrary sizes, which is the most demanding requirement for a flexible cryptosystem."
location cloaking in general seeks to prevent an attacker from being able to match queries to particular users and to thus compromise their privacy. the attacker may be in a position to observe traffic flowing through the network or even be situated at the lbs provider endpoint.
"a user sets the desired privacy level by specifying the size of the cloaking region. the ratio of the number of pois inside this region to the number of pois in the entire poi database defines the level of privacy. the privacy level can be specified in terms of cities/towns (city level), states/provinces (provincial level), and so on, to enhance user-friendliness. thus, a privacy level value of 1 indicates that the user desires query privacy at the same level as that offered by a typical pir protocol. similarly, if a user sets the query privacy level to 0.6, the pir query will execute faster. although the cost is still linear in the number of items in terms of computational complexity, the constant term is modified (i.e. in terms of big-o notation), leading to significant performance gains. at the same time, it will be disclosed to the server that a particular amount of 0.4n items are not of interest to the user; this leakage of information does not necessarily constitute a significant breach of location privacy."
"a pir technique can be used to ensure that queries and their results are kept private. specifically, pir provides a user with a way to retrieve an item from a database, without the database (or the database administrator) learning any information about which particular item was retrieved. pir satisfies our requirements for privacy and low communication cost. however, existing pir techniques have drawbacks of high computational cost for applications that require low latency."
"a closer look at eqn (3) reveals that computation of c i (x) p j (x) involves computations of the form c i (x) x r . each of the individual words like c' i, j (x) (fig. 3) in the product of the entire polynomial c(x) and x r can be computed as follows:"
hierarchical phrase-based translation [cit] explores formal synchronous context free grammar (scfg) rules for translation. two types of nonterminal symbols are used in translation rules: nonterminal x in ordinary scfg rules and nonterminal s in glue rules that are specially introduced to concatenate nonterminal xs in a monotonic manner. the same generic symbol x for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules.
"6. the client decodes the result, and automatically finds the nearest neighbour poi, or presents the full list of pois returned to the user to choose amongst."
"the rest of this paper is organized as follows. section 2 briefly reviews related work. section 3 presents our approach of learning semantic vectors for nonterminals, followed by section 4 describing the details of our semantic nonterminal refinement model. section 5 introduces the integration of the proposed model into smt. experiment results are reported in section 6. finally, we conclude our work in section 7."
"5. the server receives the request, and finds the database portion corresponding to the cloaking region. a block of rows is retrieved from this portion based on the user's specified location of interest. the pois present in these rows are transmitted back to the client."
"the location of interest may be either the user's true, physical location, or some other location that must be kept private. for the purpose of this analysis, we will simplify the job of a malicious lbs server by assuming the location of interest is the user's actual location. without this assumption, it will be more difficult for the server to infer a user's actual location because the user may issue a query for a geographical area without physically residing in that area."
"in our framework, semantic representations for nonterminal xs are automatically induced from word-aligned parallel corpus. in this section, we detail the essential component of our approach, i.e., how to learn semantic vectors for nonterminals and how to project source semantic vectors onto target language semantic space. before discussing nonterminal representations, we briefly introduce vector representations for words and phrases."
"furthermore, there is an optimal block size for any pir protocol that will allow our algorithm to give the best query response time. however, the bandwidth of the communication channel may also play some role in determining which block size is optimal. an extension would be to study what impact network bandwidth will have on the performance of our algorithm, using a variety of pir implementations."
"where a/b, c/d are strings of terminals on the source and target side, s and t are placeholders denoting the nonterminal x on the source or target side, x s and x t are aligned to each other. representations for nonterminals can be on either the source or target side. they are attached to hierarchical rules as follows:"
"we used word2vec toolkit 4 to train our word embeddings and set the vector dimension d to 30. in our training experiment, we used the continuous bag-of-words model with a context window of size 5. the monolingual corpus, which was used to pre-train word embeddings, is extracted from the above parallel corpus in smt. to train vector representations for multi-word phrases, we randomly selected 1m bilingual sentences 5 as training set and used the unsupervised greedy rae following [cit] . we used a learning rate of 10 −3 for our minimum distance method that learned the centroid of phrase representations as the vector representation of the corresponding nonterminal."
"new cryptographic standards are firmly moving towards recommending different cryptographic algorithms for different security functions, all of which are usually needed within the same communicating device. with quality-of-service based communications proliferating, there is also the need to be able to rapidly tune the performance of these accelerators according to communication medium conditions. achieving this flexibility by using a number of 'fixed function' accelerators available today is not a scalable approach, as changing cryptographic standards could easily render the security functionality out of date, requiring a respin of the entire product. such circumstances make it an attractive proposition to have a unified cryptographic accelerator that can accelerate at least a canonical set of existing cryptographic algorithms, while still providing some means to support performance tuning as well as new cryptographic algorithms. of course, it is vital not to lose too much of the high performance that makes specialised accelerators attractive, in the process."
"essentially, in vhc, the two-dimensional geographic grid is mapped to a one-dimensional space such that it has equal poi density everywhere (see fig. 1a ). assume that a typical poi database that covers the regions of canada and the u.s. will have 6 million pois. if each vhc cell must contain the same number of pois, such as 60, then there will be a total of 100,000 vhc cells that will cover this geographic region. suppose that the lowest poi density found in the database is 60 pois per 40,000 km 2 . thus, the maximum size of a vhc cell will be 40,000 km 2 . now, we create a geographic grid overlaying the u.s. and canada regions with fixed-size square cells that are 200 km in length (the area of each is 40,000 km 2 ). this corresponds to the maximum size of a single vhc cell as described above. each geographic grid cell, however, may contain any number of smaller-sized vhc cells if the poi density of the region is greater (see fig. 1b) ."
"author consider how the redefine-based crypto system can be fit as a coprocessor inside a multiprocessor system, wherein the redefine-based crypto system can be driven by one or more of the other processors in the overall system. the overall system architecture is shown in fig. 5, where an axi bus is shown as connecting all of the entities in the system."
"our solution preserves the privacy of the user's location irrespective of the number of other users initiating queries for the same location. the server can infer only the user's location based on the cloaking region. the user may adjust the size of the cloaking region based on his or her personal preferences (i.e., desired level of privacy, query performance, and cost), because a larger region will entail more computation."
elliptic curve cryptography (ecc)-based algorithms basically operate on a subset of points over an elliptic curve. the coordinates of these points are defined over an underlying finite field or galois field.
"the big challenge here is that thousands of tar-get phrasal substitutions will be generated for one single nonterminal during decoding. computing vector representations for all these phrases will be very time-consuming. we therefore introduce two different methods to handle it. in the first method, we project representations of source phrases onto their target counterparts linearly/nonlinearly via a neural network. these projected vectors are used as approximations to real target representations to compute semantic similarities. in the second method, we decode sentences in two passes. the first pass collects target phrase candidates from n-best translations of sentences generated by the baseline. the second pass calculates vector representations of these collected target phrases and then computes similarities between them and target-side nonterminals."
"users of mobile devices tend to frequently have a need to find points of interest (pois), such as restaurants, hotels, or gas stations, in close proximity to their current locations. collections of these pois are typically stored in databases administered by location based service (lbs) providers such as google, yahoo!, and microsoft, and are accessed by the company's own mobile client applications or are licensed to third party independent software vendors. a user first establishes his or her current position on a smartphone such as a rim blackberry, apple iphone, or google android device through a positioning technology such as gps (global positioning system) or cell tower triangulation, and uses it as the origin for the search. the problem is that if the user's actual location is provided as the origin to the lbs, which performs the lookup of the pois, then the lbs will learn that location. in addition, a history of locations visited may be recorded and could potentially be used to target the user with unexpected content such as local advertisements, or worse, used to track him or her. the user's identity may be divulged through the inclusion of the originating dynamic ip address, e-mail address, or phone number in requests to the lbs server so that the results of an lbs query can be routed back to the correct user via a tcp data connection, e-mail reply, or sms reply, respectively. if a location can always be correlated to each request, then the user's current pattern of activity and even personal safety is being entrusted to a third party, potentially of unknown origin and intent. although search engines routinely cache portions of previous queries in order to deliver more relevant results in the future, we are concerned when the user's exact location history is tracked, and not just the key words used in the search."
"basic pir schemes place no restriction on information leaked about other items in the database that are not of interest to the user; however, an extension of pir, known as symmetric pir (spir) [cit], adds that restriction. the restriction is important in situations where the database privacy is equally of concern. the only work in an lbs context that attempts to address both user and database privacy is [cit] . although, not strictly an spir scheme, it adopts a cryptographic technique to determine if a location is enclosed inside a rectangular cloaking region. the goal of the paper was to reduce the amount of pois returned to the user by a query. unlike ours, the approach fails to guarantee a constant query result size which defeats correlation attacks, and it requires dynamic partitioning of the search space which may be computationally intensive. it also requires two queries to be executed, whereas a single query-response pair is sufficient in ours. another cryptographic construction related to pir is oblivious transfer (ot) [cit] . in ot, a database (or sender) transmits some of its items to a user (or chooser), in a manner that preserves their mutual privacy. the database has assurance that the user does not learn any information beyond what he or she is entitled to, and the user has assurance that the database is oblivious or unaware of which particular items it received. ot can thus be regarded as a form of generalization of spirs or the superset of protocols that deal with the private exchange of information between two parties in a manner that preserves privacy for both parties."
"the advanced encryption standard (aes) encrypts/decrypts 128-bit data blocks using 128-bit, 192-bit or 256-bit keys. aes accelerators can be designed to meet a large number of possible area-performance points. when designing a flexible cryptosystem, the challenge is to make use of some of the hardware resources dedicated to obtain a high performance aes implementation, in order to accelerate other cryptographic algorithms. in redefine, we solve this problem by realising the aes application in the following manner:"
5. the grid structure is transmitted and saved on the client device in a local mapping database so that it can be referenced in a subsequent query.
"the query roundtrip or response times for block sizes 5, 10, 25, 50, 100, 250, and 500, at query level of privacy 1, are between 25 and 70 seconds. this is because each pir request runs against the entire database of 6 million synthetic pois. however, the query roundtrip time improves with lower levels of privacy. for example, the query response times for the above block sizes at a privacy level of 0.17 are between 4 and 12 seconds. one must observe that setting the query level of privacy to 0.17 is equivalent to privately querying a block of pois from a portion of the database consisting of 1.02 million pois. if we assume there are equal number of pois in all the provinces and states of canada and us, a level of privacy set to 0.17 implies a cloaking region that covers approximately 10 provinces and/or states. under a similar assumption, a user who intends to hide his or her query in a cloaking region consisting of one province or state will simply set his or her query level of privacy to a much lower value of 0.02. the query response time for this level of privacy is approximately 0.3 seconds for an optimal block size, which in our testing configuration consists of 256 pois. it is easy to observe from the graph that the block that consists of 250 pois gives the best performance. furthermore, the worst performing block size is the one consisting of 5 pois, the reason being that smaller block sizes require more rounds of computations to process the individual blocks, compared to larger block sizes. on the other hand, large block sizes, such as 500, carry performance penalties and overheads which depend on the characteristics of the underlying pir scheme, and also on the resource constraints of the runtime hardware (e.g., ram, disk and memory cache sizes, and network bandwidth). the network cost in the c++ implementation was negligible since the measurements were taken on a lan."
"the central operation in all ecc-based protocols is the point scalar multiplication operation 5, 10, 11 . this operation can be broken down to operations at lower levels as described in fig. 2 . the performance of all ecc-based schemes is finally determined by the performance of multiplication in the underlying finite field 8 . the point addition, point doubling and point multiplication algorithms are described as high level c applications on the redefine framework.this makes it easy to optimise these algorithms depending on the curve and base point chosen."
"an attacker could delay a request or a result, such that the results for multiple queries would be received by the client out-of-order. the client will not be confused in this case, as each poi result will include location coordinates, and the user can filter the pois based on the current vhc cell that he or she is occupying."
"one popular cloaking technique is based on the principle of k-anonymity, where a user is hidden among k-1 other users. queries from multiple users are typically aggregated at an anonymity server which forms an intermediary between the user and the lbs provider. this central anonymity server can provide spatial and temporal cloaking functions, so that an attacker will encounter difficulty matching multiple queries that are observed with users at particular locations and at particular points in time. many cloaking solutions for location privacy suggest either a central anonymity server as described [cit], or other means such as decentralized trusted peers [cit] or distributed k-anonymity [cit] ."
"in typical usage, however, the user is expected to define a cloaking region that bounds one or more geographic grid cells, depending on the desired level of privacy. for example, a matrix of 2-by-2 geographic grid cells, or 3-by-3 cells, etc. this user can always adjust the level of privacy for each request. this is useful in the case where the user is traveling between vhc cells. for example, if the user is traveling by car on a highway, and may cross the current geographic grid cell before the next query is issued, then a larger cloaking region formed from the neighbouring grid cells may be appropriate to request."
"aes-128 (using 10 ces of the fabric) 3 gbps ecc(over gf (2 163 )) random base point, random curve coefficients 350 ops/s ecc(over gf (2 163 )) fixed base point, random curve coefficients 700 ops/s"
"the cloaking region is selected based on the user's location. let us suppose that the user's preferred level of privacy will be satisfied with a cloaking region that is defined by a matrix of 2-by-2 geographic grid cells. the user will be located in one of these geographic grid cells. three additional geographic cells must be chosen to form the cloaking region. if the user proceeds to pick these geographic cells randomly from the surrounding set of geographic cells for each request, then each request may send a different cloaking region. after a number of these requests, and if the user remains stationary inside one of these geographic grid cells, then the server will be able to correlate these requests, and determine the overlapping grid cell which likely contains the user."
"although pir satisfies our baseline privacy constraints, current implementations of it fail to satisfy our third condition, which is usable performance on modern smartphone hardware. our challenge has been to complement pir with a new algorithmic approach that effectively reduces the amount of computations without significantly sacrificing the user's location privacy."
"the difference of our work from these studies is that our semantic representations are learned from unlabeled bilingual (or monolingual) data and do not depend on any linguistic resources, e.g., parsers. we also believe that our model is able to exploit both syntactic and semantic information for nonterminals since vector representations learned in our way are able to capture both syntactic and semantic properties [cit] ."
"2. can the target-side semantic nonterminal refinement model improve translation quality? and which method is better for integrating the target-side semantic model into translation, projection or two-pass decoding?"
"clearly, these requirements present the need for a mechanism to directly retrieve information in a secure and private way without revealing the contents of the query results, and without the need for an intermediary between the user and the database server to provide some kind of a masking function. fortunately, there is a branch of cryptography that is associated with retrieving information from a database without revealing which item is being retrieved; it is known as private information retrieval (pir) [cit] . our proposed solution is sufficiently generic to allow an application to rely on any pir scheme. we make the same assumptions as that of the underlying pir scheme, where retrieval is either by object index or keyword [cit] . we describe a server that can find the relevant poi entries based on the user's location of interest included in the request; this is possible because the entries in the poi database are indexed by their location."
"a variety of approaches have been explored for nonterminal refinement in hierarchical phrasebased translation. these approaches can be categorized into two groups: 1) augmenting the nonterminal symbol x with informative labels, and 2) attaching distributional linguistic knowledge to each nonterminal in hierarchical rules. the former only allows substitution operations with matched labels. the latter normally builds an additional model as a new feature of the log-linear model to incorporate attached knowledge."
subphrases. we believe that these subphrases determine syntactic and semantic properties of the nonterminal x. we therefore enrich each nonterminal x with a semantic vector induced from vector representations of phrases that are replaced by the nonterminal during rule extraction.
"if the user moves to a vhc cell that is outside of the original cloaking region, then a new cloaking region must be formed. the user will now occupy a new geographic grid cell that will define a new cloaking region. the server could observe that requests are being made for neighbouring cloaking regions, and could infer that the user is somewhere close to the edge of these regions, or is traveling between them. this is a consequence of the user having to create cloaking regions such that there is never any overlap between them. the assignment of geographic grid cells to cloaking regions must satisfy this requirement, and is done based on their consecutive ordering on the grid. to avoid the possibility of the server detecting movement between cloaking regions, it is recommended that the user instead increase the privacy level so that a greater number of geographic grid cells will define the cloaking region. this enlarged cloaking region should contain the original cloaking region. now, if the user moves between grid cells, the same cloaking region will still cover them, and a correlation attack will be unsuccessful. thus, the user should be aware of his or her likely movement in the future and choose a privacy level such that the cloaking region will contain all current and likely future locations. if the size of the cloaking region is constantly adjusted up and down, based on movement and stationary states, then the server may be able to re-construct a history of the user's movement patterns. note that in the general proposed scheme, it is not strictly required for the user to be able to predict the path of future travel for all requests. it simply confers an optional improvement to privacy if the user can encapsulate an intended path of travel through a more informed choice of the cloaking region boundaries."
"the weights of these two features are tuned by the minimum error rate training (mert) [cit], together with weights of other sub-models on a development set. figure 2 shows the architecture of smt system with the proposed semantic nonterminal refinement model."
"in our design, we have elected to specify fixed groups of geographic grid cells at various levels of privacy. in other words, if the user remains stationary, then for each request, the user, depending on the privacy level, will select the same 2-by-2 matrix, or 3-by-3 matrix, etc. therefore, the correlation attack described above is impossible because the client will send the same cloaking region for all queries for a given privacy level."
"most of the pir-based approaches for location privacy rely on hardware-based techniques, which typically utilize a secure coprocessor (sc) at the lbs server host [cit] . this hardware creates a computing space that is protected from the lbs, to realize query privacy. a major drawback of sc-based pir is that it requires the acquisition of specialized tamperproof hardware and it usually requires periodic reshuffling of the pois in the database, which is a computationally expensive operation [cit] ."
"when a typical mobile phone accesses a third-party lbs provider through a wireless 3g data connection, we assume that it reveals only its identity and the query itself to the provider. unavoidably, a mobile communications carrier is always aware of the user's location based on the cell towers in contact, and so it must not collude with the lbs provider. our assumption relies on the lbs provider not being integrated into the carrier's infrastructure, such as a traffic reporting service using cell tower data that discovers a user's location passively. our assumption is valid for the vast majority of lbs applications, which are unaffiliated with the carrier; these include search portals, social applications, travel guides, and many other types. when communicating with such an application, the mobile user's ip address is of no help in determining the user's physical location, as it is dynamically assigned independent of location. only a central gateway that is administered by the telecommunications carrier will be identified. we assume that no other information will be gleaned by the lbs provider. in the case where a mobile user utilizes wi-fi instead, the user will be assigned an address that points to the nearby access point, however, and may need to employ other techniques, such as tor, to mask the address."
"for the future work, we are interested in learning bilingual representations [cit] for nonterminals. we also would like to extend our work by using more contextual lexical information to derive semantic vectors for nonterminals."
"on the server side, the increased computational demands of queries may be handled through employment of multiple, distributed servers. however, it would also be atypical for more than one query to be made at a time, as a smartphone user typically runs only a single application as a foreground task due to processor, memory, operating system, and screen size limitations. overall, we have shown that the delay experienced in a poi lookup operation on a typical location based service is minimal. we have done so by testing on actual smartphone hardware available to users today."
"the common criticism against this pir-based approach in the literature is that it is too costly to be practical [cit], and that the computational overhead is unsuitable for resource-constrained hardware, such as smartphones [cit] ."
"the cloaking region is thus identified as a subset of the entire world described by the database. if we imagine that the world is mapped as a grid of so-called geographic grid cells that are equally distributed, then one of these cells will be chosen to comprise the cloaking region. if a higher privacy level is desired, then the cloaking region may be expanded to include multiple geographic grid cells, and thus a larger portion of the database that describes the world. it is sufficient to identify each grid cell by its cell number if the mapping is static and published. the process of mapping the world to a geographic grid occurs during the pre-processing phase, described next. the poi results for vhc cell 25 only will be returned in a query. if a larger cloaking region consisting of geographic grid cells 1 to 4 was specified (for privacy), the same poi results would still be returned."
"in communications where the highest possible security is absolutely necessary, for instance in national security related communications, moving away from standards based algorithms and devising custom cryptographic algorithms is an option that is desirable and often practiced. it may also be desired to change the crypto algorithm in use at a particular time on-the-fly. if high performance is needed together with proprietary algorithms, one simply cannot obtain a solution in currently available cryptographic accelerators. designing a unified platform for this purpose requires a careful shift from the usual algorithm-focused way of designing cryptoaccelerators."
"1. the user determines the area of interest, either based on the current physical position as determined through gps, or some other arbitrary area that the user may be traveling to in the future."
"in addition, if an attacker observes the communication between the client and the server, then only the client's identity will be disclosed. neither the contents of the query nor the results can be decoded if end-toend encryption, such as transport layer security (tls), is utilized for the communications link. the server must be able to identify the address of a user's mobile device in order to route its response. the address itself will not give away the user's position, however. this is the case with ip addresses dynamically assigned to mobile devices. only a central gateway that is administered by the telecommunications carrier will be identified, which will not be useful in tracing the physical location of the user. the gateway typically assigns ip addresses dynamically to all mobile users, in a manner that does not depend on the actual location. recall that our proposal does not seek to protect the identity of the user from the lbs, but rather to keep his or her location private."
"from eqn (1) we observe that multiplications of the form c(x)(x k + · · · + x p + 1) form the core of the computations. therefore, it is necessary to accelerate these multiplications in order to perform fast-reduction. it should also be noted that the only other operations involved in reduction are addition over gf (2 m ). since, there is no carry involved in addition, addition of two m-bit polynomials which span more than one word in a w-bit architecture can be realised as ceil(m/w) w-bit xor operations. multiplication on the other hand requires multi-word shift and accumulation of results. consider the two polynomials c(x) and p(x) of degree m and k respectively. these polynomials can be represented in a w-bit architecture as a collection of m c and m p w-bit words respectively. eqn (2) shows the representation. c m c -1"
"minimum distance method (md) finds a point in semantic space to minimize the sum of euclidean distances of vectors in p r to this point. formally,"
"the three important requirements for pir are correctness, privacy and non-triviality [cit] . correctness requires that the user obtain the correct database bit or block. privacy requires the database to learn nothing about the user's private input i or the retrieved block of bits. non-triviality expects a better solution than the trivial one; that is, the communication complexity between the user and the database must be sub-linear in n. another requirement, which is not often addressed in the published literature, is implementation efficiency. in fact, the literature has dedicated the most attention to reducing communication complexity at the expense of computational complexity [cit] . this neglect of computational overhead has led to pir constructions that are impractical on resource-constrained hardware. the promise of pir in several application domains like patent databases, pharmaceutical databases, online census, real-time stock quotes, and location based services, has been largely unrealized."
"using the tools available within the redefine framework, one would be able to implement different complex load-sharing schemes across the ces (based on the desired speed) without having to change any of the provided hardware. similarly, the results for the ecc point scalar multiplication with a fixed base point have been obtained with only 256 points of storage. however, the redefine framework can support a large (upto 4gb) size of addressable memory. by using more of the available memory for storing more pre-computed points, the throughput for the fixed base point ecc can be greatly increased."
"the chief problem is that the anonymity server must normally be part of the trusted computing environment and represents a single point of vulnerability. if it is successfully attacked, or collusion with the lbs server occurs, then the locations of all users may be divulged. it is also observed that although a cloaking technique by itself is advantageous in that it does not result in increased computational cost on the server, it can carry with it a high communication cost from the lbs provider to the client. this can mean a large and unacceptable penalty for mobile phone users. finally, if a reduced sample population results from the number of active users in a particular geographic area, it may not suffice to satisfy the desired degree of anonymity. if the anonymity server delays execution of a request until the k-anonymity condition is satisfied, then this delay may prove to be unacceptable to the user from a feature interaction point of view."
"if an attacker observes a request from the client and launches a replay attack against the server, then the server will respond to the request, but neither the attacker nor the server will receive any additional information on the user's location. tls can of course also mitigate this attack if necessary."
we use the stochastic gradient descent algorithm to find the minimal distance and the point v x . the component v xj can be updated by
"to defeat a server's ability to narrow down the search space for the item of interest to the user, pir protocols typically process every item, or poi, in the pir database. this results in a computational complexity that is linear in n (where n is the number of items in the pir database). this is the main hindrance to practical pir deployment [cit] ."
"for a cloaking region consisting of 400 cells (i.e., database rows), and a poi result size of 32 bits (i.e., size of a database row), the following performance was measured on the actual android smartphone: query generation required 2.04 s, result processing required 8.02 s, and the total roundtrip time (including transmission over the wireless edge packet data network) required 207.49 s. in comparison, when the cloaking region was reduced to 100 cells, with the same poi result size, the performance measurements were: query generation of 4.40 s, result processing of 3.42 s, and a roundtrip time of 62.55 s. we observed little variance in the query encoding time, but the decoding time varied nearly proportionally with the size of the cloaking region, as one would expect. overall, the implementation was usable even though it had not been originally designed and optimized for the android platform, and we were restricted to a non-3g network."
"the geographic grid is useful in specifying the size of the cloaking region and for identifying which vhc cells will comprise the cloaking region. the level of privacy, defined from 0 to 1, establishes the size of the cloaking region. the client then sends this cloaking region to the server, by identifying the bounding coordinates (i.e., the longitude and latitude of the top-left and bottom-right corners). the server will then be able to identify which vhc cells belong to this cloaking region, and therefore which portion of the database must be read. the client must also encode the vhc cell containing the area of interest inside a pir query. (each vhc cell in the system is uniquely identified by a numeric value.) fig. 2 further illustrates the relationships among a geographical grid, vhc cells and pois."
"where c 1 and c 2 are the reconstructed children, w (2) is a weight matrix for reconstruction, b (2) is a bias term for reconstruction, and f (2) is an element-wise activation function. for each node in the generated binary tree, we compute euclidean distance between the original input vectors and the reconstructed vectors to measure the reconstruction error:"
"where v x. is the source-or target-side semantic representation for nonterminal. in this way, we keep original translation rules intact and decorate nonterminals with their semantic representations."
"our contributions are two-fold. first, we learn semantic representations for nonterminals from their phrasal substitutions with two different methods. this is the first time, to the best of our knowledge, to induce semantic representations for nonterminals from unlabeled data in the context of smt. second, we successfully address the issue of time-consuming target-side phrase-nonterminal similarity computation mentioned above. we incorporate both source-/target-side semantic nonterminal refinement model and their combination based on learned nonterminal representations into translation system. experiment results show that our method can achieve an improvement of 1.16 bleu points over the baseline system on nist mt evaluation test sets."
"where src is the input vector which is learned in the source semantic space, w (3) denotes the weight matrix for connections between input and hidden neurons and w (4) denotes the weight matrix for links between hidden neurons and output, b (3) and b (4) are bias terms. to train the neural network, we optimize the following objective:"
"in the second set of experiments, we further validate the effectiveness of semantic nonterminal vectors learned on the target side. in these experiments, learning vector representations and computing semantic similarities were performed on the target language semantic space. we also compared the two integration methods discussed in section 5 for the target-side model. with regard to the projection method, we further compared the linear projection (the projection neural network without hidden layer) with the nonlinear projection (with hidden layer). experiment results are shown in table 2 . from table 2, we can see that"
"both the client and server must agree on the same vhc mapping, and this mapping must be done offline in advance. because it is dependent on population density, it will remain relatively static over time even as the population grows, and can be dynamically updated on the client if necessary. in order to contain knowledge of the mapping to define the cloaking region, the user may make use of a pre-computed map file that is stored locally on the device. this mapping technique is a replacement for a cloaking region that is simply based on cells of constant size, and ensures that a constant and predictable number of results are returned for the user's grid cell."
the size of the cloaking region is based on a particular size of geographic area and does not need to be adjusted based on the known distribution of pois within the region. the user only establishes a reasonable level of privacy based on the number of geographic grid cells that define a geographic area. the boundary of the cloaking region utilized in a request is established by the user and is based on the geographic cell map contained on the user's end and the level of privacy parameter. the size of the cloaking region and its boundaries are not controlled by the server.
some researchers suggest a decentralized approach to overcome the shortcomings and security threats inherent in an anonymity server that acts as an intermediary between the client and the server.
"where n is the number of training examples, trg i is the target vector representation for the ith example learned by rae and p i is the output of the neural network for the source vector representation src i of ith example. r(θ) is the regularizer on parameters:"
"our baseline system is an in-house hierarchical phrase-based system [cit] . the features used in the baseline system includes a 4-gram language model trained on the xinhua section of the english gigaword corpus, a 3-gram language model trained on the target part of the bilingual training data, bidirectional translation probabilities, bidirectional lexical weights, a word count, a phrase count and a glue rule count."
"we propose a tradeoff, in the tradition of pir development over the years, to make pir-based solutions practical. for example, information theoretic privacy necessitates replacing a single database with at least two replicated databases; another option is to compromise information theoretic privacy for lower privacy (i.e., attain computational privacy). our proposal is to offer users the choice of trading off privacy for better query performance, by specifying the levels of privacy that they want for their queries. a level of privacy for the query determines the number of items that the pir server must process in order to provide a response. setting levels of privacy is a common practice in several domains where privacy is important (e.g., web browsers). in the specific case of location privacy, we argue that resource-constrained device users are willing to trade off privacy to obtain reasonable performance. on the other hand, such users are equally willing to trade off some levels of performance to gain some levels of privacy support."
"one way to accomplish this would be to perform an additional query for each of the other vhc cells in the current geographic grid cell. however, there is overhead associated with each such request. another approach would be to have the user request pois for all of the vhc cells within a geographic grid cell. note that the size of the cloaking region, which consists of one or more geographic grid cells, remains constant. this is important in terms of making it difficult for the server to correlate the expanded search (for all vhc cells) with the original search (for a single vhc cell). an optimization of the first approach would be to reduce the number of queries for individual vhc cells, by querying for multiple vhc cells at the same time. the selection of multiple vhc cells within a geographic grid cell, while mitigating the effectiveness of a correlation attack, remains an open problem."
iw and p j (x)x jw denote the i-th and j-th words of the polynomials c(x) and p(x) respectively. the product of these two polynomials can be computed as follows:
"the size of the cloaking region and the performance of a query depend on the user's specified level of privacy. if the user wishes to obtain a higher level of privacy, then the size of the cloaking region can be defined to be larger, and to encompass a larger number of geographic grid cells (and thus vhc cells), but the amount of computation on the server will increase accordingly, delaying the response. nevertheless, the chief benefit is that the processing time of the query on the server is predictable, because each vhc cell in each request contains the same number of pois. the key fact is that the amount of data transmitted will be roughly proportional to the number of pois in a single vhc cell (depending on the details of the pir scheme being employed), but the server will only learn the client's location to the resolution of the cloaking region. the amount of variation allowed in the size of the cloaking region should be kept to a minimum, as this variable may be used to form part of a fingerprint of a target in a correlation attack. allowing a one-cell or two-by-two-cell region only may be a good compromise. the latter could be employed by the user on a permanent basis to avoid the threat of inter-cell movement being discovered."
"in the very unlikely scenario where the user is known to be within the area of the cloaking region in advance of the request, then the request itself may be correlated with the user's identity. this is trivial if there are known to be no other users in this cloaking region. the technique of k-anonymity can be used with our approach to prevent such attacks, since an adversary will be unable to isolate a query to any particular user or group of users. nevertheless, in practice, it is assumed that the geographic grid cells that are used to define the cloaking region will be sufficiently large to encompass a large number of users, to mitigate this risk."
"in fig. 12b, a zoom of the first stage of the step load is presented. in this case qsw-zvs conduction mode with an automatically selected switching frequency of 54khz appears."
"the current event detection is tested in fig. 10 . when the current through the inductor goes below 0a, the current event signal changes its edge. this event arrives to the control after a certain delay (tdet). at that moment the control saves the time that has passed from the beginning of the switching period to that instant, to stablish the following switching period. as can be seen, tdet is smaller than tqsw, which, as seen before, is the period of time where the current goes negative and the output parasitic capacitance of the device is discharged. in fig. 11a, b and c a step load is performed in order to prove the two methodologies proposed and described in this paper."
"among the different wide band gap (wbg) semiconductors, sic mosfets are chosen since they allow high efficiency operation of the power converter even under high switching frequencies and high voltage requirements [cit] . the use of these sic mosfets allows a wide variation of the switching frequency, providing high efficiency at low power, where high frequency is needed to maintain qsw-zvs."
"in order to obtain high performance in a synchronous boost converter, sic mosfets are employed and a variable switching frequency control strategy is implemented. this technique is beneficial to provide zvs, especially at light and medium load operating points. however, a high current ripple is the price to pay of the proposed operation mode, which increases conduction losses, especially at full load [cit] . to take advantage of the different conduction modes and to obtain a high performance of the converter in the full power range, a control strategy switching among modes is proposed"
"firstly, the boost converter block itself, considering it as the hardware used: sic power devices, driver, inductor and capacitors (which will be described in detail in section iv). gate signals (g1, g2) are given to the driver of the boost converter, so it is able to convert a certain dc input voltage value into another output voltage level (vo) under certain load demands."
three conduction modes in a sic-based synchronous boost converter are analysed and compared in this work showing both theoretical and experimental efficiency results with good match between them.
"on one side, changing between modes is tested. the control is able to detect the average value of the inductor current, il_avg and compare it with a certain value, il,1 +/-a certain hysteresis and set a certain switching frequency depending on it."
"the boost converter prototype developed (fig. 8) consists of the mosfet module ccs020m12cm2 from cree® and the 3-channel driver cgd15fb45p1. the value of the inductor is 200μh (one inductor of 600μh per branch, 3 branches in parallel). the fpga under current use is nexys 4 ddr based on a artix 7 from xilinx ®. small capacitor values are used for this application: 4 μf for the input and 12 μf for the output."
"the purpose of the dpwm is generating the proper gate signals required by the boost converter. for that, it needs the duty cycle (d) and the period (t) desired for each switching period. it also sets the mandatory deadtimes needed to avoid short-circuits or any other undesired behaviour."
"bcm is disregarded in the following sections of this work since it does not provide any improvement for any load range compared to the other two conduction modes, as can be seen in fig. 3 ."
"it is worth to remark that the communication between the power stage (boost block) and the control stage (rest of blocks) is isolated providing more reliable working conditions of the system. the communication (gate and error signals provided by the driver) between the fpga and the driver is done through optic fiber, while isolated sensors are used to obtain the output voltage and the inductor current."
"the main object of mux blocks is assuring complete control of the system during the failure procedure test. this means that when working in closed loop (cl) the muxs provide the dpwm with the period or duty cycle calculated by the voltage regulator and the frequency selector (dreg and tselec, respectively). in contrast, during failure procedure tests (open loop, ol) the user can select any desired value (duser and tuser) to analyse the performance of the topology under any load condition."
"the algorithm proposed concedes special attention to current level. to reduce the current ripple and the peak current level through the mosfets and inductor, ccm-hs is preferred for high loads. it also provides easier control since it works at a fixed frequency. when the power decreases and the peak current level of qsw-zvs is similar to that of ccm-hs at full load (considering a certain hysteresis), qsw-zvs is selected and maintained for lower loads to keep high efficiency. the average inductor current value associated to this peak current level is called il,1 in this paper ( fig. 5 and fig. 6 )."
"in order to optimize the performance for the whole power range, a control strategy based on switching among conduction modes for different load levels is applied (fig. 5 and fig. 6 ). the efficiency at high load for these conduction modes are similar for the analysed specifications (calculated using the efficiency models and experimentally validated, see fig. 3 ). it allows a control strategy based on other factors, such as, current ripple or peak current level, which are beneficial strategies during the battery charging process, because during the initial stage of charge, currents are considerably higher, and allow a reduction of the current stress of the power devices. other conditioning factors to stablish a compromise could be considered, such as, electromagnetic interferance (emi) due to frequency variation, but it is not the scope of this work."
analytical models for the estimation of the efficiency of a synchronous boost converter operating in the three specified conduction modes are developed and experimentally validated (fig. 3 ) in a synchronous sic-based boost converter going from 400v to 800v and maximum power rate of 10kw. the switching frequency is 60khz for ccm-hs and varies from 20khz to 200khz [cit] for bcm-zcs and qsw-zvs.
"in fig. 12a, a general view of the step load performed is shown, with a step load going from 3kw to 2.5kw. since both power demands correspond to average currents lower than the threshold, the conduction mode preferred is qsw-zvs."
"2) boundary conduction mode with zero current switching (bcm-zcs). large current ripple (inductance current is zero at the turn-on of s1) and variable switching frequency. switching losses are reduced but, conduction losses are increased [cit] ."
"in fig. 11c, a zoom of the second stage of the step load is presented. in this case qsw-zvs conduction mode with an automatically selected switching frequency of 45khz appears. it is appreciable how vds goes to zero just before the next switching period, avoiding hard switching, yet dealing with high current ripple."
"nowadays, most of the high-power converters are digitally controlled, making possible modifications of the operation mode. in this paper, a control strategy, based on theoretical efficiency models, that changes the conduction mode to achieve the maximum attainable performance of a silicon carbide (sic)-based synchronous boost converter is proposed. although this work is oriented to develop a bidirectional converter able to provide energy storage capability to a pet, the extracted conclusions can also be applied to other applications where a bidirectional converter with high efficiency and high voltage operation is needed. this paper is organized as follows. section ii outlines the main characteristics of the different conduction modes proposed in this work. in section iii an extensive description of the whole topology is presented, highlighting the eventfocused strategy followed to achieve automatic quasi-square wave mode with zero voltage switching and the approach taken to alternate among modes. section iv shows the most relevant experimental results obtained. finally, conclusions are drawn in section v."
"different control strategies have been proposed for boost converter topology (fig. 1a, green converter) . a brief summary of the characteristics of the three analyzed continuous conduction modes (ccm) is presented and their key waveforms are shown in fig. 2. 1) ccm hard switching (ccm-hs). reduced current ripple (inductance current always positive) and constant switching frequency (f). its key advantage is the low current ripple, performing low conduction losses. high switching losses are the main drawback. at light loads, this mode achieves zvs (triangular conduction mode (tcm))."
"in fig. 11b, a zoom of the first stage of the step load is displayed. ccm-hs conduction modeat a fixed switching frequency of 60khz is selected. it is seen that the current through the inductor is always positive and with reduced current ripple."
"power electronics transformers (pets) have been proposed as a semiconductor based alternative to conventional line-frequency transformers (lfts) [cit] . a fully modular three stage approach (ac/dc + dc/dc + dc/ac) appears to be the most popular choice [cit], being very common the use of multilevel converters to develop the ac/dc stage of the pet, as in the case chb-based pet [cit] and mmcbased pet [cit] (fig. 1a) . it is possible to integrate storage systems at the cell level (or other low voltage dc or ac power sources, such as pv panels or wind turbines) [cit] . however, if the voltage level of the cell (usually, around 1 kv) and the voltage level of the storage system are different, the use of bidirectional power converters is mandatory to adapt the energy format (fig. 1b) . the power converter connecting the battery with the pet cell must withstand high voltage providing high efficiency over a wide power range."
"consequently, the conduction mode block discerns whether if ccm-hs or qsw-zvs is the better option depending on the average current of the inductor (il_avg) given by the current sensor."
"a dpwm at constant frequency is easily implemented. constant dead-times and the duty cycle given by the voltage closed loop control are used to generate the gate signals to operate in ccm-hs. however, to assure qsw-zvs (variable frequency) a more careful design of the control is required (figure 7 )."
"besides the control strategy, an event detection strategy followed to assure qsw-zvs operation mode is developed in order to get a variable frequency, showing the expected results in the experimental tests."
"the efficiency results given by the theoretical models of each conduction mode, allow the design of a control based on switching among different conduction modes depending on the load demand to keep the efficiency almost constant in a wide operation range, assuring good performance under certain current levels, which is beneficial for certain applications, i.e, battery charging."
"ccm-hs mode provides low current ripple at the expense of hard-switching behaviour, while qsw-zvs is able to keep efficiency for really low loads with soft switching at the cost of higher current ripples. to take advantage of both conduction modes a digital control is developed to select the better one depending on the inductor current."
"regarding the current sensor, it is especially interesting to show the current event detection in which this work is focused. to obtain this event, the current sensor cq3200 and analog comparator lmv7219 are used. in fig. 9 tests under 3.5kw are performed, with 800v as output voltage and 43khz of switching frequency in order get qsw-zvs. as it can be seen, tdet is shorter than the time needed to achieve zvs tqsw, so the event control method fits the requirements. fig. 7 current through the inductor (in black), gate signals (s1 in blue, s2 in red) and current event detection signal (in green). in relation to the close loop voltage regulator, in fig. 9 a step in the input voltage is shown to prove its dynamic behaviour, going from 400v to 350v, but keeping 800v in the output. a pi regulator is designed for this purpose with a slow time response (see tregulator, which is around 35ms). in this application, fast transient is not required. in contrast, a slow and smooth recovering is desired, not to damage the system, since devices are withstanding high voltages and high currents might be circulating. the pi regulator is further tested under steps in the reference voltage and step loads fitting in all cases the expectations."
"the blocks named voltage regulator, conduction mode and frequency selector are deeply explained in the following sections. they are the key to implement an event control strategy based on variable switching frequency able to adapt to changing load conditions without losing high performance."
"3) quasi-square wave mode with zero voltage switching (qsw-zvs) [cit] . large current ripple (inductance current is negative at the turn-on of s1) and variable switching frequency. full zvs can be achieved for certain relations of input and output voltages [cit], reducing switching losses but also increasing conduction losses."
"in the frequency selector block, a period is chosen depending on the conduction mode. there are no calculations needed when ccm is desired, but the block requires extrainformation from the current sensor block if qsw is the mode selected. s1 conduction time (tons1) is given by the voltage regulator, to ensure the desired output voltage. nevertheless, s2 conduction time (tons2) depends on zero current detection (zcd) to assure qsw-zvs operation. it is difficult to find an adc fast enough to obtain an accurate digital detection of the zcd. therefore, the output of the current sensor used to measure (il) is analogically compared with a reference (lmv7219) providing an event to the fpga. this event is detected by the fpga within a certain delay that remains constant for any frequency (tdet). the value of the following switching period is determined by the exact moment where the fpga detects this current event. after the zcd a certain time (tqsw) where the resonance takes place is set. this time is big enough to allow the energy stored in the inductance discharges the capacitor of the power device achieving zvs. the delay between zcd and the current event does not compromise the accuracy of this method, since this time delay is constant for different switching frequencies and is lower than tqsw. finally, after the deadtime finishes, s1 is turned on, beginning a new period. fixed dead-times to avoid shoot-through, td1 and td2 (included in tqsw) are used."
"in fig. 12c, a zoom of the second stage of the step load is shown. in this case qsw-zvs conduction mode with a switching frequency of 69khz is automatically set."
"in the same way, in fig. 12a, b and c a step load is performed to prove how the control changes the switching frequency when operating always under the threshold il,1."
"we proceed by examining how the probability of successful transmission evolves over time for different rssi values, thus establishing a connection with the insights gained in the previous section. figure 4 shows the experimentally computed probability of successful transmission, for two values of outage thresholds (80 db and 85 db), starting from an initially observed rssi value that is above or below the outage threshold by a given margin (e.g., \"+5\" means that the initial rssi measurement is 5 db above the threshold value). the graph illustrates that the central claim of the previous section continues to hold; namely, the probability of successful transmission still decreases monotonically for \"good\" links (i.e., initially observed rssi is above the threshold) and increases monotonically for \"bad\" links. an additional insight provided by figure 4 is that when the initial rssi value is close to the threshold, the probability converges to a long-term value much more rapidly; in other words, we verify the intuition that links with rssi readings close to the threshold are more volatile (i.e., more likely to cross the threshold)."
"sensory perception-\"this includes visual perception, tactile sensing, and auditory sensing, and involves complex external perception through integrating and analyzing data from various sensors in the physical world.\" [cit] (p. 34) . in this area the performance level is median compared to humans."
"ai is woven into our lives, changing our environment. the short history of ai has shown that developments in ai go hand in hand with our understanding of ourselves. although there is still a long way to go before we can talk about a singularity point, it is almost clear that the next few steps in ai technology (e.g., asi and agi) will bring about a much more powerful machines, flexible enough to resemble human behavior."
"since the gilbert model discussed in section 3 does not capture the fluctuations of rssi value, in this section we move away from any particular model and evaluate the scheduling strategies using the rssi traces directly. we emphasize that the design of our algorithms is still based on the same insights from previous sections, extending the functionality to account for rssi dynamics rather than creating a whole new set of scheduling strategies tailored to a specific set of experimental traces."
"boden [cit] suggests that ai can be used as a test-lab for cognitive science. it raises and exposes psychological questions that were deeply implicit. it suggests new terms, ideas and questions that were otherwise hidden. in that sense we dare say that computation is playing the role of language for cognitive science. similar to the role of mathematics in physics, computation has become a language for constructing theories of the mind. computation is a formal language that imposes a set of constraints on the kind of theories that can be constructed. but unlike mathematics, it has several advantages for constructing psychological theories: while mathematical models are often static, computational models are inherently process-oriented; while mathematical models, particularly in psychology, are content-independent, computational models can be content-dependent; and while computational models are inherently goal-oriented, mathematics is not. [cit] ."
"\"an ecological approach to cognition is based on an autonomous system that learns by interacting with its environment. generalization in this regard is related to how effectively automation is able to anticipate contextual changes in an environment and perform the required context switches to ensure high predictability. the focus is not just in recognizing chunks of ideas, but also being able to recognize the relationship of these chunks with other chunks. there is an added emphasis on recognizing and predicting the opportunities of change in context.\" (para. 11)."
"artificial social intelligence is also concerned with the environment of the intelligent agent, in particular its social environment. however, there is a difference between the theory of asi and embodied cognition. asi is rooted in social science, and the notion of a 'social brain'. embodied cognition is rooted in the intersection of cognitive sciences and linguistics, it is also based on philosophical grounds. embodied cognition focuses on universal principles of understandings, or in other words, the cognitive architecture."
"the rest of the paper is structured as follows. section 2 discusses the related work, followed by section 3 that presents our system and wireless channel model and formulates the slot scheduling problem. sections 4 and 5 present our proposed scheduling solutions with and without the observation of actual rssi values, respectively. finally, section 6 concludes the paper."
"the main purpose of an active object recognition or verification system is to improve the processing time and accuracy required to determine an object's identity. in addition to this, our system also provides a measurement for how certain the system is of an object's identity. test images were captured with the relevant objects in occluded locations in cluttered environments as shown in figure 4 ."
"training: developing 'personalities' for ai requires considerable training by diverse experts. for example, in order to create cortana's personality, microsoft's ai assistant, several human trainers such as a play writer, novelist and poet, spent hours in helping developers create a personality that is confident, helpful and not too 'bossy'. apple's siri is another example. much time and effort was spent to create siri with a hint of sassiness, as expected from an apple product."
"in this paper we present a review of recent developments in ai towards the possibility of an artificial intelligence equals that of human intelligence. so far ai technology has shown a stepwise increase in its capacity and in its complexity (section 2). the last step took place several years ago, due to increased progress in deep neural network technology. each such step goes hand in hand with our understanding of ourselves and our understanding of human cognition (section 3). indeed ai was always about the question of understanding human nature."
"when logged in with a student account, the user has permissions to search, read and navigate through the course content. moreover, student has the same profile parameters and functions as the lecturer profile."
"based on a white paper that universal robots-one of the leading companies in the robot market-has published [cit], gonzalez lists the seven most common applications for cobots. one of them, for example, is \"pick and place\": \"a pick and place task is any in which a workpiece is picked up and placed in a different location. this could mean a packaging function or a sort function from a tray or conveyor; the later [sic] often requires advanced vision systems.\" [cit] (para. 3)."
"our intelligence, as yudkowsky [cit] clarifies, \"includes the ability to model social realities consisting of other humans, and the ability to predict and manipulate the internal reality of the mind.\" (p. 389). another way to put it is through mead's concept of the 'generalized other' [cit] . as dodds, lawrence & valsiner [cit] explain, \"to take the role of the other involves the importation of the social into the personal, and this activity is crucial for the development of self-consciousness and the ability to operate in the social world. it describes how perspectives, attitudes and roles of a group are incorporated into the individual's own thinking in a way that is distinct from the transmission of social rules, and in a way that can account for the possibility of change in both person and society.\" (p. 495, our emphasis)."
"lakoff and johnson's [cit] argument is that the more abstract meaning is the secondary one, it is the derived meaning. we derive such meanings from the simpler ones, the embodied and primitive meanings. it is a bottom up process."
"\"by contrast, psychology historically has made progress mainly by accumulating empirical phenomena and data, with far less emphasis on theorizing of the sort found in artificial intelligence. [cit] s [cit] s.\" [cit] (p. 2)."
"we highlight that all the techniques proposed in this paper operate by varying the transmission schedule of sensor nodes only, and the performance improvements are achieved without consuming any additional energy (e.g., by retransmissions) or increasing the latency of incoming packets. it is worth noting that in a separate work [cit] we have shown that variable scheduling techniques continue to perform well when retransmissions are employed to further increase reliability. more specifically, if the retransmission mechanism follows the same scheduling principles, taking into the account the wireless link state information of sensor nodes, then the reliability metric can be further improved for the same amount of extra energy spent on retransmissions."
"another recent and interesting project deals with machine touch. in the paper \"learning dexterous in-hand manipulation\" [cit], a team of researchers and engineers at openai have demonstrated that in-hand manipulation skills learned with reinforcement learning in a simulator can evolve into a fairly high level of dexterity of the robotic hand. as they explain: \"this is possible due to extensive randomizations of the simulator, large-scale distributed training infrastructure, policies with memory, and a choice of sensing modalities which can be modelled in the simulator.\" (p. 15). the researchers' method \"did not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity\". (p. 1)."
"an important concept to dwell on is that of artificial general intelligence (agi). agi constitute a new step towards strong ai. general intelligence is not a fully well-defined term, but it has a qualitative meaning: \"what is meant by agi is, loosely speaking, ai systems that possess a reasonable degree of self-understanding and autonomous self-control, and have the ability to solve a variety of complex problems in a variety of contexts, and to learn to solve new problems that they didn't know about at the time of their creation.\" [cit] (p. vi)."
"a tdma mechanism typically involves splitting time into super-frames, or rounds. at the start of each round, all the nodes turn their radios on to listen for the beacon packet, transmitted by the hub to convey important network management information and assist with time synchronization. this fact can be used in implementing a variable tdma schedule, namely, by informing all the nodes (either within the beacon packet itself or in a separate packet transmitted immediately after the beacon) about the slot allocations for the upcoming round. thus, any beacon-based tdma mac protocol (including the upcoming 802.15.6 standard [cit] ) can support a variable slot allocation scheme."
"a general content repository: a platform that can be both educational and non-educational, such as youtube and screen toaster. an online textbook repository: a refinery that provides textbooks for all levels and various courses, such as connexions by rice university. a module repository: it offers pedagogical support through providing modules, such as merlot in california state university. a media-focused repository: this one contains various audio-visual items which are useful for study purposes as well as simulations which facilitates a glimpse of real-life training (examples include itunes by apple and intelecom online). as for the advantages, wiley has listed the following advantages of lors, especially when utilised in institutions of higher education [cit] :"
"as mentioned earlier, when the scheduling is not based on the instantaneous channel state information at the start of the round, it is no longer necessarily true that a scheduling strategy that maximizes the expected number of successful links in each round will also achieve the best long-term performance. this is because the schedule choice in a given round has an indirect effect on the information that will be available to the scheduler at the next round. thus, a certain schedule may maximize the expected number of successful links in the upcoming round but leave the scheduler with \"inferior\" information, in some sense to make its choice in the subsequent round."
"3. slots are assigned by increasing rssi order in the \"early\" group and by decreasing order in the \"late\" group. if a failure happened in the previous round, that node is assigned a large negative rssi value, implying that it will be scheduled as late as possible."
"lors are reusable, accessible and adaptable: they are easy to blend with other resources, in digital or in traditional forms. they are adaptable with regard to the demands of local curricula. they can be designed for use on different platforms; they are generally cost-effective."
"explaining: as ai develops, it reaches results through processes that are unclear to users at times, a sort of internal 'black box'. therefore, they require expert, industry specific 'explainers' for us to understand how ai reached a certain conclusion. this is especially critical in evidence-based industries such as medicine and law. a medical practitioner must receive an explanation of why an ai assistant gave a certain recommendation, what is the internal 'reasoning' that led to a decision. in a similar way, law enforcement investigating an autonomous vehicle accident, need experts to explain the ai's reasoning behind decisions that led to an accident [cit] ."
"natural language processing-\"this consists of two distinct parts: natural language generation, which is the ability to deliver spoken messages, including with nuanced human interaction and gestures, and natural language understanding, which is the comprehension of language and nuanced linguistic communication in all its rich complexity.\" [cit] (p. 34). as for natural language generation, although there is progress in this area (such as google duplex), the levels of performance according to the report are at best median. when it comes to natural language understanding, there is a long way ahead of us."
"for making asi come true, there are some fundamental steps which needs to be solved [cit] . firstly, there is a need to discover the principles of socio-culture interactions in which the asi system could have a role. in order to formulate those principles there is considerable importance for conducting large data-driven studies aimed at validating these principles, as well as identifying and characterizing new behavioral traits. such studies are already being conducted, using the enormous amounts of socially grounded user data generated and highly available from social media; as well as the significant advancements in machine learning and the wide variety of data-analysis techniques. one such project is \"mark my words!\" [cit] . this project demonstrates the psycholinguistic theory of communication accommodation according to which participants in conversations tend to adapt to the communicative behavior patterns of those with whom they converse. the researches have shown \"that the hypothesis of linguistic style accommodation can be confirmed in a real life, large scale dataset of twitter conversations.\" (p. 754). a probabilistic framework was developed, which allowed the researchers to measure \"accommodation and, importantly, to distinguish effects of style accommodation from those of homophily and topic-accommodation.\" [cit] ."
"in a nutshell, lida is a modified version of the old copycat architecture suggested years ago by hofstadter [cit] . it is based on the attempt to understand consciousness as a working space for many agents. the agents compete one another and those that dominate the workspace are identified as the ones that constitute our awareness. the process is dynamic, information flows in from the environment, and action is decided by a set of heuristics, which are themselves dynamic."
"with the design of three continually active incremental learning mechanisms-perceptual learning, episodic learning and procedural learning-the researchers have laid the foundation for a working model of cognition that produces a cognitive architecture capable of human like learning. as the authors [cit] explain:"
new images were only captured when the belief was below a pre-defined threshold. this reduces the computational time because only the minimal number of images will be processed to perform the task.
cially true if they are occluded or appear in cluttered environments. there may also be a great variety of relevant objects with significant similarity. in such cases multiple viewpoints are necessary for recognition [cit] . active vision allows a robot to actively search an environment to obtain more informative views to increase the accuracy of object identification and verification.
"in our view, the best way to describe the developmental process of ai so far is as a stepwise incremental progress, and using an analogy from physics, ai \"percolates\" into our lives. it could indeed make a phase transition into a higher level of complexity above our own, a process we will shortly discuss. but first we want to describe the ongoing process of stepwise incremental ai."
"our contribution in this work is as follows. as a first step, we formulate the optimal variable scheduling problem based on a simple, two-state (on/off) gilbert model of wireless links, and propose a number of scheduling strategies for the hub node based only on its observation of nodes' transmission outcomes (success or failure). through simulation, based on gilbert parameter ranges extracted from a set of experimental received signal strength indicator rssi traces [cit], we evaluate these strategies numerically in terms of the reduction of loss rate as compared with a static tdma allocation, and uncover a number of important insights. subsequently, we consider the use of the additional information that may be available from actual rssi readings of successful transmissions (rather than simply the binary success/failure outcome), and we extend the above strategies with further heuristics that aim to capture the dynamics of rssi fluctuations, which are shown to reduce the loss rate even further. in particular, we make the encouraging finding that near-optimal performance can be consistently attained with a simple (so-called \"flipping\") strategy that orders the transmissions in each round according to the order of the previous round and observed rssi values, does not require any a priori knowledge of statistical parameters of each link, and is trivial to implement and compute in real time."
"lemma (1) captures the intuition based on a fundamental property of the function p(k) (which holds more generally in markov chains with multiple states as well), namely, that if a probability of a given state at some time is higher (lower) than its steady-state value, then it will monotonically decrease (increase) towards that steady-state value. consequently, it is best to bring the links initially known to be \"good\" forward to have their transmissions as early as possible, while deferring all \"bad\" links to the end of the round. however, the question of ordering within each of the subsets remains, and unless all links have identical markov transition probabilities, the expected number of successful slots will depend on the chosen order within each subset. accordingly, we define the following scheduling approaches that will be evaluated further:"
"the aim of the automatic view selection algorithm is to select the 'next best viewpoint' for object recognition and figure 2 . a schematic of the modified vocabulary tree verification i.e. the viewpoint which will provide the most amount of useful information to optimally complete the process. the proposed scheme uses a vocabulary tree [cit] . this structure is typically used in bag-of-words object recognition and visual loop closure approaches as an efficient alternative to sivic and zisserman's video google [cit] . the idea is to gather all features in the training set, cluster them hierarchically and calculate a uniqueness weighting for each feature. the vocabulary tree data structure was designed for large volumes of data and thus will scale easily if more objects are to be added to the database."
"yet, an example for an effective implementation of these capabilities (and more) is aida (http: //aidatech.io/) [cit], a virtual assistant that is being used by seb, a major swedish bank. aida interacts with masses of customers through natural-language conversations, and therefore has access to vast amounts of data. \"this way she can answer many frequently asked questions, such as how to open an account or make cross-border payments. she can also ask callers follow-up questions to solve their problems, and she's able to analyze a caller's tone of voice and use that information to provide better service later.\" [cit] (para. 16)."
"a third project worth mentioning is schmidhuber's gödel machines [cit] . schmidhuber describe these machines as \"the first class of mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers. inspired by kurt gödel [cit], such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code. the searcher systematically and in an asymptotically optimally efficient way tests computable proof techniques (programs whose outputs are proofs) until it finds a provably useful, computable self-rewrite.\" (p. 1)."
"similar reciprocal relationships are now beginning to form between social sciences and artificial intelligence to become the field of artificial social intelligence (asi). asi is an interdisciplinary science, which was introduced years ago by brent and others [cit], and is only now becoming prevalent. asi is a new challenge for social science and a new arena for the science of ai. it deals with the formalization of delicate social interactions, using it in ai to implement social behavior into robots. the prospects for social scientists were suggested years ago by anderson [cit] :"
"to sum-up the report, [cit] notes that from a mechanical point of view, they are fairly certain that perfection can be achieved. because, already today, through deep reinforcement learning for example, robots can untie shoelaces and remove a nail from the back of a hammer. however, from the cognitive point of view, although the robot's \"intelligence\", has progressed, this is still where the greatest technical challenge lie:"
"there is a clear distinction between agi and narrow ai research. the latter is aimed at creating programs that specialize in performing specific tasks, such as ordering online shopping, playing go, diagnosing diseases or driving a car. but, despite their great importance and popularity, narrow ais core problem is that \"they are inherently narrow (narrow by design) and fixed. whatever capabilities they have, are pretty much frozen in time. it is true that narrow ai can be designed to allow for some limited learning or adaptation once deployed, but this is actually quite rare. typically, in order to change or expand functionality requires either additional programming, or retraining (and testing) with a new dataset.\" [cit] (para. [cit] ."
"in our framework, the observer component updates the system's object belief. the vocabulary tree used in the view selection component is altered to store the statistics necessary for the observer component. this is explained in the next subsection."
"take machine vision as an example. the creation and assimilation of visual capabilities that surpass human vision in cameras, have been relatively easy. however, the more complex part was to add ai technology to the cameras. one such project is landing.ai (https://www.landing.ai/) [cit], formed by andrew ng, a globally recognized leader in ai. this startup focuses on solving manufacturing problems such as quality control (qc). it offers machine-vision tools to detect microscopic defects that may be found in products such as circuit boards, and which the human eye simply cannot detect."
"all the above suggests the emergence of a new interdisciplinary discipline. the main concern of this new field of science is the formalization of delicate social modules, using them in ai to implement social awareness (perhaps a type of social common sense understanding) and social behavior into robots. because of the dynamic nature of social interactions, these asi systems face difficult challenges, some of which are not even predictable. in order to address these challenges, asi systems will have to be dynamic by continuously reviewing and evolving their interaction strategies in order to adapt to new social situations. moreover, it is essential to examine and assess these strategies in as many contexts as possible, in which ongoing, continuous interactions are taking place."
"moreover, perez [cit] clarifies that \"effective conversation requires not only understanding an external party but also the communication of an automaton's inner model. in other words, this conversation requires the appropriate contextualized communication that anticipates the cognitive capabilities of other conversing entities. good conversation requires good listening skills as well as the ability to assess the current knowledge of a participant and performing the necessary adjustment to convey information that a participant can relate to.\" (para. 16). for perez, the ability to effectively perform a conversation with the environment is the essence of agi. interestingly enough, what most agi research avoids is the reality that an environment is intrinsically social-i.e., that there exist other intelligences."
"in our scenario, we set the data generation interval to be equal to the length of a superframe; in other words, each sensor node has one new data sample per round, which it transmits to the hub. once the transmission attempt is completed, the sensor immediately goes to sleep as an energy-saving measure, regardless of the success of the data delivery. generally, an arq mechanism employing acknowledgments from the hub and retransmissions by the sensor could be employed to increase the data delivery success rate. however, we do not consider such a mechanism in this work, as our focus is on the delivery rate improvements that can be achieved through variable scheduling alone, without expending additional energy for retransmissions (the performance of variable scheduling and its benefits in the presence of retransmissions are discussed in our separate study [cit] ). we stress that, while the sensors do not receive direct feedback on the outcome of their transmissions, the hub still collects and employs this information for scheduling decisions in future tdma rounds. more specifically, section 4 considers scheduling strategies that only use the binary outcome ( success of failure) of the transmission from each sensor, while section 5 discusses scheduling that uses the actual received signal strength value as well."
"the lida architecture is partly symbolic and partly connectionist; part of the architecture \"is composed of entities at a relatively high level of abstraction, such as behaviors, message-type nodes, emotions, etc., and partly of low-level codelets (small pieces of code). lida's primary mechanisms are perception, episodic memory, procedural memory, and action selection.\" [cit] (p. 1)."
"note that if the rssi reading of each node remains constant across consequent rounds, then the algorithm will behave exactly as simple flipping. from figure 6 it is evident that sorted flipping consistently improves the performance of flipping, and the gain is most pronounced for large values of the attenuation outage threshold."
"in conclusion, general intelligence is a complex phenomenon that emerges from the integration of several essential components. \"on the structural side, the system must integrate sense inputs, memory, and actuators, while on the functional side various learning, recognition, recall and action capabilities must operate seamlessly on a wide range of static and dynamic patterns. in addition, these cognitive abilities must be conceptual and contextual-they must be able to generalize knowledge, and interpret it against different backgrounds.\" [cit] (p. 147)."
"in other words, the order of the links in the \"good\" group should be reversed while the order of the links in the \"bad\" group should be kept the same as in the last round."
"intelligence, in general, \"implies an ability to acquire and apply knowledge, and to reason and think, in a variety of domains\" [cit] (p. 15). in other words, intelligence in its essence has a large and dynamical spectrum."
"cognitive architecture (ca) research \"models the main factors participated in our thinking and decision and concentrates on the relationships among them. in computer science, ca mostly refers to the computational model simulating human's cognitive and behavioral characteristics. despite a category of loose definition, cas usually deal with relatively large software systems that have numerous heterogeneous parts and subcomponents. typically, many of these architectures are built to control artificial agents, which run both in virtual worlds and physical robots.\" [cit] (p. 1)."
"note that p(k) is monotonically increasing (decreasing) if p(0) is lower (higher) than the steady-state value s. we point out that the same monotonicity property, which will be important in our subsequent analysis, holds in general for markov processes with multiple states as well."
"the idea of dynamic assignment of transmission slots has been previously studied in the context of the 802.15.4 communication standard, where the network coordinator is able to vary the allocation of guaranteed transmission slots (gts) on a per-round basis and convey it to the sensor nodes in a periodic beacon packet. initially it was noted that simple gts allocation mechanism proposed in the standard is ineffective and leads to low bandwidth utilization due to the fact that large parts of the gts remain unused [cit] . it was shown that dynamic allocation of gts on a per-round basis, where each slot is shared among several nodes, not only improves bandwidth utilization but also leads to better quality of service (qos) and reduces the number of packets dropped due to overflow. a more advanced set of gts allocation schemes, where the coordinator is able to adapt to varying bandwidth requirements of sensor nodes, was shown to improve waiting times and fairness as compared with a naive static method [cit] ."
3. fusion -each feature provided by the previous step is cascaded through the vocabulary tree by selecting children with the closest centroids. the leaf node associated with each feature contains a density as described above. every feature's density is fused recursively with the prior using
"brooks and dennett cooperated in a project involving a humanoid named cog [cit], where they tried to implement the above ideas by letting the cog computer (with a torso, camera eyes, one arm, and three fingers) interact with its environment, trying to learn new things as if it is a newborn. brooks used a 'subsumption architecture' where several simple modules were competing for dominance. a few years later, the science of embodied cognition was born [cit] and reached similar conclusions from a different point of view, the point of view of cognitive psychology."
"moreover, artificial intelligence has become a kind of theoretical psychology. researchers who sought to develop a psychological theory could become artificial intelligence researchers without making their marks as experimentalists. thus, as in physics, two branches of psychology were formed-experimental and theoretical-and cognitive science has become the interface where theorists and experimentalists sort things out [cit] ."
"as we have argued above, we believe that the next step to take to make human and machine intelligence come closer together, is to focus on the social aspect of human intelligence and on the ways to integrate social behavior in machines."
"the two focus areas of active object recognition are: selecting the next best viewpoint and integration of relevant information. for selecting the next best viewpoint many systems simply use active vision to select the sequence in which a set of pre-captured images should be processed for recognition [cit] . often these sequences are of fixed length and optimization of appraisal time is not considered. to the authors' knowledge, no systems explicitly consider the possibility of occlusion or extremely cluttered environments."
"cognitive capabilities-\"a range of capabilities is included in this category including recognizing known patterns and categories (other than through sensory perception); creating and recognizing novel patterns and categories; logical reasoning and problem solving using contextual information and increasingly complex input variables; optimization and planning to achieve specific objectives given various constraints; creating diverse and novel ideas or a novel combination of ideas; information retrieval, which involves searching and retrieving information from a large range of sources; coordination with multiple agents, which involves interacting with other machines and with humans to coordinate group activity; and output articulation and presentation, which involves delivering outputs other than through natural language. these could be automated production of pictures, diagrams, graphs, or mixed media presentations.\" [cit] (p. 34)."
"in order to gain initial insight into the design of efficient variable scheduling, we assume that each wireless link between a sensor node and the hub evolves according to a discrete markov process, in which each state is classified as either \"good\" (allowing transmissions to be received successfully) or \"bad\" (in which no reception is possible). a similar markov model for wireless links in ban environments has been suggested by the ieee ban task group [cit] and shown to capture some of the key aspects of the channel dynamics in bans [cit] ."
"could these assumptions be wrong? could it be that some part of our intelligence in embedded in our body? we interact with the world with our body, we perceive with our body, we sense with our body. could it be that symbolic intelligence is not enough?"
"eliezer yudkowsky offered the first attempt at explaining ai alignment in his seminal work on the topic, \"creating friendly ai\" [cit], and he followed this up with a more nuanced description of alignment in \"coherent extrapolated volition\" [cit] . nearly a decade later stuart russell began talking about the value alignment problem, giving ai alignment its name and motivating a broader interest in ai safety. since then numerous researchers and organizations have worked on ai alignment to give a better understanding of the problem."
"the unique roles of human values presented here, have been linked to the workplace environment, but they are undoubtedly relevant to all spheres of life. as we claimed earlier, we are at the beginning of a new developmental stage in ai, the one of artificial social intelligence. within this realm, new questions concerning human values may arise."
"there were computer scientists that took dreyfus' stand seriously. brooks [cit] came up with the idea that there is no need for knowledge representation at all: \"the key observation is that the world is its own best model\" (p. 6)."
"\"narrow ai systems cannot adapt dynamically to novel situations-be it new perceptual cues or situations; or new words, phrases, products, business rules, goals, responses, requirements, etc. however, in the real world things change all the time, and intelligence is by definition the ability to effectively deal with change.\" [cit] (para. 6)."
"it was suggested by boden [cit] that we can bridge the gap between 'humanistic' approach to cognitive science (in the sense discussed above) and physical mechanism. the way to do so is by introducing an inner representation of the self into the computer. intentionality and meaning could be aimed (given a context) into this inner representation; the reduction or mechanism of the intentionality will be enabled by the design or architecture of the inner representation. hence, in order to describe what is going on in the computer, the language of intentionality will be the most appropriate, in the same sense that we talk about our dog's intentions when we wish to describe or explain its behavior, without the need for a behavioristic language, or other physical terms. it will not be 'natural' or efficient to describe the action of the computer in the language of the state of its switches, we will say that this particular action was 'intended for' to comply with the 'state of mind' that the computer had. this sounds a somewhat pretentious goal, however it is based on the assumption that any future advancement in ai must stand on a basic cognitive architecture, much more basic and deeper than what we have today."
reliable object classification is essential for robots to perform tasks in human environments. many recognition systems operate on single views [cit] . in real-world situations a single viewpoint may be of poor quality and simply not contain sufficient information to reliably recognise or verify the object's identity unambiguously. this is espe- * this work was funded by the mobile intelligent autonomous systems (mias) group in the council for scientific and industrial research (csir) south africa.
"while ais like cortana are essentially digital entities, there are other applications where \"intelligence is embodied in a robot that augments a human worker. with their sophisticated sensors, motors, and actuators, ai-enabled machines can now recognize people and objects and work safely alongside humans in factories, warehouses, and laboratories.\" [cit] (para. 16, our emphasis)."
"\"dreamcatcher is a generative design system that enables designers to craft a definition of their design problem through goals and constraints. this information is used to synthesize alternative solutions that meet the objectives. designers are able to explore trade-offs between many alternative approaches and select design solutions for manufacture.\" (https://autodeskresearch.com/projects/ dreamcatcher) [cit] ."
"it is well established now that there are sets of brain regions that are dedicated to social cognition. it was first shown on primates [cit] and later on humans [cit] . as frith [cit] explains: \"the function of the social brain is to enable us to make predictions during social interactions.\" (p. 67). the social brain includes a variety of mechanisms, such as the amygdala which is activated in case of fear. it is also connected with the mechanism of prejudice, stereotyping, associating values with stimuli. it concerns both people-individual and group-and objects. another such mechanism is the medial prefrontal cortex, which is connected with the understanding of the other's behavior in terms of its mental state, with long term dispositions and attitudes, and with self-perception about long term attitudes."
"part of our intelligence is therefore somewhere in our bodily interactions with the world. in addition to our senses of sight, smell, hearing etc., we have 'senses' of time, space, surroundings, etc. the discovery of neural place cells [cit] emphasizes the embodiment of our sense of space. a good example to illustrate embodiment is the proven connection between movement and intelligence in baby development. free movement, such as rolling, crawling, sitting, walking, jumping, etc., is associated with the development of the frontal cortex, the area where higher-order thinking occurs [cit] ."
"one of the most complex challenges that ai faces is the issue of embodiment. at first, one should recognize that part of our intelligence is indeed embodied. we are physically situated in the world; we move in space, perceive, feel and communicate through our bodies. we argue that embodied cognition should be dealt with as a new challenge to ai and not as an imposition."
"we first point out that, despite the delayed information, it is still true that the probability of being in a good state continues to monotonically decrease over time for any link that was known to be good in the previous round, and, conversely, monotonically increase for any link that was bad. consequently, the result of lemma (1) continues to hold; in other words, it is still always better to schedule all the links in the \"good\" group ahead of the \"bad\". however, it is no longer true that, if all the links have identical markov transition parameters, then the ordering within each group does not matter. our next result explains this fact in detail and presents the optimal transmission schedule in this case."
vocabulary trees have been traditionally used in object recognition and simultaneous localization and mapping (slam) approaches for matching similiar images and for loop closure [cit] . our application of the data structure differs in that we use it to calculate weightings for features to determine the next best viewpoint. we also use it to generate statistics to update the object belief. following [cit] our system relies on a bayesian framework for updating a belief function.
"another critical aspect of social survival is the requirement for cooperative behavior. but as perez [cit] argues, effective prediction of an environment is an insufficient skill to achieve cooperative behavior. the development of language is a fundamental skill, and conversations are the highest reflection of intelligence. \"they require the cognitive capabilities of memory, conflict detection and resolution, analogy, generalization and innovation.\" (para. 15). but at the same time it is important to keep in mind that languages are not static-they evolve over time with new concepts."
"as explained earlier, requiring the nodes to actively sample their channels at the start of each round would impose an unacceptably high overhead, in terms of both time and energy. we now move away from that assumption and discuss scheduling algorithms that rely only on the outcome of the communication during the previous round, which is available anyway and does not require additional effort to obtain."
"the ulrs implements three main roles, defined as student, teacher and administrator. a user registers in the ulrs by adding details such as first name, surname, gender, address, email, date of birth, country and hometown and university. the users can update the registered details at any time, including his or her interest fields."
"these and other early attempts failed to reach their original goals, and in the view of most ai researchers, failed to make dramatic conceptual or practical progress toward their goals. some (gps for example) failed because of exponential growth in computational complexity. however, more contemporary agi studies and projects offer new approaches, combining the previous knowledge-both theories and research methods-accumulated in the field."
"the view selection component of the proposed active vision system functions as follows. for object verification, an input image is provided to the system with the necessary object hypothesis. the input image is matched using standard sift matching and a hough transform to the hypothesized object's training images to determine the closest training image which provides the initial pose estimate of the object. relative to the pose estimate the view selection component selects a view that it has not previously visited and has the largest uniqueness weighting for that object."
"the introduction of advanced technologies such as web-based and intelligent agents can render the model lightweight and flexible, thus the intelligent e-learning system becomes a reality. this specific architecture has provided a flexible integration model where all the learning applications and components are freely connected and easily distributable over the internet. in addition, the use of agents and intelligent content customization can be adapted for the various needs of individual learners. although, we have not presented a complete artificial intelligence (ai) agents, however, search agents are inspired from ai applications."
"we emphasize that the simple markov model outlined above is only used to develop an initial set of scheduling techniques that are evaluated in section 4, while the strategies in section 5 are designed and evaluated on experimental rssi traces [cit] directly."
"from the social point of view, mead [cit], in his book, mind, self and society, defines the \"social organism\" as \"a social group of individual organisms\" (p. 130), or in modern language, as an emergent phenomenon. this means that each individual, as an organism in itself, is also a part of a larger system, the social organism. hence, each individual's act must be understood within the context of some social act that involve other individuals. the social act is therefore viewed as a dynamic and complex system within which the individual is situated. as such, the social 'organism' actually defines the individual acts, that is, within it these acts become meaningful."
"furthermore, lors are highly versatile because they contain small portions of learning materials that can be updated in a straightforward manner. consequently, learning processes which are based on lor have become a new 'school', since lors allow the possibility to create various learning and knowledge-seeking patterns. even though it is not possible to fully equate lors to conventional learning and teaching methods in all countries, they have been considered to sufficiently match an ideal conception of modern learning. in fact, there is one simple prerequisite for lor-based learning: an internet connection. that is why it is categorised the designations 'online learning' or 'e-learning'."
"general search -the general search uses the exact search method for search of a keyword from the learning objects database on search text. general search provides search on learning object name, title, content type, and subject area filters. search using terms relate to the phrases you want to search for -this search performs a text based exact phrase search in the database using learning object name, title, content type and subject area filters. search by frequency of search phrases -the search text is used to query the database. results are presented based on the frequency of search text."
"interest points, which have the advantage that the representation is more robust to occlusions, clutter and noise, have successfully been used in 3d object recognition [cit] . our system uses the scale invariant feature transform (sift) [cit] detector and descriptor to extract relevant object features. sift is robust to changes in illumination and affine transformation. the structure of our system is, however, not sift dependent and thus any other descriptor or detector can be used for feature extraction."
"using multiple views for object recognition improves the accuracy of the recognition system [cit] . the focus of active object recognition and verification is how to select the \"next best viewpoint\" which will provide the most amount of information to complete the task as quickly and as accurately as possible. most active object recognition systems are based on selecting viewpoints that will minimise ambiguity using shanon entropy [cit] or dempster-shafner theory [cit], minimise a weighted error [cit] or maximise a defined activation function [cit] ."
"novamente is extremely innovative in its overall architecture, which seeks to deal with the difficulty of creating a \"whole brain\" in a completely new and direct way. the basic principles on which the design of the system is founded are derived from the \"psynet model\"-an innovative complex-systems theory of mind-which was developed by goertzel [cit] . \"what the psynet model has led us to is not a conventional ai program, nor a conventional multi-agent-system framework. rather, we are talking about an autonomous, self-organizing, self-evolving agi system, with its own understanding of the world, and the ability to relate to humans on a \"mind-to-mind\" rather than a \"software-program-to-mind\" level.\" [cit] (pp. [cit] ."
"furthermore, we have conducted additional evaluations directly on an experimental set of rssi traces, reinforcing the main scheduling principles and extending the flipping strategy to account for rssi information, making more informed scheduling decisions. it is important to emphasize that, while these strategies were inspired by a two-state markov channel model, their operation is based only on the outcome of each transmission directly (i.e., the success/failure of the transmission, or the measured rssi value), and does not require the a priori knowledge of any statistical parameters of the links. we have shown that up to 45% of all losses caused by bad channel state, can be avoided simply by smarter allocation of tdma slots if instantaneous channel state is available; or, in a more realistic setting where only the previous round outcomes are used for scheduling purposes, up to 10% of all transmission failures can be prevented."
the aim of this study is to design a proposed model for intelligent e-learning repository management system based on an earlier paper (e-learning repository system for sharing learning resources among saudi universities). the lecturers in saudi universities have urgent need for e-learning materials on repository for enhanced e-learning experience in the saudi universities.
"\"while machines can be trained to perform a range of cognitive tasks, they remain limited. they are not yet good at putting knowledge into context, let alone improvising. they have little of the common sense that is the essence of human experience and emotion. they struggle to operate without a pre-defined methodology. they are far more literal than people, and poor at picking up social or emotional cues. they generally cannot detect whether a customer is upset at a hospital bill or a death in the family, and for now, they cannot answer \"what do you think about the people in this photograph?\" or other open-ended questions. they can tell jokes without really understanding them. they don't feel humiliation, fear, pride, anger, or happiness. they also struggle with disambiguation, unsure whether a mention of the word \"mercury\" refers to a planet, a metal, or the winged god of roman mythology. moreover, while machines can replicate individual performance capabilities such as fine motor skills or navigation, much work remains to be done integrating these different capabilities into holistic solutions where everything works together seamlessly.\" (pp. 26-27)."
"as tegmark [cit] explains: \"aligning machine goals with our own involves three unsolved problems: making machines learn them, adopt them and retain them. ai can be created to have virtually any goal, but almost any sufficiently ambitious goal can lead to subgoals of self-preservation, resource acquisition and curiosity to understand the world better-the former two may potentially lead a superintelligence ai to cause problems for humans, and the latter may prevent it from retaining the goals we give it.\" (p. 389)."
"as a result of lemma (2), we add another strategy to our repertoire: schedule all links that were good in the previous round ahead of the bad ones; and within the good group only, invert the order of the transmissions from the last round. we refer to this approach as the flipping strategy, since for a given set of links, the order of their transmissions will be \"flipped\" among consecutive rounds as long as they are successful (which is the most common outcome when the link steady state values are high). while the flipping strategy is optimal only in a system with homogeneous links, it has several other desirable characteristics-specifically, it is trivially easy to compute and it does not require the knowledge of the markov parameters of the individual links. thus it becomes interesting to apply it as a heuristic in non-homogeneous scenarios, i.e., with diverse channels whose markov parameters take a range of different values."
"questions that we should ask includes; is the use of the same terms and the same language in ai and cognitive sciences only an analogy? could it imply something deeper? can we insert true 'intention' and true 'meaning' into computer agents? how can we define such terms in ai? in fact, this is the main question of strong ai. this would bring ai and cognitive science much closer."
"one of the main challenges in aligning ai with values is to understand (and to agree upon) what exactly these values are. there are many factors that must be taken into account which depend mainly on context-cultural, social, socioeconomic and more. it is also important to remember that humanity often does not agree on common values, and even when it does, social values tend to change over time."
"the robotic arm manipulator for which this system was designed, does not have a complete 360 degree range. to combat this problem we would like to create 3d models of the objects, so in the event that the next view cannot be executed by the arm, the object itself can be manipulated to achieve the desired viewpoint."
"at first, ai was mistakenly identified with the mechanical psychological viewpoint of behaviorism. the physicalism of stimulus and response looked similar to the action of computers, a reduction of man into its gears [cit] . in psychology, a more 'humanistic' view of the science was demanded. it was agreed by all 'humanistic' psychologists that a good theory should be irreducible, its terms cannot be reduced to simple physical constituents, and that terms such as 'intention' should have a major part in the theory. moreover, any action should have some meaning to the actor, and the meaning should be subjective. the 'humanistic' approach to psychology was a scientific revolution against positivistic psychology (in the kuhnian sense) [cit] (p. 396). it turned out that ai came to be very similar to the 'humanistic' viewpoint. both ai and cognitive science were beginning to ask similar questions and to use many similar terms."
one of the biggest challenges for ai is the challenge of embodied cognition. if ai could surpass this hurdle it will be very close to true human intelligence. embodied cognition was first presented as the main reason why ai is impossible. we propose to view embodied cognition as a step towards better ai and not as an imposition. let us make a small detour to the history and philosophy of computation.
"we now consider the slot assignment strategy by the hub assuming that the wireless links evolve according to the gilbert model described in the previous section. we define an optimal assignment strategy as one that maximizes the long-term throughput from the sensor nodes, or, in other words, minimizes the expected percentage of slots that result in transmission failures. while this definition seems straightforward, the precise formulation of the respective optimization problem is trickier than appears at first glance, as it depends on the information available to the hub. more specifically, if the hub knows the instantaneous states of links to all sensor nodes at the start of each round, then the optimal slot assignment for the next round is simply the permutation that maximizes"
"the published scholarship seem to agree that 'learning object repositories' are a set of large, interconnected databases which store, reproduce and deliver learning objects through the internet. prior to the emergence of this term, various other terms had been used to denote them such as 'learning resource bank', 'knowledge repositories', 'digital repositories' and 'digital libraries' among many other terms that refer to the entities which provide the learner with information or learning resources in a digital environment. however, the term 'learning object repositories' has been directly linked to e-learning and e-content objects that are used in accordance with specific teaching targets [cit] ."
"we initially consider the optimal scheduling under the assumption that full information about current channel states is available to the scheduling algorithm at the start of each tdma round. as explained above, this can be achieved in theory by probing all the links before making a scheduling decision, leading, however, to an unacceptably high overhead in terms of both time and energy costs it imposes on the sensor nodes. we therefore emphasize that the assumption of full information is only used in this subsection to obtain an upper bound reference, and it will be alleviated thereafter."
"(1) to autonomously and interactively acquire new knowledge and skills, in real time. this includes one-shot learning-i.e., learning something new from a single example. (2) to truly understand language, have meaningful conversation, and be able to reason contextually, logically and abstractly. moreover, it must be able to explain its conclusions. (3) to remember recent events and interactions (short-term memory), and to understand the context and purpose of actions, including those of other actors (theory of mind). (4) to proactively use existing knowledge and skills to accelerate learning (transfer learning). (5) to generalize existing knowledge by forming abstractions and ontologies (knowledge hierarchies). (6) to dynamically manage multiple, potentially conflicting goals and priorities, and to select the appropriate input stimuli and to focus on relevant tasks (focus and selection). (7) to recognize and appropriately respond to human emotions (have eq, emotional intelligence), as well as to take its own cognitive states-such as surprise, uncertainty or confusion-into account (introspection). (8) crucially, to be able to do all of the above with limited knowledge, computational power, and time. for example, when confronted with a new situation in the real world, one cannot afford to wait to re-train a massive neural network over several days on a specialized supercomputer. (para. 12)."
"for dreyfus [cit], embodiment is enrooted in the deep philosophical grounds of existentialism. existentialism discusses the notions of involvement and detachment. most of the time, humans are involved in the world, they interact, they solve practical problems, they are involved in everyday coping, finding their way about in the world. however, when things become difficult, the individual retracts into detachment. for most of the things you do, there is no need for any type of awareness; while climbing stairs you do not think about the next step. if you do you will probably fall. only when the stairs are too steep, you might consider your next step, and then you retract to a state of detachment. for heidegger [cit] there is the world where we live, where everything has 'meaning', and there is the universe where detachment and science lives. science is the outcome of our involvement in the world, and not the other way around. science cannot explain the 'meaning' of things. existentialism is therefore the opposite of descartes' dualism."
"metaphors are formed using hebbian learning [cit] . in early childhood we learn to associate between several concepts, such as 'warm' and 'up', or 'cold' and 'down'. the more these associations of schemas are presented to us in childhood the stronger we relate the schemas. later we use more elaborated metaphors to reason. we solve real situations by using homeomorphisms into a world of metaphors, solving the imaginary situation first. this is 'thinking by metaphors' [cit] ."
"for the initial consideration of the optimal scheduling strategies, we assume that the statistical parameters s and v of each link are known to the hub. however, we will return to this point later, and show that in fact one of the proposed strategies can be applied even without this knowledge and still achieve near-optimal performance in most scenarios of interest."
"the previous section explored scheduling strategies based only on the binary outcome (i.e., success or failure) of the most recent transmission on each link. however, this does not exhaust the information available to the network coordinator. in particular, the rssi value itself is a useful benchmark of the quality of a wireless link, which is indeed used in many practical algorithms for transmission power control and routing. in this section, we investigate how the rssi reading can be used to make informed scheduling decisions and compare the results with the strategies proposed earlier, that rely purely on the binary outcome information."
"registration and login (authentication service) -new users can first register in e-learning system with their specific details. registered students can login with a combination of user name and password. users profile details -these can include the user's first name, last name, date of birth, university, gender, age, address, email address and education level (to be selected from the options that are most recently achieved). then, there is the most important selection of \"subject of interest\" as first preferred from available courses, with three different interests to define to insure adaptive e-learning. profile management -the user can update his or her parameters and details anytime by logging into the system under the (my profile) tab. search can be use general, search relate, frequently from database lecturers/teachers agent design the lecturers or teacher's intelligent agent is used to handle all learning related contents and course resources."
"there is still a long way to go before we can talk about a singularity point. ai is still a weak technology, still too rigid, and too specific to become similar to human intelligence. however, we believe the next few steps in ai technology and in our understanding of human behavior, will bring about much more powerful machines, flexible enough to resemble human behavior. an important major research project in this context is artificial social intelligence (asi), which we shall shortly describe (section 4). the second project is a new challenge which is known as artificial general intelligence (agi) (section 5). agi brings about a new approach which is much more flexible and closer to human intelligence. it also suggests a model of consciousness, a new approach to the question of learning, a model of self-referential machines, etc."
adding or deleting courses of interest in relation to the topics. building and managing sample questions addressed to the learners in order to test their knowledge of the topic they have chosen in the profile.
"the current draft proposal of the ieee ban task group puts forth a tdma-based approach as the most appropriate mac solution to achieve the desired energy efficiency [cit] . indeed, a tdma mechanism avoids many common causes of energy waste, e.g., collisions, overhearing and idle listening, while at the same time allowing nodes to turn their radios off outside of their allocated tdma slots, thus achieving significant energy savings. on the other hand, a simple static tdma allocation may lead to significant waste due to the unreliable nature of wireless links around the body, namely, when a node's allocated time slot comes when its channel is in a bad state (while preventing a transmission by any other node that may have a good link). ideally, transmission slots should be allocated to sensor nodes only when their link state to the hub allows a successful data transfer, which implies that the scheduling should not be fixed but rather vary according to the real-time link conditions of the nodes."
"whilst e-learning is rich with learning resources, there is a common issue in re-using and sharing learning resource across different learning environments. learning resources are also known as learning objects, which are available in various forms in different learning management systems. in this study, we have presented a unified e-learning repository system for sharing learning objects in universities in saudi arabia. the ulrs has embedded with intelligent agents to search, access and use the shared learning objects."
"in our system, views are selected based on promised abundance and uniqueness of features. in contrast to existing approaches we rely on an efficient bag-of-words approach to organize the training feature database. this data structure is called a vocabulary tree and provides a measure of feature uniqueness per object and discrimination potential. the system also provides a confidence/certainty measure for the objects identity."
"all of the above works focus on application requirements and traffic characteristics when performing slot assignments, rather than the state of the wireless channel, which in a ban can sometimes be highly volatile yet at other times can have a coherence time of up to 400 ms [cit] . more specifically, variable scheduling based on application characteristics tends to focus on the goal of increasing throughput and fairness. in contrast, our work focuses on variable scheduling driven by the wireless channel variations, with a goal of increasing reliability."
"course access -a student has access to the enrolled course and content resources. access to course material, learning objects, assessment objects can be delegated by teacher or course administrator. search -can be use general, search relate, frequently from database administrator agent design the administrator has a top level access create and assign roles to teacher and student user groups. the administrator has full control on access, permission, courses, course content, and user groups."
"e-learning is ''the use of new multimedia technologies and the internet to improve the quality of learning by facilitating access to resources and services as well as remote exchanges and collaboration. e-learning can take place totally online in virtual environments or in a mix of virtual and face-to-face environments; a mode entitled 'blended learning'. e-learning has the potential to impact positively on education. it provides great opportunities for both educators and learners to enrich their educational experiences [cit] . individuals who were disadvantaged for geographic, physical or social circumstances have increasingly better educational chances via e-learning. furthermore, e-learning supports synchronous and asynchronous communications in various formats ranging from text, voice and audio. in addition, supported by the openness and flexibility of the internet, e-learning provides the teaching and learning transactions with unfathomable amounts of information independent of the pressure of time and the constraints of distance [cit] . e-learning is generally defined as learning through electronic devices such as desktop / laptop computers, smart phones, cd / dvd players, etc.), which first appeared in the 80's as a competitor to traditional face-to-face [cit] . the development of e-learning in education continues to grow steadily [cit] . in developing countries, such as saudi arabia, the most important tools of learning at anytime, anywhere concept still focused on a personal computer or pc [cit] . due to physical limitations of computer, students cannot access learning materials in a place or a location. in this case, mobile device is becoming popular among teenagers which can be fulfilled in the ubiquitous idea of learning [cit] . normally, we call e-learning with mobile device as mobile learning or mlearning in short form. in the 90s, a new form of learning was revealed, namely, the mobile learning (m-learning) [cit] . recently, many researchers have focused on m-learning and its environment, such as users' acceptance in m-learning [cit], and setting the environment for m-learning [cit] ."
"a completely different approach to agi suggests imitating the complex architecture of the human brain and creating its exact digital simulation. however, this method is questionable since the brain has not been fully deciphered yet. another, more abstract way to create agi is to follow cognitive psychology research and to emulate the human mind. a third way is to create agi by emulating properties of both aspects-brain and mind. but, as wang [cit] stresses, the main issue is not \"whether to learn from the human brain/mind (the answer is always \"yes\", since it is the best-known form of intelligence), or whether to idealize and simplify the knowledge obtained from the human brain/mind (the answer is also always \"yes\", since a computer cannot become identical to the brain in all aspects), but on where to focus and how much to abstract and generalize.\" (pp. 212-213)."
"where p i (x) is the probability of link i to be good after x slots, as given by equation (2). however, while not entirely unrealistic, a full knowledge of all the link states by the hub would incur a significant communication and energy overhead at the start of a tdma round, as each sensor node would then need to actively sample its wireless channel (e.g., with the transmission of a probe). accordingly, we are particularly interested in scheduling strategies that only use information already available without additional probing-namely, the outcome (success or failure) of the transmission by each sensor in the previous round. if we denote the number of slots elapsed since the transmission of node i by d(i) (in other words, d(i) is the amount of time that the information about the link state of node i is outdated), then the expected number of successful transmissions in the next round is given by:"
"to summarize, we have developed an active 3d object recognition and verification framework which can be applied to any active vision task. the next viewpoint selection algorithm significantly outperforms randomly selecting the next viewpoint. our system only captures a new image when necessary and successfully deals with occluded objects in cluttered environments which may be visually similar to other objects contained in the database. it also provides a measure of certainly of the object's identity."
"a major part of human intelligence is social, we interact with others, we compete, we cooperate, we imitate, etc. a symbolic 'context free' intelligence cannot be complete without this social constituent. asi is therefore necessary for building 'true' human intelligence."
"similar to our previous strategies, the hub defines two groups of nodes: those that should be scheduled as early in the round as possible (the \"early\" group), and, conversely, as late in the round as possible (the \"late\" group). previously, the \"late\" group corresponded to the subset of nodes with bad links and the \"early\" group only contained those with good links, but now we allow more flexibility. more specifically, the steps taken by the sorted flipping strategy are as follows:"
"an initial test image is presented to the system at an arbitrary pose. the belief probability is updated at each subsequent view that is processed. the system retrieves the next best viewpoint until a confidence or belief probability of 80% is reached. in accordance with previous state-of-theart active object recognition systems [cit], we compare our results to randomly selecting the next viewpoint. when randomly selecting the next best viewpoint, the experiment was conducted ten times and the average number of views for each object was taken. both methods correctly verify all objects. we are, however, more interested in the number of views required to correctly verify an object as this greatly influences the processing time of the system. table 1 displays the number of views required by each method. table i describes the number of viewpoints required for each object in the database to reach a confidence level of 80% for verification. for each of the twenty objects, our method requires fewer viewpoints, in some cases significantly so, to reach a confidence of 80%. this indicates that our method is selecting more infomative viewpoints which can significantly decrease the processing time of the system. the difference in information provided by the varying choice of viewpoints can be shown. figure 5 displays the increase in belief after each view for the 'curry 1' object. we can see that even after the second view our method has a much higher belief than randomly selecting a viewpoint for both verification and recognition. after four views in the case of verification and five views for recognition, our method reaches a confidence level of 1."
"where m is the total number of images in the database and m i is the number images in the database with at least one feature that passes through node i. using this quantity, a feature's uniqueness may be calculated. this is done in the following way. the feature's path through the vocabulary tree is determined by evaluating the closest cluster centers at each level. a measure of uniqueness is given by the sum of all the tfidf numbers, or weights, of the nodes it passes through. the higher the weighting, the more unique the feature. the uniqueness of the viewpoint may then be given by summing these totals for all the sift features extracted from that viewpoint. we term this metric the viewpoint weighting. this calculation is performed for every viewpoint in the dataset."
"our system uses test images where the object to be verified or recognized is significantly occluded and appears in a cluttered environment. even with these difficulties, our system correctly verifies and recognizes all objects requiring fewer viewpoints than randomly selecting the next viewpoint, in some cases significantly so."
"asi systems have no clear definition of goals, there is no specific task the machine is oriented towards. in a sense, the machine's social behavior is the goal. in other words, it is impossible to defined clear goals in advance, and these may even emerge dynamically. this means that measurement and evaluation methods are very difficult to apply to a socio-cultural intelligence of such a system. this is one of the biggest challenges the asi field has to deal with."
"symbolic systems are one important type of cognitive architecture. \"this type of agents maintains a consistent knowledge base by representing the environment as symbols.\" [cit] (p. 2). some of the most ambitious agi-oriented projects in the history of the field were in the symbolic-ai paradigm. one such famous project is the general problem solver [cit], which used heuristic search (means-ends analysis) to solve problems. another famous effort was the cyc project [cit] . the project's aim was to create human-like ai by collecting and encoding all human common sense knowledge in first order logic. alan newell's soar project [cit] was an attempt to create unified cognition theories, based on \"logic-style knowledge representation, mental activity as problem-solving carried out by an assemblage of heuristics, etc.\" [cit] (p. 3). however, the system was not constructed to be fully autonomous or to have self-understanding [cit] ."
"to improve the reliability in body area networks, we have presented a framework for variable tdma scheduling, where transmission slots are assigned to nodes based on the information about their wireless links, with the goal of minimizing transmission failures due to bad channel state. based on a two-state gilbert link model we have developed the simple yet effective flipping strategy, which creates an optimal slot allocation for a single tdma round when the links of all nodes are identical. additionally, flipping has several other desirable characteristics, namely it is trivial to compute and performs well in practical scenarios, where wireless links of the sensor nodes are not homogeneous."
"creating ai with more complex and subtle human traits is sought after by new startups for ai assistants. koko, a startup born out of the mit media lab, has created an ai assistant that can display sympathy. for example, if a person is having a bad day, it will not just say 'i'm sorry to hear that', but will ask for more information and perhaps provide advice like 'tension could be harnessed into action and change' [cit] ."
"this study proposes the unified e-learning repository system (ulrs) for sharing learning objects across elearning systems. as shown in \" fig.1 \" the ulrs can be used for providing anytime availability of learning resources and minimizing the expensive groups required for maintenance moreover improving the usefulness. the ulrs implements different groups with various permissions and access control. teacher groups can be implemented to observe and review the learning materials, resources and feedback from learners to scrutinize their progress. in addition, the material can be updated on the basis of new introduction, development and more importantly from learner responses. students or learner groups are the main users of the learning resources with read access. as a proof of concept, the ulrs has been developed and implemented in an online scenario. teacher and learner are two specific roles to determine access and permissions to certain resources. individual access is determined by the authentication system in place. there are three options to search learning resources in ulrs. meta data or information about learning objects, users and their roles are stored in the database. the ulrs is powered with the following search agents:"
"singularity is based on several assumptions: first, that there is a clear notion of what is human intelligence; and second, that ai can decrease the gap between human intelligence and machine intelligence. however, both of these assumptions are not clear yet. what is becoming more and more apparent is that ai goes hand in hand with our understanding of our own human intelligence and behavior."
dreyfus [cit] thought that neural network computations are indeed a step in the right direction. it is an attempt to formalize our perceptions in terms of virtual neurons which have some parallel in our brain and body.
"\"intelligence\" is a complex and multifaceted phenomenon that has for years interested researchers from countless fields of study. among others, intelligence is studied from psychological, biological, economical, statistical, engineering, and neurological perspectives. new insights emerge over time from the various disciplines, many of which are adopted into the science of ai and contribute to its development and progress. the most striking example is the special and fruitful interrelationship between artificial intelligence and cognitive science."
"one such integrative scheme described by pennachin and goertzel [cit], was given the name 'novamente'. this scheme involves taking elements from various approaches and creating an integrated and interactive system. however, as the two explain: \"this makes sense if you believe that the different ai approaches each capture some aspect of the mind uniquely well. but the integration can be done in many different ways. it is not workable to simply create a modular system with modules embodying different ai paradigms: the different approaches are too different in too many ways. instead one must create a unified knowledge representation and dynamics framework, and figure out how to manifest the core ideas of the various ai paradigms within the universal framework.\" (p. 5)."
"\"the architecture can be applied to control autonomous software agents as well as autonomous robots \"living\" and acting in a reasonably complex environment. the perceptual learning mechanism allows each agent controlled by the lida architecture to be suitably equipped so as to construct its own ontology and representation of its world, be it artificial or real. and then, an agent controlled by the lida architecture can also learn from its experiences, via the episodic learning mechanism. finally, with procedural learning, the agent is capable of learning new ways to accomplish new tasks by creating new actions and action sequences. with feelings and emotions serving as primary motivators and learning facilitators, every action, exogenous and endogenous taken by an agent controlled with the lida architecture is self-motivated.\" (p. 6)."
"1. initially, the nodes are equally split between the two groups in a random manner. 2. every round, each node is moved to the opposite group unless it failed to transmit in the previous round, in which case it is forced to the \"late\" group."
"we introduce a new framework for active object verification and recognition consisting of an selector and an observer component. the selector determines the next best viewpoint and the observer component updates the belief hypothesis and provides feedback. the observer component works independently from the selector and thus any exploration or manipulation of an object can occur without interfering with the observer component. this framework, which has proven to work efficiently, can be applied to any active vision task."
"the system was then tasked to recognise occluded objects in cluttered scenes. this differs from verification in that, the object's identity is not known to the system. it has to determine the identity based on which object has accumulated the greatest belief probability given the current database. the system retrieves the next best viewpoint until a confidence or belief probability of 80% is reached for any of the objects in the database. the next best viewpoint is selected based on which viewpoint has the highest combined weighting over all objects. both methods for select- figure 5 . confidence values after each view for verification and recognition ing the next best viewpoint (our method and random selection) correctly recognize all objects. as mentioned before, the measure of interest is the number of viewpoints required to correctly identify an object. table 2 displays the number of views required by each method. table ii describes the number of viewpoints required for each object in the database to reach a confidence level of 80% for object recognition. our method clearly out performs randomly selecting the next viewpoint. it requires fewer views for all objects to attain a confidence of greater than or equal to 80%. this leads to a significant decrease in processing time for recognising objects which are occluded in cluttered environments."
"to select the next best viewpoint, features appearing in every viewpoint were weighted based on their uniqueness in the given dataset using a vocabulary tree. for verification, the viewpoint with the highest weighting for the object to be verified was then selected as the next view. in the case of object recognition, the viewpoint with the highest weighting over all objects was selected as the next viewpoint. both these methods proved to be significantly better than randomly selecting the next viewpoint. bayesian methods are used to update the belief hypothesis and provide feedback. the path of each matched feature in the test image was traced through the vocabulary tree and the statistics contained in the leaf node were used to update the belief hypothesis."
"artificial general intelligence requires the above characteristics. it must be capable of performing various tasks in different contexts, making generalizations and tapping from existing knowledge in a given context to another. hence, as voss [cit] explains, \"it must embody at least the following essential abilities:"
"one of the unsolved problems of agi research is our lack of understanding of the definition of \"generalization\", but what perez [cit] suggests \"is that our measure of intelligence be tied to our measure of social interaction.\" (para. 7). perez calls his new definition for generalization \"conversational cognition\" and as he explains:"
our experiments show the successful use of active object exploration for 3d object verification and recognition for significantly occluded objects in extremely cluttered environments. the active vision system performs considerably better than randomly selecting the next viewpoint. the system also provides a measure of certainty for the object's identity.
"most of the recent progress in ai have been driven by deep neural networks and these are related to the \"connectionist\" view of human intelligence. connectionist theories essentially perceive learning-human and artificial-as rooted in interconnected networks of simple units, either real neurons or artificial ones, which detect patterns in large amounts of data. thus, some in the machine learning field are looking to psychological research on human learning and cognition to help take ai to that next level. although [cit] s, only today, due to an enormous increase in computing power and the amount and type of data available to analyze, deep neural networks have become increasingly powerful, useful and ubiquitous [cit] ."
"embodied cognition is a new hurdle to overcome, it is the missing bridge between robotics and ai. it should not be thought of as an imposition on ai but as a new challenge."
"in an attempt to answer these questions, we refer to the viewpoint of dennett [cit] . let's define the notion of 'meaning'; to put things very simplistically, we will say that an action of a computer agent has a 'meaning' (for the agent) if the action is changes some part of its environment and the agent can sense that change. for example, if the agent is a ribosome, then the transcription of an rna into a series of amino-acids, later to become a protein, has a meaning since the protein has some function in changing the agent's environment. the action of the ribosome has a 'meaning' in the cytoplasm environment. similarly, we can embed a 'meaning' in computer agents. it was suggested by dennett that we human can insert a derived 'intention' in computers, and computers can derive a lower type of 'intention' in other computers. this was also brought up years ago by minsky [cit], using a different language."
"\"it is time for sociology to break its intellectual isolation and participate in the cognitivist rethinking of human action, and to avail itself of theoretical ideas, techniques and tools that have been developed in ai and cognitive science\" (p. 20) ."
"the vocabulary tree is constructed using hierarchical kmeans clustering where similar features are clustered together. k defines the number of children of each node of the tree. initially, for the root of the tree, all the training data is grouped into k clusters. the training data is then used to construct k groups, where each group consists of sift descriptors closest to a particular cluster centre. this process is recursively applied to each group up to some depth d. this process is illustrated in figure 2 ."
"when logged in with an admin account, the administrator has permissions to create, delete or update the course content. moreover, administrator has the same profile parameters and functions like the student profile."
"to prove all of the above, embodied cognition scientists search for clues in language where they look for invariants. the existence of such invariants can imply that something deep, common to all languages, underlies. in many examples a word has several meanings, one is environmental and embodied, the other much more abstract. for example 'to grasp' is first of all 'to catch', however it also has the meaning of 'to understand'. we 'see' things in the sense of understanding, we talk about a 'warm' or 'cold' person, etc. old proverbs are a good source for such examples."
"as for frames, many words have a large and natural context and cannot be understood without their context, for example prisoner, nurse, doctors, etc. these are the frames. frames were suggested in social science by goffman [cit], and they were also referred to in the context of ai by minsky [cit] . minsky was also interested in issues such as: the symbolic aspect of vision, the relation of his theory of frames to piaget's theory of development, language understanding, scenarios etc. dreyfus [cit], on the other hand, stressed the fact that real frames are infinite in nature and could not be truly described in ai. this was coined the 'frame problem'."
"sustaining: ai also requires sustainers. sustainers oversee and work on making sure ai is functioning as intended, in a safe and responsible manner. for example, a sustainer would make sure an autonomous car recognizes all human diversity and takes action not to risk or harm any human being. other sustainers may be in charge of making sure ai is functioning within the desired ethical norms. for example, when analyzing big data to enhance user monetization, a sustainer would oversee that the process is using general statistical data and not specific and personal data (which may generate negative sentiment by users) to deduce its conclusions and actions [cit] ."
"another way in which we can implement embodiment cognition is by formalizing the idea of metaphors. to be able to use metaphors we need to enable the computer the capability to simulate a situation in which the machine itself resides. this was already done in the context of value alignment. winfield, blum and liu [cit] defined a 'consequence machine' that could simulate a situation, but could also observe itself in that simulation. the machine then had to decide on a 'moral' dilemma."
"once again, we start with the simple case where the information (namely, rssi readings) from all links is available to the scheduler at the beginning of the round. recall that in the corresponding scenario based on the gilbert model, we introduced three scheduling strategies: \"random groups\", \"greedy sorting\", and \"optimal\". while the \"random groups\" algorithm can be applied in a straightforward manner here as well, the other two strategies require the knowledge of the gilbert model parameters s i and v i of individual links, and cannot be applied on experimental traces directly. however, we can still employ the rationale behind the \"greedy sorting\" algorithm, which dictates that more volatile good links should be scheduled as early in the round as possible, as their probability to have a successful transmission decays more rapidly. from the previously established connection between volatility and proximity of the rssi value to the threshold, we introduce the greedy rssi sorting algorithm, as follows: schedule all \"good\" links before all \"bad\" links, and within the \"good\" group, sort the links by rssi value in increasing order. figure 5 compares the performance of the greedy rssi sorting algorithm with the baseline provided by random groups. it can be seen that smart ordering of transmissions within the \"good\" group based on rssi information provides a clear advantage for all values of attenuation threshold, reaching more than 45% reduction in losses for attenuation thresholds of 90 db to 95 db. another important observation is that the performance of the random groups algorithm is noticeably lower than that in figure 2, illustrating that the gilbert link model is, in fact, only an approximation of the real behaviour of the body-area links."
"a number of methods have explored active object recognition previously [cit], but using experimental set-ups not directly comparable to ours. we adapted [cit] to run on our data, and found our performance to be comparable and better in a number of cases, but do not quote results since we did not try to optimize their performance on our problem. we will make our data and code available on request to facilitate future comparisons."
"body area networks (ban) are an emerging technology that has attracted attention from researchers in academia and industry due to its significant potential in many different areas including health care, sports, military, and entertainment. a typical ban application involves a number of low-power sensing devices, operating on or around the human body, that collect sensor data, possibly processing it locally, and transmit the information to a central device, known as a coordinator, sink, or hub. bans exhibit some similarities with other wireless networks, such as the master-slave structure of cellular networks and the low-power requirements of wireless sensor networks. despite the similarities, bans have several characteristics that require a unique approach to the design of physical layer as well as network protocols. one of these is the extreme limitations on power usage (having to last months and even years using minuscule, unobtrusive batteries), which, in combination with the short range and existence of a central hub, lead to novel power-saving techniques [cit] . more importantly, the wireless propagation properties in ban are quite different from many other contexts, due to the low transmission power that the nodes must use and due to the prevalent absorption effects of the human body. bans must cope with deep fading effects that can last much longer (10-300 ms) than similar effects in cellular networks [cit] and possible severe shadowing effects that can cause loss of connectivity up to several minutes [cit] . the uniqueness and importance of these networks is reflected in the creation of the ieee 802. 15 task group 6 [cit] that is defining the physical and mac layer communication standards specifically for bans."
"let us consider mattersight as an example. the company provides a highly sophisticated data analysis system that tracks customer' responses on the telephone. the software analyzes varied communicative micro-features such as, tone, volume, word choice, pauses, and so on. then, in a matter of a few seconds, ai algorithms interpret these features, compare them to the company's databases, and come up with a personality profile for each customer. based on this profile, the customer will be referred to the most appropriate service agent for him [cit] ."
"login (authentication service) -a student can log into the e-learning system with a user name and password. student's profile details -these include first name, surname, date of birth, gender, age, postal address, hometown, email, education level, and enrolled courses. profile management -the student's details and parameters can be updated anytime by using the correct login and authentication details by individual user in the student role."
"social and emotional capabilities-\"this consists of three types of capability: social and emotional sensing, which involves identifying a person's social and emotional state; social and emotional reasoning, which entails accurately drawing conclusions based on a person's social and emotional state, and determining an appropriate response; and social and emotional action, which is the production of an appropriate social or emotional response, both in words and through body language.\" [cit] (p. 34)."
"login (authentication service) -the administrator can log into the e-learning system with a user name and password. users management -a user with administrator role can create, update and delete users. administrator can create, update, delete and maintain the repository file tree structure. course designer -the administrator has control over creating new courses, updating, archiving and maintaining existing courses. new course topics can be added, deleted or edited by the administrator as well as lecturers under (course designer) tab, thus enabling the management of interest topics for students. search -can be use general, search relate, frequently from database v. conclusion e-learning has been growing in popularity and use worldwide. learning objects form an important part of the e-learning pedagogy and architecture. to maximum the benefits of creating and sharing learning resources, it is eminent to have a centralized repository with defined access to all stakeholders. this study proposes the unified e-learning repository system (ulrs) for sharing learning objects across e-learning systems. the proposed design has been developed and deployed as a proof of concept. future work will focus on evaluation of the proposed ulrs or learning objects repository."
"the student is the target use of courses and learning objects. the student agent is responsible for access to course topics, online learning content, and evaluation questions in the relevant subject area."
"one of the biggest challenges for ai is that of embodied cognition. if ai could surpass this hurdle, it will get even closer to true human intelligence. embodied cognition was first presented as the main reason why ai is impossible. we propose to view embodied cognition as a new challenge to ai and not as an imposition (section 6)."
"we point out that well-known algorithms for solving the maximum-weight matching problem exist, requiring o(n 3 ) time in general [cit] . however, such a complexity cannot be considered reasonable for more than a very small number of nodes (in fact, the standard allows up to 256 nodes [cit] ), since the solution is required essentially instantly. indeed, any delay in the scheduling computation translates directly to a delay in the start of the next tdma round. accordingly, we are much more interested in very low-complexity heuristics to approximate the optimal scheduling solution, not exceeding that of a simple sorting operation."
"while artificial cognitive intelligence has become a well-established and significant field of research, and has been heavily invested by both cognitive and artificial intelligence researchers, artificial social intelligence is in its early stages and has great potential for the advancement of smart machines in a new and essential way."
"from the point of view of strategy and methodology agi sometimes uses a top down approach on cognition, as wang and goertzel [cit] explains, \"an agi project often starts with a blueprint of a whole system, attempting to capture intelligence as a whole. such a blueprint is often called an \"architecture\".\" (p. 5)."
"for object recognition, no object hypothesis is given to the system. the criteria for selecting the next best viewpoint is based on the viewpoint which has the highest combined weighting across all objects in the database and has not been previously visited. in the experiments section, we will show that both these selection methods significantly outperform randomly selecting the next viewpoint."
"brooks' robots were able to wander around, to avoid obstacles and to perform several basic tasks. a more modern version would be an intelligent swarm, where a set of simple agents interact and can bring about some emergent property [cit] ."
"the structure of the paper is as follows. section 2 discusses related work and section 3 elaborates on how the datasets for the experiment were collected. a complete description of the system's architecture is presented in sections 4 and 5. sections 6 and 7 present the experimental results and conclusions. finally, possible future work is discussed in section 8."
"what was needed for a science of cognition was a much richer notion of knowledge representation and process mechanisms; and that is what artificial intelligence has provided. cognitive psychologists gained a rich set of formalisms to use in characterizing human cognition [cit] (p. 2). some of the early and most important formalisms were means-ends analysis [cit], discrimination nets [cit], semantic networks [cit], frames and scripts [cit], production systems [cit], semantic primitives [cit], incremental qualitative analysis [cit] . through the years a wide range of formalisms were developed for analyzing human cognition, and many of them are still in use today."
"for the test set, the objects used in the training data were captured at every 20 degrees in a cluttered environment with significant occlusion. in all the presented experiments, images are captured around the y-axis, which represents 1 degree-of-freedom (dof). this is not a limitation of our proposed system. our viewpoint selection system can easily be applied to several degrees-of-freedom with a modest increase in required computation."
"by using these capabilities, ai can amplify our own abilities: \"artificial intelligence can boost our analytic and decision-making abilities by providing the right information at the right time. but it can also heighten creativity\". [cit] (para. 13). consider for example autodesk's dreamcatcher ai which enhances the imagination of designers. as explained in the company's website:"
"we can coin the above set of 'senses' as 'environmental intelligence'. the question is how much of our intelligence is grounded in our body, and how much is 'context free' in our mind? if we had no body, could we think the same way we think? could we think at all? would we have anything to think about? what is the connection between our 'environmental intelligence' and our 'context free symbolic manipulation intelligence'?"
"in an article called \"how do we align artificial intelligence with human values?\" [cit], conn explains that \"highly autonomous ai systems should be designed so that their goals and behaviors can be assured to align with human values throughout their operation\". (para. 5)."
"as the next step, we aim to find out how the newly introduced greedy rssi sorting algorithm will behave in the more realistic situation, when the scheduling is based on the outcomes of transmissions in the previous round. for this case, the flipping strategy will serve as the baseline, indicating the attainable loss rate reduction if the rssi information is not taken into account. this comparison is performed in figure 6 . as it turns out, the greedy rssi sorting strategy lags significantly behind flipping in performance. this can be explained by the fact that the greedy rssi sorting algorithm allocates transmission slots focusing greedily on maximizing the expected number of successes in the current round only and ignoring the impact on subsequent rounds. thus, it suffers from the same long-term effect that causes flipping to perform better than the single-round optimal strategy in section 4. despite the above fact, the rssi information can still be useful for scheduling decisions based on the previous round outcomes, and we devised a strategy to combine the strengths of flipping and greedy rssi sorting. the main benefit of flipping comes from the long-term effect, where sacrificing some of the performance in the current round allows more gains in the next round (by having more recent link state information). on the other hand, greedy rssi sorting algorithm considers short-term benefits by prioritizing highly volatile links. thus, we define the \"rssi-sorted flipping\" (or, for brevity, sorted flipping) algorithm as follows."
"we denote the transition probabilities between the bad and the good state and vice versa by p u (up) and p d (down), respectively; thus, p u denotes the probability of moving from a bad state to a good state within a slot interval, and vice versa for p d. we denote by p the markov transition probability matrix, namely,"
"the training database used consists of twenty everyday objects. this is much larger than other databases used for active vision experiments. to assemble the training set for the vocabulary tree, images were captured every 20 [cit] c camera. verifying or recognising objects tends to become more complicated if two or more objects have views in common with respect to a feature set. these types of objects may be distinguished only through a sequence of images which the viewpoint selection algorithm is required to determine. for this reason, objects that share a number of similar views were included in the dataset, as shown in figure 1 . the database used is available on request."
"the teacher is responsible for designing and developing courses topics, online learning content, and sample evaluation questions which the students can use to fill their respective fields of interest. the teacher is also responsible for:"
it is interesting to note that repeated slot assignments seeking to maximize (4) in every round may not necessarily lead to the long-term optimal performance. this is due to the fact that the information available to the hub depends on the scheduling decisions taken in the previous round; and hence we refer to a permutation that maximizes (4) as a single-round optimal assignment.
"image processing methods used to create object models for classification include appearance-based methods [cit], aspect graphs [cit], histogram of gradients [cit] and neural networks [cit] . following [cit] we use sift to model objects, which provides robustness to affine transformations and variable illumination."
it is important to note that sift features detected on the background will not negatively effect the weighting since all images were captured using the same background and their uniqueness weighting will be extremely low. figure 3 is an example polar plot of viewpoint weightings for a spice bottle object in the database. the polar plot indicates that the most distinctive viewpoints (highest weightings) are at 340 degrees and at 0 degrees and the most indistinguishable (lowest weighting) viewpoint is at 180 degrees.
"physical capabilities-this includes gross motor skills, navigation (these two have reached human level performance), fine motor skills and mobility (these are more difficult and hence the performance levels are currently still median and below). \"these capabilities could be implemented by robots or other machines manipulating objects with dexterity and sensitivity, moving objects with multidimensional motor skills, autonomously navigating in various environments and moving within and across various environments and terrain.\" [cit] (p. 35)."
"if the probability of a link to be in the good state in a certain slot is denoted by p(0), then its probability to be good k slots later is"
"if the channel state information is always available to the scheduler, then the optimal long-term performance is achieved simply by maximizing the rate of successful transmissions at each round independently. thus, the optimization target is given by equation (3) . to that end, we observe that finding the permutation k that maximizes (3) can be seen as an instance of the maximum-weight matching problem in bipartite graphs. indeed, define a bipartite graph with n vertices corresponding to the sensor nodes, and a further n vertices corresponding to the time slots. define an edge from node i to slot j to have a weight of p i (j), i.e., the probability for the node's link to be in a good state in that slot. then a scheduling assignment that maximizes the expected number of successful transmissions (3) is equivalent to a maximum-weight matching in the corresponding bipartite graph."
"the graph in figure 3 depicts the performance of different variable scheduling strategies together with the maximum performance achieved in the previous subsection (shown as the upper bound in this graph). we note that although the gains are much lower since the information available is more outdated, the general trends seen earlier persist here as well. furthermore, the decline of performance for high values of outage threshold is more pronounced since highly volatile links make outdated information even less useful. however, the most interesting finding is that the performance of the newly introduced flipping strategy is not only comparable with, but in fact is even consistently better than that of the single-round optimal algorithm for almost all values of the outage threshold. this happens despite the fact that optimal assignment accounts for individual gilbert parameters of each link, while flipping strategy relies solely on the outcomes of transmissions in the preceding round. we emphasize that the underlying markov parameters are not homogeneous across all links, but (for each link independently) drawn from a range of typical values, as explained in section 3. we discuss the insights behind this counter-intuitive performance effect in the next subsection."
"\"cobots\" are probably the best example here. collaborative robots, as gonzalez [cit] explains, \"excel because they can function in areas of work previously occupied only by their human counterparts. they are designed with inherent safety features like force feedback and collision detection, making them safe to work right next to human operators.\" (para. 2)."
"while cognitive artificial intelligence scientists \"essentially view the mind as something associated with a single organism, a single computational system, social psychologists have long recognized that this is just an approximation. in reality the mind is social, it exists, not in isolated individuals, but in individuals embedded in social and cultural systems.\" [cit] (p. 24)."
"in other words, the gödel machines \"are universal problem solving systems that interact with some (partially observable) environment and can in principle modify themselves without essential limits apart from the limits of computability. their initial algorithm is not hardwired; it can completely rewrite itself, but only if a proof searcher embedded within the initial algorithm can first prove that the rewrite is useful, given a formalized utility function reflecting computation time and expected future success (e.g., rewards).\" (p. 2)."
"dreyfus [cit] claimed that true ai is impossible since it implicitly assumes that human intelligence is symbolic in its essence. some ai researchers are attempting to build a context-free machine that manipulates symbols, assuming the human mind works similarly. dreyfus claimed that the symbolic conjecture is a fault, basing his arguments primarily on philosophical grounds. ai assumes that we have a type of 'knowledge representation' in our brain, a representation of the world, this idea is based on descartes' theory, and has a long tradition. moreover, descartes claimed that there is a duality, a separation between our body and our mind, therefore the mind cannot be embodied. so far, claimed dreyfus [cit], all ai research is based on these assumptions that we have a model of the world in our mind and that the mind is separated from the body."
"hence, as collins [cit] argues, \"the organism into which the intelligent computer supposed to fit is not a human being but a much larger organism; a social group. the intelligent computer is meant to counterfeit the performance of a whole human being within a social group, not a human being's brain. an artificial intelligence is a 'social prosthesis'.\" (p. 14, our emphasis)."
"the theory of consciousness was recently investigated by ai researchers. it was suggested by dennett [cit] that consciousness is an emergent property of many small processes, or agents, each struggle for its homogeneity. consciousness is not a stage with spotlights in which all subconscious processes are the audience. consciousness is a dynamical arena where many agents appear and soon disappear. it resembles an evolutionary process occurring in a very short timescale [cit] . on this very basis a few ai models were suggested, the copycat model [cit] and its more advanced learning intelligent distribution agent (lida) [cit] model. these two are examples of a strong reciprocal interaction between ai and cognitive science."
"symbolic library. in addition to the standard classes like object required by vpj we have defined an additional set of classes that are used by the translated protocols. we call this the symbolic library, because the implementation of the cryptographic primitives and data types does not perform any \"real\" cryptography or networking. the symbolic library provides a symbolic abstraction of the cryptographic primitives and channel communication. it is designed to be simple enough to simplify proving the translation secure in the future. finally, the symbolic library can also be used for debugging purposes. our expi2java tool also has a concrete library that performs \"real\" cryptographic operations and uses the actual network, but this is not formalized in coq. the two libraries (concrete and symbolic) can be used interchangeably by the generated code."
"all the features that make expi2java a usable and useful tool in practice come at a price though: the tool is rather complex; it currently has more than 16.000 lines of java code, and this code needs to be trusted to generate correct implementations that preserve the security of the original protocol models. in this paper we take the first step towards formally bridging the gap between secure protocol models and automatically generated protocol implementations: we provide the first mechanized formalization of a code generator for cryptographic protocols. and we do this for an idealized code generator that is fairly close to the core of our expi2java tool, without \"sweeping under the carpet\" the interesting details. we formalize the semantics of both the source language of the code generator, an extensible and customizable variant of the spi calculus, and the target language, which is a substantial fragment of the java programming language featuring concurrency, synchronization between threads, exception handling and a sophisticated type system with generics and wildcards. we formally prove that our translation generates well-typed programs when starting from well-typed protocol models. this confirms the validity of our source-level type-checker, and constitutes an important step towards the longer-term goal of proving the correctness of the translation."
"the user can easily customize and extend the input language by adding new types or new cryptographic primitives (constructors and destructors) to the default configuration, and by instantiating them with specific cryptographic algorithms (des encryption, rsa signature, etc.). the expi2java configuration also specifies which java class should be used to implement each of the cryptographic algorithms and allows to pass user-defined parameters to the implementation class. this can be used, for example, to specify which encryption mode and padding should be used to encrypt data or to specify the length and endianness of an integer nonce to match the protocol specification, basically giving the user full control over the low-level data format in the generated protocol messages."
"just like java, vpj allows to define custom classes with methods and fields. the syntax is similar to the java generics, with type variables and upper bounds (denoted by in vpj, and extends in java). in vpj variant parametric types require variance annotations on each type argument. in addition to the variance annotations from the expi calculus the variance in vpj can also be bivariant; any two instantiations of a bivariant type constructor are subtypes of each other. the type system of vpj is a mixture of the jinja type system [cit] with variant parametric types [cit] . we have extended the subtyping relation with two additional rules stating that the null type null t is the subtype of string and all reference types t p . additionally, we require the subclass relation to be well-founded (i.e., that it does not have infinite descending chains) to ensure termination of subclass checks."
"processes are used to model the behavior of protocol participants and the communication between them. a specific characteristic of our calculus is that replication (the bang symbol \"!\") can only appear before an input process [cit] . this is the most common way to use replication in specifications, and at the same time it is reasonably easy to implement as a server that waits for requests on a channel and spawns a new thread to process each incoming message. the expi name in the restriction process has a type annotation, otherwise the syntax of processes is standard [cit] ."
"this theorem shows that our translation generates a self-contained vpj program (e, l) that is well-typed in the initial heap and stack. the assumptions about the heap and stack are inherited from jinja. they are needed for the typing relation, because an arbitrary jinja expression can contain local variables or throw system exceptions (e.g., nullpointerexception). at the program start, we assume that the heap is \"preallocated\", i.e., it contains instances of system exception classes at some fixed addresses that are reserved for the system exceptions, and the stack contains no local variables."
"our model of tls consists of an expi calculus process (about 850 lines) and a configuration file (625 lines). it includes all the steps of the handshake and the consequent message exchange through the application data protocol. we check the validity of each received message, including macs and certificate chain (when provided with the list of trusted ca certificates) and respond with an alert message on errors. the structure of all messages is modeled completely in the extensible spi calculus, while the data formats of encryptions and certificates are defined in the configuration and implemented in corresponding java classes. for comparison, the most sophisticated generated protocol implementation we are aware of is a much simpler ssh client generated by spi2java 4 . their model is 250 lines in size and needs 1390 lines of xml configuration (i.e., more than 5-to-1 ratio between configuration and model)."
"in order to show the potential of expi2java we have generated a fully functional and interoperable implementation of the widely used transport layer security (tls) protocol [cit] from a model verified with proverif. tls provides a secure transport layer for many popular application protocols such as http, ftp, smtp, and xmpp."
"in our coq formalization, we use a locally-nameless representation [cit] for expi names (bound by the restriction process) and variables (bound by the let and input processes) to avoid the issues related to α-renaming. nevertheless, for the sake of readability, throughout this paper we use the more familiar, named representation."
"as mentioned in before, machine learning operates on descriptions of the problem domain (c code, in our case). we use code abstractions as descriptions; these should be rich enough to reflect the changes performed by the transformation rules at the ast level. for that reason, we include in the abstractions quantitative descriptions involving features like ast patterns, control flow, data layout, data dependencies, etc. the current code abstraction consists of a vector of features (described later) which capture traits which are modified by the transformation rules and also reflect code patterns that match the syntactic/semantic restrictions of target compiler/programming models."
"the fixed part of the symbolic library (i.e., the one that is not generated by our translation from the configuration) consists of the 7 classes shown in figure 2 . abstractbase is the base class of the class hierarchy for translations of all expi types. abstractgenerativebase is the superclass for generative types. the generated flat class hierarchy reflects the subtyping relation from the expi calculus. semaphore implements a counting semaphore using the synchronization primitives of vpj. this class is used in the implementation of inter-thread message passing. expi channels are modeled using the class abstractchannel x j . it implements the synchronous semantics of expi channels, where different processes are implicitly synchronized when a message is sent from one process to another over a channel."
"we have also performed a preliminary evaluation of reinforcement learning as a technique to learn and then guide a rule-based program transformation tool. we have selected four use cases and identified different transformation sequences leading to code that can be mechanically translated to opencl and executed on a gpu. these four cases and their transformation sequences have been used as training set. the cases in the training set are the image compression program (compress), mentioned before in fig. 5 and 6, an image filter that splits the different color channels of a given rgb input image in different images (rgbfilter), the detection of edges in an image using a sobel filter (edgedetect) and another one performing image segmentation given a threshold value (threshold)."
"the coq proof of this theorem is done in great detail, and so are the proofs of most of the helper lemmas. due to the lack of time we did not prove some rather obvious properties of the translation that are used in the proof of the theorem, e.g., that processes are translated only to expressions of type void or that all used types were generated and added to the program. furthermore, we did not prove some list and substitution rewriting lemmas, and other similar helper lemmas in cases that looked trivially true but were tedious to prove in coq. we have also assumed that the translation of terms and the declaration of free expi names are well-typed."
"this study was designed to assess whether a spatial filtering method such as the xdawn algorithm could enhance mental workload classification at the single-trial level. mental workload was manipulated by varying the number of digits in memory in a classical sternberg memory task. three classification processing chains were compared, one that performed classification directly on the erp signal of one electrode, and two that included a spatial filtering step and performed classification either on one or two virtual electrodes."
"we will present now some results obtained using machine learning-based transformation strategies. we will first show evidence to support our claim, made in section 5.2, regarding the non-monotonic behavior of non-functional properties for transformation sequences and, second, we will evaluate the applicability of reinforcement learning to learn from these non-monotonic sequences. non-monotonicity has two sides: on one side while transforming (sequential) code to a shape better suited to some final platform, the efficiency observed in a cpu for a good sequence can increase and decrease. on the other side, the final code which extracts the best performance on a cpu is not necessarily the most efficient on the final platform. in order to show this behavior we have identified four different transformation sequences for a use case which performs image compression using the discrete cosine transform. each of the identified sequences finally produces c code with a shape adequate to be mechanically translated to opencl and executed on a gpu. we have measured the average execution time of 30 runs for each intermediate state of each sequence. it can be seen (fig. 5 ) that these execution times do not change monotonically. as expected, we also obtain different sequential codes ready to be translated to opencl and with different execution times (still on a cpu)."
expi2java is a new code generator for security protocols designed to be highly extensible and customizable. it takes a protocol specification written in the extensible spi calculus (expi calculus) together with a configuration that provides the low-level information needed for code generation and produces interoperable java code.
"the syntax used by expi2java for writing models is very similar to the one used by proverif [cit] and includes support for proverif-specific constructs such as events and queries. the main difference is that our calculus is typed and we therefore need to annotate some terms with their types. expi2java can pretty-print the protocol specifications in proverif syntax, so that it can be formally verified before generating the implementation."
"the result of the learning process of the agent is a state-action table (fig. 3 ) which will eventually contain values for each combination (s, a) of states and actions. these values are scores for the expected profit to be obtained from applying action a to state s. this table is initially filled in with a default value and is iteratively updated following a learning process which we briefly describe below."
the generation of java code for the tls model (on the same hardware) takes only about 12 seconds. in addition to the sample web server and web client we generate a verified implementation of a tls channel for the concrete library. we use this channel to generate a simple web server offering a web page over https and a web client downloading it. we have tested that the resulting implementation is interoperable with common browsers and web servers.
"we will use a 2d convolution kernel as an example to show the resulting state-action table obtained after learning from a simple transformation sequence with five states and two transformation rules. the first rule (r 0 ) considered in the example transforms a non-1d array into a 1d array. the second rule (r 1 ) performs a collapse of two nested for loops producing a single loop. the initial, intermediate and final codes obtained from the application of these rules is described below. listing 6 through listing 9 highlights changes in code using this style to indicate the portion of the code that will be changed after rule application and code highlighted using this style indicates the code resulting after applying a transformation rule."
"1. - that the machine learning methods used work on program abstractions makes the approach generic and suitable for other imperative languages (e.g., fortran, which is also widely used in scientific computing). the application of the approach to other languages would require changes to the tool described in section 4 in order to account for some specific syntactical patterns of a particular programming language. nevertheless, most of the abstraction features identified and described in section 4 are also applicable to other imperative languages since they capture common aspects like control flow, data layout, data dependencies, etc."
"we have used the coq proof assistant [cit] to develop the mechanized formalization of our translation and machine-check our proofs. we believe that this is the only way to stay honest with ourselves when proving something about a formalization that is so complex. a proof assistant helps organizing the formal development as a software project, ensures that the definitions are well-formed and in sync with each other, provides automation for proving routine tasks, and checks that the proofs are correct, without missing any cornercase and without forgetting any assumption that was made. this gives us high confidence in the final result, which one cannot easily obtain by \"handwaving\". the formalization encompasses about 17k lines of coq code in total."
"our type system for the expi calculus features subtyping and parametric polymorphism [cit] . this makes the type system very expressive and has allowed us to devise a very precise unification-based type inference algorithm. this decreases the type annotation burden on the user and improves the readability of the protocol models. parametric polymorphism also allows us to have only a small number of \"generically\" typed constructors and destructors and still be able to specialize them. parametric types can be nested, which naturally corresponds to the types of the nested terms and allows us to keep more information about the inner terms even after several destructor or constructor applications. the nested types allow us, for instance, to express the type of messages sent and received over a channel, or to model the fact that an encryption and the corresponding key work on messages of the same type."
"where is the s-by-e eeg matrix with s the total number of samples, and e the number of eeg channels. 1 and 2 are toeplitz binary sparse matrices with the following dimension: s-by-s_trial (s_trial: number of samples for one trial). 1 and 2 correspond to the stereotypical evoked response matrices of dimension s_trial-by-e and is the additional noise term. therefore, 1 1 corresponds to the specific erp responses for the high workload condition, whereas 2 2 corresponds to the common response for all conditions (low and high workload). the equation (1) can also be written as follow:"
"formalizing our code generator in a proof assistant turned out to be harder than we expected. like the large majority of similar tools before it [18, 20, 21, [cit], expi2java targets the java programming language. this has many pragmatic advantages for building a usable and secure tool: from type and memory safety, an extensive standard library and the cryptographic service provider architecture, to the ease of integrating the generated code into existing applications. however, java is a very complex programming language, and merely adapting an existing mechanized formalization of a subset of java [cit] to suit our particular needs turned out to be a quite daunting task. about 7k loc out of the 17k loc of our formalization are solely concerned with defining our target language, vpj."
"this quotient is maximized by solving a generalized eigenvalue problem. the xdawn filters are designed to enhance the ratio between the signal and the signal plus noise ratio (ssnr); (, ). the spatially filtered signal z, made of what we call 'virtual sensors', can then be obtained by applying the weights, or spatial filters w onto the data x: figure 1 . trial structure. participants have to memorize 2 or 6 digits and then answer whether the test item was present in the memorized list. the circled segment indicates the item on which the analyses were focused."
"machine learning techniques have been already used in the field of compilation and program transformation [cit] . all these approaches share the same principles as they obtain an abstract representation of the input programs in order to apply machine learning methods. nevertheless, previous approaches target some specific architectures limiting the applicability of the approach and making it not suitable for heterogeneous platforms. in our approach, we obtain program abstractions in order to enable the machine learning-guided transformation and compilation of a program written in c for heterogeneous systems. additionally, none of the previous works have explored the use of reinforcement learning methods [cit] in the field of program transformation and compilation."
"for active bci systems it is common to use spatial filtering methods such as common spatial pattern (csp) filters to improve classification performances of electroencephalography (eeg) data [cit] . this is done using epochs of band pass filtered signal, and then classification is carried r. n. roy out using features such as the log variance of this signal. furthermore, several active bci systems use event-related potentials (erps) as features. to increase classification performance using these markers, they often encompass spatial filtering steps, e.g. using the xdawn algorithm which has originally been developed for the p300 speller application [cit] ."
"moreover, java is an imperative language, and lacks certain functional features that would have greatly simplified the symbolic representation of terms: immutable data structures, structural equality, and pattern matching. a programming language like scala"
"given the nature of the problems associated with the transformation toolchain, we propose the use of different machine learning techniques to effectively guide the transformation process. concretely, we have used reinforcement learning methods to learn transformation strategies that take into account the non-monotonic behavior of the transformation sequences, and we have used classification methods to learn appropriate states that can be translated and compiled in the target platforms. machine learning techniques require descriptions of the problem domain; therefore, we compute an abstract representation of the input programs and the intermediate code generated during the transformation process."
"the translation used for code generation can also be customized, since expi2java uses templates to generate the protocol implementation. the user can change the templates and the class stubs to simplify integration of the generated code into existing applications. our formalization targets the default translation used in the tool, with only minor simplifications."
"in machine learning and statistics, classification is the problem of identifying the category to which a new observation belongs among a set of pre-defined categories. classification is done on the basis of a training set of data containing observations for which it is known to which category they belong [cit] . different approaches can be used to classify. we have decided to start evaluating the adequacy of classification trees for our problem since it performs feature selection without complex data preparation."
"the diagram in figure 1 shows the workflow of expi2java. the expi calculus model can be verified using proverif and type-checked with respect to the expi2java configuration (we defer the discussion about the type system to §5.3). the code generator takes the model and the configuration, and generates the symbolic library and the protocol implementation using code templates (special snippets of java code)."
"the second phase of the toolchain consists of a translation process which generates code for a given platform taking as input the code generated in the first phase. this platform-specific code is fed to a platform-specific compiler. this second phase poses another problem: determine when code being subject to the transformation process is ready for the translation phase, i.e. when the transformation process can stop. since some platforms might require a long compilation process (taking up to tens of hours for some fpga compilers, for example, for moderate-sized programs), the toolchain should translate and compile only code that is likely to be successfully compiled on a given target platform, rather than trying to compile every single code obtained during the transformation phase. additionally, when considering the first and second phases together, the intermediate code states obtained in the transformation sequences of the first phase might not improve monotonically non-functional metrics (e.g. execution time, energy consumption, etc.), but still produce code with better performance in the second phase."
"showing that the code generated by the translation is well-typed was much harder. most typing rules require providing the exact types of all subexpressions and only fail in the last moment if a wrong type was chosen. another problem was finding the right invariants to type-check the code generated by recursive functions. an incorrect invariant (i.e., the type of the expression we are trying to type-check and the preconditions) usually becomes noticeable only when applying the induction hypothesis, after having constructed a big part of the failed proof attempt."
"after training, three different use case applications were used as prediction set. we have selected applications that share code patterns with the examples in the prediction set. this matches the idea that sets of transformation rules can very naturally depend on the application domain, and therefore it makes sense that the training sets also belong to the same domain. the applications in the prediction set were mechanically transformed according to the previously learned sequences and finally translated by hand into opencl. independently, opencl versions of the initial c code were manually written to provide a comparison with what a programmer would do. the evaluation of the reinforcement learning approach was made by comparing the performance of the hand-coded versions with that of the versions mechanically generated. fig. 7 shows the results obtained for the training and prediction sets in terms of speedup of the opencl versions w.r.t. the original sequential program. by comparing the results for the training and the prediction sets we can assess the behavior of the transformation system after the learning phase. looking at results in fig. 7 we can see that the transformation sequences leads to code that provide acceleration factors comparable to those of the manually coded versions. although this preliminary evaluation is based on a small sample, it shows that our approach seems promising in tackling the problem of deciding strategies for rule-based program transformation systems. the results in this section show a preliminary, but very positive, evaluation for the generation of code for gpu platforms. we believe that the same approach can be followed in order to target different platforms which are present in heterogeneous systems. in this way, a separate state-action table can be used for learning adequate transformation sequences for each target platform and rewards would be assigned based on the performance of the final code generated for each platform."
"our whole processing chain consisted of a pre-processing step, the spatial filtering step if required, and then a subjectspecific classification step. the classification step was similar for the 3 chains and was performed using a fisher linear discriminant analysis (flda), with a shrinkage covariance estimation [cit] and a 10-fold cross-validation. at 100 hz we had 60 samples per trial. as mentioned earlier we had 72 trials per workload level. therefore, for the 10-fold cross-validation process we had 65 trials per workload level to train the classifier, and 7 to test it. we compared the performances obtained using the 3 processing chains mentioned earlier (b.)."
"thus, rather than having cpu-based supercomputers a growing trend goes towards integrating a wide range of specialized (co-)processors into traditional supercomputers. these specialized architectures can outperform general purpose platforms while requiring lower energy consumption and less real estate. a study of energy consumption in data centers [cit] the study explained the reduction from 2.2% down to 1.3% as a consequence of the increasing use of energyefficient computational platforms, showing the economic impact of using greener architectures."
"the generated code relies on a runtime library containing classes that implement the cryptographic primitives used by the protocol specifications. we provide two interchangeable versions of the runtime library: a symbolic library that is used in our formalization and proofs, and a concrete library that implements real networking and cryptography. this separation allows us to abstract away from the complexity of network communication and cryptography when testing and debugging locally. the symbolic library is for the most part automatically generated by our tool and is described in more detail in §6. the concrete library contains implementations for most of the common cryptographic primitives and data types out of the box, and can easily be extended by the user. it uses the standard java cryptographic providers to ensure interoperability with other existing protocol implementations."
"the features described above were sufficient to obtain preliminary results for a set of use cases (section 6). however, we plan to increase the vector of features as the set of rules and use cases grows."
"the introduction of multi-core processors marked the end to an era dominated by a constant increase of clock-rate, following moore's law. today, even the strategy to increase performance through higher core-counts is facing physical limitations. in addition, the performance increase results in power consumption issues which are currently one of the main limitations for the transition from petascale systems to next-generation exascale systems. hence, the metric for measuring performance improvement of computational platforms has changed from absolute flops numbers to flops per watt and flops per surface unit of hardware."
"outline §2 discusses related work. §3 introduces the features and workflow of our tool while §4 reports on the tls case study. §5 presents our source language, the extensible spi calculus. §6 describes the formalization of our target language, variant parametric jinja. in §7 we give a high-level overview of our translation. in §8 we discuss our formalization, proofs, and some of the lessons we have learned. finally, §9 discusses directions for future work and concludes. the implementation and documentation of expi2java are available online 3, together with the coq formalization and proofs. the details that had to be omitted in this paper due to the lack of space are available in an online appendix."
"for both the first 2 chains, 60 features were used for classification. for the 3 rd chain, given that 2 virtual electrodes were considered, 120 features were used for classification. both behavioral performances and classification results were compared using repeated measures anovas and tukey post-hoc tests. classification performances were also compared against chance level using single means t-tests. the significance level was set at 0.05."
"tls is a very complex protocol, it supports many different cryptographic schemes, optional parameters and extensions. our tls model implements tls v1.0 [cit] with the aes extension [cit] and the server name indication (sni) extension [cit] . the model includes both client and server sides, the handshake, the application data protocol and the alert protocol. we support dynamic cipher suite selection between 6 different cipher suites (including aes, rc4 and 3des encryption with different key lengths, sha1 or md5 hmacs and rsa key exchange). one of these cipher suites is dynamically chosen during the handshake. supporting multiple cipher suites in tls in older tools such as spi2java would require duplicating the whole protocol model for each of the cipher suites. in order to prevent this, expi2java allows parameterizing processes with respect to the employed cryptographic algorithm. using the parameterized processes, we could add support for 6 cipher suites with only a few lines of code. the only noteworthy tls features we have not implemented are: session resumption (a performance tweak where a vulnerability [cit] ), the client-authenticated handshake (a rarely used feature) and record fragmentation (unsupported by some popular servers and clients, and therefore a very rarely used feature). the handshake messages that are used for key exchange algorithms (other than rsa) are also not supported."
"we prove that the expressions generated for the global expi processes, the constructor and destructor methods and the classes representing expi types are well-typed assuming that our invariants hold. in the end, we use these results to show the following theorem:"
"in the future it would be very interesting to show that the translation presented here preserves the trace properties, and, more ambitiously, the security properties (e.g., the robust safety) of the original protocol model. the former could be achieved by using weak labeled simulation to relate vpj programs to expi processes, while for the later one would have to show that this simulation is contextual and that each vpj attacker can be mapped back to an attacker in the expi calculus. we believe that the current work builds a solid ground on which the preservation of security properties can be formally investigated."
"rule-based program transformation systems support the formulation of basic code transformation steps as generic rules and arrange their automatic application. this scheme offers the flexibility of splitting complex code transformations into small steps, admitting efficient implementations that can scale to large programs. by adding more rules, the transformation system can also increase its capabilities in a scalable way. rule-based systems also allow to decouple the definition of transformation steps (i.e. rule applications) from the strategies followed to apply the rules. this decoupling provides more flexibility to try different transformation strategies and select the best one according to the purpose of the system [cit] . rule-based transformation has been used before to generate code for different computational platforms."
"implementing cryptographic protocols is a difficult and notoriously error-prone task, where even the smallest error can cause very serious security vulnerabilities 1 . one way to prevent many such vulnerabilities is to model the protocol in a process calculus [cit], check the security of the model [cit], and then automatically generate a secure implementation from the protocol model. automatic tools exist that can generate protocol implementations starting from such verified models together with configuration information that allows them to produce code that is interoperable with other implementations of the same protocol. this paper introduces expi2java 2, a new tool that brings code generators for security protocols even closer to reality. expi2java is highly extensible and customizable: the user can easily add new cryptographic primitives, configure all the relevant parameters, customize implementation classes, and even customize the code generation process by editing the provided templates. commonly used cryptographic primitives and data types are supported out of the box, and the user has full control not only over the high-level design of the protocol, but also over all the low-level details, including the precise bit-level representation of messages. to illustrate the flexibility of our tool we have generated an interoperable implementation of tls v1.0 client and server from a protocol specification verified with proverif [cit] . tls is orders of magnitude more complex and sophisticated than the protocols used by the previous code generation experiments."
"we have proved that the symbolic library and the vpj code generated by the translation are well-typed. this is an important consistency check for our definitions (it helped us to find out, for instance, that the first version of our translation was using the field access in an inconsistent way) and justifies the usage of our expi calculus type system to prevent generating ill-typed vpj programs. in the longer-term perspective these proofs will be needed for proving the correctness of our translation."
"rule-based transformation systems pose different problems, like the explosion in the number of states to be explored, arising from the application of transformation rules in an arbitrary order or the definition of a stop criteria for the transformation system. for the latter we propose the use of classification trees and for the former we propose a novel approach based on reinforcement learning. both approaches use code abstractions to effectively capture relevant features of a given code. in order to illustrate the approach, listing 1 shows code performing a 2d convolution with its associated abstraction at the top of the listing (see section 4 for a description of the code abstraction). the code in listing 1 is well suited to be parallelized, by adding openmp pragmas, for a multi-core cpu. however, different code transformations, altering different code abstractions features, are required to ease the translation to different target platforms like gpus or fpgas. for example, by coalescing the two outer loops, obtaining a linear iteration space, or by transforming the data layout of 2d arrays into 1d arrays, we obtain a sequential code easier to map onto the two platforms mentioned before (see section 5.3). the code features to be transformed are represented in the abstraction vector in listing 1 through the first element (maximum loop depth), the twelfth element (number of non-1d arrays) and the fourteenth element (total number of loops). our approach learns different sequences of code transformations that change the relevant abstraction features and obtain, for the example in listing 1, a code performing a 2d convolution within a for loop with a linear iteration space, using 1d arrays. we have performed a preliminary evaluation of the approach obtaining promising results that demonstrate the suitability of the approach for this type of transformation systems."
"in order to ease the programmability of heterogeneous platforms, we have proposed [cit] a rulebased transformation system which can help developers to convert architecture-neutral code into code which can be deployed onto different specific architectures. while the mentioned transformation system is capable of performing sound program transformations, it lacked an automatic method to guide the transformation process. this paper describes a machine learning-based approach to learn transformation strategies from existing examples and to guide that system by using these strategies."
"a classification tree is a simple representation to classify examples according to a set of input features. the input features should have finite discrete domains and there is a single target variable called the classification feature. each element of the domain of the target variable is called a class. in a classification tree each internal (non-leaf) node is labeled with an input feature and each leaf node is labeled with a class or a probability distribution over the classes. thus, a tree can be learned by splitting the source data set into subsets based on values of input features. this process is recursively repeated on each derived subset. the recursion is completed when the subset of data in a node has the same value for the target variable, or when splitting no longer improves the predictions. the source data typically comes in records of the form"
"participants' performance to the test item, i.e. reaction times and accuracy were recorded, along with their eeg activity using a brainamp tm system (brain products, inc.) and an acticap® equipped with 32 ag-agcl active electrodes positioned according to the extended 10-20 system. the reference and ground electrodes used for acquisition were those of acticap, i.e. fcz for the reference electrode and afz for the ground electrode. the data were sampled at 500 hz. the electro-oculographic (eog) activity was also recorded using 2 electrodes positioned at the eyes outer canthi, and 2 respectively above and below the left eye."
"the transformation of c-like programs and its compilation into synchronous architectures, like fpgas, and asynchronous platforms, like multi-core cpus, has been addressed before [cit] . however, the input language of this approach (handel-c) is meant to specify synchronous systems, thus limiting its applicability to this type of systems. a completely different approach is to use linear algebra to transform the mathematical specification of concrete scientific algorithms [cit] . here, the starting point is a mathematical formula and, once the formula is transformed, code is generated for the resulting expression. however, a reasonable acceleration over hand-tuned code happens only for algorithms within that concrete domain, and applying the ideas to other contexts does not seem straightforward."
"using functions a, sm, am and q, the strategy of the transformation toolchain for selecting rules at each transformation step can be modeled with function rs, defined in fig. 2 . this function takes as input a given code c and selects the transformation rule r associated to action am(r) that maximizes the value provided by q for the state sm(a(c)) associated to input code c. thus, the rule selection strategy is modeled by function rs defined in fig. 2 ."
"the eeg signal was band-pass filtered between 1 and 40 hz, re-referenced to a common average reference and corrected for ocular artifacts using the signal recorded from the eog electrodes and the sobi algorithm [cit] . it was also down-sampled to 100hz. the erps were extracted by epoching the eeg signal from 200ms before stimulus onset to 600ms after stimulus onset (test item). three processing chains were considered and the features used for classification were the erps from: 1) one channel (cz), no spatial filter; 2) one virtual electrode computed from the 32 electrodes; 3) two virtual electrodes computed from the 32 electrodes."
"in this paper we have introduced expi2java, an extensible code generator for security protocols. we have illustrated the flexibility of expi2java by generating interoperable implementations of a client and a server for tls v1.0 from a protocol model verified with proverif. we have formalized our source and target languages as well as the translation between them using the coq proof assistant, and proved that the generated code is welltyped if the original protocol model is well-typed. this increases our confidence in the translation, and justifies the usage of our expi calculus type system to catch all type errors as early as possible and present understandable error messages. additionally, we have proved the consistency of the destructors in our default configuration."
"mental workload estimation can be achieved using eventrelated potentials as neurophysiological markers. to enhance single-trial classification performance, spatial filtering is commonly done in active bcis and has proven to be particularly efficient. however, it is seldom performed for passive bci applications. in this study, we assessed the importance of enhancing the contrast between workload conditions using a spatial filtering step. the algorithm we used, xdawn, allowed us to significantly improve classification performance compared to a processing chain that does not include a spatial filtering step, and to obtain outstanding performances with up to 98% of correct classification using two virtual electrodes. it should be noted that there was an important inter-subject variability, which may explain why the filtered erps present small variations with load when averaged across subjects."
"since java is an explicitly typed language, any code generator targeting java needs to generate type annotations for variables, fields, method arguments and return values, etc. these type annotations cannot be generated out of thin air, and asking the user to manually annotate types in the automatically generated java \"spaghetti code\" would be a usability disaster. we solve this problem by asking the user to provide type annotations while writing the protocol model. our code generator uses the type information in the model to generate the typing annotations needed by the java type-checker. additionally, we want to prevent that a user who makes mistakes in a protocol model finds out that the generated implementation does not even compile in java because of typing errors in the automatically generated code, which the user does not understand. so we provide a type-checker for the expi calculus that prevents generating ill-typed java code. our type-checker immediately reports inconsistent usage of terms in an understandable way -in terms of the specified protocol model the user has written. in §8.1 we show that if the original protocol model is well-typed with respect to our expi calculus type system then our translation is guaranteed to generate a well-typed java program."
"opening the access of heterogeneous platforms to a wider spectrum of users is an important issue to be tackled. in past decades, different scientific areas showed an enormous advance and development thanks to the use of massive parallel architectures to implement in-silico models. the use of \"virtual labs\" through computational platforms allowed a better understanding of the physical phenomena under study, investigate a much wider range of solutions, and drastically reduce the cost with respect to performing real experiments. the computational requirements of scientific and industrial applications are pushing a driving the development of exascale systems. for that reason, new programming models, tools and compilers are required in order to open the access of exascale systems to a wider programming community, enabling translational research to enhance human welfare."
"this study paves the way to building better processing chains for mental state monitoring applications, such as elearning. however, it should be noted that our mental workload estimation is only based on event-related potentials of task-related or task-relevant items. therefore, although we achieved very high classification performances, this is a focused improvement, for applications in which the system knows and controls the visual (or auditory) display. hence, it has low generalization capabilities. in order to progress towards efficient passive bci systems that can generalize to any task, the next step is to evaluate how taskirrelevant probes can be used to estimate mental workload."
"once the training set is defined, the reinforcement learning process requires tuning the two parameters in (1): the learning rate (α) and the discount factor (γ). for this purpose, an empirical study was performed in which parameter values leading to transformation sequences providing the fastest opencl versions were selected. as an outcome, a value of 0.5 was used for α and 0.6 for γ. also, reward values have been chosen in order to give a higher reinforcement to those sequences leading to final codes with better performance. in our example, the reward values used for the best sequences of each use case application are substantially higher, with a ratio of 1:100, with respect to the rest of transformation sequences."
"hence, one processing chain was based solely on the erps from the cz electrode which is a relevant electrode for workload estimation [cit] . in order to be fed to the spatial filtering algorithm, the data were concatenated to form an sby-e matrix (s: number of samples, e: number of electrodes) by placing all the trials end to end."
"memory model. vpj uses a realistic memory model with a shared heap and thread-local stacks. the stacks are maps from variable names to values. the heap maps memory locations @n to a class type t c and the field values of the corresponding instance. this heap model differs slightly from the one used in java, since we store the exact parametric type of each object, while in java the information about the type parameters is lost. 5 we have decided to store the parametric types to simplify the formalization of variant parametric types in coq and avoid the problems arising from java-like semantics such as the need for run-time type-checks to enforce type soundness."
we used the xdawn algorithm to enhance the discrimination between the erps of the test item in a low and in a high workload condition. the xdawn algorithm works as follow. the generative eeg signal model is given by:
"neuroengineering is a growing research field which encompasses mental state monitoring (msm). such monitoring is performed by what has recently been named passive brain-computer interfaces (pbci), systems that perform mental state estimation thanks to neurophysiological markers and feature translation algorithms [cit] . those pbcis provide new means to enhance and supplement the human computer interaction, with a major interest for safety applications. mental workload, which can be defined as the amount of mental resources engaged in a task, and more generally as task's difficulty [cit], is currently under focus for e-learning and driving applications [cit] ."
"the classes obtained for the target variable make it possible to define the final states of the transformation stage of the toolchain described in section 3, which will also serve to define the final states for the reinforcement learning algorithm that is described next. the classification-based learning described in this section has been implemented using the python library scikit-learn [cit] . this library implements several machine learning algorithms, provides good support and ample documentation, and is widely used in the scientific community."
"in order to use the reinforcement learning state-action in our setting, we need to define some mappings. the abstraction of a concrete piece of code is provided by function a (described in section 4). code abstractions and transformation rules must be mapped to states and actions, respectively, in order to index the state-action table. this mapping is done through functions sm and am (fig. 2) . based on the mapping of abstractions and rules defined, the reinforcement learning state-action table of (1) can also be modeled as a function q (see fig. 2 )."
"however, integrating different specialized devices increases the degree of heterogeneity of the system. on the other hand, compilers typically optimize code for only one (homogeneous) destination infrastructure at a time, requiring the developer to manually adapt the program in order to be executed on hybrid architectures. as a result, the high degree of heterogeneity limits programmability of such systems to a few experts, and significantly reduces portability of the application onto different resource infrastructures."
"listing 1 shows the initial code and the associated vector of features, as a code comment, according to the description in section 4. listing 6 shows the result of applying rule r 0 to the code in listing 1. the array input image is transformed into a 1-d array and the vector of features associated to code is changed accordingly. listing 7 shows the result of applying again rule r 0 to the code in listing 6. the array kernel is then transformed into a 1-d array and the feature vector is updated. listing 8 shows the result of applying rule r 0 to the code in listing 7. again, the array output image is transformed into a 1-d array and the vector of features updated. listing 9 shows the result of applying rule r 1 to the code in listing 8. the two outermost loops are collapsed into one for loop, but keeping an iteration space with the same number of iterations. note that the code abstraction reflects the change, since the number of loops has decreased by one. fig. 4 shows a table with the resulting values of the state-action table (q) for the transformation sequence described before. fig. 4 has as many rows as states obtained from the evaluation of sm(a(c i )) for each code c i, where c 0 is the initial code and c 4 is the final code classified as ready-code for an fpga. fig. 4 shows the learned sequence composed of four steps: three consecutive applications of rule r 0 and one application of rule r 1 . the values in q for this sequence are highlighted in blue, and they decrease from the state sm(a(c 3 )), with the highest reward, down to the initial state sm(a(c 0 )). this decay behavior is caused by the discount factor (γ) introduced in (1). it should be noted that the values in q for the final states are not updated by the recursive expression in (1). thus, the final state sm(a(c 4 )) keeps the initial value of 1. it should be noted that numerical values in table shown in fig. 4 are obtained as the result of an iterative and stochastic learning process as defined in (1) and using parameter values described in section 6."
"in the second step of the translation we implement the behavior of expi terms and processes in vpj; this is much more complicated than the fist step. table 7 gives an overview of how the different expi calculus constructs are represented in vpj. expi calculus types are modeled as additional generated classes in our symbolic library, and variant parametric types are used to represent type parameters. the instances of these generated classes represent expi terms of the corresponding type. the cryptographic algorithm names used in the destructor reduction relation are stored in fields of type string in the generated classes. expi constructors and destructors are represented as special methods in a symbolic library class named fun. we use a simple naming convention to distinguish constructor and destructor methods. terms are translated to vpj expressions that either access local variables or call constructor methods. processes are translated into larger code blocks that create and modify the terms stored in local variables and use the symbolic library to interact with each other. we use threads to model parallel composition of processes and shared memory to pass data between them. please refer to the online appendix for a more detailed description of the translation process."
"the translation from the expi calculus to vpj is performed in two steps. the first step translates protocol models to the global expi calculus, a variant of the expi calculus with a different semantics for name binders. the second and much more complex step translates global expi processes into vpj programs."
"as mentioned before, two outstanding problems faced by the transformation engine are dealing with efficiently finding transformation sequences in a very large (even infinite) state space with a non-monotonic fitness function and deciding when to stop searching, since there is no a-priori general way to determine that an optimum (in the sense of the most efficient possible code) state has been reached. our approach uses classification trees to solve the latter and reinforcement learning to solve the former. we will describe our approaches in the next sections."
"we have shown that each class of the symbolic library is well-typed in a vpj program containing a small number of java standard library classes (nullpointerexception, object, etc.) and the symbolic library classes. we show that all types used in class declarations are well-formed and all declared methods are well-typed. the proof is by case analysis on the corresponding expression and using the right case of the expression typing relation. it is not complicated, but quite long and tedious, because we need to give the correct type of each subexpression and show all premises of the typing rules."
"usually, pbcis make use of tools developed for active bcis. as regards csp filtering, it has been applied to estimate mental workload from several power bands by roy and collaborators, but with only 65.51% of correct classification [cit] . moreover, to our knowledge, when pbci systems use markers such as erps, they seldom perform spatial filtering. yet, it has recently been done in affective computing with promising results. indeed, mathieu and collaborators demonstrated that a spatial filtering method such as the xdawn algorithm could be used to enhance arousal estimation for negative emotions with up to 87% of correct classification [cit] . but then they used peak values as features, therefore adding a computational step which can be costly in terms of real-life applications. it seems important to try and perform such a classification directly on the whole single-trial erp and compare the classification performances between a chain that does not include the spatial filtering step and a chain that does. also, it should be interesting to evaluate the use of such a spatial filtering method for other states than affective states, such as mental workload."
"theorem 1 (trans-wt). if p is locally-closed, γ p and p (e, l), then l is well-typed and (e, l) is well-typed in a preallocated heap and an empty stack."
"the rest of the paper is organized as follows: section 2 reviews previous work in the field of program transformation systems in general and previous approaches using machine learning techniques. section 3 describes the toolchain where our machine learning-based approach is integrated. section 4 describes the abstraction of code defined in order to apply machine learning methods. section 5 describes the methods used to learn program transformation strategies. section 6 presents some preliminary results and, finally, section 7 summarizes the conclusions and proposes future work."
"the operator arg max in function rs may return, by definition, the empty set, a singleton, or a set containing multiple elements. however, in our problem, parameters α and γ as well as the reward values r t+1 appearing in (1) can be tuned to ensure that a single rule is returned, thus avoiding a nondeterministic rs function. section 6 gives further details on how we selected their values."
"we have verified some security properties of our tls model with proverif. in particular, we have used 3 secrecy queries showing that the secret key of the server, the \"pre-master secret\" nonce used to derive the session keys and initialization vectors, and the request that the client sends to the server using the application data protocol are not leaked. we have used 3 correspondence queries adapted from fs2pv [cit] to show message authentication for the certificate and pre-master secret. additionally, we have used 9 reachability queries providing a consistency check to ensure that different parts of the handshake are reached. the verification process for all 15 queries took about 9 minutes on a laptop with an intel® core2™ duo p7450 cpu."
"the source language of our translation is a variant of the spi calculus [cit], a process calculus for cryptographic protocols. we start with the variant of the spi calculus by abadi and blanchet [cit] -a subset of the language accepted by proverif [cit] . we extend it with the expi2java configuration in §5.2 and define a type system for it in §5.3."
"also, the introduction of weights into the multi-objective rewards would offer programmers the flexibility to select which non-functional property or set of properties they want to focus on for generating the final code."
"the types in the generated java code cannot be synthesized out of thin air, so we ask the user to provide type annotations while writing the protocol model. our code generator uses the type information in the model to generate the typing annotations needed by the java type-checker. additionally, it is important to detect all the typing errors as soon as possible, before the code generation phase, so that we can guide the user with helpful error messages. our tool uses a type-checker with variant parametric types and subtyping to prevent the incorrect usage of cryptographic primitives in the protocol model. this source-level type-checker rejects models that could lead to ill-typed java code early on, and produces errors in terms of the model the user has actually written, not in terms of the automatically generated code that the user would have a hard time understanding. moreover, our source-level type-checker performs type inference, which greatly decreases the annotation burden on the user."
"in the six major releases over the last three years we made a lot of progress in the form of both practical features and usability improvements, turning expi2java from a prototype into a mature, useful and usable tool. we provide a detailed user manual and a tutorial that uses the perrig-song mutual authentication protocol as a running example [cit] . more sample protocols such as needham-schroeder-lowe, andrew and fiaba are provided in the expi2java distribution, together with the tls implementation described in §4. expi2java is free software and is distributed under the terms of the gnu gplv3."
"the crucial features of the expi calculus are its extensibility and customizability. the user can extend the sets of types, constructors and destructors, redefine the reduction relation for destructors and provide different implementations for the constructors and destructors. our whole calculus is parameterized over what we call a configuration -a collection of several sets and functions that define the behavior of types, constructors and destructors (see table 2 ). we have defined a default configuration that is sufficient to model most cryptographic protocols, please refer to the online appendix for more details. table 2 . configuration"
"the precise time scale and temperature scale of the upside models is intentionally left arbitrary because the coarse-graining process may leave us without a linear relationship to physical time and temperature. the speed-up of upside simulation due to the smoothing of side chain interactions is likely to have a disproportionate effect on time scales for condensed structures as compared to extended structures. regardless, the equilibrium population distribution that determines the free energy is expected to be approximately correct, as well as the order of dynamical folding events. the precise relationship of upside time scales to physical time scales is left to future work."
"although the slope has greatly decreased of rmsd change with respect to the number of steps over the iterations, there are indications that the parameters have not yet converged. earlier tests, however, showed that continuing the contrastive divergence until convergence does not necessarily produce better results, as has been previously observed [cit] . when large barriers surround the native states, minimal relaxation of the conformation occurs, which in turn provides little new information, and further fine-tuning may even reduce the accuracy of the model. potentially the decreased exploration in the native well in the later stages overtrains the model to distinguish between native and near-native structure at the expense distinguishing against a more diverse ensemble. early termination of optimization has been observed to favor simpler models [cit] ."
"specifically, we have developed a procedure involving extremely short simulations in the native energy well, coupled with optimization using contrastive divergence, to parameterize a sophisticated coarse-grain model. underlying the model is a re-evaluation of the common assumption that increased detail is the path to greater accuracy. this requirement for detail is mitigated with trajectory-based training because less expensive models allow more extensive exploration leading to higher accuracy. we have also shown that very large numbers of parameters (even *20000 in our case) are no obstacle to producing accurate proteins models using trajectory-based training. while over-fitting is always a concern, the severity is greatly reduced because contrastive divergence is training against the vast possibilities of alternative protein conformations explored by conformational sampling. additionally, contrastive divergence automatically obtains balanced parameters such that no particular interaction overwhelms the others. we contend that this balance between parameters is more important than the accuracy of any particular term."
"contrastive divergence training has been shown to be effective for many machine learning problems [cit], even without having simulations that converge to the boltzmann ensemble. to test the accuracy of contrastive divergence on our protein model, we attempt de novo folding of a benchmark set of small, fast-folding proteins similar to those used in references [cit] as well as various casp11 targets investigated by other physics-based approaches (figs 4 and 5) [cit] . before training, we remove homologous proteins from the training set to help ensure that this would be a true de novo prediction."
"trajectory-based training enables rapid simulations with accurate folding and ensembles is in a minor cluster even when it is within 3 å (e.g., gpw, ntl9). the designed 3-helix bundle, α3d [cit], has a mirror image as a second heavily populated cluster."
"in constant temperature simulations, we observe reversible folding to the native state for a number of proteins in our test set in core-days (figs 6 and 7) . the time scales of folding indicated by these trajectories imply that the time scales we employed in the contrastive divergence simulations are far less (often a factor of 100 or more) than required to equilibrate these proteins, implying that contrastive divergence is optimizing only over fluctuations in or near the native well."
"decoupling representations of protein physics is a key aspect of the upside model. in particular, upside decouples the representation of the protein used for dynamics, an n-c α -c backbone model, from the representation used for computing energies and forces, a complex representation that includes oriented side chain interactions. this combination allows us to build up the sophisticated coordinates needed to represent solvent exposure of side chains, geometry of hydrophobic packing, and side chain-backbone hydrogen bonding without the cost of running dynamical simulation on a complex model with slow equilibrium. the largest improvement comes from applying belief propagation to the side chain degrees of freedom so that we represent detailed side chain physics at the χ 1 /χ 2 -level without incurring the roughening of the energy landscape and slowing of the dynamics normally associated with detailed sterics of side chain interactions. it is an open question to determine how much molecular detail must be retained for accurate protein energetics, but upside provides a flexible framework to explore these issues without compromising the simple backbone representation of dynamics."
"our implementation of contrastive divergence considers two ensembles, one closely restrained to the native (crystal) structure and another that is free to diffuse away during simulations ( fig 2) . in a perfect model, an unrestrained ensemble would remain close to the native structure. for an inexact model, differences arise, such as an excess of backbone-backbone hydrogen bonding in the free ensemble. reducing the hydrogen bond energy would shift the free ensemble closer to the native ensemble. the parameter modification must be small, however, because shifting the hydrogen bond energy may adversely affect other features of the ensemble, e.g., by reducing the burial of hydrophobic residues. accordingly, after simulations are run on the first set or \"minibatch\" of 12 proteins in our 456 protein training set, we modify all the parameters with small updates to shift the simulation ensemble to better match the native-restrained ensemble. simulations are repeated on the next of the 38 subsets of 12 proteins, and the paramters are updated again. the algorithm is converged when no parameter can be altered to shift the free ensemble closer to the native-restrained ensemble."
"the balancing of these various energies has been a major effort, and the balance is continually being adjusted as new force field biases are identified [cit] . however, the adjustment of some parameters to correct one deficiency can inadvertently degrade performance of other quantities. in order to achieve the correct balance, all terms in the model should be trained together, rather than adjusted with an ad hoc procedure to correct each identified deficit."
"step 1. the loop begins (upper left corner) with each residue in the protein being represented with 3 backbone atoms, the n, c α and c. based on the position of these atoms, the carbonyl oxygen, o, and amide proton, h, are deterministically placed."
"here, we demonstrate that we can achieve de novo folding for a diverse collection of proteins by combining our fast-equilibrating upside model with a contrastive divergence procedure that optimizes the stability of the native well. we demonstrate that gradient descent on energy terms using only data from sampled trajectories is sufficient to parameterize a protein model with tens of thousands of parameters. the resulting parameters are sufficiently balanced and accurate to achieve reversible folding for many proteins in our validation set. in addition, the resulting model is an excellent starting point for large scale protein simulations using more detailed models as well as the integration of large quantities of external information (such as predictions of residue contacts)."
"for seven casp11 targets between 65-178 residues, we compared upside with two physicsbased approaches that participated in casp11 (fig 5) : the cornell-gdansk group's coarsegrained united residue model \"unres\" [cit] and maccallum, perez and dill's highly accelerated molecular simulation method \"meld\" (modeling employing limited data), a bayesian approach that utilizes physically-based heuristics combined with atomistic implicit solvent simulations [cit] . it should be noted that both methods employ psipred, a secondary structure predictor employing evolutionary information [cit] . in contrast, upside's secondary structures emerge during folding solely are a result of our energy function."
"another factor is the parameterization of the energy function with the training algorithm needing to balance the influences of all interactions. protein thermodynamics reflects a delicate balance between the free energy of the folded and unfolded states. if one interaction is slightly too large, the entire landscape can be severely distorted. for example, if backbone hydrogen bonding energies are too large compared to backbone-solvent interactions (which includes hydrogen bonds between the backbone and water), an excess of hydrogen bonding ensues and pathways become dominated by unrealistically stable native-and non-native secondary structures. in an extreme situation, the lowest energy structure may have long helices involving nearly all residues."
"the free ensemble is generated using 5000 time units of dynamics (approximately 10 wallclock minutes), with the first half being discarded as equilibration. unless the native state is particularly unstable, this time is insufficient for exploration of the conformational landscape much beyond the native basin (rmsd within 6 å) and so produces only a locally-equilibrated ensemble."
"the native ensemble is traditionally defined as a single conformation. this δ-function distribution is problematic for proteins because they are dynamical molecules. additionally, the solution ensemble may differ from the crystal structure for multiple reasons, including crystallographic packing. to reduce the impact of these issues, we replace the exact ensemble structures with the ensemble restrained to be near the crystal structure, within approximately 1 å c α -rmsd. this procedure is analogous to the restrained equilibration of crystal structures required to prepare systems for all-atom molecular dynamics. to account for changing parameters, we apply the restrained relaxation at every optimizer step."
"contrastive divergence optimization has been applied to gō-like protein potentials sampled with crankshaft monte carlo moves [cit] . these works optimized only tens of parameters, and the resulting model is used to fold protein g and 16-residue peptides."
"furthermore, our temperature-denatured states have high r g near the midpoint of the transition, consistent with experimental results and inconsistent with many all-atom molecular dynamics folding simulations [cit] . at the peak of the heat capacity, the r g is *15% smaller than the predicted from experimental data while the r g at high temperature is *10% larger than the experimental value. both r g values are significantly larger than those in most atomistic molecular dynamics simulations [cit] ."
"note that conditional on low hydrogen bonding, the radius of gyration (r g ) at high temperature and at the peak of the heat capacity are quite similar. this suggests the increase in r g for the unfolded state as temperature increases is driven by a reduction in backbone-backbone hydrogen bonds rather than side chain effects. based on these results, two observations should be reconciled. the first observation is the presence of a sharp phase transition with a single peak for the heat capacity. the shape of the phase transition, but not its amplitude, is consistent with a cooperative folding transition. the second observation is the unrealistically large level of residual hydrogen bonding in the denatured state at temperature of the maximum in the heat capacity. although the hydrogen bonding is less than that in the native state, the residual hydrogen bonding indicates that the transition is not fully cooperative. these observations may be explained by the essential feature of the contrastive divergence process, that it must balance the competing energy terms of the model so that no one energy dominates. more extensive training, for example using a more diverse ensembles that contain conformations outside the native well, may remove the excess hydrogen bonding."
"other studies have trained protein energy functions using libraries of decoys [cit] . such efforts are challenging because atomic energy functions have rugged energy landscapes where even small structural differences can produce large energy differences. this ruggedness implies that scoring decoys by energy without first relaxing them is problematic for the sharplydefined force fields necessary to describe protein physics, a problem that contrastive divergence avoids."
"the parameters are initially set to those used to optimize side chain (χ 1 ) accuracy [cit] . the contrastive divergence training rapidly improves the model's average rmsd over a minibatch from 6 å to 3 å. this decline is accompanied by rapid change in the parameters. to reduce parameter fluctuations and fine-tune the results, we reduce the optimizer step size by a factor of four after two full passes through the 38 minibatches."
"the maximum likelihood method requires the computation of the derivative of the free energy, which involves a summation over an equilibrium ensemble. such a requirement necessitates a very long simulation to update parameters. still, this approach can be viable when used with very small proteins on which the simulations converge quickly. a variant of maximum likelihood is given in ref. [cit], where decoys are generated and a maximum likelihood model is fit to adjust the parameters to distinguish between near-native and far-from-native conformations. the potential is trained on a single protein, tryptophan cage, and then the resulting potential is applied to a number of α-helical proteins with some success."
"the ready generation of boltzmann ensembles allows for a wide range of computational studies of protein folding, dynamics, and binding. for example, computational screening of large numbers of proteins for foldability should be tractable as is the study of hydrogen exchange and folding kinetics. additionally, in studies that incorporate experimental or bioinformatics data, including contact predictions, upside provides an inexpensive bayesian prior distribution over protein structures that may be updated using experimental information. this provides accurate predictions that make essential use of the totality of protein physics as encoded in the upside model, while being inexpensive enough to allow validation and iteration on large numbers of proteins."
"furthermore, it is difficult to evaluate the reliable energies of decoys without relaxing the decoys. methods based on simulation ensembles (such as maximum likelihood and contrastive divergence) are well-defined and do not need pre-constructed decoy libraries."
"the hydrogen bond strength unexpectedly appears to converge to a significantly smaller value during the late, fine-tuning stage than during the early phase with larger optimizer steps. we speculate that the extra noise in the side chain interactions during the larger optimizer steps may in aggregate cause stronger side chain interactions for the protein. this effect would necessitate a large hydrogen bond energy to balance the increase in side chain interactions. the final pair-wise energy functions between the side chain beads and either the backbone carbonyl oxygen or the amide proton, and the bead-bead interactions are shown in fig 3."
"step 2. each side chain, represented by a single oriented bead, is assigned an initial probability for being in 1-6 states, depending on the residue type and the average frequency observed in the pdb. the state of the bead is defined by its position and an orientation, (x,y,z,v), where v is a unit vector, relative to the peptide plane."
"the upside simulations tend to achieve the correct secondary structure with a small number of distinct tertiary arrangements. this diversity in tertiary structures occurs as mirrored three helix bundles for α3d and protein b, as well as the subtle re-arrangements of nug2. for the three largest casp11 targets we investigated (115-178 residues), the secondary structure performance is noticeable poorer, implying a strong coupling between secondary and tertiary structure formation for these larger systems (fig 5) ."
"by employing the computationally fast yet detailed upside model, we can use multiple trajectories to train tens of thousands of parameters simultaneously to simulate protein folding and dynamics. the training successfully produces low-energy, native or near-native structures with sharp folding transitions for most of our validation proteins. the strategy's success argues that simpler (in atomic representation) models that can be globally parameterized can rival more detailed but slower models whose parameterization is more challenging. we achieve success for some proteins in terms of accurately folding to low energy native state and achieve thermodynamic equilibration, but still fail on others. we hypothesize that the short-time contrastive divergence we are using does not provide a sufficient library of large changes in the tertiary structure to enable the potential to properly distinguish the various conformations. this issue will be addressed in future studies. coupling large computational resources with markov state models [cit] should improve training of the upside model by exploring a larger and more diverse conformational landscape on each contrastive divergence step."
two temperature replica exchange simulations are run for each of the 23 proteins (14 replicas each). the first set is initialized from the native configuration to assess the stability of the experimental structure for the potential obtained from contrastive divergence training. the second set is initialized from an unfolded state (random ramachandran ϕ and ψ angles) to test upside's capability to find the native structure which is reflection of both the accuracy of the energy function and the method's ability to search conformational space. each range of temperatures is chosen to be large enough to cover the unfolding transition for each given protein.
step 3. the pair-wise state probabilities of all side chains are simultaneously and rapidly calculated using belief propagation to produce the lowest system free energy.
"we add an additional term to capture desolvation effects by computing the approximate number of side chains n i within a hemisphere above the c β (see s1 text in supporting information). high values of n i correspond to buried residues. the total energy is depends on the chemical identity of the i − 1, i, and i + 1 residues. the ramachandran potentials are based on the turn, coil, or bridge (tcb) ramachandran probability models in the ndrd backbone library [cit] . we introduce a single parameter controlling extra stabilization of angles consistent with β-sheet geometries to allow training to counteract an observed tendency for our model to overstabilize helices. the backbone non-bonded interactions are governed by a distance-and angle-dependent hydrogen bonding potential whose magnitude (but not geometry) is chosen by contrastive divergence. the backbone n, c α, c β, and c feel a steric repulsive interaction when their internuclear distance is approximately 3.0 å. source code for upside can be obtained from https://github.com/sosnicklab/upsidemd, and the results of this paper can be reproduced using the version tagged trajectory_training_paper."
"introduction since anfinsen's original demonstration that a protein's sequence determines its structure, multiple computational strategies have been developed to predict a protein's structure from its sequence. an additional facet of this challenge is to replicate the energy landscape that defines both the folding process and other dynamical properties. in the absence of other information, coarse-grained models with one or a few beads per residue are too simplistic for de novo structure prediction. c β level models having authentic protein backbones with ϕ/ψ dihedral angles, but lacking side chain rotamers, have achieved some success [cit] . within the last decade, allatom, explicit solvent methods have become successful for the folding of some small proteins, although the ability to replicate the properties outside the native basin requires substantial improvement [cit] . for the folding process, it is unclear which representation provides the optimal combination of detail and computational expense to replicate protein folding and dynamics. integral to the choice of representation is which interactions to include, such as hydrogen bonding, van der waals interactions and hydrophobic burial."
"for all 20 proteins below 100 residues, the lowest c α -rmsd structure obtained starting from an unfolded state is within 5 å of the native state (54% within 3 å). in some cases, the lowest c α -rmsd structure is in the largest cluster, while for other proteins, the best structure upside's secondary structure predictions for the centroid of the top cluster (c α -rmsd and secondary structure accuracy provided at top). the sequences provided by casp11 organizers can be longer than the sequences used for evaluation due to disorder (e.g., for t0769-d1, simulations are conducted on 112 residues, but only the 97 folded residues are evaluated). the rmsd values provided are based on the casp11-defined folded regions, and hence may differ slightly than those provided in fig 4. https://doi.org/10.1371/journal.pcbi.1006578.g005"
"for t0765-d1, a 76 residue α/β protein, upside's major cluster contains the native fold ( fig 5) . the performance is reflected in a low flat trace for the cluster centroid in the hubbard plot of the global distance test (gdt) versus sequence percentage. this performance is superior to all five of unres's submissions (there were no meld submissions). for t0769-d1, a 112 residue α/β protein, both upside and meld perform very well, with unres's best submission being only slightly worse. for t0771-d1 and t0803-d1, 178 and 134 residue α/β proteins, respectively, neither upside nor unres's performance is very good (no meld submissions). for t0773-d1, a 77 residue α/β protein, meld performs extremely well while one of upside structure also has the native fold. unres performance is much poorer. for t0816-d1, a 68 residue helical bundle, meld performs astonishingly well while upside's and unres's performances also are commendable. for t0855-d1, a 115 residue α/β protein, both meld and unres perform similarly and better than upside, but none succeed in finding the native fold. generally, the three approaches are capable of folding proteins of up to 94 residues, but are challenged with larger proteins."
"step 5. forces on the o, h and bead are \"pulled back\" and added to the forces on the 3 backbone atoms by reversing the placement process."
"after generation of the free and native-restrained ensembles, we change the energy parameters α i, where i is the optimizer step, in proportion to the amount that the change can differentiate the two ensembles. this procedure is a form of gradient descent to reduce the \"distance\" between the free and native-restrained ensembles,"
"to achieve this balance with a detailed interaction model, we use our recently developed, extremely rapid upside implicit solvent molecular dynamics program [cit] . each residue in upside is represented with a polypeptide backbone and a side chain interaction site or bead which can adopt up to 6 positions representing up to six different side chain χ 1 /χ 2 states. the key advance of the model is the smoothing of the energy surface by approximate analytic integration of free energies for the side chains' discrete states. when trained to predict side chain conformations from the protein data bank (pdb), the method can fold a few small proteins with moderate accuracy in a cpu core-day. the majority of speedup of the procedure is a result of a unique side chain algorithm which directly calculates the side chain probability distribution and the free energy. this free energy calculation, performed at every time step, avoids the steric rattling of the side chains which can occur in the condensed phase in all-atom simulations, and so allows the backbone to move on a smoother energy landscape."
"simmerling and coworkers folded 17 sub93 residue proteins using gpus to obtain a microsecond of simulation time per day with their pairwise generalize born (gb) model trained to reproduce poisson-boltzmann solvation along with their ff99sb force field [cit] . impressively, their replex protocol folded 16 of the 17 proteins to within 3 å c α -rmsd although the top cluster was greater than 10å for five of the six largest proteins. over-all, the performance is very similar to upside's in that 1-3 å c α -rmsd structures are achievable on most proteins but the structures are not always in the largest cluster."
"in our recently-developed upside model, only the n, c α, and c atoms for each residue undergo dynamics. this simple representation of the protein allows for molecular dynamics on a smooth landscape but also makes it challenging to include the entirety of the protein physics. to address this challenge, we build additional layers of derived coordinates during the energy computation, much like virtual sites in a traditional force field. these layers include amide hydrogens, carbonyl oxygens, hydrogen bonding and residue burial scores, and the possible locations of protein side chains. all of the derivative information required is backpropagated through these layers of representation during the computation of forces for molecular dynamics. the side chain positions are the most challenging to represent because we must solve a side chain packing problem in order to determine the distribution of side chain positions for a given backbone geometry. to pack the side chains probabilistically and obtain a side chain free energy, we use a rapid self-consistent iteration as described in our recent work [cit] (fig 1) . the major computational steps are:"
"a distinction between contrastive divergence and traditional training methods, such as zscore optimization [cit], relates to the goal and the source of the decoys. in contrastive divergence, the critical task is to produce a high population of low rmsd structures with the model. z-scoring training attempts to make the energy of the native state much lower than the average energy of an pre-constructed decoy library. this is problematic because the decoys may not have structures that exhibit the pathologies of a poorly-trained model. additionally, we believe optimization should concentrate on the lowest energies that have significant boltzmann probability, not the average energy which is dominated by highly-unlikely structures."
we judge the accuracy and equilibration from the histograms of the c α -rmsd from the native structure after discarding the initial third of the simulation as equilibration (fig 4) .
"step 4. forces on the 3 backbone atoms, as well as on the o, h and side chain beads are calculated from the derivative of the free energy."
"the majority of the proteins show a small number of well-defined basins that represent the dominant conformations with the current potential. while the simulations often produce several conformations quickly, equilibration of their populations takes longer, on the order of predicted structures and c α -rmsd distributions. after equilibration phase for the lowest temperature of replica exchange simulations (see s1 text). the simulations start from either the native (blue) or a random unfolded state (red). for the refolding simulations, the lowest c α -rmsd to native structures is provided along with the value for the centroid of largest cluster (in parentheses). rmsd calculations exclude three residues at the amino-and carboxy-termini to account for possible disorder at the ends. each replica is run for about three days with one cpu-core."
"in this paper we have proposed an anfis-based video quality prediction model that combines the three rlc loss conditions. the model is trained with a combination of parameters in the physical and application layer. the results demonstrate that it is possible to predict the video quality if the appropriate parameters are chosen. however, we conclude that as long as the ip packet error probability remains unchanged the impact of rlc losses on end-to-end video quality is negligible. we also analyzed the impact of qos parameters on end-toend quality. we found that as the spatio-temporal activity of the content increases, the video quality becomes more sensitive to the rlc losses. for example, for slower moving content bler of greater than 30% affects quality due to the error correction of the rlc am mode. however, for faster moving content bler of 20% results in a great loss of quality. future work will concentrate on extensive subjective testing to validate the proposed model. also, different packet sizes will be modeled and adaptation methods proposed."
"the grasping performance is already very good with the present approach. in fact, all of the performed grasp trials were initially successful. only in a relatively few number of cases we observed the glass slipping away from the grip during subsequent arm motion. this was due to heavy tremor-like oscillations of the robot arm arising from operating the arms beyond their torque specification limits due to the weight of the shadow hand subsystem."
"the transmission of h.264 encoded video over umts network is illustrated in fig. 6 . the original yuv sequences are encoded with the h.264/avc jm reference software with varying sbr and fr values. h.264 is chosen as it is the recommended codec to achieve suitable quality for low sender bitrates. the resulting 264 video track becomes the input of the next step, which emulates the streaming of the mp4 video over the network based on the rtp/udp/ip protocol stack. the maximum packet size is set to 1024 bytes in this case. the resulting trace file feeds the opnet simulation model. for the aims of this paper, the video application model has been modified to support the incoming trace file (st) and generate the rtp packet traces in the sender module (sd) and in the receiver module (rd). finally, the last step is in charge of analyzing the quality of the received video sequences against the original quality and the resulting psnr values are calculated with the ldecod tool included in the h.264/avc jm reference software. mos scores are calculated based on the psnr to mos conversion from evalvid [cit] ."
the accuracy of the proposed anfis-based video quality prediction model is determined by the correlation coefficient and the rmse of the validation results. the model is trained with three distinct content types from parameter both in the application and physical layers over umts networks. the model is predicted in terms of the mean opinion score (mos). the predicted vs measured mos for the proposed anfis-based prediction model is depicted in fig. 11 .
"the ukr manifold of hand postures is spanned by a two-dimensional latent space as shown in fig. 5 : the first (periodic) latent dimension encodes the temporal component of the motion and the second one the corresponding cap radius. the manifold smoothly generalizes also for cap radii not seen during training, which motivates the employment of the rather costly ukr approach to manifold learning."
"coordination at the arm level is complemented by coordination at the finger level, utilizing a kernel-based manipulation manifold extracted by a learning algorithm from human movement data captured with a data glove. our previous results on this approach have been restricted to the context of simulation settings [cit] . the present contribution is the first presentation of this method for a complex multifingered manipulation movement synthesized from human training data and performed on a bimanual arm-hand system using a pair of anthropomorphic robot hands."
"universal mobile telecommunication system (umts) is a third generation (3g), wireless cellular network based on wideband code division multiple access technology, designed for multimedia communication. umts is among the first 3g mobile systems to offer wireless wideband multimedia communications over the internet protocol [cit] . multimedia contents on the internet can be accessed by the mobile internet users at data rates between 384kbps and up to 2mbps in a wide coverage area with high user mobility."
"similar results can be obtained for other coatings and other products. examples are coating of seeds with pesticides, coating of pellets, and glazing of shrimps. figure 8 shows mnf2 (left) and mnf3 (right). mnf2 appears to contain the most prominent admixture in the powder and this component allows to assess the amount of admixture as well as the blending homogeneity. mnf3 appears to contain other contaminants without any impact from mnf1 and mnf2 due to mnf being an orthogonal transformation."
"specifically, we propose a strategy motivated by the observation of human arm movements and exploiting the rotational symmetry of a to-be-grasped object for choosing a favorable configuration of a 7-dof redundant robot arm. we demonstrate that our approach significantly enlarges the set of feasible object poses that can be handled by the system, when compared to a standard method."
the model proposed in this paper is reference-free. the correlation coefficient (r 2 ) of the model was 87.1% and the root mean squared error (rmse) was 0.2412. the training error versus the validation error is given in fig. 12 . the training error is the bottom line of fig. 11 (around 0.06) and the validation error is the top line of fig. 11 (0.2412) .
"we feel that the choice of parameters is crucial in achieving good prediction accuracy. parameters such as mbl in link layer allowed us to consider the case of less bursty or more bursty cases under different bler conditions. also, in the application level the sbr and fr are dependent on the type of the content. however, if frame rate is reduced too low e.g. 7.5fps then frame rate has a bigger impact on quality then sender bitrate for faster moving content. this could be due to the bandwidth restriction over umts network for faster moving content types. also contents with less movement require low sender bitrate compared to that of higher movement. finally, to predict video quality content type is very important."
"when the lid was unscrewed, the system executed a manually triggered, preprogrammed lifting motion to remove the lid. one could use the achieved turning angle as a simple heuristic to define the end of the unscrewing motion automatically. the tactile sensors in the finger tips of the hand are not sensitive enough to notice this event."
"compared to a previous work [cit], where the authors proposed three models for the three content types, the model performs very well in terms of the coefficient of correlation."
"to quantify the improvement in utilizing the proposed normalization heuristic of sec. iv, we conducted an experiment, systematically probing whether an object pose is feasible for grasping or not. there exist several sources of possible failure: the robot might collide with an object in the scene (particularly with the other robot arm), violate the security box limiting the workspace, or violate the joint angle limits. particularly, the restriction to the narrow security box bounded by the frame bars shown in fig. 7a complicates the task, because the robot cannot extend its elbow beyond this virtual boundary. to avoid the joint angle limits, we exploit the redundancy of the 7-dof robot arm computing a weighted jacobean pseudo inverse [cit] to reduce motion of joints near their limits."
"we showed that the combination of biologically motivated movement strategies can successfully generate fairly complex movements involving multifingered fine manipulation of objects. we focused on the coordination of redundant degrees of freedom at the arm and finger levels, utilizing two complementary strategies: a heuristic derived from the observation of human arm movements to maximize the set of object poses that can be handled by the system, and a learning approach for extracting a manifold of finger synergies from human training data to carry out the finemanipulation phase. as a benchmark task, we focused on the removal of a lid from a glass jar by a periodic unscrewing motion carried out in a bimanual setting involving two anthropomorphic shadow hands."
"a strobed led spectral imaging system is the basis of the patented technology [cit] applied here. figure 1 shows a schematic of the system that can combine multiple diffuse frontlight strobes with backlight strobes and fluorescence emission filters. the camera is a monochrome silicon sensor looking through an integrating sphere, and the sample is placed in an opening port on the opposite side of the sphere. the highly reflective and diffuse coating on the inner side of the sphere will make the light from physically separate high energy leds bounce back and forth many times before hitting the sample surface. up to 20 different wavelengths from 365 nm to 1050 nm are used and for all wavelengths the light will be highly diffuse and spatially homogenous. this means that the irradiance orientation distribution is both spectrally and spatially uniform. the different wavelengths are then sequentially strobed and these strobes are synchronized with a 6-12 mpix camera running at 15-30 frames per second. thus, a spectral cube of 20 spectral bands and 6 mpix resolution per band is acquired in less than a second. through a reflectance calibration the pixel values are transformed to reflectance values. the reflectance spectrum in every pixel can then further be extended with fluorescence bands by applying one or more bandpass or longpass filters in front of the camera. this provides further chemical information about each pixel and this information can be analyzed separately or together with the reflectance spectra in an extended spectral cube."
"the relationship between mos, fr and sbr is shown in fig. 10 . from fig. 10 we observe that at higher sbrs the video quality degrades rapidly due to the umts network congestion at downlink bandwidth. also, from fig. 10 we observe that the optimum fr is 10fps and sbr is 80kbps that gives acceptable quality."
"in the first step we compare the rayleigh quotient of the different feature sets, and selects the one with the highest rayleigh quotient. figure 2 shows an image of durum wheat (left) and common wheat (right). common wheat is often used as an adulterant in durum wheat and we want to detect this as an adulteration. we paint representative areas of durum wheat and common wheat and then we calculate the ncda score images shown in figure 3 . it shows a very good signal to noise ratio in separating durum wheat from common wheat. figure 4 (left) shows a petri dish with 100 kernels of mixed seeds. these seeds are segmented to individual kernels that are extracted and aligned, and then a mean value for the ncda score image is calculated for each kernel. finally, the kernels are sorted row-wise according to this mean. from very common wheat like kernels in the upper left corner to very durum like kernels in the lower right corner. a threshold will reveal 3 common wheat kernels in the sample. figure 4 100 seeds of a mixed sample automatically segmented and sorted according to likelihood of being durum according to the ncda. table 1 shows results for 2 pure samples and 21 blind samples provided by lab of government chemist (lgc), uk. real (manually prepared) counts were provided after the analysis (est #) was complete. the average error in the counts is less than 1 out of 100 seeds. by including shape parameters this error could most likely be reduced further. all together this has the potential to become a fast and easy way to check authenticity of durum wheat [cit] . figure 5 shows two samples of the same minitabs. the only difference between the two images is that the tabs on the left are uncoated. in standard color representation (srgb d65) there is no visible difference between the two populations and a check of proper coating does not seem feasible."
"the whole robot system is controlled by numerous distributed processes tied together using the xcf middleware toolkit [cit] which features an event-driven communication scheme. the coarse structure of the system is shown in fig. 2 . a central scene simulation component, located in the robot server process, is used for collision detection based on a prediction of future motion of all robot components. to maintain an up-to-date scene description, the hand server publishes its current hand posture. the vision module is responsible for recognizing and localizing the object. to simplify the vision task, we employ a visual marker on the object as shown in fig. 3 . finally, the time flow of actions is coordinated by a central hierarchical state machine (hsm), which monitors the current system state and chooses an appropriate action in response to an incoming system event."
"for the experiments, we used a medium size nutella r jar with a plastic lid whose surface was chosen to offer good contact friction with the finger tips. when the jar was passed to the robot, it had to grasp the rotationally symmetric lid with its right hand, employing the hand positioning strategy described in sec. iv. grasping the lid allowed the left hand to grasp the jar body, thereby freeing the right hand. this hand-over was realized at a favorable (and preprogrammed) standard position. notice, that the jar body is not symmetric but rather oval (as can be seen from fig. 3), such that the full 3d orientation of the jar has to be considered. the unscrewing operation was then initiated by the now free right hand starting from an end-effector position computed from an updated jar localization in order to correct for errors during the hand-over process."
the aim is to develop a learning model to predict video quality for all content types from both application and physical layer parameters for video streaming over umts networks as shown in fig. 1 . for the tests we selected three different video sequences representing slow moving content to fast moving content as classified in a previous work [cit] .
"we obtained suitable training data by recording sequences of hand postures (in the form of joint angle vectors) from demonstrations with a hand-mounted dataglove. due to the mismatch of the kinematic structure of the human and the robotic hand, the angular values measured by the dataglove cannot directly be applied to control the robot hand. rather, the demonstrator used the glove as an input device for a hand model executing the screwing task in simulation. employing visual feedback the human is able to learn an adequate hand posture mapping to successfully achieve the task. finally, the joint angles of the simulated hand, which incorporate contact feedback from an underlying physics-engine, can be used to control the real robot hand."
the impact of mbl on quality for all contents is given in fig. 9 . from fig. 9 we observe that the mbl similar to bler has greater impact for content types with higher s-t activity. this is because as the s-t activity increases the impact of losses is greater as a loss in i-frame has a greater impact on quality compared to slow moving content types.
"these two approaches and their combination are described in secs. iv-vi and constitute the main new contribution of this paper. the third bio-inspired motion concerns the grasping phase, where we follow a tactile-driven approach developed in previous work [cit] . to make the paper more self-contained, it is briefly sketched in sec. iii."
"the present paper focuses on the combination of such motion strategies to coordinate arm and finger motions for a major generic skill: namely, the opening of containers. in humans, this skill is a complex capability developed over years of sensory-motor learning. for a robot, it represents a major next step beyond grasping, which itself constitutes one element of container opening and for which a considerable number of quite successful approaches have already been developed and published in the literature [cit] . compared to grasping, opening of containers requires the robot to master the coordination of a larger number of subtasks, including the recognition of the object, the discrimination of its essential components, the type of required grasping and opening actions, and the acquisition, representation and generation of the manipulation movements themselves. moreover, while grasping of typical objects is usually feasible with a single hand, opening a container in most cases requires bimanual activity, with one hand fixating the object and the second performing the opening action."
"the umts network topology is modeled in opnet modeler®. the proposed network scenario is depicted in fig. 5 . it is made up of a video server, connected though an ip connection to the umts network, which serves to the mobile user. with regard to the umts configuration, the video transmission is supported over a background packet data protocol (pdp) context with a typical mobile wide area configuration. the transmission channel supports maximum bitrates of 384 kbps downlink / 64 kbps uplink over a dedicated channel (dch). the rlc layer is configured in acknowledge mode (am) and without requesting in-order delivery of service data units (sdus) to upper layers. the radio network controller (rnc) supports the concatenation of rlc sdus, and the sdu discard timer for the rlc am recovery function is set to 500ms."
"enhancing the set of feasible object poses both described grasping strategies require to specify the approach direction of the grasp given as the position and orientation t of the hand relative to the object frame. the first strategy finds a feasible end-effector pose from a thorough search within the database, while the latter strategy utilizes a fixed transformation relative to the object frame which is extracted from vision in both cases. for the bioinspired method, this determines a sole hand pose to realize the grasp, thus dramatically limiting the set of object poses which can be successfully grasped. however, many everyday objects exhibit symmetries which can be exploited in order to choose the approach direction in a more flexible manner, eventually optimizing subordinary goals, e.g. maintaining a natural pose, avoiding joint limits or obstacles."
"the unscrewing operation consisted of the repetitive cycling through the motion manifold trained from human demonstration data as described in the previous section. the robot was able to unscrew the jar in the large majority of cases (18 out of 20) . a small fraction of trials failed due to blocking of the jar, or because an accumulation of undesired finger movements in the supporting left hand brought the jar into a position in which a continuation of screwing was no longer possible."
if we train on uncoated and coated pixels and use the ncda to discriminate between these two populations then the resulting score images are shown in figure 6 (left and center). the graph in figure 6 (right) shows the histogram of the ncda in the two score images. we see that coated minitabs have a mean of +1 and uncoated minitabs have a mean of -1. the histogram is based on pixels and not individual minitabs. in case the mean was taken over each minitab then the discrimination will be even better.
"the discrimination between different types of products in a spectral cube is done by ncda which is a modified canonical discriminant analysis (cda). cda dates around 80 years back and is also known as fisher discriminant analysis. it is a linear transformation and provides an optimal foreground/background discrimination in the sense that the rayleigh quotient (1) is a generalized eigenvalue problem, and a is found to be the eigenvector corresponding to the largest eigenvalue. frequently we will only have two classes: good product and bad product, and in this two-class problem there is only one cda component since the rank of s σ is one. a score image can then be calculated by projecting the spectral image onto * a . the ncda is closely related to the cda. it is extended from cda with the following extensions:"
"multimedia applications are gaining popularity over mobile terminals and are likely to be a major application in future mobile systems and a key to their success. however, the end-to-end qos of multimedia services heavily suffers from the characteristics of the underlying wireless network. therefore, the need for a thorough understanding of the impact of radio link losses on video quality prediction is necessary. hence, this is the motivation for our study."
"the shadow dexterous hands [cit] are distinguished by their human-like design: in size, number and flexibility of joints the hands resemble their human counterpart in a unique manner. their actuation with mckibben style pneumatic muscles on the one hand dramatically eases grasping tasks due to the inherent compliance of the fingers. on the other hand, complex nonlinear effects like hysteresis, friction and compressibility of the air render the generation of smooth finger motions a difficult task."
"strobed led spectral imaging systems in combination with a good calibration procedure and machine learning represents a fast, non-destructive, and versatile solution to a broad range of analysis challenges in the agricultural and food industry. this technology has the potential to augment existing technologies and provide more valuable information related to the integrity of food and feed consumed today."
"the accompanying video [cit] demonstrates the whole manipulation action, including 6d object localization, object grasping from the human operator, hand-over between hands, the unscrewing itself, and the final lifting of the cap."
"yet, surmounting this challenge is of paramount importance if robots are to become useful in many common household settings where settings are tailored to the characteristics and capabilities of human hands. besides these application considerations, given the impressive range of human manual actions, their replication with anthropomorphic robot hands is likely to be both a strong driving force and a severe \"litmus test\" for a thorough understanding of a significant part of our sensory-motor cognition."
"spectral imaging systems are gaining importance these years due to more and more powerful technologies being developed mainly addressing applications where traditional color imaging or traditional spectroscopy do not meet requirements in terms accuracy and robustness. spectral imaging systems can perform spectroscopy on highly heterogenous samples which opens up a new space of applications. different architectures are used for these systems, and sometimes they are grouped into multispectral and hyperspectral technologies, which, however, only represents a small part of the relevant system characteristics. more important is the actual selection of wavelengths, the signal to noise ratio over the entire wavelengths range, and the robustness and calibration of the hardware system involved. only when enough attention is paid to these factors then we can hope to obtain the reproducibility of measurements and the transferability of models that is needed for robust and commercially viable high performance systems."
"even in the large majority of successful cases we observed significant variation in the finger movements due to a lack of precise reproducibility in the operation of the finger joint tendons. this led to a variable number of fingers making good contact with the lid circumference during the movement and a correspondingly varying \"gear ratio\" between the finger rotation and the lid rotation. this \"gear ratio\" can be measured by the vision system and will in future experiments constitute a good candidate for a quantitative quality measure enabling a fine-tuning of the unscrewing motion. however, even without a quantitative evaluation of this measure so far we observed that the endangerment of proper task execution by the unwanted variations in finger positions was strongly counterbalanced by the availability of a redundant set of five fingers, ensuring that in most cases there existed sufficiently many \"successful\" finger contacts to drive the lid rotation."
"in this paper, we demonstrate how a combination of three bio-inspired approaches can cover a significant portion of the involved coordination tasks, such that the flexibility of the system to cope with object configurations is increased and part of the burden of prestructuring manipulation motions is replaced by learning from human movement data. this approach follows the spirit of earlier approaches that used bio-inspired methods and learning strategies, such as imitation learning and learning from demonstration [cit], to provide the system with increased flexibility. so far, the majority of these approaches have focused on \"large scale\" movements at the body or entire arm levels. the present work extends this focus into the realm of fine manipulation skills which so far has received comparably less attention along the lines of bio-inspired algorithms."
"still, many interesting issues remain for future work. so far, we used predefined arm and hand postures. since the tactile sensors of the shadow hand have recently matured to a level where tactile-driven adaptation of the grasp posture becomes feasible, we plan to integrate tactile feedback both during grasping and during the cap turning motion. this will open additional possibilities for online-tuning of the cap motion and for the transitions between the now preprogrammed movement phases. with this added flexibility, a logical next step will be the generalization of the unscrewing motion to a larger set of lid types and to other types of container opening motions. we already have shown, that the manifold approach to learn complex motion trajectories is feasible also in other contexts [cit] ."
"as a benchmark example of realistic complexity, we focus on the case of opening a glass jar that is known to the robot. a vision system provides the necessary position information when a human passes the jar to the robot, after which the robot has to autonomously perform all the necessary actions to unscrew the lid. a dialog component [cit] gives verbal feedback when the object is not properly visible or is relocated during the grasping phase."
"an inevitable overarching concern has been the integration of all required functional modules. this poses an additional challenge in itself and is beyond the scope of the present paper. however, to give the reader a feeling of how the above movement primitives have been implemented in the context of the overall system, we briefly describe the underlying integration architecture in sec. ii (for details, see [cit] . ii. system overview the bielefeld \"curious robot\" setup [cit] combines visual attention and an intelligent interface for speech recognition and understanding with dexterous bimanual manipulation skills and proactive dialog communication in order to study multi-modal task-oriented instructions given by lay persons. for manipulation tasks, the setup employs two redundant 7-dof mitsubishi pa-10 robot arms as well as two 20-dof shadow dexterous hands, accumulating to 54 dof within the whole system. the robot arms are mounted from the ceiling providing a humanoid arm configuration. they are controlled by a robot server realizing a security concept for collision detection based on an internal scene model."
"the coordination of multifingered robot hands for accomplishing \"ordinary\" human manual actions belongs to the most demanding challenges in robotics. contributing factors include the large number of involved motor degrees of freedom, the need to cope with redundancy, variable contact geometries in the face of imprecise object geometry, uncertainty in the actuator characteristics, the lack of adequate tactile feedback, the need to integrate vision, and many more."
"the use of optimized tool poses for object pick-up, exploiting the symmetry of the object, significantly enhanced the feasible space for object grasping. additionally, this strategy avoids awkward arm configurations. extending the same method to compute a flexible hand-over pose for both robot arms, only constraining their relative pose, might eliminate the remaining hand-over problems."
"this background is the rationale for a major long-term research effort at the lab of the authors, where the exploration of bio-inspired motion strategies and their integration in order to realize a manipulation system that can cope with a sizable range of manual interaction tasks is a primary focus."
"a similar task has already been successfully demonstrated by the dlr group using the humanoid platform justin: they successfully demonstrated the opening of a screw cap bottle [cit] . during the manipulation, which is preprogrammed for their four-fingered robotic hand, the cap is perfectly grasped and unscrewed. while this is an impressive feat, we demonstrate that a similar capability can also be realized by a combination of several biologically motivated movement primitives. moreover, an essential part of the action -the fine manipulation of the jar lid -is synthesized from a generalizable learning algorithm in our approach."
"unlike cps, conventional computer systems usually do not have much interaction with the physical space. thus no clear definition of physical-state recovery has been studied for them. we identify and define physical-state recovery for cps in this subsection. we consider a linear-time invariant (lti) system given by eqn. (1)(2), as an example to present physical-state recovery. fig. 2 depicts an example that illustrates the above definition, where s0 to s3 are four internal elements of the system state. fig. 2(a) presents an inconsistent physical-state (marked by the black line), because the value of element s1 corresponds to a different time point from that of other three elements. by contrast, fig. 2(b) shows a global consistent physical-state, because the values of all four elements are of the same time t k−n ."
"the uav used in our study is a smart plane called smartone-c [cit] . it has 1.2 m wing span, high strength skin and equipped with 16.1 mp calibrated ricoh gr camera. the smart plane can fly up to 200 m and the gsd ranges between 2.6-5.2 cm/pixel depending on the flying height. in our case, we set up the flying height around 140 m which corresponds to a gsd of approximately 3.5 cm. the forward and side overlap is set as 80% and 60%, respectively, to achieve a sufficient overlap between images to facilitate image matching."
"rationale of roll-forward recovery. there can be two different ways to perform physical-state recovery: one is rolling the system forward from the consistent global state while the other is rolling the plant back to that state. to realize the latter way, it needs to make the plant's state match the values of the consistent state, which requires physically rolls the plant back. this operation not only comes with high overhead but also sometimes is infeasible, e.g., for irreversible processes. by contrast, the roll-forward recovery is carried out from the other direction, i.e., matching the values to the plant's state, which is thus always feasible and with lower overhead. therefore, we choose to use the rollforward operation for the physical-state recovery. table i presents a comparison about key features of cyberstate recovery and physical-state recovery."
"in the event of a failure, data safety is primary while progress towards successful completion of migration is secondary. our failure model assumes reliable communication channels, node failures, and network partitions, but no malicious node behavior. node failures do not lead to complete loss of data: either the node recovers or the data is recovered from the nas where data persists beyond dbms node failures. if either nsrc or n dst fails prior to phase 3, migration of cmigr is aborted. progress made in migration is not logged until phase 3. if nsrc fails during phases 1 or 2, its state is recovered, but since there is no persistent information of migration in the commit log of nsrc, the progress made in cmigr's migration is lost during this recovery. n dst eventually detects this failure and in turn aborts this migration. if n dst fails, migration is again aborted since n dst does not have any log entries for a migration in progress. thus, in case of failure of either node, migration is aborted and the recovery of a node does not require coordination with any other node in the system."
"(iv) deleted states (states from t 0 to t k−n −1 ). when the most recent correct checkpoint is enough for recovery, the previous stored states or checkpoints are no longer needed and thus can be discarded."
"in this work, we have studied the problem of enhancing attack-resilience for a cps system under attacks on the environment of the controller such as attacks on sensors. we have developed a procedure of cps checkpointing and recovery. firstly, we propose a new concept of physicalstate recovery, which is defined as rolling the system forward to match values of internal elements with the states of the plant. secondly, we design a slide window based checkpointing protocol that defines how to record system states for the recovery. the proposed procedure possesses several advantages including low overhead to the system, no modification to the existing controller, and no limitations about sensor redundancy. thirdly, we present a use case of the procedure on handling sensor attacks. finally, we validate the feasibility of our design using a dc motor simulator and an unmanned vehicle case study."
"in eq. 2, denotes the aggregated orientation image for a radius r depending on the number of strictness parameters chosen (i.e., m). the denominator part in eq. 2 ensures that the computed values for each contributes equally to . note that applying the gaussian kernel not only reduces the level of noise in image space after the accumulation but also spreads the influence of highly accumulated pixels to neighbouring pixels to achieve a smooth aggregated orientation image."
"we now evaluate our prototype implementation of albatross using a variety of workloads. we measure migration cost using four cost measures: tenant unavailability window, number of failed requests (aborted transactions or failed operations), impact on transaction latency (or response time), and additional data transfer during migration. we compare performance with the stop and mi-grate (s&m), a representative off-the-shelf technique, implemented in elastras. in stop and migrate, a long unavailability window results from flushing cached updates from committed transactions. an optimization, called flush and migrate (f&m), performs a flush while continuing to serve transactions, followed by the final stop and migrate step."
the rest of the paper is organized as follows. section ii defines cps recovery. section iii presents the checkpointing protocol. section iv uses cps recovery and checkpointing to address sensor attacks/faults. section v validates our design. section vii concludes the paper.
"setup and implementation. we have considered the speed control in the previous subsection. in this subsection, we consider a different scenario to further demonstrate the capability of our approach, where the controller needs to ensure the vehicle to travel in a straight line."
"to address this case, we propose to leverage checkpointing and recovery, i.e., using historical data to recover system states. however, to design such a procedure well applicable to cps is a non-trivial task because of the following two major challenges. firstly, the conventional roll-back operation is unsuitable for recovering states of the plant. physically rolling back the plant usually incurs considerable overhead, and further it is even infeasible to roll back some irreversible processes. secondly, detection mechanisms, such as datadriven methods for sensor attacks [cit], usually have substantial detection delay, i.e., the time interval between the occurrence of an attack and the detection of it. states stored during the detection interval may be incorrect and thus using these states can result in unsuccessful recoveries."
"further, different kinds of sensor attacks or faults can affect the system with different time durations. that is, the attacked sensors may become trustworthy again multiple sampling periods later, or after even longer time, or never [cit] . our method is generally applicable to all of these cases, and its advantage is even more noticeable for short-term attacks. that is, for sensor attacks causing transient failures to internal elements or transient attacks, there is even no need to reset them if they can become trustworthy before the condition e + k+m e occurs. one possible situation for short-term attacks is that attackers just have intermittent physical access to attackees, e.g., an attacker uses some device to attack camera or lidar sensors of a passing-by autonomous car [cit] . from this perspective, our method is also a more lightweight way to deal with transient attacks. 1"
"this follows from the invariant that in the steady state, the combination of the database cache and the persistent disk image does not have changes from aborted transactions. in occ, changes from uncommitted transactions are never publicly visible. in locking based schedulers, the cache or the persistent database image might have changes from uncommitted transactions. such changes are undone if a transaction aborts. any such changes copied over during the iterative phases are guaranteed to be undone during the first round of the atomic handover phase."
"we now articulate two important properties of albatross that allow the system to gracefully tolerate failures and characterize its behavior in the presence of failures during migration. property 1. independent recovery. except during the execution of the atomic handover protocol, recovery from a failure of nsrc or n dst can be performed independently."
"in a multitenant system, due to aggressive consolidation and resource sharing between tenants, a low cost migration technique is important. it enables the system to guarantee that if the need arises, tenants can be migrated to improve performance while ensuring that their slas are met. for instance, as is evident from the experiment reported in figure 12, even though the load on every tenant at nsrc is only 50 tps, as the number of tenants at nsrc increases, it causes an overload. a low cost migration technique can help alleviate such scenarios commonly encountered in multitenant systems."
"in this paper, we present an approach to automatically detect citrus trees. the approach considers the symmetric nature of the citrus trees and detects them from a single source, dsm. in a very recent work, we presented a new form of orientation-based radial symmetry transform [cit] ) that can be directly applied to an input like dsm to detect and label the citrus trees. we expand that approach such that false positive detections by the approach are mitigated using additional local maxima (lm) information. in this study, we guarantee that the output of the orientation-based radial symmetry transform also coincides with an lm; thus, reducing the number of false positives and increasing the correctness rates. the feasibility of our approach is presented on five test patches having different contexts of orchards (first patch is shown in fig. 1 ). we also show the results of four state-of-the-art approaches on our test patches, and prove the superiority of the method presented. our contributions in this study are twofold: (i) such an integrated approach (symmetry + lm) has not been tested to detect (citrus) trees (in orchards), and (ii) the validity of such an integrated approach has not been experienced for an input, e.g. a single dsm."
"there is no roll-forward propagation phenomenon for the defined physical-state recovery, which means that failed elements will not cause unfailed elements to roll forward. the reason is two-fold. failed elements depend on which sensors are compromised, instead of depending on other elements. further, the internal elements of a system state change separately, and the failed and unfailed elements are also treated separately by the defined physical-state recovery."
"step (i): use the control inputs between t k−n and t k to make a prediction about the current system state (of all elements), that is,"
"from the above equation, we can see that the bound e + k, (1,q) increases as the time interval (t k−n, t k ) or the number n grows; while the bound e + k,(q+1,n) only depends on the information at current time t k . later on, we will discuss more about the state prediction error in the evaluation section. note that in the above, we just show an example of how the physical-state recovery operates, where the system model is used to carry out state prediction. the prediction can be also preformed by other methods, such as kalman filter or machine learning. comparison between these methods is one future work."
"t . for example, as shown in fig. 2(b), elements s0 and s1 fail at current time t k, i.e., x k, (1, 2)"
"in this section, we present a use case of the cps checkpointing and recovery, which is the capability of addressing sensor attacks/faults. we assume the existence of some mechanisms that can detect sensor attacks/faults. our method works with those mechanisms, that is, after they discover some attacks/faults, the physical-state recovery will be triggered to execute."
"during the assessment of the accuracies of the detection step, we consider a reference citrus tree object as detected if any part of the reference citrus tree is hit by the approaches. because a single tree is described with a single pixel (i.e. the location of the stem) found in the detection step, we label a reference tree as detected (i.e. tp) if any pixel within an individual tree canopy is labelled as a stem. we label a reference citrus tree as fn if no stem is available for that tree, and we mark a detected stem as fp if it does not correspond to any part of the citrus tree canopies in the reference data. using these measures, three well-known metrics, precision, recall, and f 1 -score, are calculated to evaluate the results:"
"step (ii): for the q failed elements, use the value vector extracted fromx k (given by eqn. (3)), that is,x k, (1,q) . this means that this part of state prediction is based on the historical information."
"to validate the cps checkpointing and recovery and highlight its benefits on addressing sensor attacks, we conduct two experiments: one is simulator based and the other uses an unmanned vehicle test bed. both experiments have the same control system diagram as illustrated by fig. 4 . the controller consists of our approach and a pid controller."
"we now present results from experiments varying other parameters of ycsb. figure 6 plots the migration cost measures as a function of the percentage read operations in a transaction; we vary the read percentage from 50 to 90. for an update heavy workload, the write cache has a large amount of un-flushed updates that must be flushed during migration. as a result, s&m incurs a long unavailability window of about 2 − 4 seconds, the length of which decreases with a decrease in the percentage of writes (see figure 6(a) ). on the other hand, both f&m and albatross flush the majority of updates before the final stop phase. therefore, their unavailability window is unaffected by the distribution of reads and writes. however, since both s&m and f&m do not migrate transaction state, all transactions active at the start of migration are aborted, resulting in a large number of failed requests (see figure 6(b) ). albatross, on the other hand, does not have any failed requests. as can be seen in figure 6 (c), albatross results in only 5 − 15% transaction latency increase, while both f&m and s&m incur a 300 − 400% increase in transaction latency due to the cost of warming up the cache at the destination. since albatross warms up the cache at the destination during the iterative phase, the total time taken by albatross from the start to finish is much longer compared to that of f&m and s&m; s&m is the fastest followed by f&m (see figure 6 (d)). however, since cmigr is still active and serving requests with no impact on transaction latency, this background loading process does not contribute to migration cost from the tenant's perspective. the iterative copying phase transfers about 340 mb data between nsrc and n dst, which is about 35% greater that the cache size (250 mb). f&m and s&m will also incur network overhead of 250 mb resulting from cache misses at n dst and a fetch from nas. figure 7 shows the effect of transaction size on migration cost; we vary the number of operations in a transaction from 8 to 24. as the transaction size increases, so does the number of updates, and hence the amount of un-flushed data in the write cache. therefore, the unavailability window for s&m increases with increased transaction size (see figure 7(a) ). in this experiment, f&m has a smaller unavailability window compared to albatross. this is because albatross must copy the transaction state in the final handover phase, whose size increases with increased transaction size. f&m, on the other hand, aborts all active transactions and hence does not incur that cost. the number of failed requests is also higher for f&m and s&m, since an aborted transaction with more operations result in more work wasted (see figure 7(b) ). the impact on transaction latency also increases with size since larger transactions have more reads (see figure 7 (c)). figure 7 (d) shows a profile of the total migration time. as expected, the majority of the time is spent in the first sync or flush, since it results in the greatest amount of data being transferred or flushed. as the number of operations in a transaction increases, the amount of state copied in the later iterations of albatross also increases. therefore, the percentage of time spent on the first iteration of albatross decreases. on the other hand, since the amount of data to be flushed in f&m increases with transaction size, the time taken for the first flush increases. figure 8 plots the migration cost as a function of the distributions that determine the data items accessed by a transaction; we experimented with uniform, zipfian, and four different variants of the hotspot distribution where we vary the size of the hot set and the number of operations accessing the hot set. since the cache size is set to 25% of the database size, uniform distribution incurs a high percentage of cache misses. as a result, during the iterative copy phase, the database cache changes a lot because of a lot of blocks being evicted and loaded. as a result, every iteration results in a significant amount of data being transferred. since albatross checks the size of data transferred in each iteration, this value converges quickly; in this experiment, albatross converged after 3 iteration. however, the final handover phase has to synchronize a significant amount of data, resulting in a longer unavailability window. therefore, a high percentage of cache misses results in a longer unavailability window for albatross. f&m and s&m are, however, not affected since these techniques do not copy the database cache. this effect disappears for skewed workload where as expected, albatross and f&m have similar unavailability window and s&m has a comparatively longer unavailability window. albatross does not result in any failed requests, while the number of failed requests in f&m and s&m is not heavily affected by the distribution (see figure 8(b) ). the uniform distribution results in a higher number of cache misses even at nsrc which offsets the impact of cache misses at n dst . therefore, the percentage increase in transaction latency for s&m and f&m is lower for the uniform distribution when compared to other access patterns (see figure 8(c) ). irrespective of the access distribution, albatross has little impact on latency. figure 8 (d) plots the amount of data synchronized by albatross. following directly from our discussion above, a uniform distribution results in a larger amount of data being synchronized when compared to other distributions. it is however interesting to note the impact of the different hotspot distributions on the data synchronized. for h1 and h3, the size of the hot set is set to 10% of the database, while for h2 and h4, the size of the hot set is set to 20%. since in h1 and h3, a fraction of the cold set is stored in the cache, this state changes more frequently compared to h2 and h4 where the cache is dominated by the hot set and hence its state does not change frequently. as a result, h1 and h3 result in a larger amount of data synchronized. for the zipfian distribution, the percentage of data items accessed frequently is even smaller than that in the experiments with 10% hot set, which also explains the higher data synchronization overhead. figure 9 plots migration cost as a function of the cache size while keeping the database size fixed; the cache size is varied from 100mb to 500mb and the database size is 1gb. since albatross copies the database cache during migration, a smaller database cache implies lesser data to synchronize. when the cache size is set to 100mb, the unavailability window of albatross is greater than that of f&m and s&m (see figure 9(a) ). this behavior is caused by the fact that at 100mb, the cache does not entirely accommodate the hot set of the workload (which is set to 20% of the data items or 200 mb), thus resulting in a high percentage of cache misses. this impact of a high percentage of cache misses on migration cost is similar to that observed for the uniform distribution. however, since the iterations converge quickly, the amount of data synchronized is similar to that observed in other experiments. for cache sizes of 200mb or larger, the hot set fits into the cache, and hence expected behavior is observed. even though albatross has a longer unavailability window for a 100mb cache, the number of failed operations and the impact on transaction latency continues to be low. for f&m and s&m, the impact on transaction latency is lower for the 100 mb cache because a large fraction of operations incurred a cache missed even at nsrc which somewhat offsets the cost due to cache missed at n dst (see figure 9 (b)). number of failed operations and data synchronized show expected behavior. figure 10 plots the impact of migration on latency as time progresses. in this experiment we consider a scenario where the working set of the database does not fit in the cache. the cache size is set to 100 mb when using a hotspot distribution where the hot set is 20% of the database. this experiment confirms the observation that when the working set does not fit in the cache, even though albatross results in a longer unavailability window, there is minimal impact on transaction latency. figure 11 plots the migration cost as a function of the database size. since the persistent image of the database is not migrated, the actual size of the database does not have a big impact on migration cost. we therefore vary the cache size along with the database size such that the cache is set to 25% of the database size. since the cache is large enough to accommodate the hot set (we use the default hotspot distribution with the hot set as 20% of the database), the migration cost will be lower for a smaller database (with a smaller cache); the cost increases with an increase in the database size (see figure 11(a) ). similarly, as the size of the database cache increases, the amount of state synchronized and the time taken for the synchronization also increases (see figure 11(b) )."
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. articles from this volume were invited to present their results at the 37th international conference on very large data bases, august 29th -september 3rd 2011, seattle, washington. ten consolidated to nodes (or servers) with capacity less than the combined peak resource requirements of the tenants co-located at a node. such consolidation relies on the fact that peak resource usage is not frequent and peaks for different tenants are often temporally separated. however, to deal with unpredicted load patterns, migrating tenant databases is critical to ensure that the tenants' service level agreements (sla) are met. when the resource requirements of a tenant changes, migration allows allocating more resources to the heavily loaded tenant while isolating other tenants from being impacted by the sudden increase in load of a co-located tenant."
"we vary different ycsb parameters to cover a wide spectrum of workloads. these parameters include the percentage of read operations in a transaction (default is 80%), number of operations in a transaction (default is 10), size of the tenant database (default is 1 gb), load on the tenant (default is 50 transactions per second (tps)), cache size (default is 250 mb), and the distribution from which the keys accessed are selected (default is a hotspot distribution 3 ). for zipfian distribution, the co-efficient is set to 1.0. in an experiment, we vary one of these parameters while using the default 3 in a hotspot distribution, x% operations access y% data itemsdefault is 80% operations accessing 20% data items. values for the rest of the parameters. in every experiment, we execute about 12, 000 transactions (about 240 seconds at 50 tps) to warm up the cache, after which migration is initiated. clients continue to issue transactions while migration is in progress. we only report the latency for committed transactions; latency of aborted transactions is ignored."
"the ability to safely abort migration at an incomplete state and the single owner philosophy allow independent recovery of the failed node's state even after a failure during migration . this is crucial for effective use of migration for elasticity without unnecessarily making tenants unavailable when a node fails. furthermore, one of the implications of property 2 is that in spite of using a 2pc protocol, the handover phase does not block any system resources as a result of a failure, limiting the impact of failure to only the cells being served by the failed node. this is contrary to the case where a coordinator failure in 2pc causes other transactions conflicting with blocked transactions to also block."
"in this paper, we utilize digitals surface models (dsms) extracted from a uav platform for detecting citrus trees in mersin province of turkey, a country ranking 9th in the world citrus fruit production with about 4 million tons in more than 125 thousand hectares [cit] . therefore, extracting reliable information (location and number) of the citrus trees is an important task. to our knowledge, such information related to the citrus trees is not entirely known considering the regional and country levels for most of the developing countries. therefore, a precise detection step from remotely sensed data is a requirement. for sure, intensive manual processing (either on site and/or from images) is an alternative strategy to collect the required information of citrus trees. although quite reliable results are achieved from manual investigation, such an operation requires qualified labour force, and therefore, expensive and time-consuming. for that reason, utilization of automatic methods is essential, especially for the detection step."
"then the estimated statex k is regarded acceptable and thus can be used for control. note that till now, the system only carries out the physical-state recovery, which thus only derivesx k . no new control inputs for actuation have been generated yet."
"elastras uses a data storage format, called sstable, designed specifically for append-only storage [cit] . conceptually, an sstable is an immutable structure which stores the rows of a table in sorted order of their keys. internally, an sstable is a collection of blocks with an index to map blocks to key ranges. this sstable index is used to read directly from the block containing the requested row and obviates an unnecessary scan through the entire sstable. an otm caches the contents of the sstables. due to the append only nature of storage, updates are maintained as a separate main memory buffer which is periodically flushed to the dfs as new sstables; a flush of the update buffers is asynchronous and does not block new updates. the read-cache caches blocks from sstables as they are accessed by the transactions; a least recently used policy is used for evicting blocks to accommodate new blocks. creating a database snapshot. in the first step of migration, nsrc creates a snapshot of the tenant's database. albatross does not require a transactionally consistent snapshot of the database cache. nsrc's database snapshot is a list of identifiers for the immutable sstable blocks that are cached. this list of block ids is obtained by scanning the read cache using a read lock. it is passed to n dst which then reads the blocks directly from the dfs and populates its cache. this results in minimal work at nsrc and delegates all the work of warming up the cache to n dst . the logic for this approach is that during migration, n dst is expected to have less load compared to nsrc. therefore, transactions at nsrc observe minimal impact during snapshot creation and copying. after n dst has loaded all the blocks into its cache, it notifies nsrc of the amount of data transferred (∆0); both nodes now enter the next iteration. no transaction state is copied in this phase. iterative copying phase. in every iteration, changes made to the read cache at nsrc are copied to n dst . after a first snapshot is created, the data manager of cmigr at nsrc tracks changes to the read cache (both inserts and evictions) and incrementally maintains the list ids for the blocks that were evicted from or loaded to the read cache since the previous iteration which is then copied to n dst in subsequent iterations. again, only the block ids are passed; n dst populates its cache using the ids and notifies nsrc the amount of data transferred (∆i). this iterative phase continues until the amount of data transferred in successive iterations is approximately the same, i.e. ∆i ≈ ∆i−1. the logic behind this termination condition is that when ∆i ≈ ∆i−1, irrespective of the magnitude of ∆i, little gain is expected from subsequent iterations. ∆i is small for most cases, except when the working set of the database does not fit into the cache when the cache changes frequently. a maximum bound on the number of iterations ensures termination when ∆i fluctuates between iterations. the write cache is periodically flushed during the iterative copying phase when its size exceeds a specified threshold. a write cache flush creates a new block whose identifier is passed to n dst which loads the new block into its read cache. after the handover, n dst starts serving cmigr with an empty write cache, but the combination of the read and write cache contains the same state of data as in nsrc. copying the transaction state. elastras uses occ [cit] for concurrency control. in occ, the transaction state consists of the read and write sets of the active transactions and a subset of committed transactions needed to validate new transactions; the read/write sets of active transactions and committed transactions are maintained in separate main-memory structures. two counters are used to assign transaction numbers and commit sequence numbers. in albatross, the transaction state is copied only in the final handover phase. writes of an active transaction are stored with the transaction's state and are copied to n dst during handover, along with the counters maintained by the transaction manager. state of a subset of committed transactions (ones that committed after any one of the current set of active transactions started) are copied to n dst to validate the active transactions at n dst . the small size of transaction states allows efficient serialization. after handover, n dst has the exact same transaction state of cmigr as nsrc, thus allowing it to continue executing the transactions that were active at the start of the handover phase. handover phase. the handover phase flushes changes from committed transactions. after the transaction state and the final changes to the read cache have been copied, the atomic handover protocol makes n dst the unique owner of cmigr and updates the mapping in the metadata used by the query router. the query router (elastras clients) caches the metadata. after handover, nsrc rejects any request to cmigr which invalidates the system metadata cached at the clients; the clients subsequently read the updated metadata. the metadata tables in elastras are served by one of the live otms. the otm serving the metadata tables participates in the transfer transaction of the atomic handover phase. the tm master can be a participant of the transfer transaction so that it is aware of the outcome of migration; however, it is not needed for correctness. in our implementation, the tm master is notified by nsrc after handover completes. clients that have open connections with cmigr at nsrc are notified directly about the new address of n dst . this prevents an additional network round-trip to read the updated metadata mappings. for a transaction accessing cmigr during the atomic handover phase, elastras client library transparently retries the operation; once the handover completes, this retried operation is routed to n dst . since an otm's commit log is stored in the dfs, it is not migrated. cmigr's transaction log at nsrc is garbage collected once the transactions active at the start of the handover phase have completed at n dst, though the entries for transactions that committed at nsrc can be purged after handover completes."
"at any point of time before atomic handover, nsrc is the owner of cmigr. if nsrc fails, it recovers without interacting with n dst and continues to be the owner of cmigr. similarly, if n dst fail, it recovers its state. unless the handover phase was initiated (phase 3 of the protocol), n dst has no log record about the migration in progress, so it \"forgets\" the migration and continues normal operation. similarly, once handover has been successfully completed, n dst becomes the new owner of cmigr. a failure of nsrc at this instant can be recovered independently as nsrc does not need to recover state of cmigr. similarly, a failure of n dst requires recovery of only its state; n dst can independently recover state of cmigr since it had successfully acquired the ownership of cmigr. from an external observer's perspective, nsrc is the owner of cmigr until the atomic handover phase (phase 3) has successfully completed. any failure of n dst before phase 3 does not affect the availability of cmigr. a failure of nsrc during this phase makes nsrc unavailable, which is equivalent to a failure of nsrc under normal operation where cmigr would also become unavailable. similarly, after migration is complete, n dst becomes the owner of cmigr. any failure of nsrc does not affect cmigr, and a failure of n dst which makes cmigr unavailable is equivalent to the failure of n dst during normal operation. the only complexity arises in the case of a failure in phase 3 when a coordinated recovery is needed. if nsrc fails before successful completion of phase 3, even if nsrc had locally relinquished ownership of cmigr, if the transfer transaction did not complete, n dst cannot start serving cmigr in which case it becomes unavailable. this is similar to the blocking behavior in 2pc [cit] . however, since the handover transaction did not complete, from an observer's perspective, nsrc was still the owner of cmigr, and hence this unavailability is equivalent to the failure of nsrc during normal operation. thus, it is evident, single site failures during migration does not impact availability of cmigr."
"in eq. 3, n is the geodesic dilation size and is computed by iterating geodesic dilations for n times to reach stability, and (1) ( ) denotes the elementary geodesic dilation of grayscale image j given i:"
"conceptually, cps recovery can be seen as to restore the controllability and functionality of the system. we study cps recovery through dividing it into two different operations: recovery of cyber-states and recovery of physicalstates. cyber-states are defined as computing information of a controller, such as values of data variables. physical-states are defined as physical information of a plant, such as the velocity of a motor."
"the threat model is as follows. first, we assume that attackers are able to compromise sensor information, e.g., modifying it or preventing the controller from receiving it, but they cannot compromise actuators. second, the historical information, including physical-states and control inputs, is securely stored and cannot be compromised by attackers."
"as is evident from figure 3 (a), when migrating a tenant using albatross, the transaction latencies are almost similar to that in the experiment without migration. a cluster of data points with latency about 1000 − 1500 ms correspond to transactions that were active for failed requests (4(b) ), the wider bars represent aborted transactions and narrower bars represent failed operations."
"attacks on sensors make them unreliable and thus the estimated plant's states based on their measurements also become untrustworthy. suppose that at current time t k, some sensor attacks are detected, which cause values of q internal elements compromised, i.e., there are q failed internal elements. the following two sequential steps are applied to handle the attacks."
"cloud platforms hosting hundreds of thousands of applications pose novel challenges for the database management systems (db-ms) serving these platforms. a large fraction of these applications (called tenants) are characterized by small data footprints with unpredictable and erratic load patterns [cit] . multitenancy allows effective resource sharing amongst these tenants, thus minimizing operating cost. for a database built on a pay-per-use cloud infrastructure, elastic scaling and load balancing-i.e. scaling up and down the size of a live system based on the load-are critical to ensure good performance while minimizing operating cost."
"another key concept is roll-back propagation, which is a phenomenon that upon a failure of one or more tasks, their dependencies may force some of the tasks that do not fail to roll back. under some situations, rollback propagation may extend back to the initial state, losing all the work executed before the failure."
"experimental results. fig. 8 presents the results of a deployment during experiments carried out on a rough carpet floor. the voltage value is bounded, and initially we set the applied voltage to both motors as 50% of the bound. the sensor attack starts at 2 sec and modifies each measurement of the left speed sensor to be a constant value of 2π rad. the sampling period is set to be 0.1 sec. fig. 8(a)8(b) plots the result about the system state δ ω and motors' velocity for one time of experiment without the protection of our approach. after the attack starts at 2 sec, the measurement of the left motor fed into the controller is compromised to be a rather large value (i.e., 2π rad). with this compromised value, the applied voltage to the left motor needs to decrease in order to achieve the reference state, i.e., the velocity of both motors is the same. however, this makes the real velocity of the left motor decline to zero after several periods, shown in fig. 8(b) . thus, the realistic situation is that the velocity difference between the two motors is large, shown in fig. 8(a) . the real system state is far from the reference value, and the vehicle keeps making turns instead of travelling in a straight line. fig. 8 (c)8(d) demonstrates the result for one time of experiment with the protection of our approach. the window size n is set as 10, which means that the physical-state recovery occurs at 3 sec as to the setting here. the recoverybased resilient control also starts at this time point. between 2 sec and 3 sec, the system is under attack and the physicalstate recovery has not started yet. thus, during this time interval, the velocity of the left motor declines to zero and the system state is far from the reference state. after 3 sec, the physical-state recovery and resilient control are brought on-line to start control the vehicle using the predicted state. at first, the system state is very close to the reference state, shown in fig. 8(c), that is, the velocity of the two motors is nearly the same, shown in fig. 8(d) . this indicates that the prediction by the physical-state recovery is rather accurate. as time goes on, under the resilient control, the system state slowly drifts away, and the velocity difference grows little by little. the drift here is much less than that of fig. 8(a), and grows not so fast."
"most traditional dbmss do not support live migration since enterprise infrastructures were statically provisioned for peak expected load, and elasticity was not considered a first class feature in database systems. in such a scenario, a cell is migrated by stopping the cell at the source dbms node, aborting all active transactions, flushing all the changes to the nas, and restarting it at the destination. this approach, however, results in high migration cost: the tenant becomes unavailable during migration and all transactions active at the start of migration must be aborted. furthermore, the entire database cache is lost when the cell is restarted at the destination dbms node, thereby incurring a high post migration overhead for warming up the database cache. this approach, therefore, has a high impact on the tenant's sla, thus preventing it from being effectively used for elastic scaling."
"our reference system model (see figure 1 ) uses the shared process multitenancy model where a cell is entirely contained in a single database process which co-locates multiple cells. application clients connect through a query router which abstracts physical database connections as logical connections between a tenant and its cell. even though figure 1 depicts the query router as a single logical unit, a deployment will have a distributed query router to scale to a large number of connections. the mapping of a cell to its server is stored as system metadata which is cached by the router."
"albatross leverages the semantics of database systems to migrate the database cache and the state of transactions active during migration. in shared storage architectures, the persistent data of a tenant's database is stored in the nas and therefore does not need migration. migrating the database cache allows the tenant to start \"warm\" at the destination, thus minimizing the impact on transac-tion latency. to minimize the unavailability window, this copying of the state is performed iteratively while the source continues to serve transactions on the tenant database being migrated. copying the state of transactions active during migration allows them to resume execution at the destination. albatross, therefore, results in negligible impact from the tenants' perspective, thus allowing the system to effectively use migration while guaranteeing that (i) the tenants' slas are not violated, (ii) transaction execution is serializable, and (iii) migration is safe in spite of failures. moreover, in albatross, the destination node performs most of the work of copying the state, thus effectively relieving load on the overloaded source node."
"in this study, manually delineated tree crowns are used as reference data and such data may also include subjective errors. besides, the highest peak location of a tree might not correctly represent the stem location; therefore, a detailed field work must be performed to collect the correct stem locations to perform a reliable comparison in that respect. we also plan to develop a method that mutually optimizes lm information and the orientation-based radial symmetry transform to further improve the results. moreover, the delineation of the detected trees is another essential task to be performed in a different future study. note also that radial symmetry transform can also be directly applied to the true-orthoimages generated for the test sites. therefore, it might also be interesting to compare the detection results of the image-based and dsm-based radial symmetry transforms. besides, a method that efficiently computes the radial symmetry from the combination of the two datasets may further contribute the detection results."
"experiments were performed on a six node cluster, each with 4 gb memory, a quad core processor, and a 200 gb disk. the distributed fault-tolerant storage and the otms are co-located in the cluster of five worker nodes. the tm master (controller) and the clients generating the workloads were executed on a separate node. each otm was serving 10 tenants on average. when an operation fails due to tenant unavailability (due to migration or otherwise), the elastras client library transparently retries these operations until the tenant becomes available again and completes the request. we set the maximum number of iterations in albatross to 10; albatross converged within 3 − 7 iterations in our experiments."
"in essence, the cps checkpointing and recovery is a general method that handles failed estimated states. such states can be created by different kinds of possibilities and favorably, our method is not confined to a certain kind. to be specific, our method is applicable to deal with compromised sensor measurements caused by attacks and faults. in this sense, attack and fault are treated interchangeably in this paper. we then present a use case of the procedure on addressing sensor attacks/faults, e.g., attackers modifying sensor information or preventing the controller from receiving it. finally, we evaluate our design through conducting simulation-based experiments as well as illustrate the use of our design on an unmanned vehicle case study."
"(ii) buffered states (states from t k−n +1 to t k ). the states within the detection window are first buffered, i.e., pending to be stored, because the detection has not yet given results for them."
"the uav images are processed with pix4d software (pix4d, 2016) . during the bundle adjustment of uav images, a total of 52 ground control points (gcps) were collected with an sl500 rtk gnss receiver. among the available gcps, 9 of them were reserved as independent control points (icps) and the root mean square (rms) values for the 9 icps after the bundle-block adjustment were computed to be 6 cm (≈ 1.5 pixels) and 11 cm (≈ 3 pixels) at most in horizontal and vertical directions, respectively. the powerful multi-image sgm approach (hirschmüller, 2008) available in pix4d was used to create point clouds from the overlapping uav images. note that any part of the region is covered by at least five images to minimize the matching errors and noise, thus increasing the point cloud accuracy. this point cloud was then used to generate the dense dsms (≈ 3.5 cm) and true-orthoimages."
"a cluster of dbms nodes serves the cells; each node has its own local transaction manager (tm) and data manager (dm). a tm consists of a concurrency control component for transaction execution and a recovery component to deal with failures. a cell is served by a single dbms node, called its owner. the size of a cell is therefore limited by the capacity of a single dbms node. this unique ownership allows transactions to execute efficiently without distributed synchronization amongst multiple dbms nodes."
"the processing times required by the proposed approach are provided in table 3 . the implementation and processing was performed in matlab. all experiments were performed on a notebook computer with a quad core intel i7 cpu @ 2.40ghz and 16 gb ram. our approach is quite feasible to run by parallel processing; therefore, we benefit from the builtin parallel processing (with four cores) available in matlab to speed up the processing. according the computational times computed, it is possible to detect citrus trees from images with sizes approximately 6 mp in thirteen seconds with the approach presented."
"planted trees could be detected. bearing in mind the problem occurring for the young trees, the results of other test patches prove that proposed approach is generic for different planting forms and has the ability to detect citrus trees in dense patterns. for example, only 74 out of 1395 citrus trees are missed in a difficult case in patch #5; as a result, the proposed approach reaches a recall performance of nearly 95%."
"for a 2pl scheduler, the two phase locking rule ensures serializability of a locking based scheduler. the final handover phase copies the state of the lock table such that active transactions have the locks that were granted to them at nsrc when they resume execution at n dst . therefore, a transaction continues to acquire locks using the two phase rule at n dst, thus ensuring serializability."
"the unmanned vehicle test bed, as shown in fig. 7, is used for the illustration. the test bed assembles two boards: raspberry pi and brickpi. raspberry pi runs linux os and interfaces with motors and sensors via drivers implemented on brickpi. the test bed uses the motors and sensors available in lego mindstorms [cit] . each front wheel is driven by a motor and each motor has a built-in speed sensor. the test bed also assembles other two sensors, i.e., ir sensor and color sensor, which can be used to detect and trace objects. however, to illustrate the use of our design, we only use the two speed sensors in this experiment. we implement our design in c language."
"the remainder of this paper is organized as follows. the previous studies are summarized in section 2. the details of the proposed approach are presented in section 3. our test dataset, evaluation strategy, and parameters of the approach are given in section 4. the results are reported and discussed in section 5. the concluding remarks and future directions are provided in section 6."
"by contrast, fig. 6(b) shows the result with the protection of our approach. between time 3 and time 3.2, our approach has not been applied yet and thus the actual velocity decreases to 0.5 because of the attack. note that 0.2 is the time length of the sliding window as to the setting here. at time 3.2, the system first carries out the physical-state recovery to make a prediction on the system state (i.e., velocity). comparing the red and blue curves, we can see that the prediction of this time point is rather accurate with little error. then, the system performs the recovery-based resilient control based on the prediction. between time 3.2 and time 6, the controller depends on the predicted velocity, instead of sensor measurements, to derive control inputs. although the predicted velocity is equal to the reference value, the actual velocity gradually drifts from the reference value due to the error accumulation caused by noise v k . we can see that the drift here is much less than that in fig. 6(a) . after the encoder becomes trustworthy again from time 6, the controller switches to use sensor measurements and the actual velocity turns to the reference value. there is no need to rest the encoder, because the system has not drifted too much (e.g., outside the required range) till the time when the encoder comes back trustworthy. hence, our approach well protects the system from the transient attack. in addition, if the attack continues after time 6 and lasts for longer time, the system needs to cease the resilient control before it drifts outside the required range."
"in the first experiment, we analyze the impact of migration on transaction latency using the default workload parameters described above. we ran a workload of 10, 000 transactions, after warming up the cache with another workload of 10, 000 transactions; figure 3(a) plots the distribution of latency (or response time) of each individual transaction as a box and whisker plot. the four series correspond to the observed transaction latency of an experiment when migration was not initiated (normal) and that observed when migration was initiated using each of the three different techniques. the inset shows the same plot, but with a restricted range of the yaxis. the box in each series encloses the 25 th and 75 th percentile of the distribution with the median shown as a horizontal line within each box. the whiskers (the dashed line extending beyond the box) extend to the most extreme data points not considered outliers, and outliers are plotted individually as circles (in blue). 4 the number beside each series denotes the number of outlier data points that lie beyond the whiskers."
"step 1: physical-state recovery. the physical-state recovery uses eqn. (4) to predict and estimate the current system state. after the recovery, the first key question is whether the estimated statex k is accurate enough to be used for control. ifx k is far from the plant's realistic state, the derived control inputs may drive the plant to drift even further away. let e be the vector that denotes the maximum state estimation error that the system can tolerate. given eqn. (9), if the estimation error satisfies"
"in this paper, an approach to automatically detect citrus trees is presented. the approach considers the symmetric nature of the citrus trees and detects them from a single source, dsm. the main novelty of the proposed approach is the integration of lm information into the framework to improve the output of the orientation-based radial symmetry transform."
"the first three cost metrics measure the external impact on the tenants and their slas while the last metric measures the internal performance impact. in a cloud data platform, a provider's service quality is measured through slas and satisfying them is foremost for customer satisfaction. a long unavailability window or a large number of failed requests resulting from migration might violate the availability sla, thus resulting in a penalty. for instance, in google appengine, if the availability drops below 99.9%, then tenants receive a service credit. 2 similarly, low transaction latency is critical for good tenant performance and to guarantee the latency slas. for example, a response time higher than a threshold can incur a penalty in some service models [cit] . a live migration technique must have minimal impact on tenant slas to be effective in elastic scaling."
"cache flush during the handover phase ensures that writes from transactions that have committed at nsrc, are persistent. the log entries of such committed transactions on cmigr are discarded at nsrc after successful migration."
"parameter value [cit] . considering the threshold h, we tested a range of parameters to select the best value, and we found 0.2 m provided the best balance between the precision and recall (fig. 4g )."
"to be effectively used for elasticity, database migration must be lightweight, i.e. with minimal service interruption and negligible performance impact. this feature is called live migration in the virtualization literature [cit] . due to static provisioning in enterprise infrastructures, elasticity and live migration were not critical for traditional relational database (rdbmss). even though most key-value stores [cit] support data migration for fault-tolerance or load balancing, they use heavyweight techniques such as stopping part of the database, migrating it to a new node, and restarting it at the destination. such stop and migrate techniques (or simple optimizations) have a high performance penalty resulting from aborted transactions (due to the source node stopping the tenant) and high impact on transaction latency and throughput (due to the destination node starting with a cold cache). existing dbmss are therefore not amenable to elastic scaling."
"this is ensured since albatross copies the commit log entries for transactions active during migration to n dst, which are then forced to n dst 's commit log when these transactions commit."
"we have introduced the physical-state recovery. this section discusses how to checkpoint a system, i.e., how to record system states that can be used for the recovery."
"during the handover phase which were stalled during the handover and resumed at n dst . on the other hand, both s&m and f&m result in a high impact on transaction latency with about 1500 or more transactions having a latency higher than that observed during normal operation. the high impact on latency for s&m and f&m is due to cache misses at n dst and contention for the nas. since all transactions active at the start of migration are aborted in f&m and s&m, they do not contribute to the increase in latency."
"performance (%) table 4 . comparison of the results of the state-of-the-art and the proposed approach. pre., rec., and f 1 -s. denote precision, recall and f 1 -score, respectively. of citrus trees. besides, assessments performed reveal that our approach is capable of providing the citrus trees in a scene with a promising performance (average f 1 -score of ≈ 88%)."
"the detection of individual trees using automatic and semiautomatic methods from very-high-resolution (vhr) datasets is one of the challenges of remote sensing and computer vision. the trees are one of the most significant topographic elements of the plant cover, because they are the major reservoirs of providing nutrition, forestry, shelter, co 2 storage. strategically important certain types of trees like citrus, palm etc. requires further attention since achievement of decisions towards quick and reliable agricultural production is further important to ensure the sustainability of agricultural production."
"settings and model. the scenario considered for this experiment is that the operator specifies the desired vehicle speed, and the controller needs to ensure this speed as much as possible even if the speed sensor/encoder is under attack. we use simulink to build a simulator where a dc motor drives an inertial load. dc motors are widely used in electric vehicles and many autonomous car prototypes. we use the dynamic model of the dc motor given by"
"automated extraction of trees is an open research area of remote sensing and computer vision, and numerous studies have been published so far. general trends, gaps and possible feature trends for tree extraction were examined in a recent review paper [cit] . it was reported that active data are suitable for tree species classification, whereas passive and (a) (b) figure 1 . test patch #1. (a) [cit], and (b) the photogrammetric dsm generated using dense image matching (bright tones represent elevated parts)."
"the above just introduces some key background for cyberstate recovery. there are many existing works that study cyber-state recovery and validate its feasibility [cit] . thus, in the following, we will focus on discussing our newly proposed physical-state recovery."
"therefore, for all the techniques, an impact on transaction latency (and hence throughput) is observed only in a time window immediately after migration completes. hence, for brevity in reporting the impact on latency, we report the percentage increase in transaction latency for cmigr in a time window immediately after migration, with the base value being the average transaction latency observed before migration. we select 30 seconds as a representative time window based on the behavior of latency in figure 3(b) where n dst is warmed up within about 30 − 40 seconds after the completion of migration. we also measured the percentage increase in latency in the period from start of migration to 30 seconds beyond completion of the respective migration techniques. since albatross takes much longer to complete compared to the other techniques and has minimal impact on latency during migration, this measure favors albatross and unfairly reports a lower increase for albatross. therefore, we consider the 30 second window after migration such that all techniques can be evenly evaluated. figure 4 plots the migration cost as a function of the load, expressed as transactions per second (tps), on cmigr. as the load on the tenant increases (20 tps to 100 tps), the amount of un-flushed changes in the write cache also increases. hence the unavailability window of s&m increases with load (see figure 4(a) ). but since both albatross and f&m flush the cache (at least once) before the final phase, they are not heavily impacted by load. the unavailability window of albatross increases slightly since at higher load more transaction state must be copied during the final handover phase. similarly, a higher load implies more transactions are active at the initiation of migration which are aborted in f&m and s&m, resulting in a large number of failed requests (see figure 4(b) where the wider bars represent transactions aborted and the narrower bars represent failed operations). albatross does not result in any failed requests since it copies transaction state and allows transactions to resume at n dst . both f&m and s&m incur a high penalty on transaction latency. the impact increases with load since more read operations incur a cache miss, resulting in higher contention for accessing the nas (see figure 4(c) ). albatross results in only 5 − 15% transaction latency increase (over 80 − 100 ms average latency) in the 30 second window after migration, while both f&m and s&m result in 300−400% latency increase. finally, figure 4 (d) plots the amount of data synchronized as a function of load. in spite of the increase in data transmission, we note that this does not adversely affect performance when using albatross."
"thirdly, the protocol discards those checkpoints that are no longer needed. as shown in fig. 3, the checkpoint at time t k−n −1 is discarded because the checkpoint at time t k−n is newly stored and regarded as correct."
"we visualize the results of test patches in fig. 5 . these results demonstrate that our approach can provide promising results for the detection of citrus trees. the numerical results in table 2 favour these facts. we achieved overall precision and recall as 94.9% and 81.6%, respectively. the computed f 1 -score for these five test patches is around 87.8%. our approach correctly detected 3502 of 4290 citrus trees in all test patches and the total false positive object number is just 190. according to fig. 5, the results give the impression that the detected citrus trees are convincing and representative. as shown, most of the citrus trees are detected successfully without having a strict limitation, e.g., planting pattern and orientation, texture, shape, elevation. it is also evident that the approach has capability to separate citrus trees from most of the other objects (e.g. bushes, other trees, water, roads, and greenhouse roofs)."
"(i) detection window. the window size (of n sampling periods) represents how many historical data are used for detection, for example, n latest sensor measurements used to detect sensor attacks/faults."
"checkpointing a system means occasionally storing the state of the system on safe and secure devices. note that states in this section include both physical-state information (i.e., estimated physical-states from sensor measurements) and control inputs. the stored states are called checkpoints or recovery points. if no failure has occurred, a checkpoint is regarded as correct and thus a trustworthy state. the checkpoint will be then stored and used for potential recoveries in future. the protocol stipulates that detection is carried out before saving system states. this stipulation will improve the correctness of checkpoints and thus improves the probability of successful recovery."
"considering the previous effort in this context, our approach specializes for the detection of citrus trees by taking into account two critical observations: (i) the citrus trees have a symmetric circular shape in general, and we present orientationbased radial symmetry transform [cit] to extract that information, (ii) the citrus trees present an lm with respect to their close neighbourhood, which we utilize extended maxima transformation [cit] to extract lms from a dsm. thereafter, we filter out erroneous detections arising from symmetry transform using the lm information."
"we now evaluate the migration cost in a system with high load. figure 12 shows the impact of migrating a tenant from an overloaded nsrc to a lightly loaded n dst . in this experiment, the load on each tenant is set to 50 tps and the number of tenants served by nsrc is gradually increased to 20 when nsrc becomes overloaded. as the load on nsrc increases, all tenants whose database is located at nsrc experience an increase in transaction latency. at this point, one of the tenants at nsrc is migrated to n dst . in figure 12, the y-axis plots the percentage change in transaction latency in the 30 second window after migration; a negative value implies reduction in latency. tm is the tenant that was migrated, ts is a tenant at nsrc and td is a tenant at n dst . the latency of tm 's transactions is higher when it is served by an overloaded node. therefore, when tm is migrated from an overloaded nsrc to a lightly loaded n dst, the transaction latency of tm should decrease. this expected behavior is observed for albatross, since it has a low migration cost. however, the high cost of f&m and s&m result in an increase in transaction latency even after migrating tm to a lightly loaded n dst . this further asserts the effectiveness of albatross for elastic scaling/load balancing when compared to other heavyweight techniques like s&m and f&m. all migration techniques, however, have low overhead on other tenants co-located at nsrc and n dst . this low overhead is evident from figure 12, where a small decrease in latency of ts results from lower aggregate load on nsrc and a small increase in transaction latency of td results from the increased load at n dst ."
"second, the buffered states that have moved outside the detection window, are considered to have successfully passed the detection and thus are regarded as correct states if no failure (e.g., failures caused by compromised sensors) is detected so far. as shown in fig. 3, the checkpoint at time t k−n is outside the detection window, and thus state (x k−n, u k−n ) is saved. storing after buffering can eliminate the case of saving states between the occurrence of an attack and its detection as well as the corresponding latent failures caused by this case."
"guarantee 1 ensures data safety and guarantees 2, 3, and 4 together ensure durability, thus guaranteeing the safety of albatross. therefore, in the presence of a failure of either nsrc or n dst, the migration process is aborted without jeopardizing the safety."
"capability of the recovery and resilient control. as the system keeps operating under the recovery-based resilient control, the drift e k+m can grow larger and larger over time. when the drift cannot satisfy the maximum tolerable estimation error, i.e., e + k+m e, the system ceases the resilient control and may need to reset the attacked sensors or the portion under attack. in other words, the proposed approach, i.e., the physical-state recovery together with the resilient control, is a conservative way to deal with sensor attacks, which is able to postpone resetting the attacked sensors or the attacked portion. resetting them usually incurs much higher overhead (e.g., possibly pausing the plant) to the system than does the proposed approach."
"a lightweight live migration technique is also helpful in a scenario when one tenant faces a load spike. it allows the system to either migrate other lightly loaded tenants from the overloaded node to another node or migrate the overloaded tenant to a node with more resources. the first option minimizes the total load on the source node while isolating other tenants from being impacted by the heavily loaded tenant. moreover, as observed in our experiments, migrating lightly loaded tenants is less expensive compared to migrating a tenant with a high load. on the other hand, the second option requires migrating only one tenant, though at a higher migration cost. the option chosen will depend on the workload and tenant characteristics. an intelligent system controller can make prudent use of live migration for such elastic load balancing."
"cyber-physical systems (cps) tightly couple computing and communication processes with sensing and actuation components that interact with the physical world. a typical example for such systems is modern vehicles, which demonstrate a complex interaction of many electric control units (ecus) over different types of networks. the increasing functionalities and network interoperability transition cps from isolated control systems to more open interacting architectures, which enables various new services and applications such as remote code updates and vehicle-tovehicle communication. meanwhile, this transition, however, introduces potential security vulnerabilities that are easily exploitable [cit] ."
"the low impact of albatross on transaction latency is further strengthened by figure 3 (b) which plots the average latency observed by the tenants as time progresses; latencies were averaged in disjoint 500 ms windows. the different series correspond to the different migration techniques and are aligned based on the migration start time (about 38 seconds). different techniques complete migration at different time instances as is shown by the vertical lines; s&m completes at about 40 seconds, f&m completes at around 45 seconds, while albatross completes at around 160 seconds. the iterative phase for albatross is also marked in the figure. as is evident, both f&m and s&m result in an increase in latency immediately after migration completes, with the latency gradually decreasing as the cache at n dst warms up. on the other hand, even though albatross takes longer to finish, it has negligible impact on latency while migration is in progress. this is because in albatross, most of the heavy lifting for copying the state is done by n dst, thus having minimal impact on the transactions executing at nsrc. a small spike in latency is observed for albatross immediately after migration completes which corresponds to transactions active during the final handover phase. the low impact on latency ensures that there is also a low impact on transaction throughput (see figure 5"
"tasks task0, task1, and task2 are called to be dependent on each other due to their mutual communication; while task3 is independent on other tasks because it has no communication with them. for an independent task, e.g., task3, any of its previously stored correct cyber-states can be used for recovery, and the latest one is usually used for the global consistent state in order to shorten the recovery time as much as possible. the cyber-state recovery can be that the cyber component rolls back to the global consistent state in fig. 1(a) if some failure occurs. with stored message m1, task0 does not need to resend it and thus task1 can then start to execute from that state."
"dsm patch #4 has the lowest performances for all measures (table 2 ). however, this result was expected because that test area is the most challenging case. the main reason for this challenge is that most of the citrus trees in the dsm are newly planted non-bearing trees. thus, there may not be enough evidence for the accumulation performed in image space for most the young trees even though the processed dsms have very high gsd (≈ 7 cm/pixel). nonetheless, the f 1 -score computed is still slightly above 54%. then again, the second lowest f 1 -score is observed for dsm patch #2. this is due to the recall computed around 81%, and the problem reappears in the upper right part of the area where only half of the newly table 3 . computational time elapsed by the proposed approach."
"in all experiments, except the one presented in appendix b.1.7, we evaluate migration cost when both nsrc and n dst were lightly loaded, so that the actual overhead of migration can be measured. the load on a node is measured using the amount of resources (for instance cpu cycles, disk i/o bandwidth, or network bandwidth) being utilized at the node. when resource utilization is less than 25%, it is referred to as lightly loaded, utilization between 25 − 70% is referred to as moderately loaded, and utilization above 70% is called overloaded. we only consider cpu utilization."
"our study area covers the northern part of the city mersin, a region covering one of the most productive citrus orchards of turkey. we selected five test patches from the generated dsms to assess the performance of the proposed approach (fig. 3) . in this study, gsd of all test patches is reduced as a factor of two (≈ 7 cm) to facilitate and speed up the processing. for validation purposes, the reference data are generated by manual on-screen digitizing using dsms and uav true ortho-images of the test patches ( fig. 4a-f ). partial citrus trees at the edges of the dsm patches are also included during the digitization process."
"we also ran experiments varying a number of other parameters of the ycsb workload as well as with tpc-c. most experiments follow a trend similar to that observed here, except for the case when the working set of a tenant does not fit in the cache. in that case, albatross results in a longer unavailability window compared to s&m and f&m since more state must be synchronized in the final handover phase. this is because the state of the cache changes frequently during migration. albatross, however, continues to have no failed requests and low impact on transaction latency. the low cost incurred by albatross also makes it effective for live database migration allowing for lightweight elasticity as a first class notion in databases. a detailed analysis of the experiments is provided in appendix b. in summary, albatross's migration cost is considerably lower compared to s&m or f&m. albatross has a small unavailability window, zero failed requests, and less than 15% increase in transaction latency immediately after migration."
"firstly, cyber-state recovery defines the state consistency based on logic to ensure the computational correctness of tasks of a controller; while the physical-state recovery defines the consistency based on time to ensure that values of internal elements reflect the plant's state of the same time point. secondly, cyber-state recovery is rolling the cybercomponent or the controller back to a consistent global state and the roll-back may propagate among tasks because of their dependence; while the physical-state recovery is rolling internal elements' values forward to the current time and the roll-forward does not propagate among internal elements."
"thirdly, cyber-state recovery confines to the cyber-states within one single sampling period; while the physi-calstate recovery has a scope of physical-states across multiple sampling periods. the execution of tasks in one sampling period is usually separate from their execution in other periods, and thus the cyber-states are only valid and useful within one single sampling period. by contrast, the physicalstates (i.e., values of internal elements) do not change while executing control tasks within one period, and instead they may change across sampling periods. hence, for a case that needs both recoveries, a system first performs physicalstaterecovery and then carries out cyber-state recovery."
"t . we consider elements dependent on attacked/faulty sensors as failed. by def. 4, the physical-state recovery is to perform the roll-forward state prediction for q failed elements using the following two steps."
"the rationale of defining the physical-state consistency based on time is two-fold. first, in real systems, clocks are synchronized within some very small fixed time interval, and we acknowledge that clock synchronization is nonnegligible, in general. however, to simplify presentation, here we assume clock synchronization error is small and omit clock synchronization uncertainty in the analysis. second, internal elements of a system state change separately as to the output of the system."
"with the growing number of applications being deployed in different cloud platforms, the need for a scalable, fault-tolerant, and elastic multitenant dbms has also increased. in such large multitenant systems, the ability to seamlessly migrate a tenant's database is an important feature that allows effective load balancing and elasticity to minimize the operating cost and to ensure efficient resource sharing. we presented albatross, a technique for live database migration in a shared storage architecture that results in minimal performance impacts and minimal disruption in service for the tenant whose database is being migrated. albatross decouples a cell from the dbms node owning it, and allows the system to routinely use migration as a primitive for elastic load balancing. our evaluation using ycsb and tpc-c benchmarks shows the effectiveness of albatross and analyzes the associate trade-offs. in the future, we plan to extend the design by adding an intelligent system control that can model the cost of migration to predict its cost as well the behavior of the entire system."
"during normal operation, the progress of albatross is guaranteed by the maximum bound on the number of iterations that forces a handover. since albatross does not log the progress of migration, the state synchronized at n dst is not persistent. this is because albatross copies the main memory state of cmigr which is lost after a failure, little gain can be achieved by logging the progress at either node. progress towards migration is therefore not guaranteed in case of failures."
"we focus on designing an efficient and low cost technique for live migration of a tenant database in a multitenant dbms. live migration enables elasticity as a first class notion and eases database administration by decoupling a tenant's database from the node hosting it, thus allowing virtualization in the database tier. we propose albatross, 1 the first end-to-end technique for live migration in shared storage database architectures executing oltp workloads. in a shared storage dbms, the persistent database image is stored in network attached storage (nas) servers. this decoupled storage abstraction allows independent scaling and fault-tolerance of the storage layer [cit] . we assume a multitenancy model where multiple tenants share the same database process (shown to result in more effective resource sharing [cit] ) where live virtual machine (vm) migration techniques [cit] cannot be used to migrate the individual tenant databases."
a multitenant database consolidates multiple tenant databases to a single node to improve resource utilization [cit] . tenants are of- * the author conducted this work as a visiting researcher at ucsb.
"we now present a more in-depth evaluation of migration cost using ycsb and tpc-c benchmarks that augments the experiments presented in section 6. in the figures reporting the number of failed requests, the wider bars represent transactions aborted and the narrower bars represent failed operations. figure 5 plots the impact of migration on throughput as time progresses (plotted along the x-axis). the y-axis plots the throughput measured for a second long window. the load is generated by four clients threads which issue transactions immediately after the previous transaction completes. the different series correspond to different migration techniques. as is evident from the figure, both s&m and f&m result in a high impact on the client throughput due to increased transaction latency after migration, coupled with throughput reduction during the unavailability window. on the other hand, albatross results in minor throughput fluctuations, once during the first snapshot creation phase and once during the unavailability window in the handover phase; albatross results in negligible impact during migration since the list of block identifiers in the cache snapshot is maintained incrementally and n dst performs most of the work done during the synchronization phase."
"between time t k and t k+m, the system may continue drifting away, since the control relies on the predicted states for the failed elements. let e k+m to denote the overall error accumulated from t k−n to t k+m . given eqn. (4)(12), we can have (1,q)"
"in this paper, we develop a procedure of cps recovery and checkpointing that well addresses both challenges. firstly, we propose a new concept of physical-state recovery. the essential operation is defined as rolling the system forward to the current time, starting from a consistent historical physical-state. this roll-forward operation has much lower overhead than the conventional roll-back operation in the context of recovering physical-states. secondly, we design a checkpointing protocol that defines how to record system states used for the recovery. the protocol employs a sliding window that accommodates the detection delay to improve the correctness of the stored states. note that the cps checkpointing and recovery is an reactive procedure that needs to be used with existing detection mechanisms (e.g., [cit], that is, the physical-state recovery is triggered to execute if the mechanisms discover some compromised components; otherwise, the system follows the checkpointing protocol to record states."
"occ guarantees serializability by validating transactions against conflicts with other concurrent and committed transactions. the handover phase copies the state of active transactions and that of a subset of transactions that committed after the earliest of the active transactions started. therefore, all such active transactions can be validated at n dst and checked for conflicts."
"the overall error e k caused by the physical-state recovery is composed of two parts: (i) the state prediction error e k, (1,q) of the q failed elements, and (ii) the state estimation error e k,(q+1,n) of the n − q unfailed elements. that is,"
"migration correctness or safety implies that during normal operation or in case of a failure during migration, the system's state or data is not left in an inconsistent state as a result of migration."
"conceptual discussion. the defined recovery-based resilient control has some similarities and differences with two established control concepts: open-loop control and closed-loop control. (i) compared with open-loop control, one similarity is that both methods do not use measurements of compromised sensors as feedback to a controller. one difference is that the defined resilient control uses the predicted state (based on the physical-state recovery) and measurements of uncompromised sensors as feedback, while open-loop control does not. (ii) compared to closedloop control, both methods have feedback, but the content of feedback is different. that is, one is based on sensor measurements, while the other one is the predicted state of failed internal elements as well as using measurements of uncompromised sensors."
"a network attached storage (nas) provides a scalable, highly available, and fault-tolerant storage of the persistent image of the tenant databases. this decoupling of storage from ownership obviates the need to copy a tenant's data during migration. this architecture is however different from shared disk systems which use the disk for arbitration amongst concurrent transaction [cit] . a system controller performs control operations including determining the cell to migrate, the destination, and the time to initiate migration."
"we argue migration safety using a series of guarantees provided by albatross and reason about how these guarantees are met. this is trivially satisfied when no failures occur. we now describe how logging and recovery ensures atomicity during failures. at most one owner: a failure in the first phase of the atomic handover protocol is handled similar to a failure during phases 1 and 2-both nsrc and n dst recover normally and abort cmigr's migration and nsrc remains the owner. failure in this phase does not need coordinated recovery. after receiving responses (both yes or no votes), nsrc is ready to complete the transfer transaction and enters the second phase of atomic handover. once the decision about the outcome is forced into nsrc's log, the transfer transaction enters the second phase. a failure in this phase requires coordinated recovery. if nsrc decided to commit, n dst is the new owner of cmigr, otherwise nsrc continues as the owner. if nsrc failed before notifying n dst, n dst must wait until the state of nsrc is recovered before it starts serving cmigr. therefore, the atomic handover protocol guarantees that there is at most one owner of cmigr. at least one owner: a pathological condition arises when after committing the transfer transaction at nsrc, both nsrc and n dst fail. atomic handover guarantees that in such a scenario, both nsrc and n dst do not relinquish ownership of cmigr. if the handover was complete before nsrc failed, when nsrc recovers, it transfers ownership to n dst . otherwise nsrc continues as the owner of cmigr. the synchronized recovery of nsrc and n dst guarantees at least one owner."
"authors [cit] reported 595,690 deaths out of 1,685,210 [cit] deaths out of 64,300 reported new cases of thyroid cancer, and 26,120 deaths out of 180,890 reported new cases of prostate cancer in the same year. early detection plays a significant role in the successful treatments to improve the survival rate of patients. ultrasound (us) imaging is widely used in cancer tissue screening [cit] because of its relative safety [cit], low b kamal jnawali kj5500@rit.edu 1 rochester institute of technology, rochester, ny, usa 2 university of rochester, rochester, ny, usa cost [cit], noninvasive and non-ionization nature [cit], and realtime imaging [cit] . transrectal ultrasound (trus) is a very common us imaging modality for prostate cancer detection; however, there are cancers not visible to trus [cit] . the authors [cit] reported that the sensitivity and accuracy of trus in detecting prostate cancers are 41.1% and 67.3%, respectively. this is because the us has a relatively low contrast, which is based on the detection of mechanical properties of the tissue [cit] ."
"the authors [cit] also extracted a handcrafted feature to train the classifier. the process is very labor-intensive and timeconsuming. in this paper, the entire sample was labeled as cancer if there was any encircled region of interest was corresponding to the cancer region and the sample without cancer region was labeled as normal. this helps to avoid very laborintensive co-registration work. the paper also implemented the deep 3d cnn for an automatic cancer feature extraction and detection. this helps to avoid any work related to the extraction of handcrafted features."
"the ground truth for this dataset was provided in the form of encircled region of interest corresponding to the cancer region if any (fig. 7), and the rest as the normal region. the authors [cit] implemented the logistic function on the averaged pixel-based samples extracted from the encircled region of interests corresponding to cancer and normal region provided by the histopathologist. that involves the extraction of the cancer region and normal region using co-registration of the pa image, photograph, and histopathology slide (fig. 2 )."
"the authors [cit] implemented the deep 3d cnn with seven layers on the prostate mpa image with the auc of 0.72, given the auc of 0.85 on the thyroid test dataset. the authors [cit] reported that there is a similar trend in classifying cancer from normal with prostate and thyroid tissues. with this assumption, we created the train, validation, and test datasets by mixing the thyroid and prostate mpa image datasets with more balanced samples to train with deeper 3d cnn than the previous work [cit] . a deep learning network generally improves with more samples [cit] and the depth of the network (more number of cnn filters extract more number of features [cit] )."
"activation map generated by the deep 3d cnn on the normal and cancer mpa test images. authors [cit] reported that the malignant tissue is more likely to be in irregular shape and normal tissue is more likely to be in a rounded shape as random cropping and random warping. table 5 compares the current work with the previous works [cit] . the activation map (visualization of the weights on the first convolutional layer which looks directly at the raw pixel data of the input image) generated by the proposed deep 3d cnn is shown in fig. 11 . the activation map for the cancer tissue shows broader lumpy distribution than that for the normal tissue, which may be a key feature to detect cancer. this could be due to a higher blood distribution around the malignant tissue [cit] ."
feature used dataset metric scalar value authors logistic function amplitude pixel-based handcrafted prostate accuracy 0.9 [cit] logistic function amplitude pixel-based handcrafted thyroid accuracy 0.83 [cit] 2d transfer learning (inception-resnet-v2) 2d image-based automated thyroid auc 0.72 [cit] deep 3d cnn with 7 layers 3d image-based automated thyroid/prostate auc 0.85/0.72 [cit] deep 3d cnn with 11 layers 3d image-based automated thyroid and prostate auc 0.96 this paper
"where y k (φ) represents a softmax function, k is the number of class, t k is the class target vector of 1-of-k coding scheme, a k is given by w t k φ, y nk is given by y k (φ n ) [cit] . the dataset for this paper was prepared by mixing the thyroid and prostate datasets with the assumption that the dominant discriminant features extracted by a pai for cancer detection are common in both cases [cit] . the thyroid and prostate datasets were combined to make 136 datasets."
"this section describes the step involving data preparation and the details of the algorithm for cancer tissue detection. this paper introduced the deep 3d cnn with 11 layers and proved to be the best model to date, to the best of our knowledge, compared to previous models [cit] ."
"machine learning algorithms are very popular in cancer tissue detection [cit] . the machine learning algorithms used in the previous works required feature engineering [cit] . in the previous studies, the authors [cit] trained and evaluated the logistic function with a ground truth consisted of the encircled malignant and normal region (fig. 2 ). the region of interest in each sample was extracted by spatially co-registering, tissue image ( fig. 2a ) and histopathology slide ( fig. 2b ) with the two-dimensional c-scan pa image ( fig. 2c ) based on histopathological marking ( fig. 2b), where the histopathology marking was annotated by the pathologist. the discriminant analysis was performed on pa image data that fell within the encircled regions, not on the entire image. the authors took one c-scan image out of 200 pa c-scan slices at each wavelength and later concatenated cscan at five wavelengths (fig. 5 ). the five-c-scan image at five wavelengths was converted to 4 chromophore images (oxy, deoxy, lipid, and water). the set of pixels from the encircled normal and malignant region were collected to generate the 4-channel chromophore image and averaged to generate one pixel with four-channel corresponding to oxy, deoxy, water, and fat. thus, the pixel-based amplitude feature was only used, thus ignoring any possible (2d and 3d) spatially discriminant features. in the previous studies, the region outside the encircled malignant tissue region was labeled as the normal tissue. however, in this paper, the mpa dataset was labeled as cancer if there is any cancer marking in the tissue mpa dataset. this reduces the labor-intensive co-registration to extract malignant and normal regions from the datasets. [cit] . the encircled region taken at the deoxyhemoglobin channel of the pa image (e) shows the presence of malignant region because deoxyhemoglobin absorbs more light to generate a higher pixel intensity region. the presence of deoxyhemoglobin is a strong indicator of the presence of cancer [cit] . the encircled region with the malignant tissue in the pa image at 760 nm wavelength corresponds to the higher pixel intensity [cit] . previous works required humans to extract the encircled region of interest corresponding to cancer and non-cancer regions by the co-registration of the histopathological slide, photograph-based image, and pa-based image. this manual process was very labor-intensive and time-consuming the previous methods of using machine learning algorithms [cit] ignored the spatial 2d and 3d volumetric distribution of the chromophores. the authors [cit] reported that the 2d and 3d volumetric distributions of chromophores are capable of describing the structural distribution of the malignant and normal tissues. beard [cit] reported that the mpa imaging helps to extract the spectral signature and spatial and volumetric distribution of the chromophores in normal and cancer tissues. the authors [cit] reported that pa imaging modality has been used for measuring the oxygen content of chromophores, the key feature to detect cancer, in the blood. the authors [cit] also reported that the high density of the blood vessel in the malignant region compared to the normal region enhances the contrast of the pa imaging, mak- 4 schematic of the chromophore distribution. the first 2d data structure was able to extract spatial information only inside the white circle (left image, [cit] ), and the second 3d data structure was able to extract spatial and volumetric information inside the white cylinder (right image, [cit] ) at once ing it a suitable cancer detector. since the malignant tissue is generally richer in blood volume [cit], 2d spatial and 3d volumetric distribution along with spectral signatures of the chromophores in the blood could be the additional predictive features for cancer tissue detection."
"the pai modality is capable of extracting rich details of the tissue architecture, optical contrast, and molecular distribution as the different optical energy wavelength excites the different tissue chromophores (oxyhemoglobin, deoxyhemoglobin, lipid, water) [cit] . the pa imaging at the given wavelength is capable of generating high-contrast optically active corresponding chromophore images as the chromophore has different absorption properties at a different wavelength. five wavelengths can generate high-contrast pa images of the optically active chromophores such as oxyhemoglobin (high in oxygen content), deoxyhemoglobin (low in oxygen content: presence of cancer), lipid, and water at 850 nm, 760 nm, 930 nm, and 970 nm, respectively, and the fifth wavelength was chosen at 800 nm where the absorption coefficient of oxyhemoglobin and deoxyhemoglobin is equal [cit] . in fig. 1, the encircled region corresponds to the cancerous region, hence producing brighter pixel distribution at 760 nm due to a higher concentration of deoxyhemoglobin and less bright pixel distribution at 850 nm due to lower concentration of oxyhemoglobin. cancer's region consists of higher deoxy content and lower oxy content [cit] . these chromophores make the pa imaging system to image optical biomarkers for cancer tissue detection [cit] . in multispectral photoacoustic (mpa) imaging, the large difference in light absorption coefficient between blood and other tissue constituents enables the detection of tissue angiogenesis associated with rapid tumor growth in the early stages [cit] . that makes the pa imaging capable of functional imaging. the pa image acquisition with a us transducer produces robust and less error-prone co-registered images capable of both structural and functional imagings compared to current imaging techniques such as mri with the us [cit] . the functional and structural information such as spatial distribution, fig. 1 schematic of c-scan of pa imaging taken at two wavelengths [cit] . the white encircled region corresponds to cancer. the region is brighter in the 760 nm because the deoxyhemoglobin, a key feature of cancer, corresponds to a higher abortion coefficient at the given wavelength [cit] volumetric distribution, and the spectral signature of the chromophores can be encoded in a multispectral 3d pa imaging cube [cit] ."
"this paper introduced more choices of data augmentation techniques compared to the previous work [cit] such as random cropping and warping to reduce over-fitting and improve predictive performance [cit] to develop a state-ofthe-art deep neural network for cancer tissue detection. this paper also implemented a random search algorithm to optimize the hyperparameter set to improve the performance of the current model [cit] . this paper introduced, to the best of our knowledge, a novel idea to mix two tissue specimens: prostate and thyroid for cancer detection using a deep learning method to increase the sample size where the dataset is small. hence the proposed model can be implemented to detect cancer on both thyroid and prostate mpa datasets with higher accuracy; in contrast, the previous models [cit] were trained only either on thyroid or on prostate tissue specimen and hence unable to detect cancer in both tissue specimens at once with higher accuracy."
"the authors [cit] implemented the 2d transfer learning network with inception-resnet-v2 for cancer tissue detection with auc of 0.72 on the thyroid mpa dataset. the model was implemented on the 2d c-scan pa images with the spatial distribution of the chromophores only. the method fails to extract the volumetric distribution of cancer tissue ( fig. 4) . 3d volumetric pa signal information in the image dataset is useful for cancer detection because the cancer region most likely exists three-dimensionally in any given specimen. the cancer region, if present, most likely extends in three dimensions in a 3-5-mm-thick tissue specimen ( fig. 2a ), even though it is seen in the thin histopathology slide ( fig. 2b) in two dimensions only. the pa data that are subject to the cnn analysis are volumetric because it is taken from the entire three-dimensional specimen ( fig. 3, pa image cube). as the cancer tissue spreads in all directions (x -, y -, and zdirections), it is useful to implement the deep 3d cnn [cit] which can extract all the spatial and volumetric features at once [cit] . the authors [cit] implemented the deep 3d cnn with the seven layers to detect cancer with the auc of 0.85 on the thyroid mpa dataset. the performance improvement to the network was due to the addition of the 3d distribution of the chromophores (fig. 4 )."
"photoacoustic imaging (pai) is a new medical imaging technique [cit] that is currently making a transition from bench to bedside, both in terms of technology [cit] and in terms of clinical applications [cit] . pai is based on the photoacoustic (pa) effect, which is a phenomenon of generating acoustic waves from an object illuminated by pulsed laser light [cit] ."
"the current model overcomes some of the limitations in our previous machine learning works [cit] and is an improvement over our previous deep learning algorithms [cit] . furthermore, our results provide a strong indication that this model has been trained to detect cancer effectively, both in thyroid and in prostate. this is because the model was trained and evaluated using the mixture of thyroid and prostate mpa datasets. this model may have immediate application in cancer screening of the numerous sliced specimens that result from thyroidectomy and prostatectomy. the instrument that was used to capture ex vivo pa images is now being developed for in vivo use; this model may also prove to be a starting point for in vivo pa image analysis for cancer diagnosis."
"the pa imaging instrument that we used (whose setup is shown in fig. 5 ) was particularly designed for in vivo imaging of cancer at 2-3 cm depth from the skin surface. as such, the resolution of 1-2 mm was a compromise between achieving high penetration and high resolution. the cnn model was purposely trained on images that have this level of resolution, one that we expect from any future in vivo thyroid and prostate datasets. had we obtained high-resolution pa microscopy images of the ex vivo samples and then trained the cnn on those images, it might have worked better for high-resolution ex vivo work but most likely not that well on low-resolution in vivo data."
"in this paper, the deep 3d cnn with 11 layers is introduced to detect cancer in a given specimen with the improved result compared to the previous works using machine learning [cit] and deep learning [cit] algorithms. the same dataset was used in this paper; to the best of our knowledge, this is the currently available large-scale ex vivo human specimen study of pa images."
"pai is an emerging noninvasive soft tissue medical imaging modality that exploits the pa effect to combine the strength of optical imaging-capable of producing high-contrast imaging, and ultrasound imaging-capable of producing high resolution in deep tissue imaging [cit] . pai at multiple wavelengths is capable of extracting chromophore signature of a tissue specimen, and hence, the authors [cit] reported that pai has the potential to detect cancer in the early stage. current imaging modality such as magnetic resonance imaging (mri) or xray computed tomography (ct) is not capable of detecting cancer tissue in the early stage and is capable of detecting cancer tissue when the diameter grows to 1 cm in size [cit] . furthermore, pa imaging is also safe due to its non-ionizing radiation properties which make it very promising imaging technology for cancer detection in the near future [cit] . currently, the primary medical imaging modalities used for cancer diagnosis are us, ct, and mri [cit] . although most of these technologies are well established and widely used in practice, there are problems related to low sensitivity and specificity for cancer diagnosis [cit] ."
"bold denotes the following: the method proposed in this paper reduces the effort to train a network for thyroid and prostate separately. though previous works were trained only on one set of tissue specimens (thyroid), their classifiers are unable to detect cancer in another set of tissue specimens (prostate) with a higher confidence and vice versa. to the best of our knowledge, this is one of the first applications of deep 3d cnn with mixing of the two types of tissue specimens to detect cancer at once with very promising results"
"this study adopts isdt and the publication schema for a dsr study suggested by gregor and hevner [cit] . the general overview of isdt, including the kernel theories, meta-requirements (mr), metadesigns (md), and testable hypotheses (th), corresponds to the method section of the publication schema [cit] . the md in isdt corresponds to the artifact description section of the publication schema. the evaluation section of the publication schema is used to test the research hypotheses of isdt."
"here i is the sample number, t i refers to the time of the i-th sample, and the bracket notation in the numerator of (3b) indicates the component of rotation about either the x, y or z axis with k identifying the particular axis. the rates were calculated after applying a median filter (width 5 samples) to the data to remove occasional outliers. equation (3a) represents the linear velocity of the marker origin during the experiment, dependent on both the rotations and the physical location of the marker. this parameter is important from a motion tracking perspective because it relates to the motion blur expected for a given shutter speed. in all of our phantom experiments the marker was positioned identically on the phantom and therefore the ranges and rates reported were directly comparable."
"end if 12: end while 13: return t parallel rrt. early parallel motion planning methods were based on the discretization of c space [cit] . the discretization as presented limits the algorithm to solving relatively low dimensional problems. however, these methods laid the foundation for subsequent work in parallelizing rrts."
"pet measurements were performed on a micropet focus 220 small animal pet scanner (preclinical solutions, siemens healthcare molecular imaging, knoxville, tn, usa). the micropet comprises 168 lutetium oxyorthosilicate (lso) detectors arranged in 4 contiguous rings. each detector is a 12612 lso pixelated array of crystal elements of dimension 1.5161.51610 mm 3 . the field of view (fov) is 19 cm in the transaxial (x-y) direction and 7.6 cm in the axial (z) direction. reconstructed image resolution at the centre of the fov is 1.3 mm [cit] . all data were acquired in list mode format."
"in this work, we made use of the stapl parallel graph library, one of the stapl pcontainers, as the parallel data structure for representing both the region graph and the rrt tree. we implemented bulk synchronous distributed rrt and radial subdivision distributed rrt as stapl palgorithms. the tree pruning process described in section iii-b was implemented using the stapl parallel breadth first search (bf s) algorithm."
"in this work, we present scalable parallel algorithms for computing rapidly-exploring random trees (rrts). in particular, we present two parallel algorithms: (i) an algorithm that extends [cit] by introducing a parameter that controls how much local and concurrent computation is done before a global update and inter-processor communication, and (ii) a novel algorithm that radially subdivides c space into regions and then concurrently builds subtrees in each region which are later connected to form a single tree. by controlling how the subtrees explore the space, we minimize the communication overhead -a major bottleneck in parallel processing. while these parallel approaches employ the standard rrt expansion techniques, we note that they both result in trees that are structurally different than would be constructed sequentially."
"the principle of cdt explains that people page 5078 want to avoid disharmony by changing their attitudes or behaviors with an inner drive. cdt suggests three ways to reduce dissonance. first, people can adjust their attitudes, beliefs, or behaviors to reduce their mental discomfort by removing the conflict. in the case of iot, if consumers know that there is a serious psr, they are more likely to change attitudes, beliefs, or behaviors around purchasing a product with high risks. however, many individuals have difficulty changing their behavioral responses, despite their well-learned knowledge [cit] . second, people want to eliminate the disharmony between attitudes and behaviors by acquiring new knowledge that outweighs the disharmonious beliefs [cit] . with iot, consumers are likely to expect that iot companies should provide privacy and security protection by implementing such features into their products and services. they are also likely to expect that iot providers carefully protect the privacy and security of consumers' data collected through iot. however, in practice, there are many data breaches, and some companies, such as amazon and google, use consumers' personal information to manipulate their consumers and increase revenue [cit] ."
"the effect of changing the pose sampling rate and synchronisation error are shown in figures 17 and 18. both sets of results are based on the 3-point marker, suitable for an animal. figure 17 shows how these two variables affected visual image quality for the hot rod phantom. in each case the same central transverse slice is shown. synchronisation error of 0.2 s produced noticeable degradation of the image whereas only minor degradation was apparent for sampling rates above 20 hz. figure 18 shows the effect on bias for the hot cylinder of the compartment phantom. bias at the maximum sampling rate (48 hz) with no added synchronisation error was approximately +3 %. this residual error is likely to be due, in part, to the global correction factor used to compensate for lost events during lor rebinning. lors that are spatially transformed in the rebinning process such that they no longer intersect with the detector rings are referred to as 'lost' events. the effect can be approximately compensated for by applying a global scaling factor to the reconstruction [cit] . the upper plot in figure 18 shows that bias worsened most rapidly at pose sampling frequencies below approximately 10 hz but was relatively stable above approximately 20 hz. moreover, the lower plot in figure 18 shows that bias worsened rapidly with increasing synchronisation error in either direction. a greater than 10% change in the bias resulted from synchronisation errors in excess of 0.6 s. figure 19 shows orthogonal slices obtained from the uncorrected and motion-corrected animal data. we stress that in this case no reference measurement was available for comparison. however, as in the phantom studies, it was clear that, qualitatively, there was a marked improvement after motion correction. this indicates the feasibility of using our methods in conjunction with a small marker for motion correction in the intended application."
the ecg signals for the experiments are collected from 28 individuals using own ecg registration hardware. the sampling rate is 512hz and the resolution is 12bit. the system was trained using subsets from these signals. the testing was performed two weeks later in order to prove the time invariance of the features.
"the motion of the phantom during the 20 min and 5 min scans is shown in figures 7 and 8, respectively. data are in micropet scanner coordinates and represent the cumulative motion relative to the start of the scan. the figures show the continuous nature of the applied motion and the higher frequency of motion during the 5 min scan. the flat sections in figure 7 correspond to the 1-min rest intervals in which the phantom was stationary. figure 9 shows typical measurement jitter for the two different marker sizes when stationary and moving. data are shown for rotation about the x and y axes since these dofs exhibited the greatest amount of jitter. the 3-point marker data deviated from the 8-point marker data by up to 0.4u in the stationary case and 3u in the moving case. note that the reduced jitter for stationary markers results from a moving average filter that is automatically applied in software when a marker is detected to be stationary. this filter is off when a marker is moving. tables 1 and 2 summarise the absolute range and rate of motion for each dof for the 20 min and 5 min scans, respectively. the maximum range of motion was comparable for both studies: x, y and z rotations varied from 10-30u, and translations varied from 50-150 mm. about 75% of samples were within +/210u and +/260 mm. in contrast, the maximum absolute rates of rotational and translational motion were 2-3 times greater for the fast motion compared to the slow/moderate motion. in terms of the 75th percentile metric (see tables 1 and 2) the fast motion was about 6-8 times faster than the slow/ moderate motion."
"this study potentially contributes to minimizing consumers' cognitive gaps and improving consumers' awareness of psrs by providing personal psr scores and visualizing the psr scores with a spider map and warning messages. we suggest new classifications and dimensions toward iot information types, such as security, identification, family, location, and time information. these information types are likely manipulated by vendors as well as hackers in iot settings, since iot devices and services are naturally vulnerable toward psrs. last, this research proposes a new design theory for psr disclosures called psr-cbt that explicates consumers' internal assessments toward psrs in iot settings by evaluating disclosing power and control power. furthermore, psr-cbt can be generalized for other consumers' decision making when they have two internal conflicting powers. for example, when a person posts a sensitive picture on facebook, there may be a conflict between disclosure power and control power in his or her mind."
"in this study, our aim was to establish the feasibility of obtaining quantitatively accurate motion-corrected images of the awake rat brain using pet and suitably optimised motion tracking techniques. we describe a hardware-based synchronisation approach with low latency and high accuracy and validate it in a range of phantom studies involving manually applied arbitrary continuous movements, as well as rat motion applied using a six-axis robot. we also explored the impact of synchronisation error and pose sampling rate on the quality and quantitative accuracy of motion-compensated images, as well as the effect of marker size and rate of object motion. our results demonstrate the relative impact of these various factors, and also indicate that motion tracking parameters needed for effective motion correction in preclinical brain imaging of awake rats are achievable in the laboratory setting. this could broaden the scope of animal experiments currently possible with pet."
"we apply the proposed framework on a part of the face image database from the computational vision at the california institute of technology, usa [cit] . the original database contains jpeg images of faces of 19 persons with different lighting/expressions/backgrounds/ and male or female. we select prepare a subset of the database with 10 images per person. next we split this subset in two groups with even images per class, i.e. 5 images per person for the first and second groups."
"research in robotic motion planning spans over three decades, resulting in the development of different types of sequential and parallel algorithms for motion planning [cit] . the recent renewed interest in parallel motion planning algorithms is due to the progress made in sequential algorithms, the ubiquity of parallel and distributed machines, and the demand for more efficiency in solving complex, high dimensional problems such as those arising in manipulation and reconfigurable robotics [cit], computational biology and drug design [cit], as well as virtual prototyping and computer-aided design [cit] . these new application areas test the limit and capability of existing sequential motion planners [cit] . thus, scalable parallelism has a key role to play, both in supporting existing work and in exploring new algorithms needed to solve complex, high dimensional motion planning problems. there are two main sampling-based motion planning approaches: rrt [cit] and probabilistic roadmap method (prm) [cit] . rrt, prm, and their variants are widely considered as state-of-the-art methods for solving motion planning problems. they are efficient and have been highly successful at solving many previously unsolved problems. rrt in particular is well suited for non-holonomic and kinodynamic motion planning problems [cit] ."
"existing tools for raising privacy and security awareness are also insufficient for iot devices. [cit] designed online parental consent for kid's electronic transactions (pocket), which is a practical software solution to protect children from online privacy risks and threats. pocket allows parents to choose a specific user privacy preferences file (uppf) that includes the child's name and 27 specific preferences, including the child's first name, last name, email, address, zip code, parents' credit card numbers, and so forth [cit] . although uppf contains detailed information about the child, it only focuses on basic demographic information."
"in this md, we develop four new dimensions for iot information types based on an inductive approach. [cit] by rsa which is a professional survey company [cit] . the results of these surveys indicate that financial and banking information is the most significant threat for iot consumers, followed by security information identity information, and personal activity information. in particular, consumers' activity information, such as purchasing history and location information, is introduced in new iot technology environments since vendors can manipulate consumers' purchasing behaviors or personalized advertisements via consumers' activity data [cit] . the consumer survey result is summarized in figure 3 . interestingly, there are big cultural differences in personal information types [cit] . for example, us consumers are more sensitive to sharing location data than german and french consumers. us and french consumers are more generous than german consumers in disclosing their information. early adopters want to purchase iot devices and services to improve their ability to achieve their goals, for example the monitoring of diet using wearable iot devices [cit] . figure 4 shows the results of md3. we start with specific iot information types such as credit card numbers, passwords, social security number (ssn), personal activities, and location data. based on the different types of iot data, we synthesize and create ten themes, such as financial, security, id, family, contact, activity, location, and time information. these ten themes potentially reveal four new iot psr dimensions: monetary, security, identification, and manipulation risks. in practice, although we can directly use the ten themes to analyze a consumer's personal psr score, we abstract these ten themes to four dimensions that can lead to a new theory in the privacy and security area. dixon and gellman [cit] classified consumers' information types for purchasing as demographic, contact, vehicles, lifestyle/interests/activities, financial and economic, and financial and credit data. however, the classification method does not directly apply to iot because iot collects more data types. compared to dixon and gellman's classification, the proposed iot information types adds security, id, family, location, and time information. figure 5 shows the proposed ten iot information types."
"we conduct another experiment using the stripline environment. in this environment, we varied the ammount of c f ree volume by varying the obstacles sizes. we fixed the samples sizes at 4096 per region for 256 regions and varied the processor count from 8 to 256. this experiment was conducted on linux cluster and the results are shown in figure 9 . we observed almost linear scalability in all cases."
"the rapid evolution of the internet and the explosion of internet of things (iot) technology has made life very convenient for people, but at the same time, such technological advances posed new challenges to privacy and security protection. beyond expanding traditional person-to-person communication, iot extensively uses a vast array of sensors that are the objects of communication for person-to-things and things-to-things communications, as well as existing cellular communication and wireless technologies, such as bluetooth (bt), wi-fi, and zigbee [cit] . many consumers are attracted to its new features and convenience, either knowingly or unwittingly disclosing their personal information. sometimes, they are not even aware of personal information leaks. [cit] : \"not only is alexa listening when you speak to an echo smart speaker, an amazon employee is potentially listening, too\" [cit] ."
"reconstructed slices of the phantom in three orthogonal planes, before and after motion correction, are shown in figures 10 and 11 for the 20 min and 5 min studies, respectively. qualitatively, in both cases the uncorrected slices showed obvious degradation (for the fast movement (figure 11) no detail of the hot rods was discernable) and the corrected slices showed marked improvement with respect to the motion-free. for the slow and fast motion, respectively, rod diameters of 1.6 mm and 2.4 mm were resolved after correction using both markers, though correction was better in each case using the 8-point marker."
"personal capabilities negatively moderate the impact of information types on the psr scores. to demonstrate the knowledge dimension in this study, we define knowledge as a technical understanding of the iot and general computer-related techniques. based on byrd and turner [cit], we chose to use the term technical knowledge, which represents programming languages, iot devices and services, computer operating systems, expert systems or artificial intelligence (ai), network management and maintenance, developing web or app-based applications, and big data warehousing or data mining skills."
"in this meta-design, we propose a process of generating personal psr scores. first, we identify psrs in iot technology based on existing literature and experts' opinions. given a new iot technology, experts' opinions are particularly important because it is not easy to understand and assess the new technology for novices. furthermore, we use survey data developed by professional survey firms [cit] . second, we generate a taxonomy of iot psrs by classifying personal information types. in particular, iot devices yield various information types that could be breached and used as a tool to manipulate consumers by the vendors [cit] . last, we quantify psrs as scores and then display the scores in a visualization form [cit] ."
"the proposed psr scores can improve consumers' ability to process their information in working memory and long-term memory by increasing their awareness, as well as their personal capabilities to protect their personal information and organizing their distributed information regarding psrs. such improvement will occur because the psr scores will help consumers visualize their use of data types, prior breach experiences, cultural impact, and the level of privacy literacy with a spider map and scores. furthermore, the psr scores will provide a three-level classification of psrs and display where the consumers best fit [cit] ."
our code was written in c++ using the standard template library (stl) and the standard template adaptive parallel library (stapl) [cit] as supporting libraries. stapl is a platform independent superset of stl that provides a collection of building blocks for writing parallel programs.
", respectively, and were chosen so that all the movements were completed in 660 s. this gave an average time per pose of 33 ms, identical to the measured rat data."
"in the last expression the inner products in the new space give the possibility not to deal directly with φ, but to use the kernel matrix"
"there is a clear need for fine-grained parallelism in sampling-based motion planning [cit] . the nearest neighbor search is considered a key bottleneck to scalable performance. in this work, we implement and incorporate a nested and fine-grained parallel computation of nearest neighbor search within the two parallel rrt algorithms described in section iii. our implementation has a map reduce parallel computation pattern."
where the eigenvalues in λ and the columns of v have to be rearranged in descending order of the eigenvalues. the number of principal components is equal to the original length of the input observations m so they have to be reduced. the reduced number of principal components l is calculated according to:
(a) (b) fig. 9 . distribution of the first three principal components from ecg signals taken from 5 individuals using pca -(a) and kpca -(b)
"a combination of rapid cascaded classifier and accurate monolithic one is used as a two level face detection algorithm. the first level is represented by the haar-like features' cascade of weak classifiers, which is responsible for fast detection of face-like objects. the second level is a convolutional neural network (cnn) used for filtration of falsely detected faces. the haar-like features' cascade of weak classifiers allows detecting face candidates very quickly. it consists of a cascade of one or more weak classifiers. the weak classifier's input is represented by haar-like feature with a value [cit] :"
"where p(z 1, z 2,...,z c ) is the unconditional joint probability density. it can be expressed through the conditional distributions and for this reason it could be used for the calculation of the final decision [cit] . also, using the numerator term only allows for various classifier combination rules to be obtained [cit] ):"
"in this dsr study, we adopt cdt and ipt as our kernel theories to support the proposed assessment framework for iot psrs. before discussing the theories, we first present general challenges in privacy decision making and set up the boundary conditions for this study."
"we designed a 3-point marker, of mass 0.6 g and dimensions 24 mm621 mm, for use in awake rat studies. it was small enough to be attached to a rat's head without touching the ears or obstructing its vision, which could stress the animal. for comparison we also used a larger marker (60 mm660 mm) comprising eight points arranged radially. both markers are shown in figure 3 ."
"the 20 min and 5 min studies differed primarily in the rate of motion. to better compare the impact of motion rate on motion correction, a 72 s segment of list mode data was extracted from the 20 min study, motion-corrected and reconstructed. this duration was chosen so that the two studies had comparable counting statistics. motion correction was based on the motion data from the 8-point marker in order to minimise the effect of marker size. the result is shown in figure 13 . comparison of figure 13 with figure 11 (row 3) indicates that increasing the rate of motion resulted in poorer correction, e.g. the 1.6 mm diameter rods were more clearly resolved for the slower motion. figure 14 shows commanded robot motion, simulating that of an awake rat, overlaid with measured motion collected at 30 hz for the x-axis rotation. data have been converted to robot coordinates and represent the cumulative motion relative to the start of the scan. the close agreement of the curves indicates that the tracker faithfully executed the commanded motion. occasional temporal misalignment of the curves was due to the variable time taken by the robot to assume each new pose -dependent on the change in pose, the path calculated by the controller, and the preset speed and acceleration limits. however, the average time per executed pose was 33 ms, the same as for the commanded motion. table 3 summarises the absolute range and rate of motion of the hot rod phantom for each dof. compared to the fast manually applied motion (table 2), the angular ranges were consistently higher and the translational ranges were comparable. angular rates were also comparable whereas the translational rates were comparable to those for the slower manually applied motion."
"for each group of the subset we perform training followed by testing with the remaining group and for each run we calculate the recognition rate. the final recognition rate is calculated by averaging the rates of each run. with this protocol we test our framework with 'ridge' and 'lasso' regression setting of sr, where the former is tested with different value of sparsity. the dimensionality of the subspace for 'ridge' and 'lasso' is 18 and 30 respectively. for the first one it is controlled by the number of classes and for the second one it is determined by experiments. in table 1"
"therefore, overcoming the need to use anaesthetic drugs during imaging experiments has been recognised as an important research objective. apart from physically restraining the animal (eg. [cit] ), which can readily stress the subject (eg. [cit] ), a motion compensation approach is generally adopted (one exception is [cit] ). motion compensation refers to a general methodology whereby the subject's three-dimensional (3d) head motion is measured during the study and subsequently accounted for before or during image reconstruction. this has been demonstrated in spect of mice using a stereo-optical setup to determine the motion of retro-reflective markers glued to the head [cit] ."
"a traditional approach for improving the class separability is the linear discriminant analysis (lda) [cit], however if the features are not linearly separable lda usually fails. it is possible to use a more generalized approach called generalized discriminant analysis (gda) [cit] . let the input matrix x be composed from columns arranged following their class membership. the covariance matrix of the centers of the classes σφ (x) is:"
"marker size clearly plays a major role in motion tracking accuracy and, as expected, the smaller marker gave less accurate results in our experiments [cit] . this was particularly noticeable for the simulated rat motion (figure 15). we suspect this is due in part to vibration of the robot end-effector when performing many small movements at high acceleration, and that jitter noise for the smaller marker may have been exacerbated by this vibration. in spite of this, the small marker was a suitable size for rat head tracking and our results demonstrate that effective correction of rapid motion can be achieved when it is used. in future work we will try to optimise tracking accuracy for markers of this size through the use of filtering techniques (eg. [cit] )."
"cultural differences also influence the psrs since culture determines the social norms and values. soares and shoham [cit] as \"culturally patterned behaviors are thus distinct from the economic, political, legal, religious, linguistic, educational, technological and industrial environment in which people find themselves.\" hofstede [cit] defines culture as \"the collective programming of the mind which distinguishes the members of one human group from another.\" hofstede's four cultural dimensions, such as masculinity and femininity, uncertainty avoidance, power distance, and individualism and collectivism, are the outcomes from more than 100,000 ibm employees in 40 different countries [cit] ."
"in our previous work [cit], we present c space subdivisionbased parallel methods for graph-based randomized motion planning algorithms, particularly prms. we demonstrate that by subdividing the space and restricting the locality of connection attempts, scalable performance can be achieved. however, the regular subdivision method as presented is not well suited for rrt. here, we design a novel radial subdivision technique for parallelizing rrt."
"this work was supported by national ministry of education the methods for human identity authentication based on biometrics â€\" the physiological and behavioural characteristics of a person have been evolving continuously and seen significant improvement in performance and robustness over the last few years. however, most of the systems reported perform well in controlled operating scenarios, and their performance deteriorates significantly under real world operating conditions, and far from satisfactory in terms of robustness and accuracy, vulnerability to fraud and forgery, and use of acceptable and appropriate authentication protocols. to address some challenges, and the requirements of new and emerging applications, and for seamless diffusion of biometrics in society, there is a need for development of novel paradigms and protocols, and improved algorithms and authentication techniques. this book volume on â€oeadvanced biometric technologiesâ€ is dedicated to the work being pursued by researchers around the world in this area, and includes some of the recent findings and their applications to address the challenges and emerging requirements for biometric based identity authentication systems. the book consists of 18 chapters and is divided into four sections namely novel approaches, advanced algorithms, emerging applications and the multimodal fusion. the book was reviewed by editors dr. girija chetty and dr. jucheng yang we deeply appreciate the efforts of our guest editors: dr. norman poh, dr. loris nanni, dr. jianjiang feng, dr. dongsun park and dr. sook yoon, as well as a number of anonymous reviewers."
"as mentioned above, using the time domain characteristics as unique ecg features for personal identification has many significant drawbacks. an alternative approach is extraction of morphological features from ecg. these features could be extracted from a whole cardiac cycle in ecg, thus the need of full segmentation is eliminated. in this sense these features can be considered as holistic. they consist simultaneously amplitude and temporal characteristics of the ecg waves as well as their shape. in this section two approaches for holistic features extraction are described. the first is based on linear projections in subspaces: pca and linear discriminant analysis (lda). the second uses nonlinear versions of pca and lda: kernel principal component analysis (kpca) and generalized discriminant analysis (gda). a block diagram for an ecg personal identification system based on described features is shown in fig. 7 . fig. 7 . block diagram for ecg personal identification system based on features extracted using linear or nonlinear projections in subspaces"
"reconstructed slices of the hot rod phantom for the rat motion are shown in figure 15 . correction based on 8-point marker tracking resulted in excellent agreement with the motion-free reconstruction, 1.6 mm diameter rods being clearly resolved. correction based on 3-point marker tracking was noticeably inferior as only the 2.4 mm diameter rods were resolved; this gave results similar to those obtained for the fast manually applied motion ( figure 11) . figure 16 shows transverse reconstructed slices of the compartment phantom, before and after motion correction based on the 8-point marker, together with the motion-free reconstruction. as in the earlier experiments, the degrading effects of the rat motion and figure 13 . effect of motion rate on motion correction. motion correction of the hot rod study with slow-moderate manually applied motion. a 72 s segment of the data has been corrected so that it is comparable (in terms of counting statistics) with the fast motion study. left to right shows orthogonal views of the centre of the phantom. correction was based on the 8-point marker measurements. these images can be compared with those in figure 11 (row 3) in order to see the effect that the rate of motion had on motion correction accuracy. doi:10.1371/journal.pone.0021727.g013 the improvement after motion correction were evident. the less noisy appearance of the reference images is due mainly to the increased counting statistics in this study."
"input: a set of points s, a query q output: a map of closest point to q and its distance m 1: m ← findneighbors(s, q, 1) 2: return m 6384 nodes, 217 tb of memory, and a peak performance of 1.288 peta-flops. each node consists 12 processor cores. this architectural layout influenced our choice of processor counts to be in multiple of 12. our code was written in c++ and compiled with gcc-4.5.2 on the linux cluster and gcc-4.6.3 on the cray xe6 machine. using stapl, the same c++ code was used on both architecture types."
") is a mercer kernel, sr can be extended in kernel mode. algorithmically sr is performed as follows [cit] : 1. construct the weights matrix w:"
"here k is input planes' number (as well as convolutional kernels), r and c are convolutional kernel's height and width, w l,p,k r,c is synaptic weight with coordinates (r, c) in the convolutional kernel between k-plane of the (l − 1)-layer and p-plane of the l-layer, b l,p is neurons' bias of the p-plane and l-layer. the cnn uses a sparse structure instead of a fully-connected one; also its number of layers is decreased. in order to increase the neural network's processing speed, convolution and subsampling operations are performed in each plane simultaneously [cit] fig. 2."
we leverage cdt [cit] to explicate the discrepancy between consumers' attitudes toward psrs and the actual purchase and use behaviors of iot products and services.
"the advent of iot leads to a change in the use of information types. consumers face more serious psrs because of the new information types that can be easily breached and manipulated by vendors. however, many consumers have limited information about iot. even consumers who have enough information about iot rarely take action to protect personal information because of the cognitive gap."
"by comparison, the approach described here is non-invasive, and can utilise conventional micropet scanners with high detection efficiency for improved signal-to-noise. it would also appear to be more readily scalable to mice than the ratcap [cit] . overall the results presented indicate the importance of optimised motion tracking for quantitatively accurate motion-corrected pet imaging of awake animals, and that motion tracking parameters for effective motion correction in preclinical brain imaging of awake rats are achievable in the laboratory setting. this could broaden the scope of animal experiments currently possible with pet."
"when people exhibit a conflict between their attitudes and their behaviors, this result is cognitive dissonance. this cognitive dissonance leads to a feeling of mental discomfort. one of the most popular demonstrations of cdt is the \"smoking test\": a person acquires knowledge about smoking from the media, friends, acquaintances, and physicians. the knowledge that smoking is bad is dissonant with the cognition that he or she keeps smoking [cit] ."
"where x is input image's sub-window, s w and s b -whole rectangle's and its black part's weights accordingly, sum w and sum b -whole rectangle's and its black part's sums of pixels. a weak classifier's output value is:"
"to investigate the impact of the rate of motion and marker size on tracking accuracy, arbitrary movements of differing rates were manually applied to a micro deluxe hot rod phantom (data spectrum corporation, nc, usa). the phantom had an internal diameter of 40 mm and an insert comprising rods with internal diameters 4.2, 4.0, 3.2, 2.4, 1.6 and 1.2 mm."
"as a result of iot privacy and security problems, there is a great demand for mechanisms to protect iot privacy and security. for the iot security markets, gartner predicts that, \"worldwide iot security spend will increase from $912m [cit], soaring to $3.1b [cit], attaining a 27.87% cagr in the forecast period\" [cit] ."
"support vector machine is a supervised learning algorithm used for classification in two classes. the aim of svm is to find a n-dimensional hyperplane that optimally separates the data. optimally in this case means that the margin, between nearest data points and the hyperplane, will be maximized. unfortunately in real problems data is rarely separable by a hyperplane but can be separated by a non-linear surface. svm can be transformed to a non-linear classifier by applying the kernel trick [cit] . this way data is mapped implicitly in a higher dimensional space where it can be separated by a hyperplane."
"laboratory animals undergoing in-vivo brain imaging procedures are normally anaesthetised to eliminate both stress and movement. in some countries the use of anaesthesia to minimise stress is mandatory. however, there are two important drawbacks of sedation which limit the potential of pet. firstly, the literature contains numerous examples of anaesthetic drugs affecting physiological measurements in the brain (e.g. auditory response [cit], radioligand binding [cit], glucose metabolism [cit], cerebral blood flow [cit], motor-evoked potentials [cit], neural activity [cit], neuro-hemodynamic coupling [cit] and neurotransmitter flux [cit] ). in each case, the signal of interest was either masked, inhibited, or exaggerated with respect to anaesthetic-free controls. secondly, use of anaesthesia prevents investigators from performing imaging studies on freely moving animals during normal and evoked behaviours, meaning that at present many rich experimental paradigms to elucidate the neurological response to external stimuli in animals cannot be exploited. these are significant limitations given that pet is currently the only non-invasive method to study specific biological correlates of behaviour (i.e. neurochemical or receptor changes)."
"in the awake animal, it is also recognised that, although the head is treated as a rigid body, not all parts of the head move in the same way -for example, the lower jaw and neck. for events originating from these regions, only an approximate correction can be obtained using the motion data. the resultant mispositioning of these events does not appear to significantly contaminate the brain signal."
"algorithm 4 describes the approach in the context of a distributed rrt. to compute the nearest point q near to a query point q rand, each processing element sends q rand to the other processing elements by calling m apreduce(). the mapping function (algorithm 5) receives the query point q rand and locally computes its nearest neighbor in its local portion of the tree (t p ) based on a given distance metric. the reduce function (algorithm 6) takes the two inputs returned by the mapping function and computes the nearest neighbor to q rand from the two inputs based on the same distance metric."
two types of phantom experiment were performed to investigate the ability of this motion tracking and correction methodology to correct for arbitrary continuous motion during pet data acquisition. in all phantom experiments both the 3-point and 8-point markers were fixed firmly to the end of the respective phantom (facing the tracker) for pose tracking.
"a minimum of 3 points are required to establish rigid pose. microntracker target points (known as ''xpoints'') correspond to the intersection of the surrounding black/white target regions ( figure 3 ) and can only be detected if the projection footprint of the target region on the ccd sensor exceeds a manufacturer-defined threshold. larger target regions enable a marker to be detected over a greater angular range. the specified positional accuracy for an xpoint is 0.25 mm rms. the 8-point marker used in this study had a greater range of angular detection than the 3-point marker (by virtue of its larger target regions), greater redundancy (due to the greater number of target points), and better overall accuracy (due to the target points being further apart and more symmetrically placed) [cit] ."
pca is a statistical technique for dimensionality reduction of high dimensional correlated data down to a very few coefficients called principal components [cit] . this technique is optimal in terms of retaining as small as possible the mean squared error (mse) between the original signal and the restored signal form reduced set of principal components. let the ecg signal is automatically segmented into pqrst complexes. these complexes are aligned and arranged as rows in a matrix. in general they differ in their length. the number of columns of the matrix is chosen larger than the maximal expected length of the pqrst complexes. the pqrst complexes are arranged in the matrix in such way the r peaks are in the middle of rows (fig. 8) . the elements in each row before and after the copied pqrst complex are set as same as the first and last sample of the complex respectively.
"where t -weak classifiers' number, η t -t-weak classifier's weight. the adaboost algorithm [cit] ) is used for training of the cascade of weak classifiers and the selection of the most important haar-like features. the second level uses the convolutional neural network [cit] which is more robust to variations of the input image, compared to other known classifiers. the output value of a neuron with bipolar sigmoid transfer function and with the coordinates (m, n) of p-plane and l-layer is [cit] :"
"in this case, consumers will face limitations in various communications or online social relationships, and could isolate themselves, although their risks will be minimized. using the equation in figure 9, we can conclude which power is stronger. if the result is greater than 1, the disclosure power is higher than the control power, and if it is less than 1, the disclosure power is higher than the control power. using the psr scores, consumers will be aware of their current vulnerabilities, manage their personal psrs, and minimize their privacy cognitive gaps. figure 10 is a simulated outcome with four new personal information types, three weight impact factors for the information types, and three personal capabilities to control psrs. this spider map shows all the scores of the ten dimensions; consumers can easily recognize their weak parts and strong parts. this visualization will also display the result of the psr control balance: a warning phrase and graphical signals."
"however, this study has several limitations. first, although the psr scores help consumers to increase their awareness of psrs, the direct influence of the cognitive gap between the attitude and actual behavior is not easily measured since we should measure the change of the consumers' purchasing behaviors. second, psr scores can be subjective until we have sufficient psr score data to compare individuals to populations. third, the weight for information types and cultural differences can be changed, since the individuals' personalities and experiences can be altered."
"recently, regional neurochemical changes in the brain, temporally correlated with behavioural changes, were demonstrated in awake, unrestrained rats using a miniature, headmounted pet tomograph (ratcap) secured to the animal's head and counter-balanced [cit] . in spite of the limitations of the counterbalance mechanism, the ratcap allows the animal to be relatively unrestrained -a situation which is challenging to reproduce in a conventional scanner. however, it requires a surgical procedure and acclimatisation of the animal to the apparatus. further, due to size and weight restrictions the detection efficiency of the ratcap is relatively low compared with a conventional animal pet system, resulting in poor signalto-noise ratio [cit] ."
"synchronisation of the tracker and scanner data streams was performed by inserting a data tag into the pet list mode stream at the time of each pose measurement. as shown in figure 4, the microntracker was externally triggered via the gpio interface at a frequency f by pulse waves from a signal generator. the falling edge of each trigger pulse initiated both the exposure of a tracker stereo image frame for a shutter time t e ms and, simultaneously, a strobe output pulse of duration t s ms. the strobe output was connected to the gating input of the scanner to trigger the insertion of tags in the pet list mode data. stereo images acquired by the tracker were transferred to a pc and processed to extract marker pose parameters using software provided by the manufacturer. after acquisition, inserted tags were associated with tracker pose measurements in two stages: pose-tag matching and temporal alignment."
"the psr scores consist of three major parts and ten dimensions in detail. iot information risk types have four dimensions; monetary, security, identification, and manipulation risks. weight impact factors, composed of volume, cultures, and prior experiences, play a role as a positive moderator between iot information risk types and personal psr scores. personal capabilities, such as technical knowledge, privacy literacy, and self-efficacy for iot psrs, negatively moderate between iot information risk types and personal psr scores."
"c-space subdivision. c space subdivision has been very useful in solving sequential motion planning problems. early work in c space subdivision computes the exact representation of c space by uniformly dividing it into cells [cit] . each cell is then classified as empty, f ull, or mixed depending on the obstacle position in the cell. an a * search algorithm is then used to compute a path through the purely empty or mixed cells."
profiles are shown in figure 12 for the 20 min and 5 min studies. these represent the summed activity of 5 rows for 5 central transverse slices. the level at which profiles were chosen is indicated in figures 10 and 11. good agreement between the motion-corrected and reference profile was obtained irrespective of which marker was used for tracking but the 8-point marker gave better contrast (larger peak-trough distances) in general. in all cases the uncorrected profile was severely blurred.
"to obtain head motion data the animal was positioned in a pvc tube (60 mm diameter) inside the micropet with its head protruding from the end. the tube was supported by the scanner bed which was rigidly mounted at the rear of the scanner so that the tracker could occupy the usual bed support (see materials & methods section 3). tracking followed 10 days, 20 min/day, of acclimatisation to the scanner environment and tube. a 3-point marker was glued to the rat's forehead and motion data were collected at 30 hz for a period of 40 min using the microntracker."
"in fig. 9 is given an example of features distribution of five individuals using two different techniques -pca and kpca. as can be seen the extracted features using pca aren't linearly separable. despite the complications of the process, the results from kpca are much better."
"to further understand the performance of radial subdivision in a different scenario, we evaluated the radial subdivision algorithm in a grid environment with rigid body robot on cray xe6 machine. in this evaluation, we kept the number of regions constant at 480 across all processor count and varied the sample size per region. the results from the evaluation are shown in figure 8 . given different input sizes, we saw decrease in execution time as the number of processors increases."
we investigate the scalability of the two algorithms presented in section iii: bulk synchronous distributed rrt and radial subdivision distributed rrt. we also examine the effect of robot complexity and machine architecture.
"the rat head motion data were obtained in accordance with a protocol approved by the animal ethics committee of the university of sydney (protocol number: k00/12 [cit] /2/4891). a single male sprague dawley rat, 14 wks old, was group housed in a plexiglas cage, two animals per cage. it was maintained in ambient temperature (22-24uc) on a 12:12 light:dark cycle. food and water were provided ad libitum."
"results from the rat study demonstrate the feasibility of using this motion tracking and correction approach in the intended application. however in the present work we have based conclusions regarding quantitative accuracy on phantom studies rather than live subjects in order to have a directly comparable motion-free reference scan. anaesthetic and tracer washout effects, which alter the tracer distribution, prevented use of the same animal as a gold standard."
"prior studies found that previous experiences play a significant role as a moderator between optimistic biases and risk estimates at both a personal level and a social level [cit] . according to ibm research, 28% of consumers have a data breach experience [cit] . \"users who have never experienced a privacy breach are more trusting and link easily with reciprocating users. however, after experiencing a privacy breach, users become aware of the privacy risks on sns and use the permeability rules to more cautiously share information\" [cit] ."
"as can be seen the results using holistic features extracted with linear projections in subspaces are relatively poor. the gda outperforms all approaches but the significant disadvantage of this method is the computation complexity. in addition the maximal dimensionality of the features is limited up to the number of identified individuals minus one. for combining with facial biometric modality we select kpca approach for feature extraction. despite its lower performance we prefer it because there is an algorithm, called greedykpca, in which the kernel matrix does not have to be stored."
"as an evaluation of our dsr approach, testable hypotheses of isdt are used to evaluate whether mds satisfy mrs [cit] . in this study, we use an experimental design to test three hypotheses for the evaluation of our mds. we will measure the way in which the change of consumers' awareness toward psrs is improved before and after having personal psr scores. to measure if consumers' cognitive gaps are reduced, we will provide three statements. the psr scores are calculated by a consumer survey. table 3 shows the scales of the ten dimensions for the consumer survey. all questionnaires and demographic questions are available upon request."
"previously we have applied these methods to correct for step-wise motion in phantom studies performed on a small animal pet scanner [cit] . our goal, however, is to enable correction of continuous motion, which is both rapid and arbitrary, as is expected when imaging conscious animals. due to the frequent and rapid motion exhibited by alert, awake small animals, optimal motion correction requires frequent sampling of the subject's pose and precise synchronisation of the pose measurements with events in the pet coincidence data stream. inadequacies in either of these areas are expected to reduce the qualitative and quantitative accuracy of image-based measurements. in addition, to avoid degrading the excellent spatial resolution provided by state of the art small animal pet scanners, the motion measurements themselves should be as accurate as possible."
"in this paper, we present two parallel algorithms for rrt computation. the first algorithm extends existing work by introducing a parameter m that controls how much local computation is done before a global update across processors. the second algorithm radially subdivides c space into regions and lets each processor build part of the tree in each region. by controlling local computation and subdividing c space, we minimize the overhead associated with inter-processor communication in parallel processing. we present results for both a rigid body robot and an articulated linkages on two different parallel machine architectures."
"results from phantom experiments demonstrate that the hardware-based synchronisation scheme enables robust correction of rapid and continuous arbitrary rigid-body motion, including realistic rat head motion. our data support the conclusion that synchronisation error and sampling rate are critical parameters to optimise for a motion tracking system aimed at compensating for a typical rat head motion pattern. a pose sampling rate in excess of 20 hz and synchronisation accuracy within 100 ms appear necessary to achieve quantitatively acceptable results (,5 % error) in awake rat studies. we have also demonstrated dependencies of motion tracking accuracy on the marker size and object speed. although we show that a large marker about 60 mm across provides greater tracking accuracy than a smaller one, effective motion compensation can still be achieved using a miniature marker suitable for attachment to a rat's head. in practice, the accuracy of tracking will also depend to some extent on the velocity of motion, as demonstrated by our noise and motion analysis results."
"4.1. pose-tag matching. triggering was stopped just before the end of the micropet scan to enable the last tracker measurement to be reliably identified. this was because not all pulses reaching the micropet gating input caused tags to be inserted in the list mode data. it is not clear why this occurred, but we observed that the majority of dropped tags occurred within the first few seconds of an acquisition. dropped tags were identified and accounted for using the time intervals between successive tags based on the regular 1 ms time marks in the list mode data. it was noted that although dropped tags occurred in most scans, in approximately 50 trials they represented a negligible proportion (,0.05 %) of the study time in all cases."
"as the next steps, first, we can validate and apply the proposed psr-cbt to other fields to generalize the theory. although psr-cbt is applied to iot settings in this study, psr-cbt can be applied to other privacy and security issues when consumers make a decision. for example, when a person posts a sensitive picture on facebook, there may be a conflict between disclosure power and control power in his or her mind. second, we can identify the distinction between privacy and security in a future study because privacy and security may have different influence mechanisms on consumers' decisions. third, future studies can carry out to identify consumers' trust issues toward iot companies. fourth, although we presented three reasons for privacy and security decision making, we did not consider cognitive biases in this study. we thus suggest further research on the relationship between cognitive biases and the privacy calculus model [cit] ."
"we have explored factors affecting motion tracking accuracy, which in turn could affect downstream motion compensation and reconstruction accuracy in pet studies of awake rats."
"the operation open(x, g) smoothers the convex peaks of the signal, while close(x, g) smoothers the concave peaks. the structure of morphological filter for qrs complexes exaggeration is according to the following relation:"
"one internal cylinder and the outer compartment of the 2-compartment phantom were filled with a 3:1 ratio of known concentrations (given below) of 18 f in water, respectively, and the remaining internal cylinder was filled with water. the hot rod phantom was filled with 20 mbq of 18 f. each phantom was attached to the robot end-effector in turn using a custom-made perspex tm adaptor and scanned for 12 min, during 11 min of which it underwent the pre-programmed rat movements. the phantom was stationary for the initial and final 30 s of the scan. each phantom was scanned twice, once using 30 hz pose sampling and once using 48 hz. a 20 min photon transmission scan for attenuation correction and a 30 min reference (motionfree) emission scan were also collected for each phantom. [cit] ). motion correction and reconstruction were performed similarly to the manual motion experiments described above except that the 3drp algorithm was used to reconstruct the compartment phantom data for the bias analysis, described below."
"sampling-based motion planning. the motion planning problem is to find a valid path (e.g., collision-free and satisfying any joint limit and/or loop closure constraints) for a movable object starting from a specified initial configuration to a goal configuration in an environment [cit] . a single configuration is specified in terms of the movable object's d independent parameters, or degrees of freedom (dof). the set of all possible configurations (both feasible and infeasible) is configuration space (c space ). c space is partitioned into two sets: c f ree (feasible) and c obstacle (infeasible). motion planning then becomes finding a continuous sequence of points in c f ree that connects the start and the goal."
"acquisti and grossklags [cit] suggested three challenges in privacy-related decision making. consumers have (1) incomplete information to make psr disclosures, (2) lack of ability to process their information, and (3) various cognitive biases. we will focus on the problem of having incomplete information and infeasible processes. in particular, for new iot technology, few consumers understand the vulnerability of iot and the possibility of data manipulation by vendors. if consumers had more knowledge of iot privacy and security issues, they would probably be more conservative in purchasing and using iot devices. the proposed psr assessment framework will contribute to increasing consumers' awareness about the iot devices they use, how significant the risks are, and how far they can control their information. even if consumers had complete information, they would not be able to perfectly process their detailed information because this information is usually very complex [cit] . the proposed psr assessment framework will also contribute to improving consumers' ability to understand the details of iot privacy and security issues by visualizing consumers' use of different data types and their privacy control capabilities [cit] . last, cognitive biases significantly influence consumers' decision making with psrs, but we consider this factor to be beyond the scope of the current study and leave it for future research."
"mds are a set of design elements aiming to meet the mrs [cit] . this study's mds consist of five design elements: (1) a general process design for psr assessments, (2) the process design of personal psr scores, (3) the design of ten dimensions (personal information types, weight impact factors, and personal capabilities) for psr scores, (4) a new model of psr scores, and (5) a new design theory for psr disclosures."
"in order to explore the impact of pose sampling rate and system synchronisation error on the motion-corrected images, the 48 hz motion data were manipulated in two ways: (i) down-sampled (by neglecting intermediate pose measurements) to simulate sampling rates of 48, 24, 16, 12, 9.6, 4, 2, 1, 0.5 and 0.25 hz and (ii) time- shifted by varying amounts to simulate constant synchronisation errors of 60.1, 60.2, 60.3, 60.4, 60.5, 60.6, 60.7, 60.85, 61 and 62 s. in each case the motion data were used for motion correction, including attenuation and scatter correction, and the bias for the hot cylinder in the compartment phantom was computed as"
"here, the subscripts m and r refer to the motion-corrected and reference concentrations, respectively. concentrations were computed from regions drawn in the middle third of the phantom compartments and the reference concentration was obtained from the motion-free reconstruction."
"the purpose of the fd is to locate a human face in a scene and extract it as a single image. in this work, we propose a combination of two classifiers for rapid and accurate fd. the first one is faster but less precise, while the second, compensates for the imprecision of the first classifier. the second stage of the proposed framework, namely sp, is used for dimensionality reduction of the detected facial images, when represented as vectors in high-dimensional euclidean space. thus, it is necessary to transform them from the original high-dimensional space to a low dimensional one for alleviating the curse of dimensionality. the sp is based on principal component analysis (pca) and spectral regression (sr) algorithms. the pca discovers the subspace which contains all vectors of facial images and we use it mainly to remove noises. pca also preserves euclidean distances between vector pairs in the subspace. based on this, further dimensionality reduction is done by using the sr algorithm. this algorithm is robust with respect to the variation of lightning conditions and human expressions. finally we perform classification using support vector machines classifier in the subspace. in the following, the three stages of the proposed fr framework we will be discussed in details."
"in figure 5, pose n+1, measured at time t 5, is a better temporal match with the list mode data segment between tag n and tag n+1 than pose n, measured at time t 1 . therefore, for the segment of list mode data defined by the tags tag n and tag n+1, pose n+1 was used as the best transformation to apply."
"in real case scenario, human faces often appear in scenes with complex background, rather than as a single object. in addition, they have varying appearance due to different lightning conditions, changes in pose, human expressions etc. thus, a reliable system for fr must be robust to noise, variations and be able to work in real time. to meet these requirements we are proposing a framework for recognition of facial images depicted on fig. 1 . this framework consists of three stages, namely face detection (fd), subspace projection (sp) and classification."
"to reduce consumers' cognitive gaps and improve the awareness of psrs, we propose a new mechanism for assessing iot psrs, namely personal psr scores. we determine the psr scores by collectively considering consumers' iot information types, weight impact factors, and personal capabilities. furthermore, we will propose a new design theory for personal psr assessment that can be used to explain how consumers internally make a disclosure decision [cit] . because a design science research (dsr) concentrates on developing solutions to problems [cit], a dsr approach is suitable for the development of our proposed psr scores and theory. we adopt the information systems design theory (isdt) because isdt allows us to apply a set of requirements and designs for a solution. in addition, we adopt the publication schema for dsr [cit], which guides us to communicate with prior literature."
"the first md is a general psr assessment process. before designing the psr scores in detail, we need to have a general framework for evaluating personal psrs with iot. first, we start with an information collection process to establish the types of psrs. because the concept of iot may be too broad, we focus on iot technology which collects data from various wireless sensors such as amazon echo tm and google home tm . second, we identify possible privacy risks and security vulnerabilities based on the collected information. third, we assess the identified psrs. however, existing risk assessment developed for social networks might not be applicable to iot. to determine the level of psrs, we should find out the likelihood and impact of the identified psrs [cit] . thus, we determine the likelihood and impact of potential risks. based on the likelihood and impact, we categorize information types for psrs. last, we provide a personal privacy and security evaluation result, which is then used to page 5079 develop personal psr scores."
"ipt originates from the cognitive process theory, which deals with humans' cognitive memories that consist of sensory memory, short-term memory, and long-term memory [cit] . the information in sensory memory is usually unconscious and only lasts for up to three seconds. short-term memory is also known as working memory. the information in sensory memory transfers to short-term memory and lasts for 15-20 seconds in short-term memory before transferring to long-term memory. the amount of an individual's cognitive load, the number of repeats, and individuals' selective processing capability collectively influence how information is processed in the short-term memory. although long-term memory has much space, it relies on the quality of the organization of the memory, and thus, people cannot usually remember all the information in their long-term memory [cit] ."
"this dsr study of personal psr scores in the iot settings contributes to minimizing the cognitive gap that explicates consumers' paradoxical behaviors and increasing the awareness of psrs. we followed two dsr methodologies, including isdt and publication schema for dsr to create the proposed artifact of psr scores based on cdt and ipt."
"where t i represents the i-th commanded pose streamed to the robot, t x is a transformation matrix to adjust the location of the end-effector, and t 0 and t 0' represent the pose of the robot at calibration and at the start of the experiment, respectively, both of which enabled the required movements to be performed from the arbitrary robot starting pose t 0' . the 3-point marker used to measure the original head motion was a few millimetres from the brain; to apply similar motion to the middle third of the phantom it was necessary to shift the apparent end-effector location by 45 mm towards the centre of the phantom using t x (see figure 6 ). the first 20,000 poses (representing 11 min of rat motion) were streamed to the robot motion controller in remote control mode using a tcp/ip connection established by third-party software (visual basic, microsoft corp., usa). the maximum angular speed and angular acceleration parameters for the robot endeffector were 750 deg.s 21 and 4450 deg.s"
"using the adjacency information provided by the region graph, we make connection attempts between each region branch and its adjacent neighbors. we check if any edge connection at this point creates a cycle. if a cycle exists, we prune the tree so as to remove any cycles. in the results presented here, tree pruning is performed by running a graph search algorithm. figure 3 shows a simple pictorial illustration for tree pruning."
"object speed, although not controllable, is an important factor dictating motion tracking requirements. in our experiments the maximum linear speed of the marker origin was 0.5 m.s 21 (for the fast manually applied motion). given the small angular motions in this case (,13u), 0.5 m.s 21 is a reasonable estimate of how quickly individual voxels could be tracked and corrected using this tracking system with 30 hz sampling. an analysis of the speed of voxels of interest in live subjects is likely to shed more light on speed-related motion tracking requirements for this application and will be the subject of future work."
"although there are privacy risk evaluation studies, most previous privacy scoring models focus on the context of social media, rather than the context of iot technology. prior literature regarding privacy scoring models has not considered the vulnerabilities of iot and the new information types that users normally do not encounter in social media. iot broadly collects consumers' activity data, such as purchasing habits, emotions, real-time location data, and schedules, all of which put the users at psrs from inappropriate manipulation and secondary use by vendors [cit] ."
"the hot rod phantom was filled with 17 mbq 18 f in solution and two separate emission scans were performed on the pet scanner: a 20-min scan during which the phantom was moved steadily and continuously by hand (with the exception of six evenly spaced 1 min intervals when it was kept stationary to simulate periods of relative inactivity of a subject), and a 5-min scan during which the phantom was moved rapidly and continuously by hand. in both cases the movement involved six degrees-of-freedom (dof) and was done in a roughly oscillatory manner. we attempted to keep the range of motion in each experiment similar. motion data were collected at 30 hz. finally, a 20 min photon transmission scan and a 20 min emission scan were collected, both on the stationary phantom. these provided the necessary data for photon attenuation correction and a motion-free reference, respectively. transmission data were collected in singles mode using a rotating 57 co point source and 110-135 kev energy window."
"in this section, we review previous literature to demonstrate consumers' paradoxical behaviors between their attitudes toward psrs and their actual iot purchase and use behaviors. the objective is to analyze existing psrs in the literature and identify new types of iot psrs that pose a threat to consumers. we then propose a new artifact as a solution to minimize the cognitive gap and increase consumers' awareness of psrs."
"we choose the cognitive dissonance theory (cdt) as our major kernel theory. cdt considers a privacy paradox as a cognitive gap between attitudes page 5077 and actual behaviors. is scholars have already learned that the increase of psr awareness reduces consumers' paradoxical behaviors [cit] . to improve psr awareness, we adopt the information processing theory (ipt) that can explicate the relationship between information types and individuals' processing abilities toward the information. furthermore, we propose the privacy and security risk control balance theory (psr-cbt) based on the control balance theory (cbt) that can be used to explain consumers' internal power conflicts when disclosing their personal information. table 1 shows the components of the designed psr assessment framework following isdt. mrs are a set of goals for an artifact design [cit] . this study proposes three mrs. first, we aim to develop a taxonomy of personal information types related to iot psrs using an inductive approach. second, we design personal psr scores, representing the level of perceived iot psrs, based on the two kernel theories, cdt and ipt. last, we propose a new design theory based on cbt."
"feasibility of our motion tracking and correction approach was tested in a live animal study. the animal described in materials & methods section 6.2 was injected with approximately 80 mbq of 18 f-fdg via the tail vein while under isoflurane/o 2 gas anaesthesia (1.5%). following a 20-min uptake period the head was scanned in the micropet for 20 min, during which the head pose was sampled at 30 hz by the microntracker. motion correction was applied as described in materials & methods section 5. the corrected data were then reconstructed using osem. attenuation and scatter corrections, based on a calculated attenuation map derived from this reconstruction, were included."
"the last component of isdt includes testable hypotheses related to the designed artifact [cit] . to evaluate the performance of the proposed artifact, psr scores, our study provides three evaluation approaches, including evaluation with visualizations, consumers' surveys, and experimental designs for psr scores."
"this study follows md1, which includes data collection, identification of psrs and vulnerabilities, assessment of psrs with the likelihood and impact (see table 2 ). for steps 4, 5, and 6 of md1, we suggest a matrix of personal psr levels. the risk levels are strongly associated with the risk likelihood and impact [cit] . when both factors are high, psr is the highest. figure 1 presents the matrix of different psr levels, which influence the design of information types and weight impacts for md2-md4. to apply the last step of md1 to psr scores, we propose the process of psr scores as md2."
"this md defines the calculation of personal psr scores that collectively consider four information types, three weight impact factors, and three personal capabilities. the proposed psr scores show the level of balance between two powers: personal information disclosure power and personal information control power. the personal information disclosure power is calculated as the product of the four information types and the corresponding weight impact factors. the personal information control power is measured based on three personal capabilities. the weight impact factors positively moderate the relationship between the information risk type and the psr scores. volume, culture, and personal breach experiences are the elements of the weight impact factors. volume is an important element used to determine the weight of the impact of information types on psr since the impact of risks will fluctuate according to the number of iot devices, the number of friends on social media accounts, the usage of cloud services, and the scale of disclosure [cit] ."
"the or paradigm [cit] was applied to parallelizing rrt computations on shared-memory machines where the computation is replicated on each process [cit] . processes concurrently explore c space and the first process to find a solution sends a termination message to other processes. their work also explored concurrently and cooperatively building a single tree under a shared-memory model. each process executes their own program and communicates to other processes by exchanging data through the shared memory in a concurrent read exclusive write (crew ) fashion. in addition, they study a hybrid algorithm combining the or paradigm and the crew model. the processes are divided into groups and each group cooperatively build its own tree. the first group to find a solution sends a termination message to the others."
"we also present a novel radial c space subdivision for parallelization especially suited for rrts. starting from the root q root, we subdivide c space into conical regions and build part of the tree (subtrees) in each region. these subtrees are later connected in a manner such that no cycle exists after region connection. we exploit locality by only attempting to connect branches that reside in neighboring regions. figure 2 shows an example for a two dimensional c space . each process builds a branch (shown in different colors) starting at the root that is biased toward their region of c space . fig. 2 . example of radial subdivision for a 2d cspace. each process concurrently builds a subtree (using sequential rrt) rooted at qr and biased toward a target q i (e.g., qn for the black process)."
"compared to the general knowledge of iot technology, privacy literacy focuses on privacy and security. prior scholars have suggested the concept of privacy literacy and defined it in various ways [cit] . nevertheless, leading scholars have emphasized the application of skills online as well as the knowledge of privacy: \"online privacy literacy may be defined as a combination of factual or declarative (\"knowing that\") and procedural (\"knowing how\") knowledge about online privacy\" [cit] . thus, we define privacy literacy as the ability to collect personal information and apply practical skills online for personal data protection and privacy regulation."
"6.2. rat head motion. head movements recorded in an awake rat over a period of 11 min were applied to the hot rod phantom by a six dof robot manipulator (epson c3-a601s 6-axis, seiko corp., japan). repeatability of the robot was 620 mm. the study was repeated using a compartment phantom suited to quantitative analyses. it consisted of a main cylindrical compartment with internal diameter 40 mm, and two cylindrical compartments of internal diameter 12 mm within the main compartment. the aims of the robot experiments were to test the feasibility of obtaining accurate motion-corrected images in the presence of realistic rat head motion, and to determine the impact of varying the pose sampling rate and synchronisation accuracy."
in this work an approach for personal identification based on biometric modality fusion was presented. the presented combination of classifiers is characterized by its high accuracy and it is particularly suitable for precise biometric identification in intelligent video surveillance systems.
"iot technology not only collects a massive amount of consumers' information, but it is also capable of understanding and predicting their behaviors. iot devices are inherently small and cheap with limited privacy and security protection functions because strong protection systems in hardware and software cannot be embedded in a small and cheap device [cit] . thus, the privacy and security risks (psr) of using iot technology are much more significant than those of conventional electronic home devices. consequently, iot experts have warned consumers about the privacy and security vulnerabilities of iot. despite the vulnerability of iot devices and services, it seems that in some cases, iot psrs do not appear to have an influence on the consumers' intention to purchase and use iot. this is due to consumers' cognitive gaps and lack of awareness of privacy risks and security vulnerabilities related to iot [cit] ."
the list mode data for the motion scans were corrected for motion using the lor rebinning software and sorted into normalised 3d sinograms before reconstructing them using osem as described above. both motion scans were corrected to a common reference position which was in alignment with the transmission scan (to be used for attenuation correction) and the reference (motion-free) scan.
"these building blocks include a collection of parallel algorithms (palgorithms), parallel and distributed containers (pcontainers), a general mechanism to access the data of a pcontainer similar to stl iterators called pviews, an abstraction of the computation task graph (paragraph), and an adaptive runtime system (armi) that includes a communication library, scheduler, and performance monitor."
"third, people can diminish the importance of cognition, such as attitudes and beliefs for their cognitive consonance. people try to reduce cognitive dissonance by making an excuse for their behaviors, as mentioned in the example of smokers, who despite knowing the fact that smoking is bad for health [cit] . for iot products, consumers might convince themselves that, even if their personal information is leaked, it will not be that harmful because \"they have nothing to hide.\" this, however, may lead to potentially serious consequences for everyday consumers, such as identity theft, harm to credit scores, and the leak of embarrassing photos or health conditions. the proposed psr assessment framework can help consumers switch from a cognitive dissonance condition to a cognitive consonance condition by visualizing the information types shared with iot, and how well they are able to protect their personal information. the improvement process of consumers' awareness of psrs can be explained by ipt."
the proposed psr-cbt contributes to consumers' understanding of their behaviors toward psr disclosures by addressing the individuals' two internal powers. future studies can develop the concept of psr-cbt and the psr scores as a general index that can be practically applied to consumers all around the world.
"in order to assess privacy risks, prior literature classified consumer-disclosed information into six information types: (1) demographic information, (2) contact information, (3) vehicle information, (4) lifestyle, interests, and activities data, (5) financial and economic data, such as estimated income and home value, and (6) financial and credit data, such as credit score, loan, and credit card data [cit] . however, the categorization fails to capture new types of data collected and transmitted by iot devices, such as consumers' behavioral tendencies, real-time locations, and schedules."
"the approach we have developed for rats undergoing pet brain scans also uses a stereo-optical setup for rigid-body motion tracking. correction is performed using a strategy that was developed originally for human pet scanning [cit] . in this method, each line of response (lor) representing a detected coincidence is spatially transformed according to the measured motion prior to being reconstructed [cit] (see figure 1 )."
"where x c,k is the k-th observation from the class c. the goal in gda is maximization of between-class variance and minimization of within-class variance. this is done by the following eigendecomposition:"
"an interesting result is found: since the maximum torque for the linear reactive controller was about 500 knm, the same value has been imposed on the constrained optimal control and though the power production decreases with respect to the optimal unconstrained case, it is higher than the one extracted with the linear reactive controller."
"where represents the inertia of the gyroscopic system with respect to the -and -axes and with respect to theaxis. the three torques are given to the gyroscopic system, respectively, by the pto (1), the flywheel motor (2), and the hull (3). the torques given by the latter two equations have a key role in the system behavior: their projection on 4"
"the angle between the harmonics components of the spectrum can either be chosen as random phase or can be guided by a groupiness factor [cit] or, in case of wave data acquisitions, may be the phase angle given by the fast fourier transform (fft) analysis of the time series. the approach above described is referred to as linear stochastic wave load model [cit] . figure 3 and table 1 ). [cit] . among the acquired data, a set of nine 20-minute-long waves has been chosen as representative of the site as shown in table 2 ."
"however, these considerations apply to devices for which the control force is directly applied on the floater main degree of freedom, so that this could be locked or released at the desired time instant. the wave energy converter considered in this paper is not suitable for the implementation of this strategy, since in such a device it is not possible to lock/release the relative motion between floater and gyro at a desired time instant."
"in this section, we consider the design of state-feedback gains, based directly on measured data which is perturbed by a disturbance satisfying assumption 3. first, we derive a data-driven characterization of the uncertain closed loop, using a single open-loop data trajectory. thereafter, we apply known robust control methods to this parametrization in order to design state-feedback controllers which guarantee stability and performance for all closed-loop matrices that are consistent with the measured data. finally, we extend the proposed framework to systems with mixed data-driven and model-based components."
"in this section, a review of the existing control algorithms for wave energy converters is given, so that the reader can have an overview of the state of the art in this field. in most cases, when analyzing the power extraction capabilities of a wec, a one degree of freedom system is analyzed. as described in section 3 of the paper, in the simplest case the hydrodynamic model of the device may be approximated by a 2nd order linear differential equation whose coefficients are frequency dependent. in the following considerations such a simple model may be a good reference for a reader that does not have a deep knowledge of this field."
"by means of the wiener-khinchin theorem which relates the fourier transform of the autocorrelation function of a stationary random process to its double-sided autospectral density function, we get the following:̇("
"it is initially considered that the pto can give any torque to the shaft. in these conditions, the behavior of the system is represented in figures 7 and 8 for the representative wave number 4."
"given the incident wave frequency, the maximum power output is achieved by setting the proper, parameters that can be obtained applying the maximum power transfer theorem (jacobi's theorem, 1840):"
"these simple equations are very useful for a preliminary design of the hull, the gyroscope, the pto, and the control system logic to be implemented on the machine [cit] . these equations are supposed to be valid for small angles of oscillation; for the purpose of this study, results are considered valid for pto oscillations amplitudes up to 45 degrees. the strong coupling between the floater and the gyroscope can be shown here. the action torquė̇given by the gyroscope to the pto is function of the pitch speed. the result of such torque (combined with the control torque ) is the acceleration of the pto shaft. the reaction torquė̇given by the gyroscope to the floater is function of the pto speeḋand, as shown in the next paragraph, interacts with the floater dynamics thus affecting the pitch motion."
"where is the floater width. this term is the ratio between the mean mechanical power generated by the device (which is assumed to be equal to the electrical power, i.e., electrical conversion efficiency equal to unity) and the power of the wave front and it represents somehow the transfer of energy from the wave to the floating device; it should be noted however that its value may exceed one, since the floater may absorb more energy than the one contained in the wave front due to wave-body interactions [cit] . an interesting trend is found if this indicator is plotted versus the wave energy period ( figure 5 )."
"as stated in the introduction, the pto control force for the linear reactive controller is composed of two parts: an elastic contribution and a damping one. it can be written as follows:"
"therefore, once the sea state and the hull hydrodynamic properties are known, it is possible to calculate the suboptimal, unconstrained, stochastic value of the damping coefficient for the pitching motion of the system."
"as it can be seen in figure 7, with the implemented control loop, the gyroscope is able to produce the pitching torque required by optimal control by rotating at the required speed. as already seen for the monochromatic wave in section 4.1, when the optimal control is implemented, the wave excitation torque and the pitching velocity of the floating device are \"in phase\"; that is, their maxima occur at the same time instants (figure 8 ). this is one of the first consequences of the implementation of the optimal control law."
"here, the idea is to make the controller compensate for the dynamics of the floater and then damp its oscillation, so that its motion is in phase with the wave excitation force and thus the power flow is unidirectional, from the waves to the wec. in this controller, an infinite time horizon is needed thus resulting in a noncausal control law. in order to overcome such noncausality, an approximation is introduced. the convolution integral is split into two parts: the causal part remains as it is, whilst the noncausal part is replaced by a damping term, whose value is obtained by means of a stochastic analysis of the wave-structure interaction aimed at maximizing the expected value of the power output. a more detailed explanation of this approach can be found in section 4.3 of this paper after the hydrodynamic model description."
"is the proportional gain of the closed-loop speed controller and it has been necessary to introduce a relatively small stiffness term in order to prevent position drifting in irregular wave conditions. notice that the hull parameters are known since they are characteristics of the device, while the sea state spectrum is given by the weather forecast and by an on-board monitoring system that will be installed for the sea state evaluation and prediction."
"the scientific world journal the -and -axes represents the pitch and yaw torques that the gyroscopic system discharges to the floater. in particular, for the torque related to the pitching -axis, it is possible to write"
"this result has been obtained with regular monochromatic wave, to show how it is possible to maximize the power extraction by tuning the control parameters. in case of irregular waves, the optimal parameters for the linear reactive controller can be found using an optimization algorithm [cit] ."
"stochastic suboptimal control and linear reactive control have been developed, tested, and compared for the iswec device. results were obtained with a linear dynamic model of the system. the suboptimal control maximizes the mean absorbed power at the cost of higher power peaks and generator torques if compared with an optimized linear reactive controller applied to the gyroscope. however if torque limitation is imposed, the power production is still higher than the one obtained with the linear reactive controller. nonetheless, the optimal control theory can give an upper bound of the performance of the wec under irregular sea state conditions and furnishes guidelines for the optimization of other control algorithms and its parameters can be derived analytically given the sea state and the hull hydrodynamic properties. further studies are needed in order to assess the nonlinear gyroscope performance and controllability. moreover, the hydrodynamic model has to be improved in order to take into account nonlinear wave forces and wave-body interactions when high pitch angles are involved."
"(i) the noncausal control law is used together with some prediction algorithm of the future incident wave force, as stochastic autoregressive models [cit], neural networks [cit], or digital filters."
"we conclude the section by presenting an extension of the proposed framework to systems with mixed data-driven and model-based components. to this end, we consider systems of the form"
"given the sea state, the spectrum of the wave excitation forces can be obtained, with it being related to the spectrum of the wave elevation process through the wave-to-force response amplitude operator as"
"in this section the control problem of a generic pitching device is introduced. starting from the floater hydrodynamic equation, the maximum extractable power is obtained for both monochromatic and irregular wave. the suboptimal causal control algorithm is then introduced and the optimal damping factor is obtained by means of a stochastic analysis of the wave resource. in the next section the causal suboptimal control is applied on the iswec and its performances are compared with the linear reactive control."
(ii) the optimal control law is approximated by a closely related causal process and the algorithm becomes then suboptimal. this method does not need to know the wave elevation in order to be used.
"the previously mentioned control law was obtained and tested for a generic pitching device (acting directly on the floater by means of a control torque ); in this section, it will be used to control the iswec gyroscope in order to maximize the wave power conversion. the iswec can be controlled acting through the pto on the -axis of the gyroscope by means of the control torque . two main control strategies are tested for the device under consideration, with and without pto torque saturation, and the results are compared."
"the terms on the right hand side of the equation represent the pitch torque due to the incoming wave, the pitch torque due to the mooring forces, and the pitch control torque acting on the floater. note that, as anticipated in section 2.2, the control torque is generally directly given by the pto, while in the iswec it is given by the gyroscope as a reaction torque due to its motioṅ (7) . mooring contribution will be neglected here, under the assumption that its effect on the pitching motion of the device is small."
"corollary 6 suggests a valuable alternative to sequential system identification and stabilizing robust control. in particular, in the presence of deterministic noise, identificationbased methods are usually either computationally intractable, overly conservative, or they admit no guarantees from finite data. as an alternative, one may consider a stochastic setting, where recent work has addressed finite-time guarantees on system identification with sequential robust control [cit] . these results are based on sophisticated statistical analysis and many of them rely on restrictive assumptions, such as the availability of multiple independent data trajectories, each of which only supplies one data tuple to the estimator. in contrast, our approach relies on simple matrix manipulations combined with existing robust control methods and requires only a single data trajectory."
"variables. thus, the proposed controller design method scales cubically with the data length n and proportionally to n 6 if n is the system dimension, similar as in model-based robust controller design."
"factor. assuming that the floating device is in stationary conditions and keeping the assumption that the wave elevation can be regarded as a stationary zero-mean gaussian process, through linear stochastic dynamics theory [cit], it is possible to derive the optimal control law for known sea state conditions. under these assumptions, the pitching velocity process and in turn the displacement and acceleration can be regarded as stationary zero-mean gaussian, independent random processes. moreover due to stationary conditions the following properties hold [cit] :"
"the goal is now to maximize the pto mean power production on a wide range of sea conditions. for the chosen set of waves, the best stiffness and damping values in terms of mean power production were calculated by means of a parametric analysis. from figure 4, it can be noticed that the power flux between the pto and the gyroscope is bidirectional (i.e., the pto sometimes acts as a motor), therefore introducing a reactive power component. this is why this kind of control is also referred to as \"reactive control. \""
"it is interesting that correspondingly the pitching position of the device is reasonably in phase with the wave measured at the body centre of gravity. this may be very useful in further development of the control algorithm of the system. at the same time, the oscillations of the gyroscope are relatively small which ensures some grade of reliability in using the linearized gyroscope equations. the same holds for the pitching oscillations of the device. results for the other waves are summarized in table 4 ."
"in this analysis the second approach will be followed, since the quality of the prediction algorithms is not considered high enough to control the iswec with the desired accuracy. the anticausal part of the convolution term in (17) is replaced as"
"often, the first step is to develop a control strategy able to maximize the power output under plane (2d problem) monochromatic waves. of course this means that the wave profile is composed of a single frequency contribution and this is not what happens in real sea. afterwards, the case of plane polychromatic wave is analyzed generating a wave time series based on the spectrum of a specific sea state or using acquired wave data. in the most recent studies a 3d sea state is analyzed taking into account wave contributions coming from different directions."
"one of the simplest ways to control a wec is to apply on the floater an action proportional to its velocity. this kind of controller can be called \"proportional controller (p)\" and the ratio of force to velocity is the damping coefficient. in this case, the power output is related to the square of the wave height; moreover, if the wave is monochromatic and its frequency matches the natural frequency of the device, the velocity and the force are in phase and the power absorbed by the wec is maximum [cit] . the natural frequency of a floating body is dependent on its physical features and could be varied acting on its mass, for example, in order to match the incident wave frequency, thus maximizing its response amplitude. another way to obtain such a result, without acting on the physical quantities of the device, is to use a reactive controller. this kind of controller can also be called \"proportional-derivative controller (pd)\" since the torque acting on the floater is composed of two contributions: the first one, proportional to the speed such as in the p controller, and the second one, proportional to the displacement of the body (with respect to the hydrostatic equilibrium condition). the ratio between the last force term and the displacement is the stiffness coefficient. as shown in section 4.1 in this case it is possible to tune the response of the device in order to make the device resonant with the incoming wave [cit] . a problem often arises with this kind of controller: the pto can provide an action up to a maximum value, thus limiting the capacity of the system to adapt itself to the incoming wave. moreover, the pd controller implies reactive power thus increasing the power losses due to the action generated by the pto on the floater. after these considerations, it is clear that the floater has to be designed properly in order to reduce the control reactive component for most of the incoming waves."
"ogilvie converted cummins' equation for a free-floating body in the frequency domain, under only wave excitation forces, and found out the following relationships [cit] :"
"where and are, respectively, the frequency-dependent added mass and potential damping, while is the frequency response function of the radiation. on the right hand side of the equation the wave excitation torque is given by the frequency-dependent force coefficient, representing the torque per wave amplitude unit, multiplied by the wave amplitude ℎ evaluated at the center of gravity of the floater. note that relation (9) is written in the frequency domain thus involving linear quantities and steady state conditions; moreover such relation is valid under monochromatic excitation force. however relation (10) that describes the frequency response function of the radiation is very useful because it will be used in the next section for the implementation of the time domain model."
"in such conditions, the system is resonant with the incoming wave, so the force and the speed are in phase and the power extracted by the oscillator is"
"the main advantages of the iswec device with respect to its competitors are the following ones. all the mechanical components of the system are enclosed in a sealed hull retained by a slack mooring line and, seen from outside, the system thus looks like a moored boat. this means that direct interaction between water and moving parts is avoided, thus reducing corrosion problems and maintenance. moreover, the flywheel speed is an additional free parameter that can be tuned to increase the device performance in a wide range of wave conditions."
"since the iswec pitching undamped natural period is approximately 5.5 s, the waves with longer period are less suitable for power extraction with this kind of device [cit] ."
"equation (18) is also known to be a fredholm integral equation. fourier transforming it, one gets the following relationship between the wave excitation force and the pitching velocity at a general excitation frequency for optimal control:"
"in this section, the results for the waves numbers 2, 5, 6, 7, and 8 were excluded due to high pitching floater oscillations, for which the linear hydrodynamic model loses its validity. compared with table 3, it can be noticed that the rcw of the optimal controlled system is higher but higher peak torque values are registered too."
where the operator [⋅] indicates the expected value and the autocorrelation function that in case of zero-mean process is equal to the covariance function. the substitution
"in a real machine, the pto undergoes some current and thus torque limitations. in order to be able to apply the methodology shown before, it would be needed to recalculate optimal control signals with respect to system constraints, for example, torque limitations. this could be the object of future investigations; nonetheless, it is interesting to show the effect of imposing system constraints a posteriori to the optimal unconstrained control signals. two different values for the pto maximum torque have been investigated and the results are reported in table 5 . when saturations occur, the gyroscope is no longer able to control the floater motion as requested by the optimal control algorithm. this is reflected in the fact that the wave force and the pitching velocity lose their phasing during this transition. as shown in table 5 for 500 knm pto saturation torque, the overall effect is a decrease in the mean power extracted by the machine and thus in the rcw. it has not been possible to decrease more the pto saturation torque in this section since the oscillations of the gyroscope were too high for the linear model to be still acceptable ( figure 9 )."
"the gyroscopic torque makes the pto speed deviate from its target value, and the feedforward torque is used to cancel out this effect and help the control to work better. the resulting control system for the iswec device is shown in figure 6 ."
t i m e : p i t c ha n g l e( r o t a t i o na b o u tt h e -axis) :
"the suboptimal causal control law calculated in the previous section has been implemented on the iswec device. the objective is to control the gyroscope to ensure that a given torque is discharged to the hull. once the optimal pitching torque is calculated through (21) and (36), the pto speed to be set for the linear gyroscope is calculated by means of (7) aṡ"
"recently, the design of controllers directly from measured data has received increasing interest [cit] . while established methods, e.g., those based on reinforcement learning, rarely address closed-loop guarantees, there has been a renewed effort to provide such guarantees using novel statistical estimation techniques [cit] . a potential alternative is robust control with prior set membership identification [cit], which is however well-known to be computationally demanding. in general, providing nonconservative end-to-end guarantees for the closed loop using noisy data of finite length is an open problem, even if the data is generated by a linear time-invariant (lti) system."
"in this section the iswec device is introduced. after a brief description of the device, the hydrodynamic model of the floater and the mechanical model of the gyroscope are described. [cit], are reported."
"for the pitch motion of a rigid floating marine structure, with zero forward speed, assuming that coupling with the other degrees of freedom is negligible, the equation of motion in the time domain can be written in body-fixed coordinates as"
"with the assumption that the wave elevation process is a homogeneous and stationary zero-mean gaussian process, the sea state is given by the one-sided wave spectrum * ( ). given the rao (response amplitude operator) of the system, ( ), that describes the amplitude and phase of the force acting on the floater with respect to a unit amplitude monochromatic wave, it is possible to calculate the time history of the wave forces acting on the structure as a finite sum of harmonic excitation forces:"
"in this section, we apply the results of section iii to the robust h ∞ -control problem for an unstable example system. we consider system (1) with"
"in this paper, one of the most important issues for the power optimization of a wec is faced: the control problem. developing a good control scheme is challenging and many 2 the scientific world journal solutions have been proposed in the recent years [cit] . in section 2, the main existing wecs control algorithms are described. the reviewed algorithms are as follows: the linear proportional-derivative (pd) controller, the latching and declutching controller, the optimal controller, and finally the stochastic suboptimal controller. afterwards the iswec is presented and the equations describing its working principle are discussed. [cit] in real sea, a performance analysis is carried out, comparing the results obtained with the pd controller and the stochastic control algorithm, for some representative wave conditions registered at the installation site. moreover, the effect of the maximum pto torque constraint is analyzed in order to take into account the real machine limits."
"a simple and easy controller consists in making the pto behave as a spring-damper group. this approach was proposed being similar to the one presented in \"pd controller. \" the pto torque equation can be written as"
"in the following, we exploit that the parametrization a s g is equivalent to a particular lower linear fractional transformation (lft) (compare [13, chapter 10] ). to be more precise, the matrices in a s g can be described as a lower lft of a nominal closed-loop system depending on g with the disturbance w, i.e.,"
"for more than two centuries, many devices have been proposed for harvesting such a huge power source: the earliest patent was filed in 1799 in france [cit] s and developed a navigation buoy powered by an air turbine which has been later commercialized [cit] . since then a lot of devices have been conceived and developed while a few of them arrived to the precommercial stage [cit] ."
"the numerical computation of the convolution term in (8) may be quite timeconsuming and not well suited for the design and analysis of the wave energy converter control system. pérez and fossen suggested a smart way for overcoming this problem [cit] . based on (10), it is possible to pursue a parametric frequency domain identification of the impulse response function. the objective is to find an appropriate order transfer function which satisfies the criteria of minimum approximation error, stability, and passivity. the frequency-dependent added mass and potential damping can be found for a chosen set of frequencies by means of any commercial code based on the implementation of the panel method under the assumption of potential flow. by means of the toolbox developed by perez and fossen it has been possible to identify the transfer function related to the pitching degree of freedom of the structure under investigation [cit] . figure 2 shows that it has been possible to find a transfer function able to describe the radiation frequency response function of the floater, with a sufficient approximation in the typical frequency range of the studied sea. the obtained transfer function is stable and responds to the required passivity criteria."
energy dissipations have to be introduced in the model in order to maximize the net power production of the system. comparison with experimental data will be carried out once the iswec prototype is installed and tested in real sea conditions. relative capture width of the wec : double-sided spectrum of the variable * :
"where is the angular momentum of the flywheel. a closedloop speed control is implemented acting on the pto torque. moreover, since the gyroscopic torque acting as a disturbance on the pto axis is known analytically from (5), it is possible to add a feedforward torque as"
the reported data are the result of a spectral analysis of the acquired time series where 0 is the wave spectral height and is the wave energy period.
"where ( ) represents the potential damping of the system. therefore, the optimal control law has as a direct consequence the fact that the wave excitation force is in phase with the floater pitching velocity for all their harmonic components, which is consistent with the hypothesis of maximum power transfer to the system."
"the present paper provides direct, data-driven design procedures for state-feedback gains, which achieve guaranteed closed-loop stability and performance, using noisy input-state data. based on a data-driven parametrization of the closed-loop matrices that are consistent with the data, known robust control methods can be applied. the closedloop parametrization is extended to a setting with partial model knowledge, and the design procedures are applied successfully to an unstable example system. the proposed approach leads to end-to-end guarantees for the closed loop, using a single noisy open-loop data trajectory of finite length, and is thus a promising alternative to sequential system identification and robust control. future research should extend the results of this paper to robust data-driven outputfeedback control."
"the goodness of the conversion efficiency, in order to provide a further comparison parameter for the same device using different control laws, is related here to the relative capture width, rcw, calculated as"
"energy converter. as previously stated, the control law proposed in the previous paragraph is noncausal and cannot be implemented on a real machine, unless the future evolution of the system is known or predicted with a sufficient level of accuracy. at this point, two possible approaches may be followed."
"linearizing the mean zero position of the pto shaft and assuming that the pitching accelerations of the system are small, respectively, from (1), (2), and (4), one gets eventually"
"the paper is structured as follows. after stating the problem formulation in section ii, we use noisy data to describe the uncertain closed loop under state-feedback, and we apply known robust control methods to design controllers with stability and performance guarantees in section iii. moreover, we extend the proposed, purely data-driven approach to systems with mixed data-driven and model-based components. in section iv, we apply the robust state-feedback design techniques successfully to an unstable example system. the paper is concluded in section v."
"for any matrixṽ with n rows. by the fredholm alternative, this is in turn equivalent to the existence of a solution v to the system of linear equations"
"as it can be noted such a control law is noncausal, depending on the future values of the velocity. inserting the equation above into the equation of motion of the system (8), one can get the following:"
"a majority of the literature in the area of multiuser feedback design considers the sum rate as the performance metric and, for tractability reasons, does not allow power adaptation over time and across the users. in practical systems, however, temporal and spatial power adaptation are essential as the users request instantaneous downlink data rates with specific qos constraints. therefore, the base station needs not only the channel direction information for spatial identification of the users, but also the channel magnitude information for power control and/or rate control. it is therefore necessary to study the joint direction and magnitude quantization codebook design and optimization."
"the prediction model's performance was first assessed in the testing sets with equilibrium of class distribution and balanced data. in terms of classification accuracy, confusion matrix results confirmed that there was a consistency between the predicted and actual results, which suggested a better performance of the model in the classification of multi-class objects (fig. 1c) . furthermore, roc curve analysis also verified that the model could predict and distinguish pathologic stages of nsclc with high accuracy of 0.69~1.00 (fig. 1d) . here, the accuracy scores of the original random forest model and limited feature model were 0.53 and 0.57, respectively. in addition to accuracy score, the prediction model was also evaluated from the perspectives of average precision score and precision-recall to each class. as exhibited in precision-recall curves (fig. 1e, f ), our prediction model not only yielded a higher average precision score (ap) of 0.60 (fig. 1e ), but achieved a better diagnostic performance for pathologic stages of nsclc in terms of the extension of precision-recall curve to multi-classes (fig. 1f ) . corresponding results of binarized stage scenario is depicted in fig. 2 ."
"staging plays a crucial role in the evaluation of a patient as it defines the actual extent of the disease. pathologic tumor stage is considered a pivotal factor relating to survival in nsclc, and the 5-year survival rates vary from 83% in pathological stage ia to 23% in stage iiia tumors [cit] . accurate staging is conducive to developing the effective medical treatment and to predicting patient prognosis. with the widespread application and advanced imaging technology in screening and diagnosis, the pathologic stages of more tumors have been diagnosed. however, the rate of recurrence remains unsatisfactory and ranges between 15 and 30%, even after complete surgical resection [cit] . pathologic staging of lusc and luad remains a challenge for the physician using individual pretreatment variables."
"the present study revealed that image features extracted from ct scans was correlated with pathologic stage of patients with nsclc. our study further explored the function and application of machine learning in ct image feature analysis for pathologic staging, meanwhile, unveiled potential imaging biomarkers that can be used for diagnosis and prediction of pathologic stage in nsclc. ultimately, our prediction model that incorporated nine optimal characteristics was validated to be significantly effective in the prediction of lung cancer subtypes and pathologic tumor stages of luad and lusc."
"in conclusion, it is the first time that the significance of radiomics features in prediction of pathologic stages of nsclc has been studied. nine optimal image features were identified as predictive and diagnostic biomarkers for pathologic stages of nsclc. using multiple machine learning algorithms, our prediction model has been verified to effectively predict the tumor stages of nsclc, especially for luad. our findings not only extend the application of machine learning algorithms in ct image feature prediction for pathologic staging, but identify potential imaging biomarkers that can be used for diagnosis and prediction of pathologic stage in nsclc."
"in addition to the internal testing above, we also performed an external validation for the performance of the prediction model by using luad and lusc data sets without preprocessing or equilibrium of class distribution. the original class distribution of samples in the luad cohort (fig. 3a) and lusc cohort (fig. 3b) was displayed in fig. 4 . the machine learning algorithms in the external validation were same as those used in testing sets. in terms of the classifier performance in the luad data set, confusion matrix and roc curves both indicated a high classification accuracy of the model, with auc value of 0.69~1.00 (fig. 3c, d) . likewise, we also re-confirmed the consistency between the predicted and actual results in the lusc data set (fig. 3e) . roc curves also revealed that the model could distinguish the pathologic stages of lusc with high accuracy of at least 65% (fig. 3f) ."
"the proofs of the theorems in this paper are omitted due to the space limits . also, all the computations in the paper are for the real space. extension of the results to the complex space is straightforward."
"to evaluate the performance of prediction model in training and testing sets, receiver operating characteristics (roc) curves were plotted to display classification performance in the testing set and the external validation set. the roc curve is a comprehensive index that reflects false positive rate and true positive rate of continuous variables. the area under the curve (auc) was an evaluation measure for model performance."
"in the asymptotic regime of b -+ 00, the number of direction quantization bits bk is therefore m -1 times the number of magnitude quantization bits bk. moreover, the total number of quantization bits for user k is given by"
"a suboptimal solution for problem (1) is to use zero-forcing (zf) beamforming vectors v» to eliminate the interference and find the power values pk that satisfy the constraints with equality. this solution is asymptotically optimal in the high snr regime. an important matter to consider with this solution is that the transmission powers need to be very high when the users ' channels are closely aligned, as the zf beamforming vectors would be almost perpendicular to the corresponding channels in such cases. therefore, it is not possible to always satisfy the sinr constraints with a bounded average power and, as a result, a certain degree of outage should be tolerated by the users ."
"b+mlog~+ (2m-i) log if., (36) '1 qk which shows that the kth user's share of the total feedback rate is controlled by log'1k and log 1i qk."
"hk e ]rm, v» e ]rm, pk, and \"ik denote respectively, the user channel, the beamforming vector, the allocated power, and the target sinr for the kth user, 1 :::; k :::; m. the minimization of the transmission sum power subject the user sinr constraints is formulated as follows:"
"in addition to evaluating accuracy of prediction model using roc curves, precision-recall metric was also employed to estimate the output quality of the classifier. precision-recall curves is more informative when evaluating binary classifier on imbalanced datasets with performance measures such as precision and recall metrics. a high area under the curve of a precision-recall curve can be detected with either high precision or high recall, which also suggests a low false positive rate or a low false negative rate. high scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). moreover, the higher f1-score, the more stable the classification model."
"recent advances in radiography, such as high-resolution computed tomography and the widespread practice of low-dose helical computed tomography (ct) for screening of tumors, have led to an increase in the early detection of nsclc [cit] . ct has been widely used as a noninvasive diagnostic modality for diagnosis, clinical staging, survival prediction, surveillance of therapeutic response in patients with nsclc [cit] . tumor phenotypic differences, such as irregular shapes and heterogeneity, can be measured using radiomic features derived from ct images."
"3) for the outage probability constraints to be feasible, the total feedback bandwidth should scale with log l' and log 1i ii, where l' and ii are the geometric means of the target sinr's and target outage probabilities. moreover, the minimum required feedback rate increases if the users ' target parameters deviate from the average parameters l' and ii, i.e. there is a feedback rate penalty for serving users with different target parameters. the higher the deviation, the higher is the penalty. 4) as the total feedback rate b increases, the performance of the limited csi system approaches the performance b of the perfect csi system as~. t~."
"nevertheless, it is noteworthy that there are some limitations in our radiomics analysis. although our prediction model could be used for the precise tumor staging of lung cancer, some deviations may exist due to limited sample size. moreover, the imbalanced data sets were subjected to equalization processing, while there are still some deficiencies, and thus a larger cohort would be needed for the further validation of the model. furthermore, despite our focus is the staging of nsclc, we still lack the ct images of healthy volunteers to be negative controls."
"in addition, confusion matrix was applied to examine whether there is a consistency between the predicted and actual results. confusion matrix is a useful tool to evaluate the performance of classifiers in their ability to classify multi-classed objects in addition to roc curves. in this study, we focused on the generalization properties of learning algorithm for multiclass classification problems and used the confusion matrix of a classifier as a measure of its quality. we also used accuracy score, the ration of number of correctly classified samples to the number of all the samples, to evaluate the model predictive performance. finally, we computed a new model by using original features, the accuracy score of which could be calculated according to the chosen optimal features."
the optimization of the multiuser spatial multiplexing system with quantized channel information is a two-fold problem: i) optimizing the power control and beamforming functions for fixed quantization codebooks; 2) optimization of the quantization codebooks. in this section we study the first problem; the codebook optimization will be discussed later.
"herein, our study applied a series of machine learning algorithm, and explored potential imaging biomarkers that can be used for diagnosis and prediction of pathologic stage in nsclc based on a ct image feature analysis."
"considering the limitation of single metrics-precision, recall and f1-score, we decided to adopt average precision score and precision-recall to each class to assess the overall capacity. here, average precision (ap) is used to measure the accuracy of the classifier using weighted mean of precisions achieved at each threshold. furthermore, the output would be binarized if the precision-recall curve and average precision were extended to multi-class or multi-label classification. the precision-recall curve can be plotted through considering each element of the label indicator matrix, which is considered a binary prediction (micro-averaging)."
"through observation on data of each group, we confirmed the imbalanced class distribution of original nsclc samples in the training set (fig. 1a), and then conducted over sampling by using random oversampling. another machine learning algorithm, smote, was subsequently employed to generate a new balanced data set for the following analyses (fig. 1b) ."
"for all other no-outage cases, i.e, ()k 2: ()k' all the sinr constraints should be feasible. by using the sufficient feasibility condition in (9) and noting that user k is not aware of"
"8k is the minimum chordal distance of uk. this covering (enlargement) of the regions increases the required transmission power, which is in the direction of our analysis of deriving sum-power upper bounds. in order to use the results in section iii, we define ¢k, which are referred to as uncertainty angles in section iii, as follows:"
"since we are interested in computing the expected value of the sum power, we use the following approximation for the sum-power upper bound so that the expectation operation can be conveniently applied:"
size. the direction quantization regions are formed by mapping each channel vector hk to a vector uk(hk) e uk that has the smallest angle with hk:
"first, we should confirm whether the original class distribution of nsclc cohort was balanced. if not, over sampling would be performed by means of smote algorithm, to tackle the curse of imbalanced datasets in machine learning and to achieve equilibrium of class distribution by producing a new data set. the newly-generated data sets were then split up into a training set and a testing set."
"lung cancer is one of the most frequent types of malignancy and a leading cause of cancer-associated mortality worldwide [cit] . in clinical management, lung cancer can be classified in two main categories, non-small cell lung cancer (nsclc) and small cell lung cancer, with the former occupying approximately 85% of lung cancers [cit] . nsclc represents a heterogeneous group of cancers mainly composed of lung squamous cell carcinoma (lusc) and lung adenocarcinoma (luad) [cit] . however, the 5-year survival of nsclc remains dismal, and 70% cases are diagnosed after the onset of advanced local or metastatic disease. the prognosis varies widely according to tumor staging at diagnosis [cit] . unfortunately, merely 15% of cases are not diagnosed until late stage [cit], and thus the accurate prediction of pathologic stage for patients with lung cancer is of utmost importance."
"division of qk between qk and ilk only affects the optimal bk and bk by a fixed bounded number of bits, i.e, the variable 1] in equations (31) and (32)."
"we refer to uk(hk) as the quantized direction for the channel realization hk. the corresponding quantization regions, according to the gilbert-varshamov bound argument [cit], can be covered by the following spherical caps:"
"the beamforming vectors vi; are assumed to be the zeroforcing vectors for the quantized directions uk and the optimization is only over the power control functions. this problem can be transformed to a sdp problem and therefore efficiently solved. the sdp reformulation is omitted due to the space limits. in order to obtain a closed-form answer for the sum power, we resort to a suboptimal solution that leads to an upper bound on the sum power. this bound is used in the later sections for the optimization of the quantization codebooks. a byproduct of this upper bound solution is a sufficient feasibility condition for the original problem in (4). z#k z#k"
"for product quantization codebooks that are considered in this paper, the quantization (or channel uncertainty) regions are the sector-type regions shown in fig. i 2) the share of the kth user from the total feedback bandwidth is controlled by log \"ik and log 1i qk, where \"ik and qk are the user 's target sinr and outage probability. as a general rule, a user with a higher qos (lower target outage probability) and higher target rate (higher target sinr) needs a higher channel quantization resolution and therefore requires a larger share of the total feedback rate."
"the script knowledge we have chosen is the omcs [cit] database. in the database are presented many different step by step descriptions of some daily events. to use this script knowledge, we first encode every single description as a passage using the rnn that was once used to encode our mission text. then for every event e i we calculate the event representation as an average of each description representation in that event category:"
"stateless session beans are components that do not maintain any state between invocations. essentially, stateless session beans provide in general utility functions to clients. stateless session beans have a very simple life cycle, since the same instance can repeatedly service requests from different clients without the special handling that is required for stateful beans. in our application, the stateless session bean implements the business logic."
"the pattern is applied at two points, a) at the section which targets the bookstore's customers, and b) at the administrative section. the interface action is used, containing the method performaction(), that takes an httpservletrequest object as parameter and represents the user's request. this interface is implemented by the classes a) admincatalogaction,"
"step 8 illustrates the best obtained error, in this case of minimization using the mean square error (mse), where important data is obtained by the method, such as time, iteration, and error. in step 9, the best error is compared with the stopping criterion. if the results meet the stopping criterion, the methodology ends as shown in steps 10 and 11. if this is not the case, an improvement is made to the method that produced the worst result, in order to continue generating competitiveness with the methods, and depending on the method, the adjustment or improvement is made as shown in step 12, and step 13 begins a new cycle until finding the method that meets the stopping criterion."
"it provides methods that create a new user, update a user's personal information, delete an existing user, add, remove or update a book, return a subset of the books, users or purchases that have occurred, return a user's personal information, the book category which a customer prefers to buy books from and the books that match the query entered by the user in their title, author's last name or category name. additionally, the stateless session bean provides methods that check the validity of the username and password that a user enters for his/her login in the bookstore, the validity of a credit card's number and the validity of a book's isbn and carry out all required actions for the purchase of all books currently contained in a customer's shopping cart."
"the system that has been used as a vehicle in order to investigate the usefulness of design patterns in e-commerce applications is an electronic bookstore. it is a rather typical application, variations of which can be found in any introductory textbook on developing web applications. its design follows the common three-tier architecture and as such it can be considered common enough to allow generalizing the findings of this study. four design patterns have been implemented on the selected e-commerce application."
"this article presents a state-of-the-art approach towards an enhancement of decision support tools for natural disaster management with social media. its novelty lies in the enrichment of geospatial content retrieved from geographic information systems (gis) modeling outcomes with real-time disaster-related information from social media during an emergency incident. instead of solely relying on social media sources or 'a posteriori' analysis through classification [cit], or machine learning approaches [cit], the applied methodology is based on the combination of spatial danger rating models with geo-social tweet messages. as a result, the volume of the underlying tweet messages that have to be processed is significantly reduced and the possibility to include erroneous messages is minimized. by using existing and well-studied geographical models for danger rating, the open problem of handling social media information during the occurrence of natural disasters can be tackled."
"equation (8) is applied when a drone commits a violation when leaving the limits of the search space, where violation team is equal to the infraction, tmc represents the equipment coordinates, ub j and lb j correspond to the upper and lower limits, respectively, d is equal to the number of dimensions of the problem, and finally n represents the number of drones per team."
"there is, also, notable decrease to cm and chc metrics for the classes: bookdetails, shoppingcartitem, transactiondetails and currency. the decrease percentage of cm and chc metrics is not calculated because the values of the metrics before the implementation of the mvc pattern were too low and a decrease by one, e.g. from value 2 to value 1, would mean 50% decrease. if one of the above classes changes the need for changes to the system decreases. additionally, the lines of source code have decreased by 45% (4525 to 3121 lines), because the code that has to do with the presentation has moved to jsps. although the number of code lines of jsps is approximately the number of code lines that have been removed from the servlets, the distinction between presentation code and functionality code simplifies our application a lot and conceptually its extensibility."
"iterations dimensions 30 500 128 table 11 shows the results of 30 experiments, where the performance of the wdo and dso method is compared, using 128 dimensions for the search evaluation of the objective function, which is in this case the minimum of each function; these are shown in column 1 of the table."
the dso algorithm consists of the simulation of a drone squadron with different equipment and a command center. the command center uses information collected by the drones to perform two operations:
"de albuquerque, j.p., herfort, b., brenning, a. and zipf, a., 2015 herfort, b., de albuquerque, j.p., schelhorn, s.j. and zipf, a., 2014. exploring the geographical relations between social media and flood phenomena to improve situational awareness. in connecting a digital europe through location and place (pp. 55-71"
"however some of the question in the test set cannot be answered merely with information provided in the passage. we try to embody some kinds of common sense knowledge representation into the general model, however their influence to system performance varies."
"a description of the implementation of four specific design patterns follows. we will present, as a case study, how these particular design patterns can be implemented to the bookstore pilot application."
"the proposed highly scalable architecture relies exclusively on big data components; thus, it can be applied to different geographical areas, to different types of social media and to a variety of natural disasters. even though the big data ecosystem integrates many platforms and software components, it is mainly based on distributed storage and processing of very large datasets on computer clusters."
"the implementation of transfer object design pattern decreases the need for changes of the system in case the transfersessionremotebusiness, bookdetails, bookdb and booksnotfoundexception classes change. the decrease of cm metric shows the decrease of the need for changes to the system. the average decrease of cm metric is 13%, though bookdb and booksnotfoundexception classes have the biggest decrease, 16%."
data and classes that act as data represent the model. the functionality is handled by classes or servlets and the display by html or java server pages (jsp).
"after the implementation of the two instances of the service to worker design pattern, one for the administration section and one for the customer section of the e-shop, all servlets, except admincatalogservlet and frontcontrollerservlet which have the role of front controller for the two sections, no longer exist. the two front controllers, the classes that implement the dispatcher interface and the classes that implement the action interface share the role of those servlets. table 3 shows the changes of the cbo, rfc and chc metrics for the servlets after the implementation of the design pattern. the average decrease of cbo metric is 28%, though the logoutservlet class has the biggest decrease, 70%. the average decrease of rfc metric is 27%, though the logoutservlet class has also the biggest decrease, 67%. the chc metric for all servlets except admincatalogservlet and frontcontrollerservlet that remains 2, becomes 1. the value of cm metric does not notably change."
"and l) usersaction. the interface dispatcher is also used, which contains the method getnextpage() that takes as parameter an httpservletrequest object and represents the user's request. this interface is implemented by the classes userdispatcher and admindispatcher. the former takes on the responsibility of executing the appropriate action according to user's request and calls the appropriate jsp (view) for the user section of the application, whereas the latter does the same for the administrative section. after the implementation of the pattern, the servlets are replaced by \"normal\" classes. this replacement enables the ability to call the performaction() method without the existence of a web container, as is the case with servlets. the dispatcher object also takes on the responsibility of choosing the appropriate action that must be executed and the call of the jsp that renders the response. if for any reason, the logic behind the selection of the appropriate action or jsp is changed, the implementation of dispatcher is all that needs to be changed, without affecting other classes (e.g. the class that implements front controller)."
"the big decrease of the wmpc1 and wmpc2 metrics shows the big decrease of the complexity (wmpc1) and the number of methods of the servlets that no longer exist (wmpc2) after the implementation of the service to worker pattern. additionally, the lines of source code have decreased by 8.4% (3087 to 2829 lines)."
"our model is a machine comprehension model based on textual entailment logics, and on the basis of previous works we made several renovations to embody common sense knowledge representation. we finally reached accuracy for about 63% on test dataset, however due to time limit, we have never tried any fine-tuning techniques. observing this model we are able to say that it is useful to have common sense knowledge data integrated to machine comprehension problems, though a porper knowledge representation should be worked out. we are currently switching to other kinds of common sense knowledge representations, and trying to devise new answer selection logics. from the competition result it is very clear that there's still much space for our accuracy improvements."
"equation (4) represents the new velocity that consists of a first term where one is the maximum pressure, α is the coefficient of friction, and u i t is the current velocity. the second term is composed by g, which corresponds to the gravity, while x i t is the current position, rt represents the universal gas constant and temperature respectively, r represents the range of the air pack where all the air particles are classified in descending order with respect to their pressure, and x opt is the best global position, where the coriolis constant is c."
"our approach is based on the enrichment of geospatial modeling results with real-time disaster-related information from social media during an emergency incident. compared to similar studies, the added value is by combining wildfire behavior modeling outputs with tweet messages in order to increase the accuracy and efficiency of the tweets during an emergency."
"after an inspection of servlets a) bookdetailsservlet, function exists in all of them. this common function is the check whether a user browses the bookstore either anonymously or with his/her username and password. if the user has not logged in using one of the previous two methods, then he/she is forwarded to the book-store's login page in order to login properly."
"the current position is represented by the term x t i, the second term determines the initial attraction of the firefly β 0, γ is the absorption coefficient, and r is the euclidean distance between the positions of the firefly i and the firefly j. the last term handles the exploration, where α is the parameter that controls how much randomness the firefly is allowed to have in its movement, and is a vector containing random numbers drawn from a gaussian distribution or uniform distribution at time t."
"this section is presented in two parts. the first is for case 1, which describes the results obtained by the optimization of benchmark functions using the three methods proposed for the methodology. the second part, for case 2, shows the results of the fuzzy controller optimization for the autonomous mobile robot. for this optimization, the method that optimizes generates a vector of data which goes to the parameters of the membership functions being optimized. table 6 shows the parameters used for the benchmark functions. column 1 of table 7 shows the number that represents the particular function; column 2 shows the minimum of the function; columns 3 and 4 show the averages and standard deviations respectively obtained with the wdo; finally, columns 5 and 6 shows the results of dso. each of these functions was evaluated 30 times with the same parameters to obtain the averages and standard deviations. the parameters used in the wdo, dso, and fa methods are presented in table 8 . table 6 . parameters used in the experiments."
"equation (8) is applied when a drone commits a violation when leaving the limits of the search space, where is equal to the infraction, represents the equipment coordinates, and correspond to the upper and lower limits, respectively, is equal to the number of dimensions of the problem, and finally represents the number of drones per team."
"the methodology consists of creating a competitive model based on a set of metaheuristics, and the motivation for this proposal was born with the aim of streamlining the search processes for optimization methods, because many times in research when it comes to optimizing a specific problem, time is lost experimenting with one metaheuristic after another, until finding the one that adapts better to the problem to have an optimization with a satisfactory result. the general idea of this proposal is to have a series of optimization methods, which receive an input (specific problem) to be processed, and thus optimized. the method that produces a better result than the others in the competition will show that it is the best to optimize that problem, and a metric is used to evaluate the particular results of the problem. the motivation is to develop an optimization methodology where several metaheuristics are used, thus evaluating their performance in optimizing and solving a particular problem. figure 2 shows the data flow of the proposed methodology."
"the principle behind the transfer object design pattern is the creation of an object that carries multiple data from one application layer to another, e.g. from the enterprise to the presentation layer [cit] . all class attributes are defined public, so that no getter and setter methods are created, as is the case with java beans. the use of transfer object minimizes the effort and time to transfer data from one layer to another and the communication in general of a java ee application's layers. the uml class diagram of the design pattern is shown in figure 4. in the bookstore application, there is a transfer of a great deal of data between the enterprise and the presentation layer. some of those are the list of available books, the list of users and the list of purchases. the display of each list is done partially in pages, so that they are easy to read due to their large size. however, the latter is not fixed, because of the fact that at any time books can be added or deleted, a book's supply can reach zero, users can be added or deleted or new purchases can occur. therefore, during a page retrieve process the length of each list should be known in advance, besides the list's elements that correspond to the currently displayed page, in order to create the links to the other pages."
"k. m. [cit] stated that machine reading comprehension can be defined as a task that deals with the automatic understanding of texts. in their paper, it was also mentioned that machine comprehension can be evaluated by two methods, namely (1) translating the text into formal language representations and evaluating it using structured queries. (2) evaluating it through natural language questions. recently a lot of datasets are available for evaluating machine reading comprehension systems, for example, there are squad [cit] and the mctest [cit] . on many of these datasets human-like performance has been achieved. however, one of the biggest challenges in machine comprehension is how to provide common sense knowledge regarding daily events to machines [cit] ."
"in this section, we present some basic knowledge required for a comprehensive understanding of our model. we first give a basic introduction to rnn models and the implementation of gru cell, then cast a little glance upon the textual entailment problems."
"where p is the complete perturbation formula that should be calculated to return the trial coordinates, the variable departure is a particular coordinate, o f f set is a function that returns the actual perturbation movement, and tc generates new trial coordinates through perturbation."
"therefore with the interaction of these two gate the cell is able to learn a pattern whether to reset the hidden state using current input, or to retain the previous hidden state largely."
"the following tables 12 and 13 show the results obtained with the fa for the optimization of the f1 and f2 function with 30, 64, and 128 dimensions, respectively, where it can be noted that the results are very far from the global minima of the functions. previously, we have been experimenting with this fa method, where we can note that in order to obtain a good result with it, it is necessary to increase the iterations and maintain a population of 30 to 50 fireflies. therefore, as future work in this methodology, an adjustment will be made to the method to improve its behavior in the optimization of benchmark functions, remembering that this methodology is proposed just for that: to find which method is good for a specific problem and in competitiveness with others. for the method with which a worse result is obtained, we can make an improvement to help its behavior. table 14 shows the parameters used in the fa, wdo, and dso methods, and table 15 shows the 30 experiments performed to obtain the best optimized fuzzy system, using as metric the mse, where it can be observed that the best error found is of 0.00169, and in general, the values only varied from 10 −02 to 10 −03 . in figures 9 and 10, the variation that was made in the parameters of the membership functions of the two inputs of the fuzzy controller is observed, where the uncertainty that exists between each of them can be observed, and with this, the error that generates the fuzzy controller in the simulation is considerably improved. figures 11 and 12 show the outputs of the controller optimized by the fa, respectively. as can be noted in table 16, the results obtained with the wdo method were very far from each other, since it has the best mse of 0.000019, but as the worst is 0.057094. for this reason, on average, this method cannot be the best at optimizing this specific problem. as can be noted in table 16, the results obtained with the wdo method were very far from each other, since it has the best mse of 0.000019, but as the worst is 0.057094. for this reason, on average, this method cannot be the best at optimizing this specific problem. dso on average gives a better result than the fa and wdo. as can be noted in column 2 in table 17, it appears that this is not the case, since the wdo method gives much lower values, but they are very separated from each other, and the dso remains consistent with the delivered results of the mse. dso on average gives a better result than the fa and wdo. as can be noted in column 2 in table 17, it appears that this is not the case, since the wdo method gives much lower values, but they are very separated from each other, and the dso remains consistent with the delivered results of the mse. as can be noted from figures 17-20, the overlap between existing functions helps the robot to have a better tracking of the desired trajectory, in comparison with the other methods used in this methodology. table 18 summarized the results of the optimization methods. the equation for the z test is as follows: table 19 shows the statistical data used in the z test, and table 20 the results of the test. the equation for the z test is as follows: table 19 shows the statistical data used in the z test, and table 20 the results of the test."
"moving the source code that is responsible for the presentation of the application to jsps has resulted to the decrease of servlet response, which is shown by the decrease of rfc metric. the decrease of complexity of the servlets is shown by the large decrease of wmpc1 and wmpc2 metrics."
"before the implementation of the mvc design pattern, a number of servlets had the dual role of a controller and a view and the same time. after the introduction of the mvc pattern, the servlets act only as the controller, which process the request and update the model, while jsps take over the role of the view part."
"where x is the input vector and h is the hiddenstate vector, and [·] j means the jth element of a given vector. w (·) u (·) are matrix parameters to be trained. with these variables defined, the hidden state can be updated as"
"where x i,j is the j th description representation of event i. then let e denote the event representation matrix with its i th row representing event i, then every time context representation r t is calculated, we calculate a similarity vector s as:"
"an obstacle to the efficient use of the gates platform is the content's sheer volume and its unstructured nature. very often, neither the available hardware nor software allows citizens to search social media content efficiently, and ensure that all important information is received and read. therefore, the integration and dissemination of social media content is an important and valuable contribution to the overall disaster management effort. paper results show that focusing on the geographic context of the vgi provides a useful approach to deal with the information overload by filtering and assessing the social media content based on credible and authoritative spatial information."
"in case 1, unimodal, multimodal, and fixed-dimension multimodal benchmark functions [cit] were optimized, and their definitions are summarized in tables 2, 3, and 4, respectively. the functions are optimized using the aforementioned methods, and the competitiveness among them is compared, in this way finding out more detail about their operation and discovering the advantages and disadvantages of each method."
"the proposed methodology was created to optimize problems, and was tested with the unimodal and multimodal benchmark functions with the wdo, dso, and fa methods. each of them was put into competition with the same parameters for a fair competition, with 30 experiments as the limitation. under these parameters, it was obtained that wso was better for benchmark functions. on the other hand, for the optimization of the parameters of the membership functions, the dso method was better, and it was the metaheuristic that found the data vector that managed to optimize the functions of the fuzzy controller in such a way that the robot in simulation approached the desired trajectory. the proposed methodology showed which method in the competition was the best to solve a specific case, and was expected to improve the method with which good results were not obtained. as future work, we plan to perform more experimentation with other optimization problems. in addition, it is worth mentioning that other methods for the required optimization can be added in this methodology. the proposed methodology for optimization problems is the main contribution of this work, resulting in the best method among the competition."
"we trained our model using sgd with weight decay. [cit] 0 time steps. when near convergence, our model can reach around 80% to 90% accuracy upon training set (the accuracy is sampled), and in last two model we trained that finally lead to our only submission, we get an accuracy result of about 68% on developing set. this accuracy is a little higher than our final accuracy on test set. our final result compared with baseline and the first rank system is given in the form."
"in case 1, unimodal, multimodal, and fixed-dimension multimodal benchmark functions [cit] were optimized, and their definitions are summarized in table 2, table 3, and table 4, respectively. the functions are optimized using the aforementioned methods, and the competitiveness among them is compared, in this way finding out more detail about their operation and discovering the advantages and disadvantages of each method. benchmark functions f 1-f 7"
"in addition to the absolute metric differences between the two systems, the influence of the design patterns in an application can be demonstrated more clearly by extending this application. in order to validate the influence of the design patterns in this particular application, we will consider a few extensions regarding user requirements in ecommerce setting. each extension is usually influenced the most by one of the design patterns. in the following section we will discuss the changes that have to be made for each hypothetical extension for the non-pattern and the pattern version of the code."
"iterations dimensions 30 500 30 table 9 above shows the optimization results of benchmark functions where the minimum of functions f15, f16, f17, and f18 is different from zero. with the evaluation of functions with different minima, the performance of the optimization in the functions with the wdo and dso can be better observed. table 10 shows the parameters used in the above-mentioned methods. these parameters are the same for each of these algorithms as a fair form of competition is aimed. table 10 . population, iterations, and dimensions."
"one of the commonly used rnn hidden units is lstm [cit] . this kind of hidden unit can retain short-term memory for a long time during sequence processing, thus is able to recognize long-term dependency information. gru cell [cit] ) is inspired by lstm, and is simpler to compute. for each timestep t, two gating vector is computed, i.e., the reset gate r and the update gate z by"
"in this case, we propose the optimization for a fuzzy controller of an autonomous mobile robot, using the multi-metaheuristic competitiveness model to find out which is the satisfactory optimization algorithm for this specific problem; below, the fuzzy controller is explained in more detail."
"during the use of an e-shop or any application in general, the need to change or extend it often arises. these changes might affect either the application functionality or its user interface. if no explicit boundaries between the functionality and the user interface elements in an application are defined, problems could arise. if, for example, a change in the application's display is needed, this could affect the portions of the code that implement its functionality and vice versa. the goal of the model view controller (mvc) pattern is to set explicit boundaries between the elements of an application that implement its functionality, display and model [cit] . as the uml class diagram of the mvc design pattern shows (figure 3), code is separated into three distinct segments: model (stores the data and application logic for the interface), view (renders the interface, usually to the screen), and controller (responds to user input by modifying the model). the basic principle of mvc is the separation of responsibilities. in an mvc application, the model class concerns itself only with the application's state and logic. it has no interest in how that state is represented to the user or how user input is received. by contrast, the view class concerns itself only with [cit] ."
"then we use c as the input of a two layer fullyconnected neural network, where the hidden layer in the middle has nodes only half the number of the input, and the output layer is a softmaxed probability distribution representing our model's final choice."
"unpredictable factors during a natural disaster, such as any sudden changes of winds during a wildfire, collapse of a building right after an earthquake, or human actions during the emergency are too difficult to be evaluated in the current decision support systems. real-time information collected near the disaster location might be useful when dealing with disaster situations during or immediately after an emergency [cit] ."
"1. all fireflies are the same. 2. less bright fireflies will move toward the more bright fireflies. 3. the search space is given by the objective function. in equation (1), the attractiveness of a firefly is proportional to other fireflies, and the variation of β and the distance r are given as follows."
"the goal of the service to worker design pattern is to maintain a separation between the actions that must be executed, the display and the functionality of the application [cit] . service to worker is an extension to front controller and uses an object that is called dispatcher. the dispatcher encloses the appropriate action to be executed and the corresponding display page according to the user's request. the uml class diagram of the pattern is depicted in figure 5 ."
"we have another fully-connected classifier that works as described below. let α n 1 and α n 2 denote the answer representations generated with pure rnn layer with gru cell (compare with the wwa generated representation in the bilinear classifier), and let q n denote the attended question representation generated by word-wise-attention mechanism described in subsection 2.3. then we concatenate these three representation as c:"
"the present work aims to highlight the role of the social big data, towards a more sophisticated transfer of knowledge among the civil protection authorities, emergency response crews and the affected population. the results from our case studies show that social media content encloses potentially useful information and can act as an additional communication channel for citizens who have been affected by a disaster."
"the initial bookstore application does not define any explicit boundaries between the elements that implement its functionality or display. in fact, the servlets are the application elements that implement both. after the application of mvc pattern for each servlet, besides frontcontrollerservlet -which was created after the application of the front controller pattern and it does not implement any display part of the application -one or more jsps have been created. the jsps implement the role of the view and the servlets implement the role of the controller in the mvc pattern. more specifically, a separation of code that relates to the functionality and code that relates to the display is applied. the latter is moved into jsps."
"at the same time, the growing use of electronic devices equipped with global positioning system (gps) receivers has increased the amount of geoinformation available in social media platforms (e.g. blogs, chat rooms, discussion forums, wikis, youtube channels, linkedin, facebook and twitter) and transformed them into location-based social networks [cit] . social media messages with geographic reference can be described by the terms of volunteered geographic information (vgi) [cit], neogeography [cit] and crowdsourcing [cit] ."
"2 it is a high scalable message queue storage, capable of process streaming data such as tweet messages. apache kafka works together with the apache hadoop 3 framework. the hadoop open-source work framework provides tools for organizing, managing and transforming large-scale data. on top of hadoop runs the hadoop distributed file system (hdfs) that is a distributed file system designed to run on commodity hardware. inside the big data cluster, virtual machines called workers receive the twitter messages and distribute them to the brokers, which are responsible for replicating the messages."
"we try to first adapt an existing textual entailment model to this machine comprehension problem. [cit] ., in which the premise is first contextually encoded, then a hypothesis-to-premise word by word attention is calculated. the model implicitly modify a hidden variable r t to regulate the attention distribution at timestep t as"
"moving the functionality from the servlets to classes that implement the dispatcher and the action interfaces results in the decrease of the response of the classes that implement the action and the decrease of the coupling betweens classes. the decrease of the rfc metric shows the decrease of the response and the decrease of the cbo metric show the decrease of the coupling. the decrease of the chc metric shows the decrease of the need for changes if one of those classes changes. table 4 shows the changes of the wmpc1 and wmpc2 metrics. the average decrease of wmpc1 metric is 38%, though the logoutservlet class has the biggest decrease, 75%. the average decrease of wmpc2 metric is 61%. this metric for the admincatalogservlet servlet has not been altered; for the frontcontrollerservlet servlet has had 9% increase and the remaining servlets have had a decrease of 80%."
"according to the principle behind the front controller design pattern, any common functionality is gathered into one application element, e.g. a servlet, which will be called before any other servlet [cit] . the uml class diagram of the design pattern is shown in figure 2. after the implementation of the design pattern, any source code repetition is avoided, making the process of code maintenance easier and reducing the possibility of mistakes in case of a change in the application. in this particular situation, the same portion of code is repeated eight times (in the eight servlets)."
"the application utilizes two enterprise beans, a stateful session bean and a stateless session bean. stateful session beans are components that need to maintain a conversational state on a per-client, per-session basis. that is, a different instance of a stateful session bean implementation class is used for each client, and its state is maintained between method invocations until the session is terminated. for stateful beans, the life cycle is complicated, since the instance must be associated either with a particular client across multiple method calls or with a particular persistent entity [cit] . in our application, a stateful bean implements the functionality of a customer's shopping cart during his/her visit into the e-shop."
"a stateful session bean has been selected to implement the functionality of a customer's shopping cart because, in contrary with a stateless, it has the ability to maintain information during a session. therefore, using a stateful session bean, the contents of the customer's shopping cart can be maintained during his/her navigation through the on line bookstore. the stateful session bean provides methods that add and remove a book in the cart, empty the customer's shopping cart, return the collection of books that the cart contains and the amount that the customer must pay (which is calculated as the sum of prices of all books currently contained in the cart)."
"in our application, the frontcontrollerservlet class acts as the gathering point of the common functionality, in other words, the concept of the frontcontroller. the frontcontrollerservlet servlet includes the common code from the aforementioned eight servlets and handles the common functionality. frontcontrollerservlet is executed every time the user requests one of the eight servlets, and after it completes execution, it calls the appropriate pagecontroller for execution, according to what was initially requested by the user servlet. each of the servlets has the role of pagecontroller."
"despite the intensive research activities regarding the contribution of social media in natural disasters management [cit], many challenges are still open. the form of geo-social media is highly unstructured and thematically diverse, while valuable knowledge is often implicit and cannot be easily processed through automation [cit] . such data structure heterogeneity has a direct impact on the ability to store, manage or process effectively. apart from data diversity, there is an enormous volume of social geospatial data -especially during emergencies -that must be analyzed as soon as possible. with all the high volume, high speed and varied structure of social media content, one significant challenge is to deal with this 'big data' problem [cit] . even though the amount of the available data is huge, reliable information is rare to find and difficult to locate within the enormous pool of social media postings [cit] . it is easier than ever to search the web for information, but filtering out falsehood and off-topic discussions from the huge online content still remains difficult [cit] . thus, it is still an open research question how emergency management agencies and the public can capitalize on the abundance of geo-social media by reducing the volume to credible and relevant content [cit] ."
"our approach follows a 'big data' architecture to cope with challenges of huge amounts of data, in different formats and varying quality that must be processed quickly. big data technology emerges as a technology capable of successfully addressing contemporary digital challenges. big data provide high-volume, high-velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization."
"even in the cases where the geo-social content is combined with gis data from diverse sources, this accomplishment requires 'a posteriori' analysis of the messages mostly though classification [cit], machine learning [cit] or natural language processing methods [cit] . this analysis, however, adds crucial time overheads that hinder the timely and effective response to an emergency."
"the methodology proposes competitiveness among optimization metaheuristics to improve overall performance, and this is aimed at reducing the search time from one method to another when one of them does not give the expected results. in addition to the search time, the superior method is also obtained with more certainty for each type of problem optimization, since the competitiveness that exists between them delivers only the best result. with this methodology, the method that generates the worst result is also helped, also assisting other scientists regarding the cases for which the used methods are good and for which they are not. therefore, with the obtained results in the optimization of unimodal, multimodal, and fixed-dimension multimodal benchmark functions, it can be said that the wdo in general gives, on average, better results for this type of function, while the dso algorithm gives more results that are far from the minimum of the function. however, in terms of the parameter optimization of the membership functions of a fuzzy controller in particular, as can noted in table 18, the method that on average produces better results is dso. the method that can be said to be the worst regarding these results of competitiveness is the fa, since this method did not show good results with any of the two previous cases of minimization. for this reason, as future work, it is proposed to make an improvement to this method, depending on the previous research that has its disadvantages in these cases."
"tweet messages are retrieved from the twitter source by utilizing the twitter api and stored in kafka topics. the kafka connect api is utilized that receives messages from any sources (such as twitter) and redirects them into related sinks (i.e. cassandra, postgresql, elasticsearch). in the proposed methodology, tweet messages are retrieved from the twitter api (that is used as a source) and stored in a kafka topic through a twittersourceconnector. from the topic, the producer api is used to connect the source (i.e. twitter) to any kafka topic as a stream of records for a specific category (i.e. a specific natural disaster event). from there, the consumer api is used to get out the tweeter messages from the twitter topics into elasticsearch 4, a distributed big data search and analytics engine capable for near real-time use cases. fig. 3 describes how elasticsearch is used for filtering out the off-topic tweets. the area of interest for the specific wildfire (i.e. the arrival time based on the mtt fire behavior modeling) is retrieved in a json format from the arcgis server that is used to store all output results of the fire simulations. this polygon about the area of interest is used as a geopolygon query inside the elasticsearch big data store, to exclude the off-topic messages and visualize the meaningful messages through the web-based gis visualization platform."
"social media has been used to disseminate a wide range of public safety information before, during and after natural disasters by providing assistance towards the establishment of situational awareness [cit] . before an incident, social media can be utilized by emergency management organizations to provide citizens with preparedness and readiness information. social media can be used to send and receive disaster preparedness information, disaster warnings and detected disaster signals [cit] . during the event, they can be utilized for sending or receiving requests for assistance, as well as to inform about the location and the conditions of disaster-affected populations [cit] . obtaining real-time information as an incident can help first responder organizations to determine where people are located, assess victim needs, and alert citizens and first responders to changing conditions and new threats. in post-event phases, social media can provide and receive information about disaster response and recovery [cit] ."
"the first pillar of the proposed methodology ( fig. 1) consists of the gis modeling component that utilizes the minimum travel time (mtt) algorithm [cit], as the fire behavior prediction system inside a prototype web-based gis system called gates. by running fire simulations through the mtt algorithm, parameters such as major flow paths, spread rate, time of arrival and fireline intensity can be calculated. the perimeter polygon of the simulated fire restricts the area where the tweet messages will be filtered before shown in the webbased gis system. by the time a new fire is ignited and the perimeter has been calculated, tweet messages that consist of up to 140 unicode characters are steadily analyzed by the geo-social component. the geographic location of any tweet message is described in the metadata field 'coordinates', which is also known as geotag. in general, users can geo-reference messages in twitter either manually (e.g. by entering the name of a city in the field 'location') or automatically when a client application has access to the coordinates of a gps receiver. because in most situations only a small fraction of tweets are geo-referenced by users, an external gazetteer component of the geocoding api of esri 1 is used. the component searches the tweet messages for place names (toponyms) and assigns coordinates if a toponym is found."
"the average decrease of rfc metric is 5%, though transactsessionremotebusiness class has the biggest decrease, 15%. at the same time, the average decrease of wmpc1 metric is 5%, though transactsessionremotebusiness class has the biggest decrease, 15%. the average decrease of wmpc2 metric is 3%, though transactsessionremotebusiness class has also the biggest decrease, 4.4%. the values of cbo and chc metrics have not notably changed. additionally, the lines of source code have decreased by 1% (3121 to 3087 lines)."
"this section describes the characteristics and behavior of the selected methods for the development of the proposed model. these methods were chosen from among many existing optimization algorithms, because these have been shown to provide good results in solving optimization problems, in addition to being relatively new and innovative. for this reason, this paper also intends to give them the opportunity to test their potential in optimization performance."
"author contributions: f.v. and j.s. reviewed the state of the art; o.c. contributed to the discussion and analysis of the results; m.l.l. analyzed of the original method and used fuzzy logic for parameter adaptation, contributed to the simulations and wrote the paper. all authors have read and approved the final manuscript."
"step 1 is the input of the problem that will be optimized, which is processed by the methods in steps 2-5. then, in step 6, the simulation to obtain the results is performed; in step 7, we show the ranking of errors."
"the jsps are not included in table 2 because the metrics rfc, wmpc1, wmpc2, cm and chc for jsps are too low. it must be noted that jsps in general are considered as web elements of a simple structure."
"the quality of the source code has been evaluated using a set of metrics [cit] . simple classes with few methods are easy to change and extend. classes that are not tightly coupled with other classes or have low response set, cause few or no changes to other classes when they are subject to change. the metrics that are used to evaluate how easily the source code can be maintained and extended before and after the implementation of each design pattern are:"
"in the general model, the embedding layer that converts the one-hot representation of an input word to its corresponding embedding vector is trainable, and is optimized during training using back-propagation [cit] algorithm. however, due to the relatively small database size, we have finally marked the embedding layer untrainable, and use glove [cit] word embeddings instead."
"where is the complete perturbation formula that should be calculated to return the trial coordinates, the variable is a particular coordinate, is a function that returns the actual perturbation movement, and generates new trial coordinates through perturbation."
"for identifying messages containing relevant to the incident information, twitter messages are filtered based on specific keywords that is common practice in the analysis of twitter messages [cit] . tweets containing the greek keywords 'photia' or 'pyrkaya (meaning 'fire'), 'sismos' (meaning 'earthquake') are retained. by following the aforementioned approach, tweet messages can be visualized on top of a web-gis system."
") receiptservlet, and h) showcartservlet. the decrease of the complexity of the servlets, extracting the common source code from them, is shown by the decrease of wmpc1 metric. extracting source code from the servlets has resulted in the decrease of servlet \"response\", which is shown by the decrease of rfc metric and the decrease of coupling, which is shown by the decrease of cbo metric. the common source code of servlets that was moved to frontcontrollerservlet servlet included coupling between classes that no longer exists in the new servlets. additionally, the lines of source code were decreased by 0.8% (4562 to 4525 lines). if the common functions of the application were more than one, the decrease would be much more."
"where x i,k is the kth component of the spatial coordinate, x i is the position of a firefly, and r ij is the euclidean distance between two fireflies i and j."
"our model answers the multiple choice question by first encoding the question and the passage combined using the aforementioned textual entailment encoding, and then using two different question answering classifier to choose one of the two choices. a weighted sum is then calculated from the two answers each represented by a binary distribution. the weight is dynamically decided by a feed-forward network taking two contextually encoded answer strings as input. first the contextual embedded answer string representations of two answer choices are calculated by rnn-encoder, then we concatenate them and put them into a feed-forward network to calculate weights for the ensemble of the two aforementioned classifier. a general illustration can be seen in figure 2. here gru stands for an rnn unit using gru cell, while wwa stands for word-wise attention used in textual entailment. we can see that our model can be trained end-to-end, and most of the weights can be dynamically learnt during training. during training, the text is tokenized and lemmatized using python nltk [cit], and word stemming is not performed. we have made this choice because which words should be classified as stop word is hard to decide for an rnn model that are likely to capture some of the syntactic features of a given language. the motivation for our using two different classifier is that we want to softly provide different solutions to different kinds of problems. the bilinear classifier measures similarity between questionattended and answer-attended contextual representations, which we believe should have better result on non-tf questions (by non-tf questions we mean those open questions which can not be answered by \"yes-true\" or \"no-false\"), while the dense classifier should do better on tf questions according to our expectation. a detailed analysis of the model weights can be seen in next section."
"for this purpose, the transferlist class was created which acts as the transferobject. the class has two properties, an arraylist object that holds the portion of the list that is requested and an integer that holds the length of the list. the transferlist class is not different when it holds books, user or buy objects, because of the arraylist's feature to hold any kind of object. transferlist allows for the concurrent transfer of a great deal of the same kind of objects and their count number, which are currently stored in the database. by utilizing this, the number of database accesses has decreased, since all required data are retrieved with a single function, whereas before the pattern's implementation two databases accesses were necessary, one for the page's objects and one for the total number of objects."
"before inputting the raw text into out model, we first transform words into their one-hot representation without stemming and lemmatization, and tokenization is done using nltk toolkit. then we push the data through an embedding layer in which the glove 50 was used due to time concerns."
one of the extensions that are common to expect in a web application is the addition of a utility function that is available in several parts of the application. in order to investigate the influence of design patterns on our application we choose the first extension to be the addition of a common utility and more specifically the display of day and time in a particular format in every single page of the e-bookstore.
"in this paper, we presented a model for privacy preservation in social networks. the model sanitizes the collected data and sensitive information of sn users using ldp and then attempts to reconstruct the original sequences and perform analyses using sets of selected salient points. we conserve the social structure of each user's communication pattern. the error rate of the estimated data compared to the original data is acceptable for large datasets with small timeintervals. our simulation results show that conducting anomaly detection on synthetic data results in determining the same anomalous users and activities as those in the original data. in the future, we plan to extend the proposed privacy model to include estimating noisy data with non-linear approximation."
"1. we propose a model that protects user privacy in sns compared with other solutions where sensitive user information is poorly anonymized and can be inferred using data mining. we guarantee a stronger degree of privacy and a lower expected error caused by large data streams. our privacy preserving model applies laplace's probability distribution function (pdf) to generate random noise. to guarantee privacy for each user, this noise is calculated using the user's data. in addition, it protects not only user profiles but also user activity."
"users spend significant time on sns performing all kinds of activities, such as sending messages, posting, liking posts, disliking posts, performing audio or video calling, and so on. if we consider a user's activity log per single action, it shows active periods vs. non-active periods. if we consider the sending and receiving of messages as an activity, the plot for a particular user's stream of data increases on days where a greater number of messages are sent and/or received, decreases on days where fewer messages are sent or received and remains constant on idle days."
"from table i, we can conclude that, although the potential eavesdropping of the ia network can be effectively disrupted by these two methods, there still exist some drawbacks. first, additional overload will be imposed on the legitimate ia net- work by these methods, in addition to the high requirement of csi and solutions of ia. thus, the ia network should make more effort to combat the eavesdropping through cooperating with the jammers. in addition, the number of antennas equipped at the transceivers of these two schemes increase compared to the original ia network. thus, the jamming signal should be further designed to guarantee the secure transmission of the ia network without its additional help."
"in this section, simulation results are presented to evaluate the performances of the proposed two jamming schemes toward ia networks. assume that the minil algorithm is adopted to calculate the solutions of ia [cit] ."
"as seen in the simulation results in fig 16, the proposed model improves the estimation error while being applied to large-scale data. the model conducts anomaly detection on a subset of the data without disclosing the actual values, which guarantees privacy and reduces the cost of further analyses."
"where a is the slope of the line, represented as the ( change in the yà value change in the x value ). the y-intercept parameter b is the intersection point between the linear line and the y-axis, which is represented as"
"proposition 2: in the case that the beneficial jamming exists, to eavesdrop a certain ia user free of interference and jamming signal, the number of antennas equipped at the eavesdropper should satisfy"
"in summary, as privacy concerns are being raised ever more frequently, several local differential privacy models have been suggested and proven in many application areas for protecting user privacy from untrusted entities."
which shows the quantity of residual jamming signal that is not aligned at the direction of the desired signal. we will use this parament γ to measure the performance of the proposed adversarial jamming scheme in section v.
"then, we consider the situation that the beneficial jamming is generated toward the ia network to prevent eavesdropping. to eavesdrop the information of the kth ia user, the received signal at the eavesdropper through its decoding vectorû [k ] e can be expressed aŝ"
"thus, theorem 2 is proved. from theorem 2, we can know that when the number of antennas at the jammer is no less than the total number of antennas at the k ia receivers, the proposed beneficial jamming scheme is feasible, i.e., the secure transmission of the ia network can be guaranteed by the jamming signal without affecting the transmission rate of the ia users. thus, we can conclude that in the beneficial jamming scheme, the jamming signal will not affect the dofs of the original ia network, when the feasibility conditions can be satisfied."
"in this section, we describe the simulation process, including the dataset, parameters, and evaluation metrics. we explain the setup and discuss the results in the second sub-section. we conducted our experiments by applying the ldp privacy preservation distribution to a set of user activity sequences. [cit]"
"moreover, an increasing number of attacks target personal user information on osns [cit] . thus, there is an urgent need for radical improvements in osn security and privacy measures. most previous studies on the preserved privacy of published data deal only with relational data and cannot be applied to social network data [cit] . therefore, we have taken the initiative toward preserving privacy in social network data. for each user, we use an activity profile to represent his/her sequence of data. with our model, we aim to investigate the application of ldp to user activity logs. in this model, a data collection server uses a specific partitioning of privacy levels to create laplacian random noise. however, not all user data are stored in sn repositories; only a predetermined set is selected amongst the salient points representing the data sequence. on the other hand, the data analyzer sub-model reverses these disrupted points to reconstruct the original stored data received from the repositories. moreover, the data analyzer uses the resulting noisy data to detect anomalous behavior. the data analyzers in the proposed model utilize an extension of the conventional ldp to carry out anomaly detection on reconstructed sn data."
"the data repositories in an sn collect and store everything related to its users. logs may contain the user profiles, activities, and networks of other users and may also store information created without user involvement. sometimes, the sn shares an anonymized version of this information with other parties for different purposes. unfortunately, as several recent incidents have demonstrated, releasing even anonymized graphs may lead to the re-identification of individuals within the network [cit] and the disclosure of confidential information, which has severe consequences for those involved."
"uniform_noisy_stream ( step 5: for the activity dataset, the anomaly detection sub-model extracts the number of communications between pairs of nodes as a bayesian counting process [cit] and represents the number of interactions as weights assigned to communicating nodes in the network. the anomaly detection sub-model then applies bernoulli, markov chain and dirichlet processes to find the nonparametric bayesian inference."
"4. we significantly reduce analysis costs. in our algorithm, only selected data are sent to the detection model, which estimates the data required for classification."
"we calculate the error rate for the combination of uniform-privacy division with linear estimation and adaptive-privacy division regenerated using linear estimation. fig 14 plots the average error rate for the two different approaches on various data sizes. the error rate x d þ is the average of the reconstructed values of the data stream for the same timestamp. we next apply the bayesian anomaly detection technique to the reconstructed stream of users. in this experiment, we detect outliers with respect to the duration of calls between individuals. the duration variables are treated in the same manner as the calling activity described earlier. during the first analysis phase, the model checks all 30 locations for anomalous users to apply the multinomial model with the sequential dirichlet process model with an uninformative negative binomial base measure [cit] ."
"this paper is organized as follows. in section 2, we review related work in ldp privacy preservation in social networks. in section 3, we formulate the problem and demonstrate a threat model. section 4 introduces the scientific framework and preliminaries. the proposed model is explained in section 5, followed by experimental results and a discussion in section 6. we conclude and present our potential future work in section 7."
"in this paper, we have proposed two proactive jamming schemes, which are beneficial and adversarial toward ia networks, respectively. first, in the beneficial jamming scheme, the precoding vector of the jammer is designed to constrain the jamming signal into the same subspace as that of the interference among users at each ia receiver, and thus the potential eavesdropping will be disrupted effectively without affecting the transmission of the ia network. then, in the adversarial jamming scheme, the precoding vector of the jammer is designed to project the jamming signal into the same subspace as that of the desired signal at each ia receiver, which will result in the performance degradation of the ia network without its noticing. plenty of simulation results have been presented to verify the effectiveness of the two proposed jamming schemes."
"ldp is a highly reliable and mathematically rigorous privacy standard [cit] that injects randomized noise into collected data or query results to hide sensitive details in a dataset. thus, regardless of the experience level of an attacker, he/she cannot infer any knowledge from differentially elicited data [cit] ."
"from (12), we can also conclude that the legitimate transmission of the ia network will not be affected by the jamming signal, and secure transmission of the ia network can be guaranteed by the beneficial jamming according to algorithm 1 without performance degradation."
"first, consider the situation that the jamming signal is not generated toward the legitimate ia network. to eavesdrop the information of the kth ia user, the received signal at the eavesdropper through its decoding vector u [k ] e can be expressed as"
"1) with regard to the physical layer security, the jamming signal can be either beneficial or adversarial toward ia networks. thus, in this paper, two proactive jamming schemes are proposed toward ia networks from a novel point of view, to guarantee the secure transmission or to disrupt the transmission, respectively. 2) when a potential eavesdropper may exist, we fully utilize the beneficial aspect of the jamming signal as spurious data to disrupt the eavesdropping. to further ensure that the ia transmission is not affected by the jamming signal, the precoding vector of the jammer is designed to proactively constrain the jamming into the same subspace as that of the interference at each ia receiver, without cooperation of the ia network. 3) on the other hand, when the adversarial aspect of jamming is considered toward ia networks, the ia transmission can be degraded by the jamming signal without being noticed. to achieve this goal, the precoding vector of the jammer is designed to proactively align the jamming signal into the same subspace as the desired signal at each ia receiver, and thus the ia network cannot perceive the jamming. the rest of this paper is organized as follows. in section ii, the system model is presented. the beneficial jamming scheme toward ia networks is proposed in section iii, and the eavesdropping performance is analyzed. in section iv, the adversarial jamming scheme toward ia networks is proposed. in section v, simulation results are presented and discussed, followed by the conclusions and future work in section vi."
"thus, the zero-forcing method should be leveraged by the eavesdropper with enough antennas to eliminate the interference and the jamming signal, and the feasibility condition of the eavesdropper with the existence of jamming signal can be derived in proposition 2."
"in (38), the main purpose of w is to constrain the jamming signal into the same subspace as that of the desired signal at each ia receiver, and the transmission of ia network will be disrupted without noticing. for the arbitrary vector ξ, it will not affect the alignment of the jamming signal, and the effective jamming power at the kth ia receiver is only determined by β [k ], instead of ξ. thus, the value of ξ will not affect the performance of the proposed adversarial jamming scheme."
"where the received jamming signal is treated in the same way as the background noise, due to the fact that the ia receivers cannot notice it."
"step 4: the requesting sub-model receives the noised sp and attempts to reconstruct the stream of data using linear estimation, as explained in the preliminaries section. the submodel uses the linear equation of a straight line to draw segments between every two points. the general equation is"
"2. we achieve an improved estimation error of % 0.15 over direct ldp estimation [cit] . in the direct application of ldp to data, the estimated error is linearly proportional to the size of the data set. since sn data are highly scalable, the direct ldp approach results in relatively high expected error [cit] ."
"in contrast, the data-analyzing server retrieves synthetic data from the repositories, reconstructs the original data streams, and searches the user's activity for abnormal behavior, as demonstrated in fig 3. the privacy standard model in the first sub-model avoids high error rates when applying ldp to large datasets. this model essentially groups salient points that represent similar actions (increasing, decreasing, constant) together, then applies ldp to selected points in these groups. thus, a relatively small number of points are processed."
"the laplacian generated noise depends on the privacy level. therefore, using a uniform distribution generates noise different from adaptive noise. the higher the value of the privacy level, the higher the generated noise is. therefore, it differs from one user to another. knowing that the laplace distribution performs a simple translation, it perfectly fits with the definition of differential privacy. the steps are shown in algorithm 5. since uniform privacy levels are the same, the same pdf generates the noise, whereas, in adaptive privacy partitioning, different pdfs are used to create the noise for each sp. each different pdf incorporates a different privacy level due to dynamic partitioning, and the pdfs' are independently calculated for each sp. step 3: store the 'noised' sp for analytical or other purposes. the repositories contain a sanitized representation of the sp with no indication of the original data."
"step 6: perform individual-based analysis. in this step, we assume n ij (t) to be the adjacency of node i to node j at time t. the increments determine the out-degree and in-degree of node i, and we represent the number of outgoing nodes as"
"thus, in this section, the proactive jamming scheme to combat potential eavesdropping for ia networks is presented first, and then, the performance of the eavesdropping will be analyzed for the proposed scheme."
"to obtain the representative points of a user's data sequence, we take the first order derivative of each value in his/her sequence at a specific timestamp. the user's sequence is represented as values collected at particular time intervals reflecting increasing, decreasing, or constant activity. in fig 4, the user's calling activity is represented as a curve over a ten day time period."
"information sharing platforms, such as online social networks (osns), have experienced remarkable growth and recognition in recent years. notably, osn platforms have direct access to the public and private data of their users [cit] . in some cases, these data are shared with other parties to carry out analytical and social research. although the release of social network data is considered a severe breach of privacy, osn platforms reassure their users by anonymizing their data before sharing it. unfortunately, data mining techniques can be used to infer sensitive information from released data. therefore, it is necessary to sanitize network data before releasing it [cit] ."
"proof: to eavesdrop the transmitted information of the k ia user, the interference and jamming signal should be zeroforced together with (20) and (29) satisfied."
"we apply the bernoulli process and markov chain to all network users, with mean values of [0.63, 0.48] using a threshold of 0.05, to obtain a better understanding of the messaging patterns and their variability. this phase extracts the predictive p-values of the users from their communication patterns, as shown in fig 15. the detection phase of the reconstructed data is the same as that of the original data. the same users have predictive p-values below the threshold and are flagged by the detection sub-model, which implies that the application of ldp to preserve data privacy succeeds in sanitizing the data. in addition, the data structure is maintained for further use by the anomaly detection sub-model."
"in situations where the data sequence is very long (thousands of values), or the data's time intervals are very small (seconds), the set of salient points will be considerably large and in need of further reduction. to achieve this, any successive points belonging to the same movement can be removed. therefore, if three successive points belong to an increasing period, we merge their time interval, retaining only the beginning and end of the interval. we continue this reduction process until no two adjacent time intervals have the same movement. algorithm 2 depicts these steps in detail. remove element at c_list (1, t_min) end while"
"as shown in fig. 1, we consider a legitimate ia network with an adversarial eavesdropper and a friendly jammer. the existence of the eavesdropper will threaten the secure transmission between the legitimate ia transmitters and receivers. the friendly jammer is intended to emit spurious data to disrupt the eavesdropping for the ia network. nevertheless, from the expression of transmission rate for the kth ia user in (5), we can conclude that the jamming signal will also deteriorate the information transmission of the ia users, which should be properly designed to be compatible with the legitimate ia network. specifically, the precoding vector of the jammer should be designed to constrain the jamming signal into the same subspace as that of the interference among ia users at each receiver, as shown in fig. 1, without any additional overload of the ia network. as a result, the jamming signal and interference can be eliminated together by the original decoding matrix at each ia receiver."
"in order to constrain the jamming signal into the same subspace as the interference, the following conditions for the ith ia user should be satisfied as"
"in this section, we describe the proposed scheme for sanitizing sn user activity logs using ldp. we then compare the results of applying anomaly detection to the original and reconstructed data. the model functions on two servers: a data collection server and a data-analyzing server. as shown in fig 2, the data collection server represents each activity log as a data sequence. in each sequence, we determine specific salient points. after selecting these points, we use the user's data in addition to other parameters to create random noise. this noise is then added to the data to distort it from its original value. finally, the data collection server stores it in data repositories."
"is the channel coefficient matrix from the ith ia transmitter to the kth ia receiver, 1 whose elements are independent and identically distributed (i.i.d.) and follow"
"in (9), the jamming signal can be aligned at each ia receiver to the interference from any ia user, but not the interference from all the other users, because the interference from the other users has been aligned into the same subspace according to (3) . the design of the jamming precoding vector w to achieve (7) can be summarized in theorem 1."
"in order to project the jamming signal into the same subspace as that of the desired signal at each ia receiver, the following conditions should be satisfied:"
"to perfectly achieve (11) and (12) through algorithm 1, the feasibility condition of the beneficial jamming scheme should be developed, through which we can determine the minimal number of antennas that should be equipped at the jammer. according to bezout's theorem, we can know that a generic polynomial system is solvable if and only if the number of variables is no less than the number of equations [cit] . thus, the feasibility condition of the beneficial jamming scheme can be derived as theorem 2."
"thus, the zero-forcing method should be utilized by the eavesdropper with enough antennas to eliminate the interference, and the feasibility condition of the eavesdropper can be derived in proposition 1. proposition 1: in the case that no jamming signal exists, to eavesdrop a certain ia user free of interference, the number of antennas equipped at the eavesdropper should satisfy"
"step 6: a sample of size n is selected from the population. the random variable of interest, x, is the number of anomalous individuals in the sample, while m is the number of anomalous individuals in the population, and n is the set of communicating individuals anomaly detection in online social networks"
"given a measurable set s, a base probability distribution h and a positive real number α, the dirichlet process dp(h,α) is a stochastic process whose sample path is a probability distribution over s. for any measurable finite partition of s :"
"a † is the hermitian transpose of matrix a. · is the euclidean norm of a complex vector. cn (a, a) denotes a circularly symmetric complex gaussian distribution with mean a and covariance matrix a. a ∧ b denotes the cross product of a and b. e(·) stands for expectation."
"(5) from (5), we can observe that the transmission rate of ia users will be degraded severely if the jamming signal is not properly removed. the transmission rate will decrease when the transmit power of the jammer increases. for simplicity, all the ia users are assumed to have the same parameters in the rest of this paper, i.e.,"
"the notation x~dp(h,α) indicates that the random variable x is distributed according to the distribution dp(h,α), i.e., according to a dirichlet process with parameter base distribution h and real number α [cit] ."
"theorem 1: to align the jamming signal into the same subspace as that of the interference at each ia user according to (7), the jamming precoding vector w should be designed as"
"when the condition (21) can be satisfied, the interference from other ia users will be eliminated perfectly at the eavesdropper, and the eavesdropping rate will be enhanced greatly. in consequence, the secrecy rate will also be reduced close to zero."
"we consider a k-user ia network, where a potential eavesdropper with n e antennas is intended to wiretap the confidential information from ia links. a jammer with n j independent antennas also exists to transmit spurious data to degrade the transmission of ia network or to disrupt the eavesdropping. m [k ] and n [k ] antennas are equipped at the kth transmitter and the kth receiver of the ia network, respectively. based on these assumptions, the recovered signal at the kth ia receiver can be expressed as"
"in fig 16, the abnormal activities peak on the eighth day, the same day the original activities peak, suggesting that the reconstructed data do not lower the performance of subsequent analyses, which can incorporate all the data into real-time anomaly detection."
"in which the jamming signal and interference from other users can be perfectly eliminated by the original decoding vector of ia. accordingly, the transmission rate of the kth user expressed in (5) can be rewritten as"
each data stream represents a user's calling activity over ten days. each day represents a timestamp. fig 7 illustrates the data sequences (streams) of ten users.
local differential privacy obfuscation (ldpo) is a variation of ldp tailored for iot. ldpo substitutes homomorphic encryption to distill and aggregate data at edge servers with decreased computational overhead. the model is distributed over devices and both edge and cloud servers and provides an accuracy of %90.45 when using 30 features through feature distillation [cit] .
"according to (3) and (6), after generating the jamming signal with the vector w according to theorem 1, the jamming signal can be eliminated together with the interference by the original decoding vector at each legitimate ia receiver."
"the openness of wireless channel makes it more vulnerable to be abused by some vicious organizations to commit crimes. recently, criminals and terrorists can easily establish infrastructure-free wireless communications to commit crimes or terror attack, and ia is an effective way to construct such an illegal multiuser network. therefore, it becomes increasingly important for governmental agencies to monitor the suspiciouŝ"
"in this paper, we consider two opposite aspects, i.e., beneficial and adversarial jamming toward ia networks, and two proactive jamming schemes are proposed correspondingly. first, to guarantee the security of ia network, jamming can be utilized to disrupt the eavesdropping by generating spurious data in section iii, without affecting the legitimate transmission. then, the jamming signal can also be designed to degrade the performance of the ia network without noticing in section iv."
"proof: to eavesdrop the transmitted information of the kth ia user free of interference, the interference from other users should be zero-forced perfectly with (20) satisfied."
"comparing (12) and (40), we can know that, through generating the adversarial jamming signal toward the ia network, the transmission rate of each ia users will be reduced significantly. in addition, when the feasibility condition of the proposed adversarial jamming scheme can be satisfied, the jamming signal will be perfectly aligned into the same subspace as that of the desired signal at each ia receiver, which ensures that the jamming signal cannot be easily perceived by the ia users."
"to further demonstrate the effectiveness of the jamming signal, we analyze the performance of the eavesdropping when the jamming signal is present or not. in the design of the proposed beneficial jamming scheme in section iii-a, the csi from eavesdropper is not needed; while to analyze the eavesdropping performance toward our proposed scheme, the eavesdropping csi is needed, which does not mean that the csi from eavesdropper is needed in our proposed scheme."
"to constrain the jamming signal into the same subspace as that of the desired signal at each ia receiver according to (35), the jamming precoding vector w should be designed as"
"remark 3: 1) we can observe that the proposed beneficial and adversarial jamming schemes toward ia networks are quite similar, due to the fact that in the beneficial jamming scheme, the jamming signal is aligned with the interference at each receiver, while in the adversarial jamming scheme, the jamming signal is aligned with the desired signal at each receiver. therefore, we can exploit similar mechanisms to achieve totally different goals. 2) in the adversarial jamming scheme, the performance of the legitimate ia network will be degraded severely, and the jamming signal will reduce its dofs to zero."
"when ia is performed, the transmission rate of the kth ia user has the same expression as (12) . to deteriorate the ia transmission, we propose an adversarial jamming scheme, through which the jamming signal is constrained into the same subspace as that of the desired signal at each ia receiver, and thus, it is difficult for the ia users to perceive the adversarial jamming signal."
"scientific analysis is known to be vulnerable to the identification of individuals and extraction of private data. however, when a specific breach of privacy was tackled with continuous research and proposed solutions, it was shown that data for analysis might be safely released under differential-privacy guarantees [cit] . since privacy preservation in social networks is a relatively new research area, little work has been produced on the application of ldp to user profile data and activity logs. fig 1 shows the motivational scenario of this research. the sn platform collects a pervasive amount of data and information and immediately stores it in its repositories. the data and information are then shared with the governmental sector under certain agreements. the data may also be shared with analytical parties or even advertising companies to push specifically tailored digital advertisements."
"in (10), the main purpose of w is to align the jamming signal into the same subspace as that of the interference at each ia user, so that the jamming signal can be eliminated perfectly together with the interference between users. thus, the arbitrary vector ζ will not affect the performance of the proposed beneficial jamming scheme, due to the fact that the jamming signal will be eliminated perfectly at each ia receiver, no matter what value ζ is. in addition, since we do not have the knowledge of the eavesdropping csi, we cannot design ζ to disrupt the eavesdropping more effectively."
"3) the value of β in (34) also reflects the transmit power of the jamming signal, which will be shown in the simulation results of section v. thus, we can also manage the value of β to control the transmit power of the jammer. 4) in practical systems, we may want to measure the alignment of the jamming signal and the desired signal at each ia receiver, which reflects the quality in performing the proposed adversarial jamming scheme. thus, we define an indicator as"
craft is an extension of faster r-cnn. it contains the region proposal network (rpn) stream to generated object proposals and the fast-rcnn stream which further assigns a class (including background) score to each proposal.
"pretrained models. similar to the deepid-net setting, the pretrained models are the vgg and googlenet with batch normalization. we only use the vgg in the rpn step and use both models in the later fast-rcnn classification step."
"our still-image object detectors are adopted from deepid-net [cit] and craft [cit] . the two detectors have different region proposal methods, pre-trained models and training strategies."
"our qualitative results have shown that both probabilistic approaches are superior for tracking fibers near tumors or ms lesions with respect to completeness, quality and coverage of anatomical structures at their borders. under the condition that all approaches are parameterized so that they track the same initial number of fibers, the probabilistic approaches are able to compute more fibers that pass two distant crop rois, indicating that fewer fibers were aborted during the fiber tracking process. the variational noise fiber tracking produces qualitatively very similar results compared to the bayesian approach, but is computationally less expensive, thus, enhancing its appeal for clinical applications."
"besides temporal information, contextual information is also a key element of videos compared with still images. although image context information has been investigated [cit] and incorporated into still-image detection frameworks, a video, as a collection of hundreds of images, has much richer contextual information. as shown in fig. 1 (b), a small amount of frames in a video may have high confidence false positives on some background objects. contextual information within a single frame is sometimes not enough to distinguish these false positives. however, considering the majority of high-confidence detection results within a video clip, the false positives can be treated as outliers and then their detection confidences can be suppressed."
"another intuition to improve temporal consistency is to impose long-term constraints on the detection results. as shown in fig. 1 (a), the detection scores of a sequence of bounding boxes of an object have large fluctuations over time. these box sequences, or tubelets, can be generated by tracking and spatio-temporal object proposal algorithms [cit] . a tubelet can be treated as a unit to apply the longterm constraint. low detection confidence on some positive bounding boxes may result from moving blur, bad poses, or lack of enough training samples under particular poses. therefore, if most bounding boxes of a tubelet have high confidence detection scores, the low confidence scores at certain frames should be increased to enforce its long-term consistency."
"the edges along the single rays ensure that all nodes below the surface are included to form a closed set. thereby the interior of the fiber bundle can be separated form the exterior. the edges connecting different rays and planes constrain the set of possible segmentations. the two parameters ∆ x and ∆ z used for edge construction (see e 2 and e 3 ) enforce smoothness and stiffness of the result. the greater the parameters get, the greater is the number of possible segmentations. [cit], creating an optimal segmentation of the fiber bundle, delivering a point set containing a boundary point for each ray of each plane. for comparison and evaluation a closed surface/volume of the segmented fiber bundle is needed. due to the ordering of the point set given by the ordered construction of planes and rays, the point cloud can be triangulated easily. therefore, neighbored contour point sets are triangulated like shown in fig. 9 . for volume construction the triangulated surface can be voxelized."
"in this section, we first introduce the vid task setting (section 3.1) and our overall framework (section 3.2). then each major component will be introduced in more details. section 3.3 describes the settings of our still-image detectors. section 3.4 introduces how to utilize multi-context information to suppress false positive detections and utilize motion information to reduce false negatives. global tubelet rescoring is introduced in section 3.5."
tubelet classification and rescoring. high-confidence tracking and spatial max-pooling generate long sparse tubelets that become candidates for temporal rescoring. the main idea of temporal rescoring is to classify tubelets into positive and negative samples and map the detection scores into different ranges to increase the score margins.
"the quantitative results in combination with the qualitative results have shown that the probabilistic fiber tracking is more sensitive than the deterministic approach, especially if measuring the adc values. the statistically significant interaction effect for adc values between the algorithm used (probabilistic/deterministic) and the health status results from the fact that on one level of the between-subjects factor (healthy volunteers) the algorithm used has no influence on the adc scores, on the other level (patients) it influences the values. one can interpret this effect as a brain anatomy related effect of the algorithms used to generate the adc values. the normal or more ideal brain anatomy of healthy volunteers allows less differentiation between the methods than does the pathological brain anatomy of patients. for quantification, we concentrated on one important fiber structure, the slf, however, samples of other structures have shown similar results. it is advisable to combine the quantitative and qualitative results to obtain an overall picture. for example, some ms patients could not be added to the quantitative analysis because only the probabilistic algorithm is able to produce processable results. this indicates that in clinical cases with brain lesions or neuronal diseases, the probabilistic algorithm is the method of choice. although [cit], this field of research should be examined in the future as probabilistic approaches are still an order of magnitude slower than deterministic solutions."
"in this section, we have described the creation of realistic dti software models. these can be used as ground truth to test various fiber tracking algorithms. a first quantitative analysis of the advection-diffusion based fiber tracking algorithm suggests that, in the considered experiment, the first fibers leave the modeled bundle after approximately 20mm and that a safety margin of 2-3mm seems appropriate. future work includes analyzing the precision of the algorithm in the presence of kissing or crossing fibers. we would also like to systematically analyze how precision varies in relation with the underlying image data (testing different values for image noise or artifacts, thickness of the fiber bundle, fractional anisotropy of the tensors) or in relation with fiber tracking parameters (such as step length, density of seed points). moreover, fiber tracking results should be compared with those of other approaches, such as for example probabilistic ones. in the following section 4.5, we suggested an algorithm to estimate the extent of a fiber bundle based on the tracked fibers and the underlying image data. the algorithm basically relies on dilating the tracked fibers if threshold conditions on voxel distance, fa, bd, and main diffusion direction difference are met. results are visualized as semi-transparent hulls around the tracked fibers. the algorithm was tested both on one of our dti phantoms and on a real magnetic resonance dataset. as with every parameter-dependent algorithm, the question of the optimal set of parameters arises, which shall be dealt with in the future. ultimately, this work should help clinicians in better understanding the precision of generated fiber tracking results."
"for each frame, we have about a few hundred region proposals, each of which has detection scores of 30 classes. for each video clip, we rank all detection scores on all boxes in a descending order. the classes of detection scores beyond a threshold are regarded as high-confidence classes and the rest as low-confidence classes. the detection scores of the high-confidence classes are kept the same, while those of the low-confidence classes are suppressed by subtracting a certain value. the threshold and subtracted value are greedily searched on the validation set."
"in this section, we start by introducing the general framework that we use to generate the diffusion tensor model. next, we provide details on how we model different tissues and white matter pathways. after an accuracy analysis of the employed fiber tracking algorithm, we conclude by suggesting an algorithm to construct safety hulls around the tracked fibers."
"multi-context suppression (mcs). one limitation of directly applying still-image object detectors to videos is that they ignore the context information within a video clip. the detection results in each frame of a video should be strongly correlated and we can use such property to suppress false positive detections. we observed that although video snippets in the vid dataset may contain arbitrary number of classes, statistically each video usually contains only a few classes and co-existing classes have correlations. statistics of all detections within a video can therefore help distinguish false positives."
"to each fraction of tissue in a voxel we assign a main diffusion direction and the eigenvalues of the cylindrically symmetric diffusion tensor. the resulting signal attenuation is then computed according to equation 1. [cit], from which we derive the eigenvalues for our model written in table 3 . in case we do not model one or more white matter tracts to go through a voxel v, we assign a table 3 . t 2 values and tensor eigenvalues used in the brainweb-based model for the different tissues."
"since the official test server is primarily used for competition and has usage limitations, we primarily report the performances on the validation set as a common convention for object detection tasks. [cit] are reported."
"random main diffusion direction to each tissue portion present in v. however, we let the main diffusion directions corresponding to a given tissue type vary smoothly in space, in order to have, at least locally, a realistic change in tensor orientation. otherwise, if v has a white matter tissue portion and there are one or more fiber bundles going through it, the main diffusion direction depends on these bundles. details on the modeling of fiber bundles and on setting the main diffusion direction are given in the following section 4.3."
multi-context suppression. this process first sorts all stillimage detection scores within a video in descending orders. the classes with highly ranked detection scores are treated as high-confidence classes and the rest as low-confidence ones. the detection scores of low-confidence classes are suppressed to reduce false positives.
"motion-guided propagation. in still-image object detection, some objects may be missed in certain frames while detected in adjacent frames. motion-guided propagation uses motion information such as optical flows to locally propagate detection results to adjacent frames to reduce false negatives."
"temporal tubelet re-scoring. starting from high-confidence detections by still-image detectors, we first run tracking algorithms to obtain sequences of bounding boxes, which we call tubelets. tubelets are then classified into positive and negative samples according to the statistics of their detection scores. positive scores are mapped to a higher range while negative ones to a lower range, thus increasing the score margins."
"despite their effectiveness on still images, these stillimage object detection frameworks are not specifically designed for videos. one key element of videos is temporal information, because locations and appearances of objects in videos should be temporally consistent, i.e. the detection results should not have dramatic changes over time in terms of both bounding box locations and detection confi- dences. however, if still-image object detection frameworks are directly applied to videos, the detection confidences of an object show dramatic changes between adjacent frames and large long-term temporal variations, as shown by an example in fig. 1 (a) . one intuition to improve temporal consistency is to propagate detection results to neighbor frames to reduce sudden changes of detection results. if an object exists in a certain frame, the adjacent frames are likely to contain the same object at neighboring locations with similar confidence. in other words, detection results can be propagated to adjacent frames according to motion information so as to reduce missed detections. the resulted duplicate boxes can be easily removed by non-maximum suppression (nms)."
"hyperparameter settings. for motion-guided propagations, as described in section 3.4, table 4 shows the performances of different propagation window sizes. compared to directly duplicating boxes to adjacent frames without changing their locations according to optical flow vectors, mgp has better performances with the same propagation durations, which proves that mgp generates detections with more accurate locations. 7 frames (3 frames forward and 3 backward) are empirically set as the window size."
"in the following sections, we will go into more detail and will give several examples. section 3 describes how uncertainty due to different tracking algorithms can be visualized and quantified. the next section will illustrate a software phantom for estimating the boundary of tracked fiber bundles. section 5 summarizes an alternative algorithm for computing a safety hull around the fibers. finally, we present a new algorithm for visualizing the uncertainty of the reconstruction by the fiber orientation distribution function (fodf), which is a probability distribution on a sphere."
"the multi-context suppression process can significantly reduce false positive detections, but cannot recover false negatives. the false negatives are typically caused by several reasons. 1) there are no region proposals covering enough areas of the objects; 2) due to bad pose or motion blur of an object, its detection scores are low."
"over the last years, diffusion imaging techniques like dti, dsi or q-ball received increasing attention, especially in the neuroimaging, neurological, and neurosurgical community. an explicit geometrical reconstruction of major white matter tracts has become available by fiber tracking based on diffusion-weighted images. the goal of virtually all fiber tracking algorithms is to compute results which are analogous to what the physicians or radiologists are expecting and an extensive amount of research has therefore been focussed on this reconstruction. however, the results of fiber tracking and quantification algorithms are approximations of the reality due to limited spatial resolution (typically a few millimeters), model assumptions (e.g., diffusion assumed to be gaussian distributed), user-defined parameter settings, and physical imaging artifacts resulting from diffusion sequences. in this book chapter, we will address the problem of uncertainty in diffusion imaging and we will show possible solutions for minimizing, measuring and visualizing the uncertainty. the possibility of fiber tracking (ft) [cit], monitoring the progression of diseases such as amyotrophic lateral sclerosis (als) or multiple sclerosis (ms) [cit] . several studies have shown that modified values of fractional anisotropy (fa), relative anisotropy, or diffusion strength (adc) are indicators of diseases that affect white matter tissue. ms lesions have been investigated by roi-based analysis and voxel-wise fa comparisons by which fa changes have been shown to occur in areas containing lesions and in areas of normalappearing white matter. moreover, methods for tract-based quantification have been developed for which parameters are computed depending on the local curvature or geodesic distance from a user-defined origin. [cit] allow for tracking in regions of low anisotropy and are also used to provide a quantitative measure of the probability of the existence of a connection between two regions. these approaches aim at visualizing the uncertainty present 1 in the data by incorporating models of the acquisition process and noise. the uncertainty is assessed by tracking many possible paths originating from a single seed point and by taking the tensor uncertainty into account. [cit] ."
"the contribution of this works is three-folded. 1) we propose a deep learning framework that extends popular stillimage detection frameworks (r-cnn and faster r-cnn) to solve the problem of general object detection in videos arxiv:1604.02532v2 [cs.cv] 19 [cit] by incorporating temporal and contextual information from tubelets. it is called t-cnn, i.e. tubelets with convolution neural network. 2) temporal information is effectively incorporated into the proposed detection framework by locally propagating detection results across adjacent frames as well as globally revising detection confidences along tubelets generated from tracking algorithms. 3) contextual information is utilized to suppress detection scores of lowconfidence classes based on all detection results within a video clip. [cit] . code is available at https://github.com/myfavouritekk/t-cnn."
"spatial max-pooling. after tracking, for each class, we have tubelets with high-confidence anchors. a naive approach is to classify each bounding box on the tubelets using stillimage object detectors. since the boxes from tracked tubelets and those from still-image object detectors have different statistics, when a still-image object detector is applied to a bounding box obtained from tracking, the detection score many not be accurate. in addition, the tracked box locations may not be optimal due to the tracking failures. therefore, the still-image detection scores on the tracked tubelets may not be reliable."
"in multi-context suppression, classes in the top 0.0003 of all the bounding boxes in a video are regarded as highconfidence classes and the detection scores for both frameworks are subtracted by 0.4. those hyperparameters are greedily searched in the validation set."
"for simplicity we choose catmull-rom splines, which are defined by two points p i, p i+1 and two tangent vectors t i, t i+1 . the tangent vectors are computed by"
"in addition to model finetuning, we also investigated the data configurations for training the svms in deepid-net. the performances are shown in table 3, which show that using positive and negative samples from both det and vid data leads to the best performance. because of the redundancy among video frames, we also sampled the video frames by a factor of 2 during testing and applied the still-image detectors to the remaining frames. the mcs, mgp and re-scoring steps in section 3.4 and 3.5 are then conducted. the detection boxes on the unsampled frames are generated by interpolation and mgp. we did not observe significant performance differences with frame sampling on the validation set."
"pre-trained models. [cit] has two tracks for each task. 1) [cit] datasets including classification and localization (cls), det, vid and places2. 2) for the external data track, one can use additional data and annotations."
"object region proposals. for deepid-net, the object region proposals are obtained by selective search (ss) [cit] and edge boxes (eb) [cit] with a cascaded selection process that eliminates easy false positive boxes using an imagenet pretrained alexnet [cit] model. all proposal boxes are then labeled with 200 imagenet detection class scores by the pre-trained alexnet. the boxes whose maximum prediction scores of all 200 classes are lower than a threshold are regarded as easy negative samples and are eliminated. the process removes around 94% of all proposal boxes while obtains a recall around 90%."
"network configurations. the models in deepid-net and craft are mainly based on googlenet with batchnormalization layers and vgg models. the techniques of multi-scale [cit] and multi-region [cit] are used to further increase the number of models for score averaging in the stillimage object detection shown in fig. 2 . the performance of a baseline deepid-net trained on imagenet det task can be increased from 49.8 to 70.7 with all the above-mentioned techniques (data configuration for finetuning, multi-scale, multi-region, score average, etc.)."
"we have given several examples in the context of diffusion neuroimaging where quantification techniques play an important role and have presented and discussed software and hardware phantoms for measuring their precision and reliability. without such evaluation basis, several pitfalls and systematic errors might remain undetected."
"in this framework, we use the enhanced version of faster-rcnn by cascade rpn and cascade fast-rcnn. in our cascaded version of rpn, the proposals generated by the rpn are further fed into a object/background fast-rcnn. we find that it leads to a 93% recall rate with about 100 proposals per image. in our cascade version of the fast-rcnn, we further use a classwise softmax loss as the cascaded step. it is utilized for hard negative mining and leads to about 2% improvement in mean ap."
"technical challenges like improved spatial resolution, whole brain coverage, signal to noise ratio, or magnetic susceptibility artifacts constitute the basis for reliable quantification techniques in diffusion neuroimaging. for example, high-resolution 3d imaging sequences facilitated by parallel imaging will strongly contribute towards quantitative reliability. still, in most cases, partial volume modeling will be key to yield highly reliable quantitative measurements due to the complexity or small spatial extent of both anatomical features and pathological alterations. [cit] . furthermore, preprocessing algorithms for registration, regularization, or outlier rejection are substantial influencing factors."
"the remaining white matter is modeled as having a random direction. in case there are several fibers which contribute to a voxel, we generally proceed as above, with the difference that we may have to rescale each contribution by the sum of all contributions, so that the latter sum is less or equal to one (100%). the main diffusion direction e 1 of a fiber at r is computed as a weighted sum of the vector lines ∆ i :"
"there have also been works on object localization and co-localization [cit] . although such a task seems to be similar, the vid task we focus on is actually much more challenging. there are crucial differences between the two problems. 1) goal: the (co)locolization problem assumes that each video contains only one known (weakly supervised setting) or unknown (unsupervised setting) class and only requires localizing one of the objects in each test frame. in vid, however, each video frame contains unknown numbers of objects instances and classes. the vid task is closer to real-world applications. 2) metrics: localization metric (corloc [cit] ) is usually used for evaluation in (co)locolization, while mean average precision (mean ap) is used for evaluation on the vid task. with the above differences, we think that the vid task is more difficult and closer to real-world scenarios. the previous works on object (co)localization in videos cannot be directly applied to vid."
"these false negatives can be recovered by adding more detections from adjacent frames, because the adjacent frames are highly correlated, the detection results should also have high correlations both in spatial locations and detection scores. for example, if an object is still or moves at a low speed, it should appear at similar locations in adjacent frames. this inspires us to propagate boxes and their scores of each frame to its adjacent frame to augment detections and reduce false negatives."
"top-k count fig. 5 . tubelet classification. tubelets obtained from tracking can be classified into positive and negative samples using statistics (e.g. topk, mean, median) of the detection scores on the tubelets. based on the statistics on the training set, a 1-d bayesian classifier is trained to classify the tubelets for re-scoring. detection bounding boxes according to the motion information. for each region proposal, we calculate the mean optical flow vector within the bounding box of the region proposal and propagate the box coordinate with same detection score to adjacent frames according the mean flow vectors. an illustration example is shown in fig. 4 ."
"in order to generate a synthetic tensor field, we start by computing a set a of diffusionweighted (dw) images (one image for each corresponding gradient direction). [cit] . this model contains a hindered extra-axonal compartment as well as a restricted intra-axonal compartment. we restrict ourselves to the hindered model, which gives rise to an effective diffusion tensor and primarily explains the gaussian signal attenuation observed at low b values. let us denote the diffusion time by ∆ and set"
"in the last two years, the performance of object detection has been significantly improved with the success of novel deep convolutional neural networks (cnn) [cit] and object detection frameworks [cit] . the stateof-the-art frameworks for object detection such as r-cnn [cit] and its successors [cit] extract deep convolutional features from region proposals and classify the proposals into different classes. deepid-net [cit] improved r-cnn by introducing box pre-training, cascade on region proposals, deformation layers and context representations. recently, imagenet introduces a new challenge for object detection from videos (vid), which brings object detection into the video domain. in this challenge, an object detection system is required to automatically annotate every object in 30 classes with its bounding box and class label in each frame of the videos, while test videos have no extra information pre-assigned, such as user tags. vid has a broad range of applications on video analysis."
"one of the major hurdles when developing fiber tracking algorithms is that hardware or software models of fiber bundles are needed in order to asses their validity and precision. it is therefore necessary to develop phantoms with a known fiber network. software models have the advantage that they can be easily modified to account for different scanner parameters, image noise or artifacts. while much of the previous work has focused on simple phantoms in which fiber bundles were represented as cylindrical tubes or helices [cit], in this section we suggest a framework in which it is possible to realistically model specific neural fiber bundles, simulating both the smooth transition between the actual white matter pathway and the surrounding tissue and the partial volume effects caused by the possible contemporary presence of white matter, grey matter and cerebrospinal fluid in one voxel. we focus on generating a phantom of the corticospinal tract. afterwards, we reconstruct the modeled tract by means of a fiber tracking algorithm and make a quantitative analysis of the algorithm's accuracy. this information is used to estimate what an appropriate safety margin around the tracked fibers should be and to analyze after which length the first fibers start to leave the modeled fiber bundle. lastly, we suggest an efficient algorithm to construct safety hulls around the tracked fibers."
".m]) and two additional nodes v sink and v source . the construction of weighted edges consists of different steps and is partly based on a cost function c(v i,j,k ) for every node v i,j,k . the used scalar cost function c (v i,j,k ) is derived from the scalar fractional anisotropy maps of the underlying tensor data:"
"however, the detections spatially close to the tubelets can provide helpful information. the spatial max-pooling process is to replace tubelet box proposals with detections of higher confidence by the still-image object detector."
"mgp generates short dense tubelets at every detection by our still-image detectors. it significantly reduces false negatives but only incorporates short-term temporal constraints and consistency to the final detection results. to enforce long-term temporal consistency of the results, we also need tubelets that span long periods of time. therefore, we use tracking algorithms to generate long tubelets and associate still-image object detections around tubelets."
"we investigated several training data configurations by finetuning a googlenet with bn layers. from the table 1 and 2, we can see that the ratio of 2 : 1 between det and vid data has the best performance on the still-image detector deepid-net and craft single models, therefore, we finetuned all of our models using this data configuration."
"in the case of quantitative dti, the assumption of a gaussian diffusion process may not be adequate in areas of complex fiber structures like crossing or kissing fibers not only for fiber reconstruction but also for quantitative assessment. this problem has recently been addressed by multiple-compartment models, diffusion spectrum imaging, spherical deconvolution and persistent angular structure mri (pas-mri), where higher order tensors or probability distributions describe the actual diffusion process. [cit] have already shown that with q-space imaging the difference of values in the normal appearing white matter of patients with multiple sclerosis is more pronounced than with dti. however, virtually all techniques based on hardi data are still in an early state and are subject to improvement with respect to acquisition and postprocessing time so that they become useful for clinical routine."
"the imagenet object detection from video (vid) task is similar to the object detection task (det) in still images. it contains 30 classes to be detected, which are a subset of 200 classes of the det task. all classes are fully labeled in all the frames of each video clip. for each video clip, algorithms need to produce a set of annotations (f i, c i, s i, b i ) of frame index f i, class label c i, confidence score s i and bounding box b i . the evaluation protocol for the vid task is the same as the det task, i.e. we use the conventional mean average precision (mean ap) on all classes as the evaluation metric."
"since the input only contains the original detection scores, the features for tubelet classification should also be simple. we tried different statistics of tubelet detection scores such as mean, median and top-k (i.e. the kth largest detection score from a tubelet). a bayesian classifier is trained to classify the tubelets based on the statistics as shown in fig. 5, and in our experiment, the top-k feature works best. after classification, the detection scores of positive samples are min-max mapped to [0.5, 1], while negatives to [0, 0.5]. thus, the tubelet detection scores are globally changed so that the margin between positive and negative tubelets is increased."
where δ denotes the dirac-delta distribution and α is a parametrization variable. to model the non-constant fiber density we convolve the fiber trajectory t(r) with a kernel k(r):
the remaining parameters used to compute the diffusion weighted images according to equation 1 are reported in table 4 . we select a region at the level of the internal capsule to start
"for example in fig. 3, in some frames from a video clip, some false positive detections have very large detection scores. only using the context information within these frames cannot distinguish them from the positive samples. however, considering the detection results on other frames, we can easily determine that the majority of high-confidence detections are other classes and these positive detections are outliers."
"here γ is the proton gyromagnetic ratio, g is the vector whose magnitude is the strength of the applied diffusion gradient and whose direction is along the axis of the applied diffusion gradient, δ is the width of the diffusion pulse gradient. in this case, the net signal attenuation is given by"
"data configuration. we investigated the ratio of training data combination from the det and vid training sets, and its influence on the still-image object detector deepid-net. the best data configuration is then used for both deepid-net and craft. because the vid training set has many more fully annotated frames than the det training set, we kept all the det images and sampled the training frames in vid for different combination ratios in order to training the still-image object detectors."
"in our inclusion criterion, the bd threshold is mainly used to differentiate between cerebrospinal fluid, tumor tissue, and regions of white or grey matter. on the other hand, fa has been shown to be highly heterogeneous in normal brain parenchyma and may be used as a criterion to differentiate between different white matter tracts. [cit] . as far as the connected-component analysis step of the algorithm is concerned, we make use of 6-connectivity in 3d and discard groups of voxels with a volume smaller than 50ml."
"to conclude, we sampled vid frames to half the amount of det images and combined the samples to finetune the cnn models in both deepid-net and craft. positive and negative samples from both det and vid images are used to train svms in deepid-net."
"a picture of the new greatspn gui is shown in figure 3, taken while editing a petri net model. in the upper-left panel, there is the list of open files. the editor supports multi-page file. in the current version of the editor, pages can be of three types: petri net models, deterministic timed automaton models (to be discussed later) and tables of measures. new model formalisms can be added to the editor by specifying new types of pages. the property panel is in the lower-left corner. it shows the editable properties of the selected objects. it is possible to operate to more than one object, of the same type, at a time. the central canvas contains the editor of the selected project page, in this case a gspn model."
"we proposed and demonstrated an automated parking and charging application for a model electric vehicle. apart from the vehicle functionality, logistics and infrastructure support by a back-end responsible for global scheduling of resources has been integrated. for mission-control and monitoring purposes, v2i communication is deployed."
"the analysis of colored models can proceed in two directions: (1) through the unfolding of the colored net (now available with one click through the new gui) into a (usually much larger) equivalent net without colors that can be processed using the solvers illustrated earlier, or (2) by direct analysis of the colored model using ssn specific solvers. table 1 shows the number of places and transitions in the unfolded version of model depicted in fig. 13 for different sizes of color class c. the table reports also the sizes of the ordinary reachability graph (rg), and that of the symbolic reach-configuration with asymmetric number of spares and color independent/dependent transition rates in ew 3 ability graph (srg and esrg). in a symbolic rg, multiple markings are lumped together into a single symbolic marking. srg and esrg have two different criteria for this marking aggregation. greatspn can generate both the ordinary rg, equivalent to the rg of the unfolded model, and a more compact symbolic rg for any sn model. the latter exploits model symmetries and aggregates equivalent states. a ctmc can be derived both from the rg and from the srg, on which both transient and steady state analysis can be performed. extended and dynamic srg [cit] (esrg and dsrg) are also available, to efficiently deal with partially symmetric systems. finally a simulator allows to compute estimates of steady state performance measures (with confidence intervals). the simulator supports both the plain colored marking representation and the symbolic one (which can improve the efficiency of future event list handling [cit] )."
"note that the automaton in figure 10 accepts paths depending only on the visited markings. since there are no specific requirements for actions associated to arcs, the automaton of figure 11 only accepts a path based on the transitions that occurs along that path (the atomic proposition associated to location is always the clause \"true\"). there is another less common use of probabilistic verification that can be very useful. as previously discussed, there are certain ctl formulas which are false due to the presence of some anomalous behaviour, like infinite executions in which a machine never breaks down. typically these executions are not realistic. indeed the property \"agaf en(goready) was shown to be false. we can define a similar property in csl ta through the automaton of figure 12, which accepts all paths in which transition goready fires at least once. the csl ta model-checker reveals that, for all states, the set of paths accepted by the timed automaton has probability 1, clearly indicating that in a probabilistic setting the path(s) that makes the ctl formula false are negligible. this is indeed an example on the importance of having in a single tool both qualitative and probabilistic model checking. greatspn supports also a formalism in the class of high level petri nets, namely stochastic symmetric nets 3 (ssn) [cit] . the new greatspn gui supports the design of ssn models as well as the interactive simulation with the colored token game. it also includes the unfolding function that generates a gspn model whose behavior is equivalent to that of the ssn. figure 13 shows a variation of the fms model, enriched with colors, and expressed through the ssn formalism. the model definition includes a set of finite basic color classes, and set of place color domains which result from the cartesian product of basic color classes and define the possible colors of the tokens in each place. transition color domains define the possible color instances of each transition. the arc functions define the multiset of tokens withdrawn from or added to the input and output places by a given transition instance. colors may be useful in different situations, like when there is a need to identify a specific token among a set of tokens residing in the same places (e.g. to compute first passage time distributions [cit] ). this differentiates the qualitative behavior of some entities (hence making the marking evolution to depend on colors) or the stochastic delays of the activities involving specific entities (hence defining color dependent transition rates). in general colored models are more compact and can highlight symmetries in the model."
"the development of the generalized stochastic petri net (gspn) formalism [cit] was motivated by the modeling power of spns, with their effectiveness and simplicity [cit] . a prototype solver [cit] was initially jointly developed by members of the computer science department of the university of torino and of the electronics department of the politecnico of torino with the simple aim of overcoming the tedious and error prone task of manually constructing the markov chains underlying gspn models. starting from the insights gained from the experience of using this preliminary tool, it was decided to design and implement a complete framework for the modeling, verification, and solution of gspn models. the first version of the framework [cit], written in pascal [cit], and targeted three platforms: the vax 11/780 with vms, the vax 11/780 with bsd unix 4.1 and sun 1 workstation with bsd unix 4.2. this was the first documented software package for the analysis of gspn models [23, p. 29] ."
"the suite of tools has been developed over numerous years and it is now very rich. it is not always easy to find the right solver to use and how to use it, so the wide choice of available solvers is not very easy to use. we believe that the cottbus suite is facing a situation similar to that we experienced in greatspn before deciding to re-write the gui and to link from the gui all the available solvers. the cottbus suite puts emphasis on structural analysis and on the richness of petri net extension the tool is able to deal with. great attention is devoted to stochastic simulation of petri nets of biological system. features available in greatspn and not in the cottbus suite include csl ta model-checking and efficient solution of colored models."
"although the primary use case of the mobile platform is as a demonstrator for the automated parking and charging proposed by the v-charge project, it can also be seen as a versatile research and education platform for embedded and vehicular computing. the sbc, for instance, is able to run the opencv framework which can be used for several tasks like navigation. additionally, different communication modules can be attached to the sbc, for example ieee 802.15.4 radios as in a previously demonstrated disaster-recovery scenario [cit] . the charging mechanism allows even long running automated experiments without requiring user interaction for changing batteries."
even the simplest gspn models that one can conceive are difficult to describe and analyze without the use of proper software tools. the development and the free distribution of such a tool to academic researchers was indeed a key factor in spreading the knowledge and the use of gspns within the performance and reliability research communities.
"greatspn3.0 has many features that can find in similar software packages for the analysis of petri-net based models. a full comparison of these tools would require a chapter on its own. in this section we only provide a brief overview of a few other tools that share some of the features of greatspn3.0 with hints on similarities and differences. two characteristics that are unique to greatspn and that will not be listed explicitly as differences are the availability of a csl ta model-checker and of solution techniques based on symmetries for colored petri nets. vice-versa, some of the tools listed below support compositionality through hierarchical models: this is a feature that is not present in greatspn."
"step 2: computational tree logic (ctl) model checking more sophisticated properties of the net can be investigated through the ctl [cit] model checker provided by the tool. the user defines one or more \"ctl measures\" in the measure panel, as shown in figure 6 . the analysis is performed for figure 6 ) is devoted to establish by which methods the bounds and the variable ordering have to be computed. the syntax of the ctl operators is the classical one, with a and e standing for \"for all paths\" and \"there exists a path\". operators g and f stand for \"for all states in the path\" and \"it exists a state in the path\". the term #p means \"number of token in place p\", while condition en(t) means \"transition t is enabled\". the panel displays the truth value of each formula (computed in the initial state), but a log is available with more detailed information, including counter-examples or witnesses, whenever feasible. the properties listed in figure 6 allow to investigate, from the top of the list downwards, more and more detailed aspects of the system behaviour."
"sparebroken m : for an early check of the qualitative behavior of the colored model in the design phase, it is possible to use the colored token game feature of the gui, which in any given marking highlights all the transitions that have at least one enabled instance. clicking on one such transition a list of enabled instances pops up so that the modeler can choose which one to fire, leading to a new colored marking. the trace of markings reached along the interactive simulation is shown, and the user can return to one of the visited markings by clicking on it to try another path originating in that marking. during this initial phase the transition timing can be taken into account allowing the gui to generate random delays to be associated with transitions enabled in a given marking."
"gspns are based on very few (and simple) primitive constructs that with their precise semantics make the formalism easy to learn and apply to many interesting practical problems. indeed, researchers with considerably different backgrounds found gspns easy to grasp and useful for quickly drawing and analyzing complex probabilistic models that would have been otherwise difficult to construct in a reliable manner. despite the need for more powerful formalisms for a compact representation of complex real systems, the choice of keeping the formalism simple while delegating to a different modelling language (namely stochastic well formed nets -swns) the burden of dealing with these possible additional complexities allowed many newcomers to become quickly acquainted with gspns without scaring them away with less intuitive definitions. on the other hand, researchers already familiar with the features of the basic formalism found quite interesting the possibility of using with little additional effort the more complex high-level extensions, as they realized that this additional complexity pays off when it is actually needed by the difficulty of the problem at hand."
"in this demo, we present a model electric car performing automated driving and charging controlled by a central backend server. adhering closely to the actual v-charge system, the server is responsible for global scheduling of resources, sending high-level commands, while the vehicle performs all other tasks automatically. the v2i communication over a wlan link is based on the dds middleware, resembling the real project. the car automatically maneuvers to the assigned destination, using the described line following and marker detection. the server application is constantly provided with status updates of the vehicle's position."
"petri nets (pn) [cit] are a natural, simple, and powerful formalism aimed at the modelling of the information and control logics in systems with asynchronous and concurrent activities. in stochastic petri nets (spn) [cit] all the transitions are assumed to fire with a random delay that is exponentially distributed. this feature enriches the analysis of a variety of systems by computing several quantitative indices on their efficiency (performance) and reliability. usually this is achieved by automatically constructing a continuous time markov chain (ctmc) which reflects the behavior of the system and then applying the analysis methods available for this type of stochastic processes."
"smart. smart [cit] or stochastic model-checking analyzer for reliability and timing is a software package providing command-line environment for logic and probabilistic analysis of complex systems. its main input formalism are stochastic petri nets and both discrete-time and continuous time markov chains. for the analysis of logical behaviour, both explicit and symbolic state-space generation techniques are available. in the new release, currently under development, all the symbolic algorithms will be based on meddly library as in greatspn. for the study of stochastic and timing behaviour, sparse-storage, symbolic and kronecker numerical solution approaches are available when the underlying process is a markov chain. discreteevent simulation is also provided."
"unlike the previous graphical interface, the check of the syntax of the colored definition is done while the definition is written. the editor also supports fluid places and fluid transitions (not shown in the example). places can be partitioned into labeled groups for kronecker-based solutions [cit] . the editing process supports all the common operations of modern interactive editors, like undo/redo of every action, cut/copy/paste of objects, drag selection of objects with the mouse, single and multiple editing of selected objects, etc. petri net models are drawn entirely using vector graphics, which allows for high quality visualization and print of the net. object labels may be drawn with an optional l a t e x engine. the interface is designed to avoid modal dialog windows as much as possible to streamline the use of the gui."
"in models of real systems, it is often the case that a change of state occurs not only because there has been a completion of an activity which takes time, but also because there has been a change of some logical conditions, which may depend on the current state of the system in a rather intricate manner. these two types of events may have rather different durations, and modelling these type of systems with spns yields ctmcs with quite different transition rates, making the numerical analysis of the model very difficult."
"starting from the practical observations, generalized stochastic petri nets (gspn) [cit] were proposed. immediate transitions were introduced to answer the need for events that happen in a very short time (actually zero), it was also chosen that immediate transitions have priority over timed ones (the transitions that fire after a non-negligible amount of time). priorities were introduced to simplify the analysis, by splitting markings in \"vanishing\" markings (states in which at least one immediate transition is enabled and where therefore the net does not spend any time) and \"tangible\" markings (states in which only timed transitions are enabled and where the net does spend time). soon after their introduction, gspns became very popular in the performance and reliability evaluation community. the reason for this unexpected success was probably due to three quite different reasons: the simplicity of the formalism, the presence and the role of immediate transitions, and the availability (not much later than the formalism definition) of a design and analysis tool."
we have developed two versions of line sensors. the first one consists of eight reflective optical sensors which are mounted close to the ground. they are attached to an eight bit microcontroller unit (mcu) which samples the data of the sensors and transfers the measurements to the sbc. line segments of the road network are identified via rfid tags. an rfid reader is placed on the same printed circuit board (pcb) as the optical sensors.
"in this chapter we review 30 years of history of greatspn and discuss its current role for model-based analysis: we revisit how the graphical interfaces have evolved over the years, and we also revisit many of the advances in stochastic petri net analysis and what is the current status in model-based stochastic analysis."
"the chapter starts with a review of the gspn formalism: its roots and its evolution (section 2), followed by the history of the greatspn tool in section 3. the rest of the paper shows the current value of greatspn for model-based stochastic analysis. the tool, as it is now, is presented in section 4. section 5 describes how greatspn3.0 supports the workflow of a model-based analysis of a target system: from model construction to validation through model-checking and evaluation using stochastic model-checking and standard performance evaluation techniques. a similar workflow is illustrated for the colored case (section 6). the common reference example is inspired by the various flexible manufacturing system models available in the literature. the chapter ends with a literature survey of tools with similar characteristics (section 7) followed by a summary of the status of greatspn3.0 and of its desirable future (section 8)."
"to enable the automated valet parking as described in section i, vehicle-to-infrastructure (v2i) communications are required to provide the vehicle with mission information and to aggregate vehicle sensor data. the server is responsible for the scheduling of parking and charging resources. all in-vehicle modules of the actual v-charge vehicles, such as sensors, localization and on-board planning, as well as the serverside components (e.g., path planner, parking management system) share data using the data distribution service (dds) middleware. on the wireless link between server and vehicles, however, dds has some drawbacks such as lack of disruptiontolerance and of multi-hop support. thus, dds is used both on the vehicle and the server side, while a disruption-tolerant network (dtn) is used for transporting dds data on the wireless link, as depicted by figure 3 . in the demonstrator, the dtn transport can be disabled, however, to enable end-toend dds transmissions over a wlan link."
"one of the distinctive features of greatspn with respect to other tools is the willingness of its development team to maintain, in the stochastic extension, the basic semantics of transition enabling and firing as well as the relevance of the graphical information. the underlying petri net formalism is that of place/transition nets with priorities and inhibitor arcs, which have a comprehensive graphical representation of the net behaviour. the idea was to try to diverge as little as possible from the underlying petri net formalism, so as to be able to reuse all possible solution techniques available for classical (non-stochastic) petri nets. this choice enhanced the anlytical power, but certainly decreased the modelling power, since certain modelling features, like queueing policy for places and marking dependencies on arcs, have never been included."
"spnp. the software package spnp ( [cit] was developed in the 90's at duke university, by the group of trivedi, and it has evolved over the years to account for new research results in the field. spnp basic formalism is that of srns, which incorporate several structural extensions to gspns such as marking dependencies (as marking dependent arc cardinalities and guards) and allow reward rates to be associated with each marking. type of measures that can be computed are steady-state, transient, cumulative transient, time-averaged and up-to-absorption. a discrete-event simulator is available for both srn and its non-markovian extension. limited support is provided for qualitative analysis of models, which is partially due to the choice of using a powerful text-based modelling language, for which a smaller number of qualitative analysis techniques are available. [cit] a tcl/tk-based graphical interface was added to reduce the need for the modeler to express the model in a purely textual form. spnp graphical interface can also be used to draw and simulate fluid petri nets."
"a future work list for greatspn strictly depends on what will be the research results in the performance evaluation field in the next years, given the willingness of keeping greatspn always at pace with the most useful research advances. such a list is difficult to write, but there are nevertheless a few features of the current graphical interface and associated solvers that are already planned for a (hopefully close) future. the first enhancement is to develop a ctl model-checker for colored models to improve the analysis capabilities of greatspn3.0 already made unique by the use of the csl ta model checker. it could be interesting if such model-checker could work directly on the symbolic reachability graph. another approach to solve this problem is to unfold a colored petri net (like a swn) into its uncolored (gspn) equivalent and then perform an (uncolored) model-checking. another point that deserves more attention in the tool is the definition of the color-dependent performance indices that would complement in an extremely useful manner the efficient solution techniques based on symmetries for colored petri nets already implemented in greatspn3.0. finally, compositionality of petri net models is clearly a desired feature, that would make it easier to draw complex hierarchical models by separating the logic into multiple nets, supporting both top-down and bottom-up approaches. in greatspn3.0 net composition can be performed through a command line program called algebra, yet to be integrated in the gui, which implements a parallel operator similar to the parallel operator in process algebra (in csp style) based on transition superposition; additionally, the algebra program also implements place superposition. upgrading this feature to the level of the invocation of the other analysis capabilities of the tool through the new gui is obviously an enhancement that we would like to develop as soon as possible."
"the modeler can play the token game and observe the flow of tokens by firing the transitions. the token game can be driven by the modeler, which explicitly chooses which transition to fire among the set of enabled transitions, or can be delegated to the tool (random mode execution)."
the vehicle is equipped with a sensor system which is able to detect lines on the ground representing a road network. tags are used to identify the vehicle's position. a digital version of the map is stored on-board as a directed graph. a path to the given destination is calculated using dijkstra's algorithm.
"the energy management module (emm) (d/e) supplies the computing platform and all other components attached to the vehicle with energy. up to four batteries can be connected to the module of which the most appropriate one is chosen. this is determined by the charging state, usage history (charging cycles) and other factors of each battery. the module supports different kinds of batteries (nimh, lipo and sla). the emm has two outputs for energy which can be switched on and off. one output has a maximum current of about 2 a, the other one can deliver more than 20 a. the former one is used for powering the controlling sbc, the latter one for driving. all currents and voltages are being logged which allows the system to estimate the remaining runtime"
greatspn plots directly inside the gui the distribution of the number of tokens in places (as in the case of the token distribution of figure 8 ) and the throughput of transitions directly on the net elements in the net window of the gui.
"timenet. the software package timenet [cit], now at version 4.3, [cit] at the technische universität of ilmenau, as a successor of the software dspnexpress, which was partly inspired by greatspn. the main focus of timenet is an efficient unified solution of dspn and gspn nets. steady state and transient analysis techniques include either exact numerical solutions, approximate solutions and simulations. firing delays of non-exponential transitions may have an arbitrary distribution. the graphical user interface, initially developed in motif and then rewritten in java, supports colored stochastic petri nets as well as markov chains, and is designed to be extensible to graph-like modeling formalisms. timenet provides more support for general distribution than greatspn3.0, but it does not include a complete qualitative analysis as provided by the ctl modelchecker of greatspn."
"in systems where there are (partial) symmetries which cannot be exploited by srg, it is still possible to take advantage of this symmetries by means of two solvers: the extended srg and the dynamic srg. the fms model of fig.13 has three static subclasses. the elements in the first subclass are routed to machine m 2 after leaving m 1, while the others are routed to m 3 . the subclasses c 2 and c 3 are needed only in some configuration where the rates of certain transitions (in our example ew 3 and repm3) are different for elements in classes c 2 and elements in the other classes. by executing the esrg module on the fms model the algorithm automatically detects and exploits partial symmetries. the generation of the ctmc according to the esrg method comprises two steps. first a graph is built that overaggregates the states. then a refinement step derives a lumped ctmc based either on strong lumpability or exact lumpability criteria [cit] . the choice of the type of refinement depends on the performance indices the user wants to compute. exact lumpability allows to retrieve the probability of detailed states, because it ensures equiprobability of the states in the same aggregate. in table 1 the number of states generated with the esrg algorithm are shown for some configurations. in particular observe that the size of the structure generated in the first phase of the esrg derivation is much smaller than the size of the srg for the same model (the number of states of the esrg structure includes both tangible and vanishing markings). however, the refinement with the exact lumpability condition results in a size that is close at the srg one. when the refinement is performed using the strong lumpability condition, the size of the refined lumped ctmc is only slightly larger than that unrefined one. in the strong lumpability case however only some color dependent performance indices can be computed."
"after almost 30 years of developments, improvements and tests, the greatspn framework is now a collection of many tools that support petri net modeling and evaluation in multiple ways. fig. 2 shows a (simplified) schema of the current features of greatspn. tool names are written in bold, and are grouped into logical modules. tool functions include: numerical solutions, structural analysis, state space exploration and model-checking for gspn and swn, support for markov decision well-formed nets (mdwn), conversions among multiple formalisms, monte carlo simulation, support for dspn definition and solution, and model composition. the graphical editor is the center of greatspn as it is used for drawing the models and for defining their properties. it is responsible for the invocation of various command line tools and for the visualization of the results. greatspn is now in the transition of replacing the old motif-based gui with a new interface developed in java. for the rest of the chapter, the characteristics of the framework will be shown from the user point of view, i.e. interacting with the new java gui. most of the command line tools comprised in greatspn can be called directly from the gui. the workflow of greatspn was conceived, back in its original design, to consist of three main phases: the user (\"modeler\") draws the petri net in a graphical way; then structural properties are computed (minimal p/t semi-flows, place bounds, conflict sets, ...) to understand if the model is designed properly and if it can be solved using numerical methods or via simulation; finally the user specifies the measures of interest directly on the model and calls a command line solver to compute the results. several solvers are provided for different types of models and with different characteristics. models are written to the disk in the net/def format, which contains the net description and the evaluation indexes to be computed. there are three families of models supported by greatspn: colored gspns, gspns with deterministic and/or general transitions, and markov decision petri nets (mdwns)."
"model-based analysis is supported by greatspn3.0 in various ways. the goal is to verify the correct behaviour of the modelled system through qualitative analysis and model-checking as well as to verify performance properties once it is decided that the model correctly represents the system under study (as in the case of nets automatically generated from system specifications). once the user is confident that the model represents the system behaviour, the analysis work-flow concentrates on the probabilistic aspects, through stochastic model-checking and through the computation of classical performance and/or dependability evaluation indices. the gui of the tool supports the computation and the visualization of the results for a varying set of parameter values. the analysis workflow is illustrated on a rather classical gspn model of a flexible manufacturing system (fms). all the figures and the graphs reported are directly produced by greatspn3.0, unless otherwise stated."
"the european union fp7 project v-charge 1 (automated valet parking and charging for e-mobility) offers a sophisticated combination of public transport and individual electrical mobility by introducing automated valet parking based on close-to-market sensors and coordinated charging strategies. this system allows drivers to drop off (and to pick up) their electric vehicle in front of a public transport hub (e.g., an airport) without taking care of parking or recharging. the vehicle executes these tasks autonomously. this implies three major fields of research: (i) vehicle functionality, e.g., on-board localization and planning, (ii) logistics, scheduling of parking and charging resources, and (iii) infrastructure, a secure and reliable framework to store and share a database of parking area information. [cit] in this demo, we present a research and development platform showcasing the abovementioned aspects of the actual v-charge project while also providing an educational tool for students to learn about automated driving, localization, and vehicular communications. our platform consists of a vehicle system, that is based on a 6-wheel-drive chassis equipped with a computing and sensor platform, as well as a server system, providing the vehicle with mission goals and a remote monitoring station. using the server application, a destination such as a parking spot or a charging station can be set, which the vehicle will then automatically navigate and maneuver to. in case of a charging task, the vehicle will also be charged automatically. compared to the actual v-charge vehicles, which are based on the vw golf platform, our miniature vehicles allow cheap, easy and rapid development and prototyping, which is especially useful as an educational tool."
"measures can be defined for colored nets in the same way as for the uncolored ones: their definition can be independent of the color (average number of tokens in a place regardless of their color, or overall throughput of a transition corresponding to the sum of the throughput of any instance of that transition) or be color dependent. an interesting feature of the srg is that, despite the relevant state space aggrega-tion, it allows one to derive the same performance indices that could be computed on the much larger rg. the same is true for the esrg with exact lumpability refinement and for the dsrg (which also ensures exact lumpability). the model checking facilities of greatspn currently require that colored models have to be first unfolded for the analysis to be performed. throughput utilization pr (down) t load m1 m2 m3 m2 m3 queue c1 c2 c3 c1 c2 c3 c1 c2 c3 table 2 shows some measures of interest computed using the greatspn solvers exploiting the model (partial) symmetries. these measures include system throughput (partitioned on the static subclasses of c), machines utilization, and probability for machines m 2 and m 3 to be unavailable due to a breakdown. applying little's formula we also derive the average time spent in m 2 queue (obtained as the ratio between the average number of customers in queue and the throughput of machine m 2 ). the measures are shown for the configurations (a) 2,2,2, (b) 3,2,2 and (c) 3,3,2, each of these three configurations are tested in four different scenarios, that have/have not the same number of spares for each machine, as well as in case of uniform or color dependent rates of the ew 3 and repm3 transitions."
"the analysis by p-and t-invariants, which can be activated from the graphical interface, reveals that there are 6 minimal p-semiflows and 4 minimal t-semiflows. the corresponding p-invariants prove that all places are bounded, with bounds equal either to 1, k or sp. figure 4 shows one of the four t-semiflows as displayed by the gui directly on the gspn model. transitions in the semi-flow are marked in red and their weight in the semi-flow is displayed inside the transition box (all weights equal to 1 in this semi-flow). the t-semiflow of the figure refers to a pallet that goes normally through machines m 1 and m 2 and then experiences a failure at m 3 . for this t-semiflow there is a firing sequence firable in the initial marking. the tsemiflow shows a scenario in which there is the direct intervention of the repairman to complete the work. reachability graph generation and analysis reveal that the state space contains a single connected component, that there are no deadlocks, and that all transitions are live. reachability graphs can also be displayed by selecting the \"measure\" rg (trg for the tangible reachability graph). figure 5, right, shows the trg as displayed by greatspn, while the left part is a zoom-in of a portion of it. the feature for displaying the reachability graph is very useful for petri net beginners and for teaching, but it is usually not part of the normal work-flow of model-based analysis because the size of the graph when modeling realistic systems is most of the time too large to be conveniently visualized."
"another possibility is to apply the dsrg solver. this aggregates the state space yielding a lumped ctmc satisfying the exact lumpability condition. the model specification in this case must be completely symmetric (no static subclasses partition of color classes, no guards, symmetric initial marking) while the asymmetries can be expressed in a separate file where it is possible to indicate, for each transition, restrictions on the colors that can be bound to each transition variable. in table 1 the dsrg size for the model with equal number of spares for both machines and with or without color dependent transition rates is shown. the type of lumpability condition makes it possible to compute color dependent performance indices based on the information contained in the dsrg structure."
mosfet based h-bridges are used to power the motors of the chassis. a pulsewidth modulation (pwm) signal for setting the motor speed is generated by an external mcu (b) since the used sbc does not have multiple hardware pwm channels. the external circuit can additionally generate signals for up to three rc servos.
"the gspn model of our fms model is depicted in figure 3, inside a screenshot of the gui that has been used for its definition. the net by itself could also be printed as pdf file using the classical printing facilities of the operating system. the system includes three machines m i (places m i ) and k pallets of parts to be worked (place pallets). each part is loaded and then sequentially processed by the three machines until the work is completed, the manufactured part is unloaded and the pallet goes back to place pallets through transition restart waiting for a new raw part to be loaded. for each machine m i an arriving part is taken (transition sw i ), worked (transition ew i ) and put in the input buffer of the subsequent machine or in the buffer of the completed parts. machines m 2 and m 3 can fail. in the case of m 2 there are sp spares available, while for m 3 there are no spare parts. spares in use can fail as well. both spares and machines are repaired by a repairman (token in place ready). since there are no spares for m 3 the repairman is assigned with higher priority to m 3 . this is implemented through the priority of transition repm 3 which is higher than that of transition repspares. upon failure of m 2 (firing of transition failm 2 ), if no spare is available, the work to be done waits in place m 2 ko, while if a spare is available it is taken (transition repm 2 ), the machine goes into repair, and the piece is worked (transition ew 2 bis). finally the part is put in the input buffer of m 3 (place m 3 buff) and a token goes into the machine place m 2 meaning that m 2 is available again. upon failure of m 3, since there is no spare available, the part is blocked until the repair ends (transition repm 3 e) and the part is worked (transition ew 3 bis). then machine goes back to m 3 and the part goes into the buffer of completed parts."
"greatspn was conceived about 30 years ago as a tool for performance evaluation. to overcome the (at that time) existing limitations in expressing synchronization and resource acquisition, greatspn evolved into a tool with a more holistic approach to verification. in this approach classical performance properties (like resource usage and throughput of transitions) and classical qualitative petri net properties (like liveness of transition, existence of deadlocks and traps) and, more recently, probabilistic verification properties, work in a synergic manner to establish the property of interest of a deds. in the rest of the paper we shall use the term \"stochastic analysis\" to refer to the set of analysis activities aimed at establishing the qualitative correctness of the model, using both performance and performability properties."
"oris the oris tool [cit] has been developed at the university of florence to deal with timed and stochastic petri nets. the delay associated with transitions can either be a non deterministic value, between a pair of min-max boundaries, or stochastic over a finite/infinite interval, thus subsuming also classical stochastic petri nets. the tool is equipped with a graphical interface for net specification and for the display of the analysis results. oris is oriented at the analysis of non-markovian system, for which it provides the most advanced solvers currently available."
"as mentioned before, the complexity of real systems often requires more powerful formalisms in order to build relatively simple models, where abstraction is the key element to understand highly intricate situations. colored extensions of (g)spns have thus been proposed, allowing a more compact and parametric model representation with a more efficient analysis. such analysis is based on the strong or exact lumpability conditions on markov chains. in particular stochastic well-formed nets (swn) [cit] have a structured color syntax that enables an automatic discovery of the presence of behavioral symmetries, leading directly to a reduced state space and a corresponding lumped ctmc."
"the modelling formalisms of reference in greatspn are generalized stochastic petri nets (gspn) [cit] and its colored extension stochastic well-formed nets (swn) [cit] . swn are based on the high-level petri net model of well-formed net (wn). wn have been recasted into symmetric nets (sn) in the petri net iso/iec 15909-2 standard [cit], and therefore swn are sometimes also called stochastic symmetric nets (ssn)."
"immediate transitions were originally included in gspns to allow a simple representation of very fast activities and logical choices. the extensive use of the formalism made soon clear that the structure of the underlying un-timed (autonomous) net could play an important role in allowing many more results to be derived from the model. time scale differences captured with timed and immediate transitions were related with the concepts of visible and invisible transitions in petri net theory. the priority of immediate over timed transitions led to the study of un-timed petri net with different levels of priority. drawing on these results, it became clear the danger of specifying \"confused models\", and thus the difficulty of constructing \"correct models\" including sequences of immediate transitions. to help analysts in specifying \"well behaving nets\", the concept of extended conflict set was formalized and methods were developed to find this structure at the net level [cit] . in the analysis of gspn models, immediate transitions are thus \"pre-processed\" to construct a reduced embedded markov chain defined on tangible markings only."
"the new gui [cit] integrates a modern editing pipeline which supports the entire greatspn workflow consisting of the editing phase, the visual inspection of net properties, the evaluation of qualitative and quantitative properties, and visualization of the results. the rest of the chapter describes this implementation discussing a case study represented by a sufficiently complex gspn model, that illustrates the details and the new features of the framework."
the second system for line detection is closer to the real world car since it is using computer vision (see camera a in figure 1 ). it is using the open source computer vision libraries opencv for recognizing the line depicting the road network and tags for identifying the road segments.
"graphs. finally, in the case of weighted graphs, the bound needs to be adjusted by using the range of distances in the graph instead of the term (n − 1)."
"we expect to improve this metadata design over time as lessons are learned from its use, and as progress is made within the broader metadata and scientific modeling communities."
"it is clear that zone 6 has the always highest priority with no difference in total priority of the devices in this zone. as a result, all the devices in zone 6 have the highest priority the same as the global zone priority value of zone 6. now, consider the other five zones as follows: these zones have different zone values from their total sum-up of the values of corresponding devices. now, the sum of priority values of zone 1 to zone 5 is given below:"
"in order to minimize the use of fraudulent documents as forms of identification, anti-counterfeit measures such as watermarks are often included. with an increase in the detection of fraudulent ids, security officers have recently seen a rise in the use of fraudulently obtained genuine (fog) documents. as the name suggests, these involve deception during the application process in order to obtain a genuine document, equipped with all the necessary watermarks, and so on. one method used by fraudsters is to submit a morph image (a 50/50 average of two people's faces) for inclusion in an official document like a passport. if both people sufficiently resemble the morph, they could both use the resulting genuine passport for international travel. recent research has begun to investigate whether people can detect morphs and has suggested that training might provide an effective way to increase performance. here, we reconsidered these findings with the use of higherquality morphs, where every effort was made to produce images comparable with those we expect criminals to use. we found that on-screen morph detection was poor and training did not lead to improvements. when morphs were compared to faces during a live interaction, they were accepted at concerning levels and, again, detection was error-prone. importantly, we found that a simple computer model performed better than our human participants, suggesting that security agencies should focus on automated solutions rather than training people when fighting morphing attacks."
"the university's school of psychology research ethics committee approved all the experiments presented here (approval code psy171881) which were carried out in accordance with the provisions of the world medical association declaration of helsinki. in all experiments, participants provided written informed consent before taking part."
"algorithm 1 illustrates the detailed feature extraction approach based on the differential phase. then, we use support vector machine (svm) for dpop identification. the svm is a popular and powerful binary classifier, which aims to find a hyperplane within the feature space that separates two classes. we divide the collected data into two categories, i.e., training data and testing data. from the training data, the svm parameter is optimized. define the training percentage p train as the ratio of the number of training data to the total number of collected data."
"in 802.11n ofdm frames, four sub-carriers are deployed for pilot transmission. the phase of received pilots at location l is denoted as φ l, which is calculated from the i/q components:"
"digital face photographs were taken of the models. all images were constrained to reflect neutral expression, eyes on the camera, consistent posture, distance to the camera, no glasses, and hair back. a headband was provided where necessary. there were small differences in lighting conditions across subsets of models (see fig. 1, bottom row). in addition, unlike the images used in experiment 1, we were unable to restrict the use of visible jewelry and make-up."
"we ran 100 iterations of the simulation, each time randomly selecting images for the training and test sets. for each iteration, percentage correct and d' values were calculated."
"when two or more devices have same local priority how should the application traffic be treated? moreover, zone's priority values are different. how the application traffic of two different prioritized devices from two different zones will be processed? this is the primary concern. in this case, which requests are processed first before the others? the central goal of this research is to implement a method for application traffic synchronization of zigbee nodes based on zone and individual priority and study its qos."
"algorithm 1 describes the key steps of flowgrid, starting with normalising the values in each dimension to range between 1 and (n bin + 1). then, we use the integer part of the normalised value as the coordinate of its corresponding bin. then, the searchcore algorithm is applied to discover the core bins and their directly connected bins. once the core bins and connections are found, breadth first search(bfs) is used to group the connected bins into a cluster. the cells are labelled by the label of their corresponding bins. the input of radiusneighbors is all non-empty bins, the query bin and the maximum query distance √ . the output is the bins whose distance with the query bin are less than √ (including the query bin)."
"hydroshare via hydroshare's existing web service application programming interface (api), updating the original resource. research on methods for achieving this goal, given the complexities of server-side model execution including the potential for large model instance sizes and long model execution times, has begun. being able to execute a model instance directly through"
"in fact, the synergistic sensory integration is not new. it is generally accepted that multi-sensory feedback is additively helpful to interactive task performance [cit] . this is only true provided when the respective modality feedback is consistent in its content and timing with one another [cit] . many synergistic multimodal interaction systems have been devised and studied employing gestures [cit], voice [cit], proprioception [cit], speech/audio [cit], and force feedback [cit] . aside from just improving task performance, multisensory interactions can also modify user perception, as illustrated by the famous mcgurk effect. the mcgurk effect is a perceptual phenomenon in which vision alters speech perception [cit] . simple visual tricks can easily alter the body image that is created by the proprioceptive sense [cit] . although the best known cross modal effects are those of vision influencing other modalities, visual perception can be altered by other modalities as well [cit] ."
"our work is different in that we focus on standalone model programs instead of componentbased modeling systems. standalone model programs can execute a model simulation and generate output, while a model component requires a modeling framework in order to be executed. model components can be loosely coupled using a modeling framework with other model components, while a model program does not provide this loose coupling capability. we take this focus because, while the adoption of component-based modeling systems is growing, we believe that the vast majority of ongoing studies are using standalone model applications and a metadata framework is needed to enhance the sharing of these standalone model instances. also, this work could later be merged with past work on model component metadata to create an overarching model metadata framework."
"however, the probability that it will be a shortest path is close to 0, i.e., 2ǫ − ǫ 2, since most likely, one of the paths of length 1 or 2 will be present. in this paper, we overcome the limitations of the mostprobpath distance by using statistics of the shortest path distance distribution, rather than any single path. furthermore, in section 5, we demonstrate experimentally that our distance functions outperform the mostprobpath distance in real world link prediction scenarios."
"some studies have begun to address the problem of designing a metadata framework for computational models. the content standard for computational models [cit] ) was one of the first attempts at providing detailed metadata about a numerical model that includes the input and output data for model scenarios. [cit] provide a concept for metadata-driven architecture for computational fluid dynamics simulations and a way to integrate model descriptions into spatial data infrastructures. the community surface dynamics modeling system (csdms) created a metadata framework and used it to describe over 180 geoscience models, including over 50 hydrologic models within its model catalog (see http://csdms.colorado.edu). the csdms model category focuses on the software for executing a model, what we refer to in this paper as a model program. it does not extend to the input files for a specific model simulation, or what we refer to in this paper as a model instance. the metadata included in csdms also do not follow higher-level metadata standards like dublin core."
"this work presents a model metadata framework to support discovery, sharing and interpretation of environmental models. key features of the framework are (1) that the model program and model instance are separate concepts with a one-to-many relationship (a single model program may be linked to many model instances), (2) that metadata for these concepts extend the well recognized and commonly used dublin core metadata, and (3) that the model instance concept is a hierarchy with a generic parent class implementable for any model program, and a more specific level tailored for certain model programs."
"in the following we introduce algorithms to efficiently process k-nn queries for the distance functions defined previously. first, we show how sampling can be used to compute the distances defined in the previous section."
"thus, the new priorities are also equal for both the devices. the coordinator checks their priority and respective zone priority. then, the coordinator gives access to node 8 first because of its higher zone priority. all the zones and their nodes synchronize in this manner. equation (5) computes the new priority for every device. based on these new priorities, zigbee coordinator makes a decision when certain conditions arise in synchronizing various device application traffic from diverse zone regarding the above two observations."
"the training data provides us with an average image ψ, weight vectors for each image ω i, and eigenvectors µ i . assuming γ 1, γ 2 ...γ 60 represent the initial 60 images provided for training, the following data is generated:"
"simulated data are collected properly and carefully from the different zigbee protocol layer. after analyzing the simulated data, appropriate graphs are plotted. careful study of the plots is expected to offer a quantitative measure of the effect of different network parameters. finally, qos has been measured of the designed zigbee network analyzing various qos parameters like end-to-end delays, mac delay, mac load, and mac throughput."
"in other words, ps,t(d) is the sum of the probabilities of all the worlds in which the shortest path distance between s and t is exactly d. note that there may be worlds g in which s and t are disconnected. thus, we allow d to take a special value ∞, and ps,t(∞) is consequently defined to be the total probability of all the worlds in which s and t are disconnected. we base our distance function definitions on standard statistical measures of the distribution ps,t."
"in figure 3 (both for global network and for coordinator), it is evident that without-priority based network has slightly higher mac delay than the priority based network. the average mac delay in zigbee coordinator (node 0) is about 0.12 sec and 0.14 sec for priority based network and withoutpriority based network, respectively. as a consequence, the average delay in the global network is 0.11 sec and 0.12 sec approximately for the prioritized and nonprioritized network, respectively."
"1. starting from s, we perform a computation of the dijkstra algorithm. once a node is visited it never gets visited again. to apply dijkstra in probabilistic graphs, we proceed as in the case of deterministic graphs: when it is required to explore one node we generate (sample) the outgoing edges from that node. we stop the execution of the algorithm when we visit a node whose distance exceeds d."
"the second experiment is mostly similar to the first except that the \"detached\" visual feedback effect to saltation was tested instead. we hypothesize for the existence and the improved quality of the phantom tactile sensation of the virtual objects when coupled with visual effects."
"theorem 3 shows the equivalence of the probabilisticrandom-walk to a random walk on a deterministic weighted graph. the direct computation of the transformed graph g can be performed in a per node basis. however, it scales exponentially to the number of the node's outgoing edges, making it practically intractable to compute exactly for outdegrees greater than 30. thus, we implemented the sampling algorithm for the transformation (with an option to group edges). we also implemented the k-nn algorithm from appendix c.4."
1 they are directly connected and at least one of them is core bin; 2 they are not directly connected but are connected by a sequence of directly connected core bins.
"in this and other live matching tasks, it might be the case that participants feel more suspicious than in typical psychology experiments. when presented with a single image and asked if it depicts the person in front of them, participants may scrutinize the image more than during a multiple-trial computer task, for instance. of course, these suspicions should be equally evident across conditions and would be apparent in their calculated response biases. however, this consideration is worth noting when designing such tasks."
"to evaluate the scalability of each algorithm, we downsample the largest concatenated data set from the seaflow project, generating 10 sub-sampled data sets in which the numbers of cells range from 20 thousand to 20 million."
"the answer to the random walk k-nn problem is the set of k nodes that have the largest stationary distribution values. in order to compute the k-nn results for a source node s, we propose to simulate the random walk and approximate the stationary distribution of each node by the frequency that the node was visited during the simulation. this is a standard monte carlo approach for computing pagerank, (see [cit] for discussion and analysis of the method). in contrast with the power iteration method, the monte carlo approach is well-suited for the k-nn problem because it is localized in the neighborhood of the graph around s: distant nodes from s are never (or rarely) visited. observe that we can perform the walk on (the transformed graph) g instead of g using theorem 3 from section c.2. this way, we drastically reduce the amount of randomness at each step of the walk (i.e., we save the time needed to check if each outgoing edge of the current node is active). we note that any technique for personalized or individual pagerank computation on deterministic graphs, e.g., [cit] can be directly applied to g."
"the current model was trained and tested using images with varying characteristics. the photos of individuals (and hence the resulting morphs) showed significant differences in brightness and other qualities across the image sets used; this may have resulted in a more robust \"morph detector\" since the model was trained to classify images irrespective of these irrelevant sources of variation. one might predict that further increases in this type of variation by including morphs created from many different photo sets would result in additional performance improvements."
"this sheds further light on previously reported results whereby 50/50 morph images were accepted in a computerized face matching task on 68% of trials [cit] . in that study, morph images were only ever presented with one of the two identities comprising the morph. therefore, the acceptance rates when matching each of the two identities with the morph image were not investigated. here, we show the importance of presenting the morph with both of the contributing faces, as only one of our 19 pairs produced equal morph acceptance rates for each person (pair 10 in fig. 3 )."
the weight vector reader is used by the classifier/projection module for retrieving the weight vector values that are generated in the training stage and stored in the block ram.
"in this section, we present the downsampling module used to connect the detection and the recognition subsystems. then we describe the complete face recognition system figure 5 ."
"the average throughput in a global network for the priority based network is almost 1100,000 bits/sec. on the other hand, for the without-priority based network it is only about 700,000 bits/sec. throughput for the coordinator (node 0) is relatively higher in the priority based network compared to the without-priority based network."
"the results of this experiment demonstrated that the acceptance of morph images as id photographs was highly image-dependent, revealing significant variation in their success as fraudulent images. in addition, we found that morph images typically resembled one of the individuals featured in the image more than the other (see fig. 3 ), again resulting in large variation in their success. it is unclear as to why this was the case, giving rise to an interesting theoretical question: what causes a 50/50 morph image to equally resemble both individuals?"
"it has been found that the extent or the controllability of the effect is diminished when extended to \"out of the body\" and even more so when a virtual object is used as the medium extending one's body [cit] . consequently, we seek and experimentally investigate the possible synergistic effects by associating it with \"dynamic\" visual feedback to improve the tactile experience and controllability, possibly even without the medium (real or virtual) that connects the body parts (i.e. tactile interaction with virtual objects completely detached from the body). if validated, such a phenomenon can be applied to tactically interacting with holographic objects hovering in the air (figure 2(b) )."
"the algorithm for dm is based on exploring the local neighborhood around the source node s, and computing the distribution ps,t, truncated to the smaller distances. in particular, for a distance value d, we compute the distribution pd,s,t, which is identical to ps,t for all distances smaller than d. the remaining probability mass is concentrated exactly at distance d. more precisely, we have:"
"the probabilities for dblp and flickr have been computed from real information based on simple probability assignment models. we choose to use simple models, since model selection, in general, is beyond the scope of this pa- per. biomine is already labeled with probabilities. all the probabilistic graphs are connected, but obviously many disconnected worlds can be instantiated and thus infinite distances can occur. table 3 summarizes size and maximum out-degree of the datasets. the datasets follow a power-law out-degree distribution, commonly found in scale-free networks. we present the degree distribution of biomine in figure 8 as an example and note that the others are similar. observe that there are some central nodes, connected to 5% of the database. table 4 is complementary to figure 4(b) . there are many infinite distances in our datasets. for example, for 56% of the pairs of nodes, the median distance is infinite. recall from figure 8 that there are many nodes with one or two edges. also recall from figure 4 (a) that these edges most likely have low probability. in other words, these nodes are disconnected from the main part of the graph in most worlds generated by the probabilistic graph. thus their median, expected-reliable and majority distances to the rest are oftentimes infinite."
zigbee is a sensor based special network which pulls the trigger to establish it in wireless network standard. zigbee was designed to provide high data throughput in applications where the lower duty cycle and power consumption are an important consideration. zigbee specifications are maintained and updated by the zigbee alliance based on the institute of electrical and electronics engineers (ieee) standards2
"we also perform a cross-validation using the rff feature of aoq. table iv shows the identification accuracy using aoq for nics with 5 types of models and all mixed nics. accuracy 1 and accuracy 2 are the identification accuracies where training and testing data sets are measured at the same and different locations, respectively. it is observed that the accuracies is 100% for the nics with wdr-5620, wdr-5660, cpe-500 and ws-5100 models and about 99% for mi-3a and the mixed nics. there is no significant difference between accuracy 1 and accuracy 2, which indicates that aoq is still effective in new locations."
"a key challenge in this or any other metadata framework design is providing the right balance between rich metadata for adequately describing details of resources and minimal metadata that is critical and can be easily populated. the growing number of generic data repositories available to environmental modelers (e.g., figshare.com, zenodo.org, institutional repositories, etc.) largely adopt a minimal metadata approach. these systems provide metadata roughly equivalent to the metadata used to describe a generic resource in the hydroshare system."
"flickr. flickr is a popular online community for sharing photos. among other activities, users can participate in common-interest groups and form friendships. we created a graph from an anonymized recent snapshot of flickr. in particular, we extracted information about users joining interest groups. we labeled the edges with probabilities assuming homophily, the principle that similar interests may indicate social ties. namely, we computed the edge probability between any two nodes (users) as the jaccard coefficient of the groups they belonged to. this process creates quadratic number of edges with respect to the number of users. we, thus, put a threshold of 0.05 to the probability value. in order to avoid high values of the coefficient given by users who participate only in one group, we also put a threshold on the size of the intersection to be at least 3. we computed this information for a small number of users (77k), and we obtained a dense graph of 20m edges."
"the walk is initialized at node s and world g0, which is sampled online according to p . at the t-th step, the walk is characterized by the current node ut and the current world gt. at the t-th step we either follow an active edge (with probability 1 − a) or we teleport back to s (with probability a). in the first scenario, we follow an active edge (ut, v) with probability"
"in accordance with related works, priority is used for efficient network formation. and qos is highly endorsed to analyze any network. from this context, a motivation comes to build a new priority calculation strategy for zigbee devices where each device and zone have priority. this new priority is useful for synchronizing zigbee devices from different zones. since the new priority technique has not been practiced previously, qos has been scrutinized to validate the performance of the prospective zigbee network."
"like other density-based clustering algorithm, parameter setting is important. in our experience, bin n and are data-set-dependent. we recommend trying out different combinations of bin n between 4 and 15, and between 1 and 5. to pick the best parameter combinations, some prior knowledge is helpful such as the expected number of clusters and the proportion of outliers which should be less than 10% in our experience. we found that other parameters, namely minden b, minden c and ρ are mostly robust across a wide range of values."
"we divide the data set into four categories, i.e., location a, b, c, and d, according to the measured locations. by comparison, we first cross-check the identification accuracy using the received frequency responses of two ltss over different data sets. in the testing process, when a device is correctly identified as the real one, we call it a successful identification. otherwise, we call it a failure identification. the identification accuracy ξ is defined as the ratio of the number of successful identifications to that of failure identifications during the testing process in the experiments. the identification results are shown in table iii . it is observed that the identification accuracy approximates 100.0% when the training and testing data are in the same location. however, the identification accuracy falls sharply when the training and testing data are in different locations, with a maximal value of 24.3% on the training of location c and the testing of location b. the results of table iii indicates that the received frequency responses of two ltss are seriously influenced by the locations, which is consistent with the assumption in section iv-a."
"finally figure 12 shows the responses to the five survey questions, which are consistent with the quantitative analysis. subjects were conscious of the helpful effects of the visual feedback and confident of their phantom sensations. subjects were also able to recognize the direction of the stimulation and the described the sensation to as soft bounce (15/20), hard contact (3/20) and moving vibration (2/20) in the postbriefing."
"the training data is generated using the opencv library [cit] . we used two different face image databases as training data. first, we evaluated the feasibility of the face recognition subsystem using the orl database [cit] . we refer to the orl database as \"set1\" throughout the remainder of the article. we generated training data using 100 images from 10 different individuals from set1. we also collected 60 images from 6 individuals in our lab which we call \"set2\". in the following sections, we introduce the details of the implementation based on set2."
"once the user inserts the bounding coordinates, the box will appear on the map so that the user can confirm the spatial coverage extent. the user can also specify the coverage by clicking a point on the map or dragging a box on the map. the temporal coverage metadata consists of start and end dates and times for the model instance. this is implemented in the data model based on the w3c-dtf scheme, which by default enables full specification of a date/time string, including a time zone. currently, as seen in fig. 14, the hydroshare interface supports only the entry of dates without times or timezone specifications. hydroshare uses this coverage metadata to support both spatial (e.g., map-based) and temporal searches to identify relevant resources."
most of the requests are dropped on the basis of their priority and a limited delay occurs in accessing medium. it represents the total end-to-end delays (sec) of all the packets received by the 802.15.4 macs of all wpan nodes in the network and forwarded to the higher layer.
"future work will be aimed at improving the usability of the model program and model instance resources within hydroshare. for example, to reduce the time spent manually completing metadata fields, new functionality is planned to automate metadata extraction when a resource is uploaded and the metadata are already present within files uploaded with the resource. this would be especially effective for specific model instances whose input files already contain rich metadata."
"a large variety of environmental models exists, with each model tailored to address specific challenges related to environmental science and natural resource management [cit] .. these models have grown in complexity, with many simulating increasingly detailed processes occurring within environmental systems. when scientists and engineers use models, they must devote significant effort to collect data, construct model inputs, and calibrate and validate model parameters. many environmental models also require sophisticated data pre-processing routines, often with many manual steps (e.g., [cit] ) . for this reason, many models come with supporting applications such as geographic information system (gis) interfaces, calibration tools, visualization software, and other utility software systems to assist in the data preparation process (e.g., [cit] ) . these data pre-processing steps must be repeated each time a new model is created to simulate a system. this introduces a number of challenges. from a pragmatic perspective, it is an inefficient use of scientists' time. [cit] ."
"a longer-term goal of this work is to provide server-side execution of model instances directly through hydroshare. by knowing and storing the exact model program used to execute a model instance within hydroshare, it should be possible to install the model program onto serverside computational resources and execute a model instance using these resources. the updated model instance including the newly generated output files could be automatically added to"
"on each iteration of our simulation, we randomly selected 86 individuals and 86 morphs to represent the training set. the remaining 10 individuals and 10 morphs (approximately 10% of the total) were used as novel, test images. we then ran the pca + lda procedure without these test photos (i.e. with 172 training images). next, we projected each untrained test image into the resulting one-dimensional space."
"it represents the total load (in bits/sec) submitted to the 802.15.4 mac by all higher layers in all wpan nodes in the network. figure 4 points that withoutpriority based networks have higher global mac load than the priority based network. in the priority based network, coordinator (node 0) processes all the requests. for that 0 24 48 72 96 120 144 168 192 216 240 264 288 312 336 360 384 408 432 456 480 504 528 552 576 reason, priority based networks have more mac load for coordinator than without-priority based network as shown in figure 4 ."
"coordinator is responsible for forming a zigbee network. end devices and routers have always a distance from zigbee coordinator. this distance around zigbee coordinator is defined as different area factors. the area factor is called a zone and each zone has many end devices under a router. each zone has a set of priority attributes for defining its priority. in addition, the end nodes have also priority values in their zone. when nodes of different zones are trying to communicate via intermediate nodes, then routers or coordinator always checks their priority values. in this mechanism, a child node (end node or router) connects to a parent node according to the child nodes willingness to pay for the connection. the paying preference is considered as a priority. we assume that each end device in the network has a certain willingness to pay and its zone has also a priority. end devices with higher priority zones have a higher willingness to pay and vice versa [cit] . at first, the zigbee network has been divided into six zones and set the priority values to the corresponding zones. the zones and participating nodes in these zones are listed in table 1 ."
"another commonly adopted statistic is the majority distance, which is the shortest-path distance that is most likely to be observed when sampling a random graph from g:"
"we perform the following classification experiment. we choose a random ground-truth edge (a, b0), and a random node b1 from the graph, such that (a, b1) is not a groundtruth edge. we then randomly permute b0 and b1, producing either a triplet of class 0 of the form a, b0, b1 or a triplet of class 1, of the form a, b1, b0 . given a triplet, the classification task is to assign the appropriate class. in other words, the classifier attempts to identify the true neighbor. we build various classifiers based on different distance functions. all classifiers are unaware of the ground truth and pick the node that appears to be closer to a in the probabilistic graph."
"across four experiments, we have investigated human and computer performance with high-quality face morphs. our results show that morph detection is highly error-prone and that training does not provide a useful solution to this problem. instead, computer algorithms may be a better method for minimizing the frequency with which face morphing attacks are missed. interestingly, morphs typically resemble one individual in the pair to a greater extent than the other, suggesting a possible limitation for fraudsters who plan to use such techniques. the results of these experiments have important implications for real-world national security measures and highlight that it is essential for researchers to consider the quality of morphs that are likely to be employed by fraudsters in real-world settings."
"the concept of \"out of the body\" tactile experience from a hand-held physical or virtual medium. a medium for extending and connecting the body parts is required, virtual or real. the phantom sensation is more evident with the physical medium (e.g. felt at 3cm from the left) than the virtual (felt at 1.3cm with the same stimulation) [cit] ."
where end-end is end-to-end delay; trans is transmission delay; prop is propagation delay; proc is processing delay; and is number of links (number of routers + 1). 0 24 48 72 96 120 144 168 192 216 240 264 288 312 336 360 384 408 432 456 480 504 528 552 576 600 figure 2 : end-to-end delay (sec) for the global model.
3) weight vector finder: the weight vector finder module calculates weight values for input image using the previously calculated normalized image and eigenvector values. the eigenvector values are read by the eigenface reader from the eigenface image buffer. the eigenvector values are stored in block ram. the weight vector finder is the most computationally expensive step in the face recognition algorithm.
among the five different measures presented which ones are the most appropriate? how are they computed? we address these questions in the remainder of this paper.
"by comparing these proportions to response levels predicted by simple guessing (0.5), we found that judges were significantly better than chance at detecting that the morphs were morphs, t (46 we also calculated sensitivity measures for this question, considering the 30 judges approached by each model, using the following definitions: hit -the image was a morph and judges responded \"morph\"; and false alarm -the image was an individual and judges responded \"morph.\" across all models, we found that"
"previous research has found the phantom tactile sensations for virtual objects external to one's body. however, it required a virtual imagery to be attached to the user for directly extending one's body. this paper has investigated in whether similar phantom tactile sensations exist when the virtual object is visually detached from the user's body. our results have shown that in addition to the perception of the phantom sensations with the \"detached\" visual feedback, the interaction experience was significantly enriched (vs. when without explicit visual feedback). we also discovered for the first time that for funneling, phantom sensations can be elicited without any visual feedback at all. we can further conjecture with both mediating visual feedback and the actual dynamic visual content, the tactile experience has to be improved even more with even higher localization controllability."
"we define a distance function based on random walks. in contrast with shortest-path functions which rely on one path, random walks consider all paths. also, their navigational choices are random instead of optimal. random walks have already been used for nearest-neighbor search in standard graphs [cit] ."
"observation 2 (the same priority devices). this observation mainly concerns the same prioritized devices which are from different zones. here, both node 5 and node 8 have the same priority in their corresponding zone. the initial priority value is 2 for both the devices. the new priority ( ) has been computed according to (5) and it has been found to be 2.82 for both node 5 and node 8."
"to demonstrate this robustness, we used the benchmark data sets from flow-cap for a parameter sensitivity analysis. for these experiments, we first set 3, 40, 85, 4 and 1 as the default value for minden b, minden c, ρ, bin n and, respectively. in each experiment, we only change one parameter to test its sensitivity to the overall classification result. the performance is measured by ari and runtime. in the first experiment, we varied minden b from 1 to 50 while fixing other parameters. in the second experiment, we varied minden c ranging from 10 to 300 while fixing other parameters. in the third experiment, we varied ρ ranging from 70 to 95 while fixing other parameters. figure 4 demonstrates that the clustering accuracy and runtime are largely insensitive to minden b, minden c and ρ across a large range of parameter values. the experiments are applied to all the benchmark data sets from flow-cap and similar results are observed in all the benchmark data sets. in our experiments, when minden b, minden c and ρ are set to be 3, 40 and 85 respectively, flowgrid maintains good clustering performance and excellent runtime. they are therefore set as the default parameters for flowgrid."
"in the previous sections we defined distance functions among nodes that reside in probabilistic graphs and proposed algorithms to compute distance and k-nn queries. before we explore the efficiency of these algorithms (section 6), we showcase that our distance functions outperform their alternatives reliability and mostprobpath in identifying true neighbors via a link-prediction experiment. we experiment with two real-world datasets, a protein-protein interaction network and a co-authorship graph. both datasets consist of two parts: a probabilistic graph and a set of groundtruth neighbors. the underlying assumption is that a good distance function measured on the graph should be better correlated with the ground-truth neighbors."
"all models and judges were members of a uk university. models participated as part of their undergraduate research skills course, while judges represented an opportunity sample of students and staff that were present on campus at the time of data collection. judges were strangers and did not know the models before recruitment. in addition, no judges had previously participated in experiments 1 or 2. finally, all judges confirmed that they had not already taken part in this experiment before responding."
"minden b and minden c are density threshold parameters to reduce the search space of high density bins. if the parameters are set very low, the runtime may fractionally increase but the accuracy is not likely to be affected. however, if the parameters are set very high, the runtime will also fractionally decreases but it may lead to separation of real clusters and create spurious outliers. in any case, we showed that the performance of flowgrid is generally robust against changes in minden b, minden c and ρ."
"in figure 6 (d) we present the stability of the k-nn result for the median distance, 50-nn. we considered the result in 1000 worlds as the ground truth since it was stable. clearly, the solution stabilizes for a few hundred worlds."
"so far, we have shown that naïve participants were poor at detecting high-quality morph images and that providing training or tips aimed at improving performance resulted in little or no benefit. in addition, we found that morphs were accepted as id photos often enough that they may be feasible as tools for committing fraud."
"first, application layer's parameter (end-to-end delay) is considered for user level service performance measurement. about 14% less end-to-end delay has been found for the priority based network in the application layer. second, mac layer's parameters (mac load, mac delay, and throughput) have been considered in mac performance measurement. in the global network perspective, the priority based network has almost 14% and 44% less mac delay and load, respectively. moreover, there was 36% higher throughput compared to the without-priority based network. all the qos parameters are shown in a percentage bar graph regarding their qos improvement as shown in figure 6 . the analysis of zigbee layer's parameters contrasts significant response for qos. after this study, the proposed priority based zigbee network's performance is identified to be more effective than without-priority based zigbee network with respect to these improved qos parameters. the proposed approach is handy in differentiating or synchronizing different zones and their devices with different priorities in the zigbee network."
"we conclude that our distance functions are better in identifying true neighbors in real world data than their competitors. also, this experiment demonstrates that our dis-tance functions differ from one another. finally, the remarkable difference between our functions and their competitors draws future directions for research in applications, where mostprobpath is currently used [cit] ."
"we accumulated distances running the full bfs traversal for 500 sampled nodes on a sample of 500 worlds. we set the sample-size to 500 worlds after experimentally observing that the result was stable. we present the distributions of all the distance functions in figure 4(b) . for the expected reliable distance we set the reliability threshold to 0.5 (we have removed the infinity bars from the histograms (see appendix e for details). observe that all distance functions yield similar distributions. also, they all look qualitatively similar to typical distributions of shortest path distances in non-probabilistic networks with scale-free characteristics."
"equation (5) can change the existing priority status. in addition, it helps coordinator to make the decision for request processing in terms of equal priority devices. different observations are described below."
"the rest of the paper is organized as follows. section ii introduces the experimental setup, structure of ieee 802.11n frame and the process of signal acquiring. in section iii and section iv, we present the models and algorithms of two proposed rff feature extraction approaches, respectively. the device identification methods are described in section iv-c. section v presents our implementation and experimental results. we finally conclude our work in section vi."
"in order to obtain an rff that is invariant to locations, we aim to extract a novel robust feature from the quotient of two successive received lts spectrums. due to the coherence of wireless channels, their cfrs remain unchanged within a very short time interval of 8 µs as"
"while many metadata frameworks exist, none specifically addresses computational environmental models. thus, the objective of this research was to design and implement such a metadata framework for environmental models."
"as hydroshare continutes to evolve, the types of searches users wish to complete will help to guide future expansions of the metadata framework. there are many example use cases one could imagine for enhanced discovery. for example, a user may wish to identify model programs that have the ability to execute using a hot start file, which may be required for a specific application like flood forecast modeling. in the current system, users can specifiy such details in the resource abstract as free text and/or as keywords. this reduces the metadata complexity, but if certain queries like this become a common occurence within the system, then a new metadata element (or elements) might be needed to describe this property more precisely. doing so, users would have the capability to more easily search and discover these resources without having to rely on free text searches of the generic metadata fields (e.g., dc:description/dcterms:abstract)."
"for example, in this work hydroshare specializes in model metadata, resource sharing, and resource publication. in ongoing research, we are building interoperability with the external swatshare system that focuses on swat model execution and visualization [cit] ."
"in the evaluation, we treat the manual gating label as the gold standard for measuring the quality of clustering. in the pre-precessing step, we apply the inverse hyperbolic function with the factor of 5 to transform the multi-centre data and the seaflow data. as the flow-cap and multicentre cytof data contain many samples and we treat each sample as a data set, we run all algorithms on each sample. the performances are measured by the ari and runtime, which are reported by the arithmetic means (x) and standard deviation (sd). for the seaflow data sets, we treat each concatenated data set as a data set. in the evaluation, all algorithms are applied on these concatenated data sets."
"it describes that when nodes try to communicate with other zones' node, then the request is processed on the priority based mechanism. in this sense, when one node communicates with a node in lower priority zone compared to its priority zone, then the total delay between creation and reception of application packets generated by this node increases. figure 2 represents that a global end-to-end delay is higher for network without priority compared to the priority based network. the priority based network faces on average 0.05 sec delay approximately. on the other hand, non-priority based network faces 0.07 sec delay approximately. the average end-to-end delay is slightly higher in the non-priority based network."
"model instances, for example, often include input files containing information on spatial and temporal coverage. the system should read these files, extract whatever metadata it can, and request only missing metadata fields from the user. automatic metadata extraction, along with the increased use of controlled vocabularies, would increase the usability of the system from both sharing and discovery use cases. this approach is difficult, however, given the diversity among environmental models; extracting metadata directly from model input files may require a significant amount of custom code. one potential long term benefit of this work would be for all model developers to add functionality that outputs a standard metadata file that can be read by"
"the expected-reliable-distance distance can also be efficiently approximated via sampling (see appendix a for additional details). also, we remark that lemma 1 works on graphs with conditional probabilities on their edges and we refer the reader to appendix b for additional details."
"in the more general case, where edges have both probabilities and weights, we create r groups that are characterized by having similar probability and weight (qi, ti). creating such groups is casted as a 2-dimensional clustering problem, which can be solved by the k-means algorithm."
"our distance function is based on the individual pagerank (ipr) [cit] . given a node s, ipr refers to the stationary distribution of a pagerank walk [cit] that always teleports back to s, instead of teleporting to any node in the graph."
"experiments 1 and 2 demonstrated that people were poor at identifying our face morphs and that the training methods we explored did not result in significantly improved performance. if this type of image cannot be easily distinguished from standard exemplar images, then this provides the possibility for fraudsters to use face morphs as photographic identification in the real world. more specifically, at the point of issuing identification documents, morphs that fail to be detected will be incorporated into fog documents for later use."
"various qos characters of zigbee network are observed for performance analysis of with-priority versus withoutpriority mode. almost in all the statistics shown here, priority based network performance is going prominent compared to without-priority based network."
"the metadata framework proposed in this study was designed to provide a balance between simplicity and complexity; simplicity to encourage to sharing of models by model producers, and complexity by providing a sufficient level of information to enable discovery and use of the model by potential consumers. one of the most difficult design decisions in this work was to separate model programs and model instances into two different resources rather than a single combined resource. the design decision was made for the following reasons. first, it allows the model program metadata to be entered only once within the system. second, it simplifies the task of identifying all model instances executed by a given model program stored within the system. a limitation of this work at its current stage is the ability to scale-up to support dozens of different specific model instance resource types. ideally, the creation of new hydroshare resource types would be simple enough that it could be done by the broader community of model developers. currently, however, the process of creating a new resource type within hydroshare is time consuming and requires advanced knowledge of the hydroshare system and architecture."
"this paper presented the design and implementation of a complete fpga-based real-time face recognition system which runs at 45 frames per second. this system consists of three subsystems: face detection, downsampling and face recognition. all of the modules are designed and implemented on a virtex-5 fpga. we presented the architectural integration of the face detection and face recognition subsystems as a complete system on physical hardware. different experimental results of the face recognition subsystem are presented for pipelined and non-pipelined implementations."
"computing the exact distribution pd,s,t is expensive, since there are exponentially many graphs to consider. we overcome this problem by sampling graphs and approximating the distribution pd,s,t, with the sample distributionpd,s,t."
"since saltation was used the vibro-tactile stimulations were timed rather than given simultaneously. the same \"detached\" augmented visual feedback, \"bouncing basketball\" was used. the experiment was designed as a 2x5x2 factor within-subject. the three factors were (1) inclusion of the visual feedback (with or without), (2) intended locations of tactile illusion (five locations between the fingers labeled p1 ~ p5) and (3) direction of the stimulation (from right to left or vice versa). five survey questions were answered in a 7 likert scale asking of the various aspects of the phantom tactile experience (see table 4 )."
"by adopting the same metadata and resource file structure for a swat model instance, these model instance resources can be more easily transferred between the two systems, and users can benefit from the functionality and strengths of both applications."
"moreover, to increase the network performance a large network was partitioned into several zones or areas. each zone has priority and containing devices have also local priorities in their corresponding zones. the priority indicates their task's importance. these devices send huge application traffic to the routers and coordinator for processing. for qos maintenance, this huge amount of traffic processing and end devices requests synchronization is needed. priority scheduling is the promising mechanism in this regard. beforehand, several strategic approaches had been adopted which worked on zigbee network performance analysis in terms of routing performance and effective data flow and scheduling has been discussed [2, [cit] . yet, there is a lack in forming a zigbee network efficiently. in that respect, further study is required for qos analysis on zigbee network as well as improvement of network performance."
"to further evaluate the scalability of the algorithms, we sub-sample one seaflow data set and the sampled data sets range from 20 thousand to 20 million cells. figure 3 shows the scalability of our algorithm and three other algorithms. flock has a low runtime when processing a small data set, but its runtime dramatically increases to 6640 s for a 20 million-cell data set. flowpeaks and flow-som share similar scalability but flowpeaks is not able to execute 20 million data set. our algorithm have the best performance in the evaluation as flowgrid is faster than other algorithm in all the sampled data by an order of magnitude."
"according to the proposed implementation method, a simulation scenario has been created. the scenario consists of a zigbee coordinator, six routers, and several zigbee end devices. after that, all the devices are organized into different zones. the proposed zigbee network model scenario is organized on the basis of table 2 . the zones and their devices have set of predefined priorities according to table 1 . the new priority calculation has been performed using riverbed's \"utility\" service. the service sets new priority after analyzing communicating devices."
"the remainder of the paper is organized as follows. first, a methodology section is presented discussing the design of the model metadata framework and describing an example use case where the design implemented in hydroshare was used to share results from a hydrologic modeling study. next, the results section presents the implemented software and the results from the example use case. finally, the paper concludes with a summary discussion of the proposed approach and steps that could be taken to further advance this work."
"this paper is organized as follows. in the next section, we first review previous research literatures related to phantom tactile sensation, multi-sensory integration and their application to practical interaction design. then, we describe the two validation experiments and report the results. finally, we conclude the paper with a discussion and directions for future research."
"we formed 20 same-sex pairs from 40 of our white models based on general descriptors (e.g. blonde hair), with this pairing carried out by two of the authors. the person each model was paired with is referred to as their \"foil,\" with paired individuals serving as each other's foils. due to the limited size of our model sample, we failed to form acceptable pairings for four white individuals. these four, along with the four non-white models, were paired with images taken from our set 1 database, which provided a larger selection of individuals of both sexes and multiple ethnicities to choose from."
"models were provided with three images for data collection, representing the three experimental conditions: (1) match -an image of themselves; (2) morph -a computer-generated average of the model and their foil; and (3) mismatch -an image of their foil."
the implementation of the face recognition subsystem is performed in two steps. the first step generates the training data and the second step is face recognition.
"although the dpop feature in algorithm 1 is both stable and channel-robust, it is a single dimensional feature and thus can be hardly used to distinguish a large number of devices. it is also verified by the experimental results in fig. 5 . next, we seek for another location-invariant feature which is not constrained by a certain low-dimensional feature space."
"for the task itself, from the 120 images created using our set 1 database (60 morphs and 60 exemplars), each participant was presented with a randomly selected 30 morphs and 30 exemplars. the order of presentation of these faces was also randomized. on each trial, one face appeared onscreen, along with the question, \"is this image a face morph (a blend of two faces)?\" participants selected either \"yes\" or \"no\" onscreen using the mouse. no feedback was given at any point during the task."
"we present experimental results from set1 and set2. figure 3 shows the performance comparisons between the software and hardware implementations of the face recognition subsystem using 10, 20, 25, 50 and 100 images from set1. when using 100 images, the face recognition subsystem achieves an average speed up of 15x over the equivalent software implementation. the software experiment was done on multi-core machine machine with core2 duo cpu running at 3.33 ghz with 4 gb ram. figure 4 (a) and (b) shows the latency and the latency cycles respectively for 40, 50 and 60 face images from set2 with pipelined and non-pipelined implementations. the device utilization summary when using set2 with pipelined and non-pipelined implementations is also shown in figure 4 (c) in number of slices, luts, rams (brams), and dsp48s."
"this metadata framework was designed to facilitate the description and communication between loosely coupled components of a larger model chain. the framework enables the output from one model component (e.g., a meteorological model) to be used as an input for another model component (e.g., a hydrological draingage model). [cit] 5 metadata standard as a starting point and expanded the spatial characteristics, temporal characteristics, and environmental parameters to enable models to be discovered and reused."
"sts has significant spectrum characteristics and thus can be used to find the coarse position of the frame header quickly. denote x(n) as the received baseband data measured by the rf front end and slice x(n) into m b data blocks, then the m b -th block is defined as"
"whether viewing a tips screen improved detection or not, we can say with certainty that any increase in performance was small and detection levels both with and without training/tips were low. both experiments 1 and 2 addressed the first process involved in using a fog passport -when issuing a passport, personnel must compare the newly submitted morph image to the previous passport image of one of the identities. our results have shown that it is unlikely that a morph image would be detected at this stage and that training would not help in this process. the next experiment addresses the second stage of fog passport criminality -its usage. in this situation, a border control officer or other official must compare a live person to the morph image."
"in the general case, we do not have groups of edges with equal probability, so we suggest to cluster together edges with similar probabilities. in order to choose an optimal kclustering of edges from a node u, and the respective assignment of the probability of each edge pi to a representative probability q k, we seek to minimize the function"
"to reduce the computational search space and memory requirement, our algorithm extends the idea of dbscan by using equal-spaced grids like flock. we implemented our algorithm in an open source python package called flowgrid. using a range of real data sets, we demonstrate that flowgrid is much faster than other state-of-the-art flow cytometry clustering algorithms, and produce similar clustering results. the detail of the algorithm is presented in the methods section."
"the images created for the above experiments were used as training and test sets here. we combined the images from the morph detection (60 individuals, 60 morphs) and training tasks (20 individuals, 20 morphs) from experiment 1. to these, we added a subset of images used in experiment 3 (16 individuals, 16 morphs), allowing the maximum number of images from this experiment while avoiding the use of images with overlapping identities, i.e. no individuals that formed morphs and no morphs that included individuals. this resulted in a total of 96 individuals and 96 morphs, where no identities appeared in both sets."
"the synchronization of different proposed zones is done on the basis of priority values for processing their nodes request. according to table 1, the priority values have been fixed for simulation in different zones and their corresponding devices. almost all the devices were active in communication when figure 1 had been captured. the node 0 is a central node which is zigbee coordinator whereas five routers (node 1, node 2, node 3, node 4, and node 5) are placed at a certain distance from the coordinator. the routers are intermediary nodes and they hand over the application traffic to a particular destination. on the contrary, zigbee coordinator is a full-function device which is responsible for all functionalities as well as performance issues. after the 600 sec simulation, zigbee nodes start communication as depicted in figure 1 ."
"the lts is mainly used for channel estimation, fine frequency and symbol timing offset estimation. lts uses the whole 52 sub-carriers and in each sub-carrier, it is bpsk modulated with phases of π or −π. the sequence number is denoted by"
"in the first experiment, we have compared the tactile experience of funneling for a virtual object, between (1) when it was associated with a (dynamic) visual presentation detached from the body and (2) when no visual presentation is given at all (as a reference). in a usual application setting, virtual objects will normally be rendered without a part that visually extends or connects user's body parts ( figure 2 ). our interest is first to assess whether funneling elicits phantom sensation, its extent and effects to the overall interaction experience. it is well expected that no dislocated phantom tactile experience will be elicited without any visual feedback ( figure 4, bottom right). we still test for it as a base case. also it has been shown through prior research [cit] that singular vibration (e.g. with a single vibrator) cannot create any localized phantom sensation for \"virtual\" object external to the body either. to reiterate, we are interested in and hypothesize the existence and quality of the phantom tactile sensation of the virtual objects even when it does not directly attached to the user body. we also expect that results for the detached \"out of the body\" virtual objects with the hypothesis that results will mostly extend to detached \"out of the body\" physical objects"
"this paper evolves new priority for each device considering local device and corresponding zone priority. to the best of our knowledge, this work has not been practiced yet. the goal of this paper is to construct a reliable zigbee network for device synchronization using the node priority mechanism and study its qos."
"observation 1 (different priority devices). during this observation, a lower priority (node 1) and a higher priority (node 10) device have been considered from zone 1 and zone 4, respectively. initial priorities of node 1 and node 10 are 1 and 4 accordingly. after computing with (5), the new priority ( ) values are 1.41 and 5.64 for node 1 and node 10, respectively. therefore, node 10 gets the procession first in this case. again, the updated priority scenario has been observed for node 23 and node 29 from zone 2 and zone 6. the initial priorities are 5 and 4 for node 23 and node 29 sequentially. after computation, the new priority ( ) figures have been found at 7.05 and 5.64 for node 23 and node 29, respectively. though node 23 is from the lower priority zone, the new priority allocates it to get privilege for the first processing compared to node 29 to the coordinator."
"to reduce the inter-symbol interference (isi), there is a part of guard interval (gi) in the lts part, besides the two long symbols. the length of gi is half that of a long symbol. since the gi is not used in our approaches, we do not show it explicitly in fig. 2 . the sts is primarily used for frame synchronization, automatic gain control (agc), and coarse frequency offset estimation. sts uses 12 ofdm sub-carriers which are symmetric to the dc. the sequence number is given by for each sub-carrier, the sts is bpsk modulated with the amplitude of 13 3 . the modulated phases are"
"the exact computation of the median-distance is intractable, as it involves executing a point-to-point shortest-path algorithm in every world and taking the median. a natural way to overcome the intractability of computing the median-distance is to approximate it using sampling. the idea is to (i) sample r possible graphs according to p, and (ii) compute the median of the shortest-part distances in the sample graphs. using the chernoff bound we have the following standard result:"
"the key idea of our algorithm is to replace the calculation of density from individual points to discrete bins as defined by a uniform grid. this way, the clustering step of the algorithm will scale with the number of non-empty bins, which is significantly smaller than the number of points in lower dimensional data sets. therefore the overall time complexity of our algorithm is dominated by the binning step, which is in the order of o(n). this is significantly better than the time complexity of dbscan, which is in the order of o(nlogn). the definition and algorithm are presented in the following subsections."
"discovery is an important use case that model metadata must support. in hydroshare, the metadata model for all resources was designed to support discovery. however, the search interface design that exposes metadata elements within the existing data model is still under active developement. currently, users can discover hydroshare resources by searching and filtering model resources using many of the dublin core metadata elements implemented in the hydroshare data model (i.e., the generic resource metadata also, if a user would like to discover all model program resources that are compatible with a specific operating system, the system could use the metadata element modeloperatingsystem."
"tactile feedback has become almost indispensable in improving interaction experience. realization of tactile feedback by using vibration devices is one inexpensive and practical method. however, due to its size and mechanics, a single vibrator scheme is most often employed and it is only able to convey simplistic on-off type of events. instead, as a way to improve the tactile experience, a more advanced form of vibro-tactile feedback most often involves an array of vibrators [cit] that brings about mechanical and cost complications and a constraint that a relatively significant area of the body has to be in full contact with the array."
"we assessed the quality of our functions in real-world data. our functions identify better neighbors than their competitors. in addition, our extensive empirical analysis confirmed the efficiency of our methods. we also observed that larger probabilities of the edges result to more effective k-nn pruning. future work involves enriching our framework with more powerful models that can handle node failures [cit] and arbitrary probability distributions."
"in the current set of studies, we aim to address these issues by creating higher-quality morph images and investigating both human and computer detection of these images. it is important to determine whether people accept morphs, or can detect their use, when every effort is made to produce images that reflect real-world fraud. for example, if training methods were implemented with the assumption that morph detection would be significantly improved, this might result in a false sense of security (literally) for passport control and issuing officers. therefore, in this paper, we investigate human morph detection performance with and without training, reflecting a passport-issuing context (experiments 1 and 2), whether people accept morphs as id images in a \"live\" task, reflecting a border control scenario (experiment 3), and, finally, whether computational modelling outperforms human detection, providing a more suitable alternative than training people (experiment 4)."
"we calculated the mean location along this dimension of the 86 training individuals and, separately, the mean location of the 86 training morphs. these two values were used when classifying untrained images -each novel image was labelled according to which of the two points it fell closest to on the dimension. finally, we calculated percentage correct, along with d' as before: hitthe image was a morph and was categorized as \"morph\" by the model; and false alarm -the image was an individual and was categorized as \"morph\" by the model."
"locally, the higher prioritized nodes get a chance to send application traffic via a router to the other end devices. there are two sets of priority values. one is zone's priority and the other is an individual node's priority. when end nodes try to communicate beyond their respective zone at the same time, then priority resolution is needed in coordinator for global access. as the two sets of priorities, a generalization has been done between the individual node and zone priorities. standard deviation calculation is the process to find out general deviation from a set of values. in this context, standard deviation relates to this two-set priority with the same deviation value [cit] . that paves the way for generating new priority for global deviceto-device communication. simply, in the global priority calculation is as follows."
"each zone has a priority value. the higher priority value makes the devices of the zone executed first in the coordinator. the zones' priority value and execution of these nodes according to the zone value are considered as global processing. on the other hand, every zone consists of numerous zigbee nodes and each node has to adjust priority values for their local processing. the local processing concerns the end devices application traffic processing based on their respective priority values."
"the model instance resource describes the input files used for execution by a model program. a model instance resource may optionally include the output files resulting after execution. output for some models can be large. given that these files can be recreated by executing the model, we made including output files optional. the design for metadata associated with a model instance was intended to capture the aspects required to define and distinguish between different model instances across the wide variety of environmental models. to accomplish this, the design first includes a generic model instance. this generic model instance has metadata elements applicable to any model program instance. the design is extensible including specific model instances that inherit the properties of a generic model instance and add new properties that are relevant to one or more model programs. this pattern is illustrated in fig. generic model instance resource type. however, if available, a specific model instance resource type should be used to take advantage of enhanced functionality and metadata capture. by dublin core (with names shown using the \"dc\" and \"dcterms\" prefix). one metadata element defined in dublin core that is particularly important for model instances is the coverage element."
"finally, after completing either the morph training or control task, participants were given a post-training morph detection task. this was identical to the pretraining version (see above) except that a different set of 60 faces (the remaining faces from our set of 120) appeared in the arrays."
"the classifier module utilizes weight vectors (from the weight vector reader module) and the weight vector for the unknown image (from the weight vector finder module). then the classifier finds the distance between each weight vector from the weight vector reader module and the weight vector of the unknown image. for each calculation of distance, it compares the current distance value with the previous one. if the current value is smaller, then it is stored to the distance buffer. finally, the index of the identified face, which corresponds to a minimum distance in distance buffer, is sent to the display (or other output device) as an identified face."
"in mobile ad-hoc networks, mobile nodes move and connect to each other. the connectivity between nodes can be estimated using measurements [cit], and the notion of the \"delivery probability\" can be used to quantify the probability that a given node can deliver a packet to another node [cit] . therefore, k-nearest neighbor queries can be used for addressing the probabilistic-routing problem [cit] ."
"the metadata framework design considers a computational model as two distinct concepts: 1) a model program resource, which includes software for executing a model simulation and generating outputs, and 2) a model instance resource, which includes the input files and, optionally, the output files for a specific simulation. having model programs and instances as separate resources allows a specific version of a program to be linked to several instances. if model programs and instances were stored together as one resource, the same model program would be stored with each model instance executed by that model program. additionally, with instances and programs combined, the metadata describing the model program would be repeated with each model instance. this would result in redundant data about the same model program that would need to be entered every time the user uploads an instance for sharing. this may lead to opportunities for inconsistent metadata entry by users for the same model program included in multiple resources. in order to avoid redundantly storing the same program and its metadata with each related model instance, we separated model programs and instances as distinct resource types and implemented an \"executedby\" relation as a many-to-one to link between any number of instances and the program used for execution."
"next we consider the notion of expectation. in most cases, the expected shortest path distance is trivially infinite due to the presence of ∞. thus, we seek a more meaningful definition. we consider only the events that a graph contains a path between s and t. taking expectation over the noninfinite distances gives the following:"
"while this generic metadata could be used to describe, share, and discover model programs and model instances, it misses many other properties of these resources that could be leveraged for improved search, discovery, and use of model resources. although these properties are generally included in the configuration files of the model, each model has unique configurations files, making it difficult, if not impossible, for interested users and/or an automated system to extract the pertinent metadata across models. the purpose of the metadata analysis and design presented here is to provide a more thorough, detailed metadata approach for model programs and instances."
"(1: not at all ~ 7: very well) table 5 shows that the sensed tactile locations after saltation were statistically different from the actual location of stimulation for both when visual effect was given and, surprisingly again, when not given at all. note that, according to miyazaki, phantom sensation from saltation was not observed without visual feedback nor mediating object [cit] . while our result is somewhat contrary, its extent was very small (see figure 11 (b)). table 5 . effects of saltation. perceived location for s2 were in all cases different from s1 (both s1 and s2 stimluations were given at the same physical location) with statistical significance (p-values) indicating the existence of the saltation effect. higher variance and lower accuracy/match (even at l1/p1) was observed as compared to the case of funneling, getting worse at the place of the third stimulation. when there was no visual feedback, we were only looking for (if it existed) one location of phantom sensation somewhere between the fingers. as mentioned above, this nonvisual case did exhibit a phantom sensation at one location significantly different, though very small, from the place of actual stimulation, around 0.83cm away. postbriefings with the subjects also reflected the observation this sensation was barely perceivable."
"zigbee network shows more efficiency on priority mechanism for setting up nodes in various zones of a home or office. if any zone has higher priority, then its activity will increase compared to zones with lower priority or without priority. the investigation of various qos parameters is in progress for the proposed zigbee network."
"a literature review is performed to find out the wide range use of wireless sensor network. based on the literature review, zigbee is selected to develop its versatile use of forming a smart network for home automation and office automation and performing internet of things concept. a careful study is done on how zigbee performs internet of things and how to enhance its qos. afterward, an enhanced way has been found in the zigbee network. in this study, zones and device request synchronization is performed based on the proposed priority method to construct an efficient zigbee network."
"is larger than ρ% of its directly connected bins, where ρ is a user-specified parameter. 3 den c (c i ) is larger than minden c, a user-specified parameter."
"3 the on-screen locations of the morphs and exemplars were randomized for each trial; the 60 faces (30 morphs, 30 individuals) were randomly selected from the set of 120 images described above."
"the rff identification system contains three main parts: rff feature extraction, rff library establishment, and legitimacy testing. we assume that there is a set of legitimate devices to initialize the rff library. firstly, an rff library of legitimate devices should be established after authentication. secondly, when a rogue wifi device tries to connect with the other one, our system must detect and reject it accurately. in this subsection, we briefly introduce the signal acquiring technologies which are the preliminaries for rff feature extraction."
"in the second article, the researchers investigated whether people were able to detect morph images and whether training could help with this task [cit] . participants were shown ten-image arrays containing a mixture of the morph images and exemplars (original, unmorphed faces) used in the previous article and were asked to identify which were the morphs. performance was poor, with the 50/50 morphs resulting in average d' sensitivities of 0.56 and 0.96 (for the two groups that took part: training versus none), suggesting that morphs were not readily detected. however, providing information regarding the nature of the morphs, along with some tips to help with identifying them, resulted in a significant increase in sensitivity (to 2.69 and 2.32, respectively). an additional training protocol, in which feedback was provided via a twoalternative forced choice (2afc) task, also led to a further benefit for the group that received it (the first mentioned in the values reported above). the authors concluded that people were poor at detecting morphs, but that training could significantly improve performance."
"one previous study has investigated face matching performance with morphs [cit] but used a computerized version of the task and only investigated morph acceptance for one of the two people pictured in each morph image (i.e. morph ab was compared to person a but not person b). in addition, as discussed earlier, those morphs were of a lower-quality in terms of realism than the ones used in the current work (see fig. 1 for comparison between face sets). in this experiment, we investigated whether our highquality face morphs provided acceptable identification photographs for use by both of the original people featured in the morph. in addition, we utilized a live face matching context rather than a computerized task in order to better understand the everyday process whereby people present their photo-id documents for consideration."
"this section gives a detailed description of the analysis of the obtained simulation result. the result is prepared based on the zigbee's (application and mac) layer attainment analysis. the graphs are generated by analyzing different qos parameters (i.e., end-to-end delay in the application layer, mac load, and mac delay in mac layer)."
"this paper investigated the rff feature extraction and identification issue for 802.11n devices in practical scenarios, in which the training and testing locations are different. we presented a differential phase approach using received pilots. we found that it is efficient for a small number of devices even at different training and testing locations. however, it is not applicable to the identification for a large number of devices due to the low-dimensional feature space. therefore, we explored another location-invariant rff feature suitable for large-scale devices. we found that the rffs of two ltss exhibit semi-steady characteristics since their frequency spectrums are different. inspired by this fact, we proposed a novel rff feature aoq leveraging the channel coherence between two ltss in an 802.11n beacon frame preamble. we demonstrated that aoq is invariant to wireless channels. we further addressed the problem of noise amplification by collecting aoqs from multiple locations to constitute a robust aoq feature. simulation and experiment results indicated that our proposed aoq approach can provide a good identification accuracy higher than 95% when snr is higher than 40 db and an eer around 4% for 55 wifi nics. in the future work, we will study how to improve the performance of aoq in low snr regions. furthermore, more deep learning methods can be further investigated [cit] ."
"biological networks constitute one of the main applications of probabilistic graphs. nodes represent genes and proteins, and edges represent interactions among them. since the interactions are derived through noisy and error-prone lab experiments, each edge is associated with an uncertainty value [cit] . in protein-protein interaction networks [cit], possible interactions have been established experimentally for permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. articles from this volume were presented at the a limited number of pairs of proteins. identifying protein neighbors in such interaction networks is useful for predicting possible co-complex memberships [cit] and new interactions [cit] . thus, k-nearest neighbor queries can be used to provide candidate links, whose validity can be further explored through strenuous biological experiments. in large social networks uncertainty arises for various reasons [cit] . the probability of an edge may represent the uncertainty of a link prediction [cit] or the influence of a person to another, as for example in viral marketing [cit] . thus, in the context of social-network applications, we are interested in queries such as: \"who are the ten people that alice influences the most?\""
"finding good neighbors in a ppi network is important. the ppi network consists of uncertain interactions that have been established using biological experiments. the k-nn sets can be used as a filter step to identify candidate neighbors that will be validated experimentally. our experiment demonstrates that our functions produced higher quality neighbors than their competitors, and thus, they constitute better choices for k-nn in ppi networks."
"one approach to address this would be to focus on simplifying the process for creating new resource types. another possibility would be to alter the approach described in this paper so that specific model instances are not implemented as new resource types, but still can have extended metadata for specific model programs. in this case, all model instances would be uploaded using a single resource type, but there would be a mechanism to filter the metadata fields available to the user once the user or system identifies the uploaded model instance as being a specific and known type (e.g., a swat model instance). more research is needed to test these alternative options in terms of their practicality, usability, and scalability within hydroshare."
"with the growing number of systems that serve a role within the larger cyberinfrastructure being built to support science, interoperability between these systems is becoming a more pressing need. if these systems are built from an agreed upon metadata framework, then it simplifies the transfer of resources between the systems. this would encourage each system to specialize in selected use cases while relying on external systems to handle other use cases outside of its scope."
"the findings can be applied to the tactile interaction design using minimal number of actuators on a variety of media platforms including the mobile, holography and augmented reality. from a more practical perspective, we would be much interested in comparing the relative effect between (1) the usual single vibrator scheme and (2) using perhaps 2 or 4 vibrators for 1d or 2d [cit] localized phantom tactile effect. the single vibrator scheme simply cannot be used for the case of \"out of the body\" virtual objects with virtual medium (e.g. two handed interaction with a dynamic holographic object). however, for the case of \"out of the body\" virtual objects with physical medium (e.g. two handed interaction with moving virtual objects on a mobile device), it is unclear whether there is sufficient benefit-to-cost in the additional effort to employ funneling or saltation. single vibrator scheme has been quite successful in eliciting pseudo-haptic effects in smart phones and game controllers [cit] ."
"the current implementation of flowgrid is already very fast for most practical purposes. in the future, if the data size grows even larger, it is possible to further speed up flowgrid by parallelising the binning step of the algorithm, which is currently the most computationally intensive step of the algorithm."
"then the final selected aoq feature is fig. 11 illustrates the selection process of aoq feature. according to the variance of aoqs at three different locations volume 7, 2019"
"recently, probabilistic databases have received increased interest and a number of system prototypes have been developed that can store and query probabilistic data. notable examples include the bayesstore [cit], maybms [cit], mcdb [cit], mystiq [cit], orion [cit], prdb [cit] and trio [cit] . these systems model data with relations and therefore, they cannot perform shortest path computations on graphs efficiently. also, since computing exact answers to many typical sql queries has been shown to have #p-complete data complexity [cit], research has focused on computing approximate answers [cit] ."
"in this paper, we have developed an ultrafast clustering algorithm, flowgrid, for single-cell flow cytometry analysis, and compared it against other state-of-theart algorithms such as flock, flowsom and flowpeaks. flowgrid borrows ideas from dbscan for detection of high density regions and outliers. it does not only perform well in the presence of outliers, but also have great scalability without getting into memory issues. it is both time efficient and memory efficient. flowgrid shares similar clustering accuracy with state-of-the-art flow cytometry clustering algorithms, but it is substantially faster than them. with any given number of markers, the runtime of flowgrid scales linearly with the number of cells, which is a useful property for extremely large data sets."
"according to (19), y l f (p, j) is variant to location l due to h l f (p, j) and n l f (p, j), and thus it cannot be used to identify one device in two different locations. the impact of additive noise can be effectively reduced by smoothing through frames. however, it is challenging to eliminate the negative effect of the multipath channel because h l f (p, j) is multiplicative and mixed with the frequency responses of devices. fig. 6 and fig. 7 show the frequency spectrums of two ltss for four frames from the device wdr-5620 no.4 at location 1 and location 2, respectively. the environment remains static for each location. it is observed that the frequency spectrums of the first lts and the second lts are not the same. besides, the frequency spectrums of each lts change little over time in the same location. however, the frequency spectrums of the same device have significant changes from location 1 to location 2."
"we followed the general procedure used in previous live face matching research [cit] ). the models approached people on campus and stood at a conversational distance. each judge was shown an image corresponding to only one of the three conditions (match, morph, mismatch) and was asked, \"is the image a photo of me?\" judges had an unlimited amount of time to respond to all questions. after their yes/no response was written down, judges were then asked, \"do you have any reason why you wouldn't accept this as an id photo?\" this open-ended question allowed judges to provide their own reasons as to why the image might not be suitable for use as identification. (judges also had the option of giving no reason.) after these responses were written down, judges were given a morph fraud/detection tips sheeta printed/laminated version of the tips screen used in experiments 1 and 2. finally, after receiving this information regarding morphs, judges were asked, \"do you think that this image is a morph?\" their yes/no responses were written down and demographic information was also collected. no feedback was given at any point during the experiment."
"wifi has become a pervasive communication medium in connecting various wireless devices in lan and iot due to the ease of deployment. the wifi protocol includes various physical layer technologies, such as ieee 802.11a/b/ac/g/n and the mixed ones. high rate/ direct sequence spread spectrum (hr/dsss) is used for ieee 802.11b and orthogonal frequency division multiplexing (ofdm) is used for ieee 802.11n. in this paper, we focus on ieee 802.11n devices with the ofdm modulation."
"to create morphs for our morph training task, we paired 40 individuals from our set 2 database and then created 20 morphs (12 female) using the same procedure as above (see additional file 1). similarly, we selected 20 individuals (12 female) from the same set for our exemplar images and processed the images as before. again, these individuals had not appeared in any of the morph pairings."
"coordinator may be broken down to operate the network maintaining convenient qos. the qos maintenance and measurement are a challenging issue for zigbee network. mainly, it refers to the improvement of overall network performance."
"5. we perform an extensive experimental evaluation with large real-world graphs. we show that our algorithms work in practice, and we observe that smaller probability values result in greater processing cost."
. the above conditions take care of the (worst) case that a node will appear with the same distance value in all future dijkstra executions.
"the model program resource encapsulates all of the software and files necessary to identify, install, and run a given environmental model. the model program includes a model engine, which is the core mathematical modeling logic for the model [cit] . this model engine is often, but not always, embedded within a larger application that includes visualization, typically using a graphical user interface (gui), and other utility software. it is not uncommon for multiple model programs to use the same or similar model engine; for example, there are multiple model programs with different user interfaces that all use the storm water management model (swmm) as its model engine. a key design decision was to link a model program with a model instance, rather than a model engine with a model instance. this was done because developers may make subtle but important changes to publically available model engines within their own model programs. thus, it is difficult to guarantee that two independent model programs, both making use of the same original model engine, will produce the exact same output."
"due to the ease of deployment, wifi has become a pervasive communication medium in connecting various wireless devices in local area networks (lans) and the internet of things (iot). unfortunately, the exposed security problems have been increasingly serious because of the openness of radio transmission. for example, insecure authentication protocols, implementation attacks, side channel attacks, impersonation and the replay attack. since these attacks may happen in and below data link layer, soft-identifiers using passwords, service set identifier (ssid) and/or mac/ip addresses are prone to be spoofed. therefore, it is significant to find an efficient method to identify and prevent rogue wifi connections."
"probabilistic graphs are a natural representation in many applications, ranging from mobile ad-hoc networks to social and biological networks. in this paper, we addressed the problem of processing nearest-neighbor queries in large probabilistic graphs. to that end, we extended the concepts of shortest paths and random walks in standard graphs. we defined meaningful distance functions and introduced approximation algorithms based on sampling. our algorithms prune the search space by computing a truncated version of the shortest-path distance distribution."
"the future work will address the zigbee network performance analysis based on different queuing along with the mathematical model. this is ongoing work, including network latency and an efficient load balancing technique for zigbee network. moreover, this work intended to extend to security issue to model a reliable large scale zigbee network."
"designing a metadata framework for environmental models poses unique challenges compared to other data types. first, the data required for models are heterogeneous and, in the case of environmental models, input for a single simulation can include dozens, if not hundreds, of data files. these files describe properties of the modeling elements, parameters, forcing functions, boundary conditions, and other data needed to execute the model for a given system. each model largely adopts its own structure and semantics for storing data, making it difficult to standardize across models. second, environmental modelers make use of a large and diverse set of computational models; [cit] cataloged over 65 models focusing on watershed hydrology alone. environmental modelers will likely continue to make use of a broad range of models because each model is tailored for a given application. as a result, each model adopts unique data structures and semantics for both input and output data. a model metadata framework, therefore, must not force all models into a fixed structure, but rather be flexible and able to accommodate this diversity of models."
"this metadata element defines the temporal and spatial extent of a resource. for a model instance resource, the temporal coverage provides the start and end date/time for the simulation; the spatial coverage provides a place name and geographic coordinates for the model instance. the spatial coverage can be represented by a point (e.g., the centroid of the modeling domain) or a box (e.g., the bounding box of the modeling domain). this coverage element does not represent the exact shape of the model instance, but rather its geographic location or extent."
"the resource description framework (rdf) is used for formally encoding concepts and their associated metadata using a subject, predicate, and object structure (http://www.w3.org/rdf). as a simple example, this basic structure can be used to show that a model instance (subject) is executed by (predicate) a model program (object) (fig. 1) . each resource has core metadata defined by the dublin core metadata framework and extended metadata designed through this research that is encoded and stored on disk using rdf-xml."
"in this section, we present some experimental results on our proposed aoq approach. fig. 13, it can be seen that the optimal threshold value that far equals to frr is 0.25 and 0.4 for 9c-a6-15-42-fc-f5 and d0-d7-83-ee-dd-28, respectively. when the rates of far and frr are equal, the common y-axis value is referred to as the eer. the value indicates that the proportion of false acceptances is equal to the proportion of false rejections. the lower the eer, the better the performance of the authentication system. table ii shows the maximal, minimal and average identification eers of target nics. we first carry out experiments for the nics of each model. one nic is randomly selected as the legitimate device and all other nics of the same model are the illegal ones for identification. the results are shown in the first five rows. it can be seen that cpe-500 achieves the best performance among individual models, with only 1.1% eer on average. wdr-5660 has the worst identification accuracy, but its average eer is still less than 10%. we also evaluate the eer performance for mixed devices, and the eer is 18.1% for the worst case and 4.0% on average. from the experimental results, our proposed approach achieves complete resilience to the wireless channel allowing a device to be correctly classified with near-perfect accuracy in unknown environments."
"the normalized image calculator module finds the differences between the average image and the input image. the average image reader reads the image pixels from the average image buffer, and then the input image pixels are subtracted to find a normalized image. the normalized image is stored in the normalized image buffer."
"in brief, sampling a random world from probabilistic graphs that exhibit dependencies on their edges, may require sophisticated sampling techniques. however, lemma 1 holds, as long as r independent worlds have been sampled from g. this discussion applies directly to lemma 3. in this paper, we do not elaborate further on dependencies, since we are not aware of large real-world probabilistic graphs that exhibit them. however, we foresee such graphs's existence, and thus we believe that studying their properties is an important future direction."
"recently, two articles by robertson and colleagues have specifically focused on human performance in the matching and detection of morphs. in the first, participants completed computer tasks in which they decided whether two face images onscreen depicted the same person or not [cit] . in seven trials, the two images were different photographs of the same face, and in another seven trials, the images were photographs of two different people. for the remaining 35 trials, a face photograph was paired with a morph containing differing amounts of that face and a second person. (when creating morphs, the researcher can specify the percentage weighting of each identity contained in the final image.) the results demonstrated that 50/50 morphs (weighting both identities equally) were accepted as \"matches\" for the faces they were paired with on 68% of trials. after providing instructions regarding the nature of morphs, and with the additional response option of \"morphed image,\" participants subsequently accepted them as \"matches\" on only 21% of trials. taken together, the authors suggested that erroneously accepting morphs as id images was common, but these errors can be significantly reduced through instruction."
"to the face recognition subsystem via the downsampling module as shown in figure 1 . our face recognition system automatically identifies or verifies a person from a digital image, a video frame or a video source while previous works [cit] simply implemented what we describe as the face recognition subsystem. in this work, we describe the design and implementation of a face recognition architecture using eigenface algorithm. we design and implement a face recognition subsystem on an fpga using both pipelined and non pipelined architectures. in each case, we evaluate system performance on a different number of images. then we show how to integrate face recognition and face detection using a downsampling module which is responsible for preprocessing detected face images from the detection subsystem to satisfy the requirements of the face recognition subsystem."
"therefore, as the system becomes more widely used, searches can be tracked, which will help guide future exapansions of the metadata to better support common queries."
"different features may have different granularities in device identification giving rise to trade-offs in false positive and false negative rates. once features are extracted, the next step is to develop identification algorithms that utilize these features for device identification [cit] . in general, an rff based device identification system includes two phases, i.e., training and identification [cit] . at the training phase, the receiver will first acquire signals, extract features, and save them as a template library for reference from the legitimate devices. during the identification stage, the receiver will obtain signals from the target devices, compare the same type of features with the legitimate ones in the library, and classify the devices based on the similarity between these features. due to the characters of scalability, accuracy, energy efficiency and tamper resistance, rff has been widely studied for device authentication in the iot networks [cit] . many rff prototypes have been reported among various iot systems, including uwb [cit], gsm [cit], 802.16 wimax [cit], lte [cit], wifi [cit], zigbee [cit], lora [cit], bluetooth [cit], rfid [cit], wireless audio communications [cit], usrp [cit] and so on."
"1. the random variables di are independent identically distributed and they are bounded in the range [0, n − 1]. they also have the same expectation"
"here, we explored morph detection using a forced choice paradigm. by presenting a single image on each trial, with participants deciding whether the image is a morph or not, we have a clear notion of chance performance (i.e. 50%) and there are no other images on screen with which to simultaneously compare the image in question (unlike experiment 1). in addition, making decisions based on single images more closely parallels the real-world task of morph detection that passport officers and other professionals carry out each day. there is no obvious situation in which a large array, containing both morphs and individuals' images, would be presented for consideration. finally, we used a more limited \"training\" design in that half of the participants viewed the morph detection tips screen before the detection task. we chose not to include the full training task (utilizing feedback) featured in experiment 1, given that our results provided no evidence that this method of training resulted in improved performance."
this work was supported by the national science foundation under collaborative grants aci-1148453 and aci-1148090. we greatly thank and acknowledge the work of the larger hydroshare development team.
"note that the median distance may be infinite for some pairs s and t and that all results presented in this paper for median-distance, hold for any k-th order statistic as well."
"in figure 7 (a) we present the performance of the transformation in terms of success for the k-nn query. success in the k-nn query was computed using jaccard's coefficient for the k-nn sets of our method and the true k-nn. we consider the true k-nn to be the result using 50k samples for the transformation, after observing empirically that the results were stable for that number of samples. the transformation scales linearly to the number of samples and it can take a few minutes (for 1000 samples) to a few hours (for 50k samples) for biomine, using one cpu. however, we remark that the transformation can straightforwardly be parallelized since it is local to a node and its edges. in order to compute the stationary distribution for the random-walk-distance we performed 1m random walks per experiment after empirically observing that this number was large enough. the teleport probability was set to 0.20. notice that less than 1k samples in dblp and 10k in biomine yielded more than 90% accuracy. in flickr which is a more volatile graph since it is very dense and has edges of extremely low probability, the performance is around 80% at 50k samples and we need to sample 200k worlds to reach 90% performance."
. if there are no outgoing edges we stay at the same node. we call this process probabilistic-random-walk. we define random-walkdistance as (see appendix c for examples and details):
"although experiment 1 demonstrated that neither the tips screen nor the morph training led to an improvement in performance, the current experiment produced more ambiguous results. we found that viewing the tips screen did not significantly increase morph detection abilities, but those participants did perform at above chance levels. in comparison, responses given by participants in the control group did not differ from those expected by simple guessing."
"twenty paid subjects (15 men and 5 women) participated in the experiment with the mean age of 25.2 (a different pool from experiment i). the experimental procedure was mostly identical to the first. thus, we only describe the way saltation (i.e. timed stimulation) was administered in the treatments. figure 9 pictorially describes how the timed vibro-tactile stimulations were given to create saltation effects. a total of three consecutive stimulations were given, each labeled s1, s2 and s3. the first two stimulation were given at p1 and the third at p5, intending to create a phantom sensation somewhere between p1 and p5. inter-stimulus intervals of s1-s2 and s2-s3 were given with the intention to create phantom sensations at the prescribed positions, namely, p1~p5 (800ms-50ms) respectively (and stimulation duration of 80ms) based on recommended values for best effects from prior research and our own previous experiments [cit] . half of the saltation treatments were administered with right to left stimulations (at p5, then p1) and the other half, in the opposite directions (at p1, then p5). the visual effects were rendered at p1, p5 (where the actual stimulations were given) and at the intended location of sensation and stayed on for 80 milliseconds, same as the stimulation duration. as shown in figure 10, the visual feedback was given, synchronized with the corresponding of the three tactile stimulations (s1~s3), at three locations (two at the finger tips where the actual stimulations are given and one in between at the intended location of sensation, one of p's). we stress again that the users were asked of the location and quality of the tactile feedback, not the visual. fig. 9 . the rendering method for saltation stimulation. three timed stimulations of s1 (at p1), s2 (at p1) and s3 (at p5). each subject experienced, in a balanced order, a total of 72 positional feedbacks in all the conditions: 60 times with visual feedback (2 directions, 5 intended locations of sensation and 6 repetitions) + 12 times without visual feedback (2 directions and 6 repetitions). each stimulation was followed by a 10 second inter-stimulus rest interval. as it was so in the first experiment, for each treatment condition, two exact same stimulation patterns were given, then subjects were asked to indicate the place of phantom sensations in terms of the five prescribed positions (l1~l5). the subjects were explicitly asked to report the place of tactile sensation (e.g. rather than visual) right after experiencing the stimulation. in addition, after all trials, they were asked to answer a short survey about their subject feelings (questions shown in table 4 ). the fifth survey question asked of the perception regarding the directionality. table 4 . the five survey questions for experiment ii regarding the subjective feel for the phantom sensation answered in 7 likert scale."
"all images were shape-standardized by morphing them to a template derived from the average shape of the training set [cit] . this standardization was based on the alignment of 82 fiducial points for each image (e.g. corners of eyes, corners of mouth, etc.; [cit] . assignment of these fiducial points was carried out using a standard semiautomatic process requiring just five manually entered landmarks [cit], for details) . pca was then computed on these normalized images."
"interestingly, [cit] and kilgard [cit] have found that the phantom sensation was much influenced by the subject's focus of attention, anticipation and/or the line of sight. this strongly hints the possibility of further synergistic effects with more apparent visual effects associated with the intended phantom sensation."
"finally, to study the effect of the edge-probability values on the pruning efficiency, we conducted the following experiment: we boosted each edge's probability p, by making it"
"the number of samples is polynomial and not exponential as the brute-force algorithm. at first glance, this can still be prohibitively large due to the factor (n − 1)"
"twenty paid subjects (15 men and 5 women) participated in the experiment with the mean age of 25.5. after collecting one's basic background information, the subject was briefed about the purpose of the experiment and instructions for the experimental task. a short training (3 minutes) was given for the subject to get familiarized to the experimental process. in addition to the head mounted fixture for the camera, the subjects wore ear muffs to prevent any bias from the sounds of the vibration. the ear muff was tested to make sure so that no sound could be heard during the experiment, and did not affect the outcome of the experiment. the levels of stimulations were given with the intention to create phantom and real sensations at 5 equi-distanced locations between two fingers. the linear variation of stimulus amplitudes methods of alles [cit] (figure 4 ) was used with the stimulation duration set at 200ms. preliminary studies and prior research has also confirmed that the aforementioned linear method and stimulation duration exhibited the best effect [cit] . the visual feedback appeared at the intended location of sensation, at the time of the stimulations, stayed for 200 milliseconds (same as the tactile stimulation duration) and disappeared."
"when a node tries to connect globally, then its possibility to be executed in the coordinator depends on the node's respective priority value and its zone value. as its zone priority value is in a generalized form which is the standard deviation 1.41, it is quite easy to compute the new priority for different nodes in their global participation. the following equation generates new priority for all the participating devices, computing all the individual node priorities along with its corresponding zone priority:"
this paper presented a set of observations with the emphasis on the priority of different zones and their nodes. the results were obtained by performing a simulation study. this research comes to several conclusions from zigbee network along with its qos parameters.
"interface (gui) for how a user selects a model resource type within hydroshare. in the current implementation, the model resource types are grouped together under the modeling title. once the user selects the desired resource type, adds a title, and uploads the related files, the new resource is created in hydroshare and the user sees the landing page for this newly created resource. at this point, a unique identifier specific to the hydroshare system has been automatically assigned to the resource. later, if the user decides to formally publish the resource in hydroshare, a more formal digital object identifier (doi) would be assigned to the resource. after a resource is formally published and a doi is assigned, the user can no longer make changes to the resource metadata or the uploaded files. prior to formal publication, authorized users can make changes to the resource at any time. step 11 is highlighted to indicate that only model instances require coverage and not model programs. another important aspect of the model instance resource is the coverage metadata. fig. 14 shows how the coverage metadata appears in the resource's landing page in edit mode. as explained above, there are two types of coverage metadata elements: spatial and temporal. all of the spatial metadata is expressed in world geodetic system (wgs) 84 coordinates, which is used throughout hydroshare. this allows standard web tools to search the metadata easily without full gis functionality. however, users must be aware that errors can be introduced if the spatial data is transformed from another coordinate system to wgs 84. for the use case, the spatial metadata was entered for this model instance as a two-dimensional bounding box (rather than an xy point)."
"face recognition is a challenging research area in terms of both software (developing algorithmic solutions) and hardware (creating physical implementations). a number of face recognition algorithms have been developed in the past decades [cit] with various hardware implementations [cit] . all previous hardware implementations assume that the input to the face recognition system is an unknown face image. current hardware based face recognition systems are limited since they fail if the input is not a face image. a practical face recognition system should not require the input to be a face, instead would recognize face(s) from any arbitrary video which may or may not contain face(s) potentially in the presence of other objects. therefore, an ideal face recognition system should first have a face detection subsystem which is necessary for finding a face in an arbitrary frame, and also a face recognition subsystem which identifies the unknown face image."
"hydroshare and other systems. ideally, this would be done within the model program source code itself, but it could also be implemented as an external utility program. hydroshare and other systems could then read this file for automatic metadata extraction."
"a k-nn query on a probabilistic graph g, consists of a source node s, a probabilistic distance function dp(s, t), and k. the distance dp may be any of the distances dj, dm, der, or drw. the k-nn problem is the following:"
". we can now prove lemma 3 for unweighted graphs: proof. the first part of the lemma is a direct application of the hoeffding inequality, theorem 2. observe that assuming connectivity, any distance in an unweighted graph takes values between [0, n − 1], where n is the number of vertices in the graph. the second part of the lemma is a direct application of the chernoff bound, theorem 1."
"while the two tested phantom sensation techniques generated similar qualitatively enhanced tactile experience (both quantitatively and qualitatively), the funneling technique produced higher overall accuracy than saltation. on the other hand, saltation due to its nature seems fitting as a mean to provide directional tactile experience. the post-briefing also revealed the same. while both subjects answered both techniques did produce phantom sensations for certain, they also felt the funneling to have produced more efficient and stronger sensation."
"to demonstrate the metadata design, we used the application of a swmm model used to study flooding in an urban watershed [cit] as a use case. we wish to publish the resulting model instances online. there are many motivating factors for doing this. first, we believe that a model instance, like the journal paper, is an important product from the research and should stand on its own as a citable product. second, we want to foster ways for other scientists to build from or reuse our model to address their own scientific research questions. third, we want to ensure that the model program used in our study, including the model engine, utility software, and documentation, is captured within a single online resource. this is important because, after some time, the model program developers may not provide this particular version of the software on their website. lastly, this is a way of meeting the research sponsor's data management obligations."
"previous work suggested that morph detection was difficult, but that providing a tips screen with an example morph, along with suggestions for how to identify them, resulted in significant improvements [cit] . in contrast, performance with our higher-quality morphs found even lower levels of detection and no benefit from tips or training (see fig. 2) . importantly, the tips we provided were specifically tailored to our morphs in order to give participants the opportunity to gain useful knowledge that might help with detection. it is worth noting that, had we used the tips provided by the previous researchers, participants would have been searching for artefacts and signs that were never present. in reality, as morphs become increasingly sophisticated, there will inevitably come a point at which there are no visible signs (at least, to humans) betraying the image as a morph. indeed, we may already be reaching that point with our images, given the lack of a tips/training benefit. therefore, it seems reasonable to assume that no method of training will provide human viewers with the ability to detect morphs currently or, at the very least, in the near future."
the deviation is the same for both the devices' priority and zones' priority. the standard deviation method has been applied for the two sets of priorities in order to make it generalized for further calculation.
"we illustrate our results using roc curves in figure 3 . in both ppi and dblp datasets, our functions expectedrel, median, majority clearly dominate the rest. in contrast with their main competitor, mostprobpath, which essentially treats probabilities as weights and relies on the length of just one path of the probabilistic graph, our functions take into account the possible-world semantics and use the entire shortest-path length distribution. therefore, they yield higher quality results. for instance, in the ppi network, for a false positive rate (fpr) of 0.2, all three yield a true positive rate (tpr) above 0.6, while the mostprobpath yields a tpr below 0.4. the randwalk and reliability functions are clearly worse than the ones based on shortest paths, but still more informative than the baseline random classifier. contrary to all of our distance functions, both competitors do not work for weighted probabilistic graphs, since they do not take weights into account. for example, the most probable path could have an arbitrarily large weight in a weighted probabilistic graph. to be fair, we use unweighted probabilistic graphs for both experiments. we remark that even though expectedrel dominates marginally median and majority, the appropriate function for other datasets could be different. thus, it should be chosen based on a similar qualitative analysis as the one presented above."
"dblp. we created a dblp graph by considering an undirected edge between two authors if they have coauthored a journal paper. we labeled the edges with probabilities as described in section 5. figure 4 (a) shows the edge-probability distributions in the three datasets. notice that dblp has only a few probability values. observe also that flickr probability values are generally very small, while biomine has a more uniform probability distribution. additional details about the datasets can be found in the appendix d."
"equal weight, equal probability. consider a graph where each edge is equally probable to appear with probability p, and all weights are equal to 1 (or to any other constant). this model is the probabilistic analogue of an erdős-renyi graph restricted to a given topology defined by the set of edges e."
"the results of this experiment demonstrated two important findings. first, the ability to detect morphs was poor on this task. sensitivity was only slightly above zero (and not different from zero in some cases). second, providing training did not lead to an improvement. indeed, accuracy during training was at chance levels and failed to improve over the 20 trials, despite the feedback that was given after each response."
"figure 5(a) and (b) each shows the sensed/perceived locations of the tactile sensation (vertical axis), elicited by funneling, as reported by the users vs. the intended locations (horizontal axis) of sensation with visual effects and without. to our surprise, even without a virtually mediating object, phantom sensations were perceived at all five intended locations ( figure 5(b) ). this is a first time discovery to our knowledge. there is still clear marked difference in the accuracy (or variance) for the intermediate locations, p2~p4. note that p1 and p5 are where the vibrators are actually located, thus, a correct perception even without visual effect is naturally expected. also note that the perceived locations were different among each other with statistical significances (see table 2 ). thus, a high localization controllability (~± 4mm) at approximately 2cm resolution was possible. consistently to the statistical results, subjects reported that when no visual effects were given, it was difficult to differentiate between p1 and p2 (and similarly for p4 and p5), where the vibration motors were actually placed. anova revealed statistically significant differences existed in the senses locations between when the visual effect was given and when it was not, at all five locations except at the middle, l3 ( figure 6 and table 3 ). note that with funneling, when equal stimulation strengths are given at the two finger tips and it is plausible to think that it would be easier (that is, no help needed with the visual effect) to perceive the phantom sensation to come from the middle and make the proper response. fig. 6 . a pair-wise comparison of the perceived locations between when visual effect is given (square) and not (circle). star marks indicate those with statistically significant differences. table 3 . statistical differences (p-values) in the \"differences\" of perceived locations between when with associated visual effect (e.g. l1) and without (l1')."
"as table 1 illustrates, performance on the forced choice morph detection task was poor. in order to determine whether viewing the tips screen before completing the task resulted in improved performance, percentage correct was analyzed using an independent samples t-test."
"a supported simulation tool was needed to design and implement a complete network model. a deep study has been performed on many simulation tools such as network simulator 2 (ns2), network simulator 3 (ns3), omnet++, riverbed [cit], and matlab, which support zigbee network development features. the riverbed is chosen to design and implement zigbee network as a simulator for its userfriendly interface and its wide range of acceptance. moreover, riverbed simulator provides the fastest discrete event simulation used to evaluate various parameters of network performance [cit] . the selected tool is installed in an intel core i5 processor-based workstation."
"we also use three data sets from the seaflow project [cit] and they contain many samples. instead of analysing the independent samples, we analyse the concatenated data sets as the original paper [cit] and these concatenated data sets contain 12.7, 22.7 and 23.6 millions of cells respectively. each data sets include 15 features but the original study only uses four features for clustering analysis. the four features are forward scatter (small and perpendicular), phycoerythrin, and chlorophyll (small) [cit] ."
"previous research [cit] showed poor performance on the baseline morph detection task (although at levels higher than in the current experiment) and substantial improvements after receiving detection tips and training. as fig. 2 [cit] are strikingly different when displayed on the same axes. it is clear that our morphs were more difficult to detect; even when we provided a tailored set of tips (specifically updated to address characteristics that may reveal flaws in our images), no performance increase was seen."
"the efficiency performance is measured by the runtime while the clustering performance is measured by adjusted rand index (ari). ari is used to measure the clustering performance. ari is the corrected-for-chance version of the rand index [cit] . although it may result in negative values if the index is less than expected, it tends to be more robust than many other measures like f-measure and rand index."
"the experimental system is shown in fig. 1, which works at 2.4/5.8 ghz industrial, scientific and medical (ism) band. we aim to classify 55 wifi nics of 5 different models from 3 manufacturers. we implemented our work on a ubuntu-16.04-amd64 pc with an intel core i7-4790 cpu @ 3.60 ghz processor, and this pc is connected to an ettus usrp transceiver to form an identification server."
"however the distinction must be made clear in terms of the role of the visual feedback. the single vibrator scheme can be explained to be a phantom or pseudo sensation directly caused by the visual feedback, while our paper has addressed the phantom sensation being strengthened reversely by the visual effect. note that our performance measures were tactile experience, and not visual. this suggests, that although our experiment has only tested the case with virtual medium, it is plausible to expect that the tactile experience to be significantly richer than the case of single vibrator scheme, e.g. if applied to mobile devices. the plausibility is high also from the previous research indicating the weakened sensation when the \"connecting\" medium was virtual [cit] . either way, the combined effect can be explained by the modality competition theory that the modal fusion will depend on the disparity among the stimulations in term of their consistency [cit] . for instance, the disparity between the location of the phantom sensation and the visual feedback seems less than that that in a single vibrator scheme."
"biomine. this is a recent snapshot of the database of the biomine project [cit], which is a collection of biological interactions. interactions are directed and labeled with probabilities."
"while this use case is specific to scientific research, a similar use case could be followed for consulting or industrial modeling activities. while such model applications may not result in journal publications, there is still significant value in descriptive metadata for internal cataloging and archiving purposes. additionally, in such cases models can be shared privately within hydroshare allowing collaboration among specific users while keeping the data, model, and results confidential."
"for clarity in this experiment, we labelled our participants as either models or judges. models were those individuals who appeared in the images used and subsequently presented these images and collected responses. judges, in contrast, were those who viewed the images and gave their responses, providing the current data for analysis."
"we explored the responses given when judges were asked, \"do you have any reason why you wouldn't accept this as an id photo?\" of the 1410 judges, only 18 gave reasons that specifically included mention of computer manipulation or similar, e.g. \"doesn't look real,\" \"looks filtered,\" \"looks photoshopped,\" \"avatar-like.\" of these, 15 judges took part in the morph condition (where a total of 470 judges viewed morphs). fourteen additional judges in the morph condition gave reasons that were less specific but suggested they had an issue with the appearance of the face, e.g. \"face looks strange,\" \"hair line is strange,\" \"looks odd,\" the remaining reasons given by judges for not accepting the photos either involved general issues with lighting, that the model was displaying a smile, the background color was inappropriate, or that the model was wearing a headband. these judges were approximately evenly distributed across all three conditions. the remaining judges either gave no reason or mentioned not accepting the image based on it not sufficiently resembling the model presenting it."
"recent technological advancement has made it possible to quantitatively measure the expression of a handful of protein markers in millions of cells in a flow cytometry experiment [cit] . the ability to profile such a large number of cells allows us to gain insights into cellular heterogeneity at an unprecedented resolution. traditionally, cell types are identified based on manual gating of several markers in flow cytometry data. manual gating relies on visual inspection of a series of two dimensional scatter plots, which makes it difficult to discover structure in high dimensions. it also suffers subjectivity, in flowpeaks and flock are largely based on k-means clustering. k-means clustering requires the number of clusters (k) to be defined prior to the analysis. it is hard to determine a suitable k in practice. flowpeaks performs k-means clustering with a large initial k, and iteratively merges nearby clusters that are not separated by low density regions into one cluster. flock utilises grids to identify high density regions, which the algorithm then uses to identify initial cluster centres for k-means clustering. this grid-based method of identifying high density region allows k-means clustering to converge much quicker compared to using random initialisation of cluster centres, and also directly identifies a suitable value for k. flowsom starts with training self-organising map (som), followed by consensus hierarchical clustering of the cells for metaclustering. in the algorithm, the number of clusters (k) is required for meta-clustering."
"probabilistic-random-walk extends random walks, which have been extensively studied [cit] . applications of random walks range from web search [cit] to clustering [cit] and nearest-neighbor search [cit] . finally, the need to store and query massive graph data has lead to an increased interest in graph databases [cit] . the focus here is on standard graphs, not on probabilistic."
"we present an experimental evaluation of the pruning algorithms introduced in section 4.2. we implemented the algorithms for both the median and the majority distances. tie-breaking was done by extending t k (s) to include all objects tied with t k . we experiment with the two most important components of the algorithm: efficiency and quality of the results. we measure efficiency for each run of a k-nn algorithm as a fraction of the number of the union of the visited nodes in all executions of the dijkstra algorithm, over the total number of nodes in the graph. the reason is that the number of visited nodes determines the cost (node retrieval and histogram maintenance). other aspects of efficiency, such as the number of worlds sampled, can be taken into account and factored in the presented graphs. figure 6 (a) shows the fraction of visited nodes as a function of k for the median k-nn problem and 200 worlds. the efficiency decreases sublinearly as k increases. note that a node is counted as visited if it is visited in at least one of the worlds. figure 6(b) shows the fraction of visited nodes as a function of the number of worlds sampled for the majority 10-nn problem. as expected, efficiency decreases with the number of worlds but it stabilizes for some hundreds of worlds. in both plots, all three datasets yield similar behavior. we also measured wall-clock times in cpu ticks and report the speedup of our pruning techniques in table 1 (averaged over 100 queries). observe that the gains are large, and that they decrease as k increases. for example, computing the median 5-nn with pruning and with 200 worlds in biomine was 247 times faster than without pruning; it took 0.5 seconds instead of 123. the wall-clock gains with respect to the number of worlds were almost constant."
"much of the past research on model metadata has focused on component-based modeling systems. component-based modeling systems are a tool for integrated environmental modeling where model applications are constructed from a set of \"plug-and-play\" model components that can be interchanged for different applications [cit] . metadata frameworks have been proposed for model components generally [cit], the component interfaces [cit], and the variables passed between linked components [cit] designed an un-encoded metadata framework supporting the description of environmental numerical models giving more attention to the construction of model compositions by interfacing model components."
"in appendix c.3 we presented a grouping heuristic for the graph transformation of section c.2. we performed an experiment to gain intuition about the error introduced when we force edges to participate in groups of equal probability. we present our results for various numbers of groups in figure 7 (b). as expected dblp converges very fast (4 groups are enough). recall from table 3 that the maximum out-degree is just 238; on the other hand biomine and flickr which have nodes with out-degree in the thousands need more groups to converge. still, we get the surprising result that 20 groups are enough. thus, the offline computation of the transformation can be safely sped up for nodes with large outdegree, using the grouping technique. we note that for this experiment we used everywhere 20k mc samples."
"the paper is organized as follows. section 2 describes the background and related works. in section 3, the research methodology is described. in section 4, proposed network simulation and implementation in riverbed are illustrated. section 5 evaluates the performance to show that results meet up with the designed objectives. finally, section 6 is a conclusion that presents the outcome of this work and suggests future work."
"in this paper, we present a new clustering algorithm that combines the benefit of dbscan [cit] (a widelybased density-based clustering algorithm) and a gridbased approach to achieve scalability. dbscan is fast and can detect clusters with complex shapes in the presence of outliers [cit] . dbscan starts with identifying core points that have a large number of neighbours within a user-defined region. once the core points are found, nearby core points and closely located non-core points are grouped together to form clusters. this algorithm will identify clusters that are defined as high-density regions that are separated by the low-density regions. however, dbscan is memory inefficient if the data set is very large, or has large highly connected components."
"the experiment was designed as a 2x5 factor within-subject. the two factors were (1) inclusion of the visual feedback (with or without), and (2) intended locations of tactile illusion (five locations between the fingers labeled p1 ~ p5). four survey questions were answered in a 7 likert scale asking of the various aspects of the phantom tactile experience."
"in this section, we formally present the data model considered in this paper. we assume independence among edges noting that most of our results are applicable to graphs with edge correlations (see appendix b for details). similar to normal graphs, probabilistic graphs may be undirected or directed and carry additional labels on the edges (such as weights). for sake of generality, we focus on directed and weighted probabilistic graphs. we assume that the weights are discrete."
"we move on to study the convergence of the distance functions based on the number of worlds. in figure 5 we plot the mean squared error (mse) of the distance approximations (using the distances according to a sample of 500 worlds as the \"ground truth\"), for various numbers of worlds. observe that they all converge, as expected, to 0. we conclude that 200 worlds are enough to compute distances accurately since the mse drops below 0.2 for all datasets and all distances. even though we had already established in theory that a small number of samples is needed, it was surprising to find out that 200 worlds are enough, in datasets with tens of millions of edges."
"bayesflow uses a bayesian hierarchical model to identify different cell populations in one or many samples. the key benefit of this method is its ability to incorporate prior knowledge, and captures the variability in shapes and locations of populations between the samples [cit] . however, bayesflow tends to be computational expensive as markov chain monte carlo sampling requires a large number of iterations. therefore, bayesflow is often impractical for flow cytometry data sets of realistic size."
"in general, g r f (p, j) and g t f (p, j) are assumed to be consistent and invariant to time and locations. thus, we can omit"
"then, we explore the impact of noise to identification accuracy of aoq by adding awgn to the received timedomain ltss signals. after signal acquiring, additive noise was imposed on the i/q signal via matlab's awgn() function. fig. 14 shows the identification accuracy using aoq versus snr for nics with 5 types of models and all mixed nics. the snr is defined as the power of the received signal to that of the added artificial awgn. it is observed that all the identification accuracies are higher than 95% when snr is larger than 40 db, while the accuracy of the mixed nics declines to 90% when snr decreases to 30 db. however, the performance reduces seriously for snr is below 30 db which indicates that aoq is still heavily influenced by noise. it is caused by the division. although we attempt to alleviate this problem by selecting the optimal location where the aoq has the smallest variance, the performance of aoq in low snr regions is still far from satisfactory, which needs to be addressed further."
"in order to address this problem, we collect aoqs of devices from l locations. in each location, f frames of ieee 802.11n ofdm signals are measured. denote the collected aoqs at location l asỹ"
"the mac load in global network is roughly 9,000 bits/sec and 5,000 bits/sec for without-priority and priority based network. the without-priority networks carry a huge load for lack of node's request synchronization. but coordinator faces almost similar load in both networks. 0 24 48 72 96 120 144 168 192 216 240 264 288 312 336 360 384 408 432 456 480 504 528 552 576 600 throughput (bits/sec has high throughputs compared to without-priority based network. figure 5 represents throughput for both the global network and the zigbee coordinator (node 0) much higher for priority based network compared to without-priority based network."
"the second difference is in the termination condition; a node that enters the k-nn set, may not be in the final result: another node might enter at a later step of the algorithm with a smaller majority distance. candidate nodes can be discarded if their majority distance is guaranteed to be greater than the largest distance in the k-nn set."
"we propose a monte carlo algorithm for computing the weights w(u, v). we sample different outgoing edges, for each node u, and estimate equation (1) by taking the sum of probabilities over the sampled graphs only, instead of using all possible graphs. the chernoff bound can be applied again to show that a small number of samples per node u is sufficient to approximate the weights w(u, v)."
"finally, our results suggested that judges failed to spontaneously notice that the morph images were indeed morphs (49% acceptance; see table 2 ), although after receiving information/instruction regarding these types of images, detection was at levels above chance performance but was still relatively low. it is worth noting that d' in the current experiment (0.73) was somewhat higher than the sensitivities found in experiments 1 and 2 (approximately 0.1-0.4). this may be due to the length of time that judges spent studying the images or models' faces while interacting with the models (in comparison with those presented onscreen in the computer tasks), or that the morph fraud/detection tips sheet could be consulted while studying the images (versus the onscreen images being presented after the tips screen). the suggestion that detection may be higher in a live matching context is an interesting one and may have important implications for real-world procedures if supported by further research."
"possible applications of the \"out of the body\" phantom tactile sensation: two handed/fingered interaction and feeling tactile sensations as if coming from the middle of the (a) mobile device, (b) hovering holographic-virtual imagery, (c) indirectly from a virtual object in a monitor and (d) an augmented marker (e.g. seen through a head mounted display)."
"noisy measurements, inference models, and privacy preserving perturbation processes produce uncertain data. research in probabilistic relational databases has focused on sql query evaluation [cit], mining [cit], ranking and top-k queries [cit] . however, in many prevalent application domains, such as social, biological, and mobile networks, graphs serve as better models than relational tables. incorporating uncertainty to graphs leads to probabilistic graphs."
"the results of this experiment confirmed that morph detection was poor, using a task where chance levels were easily defined and the forced choice procedure better reflected real-world decision-making. that d' sensitivities were very similar to the levels found in experiment 1 provided additional evidence of the difficulties that participants had with detecting morphs across multiple paradigms."
"thirty-eight of our white models formed pairs in which both individuals were used to create the morph and both collected data using that same image. with these pairs, we were able to investigate whether (different) judges accepted the same morph image when presented by each of our paired models (i.e. the ab morph presented by both person a and person b). figure 3 illustrates these results, highlighting that for the majority of pairs, the morph was accepted as an id image of one model noticeably more than the other. this suggests that the morphs we used, although created by equally weighting both individual images, did not resemble each individual equally."
"however, even if morphs are incorrectly accepted by viewers (e.g. passport issuing officers) as unaltered photographs, this does not mean that these images sufficiently resemble one or both original identities (i.e. the two faces used to create the morph). in order for fraudsters to take advantage of this method of deception at the point of document use, observers (e.g. border control officers) must accept morphs as believable photographs of those people presenting them."
"in order to reduce the number of dimensions describing the resulting space without significant loss of variability, we retained the highest 67 components only (which explained approximately 95% of the variance in the image rgb information). the images' projections on these principal components were then entered into an lda, where the two classes represented individuals and morphs. the result is a reshaped space comprising one dimension (the number of classes minus 1)."
"it is, however, unclear as to what should be considered chance-level detection in the pre-and post-training morph detection tasks, where perfect performance would be selecting all five morphs from each ten-image array. therefore, we designed experiment 2 in order to establish detection levels for our morphs in a task where chance was 50%."
"in many situations, however, the decision to accept an id image is left to a human operator. indeed, even in face matching scenarios where algorithms are initially employed, human users are often presented with a \"candidate list\" and are required to make the final selection, potentially reducing the overall accuracy of the process [cit] . although important across a variety of contexts, the question of whether people are able to detect morphs and/or whether they accept such images as genuine id photographs has received little attention to date. [cit] provided evidence that several computer algorithms performed with high error rates when tasked with detecting morph images. in addition, they found that human performance on their task was also poor, with morphs going undetected in most cases [cit], for similar findings). in line with previous work on face matching with expert populations [cit], their results also showed that professionals working in the field (border guards) were no better than university students and employees in detecting morphs."
"to further improve the identification accuracy, we also use dnn for identification. the aoq feature is a 52-dimensional complex-valued vector. by connecting its in-phase channel and quadrature channel, we can get a 104-dimensional realvalued feature vector for identification. since the feature dimension is not very high, we select a two-layer dnn for device identification as shown in fig. 12 . the sizes of these two hidden layers are the same as the input layer, namely, 104 neurons in each layer. besides, rectified linear unit (relu) activation functions are used for hidden layers. then, the dnn outputs the predicted probability distribution of each possible label by using a softmax layer. at last, the index of the maximum value of the predicted probability distribution is just the predicted device label. to train the dnn, the categorical cross-entropy is used as the loss function, which is a measure of the difference between the predicted probability distribution and the real probability distribution. the loss is calculated in the forward pass and weights are updated using the chain rule, which is known as the backward propagation."
"these algorithms perform well on the flow-cap data sets, but they may not be scalable to larger data sets that we are dealing with nowadays -those with tens of millions of cells. aiming to quantify cell population heterogeneity in huge data sets, we have to develop an ultrafast and scalable clustering algorithm."
"the section provides an overview of the proposed zigbee network simulation configuration. furthermore, it illustrates the analysis of qos in this zigbee network for the reliable and robust network."
"on the whole, it is still missing how to extract channel robust rff features for 802.11n devices in practical scenarios. under the assumption that the wireless channel keeps constant during the coherence time, this paper introduces two location-invariant rff feature extraction methods using pilots and long training sequences (ltss) in the ieee 802.11n beacon frame preamble, respectively. the main contributions of this paper are listed as follows:"
"in the pre-training baseline morph detection task, participants were asked to identify morph images in tenimage arrays, providing an initial measure of ability to detect morphs before any training or guidance had been given. on each of the six trials, participants were shown ten faces (five morphs, five exemplars), along with the question, \"which of these images are face morphs (a blend of two faces)?\", 2 with the option of entering between zero and all ten faces."
"errors with unfamiliar faces become especially problematic when dealing with various types of fraudulent identification. for instance, researchers in recent years have begun to investigate the issue of \"face morphing attacks\" [cit] . this term refers to the following three-step process to obtain a passport fraudulently. person a (who has no criminal record) creates a morphed photo of himself and person b (whose prior record prevents him from international travel). first, person a submits this ab morph as his id photograph with his passport application. second, the morph is compared with previous images of person a that are kept on file and the application is subsequently approved by the passport issuing officer on the grounds that the image sufficiently resembles him. third, person a gives this fog (interpol, n.d.) passport to person b, who then proceeds to use it during travel as he also resembles the morph image sufficiently to pass through border control."
"next, we perform a cross-validation to illustrate the effectiveness of aoq. dnn is chosen as the identification method. our training is carried out through optimizing the crossentropy loss function using an adam solver with batch size setting to 256 on the collected dataset. all our network models were trained and tested running on keras 2.1.6 using tensorflow 1.12.0 as backend with an nvidia geforce gtx 1070ti gpu. moreover, the l2 regularization of on both hidden layers was used to 0.001 prevent overfitting. the initial learning rate was set to 0.001 and the xavier initialization was used to initialize the hidden layer weights."
we next report empirical assessment of the efficiency of the methods presented in this paper. we implemented all our methods in c++. all the experiments were run on a linux server with 8 2.8ghz ghz amd opteron processors and 64gb of memory.
"to reduce the noise-induced variations of ω due to noise, we compute the average frequency offset over sub-carriers, multiple ofdm symbols, and multiple frames by"
"taken together, the results of the two experiments might suggest that the tips screen has the potential to increase morph detection levels. however, this benefit may be counteracted by: (1) carrying out another task (feedback training or an irrelevant letters task) before morph detection is measured for the second time; and/ or (2) the use of a ten-image array paradigm, incorporating additional noise in the data due to the uncertainty as to how many images should be selected on each trial."
"we also present wall-clock speedups in table 2 . as expected, efficiency increases as the number of groups decrease. the gains, however, are overall moderate due to the power law out-degree property of our datasets. in particular, only the nodes that have larger out-degree than the number of groups are affected from grouping. for instance, less than 5% of the nodes in biomine have degree more than 16. consequently on 95% of the nodes the grouping with 16 groups has no effect. at the same time, more than 50% of the savings come from nodes of out-degree greater than 100, which comprise less than 1% of the total number of nodes. in absolute numbers, a complete transformation of flickr with 16 groups took approximately 1000 seconds, instead of 1300."
"in table 1, zone 6 has the highest priority value 15 and lowest priority value 10 for zone 1. the devices in the highest priority zones always get preference at the coordinator for processing. the corresponding nodes in each zone have set priority value (1, 2, 3, 4, and 5) according to their appearance on the table. this priority value is true for every zone and all participating nodes. furthermore, a priority resolution technique is described in the following."
"for both datasets we used 8000 random triplets. we used 50 random worlds for median, majority, expectedrel, and reliability. we used a reliability threshold equal to 0 for expectedrel, without optimization. next, we present the specific experimental setups and the results."
"where wu,i denotes the weights on all outgoing edges to nodes of the group i (note that because of symmetry they have all the same weight). the function c(i, m1, .., mr) gives the number of possible ways in which we can choose mj nodes from group j, given that we have at least one node from group i. the formula is:"
"for additional intuition on the probabilistic-random-walk, consider this example: assume that a drifter is in boston and that there are three roads that she can take, one to new york, another one to toronto, and one to montreal. each road has a proximity value indicating the inverse of the time it takes to cross it. also, each road is labeled with a probability of being open or closed, since snowfalls are not rare in the area. now, the universe tosses coins to decide if the roads are open or closed. the roads to toronto and montreal are open, while the road to new york is closed. the drifter favors short roads so she chooses between the two roads, with probability relative to their proximity to boston. if all roads were closed she would stay another night and wait for better weather the next day."
"where ω r (k) and ω t (k) denote the carrier frequencies of a pair of receiver and transmitter, respectively. ω (k) denotes the frequency offset of sub-carrier k and t s denotes the time interval between two adjacent pilots. there are 80 sampling points during time t s . since the frequency offset is caused by the typical slight frequency difference between the transmitter and receiver crystal oscillators, it is robust to locations. thus, we use it for fingerprinting purpose. we compute the phase difference between two adjacent received pilots by"
"the objective of this prior modeling study was to better understand the potential of rain gardens as distributed stormwater controls for flood mitigation within an urbanized watershed [cit] . the specific study area of the research was the rocky branch watershed, which is located in downtown columbia, south carolina, usa. because a significant portion of the watershed is developed, high intensity storms that typically occur during the summertime result in flooding at different locations within the watershed. for this study, two different model instances were created (fig. 6 ). the first model instance is a well-calibrated and evaluated model although a swmm-specific model instance resource type could have been designed and implemented within hydroshare, we used the generic model instance resource type when implementing the use case to provide an example applicable to any environmental model. a swmm-specific model instance would have allowed for the capture of additional metadata relevant only to swmm models. software extensions to hydroshare could then provide custom functionality and applications able to operate specifically on swmm-model instances. using the generic model instance offers broad use across environmental models, but it lacks the potential for customization that becomes possible when targeting a specific model instance resource type. fig. 8 fig. 9 illustrates the metadata that can be captured for the example use case using the generic model instance and model program resources. each resource has a title, creator, and other metadata that follow the dublin core metadata standard. in addition, extended metadata elements for each resource (with names shown using the \"hsterms\" prefix) help to more fully describe the model instance and corresponding model program used for executing the model instance. fig. 9 also shows how the model program resource type, in this case the swmm model [cit], and the model instance resource type, in this case a rocky branch watershed simulation, are connected using the executedby relationship. [cit] and two model instance resources for the rocky branch watershed simulations (e.g., [cit] ) . fig. 11 shows the graphical user"
"given g ⊑ g, let the shortest-path distance between s and t be dg(s, t). we define the distribution ps,t of shortest-path distance between s and t as:"
"are specifically designed for pilot transmission. the pilots are pseudo-random binary sequences modulated by binary phase shift keying (bpsk). fig. 2 shows the frame structure of the ieee 802.11n ofdm standard. each 802.11 rf frame contains three parts, i.e., preamble, signal and data. the preamble consists of two training sequences, i.e., 10 short training sequences (stss) and 2 ltss, each with a duration of 8 µs."
"the problems of computing distance functions and processing k-nn queries are fundamental for probabilistic graphs, just as they are for standard graphs. they serve as primitive operators for tasks such as link prediction, clustering, classification, and graph mining. in this paper, we present a principled extension of these problems in the presence of uncertainty and we assess the quality of the proposed distance functions using a real probabilistic protein-protein interaction network."
"in this subsection, formulation of the zone synchronization problem is conducted for competing for end devices. at the beginning, the zigbee network has been marked in different zones. these zones have the same number of devices. in the zigbee network, various types of end devices are set up in different ranges. all nodes are connected to several routers having a common central processor named zigbee coordinator. these nodes try to communicate with other nodes via coordinator or routers. zigbee end devices send a large number of requests to the routers or coordinator and vice versa."
"figure 29: t-rome meta model for node j attempting to wake-up node j + 1. the message is at node i. in case of data transmission, there exist following two possibilities:"
"in order to verify the assumptions on current consumption and timing intervals (as discussed in section 4), we analyzed a wake-up message, communication messages and the protocol on the whole, experimentally. the results are presented in following sections 5.1 and 5.2. furthermore, we experimentally investigated the occurrence of false positive and false negative wake-ups as introduced in section 3.1. figure 23 shows the wake-up message used in this work captured at the output of the envelope detector. the wake-up message was manchester encoded to improve stability and to reduce the false wake-up rate as introduced in section 3.1. manchester encoding, in this case, means that a binary one results from a transition from high to low, and a binary zero results from the transition from low to high. so one bit manchester encoded requires two bit sent. figure 23 clearly shows that the length of the real wake-up message corresponds very well to the theoretical length of the wake-up message calculated by using the numbers provided by the datasheets as given in section 4.6. figure 24 shows exemplary the current consumption of a sensor node in the different states of the proposed protocol measured via a shunt resistor in the power line. in this example, the node sent 4 data packets consisting of 100 byte each to the next neighbor node. it can be seen that the currents provided in sections 3 and 4.7 for microcontroller and radio fit very well to the measurement results for radio calibration, sending and receiving of communication packets, low-power listening, and microcontroller run mode current. it can be further seen, that sending of wake-up packets require less current than expected from the datasheet numbers, only. this is due to the fact that the manchester encoded wake-up packets consist of an equal amount of zeros and ones and the radio power is reduced during sending of zeros. furthermore, we can see that the timing fits very well to the suggested timing calculated in sections 4.6 and 4.7. we used a logic analyzer to visualize all sending and receiving states. as laboratory test setup we used the same configuration as introduced in figure 7 with four participating nodes: node 13 as source, node 12 and node 11 as relay nodes and node 10 as sink. node 13 sent 5 data packets of 100 bytes payload each. according to the protocol nodes 12 and 11 forwarded the request to node 10 that finally received all data packets after around 90 ms."
"here, t w1 to t w3 are the times required to send the required communication packets in case of success (t w1 ), or the delay times required in case of failure (t w2 and t w3 ). as we are looking on the general case of node j attempting to wake-up node j + 1 and the message is still at node i, the message will be send to node j in case of failure ( figure 29 ). looking at the case where node i has the message and attempts to wake-up node i + 1, we figure 35 : markov chain for node i (that also has the message to be delivered) attempting to wake-up node i + 1 find equation (3):"
"next steps will include the realization of a dynamic routing protocol based on the proposed protocol scheme as presented here. in addition, we will introduce further parameters that support the decision finding at the sender node, like link quality, receiver signal strength or remaining energy level at the receiver side to increase network stability and to enhance its lifetime."
"t-rome, wake-up messages can be forwarded or send directly. in summary, there exist following four possible meta states for a t-rome branch consisting of m nodes:"
"here, t tx1 to t tx3 are the times required to send the required communication packets in case of success (t tx1 ), or the delay times required in case of failure (t tx2 and t tx3 ). e[w j, j, j+1 ] and e[w i,i,i+1 ] depict the expected times required in the corresponding t-rome meta states which are given in equations (2) to (3), below and can be extracted from figure 35 :"
"of course, the quality of wireless links can change quickly due to changes in the environment [cit] . to achieve a robust, reliable and efficient routing, stateof-the-art wireless network protocols like ctp [cit] estimate the current link quality between nodes and adjust their routing paths accordingly. the link quality estimation can either be achieved by incorporating information from different network layers like the number of received acknowledgments and the link quality indicator provided by the radio or it can be based on the β -factor [cit] that measures the burstiness of a wireless link. while link estimation is a common technique in traditional wireless network protocols, it is not standard in all used wireless routing protocols that are based on wake-up receivers since an accurate and timely link quality estimation requires a certain amount of control messages (beacons) to be sent. this is energy-wise expensive due to the high costs of wake-up messages."
"in the individual layer, each cav consists of a physical part and a cyber part. in the physical part, the on-board radar will collect velocity and position information from the nearest neighbors and the on-board sensors will collect sensor information such as the surrounding information, the lane information, and the driving information from various systems of the vehicle. in the meantime, the on-board computer will calculate the relative velocity information and the relative position information according to its own velocity and position information. the processed information will be passed to the engine management system, the energy management system, the gear management system, and the brake management system to control the speed for maintaining the desired gap. the information flow of the dcps is shown in fig. 2 ."
"definition 1: the convergence rate of the network is defined as the absolute value of the real part of the least stable eigenvalue of the state matrix a, i.e.,"
"beyond the primary goal of distinguishing different ncrnas, it is of particular interest to identify common patterns on different transcripts. establishing methods for pairwise comparison and subsequent clustering is an important step toward this goal. this allows us to find common patterns for the same class of rnas, to the detection of putative novel classes of rnas, and to commonalities among different ncrnas that share (parts of) processing pathways. the ability to compare read patterns, both at the level of individual read blocks and at the level of block groups independent of sequence and secondary structure data is a necessary prerequisite to disentangle the different influences. here, we develop the necessary algorithms and provide the deepblockalign software package that implements these tools for practical use."
"the meta-models shown in figures 28 to 33, are composed of several markov states as depicted in figures 34 and 35 . it can be seen in both figures, that there exists a certain probability of success, but the attempts can also fail. in that case, a node enters a fail state that is exited with probability 1 but has a certain delay connected to it. the delay just equals the timeout of the radio which is little more than the time required for the success case."
"to analyze the occurrence of false positive wakeups we conducted an experiment similar to the one described above. we used the same test setup and configured the sender to send every address from 0x00 to 0xff 10 times, while the receiver kept its address. only each time the sender sent 0xff the receiver incremented its address by one. since the receiver's initial address was 0x00, each address could be cross-checked with all other possible addresses during this test. the receiver just woke up 10 times, exactly what would be expected if no false positive wakeups occur. throughout the test, the received signal strength was set to -25 dbm."
"in this section, we present the three-layer architecture of the dcps, and we also elaborate the composition and related functions of each layer. to guarantee the normal operation of the dcps, the realization of the functions will be discussed in section iii."
when the number of vehicles in the platoon is large. this shows that the convergence rate of the vehicular platoon decays to 0 with a scaling law of o(1/n 2 ) as n tends to ∞. the next corollary whose proof is given in the appendix gives an explicit formula for the scaling laws of the sensitivity for disturbance for large-scale networks with lattice information graphs.
"in the cc-based distributed formulation of cavs, every vehicle needs to send the information to a cloud center, and then the cloud center sends a decision back after analyzing and processing the related data. however, the data generated by cavs can be one gigabyte every second [cit], and the speed of data transmission is becoming the bottleneck such that the real-time decision making becomes very difficult or costs too much time to respond. with the development of manufacturing and the computer technologies, the on-board micro data center loaded in cavs is already capable of processing the information collected by the cav itself, based on which ec has emerged as a typical technology. ec refers to the enabling technologies that happen at the proximity of data sources and allow computation to be performed at the edge of the network [cit] ."
"to study the performance of the networked system, we use information graph to describe the interaction topology between cavs, and two performance matrices, the convergence rate and sensitivity to disturbance, were introduced to study the performance scaling. based on our previous work, several analytic expressions were derived with respect to the smallest eigenvalue of the grounded graph laplacian. in particular, for the most common n-vehicle platoon whose information graph is a 1-d lattice, its convergence rate decayed to 0 as o(1/n 2 ), and its sensitivity to disturbance grew to +∞ as o(n 3 ). the numerical experiments show that the performance of the cavs matches very well with our analytic predictions."
"in order to analyze the performance of our proposed protocol, and to compare it to other protocols, we introduce a markov chain based model and meta-models of the routing algorithms t-rome, ctp-wur and of an algorithm that we here call the naive algorithm. we decided to compare our protocol especially to these two protocols, as most implemented networks use some derivate of the naive algorithm or a relaying mechanism similar to ctp-wur."
"as already introduced in tables 1 and 2, the sensitivity of wake-up receivers is lower than that of communication radios. this means that data can be sent over longer distances than wake-up messages as shown in section 3."
"due to this, t-rome is a cross-layer protocol, as visualized in figure 5 . above the physical layer is the link layer that supports single-hop transmissions and waking up of neighboring nodes. this is realized basically by using an rts/cts message exchange to reduce packet collisions as introduced in maca [cit] . in this context, a wake-up message works also as an rts and the wakeup acknowledgment as the cts command. the routing layer routes messages along multiple hops according to a static routing table implemented on each node. following sections introduce the cross-layer protocol and corresponding data packets in more details. the application runs above the communication layers."
"here, we only show the markov chains for the cases t i, j and w i, j, j+1, as the chains for the cases t i,m and w i,i,i+1, w m−1,m−1,m and w i,m−1,m are similar and can be achieved by plugging the markov model into the corresponding meta model shown above. now, we can analyze the markov models with respect to the expected required time to send a messages via m nodes. from figures 34 and 35 we can extract the expected times for all t i, j and w i, j, j+1 states. e[t i, j ] can be expressed by equation (1):"
"looking at the results for sending one data packet with t-rome, we find that it requires more time than the naive algorithm (factor of around 1.4) for two participating nodes due to the additional messages required in the protocol. for four participating nodes, t-rome performs equally good as the naive algorithm and for more than four nodes, it outperforms it but it does not reach the performance of ctp-wur. however, t-rome outperforms ctp-wur and the naive algorithm, when delivering two or more data packets with two or more participating nodes due to the savings of relaying."
"another advantage of wireless sensor nodes with wake-up receivers is their enhanced robustness. clock synchronization is obsolete and nodes may be reset at any time, for example, if a fatal software error occurred. existing networks can be easily enhanced by new nodes, even if the network is running on low duty cycle periods [cit] . furthermore, extracting data from the network can be done without much delay, as messages are transmitted almost instantly."
"t-rome makes use of the different communication ranges of communication and wake-up radio. the protocol saves energy by skipping nodes during data communication. furthermore, t-rome introduces a set of parameters to optimize the relaying process by dynamically choosing the most appropriate stopover nodes in case the sink is not reachable within one communication hop. the total number of wake-up packets can be reduced with t-rome by accumulating sensor data and sending up to 64 data packets (16 kbyte) in a row once a communication link is established."
"although the work in this paper can improve the performance of control and driving for cavs, we agree to the point that there are still some deep gaps between design and application. for example, our road test experiments were conducted in a relatively simple environment, and the emergencies in driving were not considered. we assumed that the dynamics of all the cavs is homogeneous, but the problem will be more complicated for real world and some more realistic models have been proposed [cit] . we will model the cavs more realistically and conduct the experiments under more complex road conditions for future research."
"the routing layer takes care of sending, receiving and forwarding packets from source to destination. figures 14, 15 and 16 show the three available routing packets, namely routing request (r_req), data (data) and acknowledge (r_req_ack). each packet consists of four bytes. all data to be sent is managed in data slots that form the message queue. the first six bits of a request type packet are reserved for the number of slots to be sent in the currently ongoing communication. r_src id and r_dest id are the routing source and destination ids of the communicating nodes which could be equal to the mac ids but can also be different. ttl (time to live) indicates how many hops a request can be forwarded. upon reception of a routing request, the receiving node decreases ttl by one, before forwarding the request to the next node. in case ttl is zero the request will not be further forwarded. forwarding of routing requests is realized with route request packet type packets keeping source and destination id untouched. figures 17, 18 and 19 show the sequence diagrams of the routing protocol in case of four participating nodes. node a is the source node, nodes b and c are possible relay nodes and node d is the sink. node a starts by sending a routing request (r_req) to node b. node b forwards the request (fwd_req) to its next neighbor node c who will again forward the request to node d. each node (b, c, and d) answers the request by sending of a request acknowledge (r_req_ack) to node a. node a collects all request acknowledgments and decides based on the information included in the acknowledges to which node the data will be sent. currently implemented parameters that support the decision, to which node data is sent to, are: available data slots at the receiving node and hop distance from starting node. further parameters like the available energy at receiver node or various status data like link quality or number of successful wake-ups can be easily used to increase the network stability."
"the naive algorithm wakes up and transmits data from node to node. here, we assume following communication scheme: node i sends a wake-up call to node i + 1 directly followed by the data packet. node i + 1 acknowledges the data packet if it was received successfully. figures 36 to 38 show the corresponding markov models."
"theorem 1: given a network of double-integrator agents, whose state matrix a is given in (10), if its information graph is connected, then the convergence rate of the system is given by"
"we presented an approach, deepblockalign, and showed that it can be used for a meaningful clustering of ncrnas based solely on read processing patterns. in particular, we find that the mapping profiles are well conserved between human and macaque. most micrornas as well as the majority of the trnas fall into well-separated clusters (fig. 4) . within the microrna cluster, a subcluster contains the majority of microrna-offset rnas, indicating that deepblockalign is able to precisely distinguish between block groups that share a common core pattern. consistent with observation that some snornas are processed by dicer, we find the examples clustered together with micrornas. several previously unannotated clusters were identified as potential antisense micrornas and as trna-derived pseudogenes, respectively, showing that deepblockalign can be used for annotating unknown read mapping patterns through unsupervised clustering. the application of deepblockalign for annotation of unknown processing patterns on a routine basis, however, will require the development of appropriate measures of statistical significance, such as p-or e-values. this will require further research as it remains unclear at this point how appropriate background distributions could be constructed. future updates of the algorithm also includes a more detailed tuning with respect to match versus mismatch scores. we found that this approach is fairly robust against parameter variation. for instance, we tested the robustness of the deepblockalign algorithm by analyzing the benchmark dataset using various values of the distance weight parameter υ dist observing consistent results (supplementary table s2 ). the clustering approach can in principle be used for constructing multiple alignments. this could in turn be useful in identifying subtle differences in processing patterns and assist the investigation of evolution of processing patterns."
"in particular, for the 1-d vehicular platoon depicted in fig. 4, when the number of vehicles is large, i.e., n 1, we obtain from corollary 2 that its sensitivity to disturbance and peak frequency are given by"
"flood-wup realizes opportunistic routing according to the acknowledgment based approach but forwarding is done after a random period of time has passed. to avoid multiple transmissions of the same data packet, each node changes its wake-up address upon reception of a data packet. although changing of the wake-up address follows a certain sequence, it can happen that a node loses the proper sequence and additional control packets are required [cit] . the opportunistic routing in green-wup is similar to that of flood-wup but wake-up addresses are additionally based on the current energy level of a sensor node and the source node goes to sleep after it sent the initial wake-up sequence. a possible relay node has to wake up the source by using a unicast wake-up packet that was initially provided by the source. due to this, green-wup requires additional wake-up packets that are usually expensive with respect to energy."
"with the aim to reduce the number of transmissions from the source to sink, and as such to increase network performance, opportunistic routing protocols rely on broadcasting data packets to several nodes (the set of candidates) to forward a message from a source to sink [cit] . usually, the most appropriate forwarder is chosen out of the set of candidates based on local and endto-end metrics. local metrics are based on link conditions and geographic positions of the sensor nodes, while end-to-end metrics are usually based on link properties between source and destination [cit] ."
"the wake-up layer is responsible for waking up of neighboring nodes. each wake-up packet consists of carrier burst, preamble and receiver id as depicted in figure 8 . the carrier burst tunes the detector to the incoming frequency, the preamble is used by the detector to estimate bit length and possible offset. the receiver id is an up to 16 bit long address to identify the receiver. when sent at a data rate of 8192 bps the wake-up message can be between 148 and 216 bytes long depending on the length of carrier burst and preamble. in a noisy environment, it is recommended to use longer carrier burst and preamble. before an attempt is started to wake up a neighboring node each node probes the wireless channel (lbt). if a communication is currently going on, the nodes back off and restart the attempt later. after the wireless channel is found to be free each communication is initiated by sending a wakeup message. the receiver acknowledges this wake-up packet (wuc) with an acknowledge message (wuc ack) that includes the address information of receiver and a protocol id as can be seen in figure 9 . if the address does not match the receiver id or if the acknowledge message was not received before a certain timeout is reached, waking up is assumed to be unsuccessful and has to be restarted. the packet flow is schematically sketched in figure 10 . to reduce collisions, the wakeup layer protocol realizes an rts/cts mechanism as depicted in figure 5 . the protocol id is transmitted at an early stage to be able to include newer protocol versions that could react differently upon reception of certain communication packets."
"remarkable advances in communication, sensor, and computing technologies have promoted the development of its. its can collect information from multiple sources such as gps, pavement management system (pms), closed-circuit television (cctv) of smart city, and so on, and help enhance the driving comfort and security. although cc technology has been used to deal with the huge amount of data, the increasing volume of data generated by edge devices can still cause communication delay. to solve this problem, we proposed a three-layer dcps. the dcps empowers the cavs in its to make their own decisions under certain rules by means of ec technology. specifically, the cavs discussed in this paper are supposed to keep a constant space with their neighbors while traveling along a desired trajectory. the layered formulation of the dcps is presented in fig. 1 ."
"in this paper, we assume that there is at least one boundary (an outermost edge) on which every node is a reference node. reference nodes are only placed on the boundaries because leading vehicles typically are the outermost vehicles in a formation. we call such a boundary a dirichlet boundary. without loss of generality, we assume the first d 0 axes in the information graph have dirichlet boundaries. fig. 4 shows the desired formation and its information graph of a vehicular platoon. the leading vehicle is in the front of the platoon and indexed by 0, and the following vehicles are indexed from 1 to n . fig. 6 depicts several 2-d lattice graphs with different boundary conditions."
"where u i is the control input, ω i is the external disturbance, and n is the number of agents (excluding the reference agents) in the network. this is a commonly used model for vehicle dynamics in studying vehicular formation, which stems from feedback linearization [cit] . for the network of cavs, the control objective is to make the cavs maintain a rigid formation geometry by following a desired trajectory. the desired geometry of the formation is specified by the desired gaps"
"sion range of wake-up receivers compared to that of the main radio, data and wake-up transmissions are realized by a multi-hop routing protocol that supports sending wake-up messages and data. the protocol stack consists of several layers. the lowest layer is responsible for the waking up of neighboring nodes. the second layer handles single-hop message transmissions and the top layer routes messages and forwards wake-up signals along multiple hops. the presented work in this paper is organized as follows. in section 2 we review existing network protocols that support the use of wake-up receivers. in section 3 we take a look at current wake-up receiver designs and present the wireless sensor node that is used in this research. in section 4 we introduce the proposed multi-hop wake-up routing protocol in detail and analyze its current consumption as well as the occurrence of false wake-ups in section 5. in section 6, we introduce markov models of the proposed algorithm as well as for ctp-wur and a naive communication algorithm. the models are verified and performance and energy requirements of the aforementioned protocols are compared and analyzed in section 7. finally, outlook and conclusions can be found in section 8."
"1) the linear growth of centralized cloud computing capacity cannot match the explosive growth of massive edge data; 2) the transmission of data from edge devices to cloud centers causes a dramatic increase in the bandwidth of the network, which will result in longer communication delay; 3) the privacy data transmitted to cloud centers from edge devices may cause security issues; and 4) the data transmission consumes too much energy for an edge device whose power is limited. therefore, investigations about datacenter [cit], fog computing [cit], and cloudlet [cit] have been introduced to the community because cc is not always an efficient way to process data, especially when the data is produced at the edge of the network."
"in order to characterize the read distribution within a block group, we measured the entropy of the start positions. let q i denote the fraction of reads in a given block group that starts at position i. we consider the entropy"
"in the information layer, the infrastructures will collect information from multiple sources. the collected information will be passed only to leading vehicles through v2i communication to help plan the trajectory. when the leading vehicles start to move, the following vehicles are expected to follow the desired trajectory. the following vehicles do not need to receive real-time information feedback from infrastructures, and thus the communication delay caused by exchanging and processing information will not happen. in this way, the interaction of information between the information layer and the cooperation layer will be compressed to a lower level to protect the privacy of cavs compared with the centralized control mode, in which every cav needs to communicate to the infrastructures. all the physical parts of the dcps are in the upper portion of each layer and the cyber parts are in the lower portion in fig. 1 ."
"1) this work proposes a layered formulation of dcps to describe and analyze the performance of cavs. the related implementing methodologies are also illustrated. 2) we generalize previous work on the performance scaling laws for the network with a one-dimensional (1-d) information graph to networks with d-dimensional information graphs; 3) extensive numerical experiments are conducted to validate the theoretical derivation and the feasibility of the proposed formulation. the rest of the paper is organized as follows. section ii presents the overall formulation of the dcps for cavs. the components and related functions of each layer of the dcsp are described. the implementing methodologies of the above functions for dcps are illustrated in section iii. in section iv, several practical and numerical experiments are conducted and the results are compared with theoretical analysis. the paper ends with conclusions and discussions in section v."
"be the best score of a block alignment subject to the constraint that c i,c j and k k,k l are two pairs of blocks that are included as a paired match into the alignment. the optimal scores then satisfy the recursions"
"our markov chain based model reflects errors on the medium access level and does not describe the dynamic routing behavior originating from changes in link quality estimations or due to changes in the energy level of certain nodes that could lead to different routes. this could potentially lead to different behaviors of the routing algorithms, and the comparisons presented in section 7 might be influenced by this. an extended markov chain based model that also reflects the dynamic behavior is clearly more complex and may be part of our future research."
"finally, we need to consider the cases where node m − 1 attempts to wake-up node m, and the case where node i attempts to send data to node m. these two cases are given by equations (4) and (5) for the wake-up and communication cases, respectively:"
"for an individual cav in the dcps, ec indicates the whole process from receiving information to making decisions. for a leading vehicle, it will receive the information from the information layer and the information from its on-board sensors. the on-board central information processor will analyze all the information to generate a reasonable trajectory. for a following vehicle, it will receive the velocity and position information, and calculate the relative velocity and relative position accordingly. together with the data collected by on-board sensors, the onboard central information processor will decide how to perform to travel along the desired trajectory. the data are heterogeneous from different sources (for example, the speed information is a numerical value, while the camera information is an image) and contains redundant information (for example, the leaves on the road generally do not affect the normal driving of the vehicle). ec is expected to first decouple heterogeneous data and get rid of invalid information, and then analyze and make a decision. compared with cc, ec do not need to upload and download data so that it will cost less time and less energy to respond, which is important for a real-time process. moreover, the omission the data transmission can also reduce the disclosure of privacy data."
"recently, wireless sensor networks [cit] have been upgraded with low-power wake-up receivers. these wake-up receivers have marginal power consumption and wake up the sensor node if a dedicated signal has been received. so, low-power wake-up receivers can greatly reduce the power consumption of wireless sensor nodes, by eliminating the idle listening time and at the same time reduce communication delays to achieve an almost latency free communication [cit] ."
"this implies that the sensitivity to disturbance of the vehicular platoon grows to +∞ with a scaling law of o(n 3 ), and its peak frequency decays to 0 on the order of o( 1 n )."
"the purpose of deepblockalign is the comparison of the read mapping patterns of two block groups obtained from short rna-seq experiments. to this end, it employs a two-tiered alignment strategy. in the first step, individual blocks of reads are compared with each other. this is motivated by the observation that start and end patterns, and hence also entropies, may differ substantially between individual blocks of reads. a pairwise alignment algorithm similar to the needleman-wunsch algorithm for sequence data [cit] ) is used to compute an optimal alignment and a similarity score from the normalized frequency of reads covering each position of the two input blocks."
"while the mac layer is responsible for the communication between neighboring nodes, the routing layer handles communications between nodes that are possibly further apart than only one hop. routing packets are embedded into mac layer data packets as depicted in figure 13 ."
"to reduce the privacy leak and communication delay of the cavs in its, this paper proposed a dcps formulation and elaborated its implementing methodologies. in the dcps, leading vehicles will communicate with infrastructures and plan a trajectory accordingly, while following vehicles, modeled as double-integrator agents, only need to interact with their nearest neighbors. a can-based vanet is used for intra-and intervehicle communication to collect necessary information. we recommend that there should be no data center that collects all user-related information in its because the fewer infrastructures have access to user information, the better the privacy can be protected. the application of vanet and ec enables an individual cav to process information and make its own decisions so that the risk of exposing privacy data is greatly reduced."
"the starting point for deepblockalign is a collection of reads mapped to a (reference) genome. clusters of overlapping reads are decomposed into blocks of reads with similar start and stop positions using blockbuster [cit] . both the length and the coverage profile can vary substantially between blocks. in the following, we introduce an entropy-like measure for the coherence of read blocks. overlapping and closely spaced blocks of reads form a block group or locus. our aim is to compare these block groups based on the relative expression of blocks, the distance between blocks and the shapes of the blocks themselves."
"after successful filling the message queue, sending of data is initiated with a wake-up signal (state send wake-up). successfully waking of the neighbor node, is indicated by a wake-up acknowledge and a routing request is sent (state send r_req) containing destination id, number of data packets and max number of wake-up hops. then, the node listens for route request acknowledgments sent by the woken nodes (state wait r_ack). if at least one node that answers has a free slot available, the node starts to send all possible data packets (state send data). after successful sending, or if any error occurs, the node exits its current state and goes back to sleep. the state machine of the receiver is similar to that of the transmitter. looking at the state machine, it becomes clear that in case two sensor nodes try to send data at the same time, the data packets would collide and packet transmission would fail. additionally, t-rome can encounter selfinterference due to the forwarding mechanism of packets that are sent at the same time. to avoid collisions, each source node (but not the relay nodes, as the channel is assumed to be busy during the complete period of data transmission) probes the wireless channel before transmission and if it finds the channel busy it backs off for a certain time period before testing the channel again. to calculate the back off period a simple algorithm is used that calculates the back-off time based on the unique node ids. this means that nodes further away from the sink node have longer back off periods than nodes nearer to the sink. this avoids self-interference and reduces congestions near the sink during periods of high data traffic."
"once a communication link to a node is established, up to 64 data packets consisting of up to 246 bytes each can be transmitted in a row. after transmission, the receiver id low-byte link gets closed and the participating nodes fall back to sleep, again. the same routing scheme is repeated until all data has reached their destination."
"penalizes the match score, as the expression difference between two blocks increases. the second term, η ±, measures the relative difference of normalized read count difference at consecutive positions. provided the previous positions, i−1 and j −1 have the same read count difference as the present positions, i and j, we set"
"the communication mac layer consists of two types of packets, a data packet, and an acknowledge packet. each data packet is answered by an acknowledge packet. if the acknowledge packet is not received during a certain time frame, it is assumed that sending of data has failed. failed data packets are reinserted into the send queue to be resent later. figures 11 and 12 show the data and the acknowledge packet. packet type is used to separate the packets. ids of the source (src id) and destination (dest id) are used to verify sender and receiver. the length byte is required internally for packet handling."
"theorem 2: given a network of double-integrator agents whose closed-loop dynamics is described by (2), its sensitivity s to disturbance and peak frequency ω p are, respectively,"
"the comparison of block groups is based both on the similarities of individual blocks and on the similarities of distances between pairs of blocks. as for other problems e.g. the maximum contact map overlap problem [cit], this is in general a hard problem, which could be solved by an ilp approach or using stochastic heuristics. we notice, however, that the emphasis on pairs is reminiscent of the problems of simultaneous computation of an alignment and a secondary structure, which is solvable in polynomial time by the sankoff algorithm [cit] . the basic idea is that the distances between a collection of blocks on a genome are already determined by a small subset of all distances, so that a collection of nested pairs of blocks already can be expected to contain most of the distance constraints."
"the wake-up signal was received at a data rate of 8192 kbit per second (bit length: 122 µs), which means a 125 khz period requires 4 byte ones and 4 byte zeros sent in a row at 250 kbit per second, resulting in a bit length of 128 µs. from sender (receiver) side, the wakeup message consisted of 42 byte (10 bit) carrier burst which is required at the receiver to detect the presence of a signal and to fine-tune its internal frequency to the incoming signal frequency. the preamble consisted of 48 byte (12 bit). its purpose is to adjust the receiver offset to be approximately at the level of the averaged input signal and to verify the bit length. the pattern depicts the 16 bit address of the wake-up receiver. it requires sending of 64 byte (16 bit). in the case of manchester coding, this results in an 8 bit address that can be used to address up to 256 independent devices. for example, the node id sent in figure 23 is decimal 85. figure 21 shows the message schematically. before sending, the radio requires a calibration cycle. preamble, a sync word, and length field are mandatory bytes which make a wake-up message 6143 µs long. out of that, the radio is for 5344 µs in sending state and 799 µs in calibration state. figure 22 shows schematically the buildup of a complete radio packet including calibration of the radio, sending of the preamble, sync word, length, mac, status and crc bytes. sending of payload and routing bytes is optional. all times (including calibration) in figure 22 are calculated for a baud rate of 250 kbit per second and gfsk (gaussian frequency shift keying) modulation. generally, sending of data is separated into hardware specific and protocol layer specific parts. in sum, each packet requires the hardware specific calibration, preamble, sync word and crc which add up to around 991 µs. the rest of the time is required to send protocol messages, either wake-up, mac or routing. a mac packet requires 1247 µs and a routing packet without payload requires 1375 µs. the payload is sent in additionally 32 -7872 µs, depending on payload size. according to the datasheet, the radio draws around 8.4 ma during calibration and when sending at 868 mhz, 0 dbm gain around 16.4 ma. in receive state, the radio requires around 16.9 ma and for sending a wake-up call at +12 dbm gain the cc1101 draws 34.2 ma. figure 22: radio packet including calibration."
"y. feng, b. hu, y. gao, and j. tan are with the state key laboratory of fluid power and mechatronic systems, zhejiang university, hangzhou, zhejiang 310027, china (e-mail: fyxtv@zju.edu.cn; 11425062@zju.edu.cn; gaoyicong@zju.edu.cn; egi@zju.edu.cn)."
"the sum run over all possible positions of read starts within the block group. small values of i indicate well-defined block patterns, and hence are indicative of specific processing, while large values arise from blurred patterns and suggest random degradation. all the ncrna classes, e.g. micrornas, trnas and snornas show varying degrees of diversity (distribution of start positions in the block group), which is reflected in varying entropy distributions as shown in figure 1 . this suggests that the entropy is a characteristic measure for each ncrna type and indicates to which degree the different families can be separated. it also indicates that this to some extent can be used in the effort to separate different ncrna classes."
"wireless sensor networks are used in many applications like environmental monitoring, home automation, smart manufacturing, infrastructure monitoring and many others. in this context, a wireless sensor network usually consists of many small self-powered sensor nodes that measure their environment, process data and communicate it to other nodes or to a base station [cit] . message transmission can be done via single-hop transmissions or via multi-hop communication resulting in complex network topologies."
"although wake-up receivers have many advantages and writer frequently reported devices in wireless sensor networks [cit], there do not exist many mac or routing protocols that support their use and the majority of existing protocols are only limited to simulations. some existing protocols for wake-up receivers support single-hop communication only, like e2rmac [cit], wur-mac [cit], rtwac [cit] and gwr-mac [cit] . these protocols show superior energy requirements compared to synchronous or asynchronous mac protocols but their performance is only based on simulation results. the main feature of e2rmac and wur-mac protocols is to use the wake-up signal as an rts/cts mechanism to avoid the hidden terminal problem. in rtwac all nodes have a unique and a common wake-up address to support broadcasting and dedicated messages. but the purpose of wake-up messages is only to trigger an event, for example, a sensor reading, at the receiver node. data communication is realized by a more common csma/ca mac protocol that is not further specified, using the main radio."
"generally, a low-power wake-up receiver consists of an envelope detector and a correlator as sketched in figure 1 that shows schematically a wireless sensor node including a wake-up receiver. the envelope detector demodulates the high-frequency (hf) carrier signal to achieve a low-frequency (lf) wake-up signal as sketched in figure 2 [cit] that depicts an on-off-keying modulated wake-up signal. the correlator analyzes the lf signal, to verify the validity of a wake-up message. in that case, the main microcontroller of the sensor node is woken up by an interrupt and, depending on the embedded software, a sensor reading might be initiated or the antenna is connected to the main radio to establish further communications. a matching network might be necessary to match the impedances of antenna and wake-up receiver."
"other factors to be taken into account are false positive and false negative wake-ups, as shown in section 3.1. to evaluate the occurrence of false negative wakeups we conducted two laboratory experiments consisting of a sender and a receiver, first connected by cables and second by antennas. the first setup was chosen to easily place an active attenuator in line to be able to reduce the incoming signal from 0 dbm to -60 dbm. the second test was chosen to verify that external interferences have no influences on the wake-up rate."
"block groups are then compared using an alignment approach. here, a similarity measure is used that combines the similarity scores of the individual blocks and differences in the distances between aligned blocks. [cit] algorithm is used."
"let us consider a network of vehicles moving in euclidean space. for ease of exposition, we only consider one dimension of the translation motion. the analysis is also applicable to all three dimensions, as long as the dynamics of an agent in each coordinate of the euclidean space is decoupled [cit] . the position of the ith vehicle is denoted by p i and it is modeled by a double-integrator agent:"
"although the single model and the interaction topology have been described in detail, the v2x communication technologies (including v2i and v2v communication) to guarantee the normal operation of the dcps is still lacking. in this paper, cavs use controller area network (can)-based vehicle ad hoc networks (vanets) to communicate with each other. the existing works on vanet mainly focus on three aspects. some studies [cit] are concerned with the spatial and transmission capacity, some involve the design of protocols and approaches [cit], and others deal with the privacy-preserving [cit] ."
"furthermore, there are 18 block groups without annotation aligning well with known classes, as exemplified in figure 4c . six of these fall into the microrna cluster, while 12 cluster with the trnas. analyzing the candidates on the microrna side, we observed that two lie in an antisense direction to already annotated micrornas (hsa-mir-486 and hsa-mir-625) . this kind of antisense microrna reads have been reported before [cit] and can frequently be observed when analyzing short rna-seq data. the antisense reads, however, do not necessarily imply the actual transcription of such an rna, since the complementary stem regions in some cases cannot be distinguished. upon a detailed inspection, we observed some strand-specific tags for both hsa-mir-486 and hsamir-625 (supplementary figs s6 and s7) . however, considering the perfect complementarity of hairpins in the two mirnas and low frequency of strand-specific tags especially for hsa-mir-625, it is difficult to assume these two mirnas as an ideal case of anti-sense mirna. two additional block groups significantly align with micrornas and show a typical microrna processing pattern. however, when analyzing the secondary structure of these candidates using rnafold [cit], no hairpin-like structure was observed. however, based on the expression patterns, these examples are clustered correctly. since deepblockalign does not take any secondary structure into account, it cannot be expected that all the results will overlap with ncrna prediction programs. these results thus require further validation. two candidates clustered together with an snrna and snorna, respectively. upon a detailed inspection of the respective block groups, none of the two candidates were observed to be having microrna-like processing pattern."
"based on above analysis, a three-layer distributed cyberphysical system (dcps) is proposed in this paper to describe and analyze the performance of cavs in its. every vehicle is modeled as a double-integrator agent and is supposed to maintain a constant space between its nearest neighbors. cavs are labeled into two different types: reference leading vehicles and ordinary following vehicles. the leading vehicles will receive information from its infrastructures and generate desired trajectory, while following vehicles only need to receive information from its neighbors and on-board sensors. the collected information of the following vehicles is processed by edge computing (ec) technology such that the following vehicles do not need to receive real-time information feedback from infrastructures. in this way, following vehicles will spend less time making decisions and take less risk of privacy exposure. in addition, the modes of vehicle-to-vehicle (v2v) and vehicle-to-infrastructure (v2i) communication are also considered. the main contributions of this research are summarized as follows:"
figure 32: t-rome meta model for node i attempting to transmit data to node j. figure 33 : t-rome meta model for node i attempting to transmit data to node m.
the next result whose proof is given in the appendix gives an explicit formula for the convergence rate of a large-scale network with lattice information graphs.
"recently [cit] presented a novel wake-up receiver design together with two flooding protocols flood-wup and green-wup. flood-wup uses different broadcast addresses to forward messages to receivers that are not in range of the first transmitter and to avoid the reception of multiple messages. green-wup includes additional information about harvested energy at a node coded in its address and nodes with higher energy levels are preferred relay nodes. evaluation of both protocols is only performed on the basis of simulation and the authors do not evaluate the power requirements of the proposed protocols. [cit] presented zippy, an ondemand multi-hop flooding technique based on wake-up receivers. zippy is extensively tested in a laboratory testbed and shows latencies in the range of tens of milliseconds to broadcast multi-hop messages."
"based on newly developed markov chain models we analyzed t-rome and other state-of-the-art communication protocols for wake-up receivers regarding energy consumption, communication duration and overhead. especially, we investigated, modeled and analyzed ctp-wur and a naive communication algorithm and compared their performance to that of t-rome. we demonstrated that our proposed protocol outperforms existing protocols in many cases particularly by sending several data packets at once and by skipping nodes during communication."
"computing a similarity score between the blocks. in the second stage, we compare the arrangements of blocks within block groups with each other. using this procedure, we conduct a clustering to group similar rnas and to identify if different rnas share common patterns. this also open up the possibility of discovering entirely new processing patterns. the output will point to cases which need further manual inspection."
"in the dcps, cavs are supposed to maintain a rigid formation while following a desired trajectory. the trajectory is planned by the leading vehicles and is specified by a constant space, i.e., the desired gaps. in the cooperation layer, leading vehicles are assumed to travel in the boundary of the cav team and perfectly track the desired trajectory. since the following vehicles will adjust their speed according to the velocity and position information of their nearest neighbors, the real-time information exchange, processing and feedback are not necessary. the velocity and position information can be obtained through an on-board radar. that is, the trajectory information planned by leading vehicles is not passed through direct instructions, but through velocity and position that can be detected."
figure 37: markov chain for node i (that also has the message to be delivered) attempting to wake-up node i + 1 using the naive algorithm.
"although many different its formulations and applications have been developed, most studies only focus on one special aspect of its, such as architecture design, control algorithm, communication technology, or application in a certain scenario. as a comprehensive system, an its consists of a cyber part (such as information collection, communication, control mode, collaborative algorithm, and so on) and a physical part (such as connected and automated vehicles (cavs), basic infrastructures, different kinds of sensors, on-board computers and controllers, and so on). hence, cps are very suitable for its to improve safety and mobility. through cps, people can directly pass instructions to physical objects in its to control them or to collect information while making decisions."
"o ver the past decades, remarkable advances in the field of communication and control technology have revolutionized our way of life. in the upcoming era of internet of things (iot), any object will have a unique way of identification and can be connected [cit] . in other words, the barriers between the virtual world and the real world will be eliminated and people can give instructions to physical objects directly. the cyberphysical system (cps), in which the cyber components (for example, on-board computers and controllers) and the physical components (for example, mechanical equipment and actuators) are highly integrated, is one of the cornerstones of iot. in the field of intelligent transportation system (its), many different formulations based on iot and cps are proposed."
"first, it appears natural to work with normalized read counts to capture similar shapes at different expression levels. furthermore, we found it useful to focus on the normalized difference"
"due to their low power consumption and the fact that they listen always on incoming signals, wake-up receivers are prone to false positive and false negative wake-ups. in the case of false positive wake-ups, a receiver detects a valid signal although the wake-up message was not dedicated to it. false negative wakeups occur when a wake-up receiver stays asleep although a wake-up message was sent to it. both kinds of false wake-ups can result from interferences on the wireless experimentally, the occurrence of false negative wake-ups can be measured for example by counting how many valid wake-ups a receiver detected out of the number of sent valid wake-up messages. the false positive wake-up rate can be experimentally measured by counting how often a wake-up receiver detects a valid wake-up message although the message does not contain a valid address."
these times intervals can be seen in figure 25 which shows the sending of 500 bytes payload in 5 packets of 100 bytes each over a row of four nodes as sketched in section 4 figure 7.
"of read coverage and start reads across the block x, where n x is the total number of reads in the block group having block x. we have normalized in order to make a meaningful comparison regardless of the absolute expression level (number of reads). a version of the algorithm could be made without normalization. finally, we disregard differences in similarity whenever two blocks are so dissimilar that they appear entirely unrelated. this leads us to a similarity measure of the form"
"after mapping small rnas to a reference genome, stacks of reads mapping to similar positions are merged to read blocks simplifying the visualization. closely positioned blocks are joined in block groups. previous reports on the degradation of structured rnas have suggested that, e.g. trna processing is largely a random process [cit] . in order to assess whether a comparison of block patterns is meaningful at all, we first tested whether block patterns of specific loci are conserved across different experiments sampled from different developmental stages, tissues and species. to this end, we extracted from the datasets in table 1 all those loci that are expressed in multiple experiments. we then aligned each block group with all block groups from another dataset and ranked the block groups by their deepblockalign scores. figure 3 shows the distribution of the ranks of the query locus (or its rhesus ortholog) among all alignments. we find that deepblockalign ranks corresponding block groups close to the top for nearly half of the queries. many block patterns are therefore highly non-random and conserved across different tissues, developmental stages and species."
"in this paper, we consider the following distributed linear control law, where the control action u i of an agent only depends on the relative position and relative velocity information from its neighbors, i.e.,"
the grounded graph laplacian l g of an information graph is obtained by removing from l the rows and columns corresponding to the grounded (reference) nodes.
"in this article, we developed for the first time an energy efficient and simple cross-layer network protocol for wireless wake-up sensor networks (t-rome) followed by its comprehensive modeling and analysis. the protocol combines the advantages of wake-up receivers such as low-power consumption and on-demand communication together with the advantage of long-range communication radios, that is their superior sensitivity."
"to reduce the occurrence of false positive and false negative wake-ups some wake-up receivers use active or passive input filter [cit], which includes a correlator unit that analyses the received wake-up messages and only creates a wake-up signal in case the addresses match [cit], or make use of manchester or similarly encoded wake-up signals [13, 15, 19 ]."
"alba-wur, for example, calculates the link quality by taking into account how many packets have been lost on a specific link in the past. this achieves a good average link quality information but cannot resemble fast or short link quality changes. to avoid collisions and to improve the reliability wur-mac chooses dynamically one out of several available channels of the 2.4 ghz ism band for wake-up transmissions. to choose a channel, the protocol keeps track of all channels used in neighboring nodes for communication and then takes randomly one of the remaining channels for its own communication. this approach does not avoid collisions and like alba-wur only calculates an average channel usage without the possibility to react on rapid channel fluctuations. in t-rome we introduce a parameter to assist the sender in order to dynamically choose the best next hop node based on multiple values like distance to the source and link quality estimation. this also enables route adjusting on rapidly changing link conditions."
"cavs are expected to carry more on-board sensing and computing facilities to support and promote the its. hence, vehicular networking is starting to emerge as an effective way to help manage vehicular platoons. vehicular cloud computing (vcc) is one of the most popular solutions that were proposed to enable vehicular networking technologies. vcc can provide different types of services such as network as a service (naas), storage as a service (staas), cooperation as service (caas), and so on [cit], but there are still several drawbacks:"
a linear discriminant (ld) classifier was simply adopted in this work since it has been previously shown to be an appropriate method in sleep staging or sleep stage detection using hrv data [cit] .
"a general weakness of this type of \"population smoothing\" is that it pulls all estimates in the same direction, i.e., towards the global θ p in our example above. an alternative approach is to smooth towards data more relevant to individual i. if we have some knowledge about the similarities of our rows (individuals) and our columns (locations) then two additional smoothing strategies immediately suggest themselves, namely smoothing using data from individuals that are similar to individual i, and smoothing using data from locations (columns) that are similar to the columns that are present in individual i's data. in particular, in this paper we use social network friendships to measure similarities of individuals and spatial distance for similarity of locations."
"after using cfs during the cross-validation procedure, three hrv features were selected for deep and non-deep sleep classification. they are: 1) sdnn, the standard deviation of inter-beat intervals, 2) mrf, the mean respiratory frequency estimated from hrv which corresponds to the frequency of spectral peak in the high-frequency band between 0.15 hz and 0.4 hz, and 3) pmrf, the power of mrf. the separabilities of these three features without and with applying subject-specific z-score normalization are compared in table ii . it can be seen that normalizing the features per subject clearly increases their separability (as measured by the mahalanobis distance md). table iii presents the overnight deep and non-deep classification results (obtained through the losocv) using the selected hrv features. in the table, the precision, sensitivity, specificity, accuracy, kappa, and auroc are shown and compared with and without using subject-specific (z-score) normalization. after applying the normalization, an average kappa of 0.42 ± 0.16 (versus 0.35 ± 0.22) and an average accuracy of 81.3 ± 3.5% (versus 79.2 ± 7.6%) were achieved. to examine the significance of their differences, a paired wilcoxon signed-rank test (one-sided) was used accordingly. we notice that the normalization can significantly improve the performance of deep sleep detection. moreover, the variances of the results decrease after using the proposed z-score feature normalization method for each subject, which indicates that this method can help reducing the betweensubject variations to some extent. as shown in figure 1, the roc curves of deep sleep detection obtained with and without the normalization are compared in a two-dimensional solution space. in the figure, we also observe a performance enhancement after normalizing the features for each subject. the confusion matrix obtained with the z-score normalization is shown in table iv, where the misclassifications of different sleep stages and wakefulness are indicated. it can be clearly observed that around 17.5% of n2 epochs were misclassified as ds, which implies a presence of difficulty in discriminating between n2 and deep sleep based on hrv data. as a matter of fact, it has been shown that n2 and n3 sleep are very similar in regard to autonomic nervous activity [cit] . this would result in performance limitation in classifying deep and non-deep sleep. nevertheless, further explorations are encouraged in better separating these two sleep stages through the use of cardiac activity."
"our focus in this paper is on building models from location data in the form of event tuples i, x, y, τ where i is the individual who generated the event, (x, y) is the location (e.g. longitude and latitude from gps) and τ is a timestamp. figure 1 shows an example of this type of data for san fransisco. if time is collapsed, this type of data can be represented in the form of a sparse matrix of n individuals by m locations where cell i, j contains a count of the number of times individual i visited location j. from this data we wish to infer predictive distributions θi over the m locations for each individual."
"in our learning setup we have three disjoint sets of data: training data dtr for estimating the components in the mixture model (as described above), validation data dv for estimating the mixing weights π (described below) and test data dte."
"sleep stages were manually scored as wake, rem sleep, and n1-n3 of nrem sleep on each 30-s epoch by sleep experts based on the multi-channel bio-signals of psg according to the aasm guidelines [cit] . to perform deep and non-deep sleep classification, n3 was considered the deep sleep class (ds); and wake, rem, n1, and n2 sleep were merged into a single non-deep sleep class (nds). table i summarizes the subject demographics and sleep statistics."
we applied a correlation-based feature selection (cfs) algorithm [cit] to reduce feature dimensionality and meanwhile remove correlated features. cfs is a filter-based algorithm taking the correlation between features and between features and classes into account. it towards finding an 'optimal' feature subset containing features that are as much as possible uncorrelated with each other and highly correlated with class. the heuristic evaluation criterion of a feature subset f containing k features based on cfs is given by
"we computed the apr test metric using all of the models described above and the average log-probability test metric for models that produce probability distributions as predictions. tables 2 and 3 shows the results for the different models (rows) for the out-of-sample evaluation. the column shows the results across the 4 different data sets as well as a 5th column that shows the average for each model across the data sets. table 3 : average log-probability across individuals, for each data set and averaged across data sets. lower score is better."
"in order not to bias the classification results, experiments were conducted using a leave-one-subject-out crossvalidation (losocv) to evaluate the classifier. during each iteration of the cross-validation, data from 14 subjects were used to train the classifier and the data from the remaining subject were used for testing. afterwards, results of all testing sets were then averaged, yielding the overall classification performance. note that feature selection was performed on each training set of the cross-validation, resulting in 15 feature subsets. to assemble a single feature set for evaluation and avoid biasing the results, only the features included in all those feature subsets were eventually selected and then used to test the classifier using losocv."
"all the features were normalized via a z-score method for each subject. let us consider a feature x s from subject s containing feature values of n epochs throughout the night, the normalized feature values can be computed such that"
"symmetry is ubiquitously observed phenomena in both natural [cit] and human-made objects [cit] . many living organisms such as birds [cit], animals [cit] and insects [cit] perceive symmetry patterns of the natural environment. it has been studied that there exists a significant correlation between symmetry and aesthetics, excellence in manufacturing and health [cit] . symmetry perception is also the matter of survival for jungle animals helping them recognize the natural enemy. because symmetry is fundamentally inherent to objects of this world, symmetry perception is evolved as a crucial necessity for visual object recognition for humans as well [cit] . it is determined that human can perform core object recognition task in a fraction of seconds [cit] . the human object recognition process, which is believed to be performed in the ventral visual stream of the brain cortex, consists of multiple stages: line and edge detection [cit], shape representation (e.g. grouping the stimuli coming from retina's visual sensors) [cit] and symmetry perception [cit] . being in complex interaction with object recognition in the human brain, symmetry perception is a preattentive process."
"in an additional series of experiments we also evaluated all of the models in terms of their predictions only on test data locations that were new (unseen in their historical data), e.g., for point-of-interest recommendation systems. when compared to the other approaches we found that the am4 method was again systematically the most accurate method overall in terms of apr metric and was among the most accurate in terms of the log-probability metric. the detailed results are omitted because of space constrains."
"the primary contributions of this paper are (1) an adaptive mixture model framework that infers appropriate levels of smoothing, and (2) experimental results demonstrating systematic improvements in prediction accuracy (compared to traditional global methods) on a range of large-scale location data sets."
"is a normalization constant and the r l ij values can be viewed as smoothed pseudocounts for a particular location (column) j, obtained from a sum of weighted counts from similar locations (similar columns), with the weights being a function of location-to-location similarity. from these pseudocounts we can estimate a predictive distribution for each individual i and location j as θ"
"with polysomnography (psg), the \"gold standard\" for objective sleep assessment, overnight sleep can be classified as wake, rapid-eye-movement (rem) sleep, and one of non-rem (nrem) sleep stages n1, n2 and n3 according to the guidelines of american academy of sleep medicine (aasm) [cit] . n3 usually corresponds to slow wave sleep (sws) or \"deep sleep\". deep sleep is the most restorative period of sleep for metabolic function, where body energy can be efficiently conserved and recovered. it is therefore important to detect deep sleep throughout the night from a healthcare point of view."
"cardiac information or more specifically, heart rate variability (hrv), has been proved to correlate with autonomic nervous system where autonomic activity differs across sleep stages [cit] . for example, deep sleep is in association with decreased sympathetic activity which is reflected by the hrv low-frequency power [cit] . in this matter, hrv information can thus be in turn used to detect deep sleep."
"conventional evaluation of the reflection symmetry detection methods does not provide necessary insight into their behaviors on various visual stresses. however, the evaluation technique, which can demonstrate the limitations and advantageous aspects of detection methods, is beneficial and helpful in their further improvement. this kind of evaluation can also help to point out the applications where they can play best. in this section, we propose the evaluation technique that contains the properties mentioned above. the first proposing evaluation technique evaluates the performance of symmetry detection methods over increasing visual stress intensity. this technique shows how the evaluating method reacts to the visual stress types. the second evaluation technique that we propose unleashes the detection limits (the best possible performance) of the evaluating methods on particular visual stress type. before moving to the detailed descriptions of these techniques, let's introduce necessary notations and performance measures. denote the symmetry decision rule sdr as a function of τ, and call τ a symmetry decision threshold. the symmetry decision threshold τ indicates the decision boundary between symmetry and non-symmetry ones of stressed symmetry images. in other words, given a reflection symmetry image, τ indicates the maximum intensity of visual stress in the image at which the stressed image still keeps its symmetry property. an image with stress intensity above τ has no symmetric pattern and is considered as non-symmetric. we can write the formulation of sdr function as follows:"
"in this work, we define five primitive types of visual stresses: (1) blurring, (2) brightness, (3) additive white noise, (4) size/resolution, and (5) affine skewness. for utilizing the psychophysical method, the relationship between visual stress type and human symmetry perception (sensation) strength should be monotonic; the more significant the amount of visual stress in an image, the less the human symmetry perception strength. this requirement applies to all types of visual stresses except brightness, because, when an image is at perceptually optimal brightness, both the increase and the decrease of brightness cause visual stress. therefore we divide brightness stress into two types: positive brightness change (brightening) and negative brightness change (darkening). we apply blurring, brightening, darkening and additive white noise in two ways: stress in whole image and stress in one half of reflection symmetry pattern. stress in one half simulates reflection symmetry pattern on a mirror-like surface such as water surface. followings are eleven visual stress types evaluated in this work:"
"a key challenge in personalization is being able to generalize about an individual's behavior and preferences beyond their historical data. for example, with location data it is likely that individuals will visit both old and new locations. the challenge then becomes how to model their propensity to visit new locations while respecting their tendency to revisit locations from their past. this problem is particularly permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. challenging in the presence of sparse data."
"we built a web-based system (tool) in order to conduct the psychophysical experiment for multiple participants simultaneously. participants are requested to register and attend the experiment. before the experiment starts, the system provides the participants with the definition of the reflection symmetry with multiple example pictures. participants are also provided the instruction about the experiment and the user interface. next, the web-based system provides multiple samples of stressed images for each visual stress type. it is done to help the human brain to develop a perceptual scale and measure for each visual stress. fig. 4 illustrates a screenshot from the web-based system for sample images with lh stress type. after exploring all samples with various stress level from all visual stress types, the actual experiment starts. on the experiment page, a participant is requested to select one of two choices: ''symmetry'' or ''not symmetry'' for each presenting stressed image. the user interface also provides the tentative experiment progress. fig. 5 shows the screenshot of the experiment page."
"it is known that ld classifier is sensitive to prior probability of each class. a time-varying prior probability (tvpp) has been successfully used for classifying wake, rem sleep and nrem sleep [cit] . this was based on the observation that the probabilities of different classes change over time throughout the night. similarly, the tvpp of ds and nds for each epoch was obtained by computing the percentage that specific epoch was labeled as each class with respect to time (or epoch index) based on training set [cit] ."
"we extracted a total of 42 features from the hrv data. these features were computed on each 30-s interval (or epoch) based on the aasm guidelines [cit] . for each subject, the values of each feature were normalized to have a zero mean and unit variance (i.e., z-score normalization), aiming at reducing the between-subject variation reflected by the features (caused by the difference in cardiac physiology). this was expected to help improve the deep and non-deep sleep classification."
"before moving to the analysis, let's define some terms (nouns and adjectives) that we use to describe the behavior of the methods in the context of proposed evaluation framework. following are the necessary behavior properties which are vital to describe the performance and its behaviors."
"we consider different data sets from twitter and gowalla in our experiments. the twitter data set contains tweets from two different regions: orange county (california), and new york referred to as twoc, twny. the tweets were gathered using the twitter api 2 [cit], selecting tweets that have geolocation (gps) coordinates for each tweet. the gowalla data set 3 covers san francisco, and new york, referred as gosf, gony. for the experimental results reported later in the paper we retained individuals with information from at least 5 unique days and locations with 3 events or more."
"in this work, we propose a novel evaluation framework for computational reflection symmetry detection methods in human perception perspective. unlike traditional evaluation techniques, the proposed framework demonstrates the performance behavior of reflection symmetry detection methods on the visual stress dataset that is carefully organized based on the investigation of human perception. stressed image samples are categorized into symmetry or non-symmetry groups based on human evaluator study. for accurate categorization, we psychophysically determine the absolute threshold of human symmetry perception on each visual stress by conducting an experiment with 25 human evaluators. the threshold values are determined for each of the 11 visual stress types which are under consideration in this work. in order to comparatively verify the validness of the thresholds for each stress type, we conduct an additional psychophysical experiment to discover the thresholds for each image of those degradation types. the workflow of the proposed work is illustrated in fig. 1 . followings summarize contributions of the proposed work:"
"where m f,k represents the \"merit\" of the feature subset f, ρ cf is the mean feature-to-class correlation, and ρ f f the mean correlation among features. starting with no features in the subset, a forward search can be used to combine additional features one-by-one until no increase of merit was observed when in combination with them. more details of the cfs algorithm can be found elsewhere [cit] ."
"we compared the performance of our adaptive mixtures with well-known general-purpose baselines as well as with several state-of-the-art recently-published models for location recommendation. we use mpe and mle variants of the simple multinomial model (as defined in section 3.1), as well as non-negative matrix factorization (nmf) and hierarchical bayesian probabilistic matrix factorization [cit] (bpf) as our simple baselines. we also compared to well-known methods that incorporate the geographical preferences of an individual in order to increase the level of individual-level personalization: fused [cit], igslr [cit] and rgfm [cit] . we compared all of these methods to our proposed 4-component adaptive mixture model (which we will refer to as am4), with individual, population, location, and social components."
"in this paper we described a general framework for smoothing of individual-level categorical distributions using mixture models and we applied this framework to the problem of learning location models from historical data of individuallocation counts. experimental results on two data sets from twitter and gowalla indicate that the proposed mixture approach can lead to significantly improved predictive performance compared to non-mixture approaches. a key aspect of the method is its ability to weight different sources of information and to learn how to generalize beyond observations in the training data. the framework is relatively simple to implement and can be implemented in a computationally efficient manner. various extensions and generalizations are possible, e.g., conditioning the prediction model on time of day and day of week and extending to continuousspace models such as kernel density representations."
"in this work, we proposed a novel evaluation framework for computational symmetry detection methods based on human symmetry perception. the proposed framework evaluates the robustness and behavior of computational reflection symmetry detection methods on various visual stresses. initially, we determined human symmetry perception limits on 11 visual stress types. for that, we conducted a psychophysical experiment. we, psychophysically, showed that the thresholds for each visual stress types are consistent with individual thresholds of images for those visual stress types. we introduced modifications to the up-down staircase method and developed a web-based system to conduct the psychophysical experiment. based on human perception limits, we built a human annotated dataset for all 11 stress types with various stress intensities and introduced necessary performance measures for the proposed evaluation framework. we evaluated three state-of-the-art computational reflection symmetry detection methods using the proposed framework. the proposed evaluation framework showed how the evaluating methods are robust to various visual stress and behavior of evaluating methods as a function of stress intensity. in our view, the proposed evaluation framework provides more increased insight into the weak and strong aspects of the evaluating methods than the traditional evaluations could do."
"the two key ideas behind our approach are to (1) smooth individual-level information towards population patterns, geographic constraints, and social contexts, and (2) learn to combine these sources in a manner that optimizes predictive performance."
"an overnight deep sleep detector based on cardiac activity was developed. a total of 42 features were extracted from the hrv series for each 30-s epoch and three features were selected using the cfs feature selection method. by normalizing (z-score) the feature values over the entire night for each subject, the difference between subjects in physiology manifested by the features can be reduced to some extent. this can yield deep sleep detection results that are superior to those obtained without performing the subject-specific normalization on the features. with the normalization, we achieved a cohen's kappa coefficient of 0.42 and an overall accuracy of 81.3% in classifying deep and non-deep sleep, tested with an losocv on an ld classifier. in addition, we found that most of the misclassified deep sleep epochs are in n2 sleep."
"the primary goal of this experiment is to detect the human perception threshold for each visual stress type. unlike experiment 1, which runs separate staircases for each image of visual stress type, in experiment 2, we create two interleaved staircases for each visual stress type. the maximum number of trials (presentations) for the progress is 50. for each visual stress type, at each presentation, stressed image is randomly selected and presented with stress intensity that is calculated based on the participant's response to the previously presented stressed image. in other words, all images of a particular visual stress type share the same staircase process. over 40 participants attended this experiment in total, and for each visual stress type, thresholds of over 25 participants are collected. fig. 7 illustrates a box plot that describes the distribution of threshold values for each visual stress type. the absolute thresholds for bh, bw, and nh have broader distributions (high variance) than others. the average of the median thresholds in experiment 1 is consistent with those in experiment 2. we use the threshold values of experiment 2 for building our evaluation threshold. see figure 8 to observe sample stressed images that can be perceived as symmetry by humans."
"human symmetry perception is robust to various visual stresses. we define 11 visual stress types on reflection symmetry patterns. we psychophysically find the absolute threshold for each visual stress type. the absolute threshold is the biggest intensity level of visual stress (the smallest intensity level of stimulus) at which a human still can perceive the symmetry. next, we conduct two psychophysical experiments. the first experiment determines the absolute threshold for each image of all visual stress types. the second experiment determines the absolute threshold for each visual stress type."
"in this experiment, for all 55 images (5 images for each visual stress type), two interleaved processes are created. a process describes one up-down staircase with at least 20 trials for each. at each presentation, one visual stress type is selected randomly among eleven. afterward, one image from the dataset of that stress type is randomly selected. then, one staircase process is randomly selected out of two staircases. based on the previous response of a participant on that process, the next image is generated by applying stress of newly updated intensity and presented to the participant. then the participant gives his feedback to that presented stressed image and requests the next one to present. this procedure lasts until all processes terminate. 10 participants attended in this experiment. the absolute threshold is selected at stress intensity getting 50% population vote on the psychometric function of each image. fig. 6 provides the results of the psychophysical experiment by describing the absolute threshold distributions as a box plot for each image of visual stress types. for images of r, sa, and sc stress types, the threshold values have narrow distributions, and threshold medians are also close to each other. for the other stress types, the threshold distributions are various, because the nature of how the applied stress changes symmetry pattern depends on the content of the image as well. however, for the majority of images, the perception thresholds are still close: for each of bh, bw, dw, dh, lh, nh stress types, the medians of thresholds of 4 images out of all five images have close values. for each of nw and lw, the thresholds of 3 images out of all five images are close to each other."
"a number of trends emerge from the data in tables 2 and 3 . the simple mpe predictor outperforms all of the baselines (including nmf, bpf, igslr, rgfm, and fused) on all 4 data sets and for both metrics, confirming that including individual-level detail is important for these types of applications. the proposed am4 method is systematically better than mpe and better than all of the other baselines, across all 4 data sets and under both metrics. this clearly illustrates the superiority of the proposed mixturebased smoothing approach relative to smoothing towards the population (mpe) or using matrix factorization."
"a well-known ld classifier was adopted to classify deep and non-deep sleep on an epoch-by-epoch basis. conventional metrics sensitivity, specificity, precision, and overall accuracy were first used in a binary classification to assess the classification performance. in addition to these, we also utilized the cohen's kappa coefficient of agreement. it is considered a better metric when class distribution is imbalanced (here ds epochs account for an average of approximately 20% of the night which is much less than nds epochs). note that in this study ds and nds were considered the positive and the negative class, respectively. to compare the classifiers across the entire solution, we used the receiver operating characteristic (roc) curve which plots sensitivity (true positive rate) versus 1-specificity (false positive rate) on a graph. the 'area under the roc curve' (auroc) was then computed as a single metric that quantifies the classification performance in the solution space. a better classification performance corresponds to a larger auroc value."
"the second evaluation technique focuses on analyzing the performance of the computational symmetry detection method under various symmetry decision thresholds (sdr) of a specific visual stress type. it also determines the best possible performance and its corresponding decision threshold. in order to measure the performance of symmetry detection method on a specific stress type, f1-score is used:"
"to evaluate the models we use 60% of the data used for training, 20% for validation and the other 20% for test. we then perform evaluation using the test data using average percentile rank (apr) and log-probability scores. to compute percentile rank, we sort the locations for each individual using the model's probability for each location. we then find the position (or rank) r for the actual locations that the user visited in the test data. results below are reported on a scale from 0 to 100 with 0 being best (ranked at the top of the list). the second metric we use is the log-probability (for each individual) for location events in test set for that individual. both metrics are averaged over all the test location events for an individual and then averaged over all individuals to produce a single score."
"single-night psg data of 15 healthy subjects were included in our data set. they had a pittsburgh sleep quality index (psqi) of less than 6 [cit] . nine subjects were monitored (alice 5 psg, philips respironics) [cit] at the sleep health center and six were measured (vitaport 3 psg, temec) [cit] at the high tech campus. each subject provided an informed consent and the study protocol was approved by the ethics committee of the two sleep laboratories. the psg recordings are comprised of multi-channel signal modalities such as electroencephalogram (eeg), electromyogram (emg), electroocculogram (eog), electrocardiogram (ecg), oxygen saturation, and respiratory effort. from the psg recordings, only the ecg data (modified lead ii, sampled at 500 hz) were used for deep sleep detection. we clipped each psg recordings to the time interval from the moment when the subject turned off the lights with the intention of sleep until the moment 978-1-4244-7929-0/14/$26.00 ©2014 ieeethe lights were turned on before this subject got out of bed in the morning."
"a visually stressed symmetry image is presented to a participant with the symmetry axis drawn on it. a participant is asked if the image contains a reflection symmetry pattern by provided symmetry axis. the symmetry axis drawn on the image gets the participant's eye fixated on the reflection symmetry pattern of interest. in this work, we are not interested in reaction time or detection time to the symmetry pattern. therefore, there is no limitation for both stimulus presentation and the participant's response time."
"the origin of human symmetry perception studies dates far back to xix century [cit] . since then, the numerous concepts of modeling human symmetry perception process are proposed and empirically validated by conducting various psychophysical experiments [cit] . in order to understand internal brain processes responsible for symmetry perception, researchers of neuroscience [cit] and psychophysics [cit] fields investigated human brain activities using fmri (functional magnetic resonance imaging). they observed a correlation between symmetry perception and activations in v3a, v4, v7 and lo, dlo regions of brain cortex [cit] . regarding work of tyler [cit], the symmetry detection is three stages of processes: (1) elaboration of dimensionality of stimuli properties and passing the information to neural analyzers that impose varieties of symmetries; (2) the selfmatching feed-forward process that is performed in parallel on each feature across all possible symmetries; (3) the active and manipulative recognition process, which identifies object properties that are too complex to perform at previous stages."
"we used a mahalanobis distance metric md to assess the separability of each feature between classes. for a single feature x, its separability is given by"
"an alternative approach is to use the geographical preferences for each individual during the learning process [cit] . these methods project the data into a lower dimensional space while taking into account geographical preference. while this improves location prediction over direct matrix factorization, it does not take advantage of other available sources of information such as social ties."
"the heart beats of an ecg signal (high-pass filtered with a cut-off frequency of 0.8 hz and normalized in regard to mean and amplitude) were identified with an r-peak detector based on the algorithm proposed by hamilton and tompkins [cit], resulting in inter-beat intervals or an hrv series. it was then re-sampled at a sampling rate of 4 hz using linear interpolation. the ectopic rr intervals that are longer than 2 s or shorter than 0.3 s (possibly caused by, e.g., motion artifacts) were excluded. here 42 epoch-based hrv features (known from literature) were extracted. they are time-domain and frequency-domain features [cit] and nonlinear features measured by multi-scale sample entropy [cit] and detrended fluctuation analysis [cit] ."
"first, we evaluate the behavior of the computational symmetry detection methods by using the evaluation technique 1. second, we determine the performance trends and the best possible performance values by using the evaluation technique 2."
our contribution in this context is to develop alternatives that are better at retaining the details of each individual's behavior. in particular our model blends an individual's historical data with information about geographic and social similarity using a mixture model approach.
"is the maximum likelihood (frequency) estimate given individual i's data, θ p is the population distribution based on pooling the location counts across all individuals, i.e., θ"
"the proposed evaluation framework for computational reflection symmetry detection methods is built upon the knowledge in two emerging directions of symmetry study: (1) human symmetry perception, and (2) computational symmetry detection methods."
"in recent years we have gained the ability to record human spatio-temporal behavior in increasingly fine-grained detail. at the individual level this type of data holds the promise of personalization: delivering information, products and services in a manner that is optimized for each specific individual."
"hrv data have been more and more considered for sleep staging or sleep stage detection [cit] . this is because, com-pared with traditional psg, hrv data can be acquired easier or more unobtrusively with, e.g., photoplethysmography [cit] and balistocardiograhy [cit] . many studies have investigated classifications between sleep and wake [cit], between rem and nrem sleep [cit], and between wake, rem and nrem sleep [cit] . however, detecting deep sleep has not been well studied. [cit] developed an hrv-based deep sleep detector and achieved an overall accuracy of ∼80%, but they only chose a very small portion (a total deep and non-deep sleep duration of 50 minutes each) of the wholenight recordings from all subjects for classification. in this study, we addressed the problem of overnight deep and nondeep sleep classification using solely hrv data."
"in location-based modeling the gps coordinates of events are typically categorized into a \"vocabulary\" of m locations. for the gowalla dataset the vocabulary is pre-defined by a set of known high volume locations such as businesses and public venues (e.g., [cit] ). for the twitter data, we defined our dictionary using reverse geocoding based on publicly-available geoparcels. geoparcels represents a set of disjoint polygons on a map and represent a specific property or lot (such as a house, a stadium, a store/shop, an airport etc.). the polygons vary in size and shape depending on the function of the property (e.g., typically smaller polygons for houses, much larger polygons for sport stadiums and theme parks). statistics about the number of users and locations are presented in table 1 ."
"despite extensive experiments and researches dedicated to understanding the mechanics and physics of human symmetry perception, it is still unclear how the human brain perceives symmetry such effortlessly, and what exact neural processes are responsible for it. among symmetries, reflection symmetry is more salient type [cit] for humans. researchers vastly studied reflection symmetry and psychophysically revealed the properties that attract human perception [cit] . regions supporting the symmetry structure are called the integration regions. the perturbations and distortions in those integration regions are perceived much easily [cit] by humans. the shape and size of the integration region of reflection symmetry volume 6, 2018 pattern scale along with its spatial frequencies [cit] . authors psychophysically determined that the integration region is 2:1 aspect ratio radii ellipse where longer radius equals the length of the reflection line. as eccentricity increase in human eye retina, the integration region of symmetry gets narrower [cit] . in other words, the symmetry pattern that falls on the peripheral vision has a narrow integration region. therefore, symmetry detection is found preattentive only in fovea area of the retina [cit] . however, it is still possible to discriminate and detect symmetry in the periphery area of the retina, but that symmetry pattern has small perceptual strength [cit] . the orientation of the symmetry pattern has a little effect on symmetry detection [cit] . however, the orientations of supporting features in the integration region of the symmetry pattern have a considerable impact to the robustness of symmetry perception; humans have the higher robustness to features orthogonally oriented to symmetry axis than to those that have parallelly oriented [cit] . human symmetry perception is robust to distortions caused by perspective projection too. moreover, the perception of skewed symmetries helps to perceive the orientation of 3d surfaces which contain those skewed symmetries [cit] . therefore, the perception of skewed symmetries is a crucial tool in the judgment of object orientations in space [cit] . another interesting outcome of psychophysical experiments shows that having the opposite contrast on reflected feature pairs makes the symmetry pattern imperceptible [cit] ."
"in the case implementation, within the area of interest in fig. 6, there were seven dnpls detected. the designed parking hours of each parking lot was set to three [cit] . each dnpl was analyzed by the developed pcp scheme in terms of total cost and total journey time which included both the travelling time and walking time. take parking lot a as an example, from fig. 6, it has the shortest journey distance 9 km among other dnpls from current position while the parking charge was comparably high (167%) than benchmarked parking charge. the formulated moo problem was then solved by nsga. the key measured data for the scheme validation is listed in table 1 . parameters setting of nsga is listed in table 2 . table 3 is presented to summarize the characteristics of literature work and the developed pcp scheme. as discussed, a good parking scheme should consider both objectives of time and cost and finally provide an optimized dnpl which is a faster and more economical route for drivers to reach. in the remaining parts of this section, the importance of the two objectives will be explained."
"resource model (scene ii): physical travelling distance from staring point/current location to dnpls. it involves travelling time and fuel consumption. the relationship between vehicle's speed and fuel consumption was nonlinear [cit] . this scene contained the modeling of two main objectives. after scene i, the objective of total journey time including travelling time to dnpl will be formulated. fuel consumption during travelling is also considered which is a part of the total cost."
where t w ij is the queueing delay due to the signalized ij th intersection road. g is defined as the average fuel consumption rate in idling while waiting at the intersection road. the total idling fuel cost due to intersection roads between j th position and i th position is now became:
"after the pm, the moo is carried out with non-dominated sorting genetic algorithm (nsga) where the output of the nsga is a set of pareto front (pf). section iv will elaborate in detail. the results of the formulated moo problem indicate that which dnpl is the fastest or the most economical to reach. moreover, it also indicated which dnpl is optimized to reach based on fom considering both the total journey time and total cost. in the following sub-section a, queueing model (scene i) at intersection of roads is considered first. in sub-section b, resource model (scene ii) is built and pm (scene iii) is presented in sub-section c."
"during the model checking phase, the model checker uses the modules outlined in figure 2 . the lowest is a module which allocates the necessary memory for the interpreter structures. it can also duplicate and compare memory regions where structures are allocated. in this way, the interpreter may execute different process instructions, and the model checker can build other kripke structure worlds. the specification language uses first-class reaction rules; as a result, it is difficult to deallocate the activation frames of the processes. this task is accomplished by the memory manager, which uses a garbage collector to deallocate all inaccessible interpreter structures. the interpreter executes all running processes and, in particular, matches sent synchronous and asynchronous messages with join patterns. when a match occurs, it executes a new process and may stop the current one. in the case of the model checker, all of the possible process executions and pattern matches are taken into account, and the model checker module creates a process by executing the kripke structure build algorithm. the ctl formula under verification is normalized and verified by the ctl verification module. the following subsection gives more detail about the model checker modules."
"to avoid drivers spending unexpected time roaming on streets for a parking lot and hence to reduce the emission of co 2 as well as resources saving, a new car parking scheme for drivers is designed. to illustrate the problem that drivers encountered, fig. 1 shows the process of parking lot seeking process."
"cost of fuel per gallon is defined as cc k . hence, the total cost from j th position to i th parking lot is calculated as"
"the proposed model checker can be used for the specification and verification of concurrent systems. the tool uses join-calculus process algebra to define and reason about complex concurrent systems and is an interesting example of how concurrency can be modeled thanks to the usage of the cham, through chemistry abstraction. simple notions such as asynchronous messages, join patterns, and reaction rules serve as the basis for their specification. the way in which join-calculus algebra is used to verify concurrent systems differs from specifications which use finite state machines, petri-nets, or temporal logics. the developed methodology offers a high-level abstraction, where reaction rules are first class and can be passed between processes. this possibility offers sophisticated methods of system specification. in this paper, we presented two synchronization mechanisms, semaphore and rendezvous. it is possible to define other mechanisms, such as buffers, barriers, and synchronous and asynchronous channels, as well as communication protocols. the presented model checker is still under development, while its major modules have already been implemented -scanner, parser, semantic and types validator, code generator, interpreter, kripkestructure construction algorithm, and ctl formula verification algorithm. there are still some tasks to be completed (implementation of labels, code loader). since the tool is written in the c programming language, it is possible to extend it with kripke structure optimizations such as cone of influence, on-the-fly verification, and others. it is expected that the model checker can be extended by the notion of localizations which are defined by distributed join-calculus [cit] . such an extension would create the possibility of verifying mobile or multi-agent systems, where mobility and agents would be specified using the notion of locations. the article contributes to the field of formal methods in finding best formalisms, methodologies, and tools, which bridge the gap between theoretical results and the everyday practice of software engineering."
"this article describes a model checker which uses join-calculus algebra [cit] to specify concurrent systems with their properties defined in ctl logic [cit] . join-calculus algebra uses the notions of asynchronous messages, processes, join patterns, and reaction rules to facilitate the formal specification of a concurrent system. its operational semantics are defined by a chemical abstract machine (cham) [cit], where terms are added and removed from a chemical solution according to reaction rules. informally, processes are executed as a result of matching asynchronous messages to join patterns. when such a match occurs, a process is executed with formal parameters of a message pattern substituted with actual parameters of sent messages. furthermore, processes may define new reaction rules which can be used for matching further messages (reflexive cham). the exact matching between messages and join patterns is not specified and, as a result, it is undetermined which reaction rule will be executed. despite its apparent complexity, this property lets us express more advanced constructs and synchronization mechanisms between processes. after messages are matched, they can no longer be used to match other join patterns. the names of join patterns are recursively bound in the process which defines them and within reaction rules within that process."
"the next section will present the state of the art (sec. 2). section 3 and its subsections describe the model checker in more detail. the specification language is described in section 3.1. lexical and semantic analyses are outlined in section 3.2, followed by descriptions of the code generator (sec. 3.3) and interpreter (sec. 3.4). the kripke structure construction algorithm, which is one of the most important parts of the model checker, is mentioned in section 3.5. the ctl verification algorithm used in the tool is referenced in section 3.6. section 4 demonstrates a simple mutual exclusion problem, which is specified and solved with the model checker. the last section summarizes the article and presents some possible future extensions."
"to validate the effectiveness of the queueing model in the developed pcp scheme, the queueing model was removed in (13) and the pcp scheme validation was carried out again. since one of the time elements was removed from (13), a decrease of minimal fom for all dnpls was expected. fig. 11 showed the difference on minimal fom with and without the queueing model. volume 7, 2019 from fig. 11, it is observed the fom without queueing model is lower than that of the fom with queueing model. this is due to the removed queueing model in pcp scheme and hence all the fom without queueing model is lower. when the queueing model is removed, the minimal fom of the parking lot g is less than that of parking lot d. hence, parking lot g now become more favourable."
"for the same dnpl, different optimized vehicle's speed will be adopted to reach the minimal cost or minimal time respectively. let's take fig. 7 as an example. for cost consideration, the optimized vehicle's speed is 27.66 km/hour while for time consideration, the optimized vehicle's speed is 79.95 km/hour. the difference of the two optimized vehicle's speeds will finally lead to significant change of total journey time. similarly, the pcp scheme was applied to rest of six parking lots b to g and their results were shown in fig. 8(a) and fig. 8(b) respectively."
"for fig. 8(a), cost dominance point is considered for the seven dnpls. it showed the total cost required to reach for each of the dnpl. when total cost is considered, parking lot d is favorable for drivers."
parking model (pm) (scene iii): charge of the parking lot and walking distance from dnpl to destination are considered. it is indicated that walking distance is considered from dnpl to destination [cit] . charge of parking lot and walking distance from the dnpl to destination are also contributed to total cost and journey time respectively. formulated models in scene i and scene ii will be considered in this scenario.
"some researchers considered the cost element in car parking problem such as the cost between two positions and revenue of a parking lot. [cit] recommended the use of the temporal vacant parking space in residential area when parking space owners were out for work. the proposed 54498 volume 7, 2019 scheme demonstrated that the model can maximize total revenue generated from charging of users. [cit] suggested a cost model to be minimized among two positions on map. the cost model included the distance between the two positions and percentage of free parking spaces at each node."
"it is observed that one of the main reasons causing traffic congestion is vehicle cruising for a dnpl. this will cause drivers to spend unexpected time and cost to find a dnpl and hence it will jeopardize the traffic congestion. unfortunately, there is no vehicle cruising prevention scheme which alleviated the burden of traffic congestion by guiding drivers to dnpl effectively. the contributions of this paper are (1) new queueing model and resource model are developed for parking lot seeking; (2) a new pm is developed for a holistic optimization of time and cost; (3) performance is measured by a new indicator referred as fom. analysis reveals that the incorporation of pm improves the performance by 24% ∼ 43%. appropriate parking lot can be obtained from the fom. hence, drivers can reduce the time of vehicle cruising on roads and this will finally alleviate the burden of traffic congestion."
"recent changes in processor design have influenced software development methodologies. increases in computational speed are gained by adding cpu cores rather than increasing processor frequency. to benefit from this, applications need to have several threads of execution; in turn, increased concurrency leads to algorithms whose execution is difficult to understand and verify. one of the approaches in dealing with this complexity is to analyze systems using formal methods."
"the described tool is implemented in the c programming language using the flex and bison tools [cit] to generate a language scanner and parser. its main modules ( fig. 1 ) are separated into two groups; the first does static processing for a system specification and generates bytecode, while the second loads bytecode, executes it, and creates a kripke structure. the kripke structure is checked for whether or not the ctl formula under verification is satisfied in the model. when a formula is satisfied, a sequence of instructions (witness) is generated, which leads to the appropriate state. otherwise the model checker outputs sequences which lead to states where the formula is not satisfied (counterexample)."
"in the first step, the model checker tokenizes and parses the system specification. the scanner removes comments and distinguishes tokens such as keywords, constants, and identifiers. the parser checks if the specification is a proper sentence in the language grammar and accordingly builds a parse tree. the parser accounts for operator priority and associativity. during parsing, all found symbols are placed in a symbol table. the syntax tree is checked for any semantic errors (e.g. unmatched types) during semantic analysis. variable scope is also analyzed for proper value assignment. when the specification does not yield any syntactic or semantic errors, a bytecode is generated."
"the execution of processes and creation of interpreter structures leads to a spaghetti stack. such structures are difficult to deallocate after a process stops execution. thus, a garbage collector is used to remove inaccessible structures."
"it is assumed that a driver is going to a destination from starting point or current location and would like to seek a dnpl. there are multiple dnpls as shown in fig. 1 . after the driver set his/her destination, it is assumed that there are p i dnpls detected. the driver would like to choose an optimized dnpl among p i from the perspective of cost and time."
"the types definition lets us declare types used within the system specification. types include interval, enumeration, message pattern, and vector. the introduction of types allows us to verify whether or not a language can be executed during syntactic analysis and whether there will be no runtime errors caused by a types mismatch. the interval and enumeration types allow the smallest possible representation of numeric values and, thus, limit memory usage. the vector type lets us return multiple values from a process. the message pattern type allows us to create variables and assign message patterns of such type."
the specification was verified by the described tool and no counterexample could be found. the tool printed only witnesses which lead to a proper system state.
"before choosing a parking lot, it is necessary to know the parking charge of each parking lot. since the parking charge for different parking lots varied, it is defined that m i as the parking charge of i th parking lot. moreover, the expected parking hour of user is defined e i and it is represented as"
"the deficiencies (a)-(c) are considered in this investigation since they affect the choice of parking lot. therefore, in this paper, the deficiencies are considered through the pcp scheme. the performance of the developed pcp scheme will be evaluated by using a newly defined fom which considers the dependence of time and cost. fom, comprising of composite factors described in section ii (a)-(c), will be evaluated to show the effectiveness of the optimized parking scheme. the fom evaluation will be shown in section v."
length of platoon for vehicle through a single signalized ij th intersection road is further incorporated into (5) where l v is the averaged vehicle length and t is the time required for a vehicle from idling to averaged speed.
"ncr(i, j) iii: within this pair of nodes, node i and j are directly connected to each other but either of them is not connected to third node. it means that this pair of nodes is isolated from other nodes. from fig. 4, node pq is an example of isolated pair of nodes. since there is no way to reach either node p or node q, this relationship will not be considered furthermore."
"the described model checker is a proof of concept for the usage of new methodologies of formal system specification and verification in software engineering practice. the proposed application of join-calculus algebra for system specification, together with its representation by a chemical abstract machine, is a 6tnovel approach. the devised specification language has been designed for ease of use. as new programming languages (cω, join java) which use the notion of join patterns are developed, the presented methodology serves as an evaluation of how these languages can be formally verified. the choice of model checking for system verification was determined by its successful application in other domains and its recent developments. the possible extension of the tool through the notion of localizations (defined in distributed joincalculus [cit] ) would allow the specification and verification of distributed, mobile, or multi-agent systems."
"the driver is now considering two objectives (i) total journey time required to reach the destination; (ii) total cost including fuel and parking charge to be spent for dnpls. to understand the underlying process during parking lot seeking, the process in fig. 1 is divided into three scenes."
"to enhance the rate of convergence during the optimization, it is necessary to assign upper and lower boundaries of the parameters which fulfilled to the unique design of the transportation network. during the operation, the quality of undesirable individuals can be effectively reduced by assigning reasonable boundaries and hence fasten the computation."
objectives of total cost and total journey will be formulated into a single moo problem with a set of constraints which will be discussed in the next section.
"figures 4 and 5 demonstrate the specification language's most significant features. process which was invoked finishes. after messages are sent, it is possible to execute processes which pertain to each of the reaction rules. this non-determinism in choosing reaction rules lets one define more advanced synchronization mechanisms and is one of the strengths of join-calculus process algebra. figure 5 presents other instructions from the specification language. line 1 defines an integer type which is then used to declare variables (10, 20) . lines 3 -6 define types of messages which can be returned from reaction rules. variables of those types are declared in lines 11 and 12. it is also possible to return a vector of values (defined in 6) which is used in line 30. reaction rules are defined in lines 14, 18, and 19. a concurrent statement which is used in line 33 allows us to execute two processes simultaneously. the specification also demonstrates how to pass message patterns between processes. in this example, addmul is passed as a parameter of accept and used to compute some value. furthermore, lines 15, 23, 25, and 27 show how to return values from processes."
"in short, based on the above literature, it is concluded that objective of time and cost are distinct aspects and there is no former vehicle cruising prevention scheme that considers the followings:"
"the described model checker uses formal grammar to specify concurrent systems and their properties. the specification language is statically typed with first-class reaction rules. some improvements were made to facilitate the system specification and its ctl properties -the possibility to express ctl formula under verification and its fairness constraints. the language uses the following keywords: async, boolean, ctlspec, else, fail, false, fairness, if, print, return, to, typedef, true, var, vector, void. some keywords are reserved for distributed join-calculus location, go, halt, fail and for the specification of ctl formula constraints ctlspec, fairness. a system specification consists of three main parts: types definition, system specification, and the ctl properties which need to be verified."
"on the other hand, the total driving and walking time between any two points are calculated as (13) where d w is the walking distance from parking lot to destination and v w is the walking speed of user. the total journey time from j th position to i th parking lot is calculated as"
"where f m (x) is the objective values for each individual in the whole population, and is the variable range. a binary variable is defined to indicate whether parking lot i is chose from position j:"
the contributions of the paper are as follows: the rest of the paper is organized as follows; section ii provides the related work on recent car parking development. section iii describes the pm design in the pcp scheme. section iv formulates the objectives of total journey time and total cost into a multi-objective optimization (moo) problem with a set of constraints. section v shows the performance of our work and the insight of fom. section vi gives a conclusion.
"during the whole process from starting point to destination point, two main objectives were considered, namely total cost and total journey time. for total cost represented in (12), it consisted of the fuel consumption during vehicle idling, driving as well as car parking charge. for total journey time in (13), it consisted of queueing at the intersection of road, vehicle travelling time and walking time from dnpl to destination."
"in the case study, seven dnpls are found. when multiple parking lots were considered, it is required to display the results in an more observable way. therefore, the pcp scheme was applied to the seven dnpls and result was illustrated in fig. 9 . from fig. 9, each color represented the performance of the pcp scheme correspond to a particular dnpl. since seven dnpls were found and hence seven different colors were used. fig. 9 is provided to show how the change of vehicle's speed affects the performance of the two objectives which are total cost and total journey time. at the initialization process of the nsga, the value of vehicle's speed is set from 10 km/hour to 80 km/hour where 80 km/hour was speed limit of most expressways in hong kong. it is observed that increasing vehicle's speed contribute to time saving while bringing higher total cost."
"for fig. 8(b), time dominance point is considered for the seven dnpls. since the objective of total journey time is emphasized, different optimized vehicle's speed will significantly vary the total journey time. when time is considered, parking lot a is favorable for drivers."
"queueing model (scene i): driver is waiting at intersection road where vehicle queueing and idling consumption are considered. fig. 1 shows an illustrative example of parking lot seeking process. the signalized intersection will stops/allows vehicle to pass depending on the cycling of the traffic light [cit] . investigation [cit] further indicated that idling of vehicle also contributed to extra fuel cost. this queueing model considered the cycle of traffic light and vehicle stream from starting point/current location to dnpl. apart from the physical travelling time, the queueing at traffic light or intersection road is another key element to be considered. fuel cost due to idling is also considered."
"to validate the performance of the developed pcp scheme, it is appropriate to design a route for testing. the university of hong kong was designed as the starting point and city university of hong kong was chose as the destination point and the route is shown in fig. 5 . the scenario became a driver driving his vehicle from staring point to destination point and an optimized dnpl is sought for. there are seven dnpls detected, namely parking lots a to g. it is worth to emphasize that after the driver has parked his car, say, in parking lot a, he still is required to walk from parking lot a to destination. parking lot a was 9 km from starting point of user while parking lot g was 10.6 km which is the longest distance among the seven dnpls from the starting point."
"however, the problem now became which parking lot, a or d, should be chose for drivers since parking lot a has the minimal total journey time while parking lot d has the minimal total cost. the pcp scheme provided a solution to this problem."
"the system specification contains reaction rules and statements. figure 3 presents an example reaction rule. each reaction rule is comprised of a join pattern and corresponding process instructions. a process is executed when a match occurs between sent messages and the messages patterns of a join pattern. each process may define nested reaction rules. join pattern names are recursively bound inside each join pattern at the same nesting level. statements include assignment, conditional instruction, return, message send, and parallel execution. the values of variables are calculated with arithmetic expressions. it is also possible to declare instruction blocks."
"cost dominance point: it indicated that the objective of cost is emphasized, it means that this point has the lowest cost which is $96.69 corresponding to 23.65 minutes using optimized vehicle's speed of 27.66 km/hour. time dominance point: it indicated that the objective of time is emphasized, it means that this point has the lowest time required which is 10.86 minutes corresponding to $100.8 using optimized vehicle's speed of 79.95 km/hour."
"the proposed tool uses a ctl-labeling algorithm [cit] . the algorithm labels all kripke structure states with subformulas of a formula being verified. as any ctl formula can be expressed using only five operators: ¬, ∨, eg, ex, eu, the algorithm labels only states which satisfy these operators. thus, a formula is normalized before verification; i.e., expressed using only these operators. if the initial state of the kripke structure is not labeled with a label from the formula under verification, then the whole formula is unsatisfied and a counterexample is generated. otherwise, a witness is generated."
"as shown in fig. 7, the pcp scheme was applied to parking lot a for explanation purpose. it is observed that there are multiple points on the curve and each point represented a solution for the problem described in (16) and (17) . it is observed that the relationship between the time and cost in parking scheme is contradictive implying that time is saved when extra cost is spent and vice versa. this phenomenon corresponded to (8) which indicated that the fuel consumption per unit distance and vehicle's speed was in non-linear relationship. in fig. 7, two extreme points are defined which are referred as cost dominance point and time dominance point."
"is even more severely jeopardized. therefore, the demand for a vehicle cruising prevention scheme to alleviate the traffic congestion problem is desperately needed. to achieve this goal, it is envisaged that an optimized parking scheme, that considers the holistic journey from the beginning of travel with the inclusion of the road-to-park conditions, should be developed. the road-to-park conditions should include the charging fees of parking lot and vehicle queueing at signalized intersection roads since the conditions will affect the choice of parking lot. hence, a parking cruising prevention (pcp) scheme, which incorporates a new parking model (pm), is developed to help drivers to seek a dnpl. a new indicator, referred as figure of merit (fom), will be developed to evaluate and thus effectively indicate the parking effectiveness."
"the objectives of time and cost are important in choosing a parking lot and it is found that there are no vehicle cruising prevention schemes considering both time and cost in the market. meanwhile, although some parking models were commonly used in analyzing and simulating transportation network [cit] and algorithm was developed to help driver to look for parking lot [cit], the car park guidance for drivers was ignored since drivers are ignorant of available car parks around the destination. therefore, it is necessary to design a cruising prevention scheme to guide drivers to reduce the time of vehicle cruising for a parking lot which in turn will alleviate the burden of traffic congestion in cities."
"from fig. 11, it is observed that there is a decrease on minimal fom when queueing model is removed. the decreased percentages from parking a to g are 35%, 31%, 28%, 24%, 24%, 25% and 43% respectively. such a percentage difference is due to the number of signalized intersection roads encountered from starting point to the seven parking lots a to g are different. for the path with more signalized intersection roads such as parking lot g, the path has the maximum number of signalized intersection roads. in means that the queueing model played a significant role in the formulation of total journey time. once the queueing model is removed from the total journey time formulation for parking lot g, an obvious decrease on the overall minimal fom is expected. figure 11 showed the removal of queueing model is able to affect the minimal fom and finally the preference of parking lot. hence, the influence of queueing model should not be negligible."
"to determine the performance of the developed pcp scheme, the system requirement is formulated and an optimization is needed. therefore, an optimization that satisfies all the objective requirements is utilized. it is known that the nsga is genetically strong and is a powerful seeking skill which imitates the nature of evolution. in many practical engineering problems such as telecommunication network design, global optimum does not exist. moreover, most of the problems in engineering desired the consideration co-existence of multiple conflicting objectives to provide a comprehensive and better result. compared to single objective optimization, moo has outstanding advantages as follows: the diversity of multi-objective optimization was much wider than singleobjective optimization [cit] . therefore, the moo problems rendered the launch of multi-objective evolutionary algorithms (moeas). the moea was a kind of ga that always seeking for a set of non-dominated optimal solution which is regarded as pf [cit] ."
"the generated bytecode uses instructions which are interpreted by a stack machine. there are eight groups of instructions: stack operation (push, pop, dup), jump (conditional, unconditional), message send (synchronous, asynchronous), return, store, block (begin, end), concurrency (beg, end), and primitives. as the tool uses a stack machine, all expressions are transformed into reverse polish notation."
"during the initialization stage, the number of generation, population size, objective functions and constraints were determined. the averaged distance of two nearest solution points represented the optimal solution. the main point to solve a moo through ga is to generate a new population for further optimization in order to reach optimal solutions. process of selection, crossover and mutation simulated the process of natural evolution [cit] . such an iteration process will be stopped when either the maximal number of generation is reached or the output solutions converge and the final pf is obtained. each solution point on pf is an optimal solution and does not dominate each other."
"(a) vehicle queueing at intersection road is ignored; (b) no consideration of both time and cost simultaneously in car park hunting process; (c) driver's guidance-to-parking information is ignored. in this paper, the deficiencies are considered in the pcp scheme."
let s ij be the fuel level of the vehicle at position j and the vehicle is going to i th parking lot. now the moo is designed to 1) minimize the total cost z ij and 2) minimize the total journey time t d ij during car park process. the two objectives are formulated as
"from the persepctive of drivers, the criteria for choosing a dnpl is to keep both cost and time in a relatively low level. to do so, it is necessary to illustrate the scheme's result in a more observable way."
"vehicle cruising for a parking lot in highly developed cities may jeopardize the traffic congestion in a transportation network. when the transportation network is not well-designed or optimized to effectively guide drivers, congestion problem will become worst. it is reported that traffic congestion is commonly found in the urban cities like usa. [cit] urban mobility scorecard, it was indicated that total cost due to congestion was about $160 billion along with 6.9 billion hours [cit] . it implied that drivers were required to spend extra time and cost than they expected to reach their destination. a survey conducted by shoup [cit] showed that 30% of traffic congestion was caused by vehicles cruising for parking spaces. for some highly urbanized cities such as hong kong, a car park is regarded as a valuable asset since the parking lots are very limited and the parking charge is expensive. in such cities, when drivers found difficulty in finding a destination nearby parking lot (dnpl), the traffic condition"
"the interpreter uses six basic structures, depicted in figure 6 and figure 7 . each process (proc.rrule) is associated with a reaction rule (rrule) which contains the names of messages needed to execute it (rrule.msgn), literals in bytecode (rrule.literaln), pointers to reaction rules nested in the reaction rule itself (rrule.rrulen), and bytecode. processes execute the instructions (proc.ip of reaction rules. process data is kept in messages used to execute it (frame.msgn), in activation frames (frame.varn), and in blocks (block.vars). local data is kept on a stack (proc.stack) whose top is pointed to by proc.sp. the current frame and block are pointed to by the proc.af and proc.hc links. for optimization reasons, the proc.af link always points to the current acti-vation frame or block, while proc.hc points to the process activation frame. frame.parent is a data access link to the activation frame from which each given process was executed. a data access link lets us access variables which are nested in upper levels. when a code block is executed, proc.ip and proc.sp are stored in block.ip and block.sp respectively. a block of code holds a local stack (block.stack) which is used to perform calculations. sent messages (message) are delivered to activation frames (frame.msg#). each message contains its name (message.name) and the values of its parameters (me-ssage.params). they also hold control links (message.proc) to processes which have sent synchronous messages. a message control link allows the return of execution to the original process which sent the synchronous message. when processes are created using parallel instructions, their proc.parent proc control link points to their parent process and allows the resumption of execution when both end. the running state of a process is kept in the proc.running variable. the ability to store and return message patterns from reaction rules is implemented using closures (rrpointer), which hold the name of the message and a pointer to the activation frame. interpreter structures allow us to execute processes. messages are delivered and matched to reaction rules in order to create new processes. in the case of synchronous messages and parallel instructions, the parent process is stopped and the child process starts execution. for the model checker, how the process scheduler works is not relevant. the model checker algorithm finds all possible message matches and process instructions."
"the system's ctl properties are declared as the last part of its specification. they are expressed using arithmetic and ctl ag, af, au, ax, eg, ef, eu, ex operators."
"assume a vehicle stream is travelling in i th traffic road of distance and going to pass through the signalized ij th intersection road as shown in fig. 3 . the green light duration which will allow vehicle stream to pass through line b into the signalized intersection ij th road is g i, the cycle for the traffic light system within the signalized intersection is c i which means that the probability for the vehicle stream pass through the ij th intersection without waiting time is represented as"
