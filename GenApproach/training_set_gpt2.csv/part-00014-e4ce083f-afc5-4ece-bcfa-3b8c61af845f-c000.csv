text
"the lack of formalization of semantic data models has been one of the two main motivations for the development of description logics [cit], the other motivation being computational amenability. description logics are the theoretical counterpart of the ontology web language (owl) [cit], a distinguished member of the semantic web suite. this suite includes also the resource description framework (rdf), a rather basic knowledge representation language endowed with a model-theoretic semantics [cit] . rdf offers a vocabulary for semantic modelling known as rdf schema [cit] . it would seem natural, then, to resort to one of these languages to express the inferential apparatus of the crm. there are several reasons why we will not follow this route."
"by so doing, a new set of axioms is obrained from t c, which we denote as t + c . intuitively, t c and t + c are equivalent sets of axioms, since they state the same constraints in different ways. formally, this is proved by propositions 1 and 2, given in appendix. finally, we apply the above transformations to remove negation also from the abox of a kb. to this end, it suffices to replace:"
"it is not difficult to see that we need to add more rules to those of p c in order to be able to derive the implicit negated atoms by means of a datalog-based inference system. in particular, we need to add the rule"
"ontological knowledge is expressed in logic by means of logical axioms, which correspond to the constraints of semantic modelling. factual knowledge is expressed in logic by means of ground atoms, such as place(pisa) and falls within(pisa,italy), which represent the same kind of knowledge expressed by the class and property instances in semantic modelling. these basic considerations can be used as general principles to establish a correspondence between the crm and a first-order logical language. however, our task requires to identify a specific first-order language l c that is able to capture the intended meaning of the crm vocabualry. in order to achieve this goal, we will build on the correspondence outlined above and carry out a detailed analysis of the features of the crm, to the end of determining the specific alphabet and syntax of l c that will be needed to express such features."
"generally speaking, a crm kb is a set of l c sentences that describe some slice of reality. we now turn to the task of defining exactly what kind of sentences we expect to find in a crm kb."
"in order to axiomatize quantification statements we have introduced co-reference, which will also play an important role in aboxes, as it will be showed later. we have therefore to introduce axioms to capture the basic characteristics of co-reference. co-reference axioms are identical to equality axioms, which are well-known (see, e.g., [cit] ):"
"if an atom of the third kind above is in a, then the application of p c to the kb reveals an inconsistency in the kb. as already noted, skolemization preserves"
"the objective of this paper is to investigate the use of a pre-processing pipeline [cit] designed to speedup the personalization of 3d cfd models. twelve real-time 3d echocardiography (rt3de) sequences of healthy volunteers are used to highlight the benefits and limitations of the proposed pipeline. the capabilities of the pipeline to quickly generate realistic cfd simulations are then leveraged to study the intracardiac hemodynamic variability in the presence of diseased mitral valve. finally, the results from those numerical simulations are used to assess the potential limitations of the pisa method with respect to different mr types."
"which reads: if the individuals x, y and z are linked by p.n, then x and y are linked by p and z is a e55 type. for example, meta-property p62.1 mode of depiction specializes the property p62 depicts (is depicted by) by indicating the type of the depiction, which is a e55 type. the instance of the above axiom schema capturing this meta-property is therefore"
"it is not difficult to see that llcl2 can be derived by symeq and llcl1, so it can be dispensed with. we are therefore left with llcl1. based on these considerations, in the tbox t c we remove axiom refeq and replace the axiom schemas llcl, llpr and llmp, respectively, by:"
"as a result of this first registration step, the two ventricle surfaces are considered superimposed with their main anatomical features positioned at approximately the same location. the objective of the second registration step is to obtain a transformation φ 2 which fits the surface of the generic mesh onto the image segmentation. in order to maintain the original generic mesh integrity, several constraints are imposed: the deformable registration should not change the mesh connectivity, and mesh triangle elements should not be degenerated by the transformation. for those reasons, a special interest was given to the group of diffeomorphisms, a special class of maps which enforces those constraints. in this work, the large deformation diffeomorphic metric mapping (lddmm) framework [cit] implemented in the open-source software deformetrica [cit] was used."
"the computational domain has been created based on the zygote 3d human heart model 1, a geometrical model based on computed tomography and magnetic resonance imaging of a healthy, middle-aged, caucasian male. from this geometry, the inner layer of cardiac tissue, i.e. the layer in contact with the blood, was extracted. the two right cavities were discarded and the left ventricle and left atrium inner surfaces were used as a basis of the so-called generic geometry. manual edition of the surface mesh, using the software materialise 3-matic v11.0 2, was performed to add the atrial septum. surface smoothing was performed to ensure that the surface was free of artifacts. a portion of the ascending aorta was also included in the fluid domain in order to model a short section of blood after the aortic valve location. the resulting fluid domain is shown on figure 1a ."
"a query q( x) is any open well-formed formula of l c . the answer to a query q( x) against a c kb k, ans(q( x), k) is the set of tuples of standard names that are instances of the query in every model of k :"
"atom e21(d) implies e77(d) (via subc), which implies e2(d) (via disc) which implies e5(d) (via the correspondent of subc). on the other hand, atom p 12(d, b) and (9) imply e5(d) and so a contradiction is generated."
"finally, we deal with the co-reference axioms, showing in the fourth group in table 3 . the refeq axiom is not a dpc, but it can be dispensed with, since it states a mathematical property of co-reference that does not have any computational import: no user will ever be interested in the fact that every term co-refers with itself, nor this fact is going to be used for deriving new knowledge, as it can be checked from the rest of the axioms. symeq and transeq are clearly dpcs. each one of the three leibnitz laws can be restated into an equivalent dpc. for brevity, we show how this can be done for llcl, the other two can be trated in a similar way. llcl is equivalent to the conjunction of the following dpcs:"
"several additional limitations limit the simulation of a fully patient-specific blood flow. first, due to the acquisition process, the atrium is not available in the rt3de sequences. as a result, the same generic atrium was used for all cases. moreover, due to image quality, there is no clear definition of internal lv structures such as the trabeculae, papillary muscles or chordae. as a result, smooth ventricular geometry, a strong simplification of reality, was used. indeed, it has been shown in the past that the papillary muscles [cit], trabeculae [cit] and chordae [cit] have an influence on the intracardiac hemodynamics. finally, as it is difficult to obtain personalized pressure curves for the pulmonary veins inlet and aortic outlet in a clinical setting, population-derived pressure bc was used."
"a strong trend in the cardiac modeling community is to personalize models using patient-specific data, geometry and dynamics derived from imaging devices such as computed tomography [cit], magnetic resonance imaging [cit] or echocardiography [cit] . the reasons behind this trend is multi-fold. first, by exploiting the patient-specific hemodynamics, relevant clinical information can be obtained [cit] . for example, local informations that cannot be directly obtained on the images, such as the pressures, can be obtained from the simulation results. moreover, if the predictive power of the models are good enough, medical interventions can be simulated in advance to predict their outcomes and provide tailored care to patients. finally, by building a big database of in silico cases based on real patients, the medical community can build a statistical description of the population hemodynamics. personalizing models, however, brings several additional layers of complexity."
"happily, all these axioms are dpcs. notice that the same axioms would create undesired results, if expressed through negation. for instance, rdom would be expressed as ¬p (x, y) ⊃ d p (x). on the other hand, dom is p (x, y) ⊃ d p (x). considered together, these axioms imply (∀x)d p (x), a definitely undesired outcome."
"from a knowledge representation point of view, the present study can be viewed as an attempt at defining the knowledge level of a crm knowledge base [cit] ."
"in addition to the ontological knowledge, a kb must also contain sentences representing the state of the world in the domain of discourse. these sentences form domain knowledge. based on the analysis carried out in section 2, we envisage two kinds of domain knowledge:"
"the pipeline was applied on all twelve cases. regarding the computational effort, all the calculations were done on a computed using a intel core i7-4810mq cadenced at 2.8 ghz, 16mb of ram and a nvidia quadro k4100m graphic card. table 1 reports the computational time required for the pre-processing (i.e. automatic segmentation, registration and dynamics transfer) of the image sequences. to provide the best possible estimation, no other task were launched at the same time on the computer. an average time of 55 minutes was dedicated to the automatic registration and dynamics transfer. the deformable registration was the most time-consuming task as it represented around 77 % of the total pre-processing time. the second most time-consuming task was the spatial interpolation of the data, and more specifically the writing of the data files for all the time steps. writing 1600 individual velocity files (two cardiac cycles) accounted for around 22 % of the total time. the affine registration, time interpolation and distance map generation accounted for less than 1 % of the total computational time. after this pre-processing step, the numerical simulations could be performed without any additional manual intervention."
"from the application of one of the rules having a co-reference atom in their heads, i.e., either rules symeq, transeq or an instance of one of funcp, funcip. in this case two different standard names, and possibly some constants, have been associated to the same individual through a functional property, and this too is an obvious inconsistency."
"using the deformed generic meshes as the computational domain and the patient-specific lv dynamics as the boundary conditions d n applied on the lv surface, the mathematical model was used to compute the hemodynamics of all the cases with the model parameters available in appendix appendix b. simulations were carried out for one cardiac cycle. the rv and rf was computed for each synthetic pathological cases and can be found in table 3 . a strong variability in the fraction of blood regurgitated was observed as it ranged from approximately 17 % for case c02 t3 to approximately 82 % for case c07 t1. this reflects the variability of the designed synthetic regurgitant cases. the rf was positively correlated with the area of the hole (figure 3), which is coherent with the current philosophy of assessing mr severity by estimating the eroa as a surrogate value. in all the simulated cases, the major features of the diastolic blood flow were similar. as a result of a drop in ventricular pressure, caused by the dilation of the ventricular cavity, a backflow closes the aortic valve. nearly simultaneously, the mv opens and blood flows from the la into the lv. due to the viscosity of the blood and the difference between the mitral inflow velocity and the lv blood velocity, a vortex ring appears at the tip of the mv leaflets. due to the assymetry of the valve leaflets and their opening angle, the blood entering the lv is directed towards the lv posterior wall. as the vortical structure propagates within the ventricle and reaches the posterior wall, the posterior side of the vortex gets squished whereas the most anterior side fully develops. due to the variability of the designed mr, both in term of valve morphology or in term of region marking, the hemodynamics during systole was, however, quite different from case to case. to highlight some of the differences between all those simulations, the blood flow is investigated at mid-systole for all pathological cases and grouped in several categories."
"according to the first point, each standard name denotes itself in every world state. thanks to this choice a one-to-one correspondence between standard names and domain objects is established, satisfying at once all three requirements on crm individuals spelled out in section 2.2. in fact, every occurrence of a standard name in a sentence can be seen as an occurrence of the \"real thing\", immediately revealing what the sentence is referring to. in other words, standard names aim at representing exactly what a particular team of maintainers of a crm kb are convinced to exist in the domain they are modelling by this kb at a particular time. in contrast, the interpretation of constants is not fixed, so that different interpretations may assign different standard names to the same constant. this behavior respects the intuition that is at the basis of constants: as already pointed out, constants are used whenever the identity of an individual is not certain, which is to say that the individual at hand may be anyone of the known standard names. the denotation of predicate symbols amounts to say which standard names belong to each class (technically, this is the extension of unary predicate symbols), and which individuals are linked by each property and metaproperty (the extension of binary and ternary predicate symbols, respectively). this can be done with a single rule as follows:"
"we are now ready to define a crm kb. following a standard practice in knowledge representation, a crm kb holds the ontological and the domain knowledge in separate sets of sentences, known as the tbox and the abox of the kb, respectively. while the tbox will be the same in every application, the abox is expected to vary from application to application."
"a sentence of l c is a formula each variable of which is bound to one quantifier, i.e., a formula with no free variables. as already pointed out, sentences are the elements of a logical kb, since they are used to express both axioms (ontological knowledge) and ground atoms (factual knowledge)."
"mitral regurgitation (mr), resulting from pathologies affecting the mitral valve complex [cit], is one of the most prevalent valvular heart disease [cit] . mr is classified using the carpentier classification in type 1 (normal leaflets motion), type 2 (excessive leaflet motion) and type 3 (restrictive leaflet motion) [cit] . the current clinical guidelines recommend integrating specific, supportive and quantitative features to provide the best mr severity assessment [cit] . in that regard, echocardiography plays a significant role in the evaluation of the severity of the mr [cit] . using this imaging technique, effective regurgitant orifice area (eroa), regurgitant volume (rv) and regurgitant fraction (rf) can be estimated using the proximal isovelocity surface area (pisa) method [cit] . although easy to apply, the pisa method suffers from an important number of limitations, due to oversimplifiying assumptions and to the fact that echocardiography is highly physician dependent [cit] . we believe that numerical simulation could provide a better insight about the heterogeneous hemodynamics resulting from the different types of mr and help assess the limitations of the current clinical evaluation tools, in particular the pisa technique."
"several synthetic mr cases were produced by combining a rt3de sequence, a generic mesh including a pathological valve and an arbitrary mr marking strategy. for example, using the rt3de sequence of patient c01 with a standard flat mv geometry marked by a circular mr region, a type 1 mr case referred to as c01 t1 was produced. a total of 11 mr cases were designed, including four type 1, four type 2 and three type 3 cases, in order to reproduce mr variability. four cases were designed for t1 mr as it includes a lot of different geometrical subclasses. moreover, four cases were designed for t2 mr as it is highly prevalent among the different mr types [cit] . specifics about the different synthetic pathological cases can be found in table 2 ."
"in this section, the strategy considered to generate image-based cfd simulations is detailed. first, a generic geometry of the left heart and a mathematical model of the intracardiac hemodynamics are presented. the strategy used to automatically adapt this generic model to rt3de sequences is then described."
"as customary, we will consider sentences including the universal quantifier ∀ and the connectives ∧ (\"and\"), ⊃ (\"implies\") and ≡ (\"if and only if\") as part of l c, obtained as abbreviations of the equivalent sentences using the previously introduced symbols. for instance, the sentence ∀x.e70(x) ⊃ e77(x), stating that every e70 thing isa e77 persistent item, is an abbrevation of the sentence ¬(∃x.e70(x) ∧ ¬e77(x)), stating that no thing is not a persistent item and which in turn abbreviates ¬(∃x.¬(¬e70(x) ∨ e77(x))) built with the official connectives and quantifier. this last sentence is very hard to read, and this is why abbreviations are introduced."
"a reference frame of the rt3de sequence is chosen and its associated segmented mesh is considered. the generic mesh, described in section 2.1, and the segmented mesh exhibit different local geometries. moreover, their position in space, topology and size are different. as no atrium and aorta exist in the segmented mesh, those two elements will not be used to drive the registration process. formally, the objective a the registration process is to find a spatial transformation φ that maps an object a onto another object b. this amounts to minimize the distance between φ(a) and b under some appropriately chosen norm."
"4. scaling of the generic mesh so that its base to apex distance is equal to the base to apex distance of the segmented ventricle (transformation φ 1,s );"
"echocardiographic images of twelve healthy volunteers were acquired using an ie33 ultrasound system (philips, andover, ma) equipped with a 1-5 mhz transthoracic matrix array transducer (xmatrix x5-1). rt3de images were reconstructed as volumes over one cardiac cycle from acquisitions of sub-volumes over four cardiac cycles. an example of such data is depicted on figure 2a . using the software qlab, 3dqadvanced plugin (philips, andover, ma), segmentation and tracking of the left ventricle was automatically performed. as a result, sequences of meshed surfaces depicting the left ventricle surface and dynamics were obtained. each segmented surfaces are described by a triangular surface mesh that is composed of a constant number of 795 vertices and 1584 triangles. examples of the resulting mesh in end-diastolic and end-systolic phases are depicted on figure 2b ."
"the cidoc conceptual reference model 1 (hereafter crm) is a well-known conceptual modelling language for documenting cultural heritage artifacts, with a special attention to museum objects. [cit] (iso21127:2006) [cit] (iso21127:2014)."
"under the some simplifying assumptions, blood is converging towards the regurgitation and the isovalues of this aliasing velocity are hemispherically distributed around the mr center in the convergence region. the physician chooses a second landmark l 2 located on this isovelocity surface and the distance r alias between the two selected landmarks is computed. the physician computes the area of an hemisphere of radius r alias and multiplies it with the velocity v alias to obtain the flow rate at the isovelocity surface level. by dividing this flow rate by the maximal velocity of the regurgitation v max, the physician obtains the eroa. using the velocity time integral (vti), the integral of the velocity magnitude at the valve level with respect to time during the mr, the total rv can also be estimated."
"the notion of crm knowledge base (kb) had been further defined, presenting a set of sentence types that are suitable for representing the state of the world in the domain of discourse. to allow consistency checking on a kb, a reduction of the axioms of the theory to the datalog program p c has been presented. the datalog program efficiently computes the closure of a kb, including all implicit atoms of the kb."
"the abox a of a kb can be viewed as an instance of the symbols in the datalog program p c . by applying p c to a, the minimal model a of p c that includes a is obtained in an efficient manner, that is using limited space and time resources. more specifically, a includes the following types of atoms:"
"case c01 t1 was produced by considering the standard mv geometry with a circular hole. as a result, while part of the blood pushed by the lv endocardium is ejected towards the aorta as expected during systole, part of the blood is also directed towards the mv hole and creates a jet of blood with high velocities directed towards the posterior-mid atrium wall (figure 4a ). the isovelocity surfaces of the converging blood are hemispherically shaped, as depicted on figure 4b . cases c07 t1 and c11 t1 were produced by considering the standard mv geometry with elongated slit holes. as a result of this geometry, the isovelocity surfaces of the converging blood are not hemispherically shaped anymore, they are elongated hemiellipsoids as can be seen on figure 5 for case c11 t1. case c05 t1 was produced by considering the standard mv geometry and two small holes located at the antero-lateral and postero-medial commissure. instead of converging towards a single hole, the blood flow splits and part of it passes through one hole while part of it passes through the other. this very specific behavior results in two separate hemispherical isovelocity surfaces ( figure 6 ). cases c04 t2, c05 t2, c10 t2 and c12 t2 were produced considering the type 2 mv geometry which included a prolapsed valve with flail posterior leaflets. contrary to the previously described cases, the resulting mr is highly asymmetric. the blood passes through the mv and the posterior leaflet flail channels the blood flow tangentially with respect to the mitral valve plane. even though the opening through the mv is rather circular, the isovelocity surfaces are partial hemispheres that are restricted on one side by the presence of the mitral valve ( figure 7) . cases c01 t3, c02 t3 and c09 t3 were produced by considering the type 3 mv geometry which included a restriction of the anterior leaflet. similarly to the type 2 cases described above, the valve configuration is asymmetrical. as the anterior mv leaflet is restricted, a channel orients the flow towards the posterior-inferior atrium wall. as a result of the slit orifice, the isovelocity surfaces are elongated hemiellipsoids, similarly to cases c07 t1 and c11 t1 (figure 8 ). the hemodynamics during systole is therefore highly dependent on the underlying mr pathology, mv geometry and orifice. this variability was partly highlighted by the differences of the isovelocity surfaces in the flow convergence region. indeed, while the circular orifice resulted in an hemispherical isovelocity surface, elongated orifices resulted in hemisellipsoidal isosurfaces, prolapsed valves resulted in partial hemispherical shapes and collection of small regurgitation orifice resulted in a flow convergence region composed of several small isosurfaces. as the pisa method is based on the assumption that the isovelocity surfaces are hemispheric, questions can be raised about potential inaccuracies of the pisa method."
"the image-based cfd model presented in the previous section is applied to a database of twelve volunteers. accuracy and computational effort is shortly investigated. the capabilities of such methodology is then leveraged to study the hemodynamic variability of heart with diseased mitral valve. finally, results of those numerical simulations are used to assess the potential limitations of the pisa technique."
"the range axiom reads in a similar way, it has the same premise and concludes that y is an instance of class author r p . similarly to domain, we will use the symbol r p to denote the class that is the range of property p."
"in the crm definition, meta-properties are specified in the context of their parent properties. the definition indicates the parent property and the type, as follows: p has meta-property p.n: e55 type"
"the closure of a kb is a natural candidate to answer queries directly stated in datalog, using the predicate symbols employed for the reduction. however, it cannot be used to compute answers to general queries stated against a l c kb, due to the fact that query answering is defined in terms of logical implication and skolemization does not preserve logical implication. on the other hand, query answering cannot be reduced to satisfiability since the reduction would introduce existential quantifiers in the negation of universally quantifies queries. to address this problem, an investigation of a different implementation strategy is ongoing."
3 is the standard lagrange finite element space of continuous piecewise affine functions. the pressure space q h ⊂ l 2 (ω) is also made of piecewise affine functions which are globally continuous except across σ i . the resulting fully discrete method can therefore be written as:
"in this section, we re-write the c axioms in the tbox of a kb as equivalent dpcs, by removing negation and existential quantification, and by suitably re-writing the co-reference axioms; the last two transformations imply a loss in equivalence, but one can be afforded, as it will be argued."
"in spite of this practical limitation on querying, the present study offers a solid logical foundation to the crm ontology, which was the main goal of the paper. the crm community can use this foundation to build the logical inferential apparatus of the extensions to the basic cidoc crm ontology that are being proposed to address specific application domains."
"which reads: for each individual x, x is an a if and only if it is not a b. for every interpretation, the last axiom forces every individual in the domain of the interpretation to be either in the extension of a or in the extension of b. the weaker form, instead, allows individuals to be neither in the extension of a nor in the extension of b, while keeping these extensions disjoint. we note that the weaker form is also the interpretation of class disjointness in owl [cit] ."
"finally, logic provides a language for expressing constraints that go beyond the representation machinery of object-oriented modelling, and which may turn out to be useful in more refined definitions of the crm. logic also provides two useful notions, such as consistency and implication; consistency is important for the integrity of a crm knowledge base, while implication is important for extracting knowledge from a crm knowledge base."
"as already mentioned, a meta-property is a property whose domain is a property (hence the name): any instance of a property associates an instance of the domain property with a value which is always an e55 type. \"their purpose is to simulate a specialization of their parent property through the use of property subtypes declared as instances of e55 type 7 \"."
"1. the definition of each crm quantifier is reduced to an equivalent set of simpler statements. this is done in table 6, also given in the appendix. as a result of this step, we have that each quantification statement can be expressed in terms of two simpler statements, total property and functional property, that can be applied to the property or to its inverse. therefore a property or its inverse falls into one of the following cases:"
"the mathematical model of the valve, described in section 2.2, requires inclusion of the valve geometrical surfaces in open and/or closed configuration. the mitral valve was designed, both in open and closed configuration, using the computer aided design (cad) software salome v7.7.1 3 . carpentier's functional classification of mv diseases [cit] was used to design three different mv closed-state geometries (appendix appendix a) to allow the modeling of type 1, type 2 and type 3 mr. on the other hand, the closed aortic valve could be taken directly from the zygote 3d human heart model and the aortic valve in open-configuration was not included. the valve surfaces were inserted in the full geometrical domain, as illustrated on figure 1b."
"to apply the pisa technique, in-silico, the numerical simulations of the pathological cases were processed as follows. the isosurface tool of the ensight 4 software was used to generate 3d isosurfaces at different aliasing velocities ranging from 20 to 60 cm s −1 using increments of 10 cm s −1 . a frame of the sequence was chosen, during systole, in which the isovelocity surfaces were fully developed. the landmark l 1, representing the mr center, was defined as the spatial average of five to ten user-defined landmarks to account for some user variability. since the assumption of hemispherical isovelocities is only valid when the hole is round and the valve is flat, as reported in previous studies [cit], there is no unique distance between the mr center and the computed isosurface for the complex mr geometries such as the one presented in this study. therefore, to imitate the fact that the physician chooses a specific landmark l 2 on the isovelocity surface, the methodology described in the following paragraph is adopted. for each aliasing velocity, the distances between the mr center and the triangle centers of the discretized 3d isovelocity surface have been used to produce rv estimates using the pisa method. those different rv estimates were aggregated into weighted histograms where each estimated value was weighted by the area of its associated triangle to account for non-uniform discretization of the surface. each sample of this histogram can be thought as one possible choice of landmark l 2 by the physician."
"eleven synthetic pathological cases were designed to model various type 1, type 2 and type 3 mr. investigation of the hemodynamics in those cases revealed heterogeneous systolic blood flow resulting of the underlying pathology and mv geometry. even though the obtained blood velocity was physiologically coherent, the shortcomings of directly using the rt3de images as bc for the lv motion is that one strongly relies on the image quality and segmentation artifacts (e.g., non monotonous volume curves during systole or diastole)."
"in order to remove existential quantification from t c, we resort to skolemization, replacing each existential variable with a new constant, i.e., a constant that does not occur in the kb. technically, this amounts to introduce a set of new predicate symbols s i, t i, and v i for holding new constants, and use these symbols for replacing the axiom schemas of the third group in table 3 by the following schemas:"
"we now turn to the last example, in order to show how the rules in p prevent the occurrence of undesired negative knowledge in the kb. axiom rdom is instantiated on property p 12 as:"
"regarding the evaluation of the pisa method, good agreement between the estimated rv using pisa technique and the simulated value was found in the case of a circular hole on a flat mitral valve. however, the pisa method did not offer satisfying estimates for other types of regurgitation. it was also noticed that choosing higher aliasing velocities led to a wider range of potential rv estimates. those observations highlight the difficulties of properly quantifying mr in a clinical setting. in the presented work, the pisa estimates were obtained using the velocity field of the different numerical simulations. however, in the clinical setting, us doppler images only provide the component of the velocity field that is aligned with the us beams. moreover, additional measurements artifacts such as aliasing can perturb the us images. as such, using unaltered velocities coming from simulations might limit the scope of the findings. nevertheless, this choice allowed to remove the bias of having to choose a specific probe positioning and using unaltered velocities allowed to draw conclusion in a best case scenario."
"pso was introduced by kennedy and eberhart [cit] . the behaviour of pso can be envisioned by comparing it to bird swarms searching for optimal food sources, where the direction in which a bird moves is influenced by its current movement, the best food source it ever experienced, and the best food source any bird in the swarm ever experienced. in other words, birds are driven by their inertia, their personalknowledge, and the knowledge of the swarm. in terms of pso, the movement of a particle is influenced by its inertia, its personal best position, and the global best position. pso has multiple particles, and every particle consists of its current objective value, its position, its velocity, its personal best value, that is the best objective value the particle ever experienced, and its personal best position, that is the position at which the personal best value has been found. in addition, pso maintains the global best value, that is the best objective value any particle has ever experienced, and theglobal best position, that is the position at which the global best value has been found. classical pso uses the following iteration to move the particle:"
"the structure of the c axioms is very close to that of definite program clauses (dpcs, for short), which are sentences of the form [cit] :"
"it was shown that point trajectories are entirely determined by the initial control point positions q i (0) and by the time-varying momentas µ i (t) [cit] . it was also shown that such deformation belongs to the class of diffeomorphisms [cit] . the registration algorithm aims to find a set of momenta µ i (t) such that a similarity criterion is minimized. as such set may not be unique, the set which minimizes the deformation kinetic energy is chosen. using deformetrica, the transformation φ 2, parameterized by ("
3. the definition of each crm quantifier is captured as a set of axiom schemas by conjoining the capture of the simpler statements as shown in the previous author point. we show how it is done for two crm quantifiers:
"the crm is specified in a semantic data modelling style and relies on consolidated notions for the representation of knowledge such as classes, properties, isa hierachies, domain and range constraints and cardinality restrictions. these notions are considered sufficiently clear and unambiguous both in the documentation and in the standardization communities. the crm specification has been used several times for implementing the model, and some of these implementations support large knowledge bases, such as the one at the british museum. the status of the crm as a data definition language is therefore quite solid, from a practical point of view."
"as it turns out, crm-entities are not the only individuals in the domain of discourse of a crm kb. there are also individuals that lie outside the intended scope of the crm but are nevertheless related to the crm entities and as such they must be documented in a crm kb. examples of these individuals are numbers, temporal intervals, and geometric regions. in the crm, these individuals are called primitive values and are instances of the crm class e59 primitive value which is the only crm class that is not a sub-class of e1 crm entity. in this sense, primitive values make up the complement of the crm domain proper. however, since a kb needs to refer to primitive values, they are modelled in the crm as first-class objects, although no knowledge about primitive values is expected to be represented in a crm kb. indeed, no crm property has e59 primitive value as domain, while a few classes have it as range. in the present version of the crm, there are just a few subclasses of e59 primitive value, but these sub-classes may be expected to vary, in particular: the set of these classes will shrink as the crm is extended and some primitive values become citizens of the crm, or will enlarge as the crm is applied to new domains and new primitive values are needed."
"it is proposed to assess the accuracy and some of the limitations of the pisa method. we first shortly describe the usual pisa method. using 2d color doppler echocardiography, the physician choses a frame during systole in which the mr is visible and fully developed. he selects a first landmark l 1 at the mr center and then chooses an aliasing velocity, denoted v alias, usually ranging from 20 to 60 cm s −1 [cit] ."
"the resulting histogram is therefore a probabilistic evaluation of the rv, for which one assumes a uniform probability of picking any landmark l 2 on the isosurface. modes in this histogram represents the fact that a specific rv has a higher probability of being the one estimated by the physician given the shape of the isovelocity surface. the rv estimates of all aliasing velocities can also be aggregated into one single histogram. both possibilities (either one histogram per aliasing velocity or an aggregated histogram combining all aliasing velocities) was considered. a red vertical line depicting the ground truth rv that needs to be estimated (as reported in table 3 ) was added to the figures depicting the histograms to provide an indication on how successful the pisa method would be at assessing mr in each cases. all the histograms can be found in appendix c. representative examples are now detailed. figure 9 depicts the histogram combining all aliasing velocities for case c01 t1. while the rv estimates are a little bit spread, a strong mode is visible, corresponding roughly to the rv that needs to be estimated. this mode is representative of a nearly hemispherical isovelocity surface. as the case c01 t1 regurgitation results of a circular hole on a nearly flat valve, the pisa method therefore provides a reasonable rv estimate in this case. figure 10 depicts the histogram combining all aliasing velocities for case c07 t1. the histogram is dominated by a main mode but one can observe a strong tail of high rv estimates. this is a result of the hemiellipsoid isovelocity surfaces resulting a the slit hole. in this case, the ground truth rv does not overlap the main mode of the histogram revealing the difficulty for the physician to properly quantify mr using the pisa method. figure 11 depicts the histogram combining all aliasing velocities for case c05 t1. note that c05 t1 is composed of two small regurgitation and, therefore, defining the landmark l 1, the mr center, was difficult and had to be chosen among one of the two holes. for this reason, the resulting histogram is difficult to investigate. the histogram is quite uniform in terms of the representativeness of the estimates and one can barely distinguish two modes that might be the result of including the isosurfaces of both regurgitation at the same time. it is difficult to see a relationship between the actual rv and the provided rv estimates. figure 12a . however, one can observe that at specific aliasing velocity, this mode is difficult to observe (figure 12b) . inspection of the individual histograms associated with specific aliasing velocities (appendix c) reveals that at low aliasing velocities, the rv might be overestimated. at higher aliasing velocities, this overestimation is less and less pronounced. this result indicates that in some cases, it might not be sufficient enough to use a single aliasing velocity to estimate the mr severity. finally, figure 13 depicts the histogram combining all aliasing velocities for case c09 t3. conclusion similar to the type 1 mr with slit holes can be reached: due to the elongated hole resulting of the anterior leaflet restriction, the histogram contains a main mode with a long tail of high rv estimates. moreover, the ground truth does not overlap the main mode of the histogram. looking at the spread of the rv estimates across the range of aliasing velocities reveals that lower aliasing velocity leads to a lower spread of the potential rv estimates case-wise. as an example, figure 14 depicts the histograms of the case c01 t1 placed side by side as a color-coded image. while the rv estimates range from 5 ml to 50 ml at an aliasing velocity of 20 cm s −1, the rv estimates produced using an aliasing velocity of 60 cm s −1 range from 5 ml to 100 ml. even though this observation advocates for the use of a small aliasing velocity, one needs to remember that isovelocity surfaces resulting from small aliasing velocities might be disturbed by the aortic outflow. it is therefore important to carefully choose adequate aliasing velocity."
"the term optimization refers to the study of problems in which one seeks to minimize or maximize a real function by systematically choosing the values of real or integer variables from within an allowed set. on one hand, a vast amount of research has been conducted in this area of knowledge with the hope of inventing an effective and efficient optimization algorithm. on the other hand, the application ofexisting algorithms to real projects has also been the focus of much research. the most commonly used optimization technique known as evolutionary computation (ec) [cit] . broadly speaking, ec constitutes a generic population-based metaheuristicoptimization algorithm. evolutionary algorithms tend to perform well with regard to most optimization problems. this is the case because they refrain from simplifying or making assumptions about the original form. as a newly developed subset of ec, the particle swarm optimization hasdemonstrated its many advantages and robust nature in recent decades. it is derived from the social behaviour of bird flocks in particular. inspired by the swarm intelligence theory,kennedy created a model which eberhart then extended to formulate the practical optimization methodknown as particle swarm optimization (pso) [cit] . the algorithm behind pso is based on the idea that individuals are able to evolve by exchanging information with their neighbours through social interaction. this is known as cognitive ability. three features impact on the evolution of the individual: inertia (velocities cannot be changed abruptly), the individual itself (the individual could go back to the best solution found so far) and social influences (the individual could imitate the best solution found in its neighbour). pso has a very common problem of stagnation and premature convergence, especiallyin multi modal function [cit] . a lot ofresearch work have been done to improve the convergence behaviour of pso. with this motivation we have tried to invent a new variant of particle swarm optimization using the concept of guaranteed convergence particle swarm optimization (gcpso) [cit] which yields much better result than gcpso.there is still a problem in gcpso [cit], however, in that particles tend to converge to a local minimizer before encountering a true global minimizer due to lack of exploration. addressing this problem, van den bergh [cit] developed multi-start pso (mpso) whichautomatically triggers a restart when stagnation is detected. various criteria for detecting premature convergence were tested in order to avoid the undesirable state of stagnation. it was thought that restarting on the original search space might cause unnecessarily repetitious searching of regions not expected to contain quality solutions. gcpso might even allow the swarm to escape local optima ifparameters were designed with exploratory intentions, but this approach would effectivelyleave the rest of the swarm trailing almost linearly behind the globally best particles random movements, which would not be ideal. so a mechanism known to be regrouping pso(regpso) [cit] became desirable by which the swarm could efficiently regroup in a region small enough to avoid unnecessarily redundant search, yet large enough to escape wells containing local minima in order to try to prevent stagnation while retaining memory of only one global best rather than a history ofthe best of them. consequently, there is one continuous search with each groupingmaking use of previous information rather than a series of independent searches.it produced better results than it's previous versions but this approach also lacks the exploration ability within the particles so to deal with this problem we propose an approach called guaranteed convergence pso(pgcpso) which could increase the exploration ability of particles while keeping the convergence rate fast and keeping a common approach of escaping local minima. a roadmap of this paper is as follows. related work is discussed in section 2. a novel variant of gcpsocalled pgcpso is presented in secton 3. experimental results from benchmark functions is discussed in section 4. section 5 covers the conclusion."
"the paper is structured as follows: we will start by examining the expressive requirements of the crm (section 2). based on the results of this examination, we will define the first-order language l c for the logical expression of the crm (section 3). next, we will introduce the axioms that capture the crm ontology (section 4). we will then define the notion of knowledge base and query, thus concluding the logical formulation of the crm (section 5). the remaining part of the paper (section 6) is devoted to design a datalogbased implementation of a knowledge base. section 7 concludes."
"secondly, there are sentences representing the referential relationships between the constants and the standard names, as discussed in the last part of section 2.2. we call these sentences co-reference literals, and they come in two sorts:"
we therefore introduce a countably infinite set d c of standard names that are in one-to-one correspondence with the individuals in the domain of discourse of the crm. d c will play both a linguistic and a semantic role.
"the semantics of the language defines in a mathematical way the form of reality, that is the universe of discourse of the language. the mathematical notion of reality is a world state, traditionally known as interpretation. we prefer to use the former term, to keep with [cit] . moreover, semantics provides rules for establishing the truth or falsity of the sentences of the language in a given world state."
"firstly, there are sentences representing the instantiation of classes and properties, as discussed in section 2.1. in order to obtain a greater expressivity, we model also negative instantiation, asserting negative knowledge, such as that an individual is not an instance of a class. we call these sentences instantiation literals, as they are positive or negated ground atoms of l c, in which both standard names and constants may occur, for the reasons spelled out in section 2.2. it turns out that in the scholarly world, which is one of the domains addressed by the crm, negative knowledge is as important as positive knowledge, if not more."
"now that we have a language and a semantics, we can encode the crm ontology as axioms of the language, and then use the inference relation just defined to establish basic properties of the crm ontology."
"where the first two axioms capture totality and functionality of p, respectively, and the last two axioms do the same for the inverse of p."
"based on the last proposition, we introduce a function · + that maps every world state w of l c into its unique extension w + as defined by the previous proposition."
"however, the crm still lacks a formal specification of its semantical and inferential apparatus. this lack makes it difficult to clearly define fundamental operations on a crm knowledge base, such as querying or consistency checking, while preventing any investigation on the computational properties of the language."
"where the last term stands for the supg/pspg stabilization (see, e.g., [cit] ). remark: in (5), the fluid integrals in the deformed configurations has to be evaluated by composition with the corresponding discrete ale map. for example, for the the second term we have"
the ale formulation was considered to alleviate the difficulties arising from the large deformation of the cardiac cavities. an arbitrary mapping a is defined to map the reference domain ω into the current spatial domain ω(t) at each time instant.
"we will exemplify the last observation by considering a scholar who is developing a kb k establishing whether or not dante alighieri (whom he denotes by the standard name d) was present at the event (standard name b) of the birth of francesco petrarca. this piece of knowledge can be represented in the crm by using property p12 occurred in the presence of (was present at), linking an event (instance of e5) to a persistent item (instance of e77) that was present at the event. so, the domain of p 12 is e5 and its range is e77. further, we assume that the kb is powered by a sound and complete inference engine. now, our scholar knows that d is an instance of class e21 person, while b is an instance of class e63 beginning of existence, and he records this knowledge by entering the positive instantiation atoms e21(d) and e63(b) in the abox of k. next, our scholar finds enough evidence that dante was not present at the birth of francesco petrarca and so he inserts into the k's abox the negated atom ¬p 12(b, d) which reads \"the birth of petrarca did not occur in the presence of dante\", or alternatively, \"dante was not present at the birth of petrarca\". now the scholar checks k out and finds that it contains the three assertions that he has inserted, but in addition it contains also the assertion ¬p 12(d, b). this sentence reads \"dante did not occur in the presence of the birth of petrarca\" and is hardly of any use, in fact it does not even read correctly. so the scholar wonders how came that such a useless, ungrammatical piece of knowledge ended up in his precious kb."
"an approach for dealing with the stagnation problem in pso has been tested by building into the algorithm a mechanism to automatically avoid premature convergence. the gcpso mechanism helps liberate particles from the state of premature convergence and enables continued progress toward a global minimizer. pgcpso has been shown to have better mean performance than the algorithms compared with a result that would have been more pronounced had only multi-modal bench-marks been used. pgcpso also consistently outperformed in the presence of noise. given sufficient function evaluations, pgcpso was able to solve the stagnation problem for each benchmark tested and approximate the true global minimizer with each trial conducted. though the parameters used for pgcpso worked consistently across the benchmark suite, it is not claimed that parameters have been fully optimized and it is not claimed that particles will not converge prematurely or itis absolutely free from stagnation. while pgcpso seems capable of reducing the problem-dependency usually seen in the standard pso algorithms so that parameteroptimization may be less important. pgcpso appears to be a good general purpose optimizer based on the benchmarks tested, which is certainly encouraging; however, it is cautioned that the empirical nature of the experiment is not a theoretical proof that pgcpso will solve every problem well certainly, its performance must suffer somewhere. future workwill try to understand where the algorithm suffers in order to understand any limitations and apply it to the proper contexts. this would allow eventual solution refinement of greater precision rather than repeatedly cutting of the local search in favor of exploration elsewhere. it has been empirically observed that clamping velocities to fifteen percent of the range of the search space on each dimension often provides a quicker convergence to solutions of higher quality in conjunction with standard pso. pgcpso using standard gbest pso as its core, however, appearsto benefit from larger velocities such as those clamped tofiftypercent of the range on each dimension. the larger maximumvelocity facilitates exploration and moresignificant momentum by which to resist repeated premature convergence to the remembered global best. pgcpso seems to improve performance consistency with one set of parameters by facilitating escape from potentially deceitful local wells and to solve simple, unimodal problems free of entrapping wells quite well. it is suspected that pgcpso may provide a degree of scalability previously missing in the standard pso algorithm."
"ontological knowledge is expressed in semantic data models by means of various kinds of constraints, such as isa hierarchies. we will examine in detail the crm constraints in section 4. factual knowledge, on the other hand, is expressed in semantic data bases by means of instantiation. in particular,"
"focusing on intracardiac hemodynamics, several steps are usually required to personalize 3d cfd models based on patient-specific images. the cardiac cavities are first segmented in order to extract the cardiac geometry and dynamics. while this step is nowadays mainly automated, thanks to robust image processing algorithms, the resulting segmented surfaces usually need to go through a serie of operations including mesh cleaning, valve insertion, volumetric mesh generation, surface labeling, inlet and outlet extrusions and refinement of region of interest. finally, the cardiac dynamics also need to be extracted before being used as boundary condition of the numerical simulations [cit] . recent review of patient-specific cardiac flow simulations report that those pre-processing steps are time-consuming, with an order of magnitude of several hours [cit] ."
"this pipeline was applied on twelve different rt3de image sequences. even though the rt3de sequences exhibited a strong variability in shape and dynamics of the ventricle, the combination of an affine registration step followed with a deformable registration step allowed to produce patient-specific ventricular surfaces in all cases. moreover, as specific constraints on the registration were specified, the resulting meshes were all suited for numerical simulations (i.e. no mesh defect were observed in any of the cases). the transfer of the lv dynamics was also successful in providing a smooth velocity field of the deformed generic ventricular surface. regarding the computational effort, the fully automated pre-processing step averaged 55 minutes for the tested database. this is a huge progress compared to the methodology usually reported in the literature. indeed, we recall here that recent reviews of patient-specific cardiac flow simulations claimed that the pre-processing step usually accounted for more than 20 hours of human effort [cit] ."
"the goal of the proposed gcpso using personal best (gcpso) is to not to limit the exploration ability to the few particles but to provide the opportunity to the other particles in the swarm so that they can escape from the local minimum. inpgcpso an additional particle(φ) similar to gcpso is introduced which searches the region around the current personal best position, in that manner, the current update formula for this particle is seen below:"
"first of all, a kb must include the axioms that capture the logical relationships between the terms of the l c vocabulary. these axioms are derived from the axiom schemas introduced in the previous section and recapitulated in table 3, by replacing the predicate symbols with actual predicate symbols in l c, as described in section 4. for instance, to capture that the crm class e5 event is a sub-class of e4 period, we instantiate the subc axiom scheme and obtain the c axiom:"
"from the application of one of the rules having the sentence in their heads, i.e., either rule cleq or an instance of one of clun, clbin, clter. in this case a common instance is shared by a predicate symbol and its complement, an obvious inconsistency;"
proposition 1: for each model w of t c there exists a unique extension of w that is a model of t c . proof: let w be as follows:
"(c) the property is neither total, i.e., some domain elements can miss it, nor functional, i.e., more than one value can be provided for any element of its domain;"
"the last requirement concerning individuals is the necessity of representing knowledge about individuals whose identity is presently uncertain, in the sense that it is not known whether these individuals have already been assigned a standard name in the current kb and which standard name that would be. this uncertainty may be resolved at a later time, either by discovering the standard name that is used for these individuals, or by ascertaining that no standard name has yet been assigned to them. but it is required that the kb be able to hold knowledge about these individuals until their identity is cleared and a standard name is available for them. as discussed above, first-order logic offers constant symbols for naming individuals whose identity may vary from interpretation to interpretation, therefore we will also include constant symbols in our language."
"for brevity, p c is not reported. however, the users of a crm kb are also interested in the implicit negative knowledge, and it is not difficult to see that p c is not sufficient to capture all such knowledge. to exemplify, let us consider a kb including rule (7) in its tbox, and the negated instantiation atom ¬e4(n 2 ) in its abox. these pieces of knowledge imply ¬e5(n 2 ). a sound and complete first-order inference system, such as one based on resolution [cit] would indeed derive ¬e5(n 2 ). but there is no way to obtain ¬e5(n 2 ) (or its corresponding complement e5(n 2 )) from p c . this is not surprising, since datalog aims at deriving positive atoms that can be seen as elements of the interpretation of a program."
"since disjointness axioms propagate down the isa hierarchy, these two axioms sanction the disjointness of many classes. but of course we do not need to state any of the corresponding axioms, because they logically follow (in the sense defined in section 3.2) from the instances of (1) and the last two axioms."
"the doubts of the scholar are amply justified. in fact, ¬p 12(x, y) is true of all the x that are not events, or of all the y that are not persistent items (as b in the last example), or both. and the same applies to every other property. in other words, if the abox of a kb contains all negated atoms that follow from the explicit knowledge, we may end up with a very large set of irrelevant facts. the semantics of negation makes this unpleasant fact unavoidable. however, in our language we do not use negation directly, but we simulate it author through complements. this gives us the possibility of avoiding undesired negative knowlede in our kb."
"p best is the best value of an individual particle and g best is the global best position and c 1 and c 2 are the constant with value which is equal to 2 whereas r 1 and r 2 are random variables.pso can focus on either convergence or diversity at any iteration. to focus on diversity means particles are scattered, searching a large area coarsely. to focus on convergence means particles are close to each other, searching a small area intensively. a promising strategy is to focus on diversity in early iterations and convergence in later iterations. pso has two major drawbacks. the first drawback of pso is its premature character, i.e. it could converge to local minimum. although pso converges to an optimum much faster than other evolutionary algorithms, it usually cannot improvethe quality of the solutions as the number of iterations is increased. pso usually suffers from premature convergence when high multi-modal problems are being optimized. the main reason is that for the standard pso particles converge to a single point which is on the line connecting the global best and the personal best positions. nevertheless this point is not guaranteed to be a local optimum and may be calledequilibrium point. the second drawback is that the pso has a problem-dependent performance. this dependency is usually caused by the way parameters are set, i.e. assigning different parameter settings to pso will result in high performance variance. after the pso was issued, several considerations has been taken into account to facilitate the convergence and prevent an \"explosion\" of the swarm. these considerations focus on limiting the maximum velocity, selecting acceleration constants, constriction factor etc."
"all these aspects of a property are expressed as shown in table 1 . to illustrate, the domain axiom schema reads: for each pair of individuals x and y, if x and y are linked by p, then x is an instance of class d p . throughout the paper, we will use the symbol d p to denote the class that is the domain of property p."
"the crm specification includes constraints on the cardinality of properties, named quantification statements, or simply crm quantifier. for selfcontainedness, the definitions of the crm quantifiers are reported in appendix, in table 5 8 ."
"notice that the standard names include all appellations one may want to use in a crm kb. this does not pose any problem, since d c is a countably infinite set, and can therefore accommodate any finite number of countably infinite subsets. from a more theoretical point of view, some crm classes, such as e60 number, have an extension that has the cardinality of real numbers, so a countable domain might seem indaquate to account for such classes. but, as it is well known 5, any consistent theory with an infinite domain has a model at each infinite cardinality, therefore the restriction to a countable domain is not a limitation even from a theoretical point of view."
"which we have already seen previously. for each subclass constraint in the crm specification, an axiom of the form (1) goes into t c . for this reason, a sentence like (1) is called an axiom schema. for simplicity, from now on we will omit the universal quantifiers in sentences and tacitly understand free variables as universally quantified."
"in search for an explation, he further inspects the kb and finds out that the atom ¬e2(d) is also in the abox. the scholar is able to explain this fact: since e21 is a subclass of e77 which is disjoint from e2, any instance of e21 is not an instance of e2, therefore ¬e2(d) is implicit in the abox, and the inference engine of the kb correctly derived it. but then, the scholar reckons, if d is not an event then d is not in the domain of p 12, hence it cannot be true that p 12(d, b) (no matter what b is) and therefore its negation ¬p 12(d, b) is true. again, soundness and completeness of the underlying inference engine explain also the presence of this piece of knowledge in the kb. the scholar is now very satisfied to have found the explanation, but then he wonders whether the inference engine of his kb is really useful."
"is the same name as w(t 2 ). we use the same method as in the previous point to reduce each argument in a co-reference statement to the corresponding standard name. then, the coreference statement is true just in case the resulting standard names are the same."
"a weak shortcut is clearly expressible as an owl 2 dl complex role inclusion axiom [cit] . however, we notice that the object property expression that occurs in the head of each such axiom (termed superobjectpropertyexpression) is composite and as such it is subject to several restrictions. namely, it cannot occur in the following axiom types: objectmincardinality, objectmaxcardinality, objectexactcardinality, objecthasself, functionalobjectproperty, inversefunction-alobjectproperty, irreflexiveobjectproperty, asymmetricobjectproperty, and disjointobjectproperties. moreover, the property expressions that occur in the body of each such axiom (termed subobjectpropertyexpression) are also subject to restrictions related to the property hierarchy. needless to say, the datalog formalization does not suffer from these limitations."
"in this paper, it was shown that the valve shape has strong impact on the blood flow. therefore, dynamic changes of the valve geometry either during closing of the valve or during systole might introduce additional hemodynamic variability. while the presented work included dynamic motion of the ventricle walls, the mv dynamics was not considered and this might be an oversimplification of the model in some cases. in particular, it is known that the regurgitation and mv dynamics can be highly dynamic in mr caused by mitral prolapse [cit] . additional work should be pursued to further characterize the influence of valve dynamics."
"even though a patient-specific geometry can be obtained using the previously computed map φ, the lv dynamics from the rt3de sequence cannot be directly applied as simulation boundary conditions for several reasons. first, the time-resolution of the rt3de sequence depends on several physical factors and is therefore limited. the cfd simulations usually requires a time resolution (≈ 1000 hz) that is two orders of magnitude higher than the usual rt3de time resolution (≈ 30 hz). as a result, temporal interpolation of the dynamics is required. moreover, as the deformable registration is a result of minimizing a global energy functional, the two registered ventricular surfaces are not ensured to be exactly matching. this nonconformity of the two surfaces is accentuated by the fact that the resolution of the two surface mesh might also differ. therefore, a spatial interpolation strategy is also required."
"the fluid is initially considered at rest (6) . on the solid surfaces, it is assumed that the fluid has zero velocity relative to the solid boundary ∂ω s (7) (i.e. no-slip bc). at the inlet (pulmonary vein cross-sections), a normal stress is applied to impose a constant static pressure p pv (8) . finally, a normal stress was also applied at the outlet of the domain (9) . as it was shown that proper choice of outlet boundary condition is critical to obtain realistic physiological behavior [cit], a windkessel rcr lumped parameter model was therefore chosen to represent the afterload of the heart related to the arterial system (10) . a backflow stabilization based on a local regularization of the fluid velocity is applied at the aortic outlet [cit], eventually modifying (9) . in (10), the proximal resistance r p, distal resistance r d, capacitance c and distal pressure p d (t) are parameters of the model that must be chosen appropriately."
"in this paper, the use of a pipeline to automatically prepare personalized numerical simulations based on rt3de image sequences was investigated. this pipeline combines of rigid and deformable registration to morph predefined generic meshes to fit patient-specific lv geometries. spatio-temporal interpolation of the us data produces appropriate lv surface dynamics on the vertices of the deformed generic meshes."
"1. t c, the tbox of k, includes the crm axioms, obtained by instantiating the axiom schemas introduced in the previous section; 2. a, the abox of k, is a finite, possibly empty set of instantiation and co-reference literals, as discussed above."
"in order to derive the axioms of our crm theory, in the next two sections we will examine the definitions of the classes and of the properties of the crm, capturing (the formalizable part of) these definitions into logical form."
"the sentence reads: for each individual x, if x is an a, then it is also a b. to exemplify, the constraint that e70 thing is a sub-class of e77 persistent item is captured by the sentence"
"in general, a shortcut can be seen as a pair (p, s) where p is a property, called the shortcut property, and s is a sequence of properties, called the shortcut path. the shortcut property of the top shortcut in table 2 is p2, while its path is ((p41,p42). the crm specification of a shortcut includes also the class where the shortcut path originates and the classes met along the path. we ignore these classes as they can be derived as the domains and ranges of the properties on the shortcut path."
"first, both rdf and owl have a syntax that is closer to that of implementation language rather than to a specification language; for instance, they use international resource identifiers as non-logical symbols, which are hard to read and unnecessary for our purposes. second, and more important, our study will show that none of these languages has the expressive power required by crm: rdf schema does not allow to capture property quantification; strong shortcuts are not expressible in owl 2 dl, while weak shortcuts can be expressed in owl 2 dl but with several limitations, as explained in section 4.2.2. third, it turns out that the treatment of individuals at the basis of the crm cannot be captured by standard first-order logic, and consequently by description logics, which are contractions of standard first-order logic."
"the intended scope of the crm is 2 \"all information required for the exchange and integration of heterogeneous scientific documentation of museum collections\", where \"the documentation of collections includes the detailed description of individual items within collections, groups of items and collections as a whole\". this amounts to say that in a crm kb we expect to find statements about individual items within museum collections, other groups of such items and collections themselves. we will refer to these individuals as crm-entities. amongst the crm-entities, a very important role is played by appellations. appellations are the names that crm-entities are called, or have been called in their context of existence, and are included in a crm kb because the use and assignment of appellations are historical facts of great relevance. indeed, the provenance of museum objects can be verified by tracing appellation use without reference to any other feature. the registration of all previous identifiers of a museum object is a requirement of the cidoc 3 documentation guidelines. this requirement is common to other domains: the union list of artist names is a resource by the getty registering all names ever in use for artists; similarly, the getty thesaurus of geographic names registers all names ever in use for places. since museum objects, artists and places are all author in the crm scope, appellations have been introduced to document the names these resource have had at any time. technically, appellations are instances of the crm class e41 appellation, which has several sub-classes, each capturing a specific name type (e.g., place, time or agent appellations, titles, and others). an appellation is therefore a crm-entity of a special kind: it is a piece of language, entirely identified by its lexical form. this feature of appellations will be taken into account in the development of the terms of l c ."
"in this section, we examine the basic principles of the crm in order to obtain a clear understanding of its expressive requirements. we start by outlining a general correspondence between semantic data models and logical languages. subsequently, we will consider the specific features of the crm. as a notational convention, crm terms are written in sans serif, e.g. e1 crm entity, while first-order symbols are written in italics, e.g. e1 crm entity."
"in sum, the individuals in the domain of the crm are: crm-entities, which include appellations, and primitive values. as pointed out in the previous section, when applied to a knowledge base, the crm models these individuals as objects, identified by object identifiers. (if the crm is taken as an ontology in the philosophical sense, instances of classes are the individuals of the domain themselves, such as any person). we note that the crm object identifiers have the following features:"
it suffices to inspect the heads of the rules in p c to see that no atom can be in a other than the foregoing ones.
"as the two lv surfaces exhibit positioning differences as well as local surface differences, a two-step strategy is proposed to handle this registration process. first, an initial affine transformation φ 1 will be used to register the main anatomical parts of the generic ventricle to the patient-specific surface. a deformable transformation φ 2 will then be used to deform this coarsely registered generic surface to fit patient-specific surface. the goal of this two-phase strategy is to use as much a priori information as possible in the first phase to simplify the requirements of the deformable registration phase, usually more computationally intensive."
"the logical formalization is expected to bring several advantages to the crm, from a scientific point of view. in particular, the first-order logic expression of the model may serve as a better communication medium with other researchers, allowing for a better understanding of the crm that can serve several purposes:"
"disjointness constraints capture incompatibility between classes or properties, making inconsistent any kb in which such classes or properties share a common instance. the crm makes the following class disjointness statements 6 :"
"2. replace any instance of the ametap axiom schema p.n(x, y, z) ⊃ ¬p.n(y, x, z) in the tbox of the kb by the corresponding instance of the schema: p.n(x, y, z) ⊃ p.n(y, x, z)"
"the axioms in the set t c resulting from the transformations seen so far, are dpcs that can be expressed as datalog rules forming a datalog program that we call p c . the middle column of table 4 presents the rule schemas from which p c is derived, in the same way the actual axioms of c are derived from the axioms schemas in table 3 . in order to highlight the correspondence between the rule schemas of tables 3 and those in the second column of table 4, in the latter table we have used the same rule schema names as in the former, showed in the left column. for instance, rule scheme subc gives raise to the actual p c rule:"
"it is important for a trust management scheme to select proper trust factors to evaluate the trustworthiness of a sn. from the literature on this topic, we can find that the trust factors of a sn is mainly based on the nodal communication behavior, energy level, or recommendation from the third party and there is no unified standard in the selection of the trust factors. the attacks initiated at each protocol layer and their influence on the parameters of the corresponding protocol layers lack comprehensive analysis. to the best of our knowledge, there is still no trust management scheme which elaborately describes the trustworthiness of a sn from the standpoint of protocol layer. thus, it is interesting to build the trust evaluation model based on the protocol layer trust. in view of the reality of intrusion detection scheme, we mainly consider the direct trust of a node in our scheme and the trustworthiness of a node is evaluated according to its behaviors at different protocol layers. the consideration of trust worthiness from the viewpoint of multiple protocol layers distinguishes this paper from the previous related works [cit] . since the deviations of the key parameters of multiple layers are used to evaluate the trustworthiness of a node, it is helpful for our scheme to detect nodal malicious behaviors initiated from different protocol layers, which is effective for detecting cross-layer attacks."
"purposefully creating collisions, obtain unfair priority in the contention for the channel or dissipate the limited energy of nodes. attacks at the mac layer include collision, denial of sleep, guaranteed time slot (gts) attack, back-off manipulation and so on [cit] . attacks at the network layer aim to disrupt the network routing, and acquire or control the data flows. examples are spoofed routing information, selective packet forwarding, sinkhole, wormhole, blackhole, sybil, and hello flood attack [cit] . besides the attacks aiming at a single protocol layer, there are cross-layer attacks which relate to multiple layers in wsns [cit] . cross-layer attack can achieve better attack effects, better conceal the attack behavior or reduce the cost of attack compared to the attacks at a single layer."
"where m denotes the number of route update packets that the monitoring node has received during the time period of ∆t. therefore, the lqi trust that node i evaluates toward node j can be described as:"
"the values of the weights w 1, w 2, and w 3 are determined according to the concrete requirement of a detection system under implementation. generally speaking, the number of attacks aiming at the network layer is greater than those aiming at the mac layer and physical layer. hence, the value of w 3 is usually slightly larger than that of w 1 or w 2 . in order to evaluate the trustworthiness of each protocol layer, we can choose some important parameters at each protocol layer and calculate the deviations of these parameters. actually, our scheme is scalable. if a more accurate trust value is needed, we can choose additional parameters at each protocol layer and calculate the deviations of these parameters. certainly, the more parameters are selected, the more complex the detection system will be. hence, we can select parameters according to the requirement and complexity of the detection system."
"wireless sensor networks are vulnerable to variety of attacks at different protocol layers. in the existing trust-based intrusion detection schemes, there is no unified standard to select trust factors, and cross-layer attacks are seldom considered. in order to identify malicious nodes more efficiently, we have proposed a protocol layer trust-based intrusion detection scheme for wsns. in our scheme, the key parameters of different protocol layers are monitored and the trust values of sensor nodes can be calculated according to the deviations of parameters. by comparing the trust value with a predefined threshold, we can decide whether the sensor node is compromised or not. it can describe the trust values of sensor nodes more accurately by considering the deviations of parameters of multiple layers, hence our proposed scheme is effective for detecting cross-layer attacks. we utilized the t-distribution and simulation to analyze the detection probability and false positive probability of our scheme. the results indicate that there exists an optimal trust threshold at which both false positive and false negative probabilities are minimized. our proposed scheme outperforms the nbbte scheme in terms of the detection probability and false positive probability. the weakness of our scheme is the communication overhead will increase with the increasing of the hop count to the ch. our scheme is extendable, the selection of the trust factors at different protocol layers can be adjusted according to the requirements of a system, and it is applicable to both clustered wsns and flat wsns. as for future works, we will analyze the attacks initiated at the transport layer and application layer, as well as mac-transport cross-layer attack, network-application cross-layer attack and their influence on the protocol parameters to further optimize our scheme. in addition, we will perform experiments to test the performance of our scheme on a real wsn testbed to assess its real-life performance."
"as described above, if the number of average neighboring nodes in a network is relatively large, the communication overhead of the nbbte is greater than that of our scheme. if the hop count to the ch is relatively large, the communication overhead of our scheme is greater than that of the nbbte. as a result, our scheme is more applicable to the network with less hop count. moreover, from the angle of computation complexity, the calculation of trust value and the decision approach in the nbbte are more complex than those in our scheme. figure 12 shows the comparison of the communication overhead of the two schemes under different average of hop count in the case that the average number of neighboring nodes in the network is 3. the communication overhead of our scheme will increase with increasing of the average hop count to the ch. if the average hop count to the ch is greater than 7, the communication overhead of our scheme will be greater than that of the nbbte. as described above, if the number of average neighboring nodes in a network is relatively large, the communication overhead of the nbbte is greater than that of our scheme. if the hop count to the ch is relatively large, the communication overhead of our scheme is greater than that of the nbbte. as a result, our scheme is more applicable to the network with less hop count. moreover, from the angle of computation complexity, the calculation of trust value and the decision approach in the nbbte are more complex than those in our scheme."
"this means if hop_count j is less than the average hop count, the more deviation there is, and the lower the trust value will be. in order to obtain the packet forward trust, the monitoring node i can obtain the packet forwarding rate of the monitored node j by:"
"the nodal trustworthiness consists of the trust degree of each protocol layer, including physical layer, mac layer, network layer, transport layer and application layer. since most of the attacks against wsns aim at the physical layer, mac layer and network layer, for simplicity, in this paper we mainly focus on the trusts at these three layers. let t direct ij (t) denote the trust value that the sensor node i directly evaluates toward its neighboring node j at time t. it can be calculated by:"
"where n denotes the number of neighboring nodes of node j and makes decision by comparing the trust value with th trust . if t cj (t) is less than th trust, then node j is regarded as compromised. the method of bs-to-ch trust evaluation is similar to that of ch-to-sn trust evaluation."
"there are different route metrics for routing protocols in wsns. for example, in the mintroute protocol, it uses link estimates as routing metric and includes the lqi within its route update packet [cit] . in the tinyaodv (tiny ad-hoc on-demand vector) protocol, the routing metric is the number of hop count and includes the hop count in route reply (rrep) packet [cit] . a malicious node can make its neighbors change their current parents and choose it as their new one by advertising an attractive lqi for itself in the route update packet or giving a small value of hop count in rrep packet. we then take lqi and hop count as basis to calculate the route metric trust."
"trust management is an effective method to identify malicious, selfish or compromised nodes. in recent years, research on trust management and its application to intrusion detection has received considerable attention from researchers. the current trust evaluation schemes aim to improve the detection performance, resource efficiency, robustness etc., by using fuzzy theory, probability theory and statistics, weighting method, etc. [cit] ."
"where t c cj (t) (s c j (t)) is the mean value (standard deviation) under the condition that node j is compromised, superscript c denotes compromised."
"energy consumption rate is an important parameter at the physical layer. a malicious node usually sends or receives more packets than a normal node. it will inevitably consume more node energy, so we choose energy consumption as trust metric at this layer. the monitoring node i can obtain the energy consumption of its neighboring node j during the time period of ∆t. the relative deviation of energy consumption of node j can be calculated by:"
"the rest of this paper is organized as follows. section 2 surveys existing work on trust-based intrusion detection in wsns. section 3 describes our intrusion detection scheme. section 4 analyzes the performance of our scheme by using analytical and simulation approaches, and compares its performance results with those of an existing scheme in the literature. section 5 concludes the paper."
"indicates the residual energy of node j at time t and ∆e j (t) represents the energy consumption of node j during the time period of ∆t. ∆e(t) is the average energy consumption level of all neighboring nodes of node i during this time period and n denotes the number of neighboring nodes of node i. node i can roughly evaluate the energy consumption of its neighboring nodes during the time period of ∆t by monitoring their packet transmission activities. the greater the deviation of energy consumption is, the lower the nodal trustworthiness will be. so we obtain the physical layer trust as:"
where u denotes the number of successful transmissions by the ch during the observation period of ∆t. we then calculate the deviation of the idle time:
"2 is the standard deviation of the trust value that node i evaluated with respect to node j, and n is the number of neighboring nodes of node j. we can obtain µ j (t) by running simulations for many times. thus, according to equation (20), the probability that node j is evaluated as a compromised node is given by:"
"considering the limited resources of the sns, it is not realistic for wsns to implement high-strength security mechanisms. furthermore, the attacker may have the ability of breaking through or bypassing the protection of security mechanism with the progress of attack technologies. thus serving as a second wall, intrusion detection plays an important role in protecting the network. the intrusion detection system for wsns can detect whether there are behaviors violating the security policy and record evidence of being attacked by collecting and analyzing the information from sensor nodes and networks. it can send alarm timely to the system administrator and perform some countermeasures against the attack."
"the purpose of the analysis is to derive mathematical results of the false positive and false negative probabilities. the false positive probability is the probability that a node is evaluated as compromised whereas it is not. on the other hand, false negative probability is the probability that a node is evaluated as not compromised whereas it is. the expressions for the false positive and false negative probabilities are derived using a statistical approach. we also calculate the communication overhead of our scheme."
"next we calculate the mac layer trust. there are variety of attacks initiated at this layer whose main objective is to get the priority of channel access. a malicious node can select a small back-off time, choose a small size of contention window (cw), or wait for shorter interval than distributed inter-frame spacing (difs), aiming to gain significant advantage in the contention of channel over the unmalicious nodes. therefore, the interval time between two consecutive successful transmissions of malicious node, which we define as idle time, will be less than that of the unmalicious node. the malicious node can also scramble the frames sent by other nodes in order to obtain the priority of channel access. as a result, the average number of retransmissions of the malicious node will be less than that of the unmalicious node. as described above, we choose two important parameters, the idle time and number of retransmissions, as the trust metrics at the mac layer. thus, at the mac layer, the node i evaluates the trust value of node j as: (t), the monitoring node i can obtain the idle time x k (k means the k-th transmission of the monitored node) according to request to send (rts)/clear to send (cts) access in distributed coordination function (dcf) mode, and x k can be calculated by:"
"in our scheme, each sensor node monitors the key parameters of its neighboring nodes at each protocol layer and transmits the trust values toward the monitored nodes to its ch, so the communication overhead of our scheme mainly comes from the packets transmitted from sns to the ch. as a result, the communication overhead of our scheme is related to the hop count from sns to the ch. in nbbte, it includes direct evaluation and indirect evaluation. in the direct evaluation, the monitoring node collects the key parameters of the monitored node and calculates the trust factors of the corresponding parameters. there is a factor of availability which evaluates the availability of the neighboring nodes. to obtain this factor, the monitoring node needs to transmit a hello packet and its neighboring nodes should reply to it with ack-hello packets. in the indirect evaluation, the neighboring nodes of the monitored node will transmit their trust evaluation results towards the monitored node to the monitoring node as the indirect recommendation values. thus, the communication overhead of nbbte includes two parts, the hello packets for the factor of availability and the indirect trust evaluation from the recommendation nodes, which are related to the average number of neighboring nodes in the network."
where lqi max equals to 255 in mintroute protocol [cit] . this formula means that the trust degree of the monitored node will decrease if the advertised lqi value is larger than the actual one.
"compares the trust value of the sn with the node density varying from 30 nodes to 50 nodes per 10,000 m 2 (e.g., 30 nodes mean 1 ch and 29 sns). we observe that the trust value of the sn fluctuates in a narrow range (0.982, 0.984), when the simulation time is relatively short. if the simulation time is long enough, the trust value of the sn becomes stable, because the longer the simulation time is, the more data are collected and the more accurate the results are. we also notice that with the increase of node density the fluctuation of the trust value of the monitored node becomes smaller. this is because if the node density is small, the number of neighboring nodes of the monitored node will be small, the data that the monitoring node can obtain will be less and hence the trust value of the monitored node will not be so accurate. figure 4 shows the variation of the trust values of the monitored node under the scenario of several types of attacks. we simulate four typical attacks at the mac layer and network layer, including back-off manipulation, selective forwarding attack, sinkhole attack and mac-network cross-layer attack. in the back-off manipulation attack, a malicious node gets unfair priority access to the channel by setting a small cw. in the selective forwarding attack, a malicious node selectively drops packets passing though it according to a predefined criterion. in the sinkhole attack, an attacker tries to attract network traffic by sending bogus rrep messages. in the mac-network cross-layer attack, the malicious node initiates attacks at the mac layer and network layer simultaneously to make itself a node on the routing path by using a small cw and sending a fake routing message with small hop count. we observe that if a node initiates attacks its trust value will decrease obviously (less than 0.8). the behavior of back-off time manipulation of the malicious node will affect its idle time trust value, number of retransmissions trust value and physical layer trust value, so its trustworthiness will decrease to about 0.78. the selective forwarding attack will reduce the packet forward trust value of the malicious node. in the scenario of cross-layer attack, the parameters of both mac layer and network layer will be affected, so the trust value of the malicious node will decrease markedly. the sinkhole attack will affect the hop count trust value, packet forward trust value, physical trust value of the malicious node because the malicious node will drop the route request (rreq) packet, send the rrep packet with small hop count. actually, the trust value of the malicious node is closely related to the selection of the attack parameters. in the cross-layer attack, in order to conceive its attack behavior, the malicious node will reduce the attack strength at the mac layer and network layer, so in the simulation, the trust value of the malicious node in the cross-layer attack is slightly higher than that in the sinkhole attack. malicious node will drop the route request (rreq) packet, send the rrep packet with small hop count. actually, the trust value of the malicious node is closely related to the selection of the attack parameters. in the cross-layer attack, in order to conceive its attack behavior, the malicious node will reduce the attack strength at the mac layer and network layer, so in the simulation, the trust value of the malicious node in the cross-layer attack is slightly higher than that in the sinkhole attack. to get the detection threshold, we simulate false positive and false negative probabilities of our scheme with different thresholds under different types of attacks. we observe that the false positive probability curves under different attacks are similar except fluctuations within a small range. this is because the attacks have little influence on the trust values of unmalicious nodes but greater impact on those of malicious nodes. the intersection of false positive probability curve and false negative probability curve is the optimal trust threshold. under the four attacks, we obtain an optimal detection threshold at which both false negative and false positive probabilities are minimized. as illustrated in figure 5, the optimal detection threshold is about 0.83 at which both false positive and false negative probabilities are less than 0.05 for all types of attacks. we also obtain the false positive and negative probabilities according to equations (22) and (23) . figure 6 shows the theoretical results are consistent with the simulation results. to get the detection threshold, we simulate false positive and false negative probabilities of our scheme with different thresholds under different types of attacks. we observe that the false positive probability curves under different attacks are similar except fluctuations within a small range. this is because the attacks have little influence on the trust values of unmalicious nodes but greater impact on those of malicious nodes. the intersection of false positive probability curve and false negative probability curve is the optimal trust threshold. under the four attacks, we obtain an optimal detection threshold at which both false negative and false positive probabilities are minimized. as illustrated in figure 5, the optimal detection threshold is about 0.83 at which both false positive and false negative probabilities are less than 0.05 for all types of attacks. we also obtain the false positive and negative probabilities according to equations (22) and (23) . figure 6 shows the theoretical results are consistent with the simulation results. we analyze the influence of the proportion of malicious nodes on the detection probability using the optimal threshold 0.83, as illustrated in figure 7 . we observe that the detection probability of sinkhole attack is the highest among the four types of attacks and the detection probability of back-off manipulation attack is the lowest because in the simulation sinkhole attack influences the trust value of the malicious node strongly and back-off manipulation attack has the minimal impact on it. if the proportion of malicious nodes is less than 5%, the detection probability will be more than 97%. if the proportion of malicious nodes is greater than 5%, the detection probability will decrease obviously, because with the increase of the number of malicious nodes, the trust value of the unmalicious node is closer to that of the malicious node, and then it is difficult to distinguish between the unmalicious node and the malicious node. we analyze the influence of the proportion of malicious nodes on the detection probability using the optimal threshold 0.83, as illustrated in figure 7 . we observe that the detection probability of sinkhole attack is the highest among the four types of attacks and the detection probability of back-off manipulation attack is the lowest because in the simulation sinkhole attack influences the trust value of the malicious node strongly and back-off manipulation attack has the minimal impact on it. if the proportion of malicious nodes is less than 5%, the detection probability will be more than 97%. if the proportion of malicious nodes is greater than 5%, the detection probability will decrease obviously, because with the increase of the number of malicious nodes, the trust value of the unmalicious node is closer to that of the malicious node, and then it is difficult to distinguish between the unmalicious node and the malicious node. figure 8 describes the relationship between the false positive probability and the proportion of malicious nodes. if the proportion of malicious nodes is less than 5%, the false positive probability will be less than 0.05. it increases rapidly with the increase of the proportion of malicious nodes. figure 8 describes the relationship between the false positive probability and the proportion of malicious nodes. if the proportion of malicious nodes is less than 5%, the false positive probability will be less than 0.05. it increases rapidly with the increase of the proportion of malicious nodes. figure 7 . detection probability with different proportion of malicious nodes. figure 8 describes the relationship between the false positive probability and the proportion of malicious nodes. if the proportion of malicious nodes is less than 5%, the false positive probability will be less than 0.05. it increases rapidly with the increase of the proportion of malicious nodes. we compare the detection probability of our scheme with that of the nbbte [cit] . as shown in figure 9, the detection probability of the selective forwarding attack and sinkhole attack have been improved by more than 10% and that of cross-layer attack has been improved by more than 20% as the proportion of malicious nodes is 2%, because many key parameters of multiple protocol layers are monitored and the trust values of sns are calculated more accurately in our scheme. in nbbte, the back-off manipulation attack can hardly be detected, because nbbte only focuses on the node behaviors at the network layer, but ignores the malicious behaviors at the mac layer, so it is not effective to detect the attacks at the mac layer. we compare the detection probability of our scheme with that of the nbbte [cit] . as shown in figure 9, the detection probability of the selective forwarding attack and sinkhole attack have been improved by more than 10% and that of cross-layer attack has been improved by more than 20% as the proportion of malicious nodes is 2%, because many key parameters of multiple protocol layers are monitored and the trust values of sns are calculated more accurately in our scheme. in nbbte, the back-off manipulation attack can hardly be detected, because nbbte only focuses on the node behaviors at the network layer, but ignores the malicious behaviors at the mac layer, so it is not effective to detect the attacks at the mac layer. figure 10 shows that the false positive probability of nbbte is higher than that of our scheme. because we use the deviations of protocol parameters instead of the variations of node behaviors as nbbte does for detecting malicious node, the reduction of the trust value caused by the normal change of the network can be avoided in our scheme. in nbbte, the false positive probability curves under different attacks are very similar because the malicious behaviors have little influence on the trust values of normal nodes according to their algorithm. figure 10 shows that the false positive probability of nbbte is higher than that of our scheme. because we use the deviations of protocol parameters instead of the variations of node behaviors as nbbte does for detecting malicious node, the reduction of the trust value caused by the normal change of the network can be avoided in our scheme. in nbbte, the false positive probability curves under different attacks are very similar because the malicious behaviors have little influence on the trust values of normal nodes according to their algorithm. figure 10 shows that the false positive probability of nbbte is higher than that of our scheme. because we use the deviations of protocol parameters instead of the variations of node behaviors as nbbte does for detecting malicious node, the reduction of the trust value caused by the normal change of the network can be avoided in our scheme. in nbbte, the false positive probability curves under different attacks are very similar because the malicious behaviors have little influence on the trust values of normal nodes according to their algorithm."
"where t k denotes the time of the k-th rts packet reception, t k−1 is the end time point of the reception of the previous data segment, t sifs is the duration of the short inter-frame spacing (sifs) frame, and t ack is the duration of acknowledgement (ack) frame, as illustrated in figure 2 ."
"it is important for a trust management scheme to select proper trust factors to evaluate the trustworthiness of a sn. from the literature on this topic, we can find that the trust factors of a sn is mainly based on the nodal communication behavior, energy level, or recommendation from the third party and there is no unified standard in the selection of the trust factors. the attacks initiated at each protocol layer and their influence on the parameters of the corresponding protocol layers lack comprehensive analysis. to the best of our knowledge, there is still no trust management scheme which elaborately describes the trustworthiness of a sn from the standpoint of protocol layer. thus, it is interesting to build the trust evaluation model based on the protocol layer trust. in view of the reality of intrusion detection scheme, we mainly consider the direct trust of a node in our scheme and the trustworthiness of a node is evaluated according to its behaviors at different protocol layers. the consideration of trust worthiness from the viewpoint of multiple protocol layers distinguishes this paper from the previous related works [cit] . since the deviations of the key parameters of multiple layers are used to evaluate the trustworthiness of a node, it is helpful for our scheme to detect nodal malicious behaviors initiated from different protocol layers, which is effective for detecting cross-layer attacks."
"attacks at the network layer aim to disrupt the network routing, and acquire or control the data flows. a malicious node can make itself a part of a routing path by advertising bogus routing messages, such as a good link quality indicator (lqi) or a small hop count. it can also initiate sinkhole or selective forwarding attack and result in dropping all or part of forwarding packets. therefore, we choose route metric and packet forwarding rate as trust metrics to evaluate the network layer trust. the network layer trust is described as:"
"wireless sensor networks are vulnerable to variety of attacks at different protocol layers. in the existing trust-based intrusion detection schemes, there is no unified standard to select trust factors, and cross-layer attacks are seldom considered. in order to identify malicious nodes more efficiently, we have proposed a protocol layer trust-based intrusion detection scheme for wsns. in our scheme, the key parameters of different protocol layers are monitored and the trust values of sensor nodes can be calculated according to the deviations of parameters. by comparing the trust value with a predefined threshold, we can decide whether the sensor node is compromised or not. it can describe the trust values of sensor nodes more accurately by considering the deviations of parameters of multiple layers, hence our proposed scheme is effective for detecting cross-layer attacks. we utilized the t-distribution and simulation to analyze the detection probability and false positive probability of our scheme. the results indicate that there exists an optimal trust threshold at which both false positive and false negative probabilities are minimized. our proposed scheme outperforms the nbbte scheme in terms of the detection probability and false positive probability. the weakness of our scheme is the communication overhead will increase with the increasing of the hop count to the ch. our scheme is extendable, the selection of the trust factors at different protocol layers can be adjusted according to the requirements of a system, and it is applicable to both clustered wsns and flat wsns. as for future works, we will analyze the attacks initiated at the transport layer and application layer, as well as mac-transport cross-layer attack, network-application cross-layer attack and their influence on the protocol parameters to further optimize our scheme. in addition, overhead of our scheme will be greater than that of the nbbte."
"there are now two kinds of intrusion detection systems [cit] . one is the misuse detection system, the other is the anomaly detection system. misuse detection is based on predefined rules, where it is easy to detect known attacks, but impossible to detect unknown attacks. anomaly detection compares present activities with normal system status and user behaviors to detect anomalies. compared with misuse detection, anomaly detection has higher detection rate and the ability to detect unknown attacks, with its false positive rate increasing correspondingly. the focus of this paper is on anomaly detection schemes. recently, different types of anomaly detection schemes based on traffic prediction [cit], statistical method [cit], data mining [cit], game theory [cit], immune theory [cit], or trust management [cit], etc., have been proposed."
"however, there are still some unsolved issues in the existing intrusion detection schemes for wsns. many of the schemes detect attacks according to the anomalies of network traffic. actually, it is a great challenge to distinguish normal behavior from abnormal behavior because not all of attacks on wsns will introduce abnormal network traffic. many intrusion detection schemes only aim to detect several typical types of attacks, while the scenarios of different types of attacks carried out concurrently or cross-layer attacks are seldom considered. the attack behaviors on wsns are usually interconnected and transformed mutually. it is difficult to obtain good detection performance by only studying how to detect a certain kind of attack. therefore, it is necessary to pay more attention to complex attack behaviors, such as cross-layer attack, and study how to utilize the protocol feature parameters at different protocol layers, especially the key parameters which may have an important influence on the performance of the network in order to improve the detection ability of intrusion detection systems [cit] ."
"in equation (4), if the relative deviation of energy consumption is less than or equal to 0, which means the energy consumption of the monitored node is less than the average energy consumption, the monitored node is considered trustworthy at the physical layer. if rd ec is greater than or equal to 1, which means the energy consumption of the monitored node is more than double or double the average energy consumption, the monitored node will be considered untrustworthy at the physical layer."
"we consider a wsn where the network can be divided into multiple clusters, as illustrated in our trust-based intrusion detection scheme includes two levels of trust evaluation, one is ch-to-sn trust evaluation, the other is bs-to-ch trust evaluation. in ch-to-sn trust evaluation, each sn evaluates its neighbors and sends the trust evaluation results to its ch periodically. the ch evaluates all the sns in its cluster by analyzing statistically the trust evaluation results reported by other sns. the trust update period is ∆t, which is a system parameter. the length of ∆t could be made shorter or longer based on network analysis scenarios. similarly, in bs-to-ch trust evaluation, each ch performs trust evaluation toward its neighboring chs and sends its trust evaluation results to the bs. the bs evaluates all the chs in the network by using the same methods as adopted in ch-to-sn trust evaluation. since the two levels of trust evaluation use the same method, we mainly describe ch-to-sn trust evaluation."
"the trustworthiness of a sn (or ch) should be updated periodically. node i evaluates the trust of node j during a time window of length ∆t, so the updated trust of node i toward node j is:"
"where m is the observed number of successful transmissions of the monitored node. therefore, the relative deviation of the idle time can be expressed as:"
"(t) represents the trust value that node i evaluates toward node j at the physical (phy) layer, medium access control (mac) layer, and network (net) layer, respectively, w 1, w 2, and w 3 are the corresponding weight values associated with these three trust components,"
"in our scheme, each sensor node monitors the key parameters of its neighboring nodes at each protocol layer and transmits the trust values toward the monitored nodes to its ch, so the communication overhead of our scheme mainly comes from the packets transmitted from sns to the ch. as a result, the communication overhead of our scheme is related to the hop count from sns to the ch. in nbbte, it includes direct evaluation and indirect evaluation. in the direct evaluation, the monitoring node collects the key parameters of the monitored node and calculates the trust factors of the corresponding parameters. there is a factor of availability which evaluates the availability of the neighboring nodes. to obtain this factor, the monitoring node needs to transmit a hello packet and its neighboring nodes should reply to it with ack-hello packets. in the indirect evaluation, the neighboring nodes of the monitored node will transmit their trust evaluation results towards the monitored node to the monitoring node as the indirect recommendation values. thus, the communication overhead of nbbte includes two parts, the hello packets for the factor of availability and the indirect trust evaluation from the recommendation nodes, which are related to the average number of neighboring nodes in the network."
"real-world relationship topic includes the issues related to the concerns about the creation of personal relationships outside the internet, which is sometimes difficult due to the excessive use of social networks."
"and be labeled as the medial axis. therefore, we find those medial axis segments with length l a small fixed value (in our implementation, we took the thickness of obstacle), and the midpoints of segments are marked as potential target for task allocation. figure 1(c) shows the extracted potential targets which are precisely doorways of the structured environment."
"it is interesting therefore to investigate the indicators that characterize the main features of millennials that underlie their psychological profile on sns [cit] covering a research gap in the literature. according, the third research question addressed in this study was as follows: can the analysis of the ugc on twitter help identify the key psychological indicators and behaviors that characterize the millennial generation? (rq3)."
"from these results, it can be concluded that thai grade one students had positive views on enhancing phonemic awareness through the multimedia call program. they were both motivated by and interested in the program. [cit], in which students who learned through a multimedia program enjoyed this style of learning. the researcher interviewed kindergarten students who had been given activities through multimedia computer software in order to investigate their attitude towards developing phonemic awareness with multimedia computer software after training for six months. the findings of the study showed that they all had positive attitudes towards practicing their phonemic awareness through this material. [cit] demonstrate that kindergarten students enjoy practicing phonemic awareness through this instructional material; additionally, this material can motivate students to learn english."
"in this research, we trained a cnn network to classify the given images in the mnist dataset in their respective classes. architecture of cnn model used for the classification is shown in table 1. python programming language is used for programming of the model. keras is the primary deep learning library used for computation purposes along with numpy. matplotlib was used for graph plotting purposes. for fair splitting of data, we have incorporated stratified splitting of data for training and testing purposes. for further tackling the class imbalance, we have incorporated the concept of class weights in which the misclassification of a minority class is penalized heavily. dropout was also used as regularization technique to ensure better generalization of model over test data. transfer learning technique was also used to compare the accuracy of the model with that of the proposed deep learning model. this model is then trained with the new dataset by freezing more than 70% of the layers in the vgg network. the last few layers are re-trained with the new dataset to mould the network according to the new dataset. finally, this network is concluded by adding a few fully connected layers. we used the transfer learning methods using models like resnet or vgg16 which was pre-trained with the imagenet dataset. adam optimizer was used for optimization purpose and categorical cross-entropy loss was used for calculating the loss in the model."
"moreover, we mentioned earlier that our acs-prm approach is more effective than our previous approach because the acs-prm spends much less time for the learning phase. the experiments show that, with the new approach, the mapping times are respectively 0.321 seconds and 0.329 seconds for map a and map b with 200,000 random samples, which are averages of the 10 runs. we also counted the average number of occurrences of waypoint mutex in each map as shown in table 1 . this table shows that the problem of waiting situation is significantly reduced by using our acs-prm approach, because our approach is able to plan separate paths for robots, especially in the corridor. unlike the voronoi-based approach, there is only one path for all robots."
"the dataset had to be cleaned and organized before being fed into the model. however, the data is highly imbalanced with the lesion type 'melanocytic nevi' comprising of more than fifty percent of the total dataset. we have applied several pre-processing networks to enhance the learnability of the network. we have performed data augmentation to avoid the overfitting of data. we have created several copies of the existing dataset by translating, rotating and zooming the images by various factors. also, in this paper, we have increased the contrast of skin lesions using histogram equalization."
"using the internet as a means to emotional expression usually causes a depressive disorder in this generation. this causes frustration and, when the millennials do not feel accepted in this digital environment, they feel unsupported, hopeless and lacking strength to continue with their daily tasks."
"extensive use of sns generates dopamine and, therefore, can be considered addictive, such as drugs or gambling [cit] . as shown by the number of centers that have emerged in recent years, the problem of technological addictions is a concern in many countries [cit] and should be further analyzed as well as its relation to the business environment, global economy development and the influence of technology in the globalization phenomenon. the characterization of the millennial profile, both psychological and its abilities, has already been provided in several previous studies [cit] ."
"if the sole purpose of the model is to derive word embeddings this can be exploited by using a much lighter output layer. [cit], which swapped the heavy softmax against a hinge loss function. the model works by scoring a set of consecutive words, distorting one of the words, scoring the distorted set, and finally training the network to give the correct set a higher score."
"body image topic includes the questions related to the cult of the body, the creation of content related to one's image and the culture of concern about what other users think of their body image."
"we performed experiments to characterize the liquid replacement process for different liquid flow rates (24.6 nl/min, 123.1 nl/min, and 153.9 nl/min). as shown in figure 3 (a), we flowed color dye and water (transparent) alternatively along one chamber of the device and then recorded the color change in the chamber over time. for each specified flow rate, microscopic videos for both the water-dye and dye-water processes were captured with at least three repeated experiments. we have written a customized matlab script to quantify the intensity changes in the chamber area and the results are summarized in figure 3(b) . it can be expected that the time for liquid replacement, indicated by the chamber color change, is shorter for flows with a higher flow rate. in essence, these experimental results provide the required time of injection for a target ratio of liquid replacement at different flow rates. such information can help to determine the opening time of the culture chambers for different operations."
"nowadays, new technologies help to improve text data mining techniques and make it possible to automatically recognize the meaning of large databases or even topics within a database of comments, reviews or other types of content produced by sns users [cit] . a representative example of such technologies is the lda technique which, in essence, is a modeling tool that can identify the topic of a database, such as qualitative reviews and comments, and quantify the number of mentions of a specific topic [cit] ."
"the millennial generation openly shows their beliefs to their friends, even if these beliefs are not accepted. 0.29 319 the millennial generation strives to generate their own social identity and feel belongingness in groups with similar ideologies."
"secondly, due to the fact that the center for the improvement of early reading achievement [ciera] (2003) has classified phonemic awareness into eight levels, the researcher of this study chose only three levels of phonemic awareness. the researcher suggests that further study should enhance more complex phonemic awareness levels of thai efl learners."
it has been demonstrated the method used is valid for topic and insights discovery as we have reached similar conclusions that other studies in this area using a similar method [cit] .
"likewise, [cit] investigated user motivations by studying their textual expressions on the internet. another technique to improve the development of these methodologies is to estimate correlations between users and the rating number so that to obtain metrics related to the motivations and satisfactions of millennial internet users when generating content. for instance, in a study of millennial users' feedback on tripadvisor, [cit] found that the behavior of the users can be understood through the study of their generated content. owing to these ugc approaches, the companies can now understand the level of their consumers' satisfaction with the products or services these companies offer [cit] . for their part, sandfort and haworth [cit] have deepened the understanding of millennials through the content generated in sns to determine the personality traits of users and their degree of emotional intelligence. table 1 provides a summary of previous studies that used the ugc analysis."
"in the experiments, we assumed that there exists a central server which is able to communicate with all mobile robots and assign the transportation tasks to each individual robot. the results of our experiments are given in figure 3 . we measured the transportation time gained by our approach and compared to the voronoi-based approach. in each plot, the abscissa denotes the team size of the mobile robots, the ordinate denotes the percentage of the transportation time in the total transportation time, and the error bar indicates the confidence interval of each corresponding gain of robot team size with the 0.95 confidence level. figure 3 shows that, a transportation time saving of 6.7% to 12.2% in map a and 6.1% to 12.0% in map b is obtainable under our acs-prm approach compared to the voronoi-based approach. these results proved that our technique could significantly improve the system planning efficiency."
"we injected a dye solution and water into a microchamber alternatively with different flow rates and captured videos for the liquid replacement processes in the chamber using an inverted optical microscope. we then extracted image frames from the videos. the images were processed by a customized script written in matlab [cit] b, mathworks, natick, ma) to quantify the results and obtain the relationship between the liquid content, flow rate, and injection time."
"phonemic awareness through the multimedia call program while learning the english language through the whole word approach. in order to avoid misunderstanding and to prevent miscommunication, the interview conducted in thai which is the first language of the participants. during the interview, a tape recorder was used to record all the information supplied by the interviewed participants. each interview took between 10 and 15 minutes."
"with regard to attitudes and behaviors that characterize millennials according to ugc online chatting, our findings suggested the following three topics: habits based on a digital life (dl); attitudes to travel (tr); and startups (st) companies' preferences to work."
"our recent research focuses on the issue of multi-robot goods transportation [cit] . the objective is to complete the transportation mission with high efficiency and low cost. we proposed a heuristic method based on the empirical model, which aims at planning the transportation task for each individual robot by estimating the production rate of goods based on multi-robot coordination, so as to improve the system performance. in the module of robot motion planning, we used the wavefront propagation algorithm to global path planning and the nearness diagram algorithm to goal seeking and local obstacle avoidance. nevertheless, in the experiment, one important reason which influences the system performance is still the waiting situation. furthermore, if the speed of the robot is too fast, then the goods would be damaged or lost in transit because of the collisions with obstacles or other robots. we limited the speed of the robot to handle this problem, whereas this strategy limited the efficiency of the whole transportation system as well. therefore, in this paper we discuss the essence of the problem and propose a novel approach to address such problems."
"concerning the first topic, our results suggested that the most frequently used channels for ugc of the millennials' chatting are youtube, instagram, facebook and twitter. these findings are important because each sns contributes to the activation of certain psychological characteristics of individuals, such as concentration, intelligence, or patience [cit] ."
"we used the mnist ham-10000 dataset for skin cancer which is available on kaggle [cit] . it contains 10015 images of skin pigments which are divided amongst seven classes. the number of images present in the dataset is enough to be used for different tasks including image retrieval, segmentation, feature extraction, deep learning, and transfer learning, etc"
"finally, our results showed that positively evaluated topics in millennials' ugc include startups preferences to work (st). we linked this finding to the fact that work in startup promotes labor flexibility and allows millennials to better express their ideologies than in other types of professional environments [cit] . this conclusion was also supported by baum's [cit] findings."
"where k represents a query that allows the program to search the text. the behavior of each of the words and for each tweet can be seen. therefore, the k value was found for the hashtag #millennials. in this way, the average k for all the tweets was calculated in order to obtain the global value."
"in the past decades, there has been tremendous interest in studying circadian rhythms of mammals at the cellular level [cit] . based on experimental findings of system biology and network biology, several gene regulatory network models have been established to describe the circadian rhythm system [cit] . experimental evidence has shown that the circadian rhythms are controlled by a pacemaker located in the suprachiasmatic nucleus (scn) of the hypothalamus [cit] and the circadian oscillator is usually described by a goodwin oscillator model, which describes a protein which represses the transcription of its own gene via an inhibitor [cit] . now, the goodwin oscillator model and its variants have been widely adopted as one of the classic hypothetical genetic oscillators [cit] . the scn consists of a dorsomedial shell and a ventrolateral core, and the ventrolateral core can be defined by cells containing vasoactive intestinal polypeptide (vip) [cit] . the circadian oscillators are coupled with each other via the rhythmic influence from vip, and the vip is required to maintain circadian synchrony of the scn [cit] . however, we know nothing about the dynamics of the vip except its fundamental observational properties. based on these observational properties, the van der pol oscillator was usually employed to describe the dynamics of the vip [cit] . in this paper, we will carry out a modified circadian rhythm network model with unknown parameters and investigate its several dynamical properties from the viewpoint of complex network dynamics with the help of nonlinear dynamics theory."
"as for teaching the english language in thailand, thai efl teachers employ various teaching techniques in the classroom which focus on developing the four major skills of thai learners of english. these are often taught by the whole word approach [cit] . [cit], teaching the english language in thailand normally starts from top to bottom, also known as the top-down approach, which begins with reading words by recognizing them by sights, that is, the whole word approach. even though this approach requires learners to utilize whole word recognition skills to identify the spoken word and its meaning, it can also present the problem of a lack of phonemic awareness and result in language learning difficulties among many thai learners. this shows that solely teaching with the whole word approach in the english classroom may not be enough."
"(2) where tf w,i and tf w,j are the number of occurrences of w in sentence i and j, and idf w is the inverse document frequency (idf ) of w."
"therefore, the major aim of the present study is to establish a profile of the millennial generation users based on the online chatter in sns, particularly twitter, which has previously been reported to be a widely used medium for communication among this cohort [cit] ."
"word embeddings have proven useful in many natural language processing (nlp) tasks. for summarization, however, sentences need to be compared. in this section we present two different methods for deriving phrase embeddings, which in section 5.3 will be used to compute sentence to sentence similarities."
"thirdly, this study was conducted with grade one students. it is recommended that further studies should be conducted with other grade levels of students or other groups of students in different contexts."
"thirdly, to start with the textual analysis approach, the different databases of feelings were processed by using nvivo, and the tweets were categorized into the following three nodes: positive (n 1 ), neutral (n 2 ) and negative (n 3 ) [cit] . the process of entering the data in nvivo was manual, although the databases were already divided based on the expressed type of sentiment. the next step was creating the structure of the nodes. through filters, words such as connectors, prepositions or articles and their plural forms were eliminated [cit] ."
"which is similar to the inequality (27) . the remainder of the argument is analogous to that of theorem 1, and it is omitted here."
pooling layer executes the next operation after each convolution layer. these layers are utilized to minimize the size of the neurons. these are small rectangular grids that acquires small portion of convolutional layer and filters it to give a result from that block. the most commonly used layer is max pooling that fetch that maximum pixel from the block.
"the expected output. in order to minimize this function the gradient ∂e ∂θ first needs to be calculated, where θ is a matrix of all parameters, or weights, in the network. this is achieved using backpropagation [cit] . secondly, these gradients are used to minimize e using e.g. gradient descent. the result of this processes is a set of weights that enables the network to do the desired input-output mapping, as defined by the training data."
"from practical and managerial perspectives, our results are meaningful for industries that target millennials as their major consumer group. managers and executives who have to develop business on the internet and digital marketing strategies can use our results to segment and personalize their advertising on sns, if the target audience is millennials. in addition, managers and executives could consider our discoveries to develop content generation plans in sns to increase the engagement with millennials in this digital environment."
"based on the previous results [cit], we set the parameters of the drive network (4) figures 3 and 4 are presented to verify the validity of the parameter laws (14) . as can be seen, the parameter updating laws (14) are in a good agreement with the actual value of the corresponding parameters; i.e., therefore, the first item of theorem 1 is verified by figures 3 and 4 . figure 5 depicts the time evolutions of the control gains η xi t of the adaptive controllers (15) . as can be seen, for each adaptive control gain η xi t, there exists a positive constant η * xi such that"
"several previous studies [cit] demonstrated that it is possible to obtain key topics when analyzing the ugc in different industries and specific subjects and that the latent dirichlet allocation (lda) model can be meaningfully used to identify the topics in the ugc in sns [cit] . therefore, the first research question addressed was as follows: can the topics that psychologically characterize the millennial generation from the ugc on twitter be identified? (rq1)."
"seeding. balb/3t3 murine embryonic fibroblasts (ccl-163, advanced type culture collection (atcc)) were cultivated in dulbecco's modified eagle media (gibco, lift technologies, new york, usa) containing 10% fetal bovine serum. the cells were cultured in a 5% co 2 humidified incubator at 37 ∘ c. when the cells grew until confluent, they were trypsinized with 0.25% trypsin in ethylenediaminetetraacetic acid (edta, sigma-aldrich) and passaged at a cell density of 3000 cells/cm 2, manipulated under a sterile tissue culture hood."
"thirdly, the psychological characterization of the millennial generation, its beliefs and distinctive attitudes and behaviors are addressed in doster [cit] or rodden and hritz [cit] . for instance, ordun and akun [cit] studied attitudes and beliefs of the millennial generation. furthermore, sandfort and haworth [cit] identified the levels of emotional intelligence and personality characteristics of the millennial generation. however, none of them proposed to obtain these indicators by analyzing sns [cit] ."
"excessive use of social networks makes the millennials feel alone in the real world. they create online communities. 0.39 317 the feeling of loneliness is generated by two factors: the non-acceptance of the content published in social networks, and failure to achieve the expected success of engagement, (via followers) in social media"
"an issue we encountered with using precomputed word embeddings was their limited vocabulary, in particular missing uncommon (or common incorrect) spellings. this problem is particularly pronounced on the evaluated opinosis dataset, since the text is of low quality. future work is to train word embeddings on a dataset used for summarization to better capture the specific semantics and vocabulary."
"∘ and 180 ∘ ). in this step, a \"prethinned\" edge image (, ) was generated with an edge pixel in white and a nonedge pixel in black (zero intensity). the image thinning process was applied afterward such that the edge image (, ) included its most edges around one pixel wide (figure 4(a), middle) ."
"likewise, previous studies [cit] employed various approaches to ugc content in order to identify the opinions and feelings in the comments and opinions of users on sns such as twitter, google maps, tripadvisor, or booking.com widely used by millennials [cit] . in the present study, we focused on twitter and sought to determine whether it was possible, through the analysis of the ugc on twitter, to analyze user feelings (positive, negative and neutral) so the following research question was formulated: can the analysis of the ugc on twitter help identify the feelings associated with the topics that characterize the millennial generation? (rq2)."
"self-identity topic includes the contents related to the desire to create an identity of one's own, both in the real world and on the internet."
"if researchers take these insights as variables and constructs for their future models, they may be able to enhance their understanding of whether positive links exist between them by developing, for example, models based on partial least squares structural equation modeling (pls-sem) or statistical package for the social sciences (spss), analysis of moment structures (amos) among others, thus contributing to a field of research that emerges from approaches that extract knowledge from large amounts of ugc data."
"in summary, the results of previous body of work that used sentiment analysis suggest that sentiments in ugc can be analyzed using neural connections consisting of groups of users that interact in the context of ugc [cit] . secondly, textual analysis is the analysis of words and sentiments of online users and can help determine key factors and time. thirdly, hashtags, urls and mentions can be used to analyze specific groups of the users or specific topics [cit] . fourthly, a topic consists of the analysis of the sentiment based on the categories of the same content and, finally, the classification of the information regarding the sentimental analysis as sample keywords of information [cit] . table 2 shows the main characteristics of sentiment analysis."
"the nodes were predefined data containers grouped according to their characteristics. it should be considered that the design and development of nodes is a conventional procedure used to analyze pure data and to achieve the highest possible descriptive and exploratory quality [cit] . in this sense, a relevant indicator in the analysis with nvivo shows the weighted percentage [cit] . this represents the weight of the indicators grouped into nodes based on the times words were repeated in the sample. to calculate the weighted percentage (wp), nvivo was used with the following formula (see equation) ."
the simplest way to represent a sentence is to consider it as the sum of all words without regarding word orders. this was considered by
"we then applied the sobel edge operator to recognize the edges in an image. edge detection was an essential step to extract key features of the alignment mark. similar to the gaussian filter, convolution was utilized to calculate the intensity gradients at the processing pixel (, ) and (, ) in and directions, respectively: we considered the magnitude of the total gradient s(x, y), approximated by"
"this step also corresponds the learning phase of classic implementation of prm. figure 1 illustrates the process of generating a roadmap for an example occupancy grid map by using our approach with 200,000 random samples."
the millennial generation seeks immediacy in achieving everything that is proposed. impatience and the desire to achieve their goals quickly are factors that generate anxiety among this generation. 0.24 129 table 8 . negative characterization of the millennial generation according to ugc chatting in n 3.
"qualitative data analysis: a qualitative data analysis was conducted with the data obtained from the semi-structured interview. all the information from the interviewed was analyzed using content analysis; thus, the data was classified into positive or negative views."
"in our previous work [cit], we considered the issue of using separate topological graphs to coordinated multi-robot motion planning for exploration mission. this work aims at solving the waiting situations in the process of the robot motion planning. in particular, if all the robots take the same topological graph derived from grid map, then they might follow the same exploration path partly or wholly, and this contributes to the problem of waiting situation. we proposed an approach based on sampling environment map iteratively to support the coordinated multi-robot exploration. this research is related to the approach proposed in this paper, even if the methodology is substantially different. compared to our previous approach, we obtain a significantly reduced mapping time by this approach."
in this paper we introduce a novel application of continuous vector representations to the problem of multi-document summarization. we evaluate different compositions for producing sentence representations based on two different word embeddings on a standard dataset using the rouge evaluation measures. our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework which strongly indicate the benefits of continuous word vector representations for this tasks.
"the scientific world journal 3 0.7 mm; new england small tube, litchfield, nh) to inlets of the device. it should be noted that the outer diameter of the adaptors should be chosen to be bigger than inner diameters of the tubing and holes on the device (0.5 mm) to ensure perfect sealing against leakage. the tubing sections close to the microfluidic device were mounted on and moving with the motorized stage during operations; therefore device misalignment due to pulling of tubing during stage movement was negligible. furthermore, we developed a customized graphical user interface to monitor the platform status (e.g., microscopic view and temperature) and control all of the xy stage, camera, heater,and solenoid valve operations."
"the results from the rouge evaluation are compiled in table 1 . we find for all measures (recall, precision, and f-score), that the phrase embeddings outperform the original lin-bilmes. for recall, we find that cw_add cos achieves the highest result, while for precision and f-score the cw_add euc perform best. these results are consistent for all versions of rouge scores reported (1, 2 and su4), providing a strong indication for phrase embeddings in the context of automatic summarization."
"when the three participants from each of the good, fair, and poor groups were asked \"do you enjoy learning through the enjoy the sounds! program at the computer laboratory? why or why not?\", all participants answered \"yes\". this indicated that they all enjoyed practicing phonemic awareness through the multimedia call program while learning the english language through the whole word approach. below are the responses they gave in support of their answers:"
"there are two major types of automatic summarization techniques, extractive and abstractive. extractive summarization systems create summaries using representative sentences chosen from the input while abstractive summarization creates new sentences and is generally considered a more difficult problem. for this paper we consider extractive multidocument summarization, that is, sentences are chosen for inclusion in a summary from a set of documents d. typically, extractive summarization techniques can be divided into two components, the summarization framework and the similarity measures used to compare sentences. next we present the algorithm used for the framework and in sec. 2.2 we discuss a typical sentence similarity measure, later to be used as a baseline."
"in the first decade of the 21st century, the internet has caused innumerable changes at an economic, organizational and strategic level [cit] . these changes brought about innovation initiatives in professional environments and have caused society to create new habits based on technology [cit] . the study of these social changes has aroused the interest of researchers in recent years as well as brought considerable changes in the conceptualization of certain periods in human life and society [cit] . in this way, scientific studies begin to focus their attention on the studies of social generations to learn to understand and empathize with them [cit] ."
"sns users leave a digital trail in the form of data that can be analyzed [cit] . while there are many sns, in the present study, we focused on twitter, which is one of the main sns platforms that makes it possible to measure the opinion of millions of users, and therefore obtain an extensive sample of homogenous data [cit] which is also massively used by millennials as presented by godwin-jones [cit] ."
"once these similar words were grouped in the same independent nodes, a qualitative approximation was made to determine what the factor of each indicator was. consequently, node one (n 1 ) collected the factors related to positive sentiment, n 2 collected the factors related to neutral feelings and n 3 collected the factors related to negative feelings. table 7 . neutral characterization of the millennial generation according to ugc chatting in n 2."
the final layer of a convolutional neural network(cnn) is a fully connected layer that is formed from the attachment of all preceding neurons. it reduces the spatial information as it is fully connected like in artificial neural network. it contains neurons beginning at input neurons to the output neurons.
"accessibility. an experiment was conducted to verify for the accessibility of injection of a selected inlet liquid into every microchamber in the device. blue dye, red dye, and water (transparent) were used for visualization of liquid types from the inlets. as mentioned, the inlet liquid was specified by the inlet multiplexor and the opening of the chambers was defined by the row selection valves and the column multiplexor. the selected dye was injected into each column of chambers from left to right and into each chamber from top to bottom for the column. for each chamber, water and then the dye were firstly injected via the side channels around the selected chamber to ensure the selected inlet solution located at the chamber entrance (figure 2(b), the two left subfigures). following the opening switched from the side channel to the chamber, the dye then flowed into the chamber and the color change was observed (figure 2(b), second right) . then, the opening switched back to the side channel and water flowed along the channel for washing (figure 2(b), rightmost). this step should be particularly important to minimize contamination in the culture applications because the working liquids (represented by dyes here) may include abundant nutrients for growth of contaminants. in this experiment, we utilized the customized graphical user interface program to automate the entire liquid injections with a preset script loaded into the program to define the working procedures, including timing and sequences. we successfully generated a \"checker\" pattern in the multichamber device using the automated system as shown in figure 2 (c). in essence, this experiment has demonstrated that the accessibility of the chambers is sufficient to support the general liquid insertion operations, which require a selected liquid flowing into any of the chamber regions at different time points in defined sequences."
in cnn the main layer is convolutional layer. in this layer the result of the output layer is gotten from the input by filtering in specific condition. this layer is constructed by the neurons which is in the shape of cubical blocks.
"transfer learning is a technique in which a pre-trained model is used on another dataset. this technique is mainly used when there are not enough input data to properly train the model. in such cases, a different model, which is already trained in a different large dataset is used. here, we used some of the models which were pre-trained in the imagenet dataset that contain millions of images that are associated with 1000 classes. these models are further appended with different untrained layers and further trained in the ham10000 dataset. the architecture of the models used namely vgg16 is shown in figure 2"
"monitoring of parallel microfluidic operations can be achieved by connecting the microfluidic device to a highend automated positioning system. this long-term process requires stable physical conditions such that samples in the microchambers are at the expected position (within precision of a few microns or smaller) and in focus for the imaging or other recording schemes. misalignment of the detection regions may lead to failure of the microfluidic devices that their recorded information is incomplete and insufficient. for instance, the displacements can be attributed to position 2 the scientific world journal errors of the moving components, external forces from the peripherals such as tubing for fluid connections, and deformation of device materials (e.g., polydimethylsiloxane (pdms) elastomer). in view of the accumulation of possible device displacements during device movements and microfluidic operations, precise positioning of the device regions is an essential requirement to achieve automation of high-throughput microfluidics for the long-term monitoring of parallel microfluidic operations."
"alignment. the precise movements of the motorized stage alone might not guarantee the device position in the camera. dislocation of the device on the mounting compartment of the motorized stage could be caused by loosen fixation of the mounting compartment, vibration caused by the stage, pulling force induced by the external tubing connecting to the device inlets, and so forth. here, we designed and fabricated an alignment mark located near each chamber at a fix distance between the chamber and the mark itself. the image processing schemes were also developed in order to identify the chamber/alignment mark position and automate the continuous chamber inspection effectively. it should be mentioned that the addition of alignment marks would induce only minor additional complexity of the device fabrication for an extra layer of mold structure fabrication. an alignment mark included three circles which are formed as a right angle triangle as shown in figure 4(a) (left) . this pattern induced significant success rate of the position detection due to the circular pattern that was sensitive to its position rather than the orientation, which was then indicated by the arrangement of the three circles. this configuration allowed the system to detect the shifted position of every chamber via image processing, followed by automatically moving the device in position to recalibrate the viewing area."
"since thai efl learners learn english as a foreign language, teaching english phonemic awareness to them may cause them more difficulties in improving their phonemic awareness [cit] . the major factor that causes difficulty is the differences between the english and thai phonological systems [cit] showed that the english sounds /g/, /v/, /z/, /θ/, /ð/, /ʃ/, /ʒ/, /ʤ/, /ʧ/, and /r/ are problematic for thai efl learners to recognize, distinguish and pronounce. since these nine sounds do not exist in the thai consonant system, thai efl learners have language learning difficulties in identifying and discriminating these sounds [cit] . this results in thai efl learners being unaware of these nine problematic sounds and thus learning the language ineffectively. consequently, the lack of awareness of some english consonant sounds among thai efl learners is one language learning problem that should be addressed [cit] ."
"the second step is roadmap building, in which the potential targets and milestones should be extracted and connected to the roadmap. in the previous step c-space sampling, if there are sufficient points generated, then the points will gather into segments. the main idea of this step is post-processing the graph resulted from the previous step while identifying three types of point as follows:"
"as far as the circadian rhythm model is concerned, it is more difficult to get the exact values of the system parameters, and this becomes one of the interesting and significant questions remaining open for discussion. in order to estimate or evaluate the unknown parameters existing in the circadian rhythm model, lots of researches have been carried out. based on the time series data of a certain group of individuals in a circadian rhythm model, tong [cit] obtained the estimations of the group level, group amplitude, and group phase. later, the estimations of the true values of unknown parameters were investigated for different circadian rhythm models [cit] . now, it has become an interesting and significant research direction in the fields of system biology. to the best of our knowledge, most of the previous results were based on the statistical method or experimental data, and few theoretical researches have been carried out. motivated by the discussions mentioned above, this paper aims at providing theoretical estimations of unknown parameters existing in such a network. it may help us build more accurate mathematical models and better understand the circadian rhythms of mammals. therefore, the subject of this paper has a certain degree of innovation, and it may also have some latent applications. by proposing appropriate parameter updating laws and adaptive control strategies, we identify the unknown parameters successfully and the network realizes outer synchronization. based on lyapunov stability theory and matrix theory, we give theoretical proof for adaptive outer synchronization of the goodwin oscillator network with unknown parameters. as special cases, we present two succinct corollaries for different instances."
"where θ i are the weights associated with neuron i and x is the input. here the sigmoid function (g) is chosen to be the logistic function, but it may also be modeled using other sigmoid shaped functions, e.g. the hyperbolic tangent function. the neurons can be organized in many different ways. in some architectures, loops are permitted. these are referred to as recurrent neural networks. however, all networks considered here are non-cyclic topologies. in the rest of this section we discuss a few general architectures in more detail, which will later be employed in the evaluated models."
"with regard to the score from the phonemic awareness test of /f/ and /v/, the posttest score of the experimental group was higher than the pretest at the .001 level of significance. the posttest score of the control group was lower than the pretest."
"where x p is a phrase embedding, and x w is a word embedding. we use this method for computing phrase embeddings as a baseline in our experiments."
"in summary, we can conclude that using the internet as a dominant mean of self-expression defines the emotions experienced by millennials [cit] . on the one hand, they can become increasingly irritable and, sometimes, their non-acceptance in digital environments generates a strongly negative affect with potentially devastating consequences, such as de or lo. by contrast, millennials' positive emotions are linked to their engagement in sns communities and forms of social approval they receive there (such as likes, comments and the number of views of their actions on social platforms)."
"1.1. model descriptions. the goodwin model describes a circadian oscillator consisting of three variables, which is illustrated in figure 1 . a clock gene mrna (a) produces a clock protein (b), which activates a transcriptional inhibitor (c), and in turn inhibits the transcription of the clock gene. by the repression exerted by the inhibitor to the mrna synthesis, the three variables build up a closed negative feedback loop."
"the input data is placed in the leaf nodes of the tree, and the structure of this tree is used to guide the recursion up to the root node. a compressed representation is calculated recursively at each non-terminal node in the tree, using the same weight matrix at each node. more precisely, the following formulas can be used:"
a multimedia computer-assisted language learning program (call) is considered effective when used as a supportive tool to enhance young learners' phonemic awareness [cit] . studies on enhancing young learners' phonemic awareness with multimedia call programs (cassady &
"phonemic awareness tests: phonemic awareness tests were used to measure thai grade one students' performances in the phonemic awareness of english. in this study, the researcher constructed the tests by adapting two types of phonemic awareness test: [cit] and kirwan assessment [cit] . the researcher constructed three phonemic awareness tests which were tracked by a pretest and posttest. these tests assessed three consonant pairs /k/ and /g/, /f/ and /v/, and /s/ and /z/. each phonemic awareness test was divided into three levels of phonemic awareness: (a) phoneme isolation, (b) phoneme identity, and (c) phoneme categorization. each test consisted of 15 items with 5 items for each level of phonemic awareness. there was a time limit of 20 minutes for each test."
"motion planning is a fundamental problem in robotics. it could be explained as producing a continuous motion for an agent, that connects a start configuration and a goal configuration, and avoid collision with any static obstacles or other agents in an environment. agent and obstacle geometry are generally described in a 2d or 3d workspace, and the motion could be represented as a path in configuration space. motion planning algorithms are widely applied in many fields, such as bioinformatics, robotic surgery, industrial automation, planetary exploration, and intelligent transportation system. the multi-robot system (mrs) is proposed to deal with some problems that are difficult or impossible to be solved by a single robot, or to improve the system implementation efficiency in some missions completed by multi-robot rather than a single robot [cit] . the biggest challenge for the mrs is coordination. without coordination, it will not only lower the system efficiency, but also lead to the failure of the entire system in extreme cases. in this paper, we consider the issue of coordinated motion planning for a homogeneous team of autonomous mobile robots in structured environments. most of the proposed approaches for multi-robot motion planning usually have the problems of resource conflict such as congestion and collision [cit] . we arrange these cases to the waiting situation problem [cit] . to handle this practical problem, this paper presents a novel approach for multi-robot motion planning by using a probabilistic roadmap planner (prm) which is based on manner of adaptive cross sampling (acs)."
"with regard to the second topic, traveling, it does characterize the preferences of millennials, but has several downsides, such as the development of addiction to sharing information about one's journeys [cit] . this addiction is promoted by getting instant rewards from others, in the form of likes or comments on social platforms and can lead to generating addictions to social acceptance and the creation of parallel realities. such additions can also have a negative impact on de, lo or rw."
"nine participants, comprising three participants from each of the good, fair, and poor groups, were randomly selected to be interviewed in this study on their views on improving phonemic awareness through a multimedia call program while learning the english language through the whole word approach. the findings from the semi-structured interview are presented as follows:"
"in the present study, we propose a new and original approach to the analysis of millennials based on the online chatter about this generation on twitter, covering a research gap on the literature. consequently, the primary purpose of this study is discovery, not hypothesis testing and not trying to control variables, but to discover them [cit] ."
"however, there are still lots of urgent and challenging problems in practical application. for instance, we often know very little about the exact values of system parameters, or there are some time-varying parameters. under the effects of these uncertainties, the achieved synchronization might be destroyed and broken. therefore, it is necessary to design an adaptive control law that adapts itself to these uncertainties, which is a popular control technique used for complex network models with unknown parameters [cit] . the theoretical basis of adaptive control is parameter identification. as far as complex networks are concerned, three dynamic properties of uncertain networks need to be discussed, i.e., parameter identification, adaptive control, and outer synchronization [cit] . in order to achieve each one of the three research goals above, lyapunov stability is usually employed, which will show the convergence of the error systems and the parameter identification laws at the same time. due to the convenience and effectiveness of adaptive control, it has been widely applied to many fields of science and technology, including secure communication, chaos generator design, biological systems, and information science."
". thereby, the center of a chamber can be aligned with a target position based on these measured displacements and orientations of the alignment mark accordingly. figure 5 illustrates typical outcomes of the steps during the imagebased alignment process. most of the intensity noise in a representative raw microscopic image was removed by the gaussian lowpass filtering. edges of the filtered image were extracted by the sobel edge operators, followed by the thinning operation for the estimation of finer edges. afterwards, the map of circle (, ) was generated by the template-matching process and positions of the three circles in the alignment marks were indicated with the maximum values in circle (, ). based on these circle position, the required movement of the stage was computed and"
"the models created were trained by using the balanced and resized images from the dataset. kaggle's kernels were used for performing the training testing and validation of the models. the number of epochs for which the models were trained was 50. we then calculated the confusion matrix, shown in figure 3 and evaluated the models using the overall accuracy of classification. we have developed a cnn and model for the classification of skin cancer. our cnn model gave a weighted average value of precision of 0.88, a weighted recall average of 0.74 and a weighted f1-score of 0.77. table 2 shows the result matrix of different transfer learning models. table 3 shows the performance of our cnn model on the various classification of various classes. we also tried to test our dataset of different models like random forest, xgboost and support vector classifiers. however, we did not see promising results in these learning algorithms. the results are displayed in table-5"
"phonemic awareness is a necessary early language literacy skill for the development of language skills in young learners [cit] . having a low level of phonemic awareness can result in language learning difficulties for many learners, especially in reading and spelling [cit] . previous studies [cit] have shown that many thai efl learners at all educational levels face many difficulties in learning a language and that one such problem is a lack of phonemic awareness. for example, many thai learners who have weak phonemic awareness cannot distinguish between voiced and voiceless consonants in the english language [cit] ."
"textual analysis is a text mining analysis approach that determines the key factors in a big amount of data [cit] . it is also a qualitative approach where, based on the frequency of specific items within a text, helps determine the keywords and sentiments in the analyzed content [cit] ."
"it should be noted that positive sentiments are the feelings related to positive values and connotations within the theme. said differently, the fact that a subject obtains a positive feeling means that the factors that comprise this theme are perceived as positive by the millennials chatting on twitter. for example, the topic of \"traveling\" (see table 5 ) gets a positive feeling, meaning that the millennials, according to their ugc on twitter, perceive traveling, adventures and international activities in a positive way. the same principle applies to each of the identified topics. table 5 shows the most frequent words in each topic and the identified sentiments and kav. in the textual analysis step, we defined the factors or indicators that characterize each topic according to its sentiment (see tables 6-8) . to this end, we attended to both the psychological attitudes and behaviors linked to the sentiment of the identified topics. specifically, using textual analysis with nvivo software, three nodes were established to correspond to each topic according to their feeling, and based on these results, the text data mining was performed to determine the most important factors according to their weight within the selected topic. in this case, according to the number of times the words repeated in the dataset, they were classified in different nodes. table 6 . positive characterization of the millennial generation according to ugc chatting in node one (n 1 )."
"based on the results of our analysis, we concluded that these two negative descriptors, de and lo, may lead to millennials' inability to establish real-world relationships (rw). this inability to express feelings in a lasting way in time with people outside the internet and without an immediate reward has already been identified as a problem of future generations using the internet (e.g., [cit] . yet, the immediate connection with the present-day generation of millennials has not been made yet."
"we hope that the results can provide theoretical guidance for biology experiments in spite of the confusing biological applications, and we will continue to study the context of the biological interpretation later. another possible further work is the identification of unknown time-varying parameters since this paper is only applicable to the identification of given and fixed parameters."
"seven different configuration were evaluated. the first configuration provides us with a baseline and is denoted original for the lin-bilmes method described in sec. 2.1. the remaining configurations comprise selected combinations of word embeddings, phrase embeddings, and similarity measures."
"with regard to neutral psychological/personality traits, our results demonstrated that the need for the feeling of self-identity (si) drives millennials to create their own reality linked to their lifestyles and values that they share through sns (see also doster [cit] ). therefore, self-expression is a typical characteristic feature of this cohort."
"however, as demonstrated by olson [cit], energy and adventurous eagerness of this population group are counterbalanced by anxiety (an) that arises due to the inability of getting instant reward. interestingly, several other authors argue that anxiety in the millennial generation is linked not to instant rewards, but to their skills with technology at work (e.g., reference [cit] )."
"quantitative data analysis: a quantitative statistical analysis was conducted using the data obtained from the phonemic awareness tests. the data was analyzed using descriptive statistics (mean and standard deviation). the t-test was used to discover whether there were significant differences within the experimental group and the control group; moreover, between the experimental group and the control groups in terms of the scores gained from the pretests and posttests."
"recent years have seen significant advances in the study of complex network dynamics [cit], and its related studies will lead to more potential applications in the future. synchronization is a kind of typical collective behaviors and basic motions in nature, which is one of the main research focuses in complex network science. from the viewpoint of mathematics, the core of synchronization is the stability of the zero solution of network error systems [cit] . in previous studies, two effective methods are usually employed: the first one is to study synchronization induced by the mutual couplings between nodes [cit], and the second one is to design reasonable control laws [cit] . a great number of researches on the first method have indicated that synchronization without external control needs certain requirement in both network structures and node dynamics. therefore, a variety of external control approaches have been developed such as pinning control [cit], sliding mode control [cit], and feedback control [cit] ."
"subsequently, positions of the three circle patterns were detected by finding the three maximum values in circle (, ) using the gray-boundary template image for matching. we defined the positions of these circles as ( 1, 1 ), ( 2, 2 ), and ( 3, 3 ) for the upper left, right, and lower circles, step 1"
"as regards taking each phonemic awareness test, participants in both the experimental and control groups were limited to 20 minutes. moreover, practicing phonemic awareness with a multimedia call program took 60 minutes for each period and one assistant attended the computer laboratory with the participants in the experimental group in order to help them when they had questions or problems while practicing. the assistant was an american teacher who taught english at primary level -grades one to six. furthermore, the interview section took between 5 to 10 minutes each person."
"on the other hand, we investigated also variations of the shifted positions of all of the chambers after xy stage moving repeatedly with and without the position alignment. the system recorded the values during multiple scans of imaging over the 32 chambers in the device. paths of the scans were implemented by the motorized xy stage moving back and forth as described in figure 7 (a). we wrote scripts for the graphical user interface program to define sequences of the movements of the stage and the imaging operations as well as the moving distances between chambers. here, we show in figure 7 (b) the sample images of a microchamber (at row 3 and column 3 in the device) for its position errors (e) during five imaging scans. apparently, the was maintained by the position control operations, while the without the control accumulated with the numbers of scans. indeed, the similar trend happened also in all other chambers as shown in figure 7 (i.e., a larger chip size) should have an even larger demand for the automated alignment function."
"in this paper, we presented a novel approach for coordinated motion planning of multiple robots by using the probabilistic roadmap planner based on a manner of adaptive cross sampling, which we called acs-prm. the basic thought of the proposed approach is to build separate kinematic paths for multiple robots to minimize the problem of waiting situation such as collision and congestion caused by waypoint mutex in an effective way, thus to improve the efficiency of automated planning and scheduling. in consideration of the context of the issue of multi-robot goods transportation, the experiments were conducted to transport a certain amount of goods by a fleet of mobile robots in structured environments. the experimental results demonstrate that, by using our acs-prm approach, the total time needed to complete the transportation mission has been significantly reduced compared to the voronoi-based approach."
"the third step is motion planning, in which each individual robot's kinematic path should be planned by querying the constructed roadmap. the main idea of this step includes the following three points:"
"semi-structured interview: to elicit the thai grade one students' views on utilizing the multimedia call program to improve phonemic awareness while learning the english through the whole word approach, three participants from the good, fair, and poor groups were randomly selected to take part in a semi-structured interview after finishing the last posttest. the questions in the interview were open-ended and designed to determine the students' views on improving"
"the participants in both the experimental and control groups learned the english language in the classroom with the whole word approach through a thai teacher. besides learning english in the classroom, the participants in both groups were given additional activities. the experimental group was provided a multimedia call program to practice phonemic awareness at the computer laboratory, whereas the participants in the control group joined the fun english activities arranged by the school."
"the millennial generation has problems creating consolidated relationships. they are more comfortable in the digital environment, which hinders their relationships in the real world. 0.26 291 skills such as perseverance and hard work are sometimes lacking, as the millennials are accustomed to instant rewards. this frequently causes failures to establish lasting relationships in the real world."
"which is similar to the inequality (27) . the remainder of the argument is analogous to that of theorem 1, and it is omitted here."
"the implementation of this step is summarized in algorithm 1, where the time complexity is o(n) and the space complexity is o(1). this step corresponds the learning phase of classic implementation of prm."
"sns users generate large amounts of personal data stored on these platforms known as ugc [cit] . therefore, the public part of these data was used to develop a text-mining analysis approach. this approach was elaborated to find the main characteristics and behaviors of the millennials, including their concerns, motivations and abilities in order to be used by companies, social agents, policy makers and other interested public or private agents who have the millennial generation as their target audience in their business, marketing, social, cultural or sociological projects."
"they employ a ffnn, using a window of words as input, and train the model to predict the next word. this is computed using a big softmax layer that calculate the probabilities for each word in the vocabulary. this type of exhaustive estimation is necessary in some nlp applications, but makes the model heavy to train."
"in order to further investigate the applicability of continuous vector representations for summarization, in future work we plan to try other summarization methods. in particular we will use a method based on multiple kernel learning were phrase embeddings can be combined with other similarity measures. furthermore, we aim to use a novel method for sentence representation similar to the rae using multiplicative connections controlled by the local context in the sentence."
"such awareness can be taught to learners of all levels and ages; however, it should be taught at an early agebetween three to eight because they can benefit the most (center for the improvement of early reading achievement . the emphasis on teaching phonemic awareness is, thus, essential for the enhancement of thai young learners' phonemic awareness in order to build the strong foundations for avoiding language learning difficulties."
"in the present study, we applied a three-phase methodology to analyze twitter ugc to extract the main topics that, taken together, provide a thorough psychological characterization of the millennial generation and its behavior. we also identified the types of feelings associated with salient topics in this cohort. this made it possible to identify key factors within each of the identified topics."
"the limitations of the present study include a relatively small sample size and a limited time horizon. in further research, in order to properly address the concerns of this user group (including psychological problems and behavioral disorders), it would be necessary to determine other factors that could lead to a better understanding of the millennials as a generation. likewise, one of the weaknesses that should be highlighted is that the diagnosis made about the millennial generation and its characterization and behavior analysis is the consequence of ugc chatting in social media and not by a clinical, social or in-depth interview method."
"after applying the lda model, the topics that characterize the millennial profile were identified (see table 4 ). that is, the different words contained in the database were categorized into topics. in this respect, it should be noted that the names of the topics were defined automatically. this is a natural outcome when the lda-based topic identification methodology is used [cit] . in essence, the process of the development of the nomenclature of each topic is usually as follows. first, the most frequently used words mentioned by users are selected [cit] . second, phrases consisting of those most frequent words are formed [cit] . once the content categories make sense and form groupings of well-differentiated topics, the topic identification process is completed. table 4 shows the topics identified in our ugc database. table 4 . identified topics related to millennials."
"likewise, for the phonemic awareness test of /s/ and /z/, the posttest score increased, compared to the pretest at the .001 significance level. meanwhile, the posttest score of the control group decreased slightly, compared to the pretest."
"in this work, we developed a microfluidic position control system capable of implementing microfluidic manipulation and automated image-based microchamber alignment. the system included also temperature and gas control to regulate the desired microenvironment for cell or chemical applications. we designed and fabricated an integrated microfluidic device containing 32 chambers with the alignment marks, which were composed of three circular patterns. the addition of these alignment marks did not induce any extra fabrication efforts. the chamber accessibility and fluidic manipulation were implemented by the configuration of channels and control valves in the device. we characterized the required durations for liquid replacement in the chambers under different flow rates. we demonstrated also that each chamber in the device can be flowed with a liquid selected from the inlets. on the other hand, we applied image processing techniques (e.g., sobel edge detector and temple matching scheme) on the alignment mark regions to identify the chamber positions and their dislocations from the target locations. [cit] 9 ms to 452 ms (∼2.2%). the effectiveness of the automated chamber alignment was shown by our experiments where position errors for the unaligned case were ∼2-80 times larger than errors for the aligned case in every scan of the 32 chambers. the errors for the unaligned case accumulated throughout multiple scans, whereas the errors for the aligned case were maintained within 3 pixels. further, we demonstrated also that the microfluidic parallel monitoring platform can be applied to cell growth analyses. altogether, in view of the fact that the alignment marks can be embedded in any microfluidic designs, this automated imagebased chamber alignment can be applied in the general highthroughput microfluidics for continuous monitoring of cell and biochemical activities at multiple regions simultaneously."
"a feed forward neural network (ffnn) [cit] ) is a type of ann where the neurons are structured in layers, and only connections to subsequent layers are allowed, see fig 2. the algorithm is similar to logistic regression using nonlinear terms. however, it does not rely on the user to choose the non-linear terms needed to fit the data, making it more adaptable to changing datasets. the first layer in a ffnn is called the input layer, the last layer is called the output layer, and the interim layers are called hidden layers. the hidden layers are optional but necessary to fit complex patterns."
"output layer θ e θ d figure 6 : the structure of an unfolding rae, on a three word phrase ([x 1, x 2, x 3 ]). the weight matrix θ e is used to encode the compressed representations, while θ d is used to decode the representations and reconstruct the sentence."
"our results suggest that the phrase embeddings capture the kind of information that is needed for the summarization task. the embeddings are the underpinnings of the decisions on which sentences that are representative of the whole input text, and which sentences that would be redundant when combined in a summary. however, the fact that we at most achieve 60% of maximal recall suggests that the phrase embeddings are not complete w.r.t summarization and might benefit from being combined with other similarity measures that can capture complementary information, for example using multiple kernel learning."
"the microfluidic devices were sterilized by flushing the flow channels with 70% ethanol and 1x phosphate-buffered saline (pbs). the flow channels were subsequently precoated with 20 mg/ml human fibronectin (sigma-aldrich) in 1x pbs for 1 h to promote cell adhesion and growth, followed by replacing the liquid with pure 1x pbs and then the culture media. afterward, the 3t3 cells at a density of ∼10 5 cells/ml were loaded into the microchambers sequentially. the procedures for liquid replacement and cell seeding are described in section 3.3."
"in this way, it can be concluded that textual analysis helps determine and identify the keywords of a greater relevance in a given sample and to investigate their impact by analyzing the content [cit] . therefore, textual analysis approaches have been widely used to prove that insights can be discovered from big amounts of data as presented by arnold [cit] and reyes-menendez [cit] . table 3 shows the main characteristics of the approximations based on textual analysis in order to identify the key factors in the ugc analysis."
"after data collection, to start the proposed methods first, we developed an lda model, which was based on a probabilistic assumption, assumed that content was generated in the following two steps: the first step required the identification of words and each word should be in an independent document leading to two consecutives steps [cit] . these two steps consisted of randomly identifying the distributions of the topics identified in a sample and selecting the main topics found in that sample [cit] . in real situations, it is difficult to estimate the distribution of topics over the document or the distribution of words over topics in advance [cit] ."
the scientific world journal therefore the chamber was aligned with the target location for time-lapse recording and further analyses. describe effectively the level of misalignment of the device.
"the results of this paper show great potential for employing word and phrase embeddings in summarization. we believe that by using embeddings we move towards more semantically aware summarization systems. in the future, we anticipate improvements for the field of automatic summarization as the quality of the word vectors improve and we find enhanced ways of composing and comparing the vectors. it is interesting to compare the results of different composition techniques on the cw vectors, where vector addition surprisingly outperforms the considerably more sophisticated unfolding rae. however, since the unfolding rae uses syntactic information, this may be a result of using a dataset consisting of low quality text."
"startups topic includes the issues related to start-ups, technology-based companies in which millennials are comfortable because they can share their work life in the social network. owing to the use of innovation and technology, such business models are more flexible."
"unfolding rae on cw vectors and vector addition on w2v vectors gave comparable results w.r.t. each other, generally performing better than original linn-bilmes but not performing as well as vector addition of cw vectors."
"in conclusion, english phonemic awareness of thai grade one students can be enhanced by the use of a multimedia call program in combination with the use of the whole word approach. furthermore, all students provided a multimedia call program had positive views towards improving phonemic awareness through the multimedia call program while learning the english language through the whole word approach. this seems to indicate that a multimedia call program is effective as a supportive tool for building phonemic awareness. a multimedia call program can also increase students' interest and motivation because it can be created with a variety of activities that can encourage students to practice phonemic awareness [cit] . moreover, it can produce a positive language learning environment because students find the program enjoyable and thus it encourages them to learn the language [cit] ."
"in addition, an alignment micropattern was fabricated right next to each culture chamber for position identification of the device during operations. the alignment mark consisted of three equal-dimension circles arranged as a right angle triangle (upper left inset in figure 2(a) ) such that the position and rotation of the device could be identified by an image-based position recognition algorithm. overall, the microfluidic device was capable of liquid manipulation from a selected medium/reagent to every chamber and provided appropriate microenvironments in each chamber."
"figure 3: the figure shows an auto-encoder that compresses four dimensional data into a two dimensional code. this is achieved by using a bottleneck layer, referred to as a coding layer."
"the sampling in this study was thai grade one students, comprising both males and females. out of 78 students, 50 students were selected to participate in this study by purposive sampling using english language capabilities. the researcher obtained the students' english proficiency scores from their teacher and used these scores to classify them into three groups; good, fair, and poor. a student who scored more than 75 out of 100 points was classified as being a student with good english. a student who scored between 65 to 74 points was classified as a student with fair english. a student who scored less than 65 out of 100 points was classified as a student with poor english. it was found that there were 16 students in the good english proficiency group, 24 students in the fair english proficiency group, and 10 students in the poor english proficiency group. next, the researcher used simple random sampling to divide the students of each group into the experimental and control groups equally. thus, there were 25 students in the experimental group and 25 students in the control group."
"we established an automated platform to control microfluidic operations (figure 1(a) ). this platform included an inverted microscope (pw-20 bds500epi, proway optics and electronics, ningbo, china) placed in a confining shield with the capabilities of temperature, humidity, and surrounding gas (e.g., 5% co 2 in air for cell culture applications) controls (figure 1(b) ). the temperature control was achieved by first measuring temperature in the confined shield with a temperature sensor (lm35, texas instruments, dallas, texas, united states). the sensor signals were then passed to a temperature controller consisting of a microprocessor (atmega16, atmel, san jose, california, usa) for computing the driving voltage for a heater (nt20-12d, midea, hong kong) based on a proportional-derivative feedback control scheme, in order to maintain temperature in the confined shield. the stabilization and homogeneity of temperatures at different positions of the microfluidic device were validated by an experiment described in figure s1 in supplementary material available online at http://dx.doi.org/10.1155/2014/608184. the gas condition and humidity were conditioned by continuously flowing the desired gas (compressed air mixed with 5% co 2 ) through a water tank located next to the microscope for humidification ( figure s2a) . the humidified gas then passed into the incubator region surrounded by another inner confining unit moving with a motorized stage ( figure s2b ) installed in the microscope. the motorized stage contained a customized device holder for the controlled movement of microfluidic devices. a cooled ccd camera (tch-5.0ice, xintu photonics, fuzhou, china) connected to the microscope for monitoring and recording experiment processes. as this automated platform mainly focused on the microfluidic devices driven by the pneumatic valves based on multilayer soft lithography [cit], it included up to 96 solenoid valves to achieve programmable air pressures from a common compressed air source. these 96 solenoid valves were driven by a computer, in which the commands were transmitted via an interface card (pcie-dio96h, measurement computing, norton, ma) installed. in this case, we could define the gas lines pressurizing valves in a microfluidic device, through tygon tubing (inner diameter: 0.5 mm; cole-parmer, vernon 24 hills, il) and stainless steel adaptors (outer diameter:"
the rest of the paper is organized as follows: section 2 describes an overview of some related works; section 3 describes our acs-prm approach; section 4 presents the experimental results obtained with our approach; and the paper is concluded in section 5 at last.
", ∀ (, ) in the pattern. (6) circle (, ) reflects the degree of matching between the edge image and the template image at the pixel position (, ). therefore, the three maximum points of circle (, ) should indicate the position of circle patterns."
"to evaluate our acs-prm approach, we conducted a series of simulation experiments with the 2d multi-robot simulator stage [cit] . the experiment is to transport a certain amount of goods from one origination to divers destinations by a fleet of mobile robots. the simulated robot is the pioneer 2-dx robot equipped with a laser range finder providing 361 samples with 180 degrees field of view and a maximum range of 8 meters. each robot can localize itself based on an abstract localization device which models the implementation of gps or slam. to transport goods, the robots are equipped with a gripper that enable them to sense, pick up and put down the goods, and the carrying capacity is limited to one unit per robot. we used a different number of robots to conduct several experiments in various environments. two maps ( figure 2 ) were used in our simulation which are both structured environments. for each map, the green area signifies the original position of goods, and the yellow areas represent the destinations which are always placed in the rooms. the transportation team size is varied from 2 to 8 robots. on each team size, 10 experimental runs are performed for a transportation mission of 50 goods. the mission objective is to transport the goods to every room equally. the ratio between real-world time and simulation time is about 1:1. we also compared our approach to the commonly used voronoi-based approach [cit] in which a topological map is built on top of the grid map by using the voronoi diagram, and the critical points are extracted like milestones for mobile robot motion planning. all experiments reported in this paper were carried out on a system with an intel core 2 duo e8400 3.00ghz processor, an intel q43 express chipset and two ddr2 800mhz 1024mb dual channel memory."
"multimedia call program: the study developed a multimedia call program, enjoy the sounds!, which was an integrative call program that ran from a cd-rom. it integrated phonemic awareness with multimedia-texts, sounds, animations, and pictures. the multimedia call program covered the problematic sounds in the english language that thai efl learners experience difficulties in recognizing and distinguishing, namely the three pairs of english consonants /k/ and /g/, /f/ and /v/, and /s/ and /z/. for each pair, there were three levels of phonemic awareness: (a) phoneme isolation, (b) phoneme identity, and (c) phoneme categorization. there were 20 items for each level of phonemic awareness. the participants were limited to 35 minutes of practice for each level of phonemic awareness. the multimedia call program was provided as a supportive tool for the participants with the aim of enhancing their english phonemic awareness."
"accordingly, several complementary methods and approaches have evolved to use sentiment analysis in a wide array of applications [cit] . some of these approaches are based on machine learning techniques and artificial intelligence, while other hybrid models are operated by specific software. another alternative is training of algorithms along with data mining techniques to improve the probability of success of an algorithm operated with machine learning and to achieve more accurate results [cit] . furthermore, in several previous studies, models based on machine learning were developed for the analysis of different sns, including online users' opinions, or for the identification of the key factors related to a particular topic [cit] . supervised methods based on classification and categorization of key factors such as maximum entropy (maxent) and svm are widely used for sns analysis [cit] . to this end, one can use keywords, ratings of feelings regarding a topic, semantic meaning, concepts, semantic theories and sentiment-topic features such as hashtags, retweets or other valuations identifiers of any product and service on the internet [cit] ."
"the organization of the remaining sections is as follows. in section 2, some preliminaries are introduced, including the model descriptions of the goodwin oscillator network with unknown parameters. in section 3, adaptive controllers and parameter updating laws are designed, and their effectiveness is also proved theoretically. in section 4, a simple example is provided to verify the validity of the theoretical results. in the last section, conclusions are provided to summarize the contributions of this paper and to highlight some interesting issues as a further work."
"where s is the scoring function, i.e. the output of a ffnn that maps between the word embeddings of an n-gram to a real valued score. both the parameters of the scoring function and the word embeddings are learned in parallel using backpropagation."
self-expression topic includes the issues related to the need to find a digital channel to share the acts of self-expression. the issues reflect the need to show user self-identity.
"to the best of our knowledge, continuous vector space models have not previously been used in summarization tasks. therefore, we split this section in two, handling summarization and continuous vector space models separately."
"the basic formula for kav is a relationship characterized by observed disagreement/expected disagreement. in equation, an apparently simple proportion is shown, because the calculation method is computationally very complex. the calculation process involves resampling methods such as bootstrap [cit] . the calculations in equation are explained in depth in krippendorff [cit] ."
"an auto-encoder (ae) [cit] ), see fig. 3, is a type of ffnn with a topology designed for dimensionality reduction. the input and the output layers in an ae are identical, and there is at least one hidden bottleneck layer that is referred to as the coding layer. the network is trained to reconstruct the input data, and if it succeeds this implies that all information in the data is necessarily contained in the compressed representation of the coding layer."
"a shallow ae, i.e. an ae with no extra hidden layers, will produce a similar code as principal component analysis. however, if more layers are added, before and after the coding layer, nonlinear manifolds can be found. this enables the network to compress complex data, with minimal loss of information."
"also, academics can use this research to better understand millennials main concerns and behavior to focus on the development of research within the field. in addition, they can focus on content analysis by users of different sns to better understand what the main habits are when sharing information publicly in digital ecosystems and ugc."
"it follows from the proof of theorem 1 that the functions γ xi t and γ yi t are designed for the disturbances δ x t and δ y t . therefore, we obtain the following corollary without considering the disturbances."
"after all the tweets were downloaded, some tweets were deleted due to repeated content, news, or retweets [cit] . the images and multimedia files that were published along with the text of the tweet were excluded from the analysis [cit] . following saura and bennett [cit], the sample of tweets was considered valid according to the following criteria: (i) active twitter profiles, i.e., with activity during the three months after the use of the indicated hashtags; (ii) twitter user profile with a profile photo and a cover photo; (iii) retweets from the same tweet using the indicated hashtags were removed, as such tweets were considered to be duplicate content; (iv) only public profiles and tweets in english using #millennials, #millennial and #millennialgeneration were included and (v) tweets should have been at least 80 characters long with spaces and had to have the hashtag as indicated. (i.e., #millennials. tweets without the \"#\" or with a wrong label like \"#milennials\" were omitted)."
"furthermore, we found that the millennials are concerned with self-expression (se), which leads them to frame their habits and abilities as parts of their ideologies and tastes through sns. this result converges with the conclusion made by foltz [cit] . overall, several previous studies have demonstrated that, for millennials, important factors are the need to show the body to be accepted by the closest circle of friends as an act of self-expression, healthy habits (including healthy eating) linked to the need to show their body and the need to share positive emotions from their experiences (e.g., grotts and johnson [cit] ). based on these findings, could we conclude then that se is a predictor or consequence of bi in millennials? our results linked both psychological/personality traits in this generation."
"additionally, data mining techniques and textual analysis have been performed to obtain insights and knowledge from the database. these approaches have allowed us to answer positively to rq3 as the insights obtained identify a psychological profile of the millennial generation according to the ugc chatting on twitter and shows it behavior and habits [cit] ."
"sentiment analysis is a research methodology used to capture and analyze sentiments expressed in a given text sample [cit] . the data are usually gathered in digital environments, such as online platforms or sns. in previous research, sentiment analysis has proven to be able to effectively identify the expressed feelings, which makes this methodology particularly valuable for research on consumer decision making, attitudes and behavior [cit] ."
"by the parameter updating laws (14) and the controllers (15), the derivative of v 2 t and v 3 t along the trajectories of (4)-(7) can be calculated as follows: it follows from hypothesis 1 that the derivative of v 1 t can be written as"
"the proposed method follows an approach in which first step is feature extraction and then these features are used to train and test the transfer learning model. based on the observation, we have concluded that the transfer learning mechanism can be applied to ham10000 dataset to increase the classification accuracy of skin cancer lesions. also, we have found that the resnet model which is pre-trained in imagenet dataset can be very helpful is the successful classification of cancer lesions in the ham1000 dataset. we have further seen that learning algorithms like random forest, xgboost and svms are not very effective for classification tasks in the ham10000 dataset. encouraged by these outcomes, future work will include the improvement of prediction result and classification accuracy."
"the results denoted opt in table 1 describe the upper bound score, where each row represents optimal recall and f-score respectively. the best results are achieved for r-1 with a maximum recall of 57.86%. this is a consequence of hand created gold standard summaries used in the evaluation, that is, we cannot achieve full recall or f-score when the sentences in the gold standard summaries are not taken from the underlying documents and thus, they can never be fully matched using extractive summarization. r-2 and su4 have lower maximum recall and f-score, with 22.9% and 29.5% respectively."
"in terms of the results of phonemic awareness tests between the experimental and the control groups, the experimental group made significantly greater gains in english phonemic awareness than the control group."
"this algorithm training was performed considering several factors, such as the identification of irony, sarcasm and contents related exclusively to the objective of this research. throughout this process, the contents unrelated to #millennials were excluded from the sample and training."
"we have utilized a graphic processing unit built in a graphic acceleration card nvidia (geforce gt 640, nvidia, santa clara, ca) to speed up the image processing in our microfluidic position control system. specifically, we applied cuda, a parallel computing architecture in order to greatly reduce the processing time. such improvement is mainly based on the optimization of (1) memory usage, (2) parallel execution, and (3) instruction usage. the balancing of these three terms directly determines the computation efficiency. instead of implementing an optimized image processing algorithm, which focuses only on reducing the number of processed pixels by more complicated calculations, cuda considers both the cost of algorithms and the number of processing units. [cit] 9 ms (without cuda) to only 452 ms."
"the results contribute to the literature on psychological and behavioral profiling of different social and demographic groups, as well as to the research on ugc opinion [cit] . furthermore, we also propose a novel method, a psychological text-mining analysis technique that can be meaningfully used in further research to study other population cohorts and online communities as well as to topic discovery according to the online chatting on social media."
"in this paper, we have generated a cnn model that analyses the skin pigment lesions and classifies them in using a publicly available dataset by employing various deep learning techniques. we have improved the accuracy of classification by implementing cnn and transfer learning models. we have tested our model using the publicly available ham-10000 dataset 2"
"after data collection, to start the proposed methods first, we developed an lda model, which was based on a probabilistic assumption, assumed that content was generated in the following two steps: the first step required the identification of words and each word should be in an independent document leading to two consecutives steps [cit] . these two steps consisted of randomly identifying the distributions of the topics identified in a sample and selecting the main topics found in that sample [cit] . in real situations, it is difficult to estimate the distribution of topics over the document or the distribution of words over topics in advance [cit] . regarding data extraction, our profile was connected to a public twitter application programming interface (api). the initial sample size was 44,180; however, after the initial screening of the tweets using the inclusion criteria, the final sample size was reduced to 35,401 tweets. during this phase, python software 3.7.0. was used in mac version, which allowed us to collect tweets in english with the following keywords: #millennials, #millennial and # [cit] following [cit] and bologna and hayashi [cit] . as indicated in saura and bennett [cit], the days of data collection in sns can be established according to the parameters designed by the researchers. this procedure can be used if the main objective of the research is exploratory, rather than hypothetical testing and based on social media listening or social media analytics. in addition, for the collection of data, researchers must consider that there are no online events or movements on sns during these days, within the industry studied, that may add noise and hinder the database."
"the methods are named according to: vectortype embeddingmethod similaritym ethod, e.g. w2v_add cos for word2vec vectors combined using vector addition and compared using cosine similarity."
"this research was conducted as an embedded mixed method design to investigate the improvement of the english phonemic awareness of thai grade one students through a multimedia call program while learning the english through the whole word approach. in addition, it aimed to reveal the thai grade one students' views on practicing phonemic awareness through the multimedia call program while learning the english through the whole word approach."
"a recursive neural network (rvnn), see fig. 4, [cit], is a type of feed forward neural network that can process data through an arbitrary binary tree structure, e.g. a the recursive neural network architecture makes it possible to handle variable length input data. by using the same dimensionality for all layers, arbitrary binary tree structures can be recursively processed. binary parse tree produced by linguistic parsing of a sentence. this is achieved by enforcing weight constraints across all nodes and restricting the output of each node to have the same dimensionality as its children."
"to get an upper bound for each rouge score an exhaustive search were performed, where each possible pair of sentences were evaluated, and maximized w.r.t the rouge score."
the first group of configurations are based on vector addition using both word2vec and cw vectors. these vectors are subsequently compared using both cosine similarity and euclidean distance. the second group of configurations are built upon recursive auto-encoders using cw vectors and are also compared using cosine similarity as well as euclidean distance.
"regarding rq1, it has been shown that ugc can be used to identify topics related to millennials as well as other topics [cit] thus allowing the extraction of insights and generation of knowledge about the object of study of the investigation to be used in future investigations. the topics identified have also been classified into three sentiments and two categories, demonstrating that rq2 proposed has been validated. the feelings identified in the topic related to the millennial generation can denote the importance that ugc communities bring to those. therefore, based on these feelings, other research may focus on the specific analysis of expectations created by the ugc on negative or positive indicators shared through twitter [cit] as well as how users participate collaboratively [cit] ."
"as a result of these processes outlined, an important theoretical implication is the study of the 10 topics now directly linked to the study of millennial generation based on ugc that is the most significance contribution of this study (positive topics: body image (bi), self-expression (se), travelers (tr), startups (st), digital life (dl); neutral topics: self-identity (si), anxiety (an) and negative topics: depression (de), loneliness (lo), real-world relationship (rw))."
"the optimal r-1 scores are higher than r-2 and su4 (see table 1 ) most likely because the score ignores word order and considers each sentence as a set of words. we come closest to the optimal score for r-1, where we achieve 60% of maximal recall and 49% of f-score. future work is to investigate why we achieve a much lower recall and f-score for the other rouge scores."
convolutional neural networks have recently been used for the identification of skin cancer lesions. several cnn models have successfully outperformed trained human professionals in classifying skin cancers. several methods like transfer learning using large datasets have further improved the accuracy of these models.
"convolutional neural networks were inspired by biological processes. in these, the connectivity pattern between neurons of a network resembles the organization of the animal visual cortex. the response of an individual cortical neuron in a restricted region of the visual field is known as the receptive field. the receptive fields of different neurons partially overlap such that they cover the entire visual field. cnn is comprised of three stages of neural layers to assemble its structures: convolutional, pooling, and fully-connected."
"where p(q) is the position for the point p to retract. in this way, the random points are adapted around to the obstacle (see figure 1(b) ), then:"
"the results of the present research were consistent with studies of researchers in the fields of phonemic awareness and call programs. [cit], in which there were higher gains after practicing phonemic awareness with multimedia programs. the researcher investigated the effect of multimedia computer programs on increasing american children's phonemic and phonological awareness. their results showed that 36 kindergarten and 36 first grade students improved their phonemic and phonological awareness. this demonstrates that a multimedia call program is effective as a supportive tool for building phonemic awareness. it can be applied to assist not only young learners, but also children with a poor standard of english."
"the scientific world journal 13 using image processing software (imagej, national institutes of health) as shown in figure 8(c) . essentially, these results indicate that the reported microfluidic platform can regulate microenvironments for cell proliferation and can support further parallel cell analyses and monitoring applications using high-throughput microfluidics."
"results of phonemic awareness tests of the experimental group and the control group: the results of the quantitative data showed that the students in the experimental group who were supported by a multimedia call program in combination with the use of the whole word approach could improve their english phonemic awareness. the students in the experimental group obtained higher scores on three posttests of phonemic awareness tests compared to the pretests. this revealed that phonemic awareness appears to be improved by a supportive tool, such as a multimedia call program. according to table i, for the phonemic awareness test of /k/ and /g/, the posttest score of the experimental group was higher than the pretest score at the .001 significance level. conversely, the posttest score of the control group was only slightly higher than the pretest score. that is, there were no significant differences in the posttest scores of the control group."
"firstly, there are many problematic consonant pairs of english that thai efl learners find difficulty in perceiving, discriminating, and pronouncing. consequently, more research should create a multimedia call program or other materials to improve other problematic consonant pairs -not only the three pairs of /k/ and /g/, /f/ and /v/, and /s/ and /z/."
"lastly, this study used semi-structured interviews as one of the research instruments to obtain the views of nine students towards developing phonemic awareness through a multimedia call program while learning the english language through the whole word approach. it is suggested that further study should be conducted with a greater sample size which may help to gain more credible and detailed information regarding grade one students' views."
"based on lyapunov stability theory, this paper has discussed the dynamic principles of mammals' circadian rhythms, where the suprachiasmatic nucleus (scn) of the hypothalamus was modeled by a goodwin oscillator and the vasoactive intestinal polypeptides (vip) was modeled by a van der pol oscillator. considering that it is very difficult to get the exact values of the system parameters, this paper has proposed effective parameter updating laws to identify the unknown parameters. the result has been proved based on strict theoretical reduction, and it should have a theoretical advantage over the previous results, which were based on the statistical method or experimental data. another contribution of this paper is the problem of outer synchronization under adaptive control. noticing that the coupling manner is different from the widely accepted coupling of the classical complex network, this paper has designed targeted adaptive controllers to synchronize the drive goodwin oscillator network and the response one. the effectiveness of the obtained results has been verified both theoretically and numerically."
"vgg-16 is a convolutional neural system that is prepared on more than a million pictures from the imagenet database. the system is 16 layers profound and can arrange pictures into 1000 item classifications, for example, console, mouse, pencil, and numerous creatures. accordingly, the system has learned rich component portrayals for a wide scope of pictures. the system has a picture info size of 224-by-224. the model accomplishes 92.7% top-5 test precision in imagenet, which is a dataset of more than 14 million pictures having a place with 1000 classes."
"after analyzing the results, we have divided the topics that characterize millennials into the following two categories topics related to psychological/personality traits and topics linked to attitudes and behaviors. it should be noted that this research has an exploratory based approach on the online chatting about millennials on twitter. the insights presented as findings are grouped into the knowledge discovery science approaches that should be studied in future research to perform hypothesis testing and quantitative tests [cit] ."
"the millennial generation needs to feel that it is connected to the internet, it is their livelihood and they need to share their experiences through this medium. 0.28 303 the main social networks used by the generation are, in the descending order of importance, youtube, instagram, facebook and twitter."
the millennial generation needs to feel accepted by their followers on the internet. social networks are the channel through which they constantly demonstrate their social identity.
"convolution neural networks and transfer learning methods are used for the classification task. for the purpose of transfer learning, deep learning models pre-trained on imagenet dataset was used. it consists of a little more than 14 million labeled images belonging to more than 20,000 classes. later on, these pre-trained models are further trained on the ham10000 dataset by adding some additional layers to the models and freezing some of the initial layers. we also applied different learning algorithms like xgboost, svm and random forest algorithms to perform the classification task in the ham10000 dataset to compare the reuslts"
"the opinosis dataset [cit] consists of short user reviews in 51 different topics. each of these topics contains between 50 and 575 sentences and are a collection of user reviews made by different authors about a certain characteristic of a hotel, car or a product (e.g. \"location of holiday inn, london\" and \"fonts, amazon kindle\"). the dataset is well suited for multidocument summarization (each sentence is considered its own document), and includes between 4 and 5 gold-standard summaries (not sentences chosen from the documents) created by human authors for each topic."
"the second model is more sophisticated, taking into account also the order of the words and the grammar used. an unfolding recursive auto-encoder (rae) is used to derive the phrase embedding on the basis of a binary parse tree. [cit] and uses two rvnns, one for encoding the compressed representations, and one for decoding them to recover the original sentence, see figure 6 . the network is subsequently trained by minimizing the reconstruction error. forward propagation in the network is done by recursively applying eq. 5a and 5b for each triplet in the tree in two phases. first, starting at the center node (root of the tree) and recursively pulling the data from the input. second, again starting at the center node, recursively pushing the data towards the output. backpropagation is done in a similar manner using backpropagation through structure [cit] ."
the millennial generation feels the need to show their body to be accepted by their closest circle of friends. 0.37 470 healthy habits and healthy food consumption are necessary to feel the feeling of belonging to a group
"according to table ii, there were significant differences in the scores regarding three phonemic awareness tests for (/k/ and /g/), (/f/ and /v/), and (/s/ and /z/) between the experimental and control groups stood at the .001 level of significance. additionally, the posttest score was higher than the pretest score, with the statistically significant theory and practice in language studies 2297 difference at .001. it shows that the students in the experimental group got higher scores in the three posttests than the students in the control group. as a result, phonemic awareness can be enhanced through a multimedia call program. the results were in accordance with three research studies, in which there were greater gains after training with multimedia program. [cit] . forty-two kindergarten students were assigned to learn with phonemic awareness software; however, another 34 students received no training. the study revealed that the students in the treatment group had better scores on the posttest than the students in the control group."
"additionally, companies that develop projects in the field of sociology or the study of society in general, can use our results to establish patterns and models that characterize the millennial generation as a segment of the target market."
"in the first period, the participants in both the experimental and control groups took the pretest to measure their english phonemic awareness of the english consonant pair /k/ and /g/. afterwards, from the second to the fourth period, the experimental group practiced phonemic awareness of the pair /k/ and /g/ through a multimedia call program, consisting of phoneme isolation, phoneme identity, and phoneme categorization. additionally, they filled out worksheets after practicing phonemic awareness through a multimedia call program for each period. next, in the fifth period, the researcher asked the experimental group to review three lessons of phonemic awareness of the pair /k/ and /g/ through a multimedia call program and played game in the activity room. then, in the sixth period, the participants in both the experimental and control groups completed the posttest of english phonemic awareness in consonant pair /k/ and /g/ and took the pretest of english phonemic awareness of pair /f/ and /v/. from the seventh through the sixteenth period, students in the experimental group repeated activities in the english consonant pairs /f/ and /v/, and /s/ and /z/. additionally, in the eighteenth period, nine participants, consisting of three participants from each of the good, fair, and poor groups, were randomly selected to join a semi-structured interview."
"to demonstrate potentials of the automated parallel monitoring strategy in cell research applications, we cultivated balb murine embryonic fibroblast cells (3t3) in all of the 32 chambers in the microfluidic device over one week until a confluent cell density was observed in all of the chambers. cells were first seeded into each chamber and maintained in the growth condition (37 ∘ c and 5% co 2 in humidified air) for 8 hr to facilitate the cell attachment and spreading. during the cell culture, media in chambers were replaced for every 4 hr. images were then taken at all of the chambers regularly. as described in figure 8 (a), we adopted a position-shifting scheme between the alignment mark and the chamber center in order to maximize the regions of the chamber for the automated imaging. in more details, the motorized stage would first align with the corresponding target alignment mark (green circle) with the aid of the presented automated alignment scheme. the observation position would then shift from the alignment mark to the chamber center for 1.2 mm in both vertical ( −) and horizontal ( −) directions of the microfluidic device (red arrow), followed by the imaging step. it should be mentioned that this single step of stage movement with a short distance did not induce any significant misalignment during the implementation. the observation position then shifted back to the alignment mark (blue arrow) for the alignment with the next selected chamber. as a representative result of the cell growth in the microfluidic device, microscopic images of the cell population in a chamber at different time points during the experiment are shown in figure 8(b) . images for cell growths in all of the 32 chambers of the microfluidic devices are also available in figure s4 . results indicated that cell growth could be maintained in the microfluidic device for at least 7 days. we quantified the cell densities at different time points (each with 5-20 repeated experimental values)"
"in order to have a high similarity between sentences using the above measure, two sentences must have an overlap of highly scored tf-idf words. the overlap must be exact to count towards the similarity, e.g, the terms the us president and barack obama in different sentences will not add towards the similarity of the sentences. to capture deeper similarity, in this paper we will investigate the use of continuous vector representations for measuring similarity between sentences. in the next sections we will describe the basics needed for creating continuous vector representations and methods used to create sentence representations that can be used to measure sentence similarity."
"the goal of summarization is to capture the important information contained in large volumes of text, and present it in a brief, representative, and consistent summary. a well written summary can significantly reduce the amount of work needed to digest large amounts of text on a given topic. the creation of summaries is currently a task best handled by humans. however, with the explosion of available textual data, it is no longer financially possible, or feasible, to produce all types of summaries by hand. this is especially true if the subject matter has a narrow base of interest, either due to the number of potential readers or the duration during which it is of general interest. a summary describing the events of world war ii might for instance be justified to create manually, while a summary of all reviews and comments regarding a certain version of windows might not. in such cases, automatic summarization is a way forward."
"the millennial generation is an active generation, travels a lot and likes to share their experiences in social networks. 0.29 310 the search for adrenaline and emotion is the basis of the adventure spirit of the millennial generation, above the price or the cost of the trip."
"ρ(β 1:k, θ 1:d, z 1:d, ω 1: consequently, the identification of the topics and words was conducted using gibbs sampling using equation (see saura and bennett [cit] and jia [cit] for details of the process). python software lda 1.0.5 was used for the estimation."
"as special cases, when some of the three unknown parameters are given and fixed, theorem 1 remains valid. for example, assuming that the parameters α 1 and α 2 are both determined, one gets the following corollary."
"in this section, a special case of the system (4)- (7) is given to illustrate the effectiveness of the three statements of theorem 1 one by one."
"we conducted experiments to demonstrate the distributions of circle (, ) using the two template images: (1) only a white circle and (2) a white circle with one-pixel thick gray boundaries around the circle. we first computed the circle maps for the template-matching on an ideal alignment mark, which contained three circle patterns identical to the whitecircle template pattern (upper maps in figure 4(b) ). both template patterns could precisely identify the circle pattern locations. the gray-boundary template induced transition regions around the maximum circle values. we further tested performance of the template-matching scheme on the microscopic image shown in figure 4 (a). it can be observed that using the white-circle template could induce multiple bright pixels around the circle centers in the circle map. this characteristic made a precise position detection of the circle patterns to become challenging. for the case using the gray-boundary circle, the outcome circle map showed a more reasonable profile of the values where the brightness regions were all at the circle centers. hence, the gray-boundary template image should achieve a more robust position identification of the circle patterns using the template-matching approach."
"on the other hand, nine participants were asked the question \"which one do you like the most -learning through the enjoy the sounds! program at the computer laboratory or learning english in the classroom? why?\" all participants chose learning with the enjoy the sounds! program at the computer laboratory since they could learn by themselves and it was more attractive. all of the responses are shown below:"
"each summary is evaluated with rouge, that works by counting word overlaps between generated summaries and gold standard summaries. our results include r-1, r-2, and r-su4, which counts matches in unigrams, bigrams, and skip-bigrams respectively. the skip-bigrams allow four words in between [cit] ."
"in this study, an online review or comment was defined as a piece of text that describes an experience about a specific product, service or topic generated by a sns user in a public profile on the internet [cit] . according to several researchers, by studying this type of ugc on the internet, a solid and powerful relationship can be built, thereby yielding robust results [cit] ."
"training is achieved by minimizing the network error (e). how e is defined differs between different network architectures, but is in general a differentiable function of the produced output and"
"after all the tweets were downloaded, some tweets were deleted due to repeated content, news, or retweets [cit] . the images and multimedia files that were published along with the text of the tweet were excluded from the analysis [cit] . following saura and bennett [cit], the sample of tweets was considered valid according to the following criteria: (i) active twitter profiles, i.e., with activity during the three months after the use of the indicated hashtags; (ii) twitter user profile with a profile photo and a cover photo; (iii) retweets from the same tweet using the indicated hashtags were removed, as such tweets were considered to be duplicate content; (iv) only public profiles and tweets in english using #millennials, #millennial and #millennialgeneration were included and (v) tweets should have been at least 80 characters long with spaces and had to have the hashtag as indicated. (i.e., #millennials. tweets without the \"#\" or with a wrong label like \"#milennials\" were omitted)."
"in the interest of comparing word embeddings, results using vector addition and cosine similarity were computed based on both cw and word2vec vectors. supported by the achieved results cw vectors seems better suited for sentence similarities in this setting."
"since the function f x, α 1, α 2, β of the network (4) is differentiable and the oscillator is a damped oscillator, there exists a positive constant l satisfying hypothesis 1 in theory."
"an interesting point that we can observe is that when increased distance values, the vloci algorithm is not affected. this fact can be explained because this algorithm uses the weighted average technique. vloci puts more weight in small distance values while putting less weight for higher distances, which means that vloci is capable of keeping its performance constant even with different distances between the target vehicle and its neighbors."
"overall, the results support that our method can be used to circumvent the real-time position estimation problem in vanets using fewer vehicles than the vloci algorithm."
"we can notice that when the number of vehicles increases to 3, according to rmse values, the covalid algorithm had a slight decreased in its accuracy, which can be explained due to the use of random distance among vehicles, as well some obstacles and turns during the trajectory. these factors can affect our proposed solution since it assumes that the distance information is perfect. in other words, it does not take into account noise in distance information, which is not true in real-world scenarios. furthermore, the accuracy of the distance information depends on which sensor is used."
"the feed forward network, consisting of multi-layer perceptron (mlp), probabilistic neural network (pnn), radial basis function (rbf), and generalized regression neural networks (grnn) is the commonly used ann model in remote sensing application [cit] . the neural networks can be expended using multiple different training algorithms, but they need larger computational resources."
"this section aims at evaluating the covalid, vloci, and gps methods in real-world scenarios. thus, we used three different scenarios: downtown, highway, and neighborhood. the figure 9a shows downtown toronto, which we took as scenario the most famous streets with a considerable amount of traffic, such as dundas st., yonge st., church st., queen st. and bay st. in order to use a highway scenario, we chose highway 401 (seen in figure 9b ) which has heavy traffic once it does not charge tolls. finally, as the neighborhood scenario, we took into account the one named windfields farm that is close to the uoit north campus, as we can notice in figure 9c ."
coastline indicators must have the ability to represent schematically but correctly the overall coastal state from the point of view of its sedimentary evolution [cit] .
"on the other hand, some approaches use a cooperative positioning (cp) technique. these approaches benefit from using vehicle-to-vehicle communication (v2v), in which nearby nodes exchange information about their positions and the relative distance between them and their neighbors [cit] ."
"as aforementioned, each vehicle sends its gps position along with the distance information every second. thus, the target vehicle, when it receives the neighbors' information can perform a weighted average on its gps position and use the sensor's distance information along with the similarity of the triangles method for each pair of vehicles. it is worth mentioning that in this work, we are focused only on distance information that is given by sensors, such as cameras, lasers, or radars. hence, how these sensors gather this information is not our focus."
"the frost filter has the advantage of smoothing homogeneous areas, and preserving edge and transition lines. however, the micro strokes are also smoothed [cit] ."
"the performance of artificial neural networks is well established. however, they require the extraction of a feature vector. the convolutional neural networks (cnn) make it possible to abstract this step of feature extraction, by taking images as input. currently, cnns are widely used for deep learning algorithms."
it is the end of the swash at high tide and during the ebb tide; it migrates to the sea and marks the land side limit of the sands darkened by the breaking of a wave. [cit] high water line it is defined as the level of the last high tide and therefore corresponds to the upper wetting limit of the foreshore by the previous open sea. the instantaneous high water line is commonly mapped on aerial photographs as the shoreline proxy because it is easily identified.
it is also noted that some articles focus on the algorithmic approaches used while others merely present results without providing sufficient details on how they were obtained.
"furthermore, since the rmse and mae values demonstrated similar behavior, we will only use the rmse values to assess the accuracy of the vloci, covalid, and gps regarding the impact of increasing the number of vehicles, the distance information error, and the distance among vehicles in simulation scenarios. from now on, we are using our proposed solution covalid along with the weighted average from section 3.3. also, as before, all graphs presented were plotted with a 95% confidence interval."
"hence, we detailed the impact of the sensors used to provide distance information, and the results presented in this section suggest that either the trajectories and the noisy distance information can affect our proposed solution in some way."
the ability of very high resolution optical satellite images to discern geometric objects in the landscape has led many studies to consider the use of morphological segmentation for shoreline detection.
"seaward-most edge of hardening structures on beaches with hardening structures, a tree kind of indictor may be used: the seaward most edge of hardening structures, the landward edge of shoreline protection structures and crest of the shore-protection structure. these reference lines are not able to show shoreline evolution in this type of beach since they are intended to freeze the shoreline and can be modified at any time [cit] [ 38, 39] landward edge of shoreline protection structures [cit] crest of the shoreprotection structure [cit] berm crest"
"overall, of all three sensors, the higher cost is the laser, whereas the cheapest are the cameras, and both require a massive amount of data since they use 3d environmental representation. hence, they are considered a high computational cost solution. on the other hand, radars cannot reach the resolution for object identification, but they can detect it."
"golestan [cit] demonstrated two data fusion methods using gps data, wheel sensors information, gas and brake pedal, and v2v communication. in the first method, the authors measured the belief of each vehicle related to its current location using the extended kalman filter (ekf). in their second method, they extended a particle filter (pf) to weight the particles equivalent to the beliefs. their results show a high accuracy of 1.65 m of mean absolute error (mae) values. however, the methods can only reach that accuracy when the number of vehicles is at least five."
"edges are significant local changes of intensity in an image. they typically occur on the boundary between two different regions. an edge is defined by a \"fast\" variation of the grey-level, the color or the texture function of an image."
"where (x, y) and (x, y ) are respectively the perfect and estimated vehicles' positions, while the latter varies between gps, vloci, and covalid. furthermore, we used the mean absolute error (mae) as a metric to evaluate our method since some works in literature [24, [cit] also use it to assess their results."
"the use of methods based on texture features is a common task in classification for shoreline detection. with the arrival of very high-resolution imagery, the details that were unremarkable in the decametric resolution images are clearly perceived. therefore, these images require more complex processing; hence the development of a new approach that focuses on the objects of the image and not on the pixels. in this way, object based-classification is introduced."
"the vegetation line is a natural line formed by the plants in the beach. it is easily identifiable, even on older photographs that cannot be used for beach toe identification."
"boak and turner [cit] listed 45 coastline indicators used around the world for coastal monitoring studies. a sketch of the spatial relationship between the most commonly used shoreline indicators is shown in figure 2 . a compilation of these indicators, their description and some references where they are used are given in table 1, table 2, table 3, table 4 and table 5 . the different kind of indicators represented in this table are organized into seven types: geomorphological reference lines, figure 1 . schematic typical beach profile, terminology and zonation [cit] ."
"texture analysis groups together a set of techniques allowing quantifying the different grey-levels present in an image in terms of intensity and distribution in order to calculate a number of parameters characteristic of the texture to be studied. it is a very important task, which is useful for image segmentation and object detection."
"a kalman filter or one of its derivatives can be a suitable method to perform data fusion. the kf is used as a filtering component based on an iteration process that is divided into two phases: a prediction and an update phase [cit] . moreover, it is an optimal linear estimator for gaussian noise. also, it can be used even with non-linear systems due to its variations such as the extended kalman filter (ekf) that can linearize the problem by calculating its partial derivative. due to our proposed solution nature, we implemented an ekf, and it is fed by both the gps coordinates, corrected by equations (10) and (11), and the sensor distance information."
"another known technique used to decrease localization error is data fusion [cit], which combines location information from different sources to generate a more precise result. in these solutions, data from gps, geographic information systems (gis), sensor information, and other sources can be combined using techniques such as particle filter (pf), kalman filter (kf), or even in linear transformation to estimate more precisely the vehicle's location [cit] . nowadays, vehicles come with vehicular safety systems which are composed of several associated sensors, such as cameras, radars, and lasers, to mention a few. so, data fusion techniques can fuse all of this additional information to minimize the gps error."
"it is obvious to remark that a high number of methods have been developed using landsat data. this is due to the accessibility of these data, which are available from the uscg web site. in addition, landsat images cover all the areas of the earth and allow diachronic studies over a long period."
"it is important to note that our solution focuses on gps inaccuracies, and not on gps outages since we still need the (possibly inaccurate) position of the vehicles to apply covalid."
"moreover, shoreline detection is not a simple task that can be executed using a single image processing technique, but rather it is a complex mechanism that requires the use of several techniques."
"shoreline detection process involves image segmentation into regions but also requires edge detection, since the shoreline is naturally an edge. table 6 . summary of segmentation approaches."
"it is important to mention that the bigger e gps, the bigger the gps distance information error. thus, our covalid solution is directly affected since gps distance information is used in equation (5) results. these results are a key feature in our proposed solution since it is used to compute the new estimated vehicle position through the concept of similarity of the triangles. however, in our solution, this problem is minimized, as shown in section 4.3.2."
"overall, we can notice that all tested approaches presented similar behaviors for both trajectories simulated. as we can see in figures 13c and 14c, on average in both axes, covalid had the best performance when compared to vloci, and gps. on the other hand, vloci was able to overcome gps only in highways scenarios. however, it is worth mentioning that the covalid approach is dependable on the high quality of sensors information about distance among vehicles."
"another interesting gps assisted method is presented by farhan [cit], where the authors proposed the vloci algorithm. similar to our solution, vehicles exchange gps position information. also, they assume that all vehicles are capable of measuring the distances among themselves. they also consider that vehicles are traveling in one lane and following the same direction. thus, the distance information is used to improve the position only in one axis. on the other axis, they assume there is no error since vehicles are moving in a straight-line trajectory. after the gps data exchange, the vloci algorithm is executed, and a set of neighbors coordinates is computed. a weighted average technique is applied to use the more reliable information from closer vehicles while giving less priority to further vehicles. as a result, the best mae value was of 2.38 m, and at least 5 vehicles are needed to reach this accuracy. it is worth mentioning that, despite its limitations, the vloci is a state-of-the-art localization technique that uses only gps and v2v communication [cit] . for this reason, we chose the vloci algorithm to compare with our proposed solution. algorithm 1 summarizes more clearly how the vloci algorithm works."
"vehicular ad hoc networks (vanets) require precise localization information, mainly in critical safety-based applications, such as driverless vehicles and blind crossing [cit] . however, precise location is a drawback yet that needs to be addressed [cit] . to deal with this problem, vehicles are commonly equipped with global positioning system (gps) devices that provide location information [cit] . however, the accuracy of gps information can be affected by dense urban areas, such as urban street canyons and indoor parking lots, because of the absence of direct satellite visibility, which turns the gps into an inaccurate instrument to provide precise location information [cit] ."
"as future work, we will test our solution using different bayesian statistical models. furthermore, we will evaluate our proposed solution combining covalid with vehicle-to-infrastructure (v2i) communication, where vehicles can communicate with a roadside unit (rsu). we also will extend our solution to solve the gps outage problem in scenarios where gps signal is not available, such as in tunnels."
"traditional classification methods are based only on the pixel value. for this reason, they use only the spectral information provided by the pixel and do not take into account the spatial organization of these pixels. a classification can be supervised or unsupervised."
"moreover, all these methods try to combine existing process and adapt them to a specific problem. their fundamental difference lies in the point of view of the target application. some methods are limited to show an application on a specific satellite; more general ones propose solutions to a specific kind of image."
"to study the impact of gps error regarding the accuracy of the tested solutions, we varied the gps error parameter by 1, 5, and 10 m, respectively."
"the vegetation line is a biological indicator of the limits of regular flooding by high water and therefore it represents a nearly ideal indicator of shoreline movement [cit] . [22, 30, [cit] line of permanent (stable, long-term) vegetation"
"to evaluate our proposed solution, we conducted an analysis using the root-mean-square-error (rmse) method, described in (equation (21)). this metric is commonly used to measure the error of the localization approaches."
"the ekf prediction phase uses the information from the last time step to produce an estimated state at the current time step, as seen in equations (13) and (14):"
"mapping the seabed using acoustic waves such as sonar is currently the only technique that can be used without depth limitation. however, swell conditions and tidal currents can make embedded operations difficult to realize."
"where (x i, y i ) and (x j, y j ) are respectively, the inaccurate position of vehicles v i and v j, given by the gps, and d i,j is the distance between them."
k-means/isodata k-means and isodata are the most popular classification methods. they are easy to implement and they give good results when they are applied to images in which the different regions are easily separable [cit] neural network
"we evaluate the accuracy of our proposed solution related to the impact of three different aspects. first, concerning the number of vehicles, to verify the behavior of the presented solution, we used multiple increasing values. second, to evaluate the impact of the trajectory on the accuracy over the tested approaches, it was divided into two parts, straight-line and curve. finally, to verify how the noise in distance measurements can affect the proposed solution, we evaluated the impact of distance information error."
"the region growing methods are fast and conceptually simple, but they are very sensitive to the distribution of the objects in the image. [cit] watershed transform"
"thus, the coordinates are estimated by the ekf and they sometimes can result in an off-road position. so, they need to be adjusted according to the road boundaries."
"to evaluate the impact of the distance between two vehicles in the rmse and mae values, we kept the gps error at 2 m and increased the distance between them. the distance values used in this scenario were: 11.8, 23.7, 35.6, 47.5, and 59.4 m. all graphs presented in this section were plotted with a 95% confidence interval. figure 8a,b show the rmse and mae values of the average of both axes. we can notice that covalid is directly affected when the distance between neighbors increases. however, our proposed approach had better performance when compared to the vloci for vehicles near the target. although, for long distances between the vehicles, more specifically when the range is greater than 35 m, the vloci overcomes our proposed solution."
"the median filter replaces each pixel in the image with the median of those values within the moving kernel. it is always applicable to optical images, as well as lidar data [cit] ."
"to adjust the vehicle position, we compare the new estimated vehicle position computed by ekf with the path geometry of the road. we use map information to restrict the estimated vehicle position onto the identified road. moreover, we assumed that our proposed solution has access to a digital road map. thus, we can verify if the vehicle's estimated position is within the road limits. if that is not the case, the algorithm shifts the vehicle position to the nearest point onto the road."
this section aims at analyzing and assessing the sensors that are suitable to provide the distance information in all tested scenarios. we used the sensor's specifications provided in the literature [cit] . the used parameters and their respective sensors are described in table 7 .
"to tackle this drawback, there are some solutions proposed in the literature that use anchor nodes [cit] . in these approaches, anchor nodes are aware of their positions, so the other nodes can measure their distances using the anchor nodes as references to compute their relative positions [cit] ."
"for having a good shoreline detection method, it is necessary to evaluate the existing ones, to know their drawbacks so that we will be able to propose better approaches. shoreline detection can be achieved using different processes."
"when compared straight-line against turning trajectory in the downtown scenario, in the x-axis, as depicted in figures 13a and 14a, we can notice that covalid had better accuracy in the straight-line trajectory. the same occurred in the highway scenario, but with a just slightly better result when compared to the turning trajectory. on the other hand, in the neighborhood scenario, the turning trajectory had almost the same performance as in a straight-line scenario. in the y-axis, we can notice that the behavior of covalid in a straight-line trajectory was the opposite presented in the x-axis. as shown in figure 13b, the rmse values show that in the downtown scenario, the covalid performance decreased, whereas, in both highway and neighborhood scenarios, the accuracy was improved. regarding the vloci algorithm, only in highway scenarios, it can overcome the gps accuracy. surprisingly, in y-axis simulations and using trajectory with turns, the accuracy of covalid was improved, as shown in figure 14b . it can be explained because usually, the vehicle position given by gps does not lie in the same line as the distance information provided by sensors which implies in an automatic triangle rotation when triangle similarity concepts are performed."
"our results show that our solution can minimize the average error between the perfect position and the position given by gps by 63%. in addition, our solution can estimate the node position better than when compared to the state-of-the-art vloci algorithm, in all real-world-tested scenarios using a fewer quantity of nodes as verified by rmse and mae values, in section 4.4.1. thus, the increasing of the number of vehicles did not severely affect our proposed solution due to the weighted average method applied in covalid. moreover, it is noticed that in a straight-line trajectory covalid presented a better performance in the highway scenario, in the x-axis, whereas, in the y-axis, our proposed solution had similar rmse values for all three tested scenarios. it is important to note that our solution focuses on gps inaccuracies and not on gps outages. furthermore, the results in section 4.4.3 support that covalid can be affected by the distance information error, which is provided by sensors, such as radars, lidars, and cameras. lastly, we presented an exploratory analysis of these three sensors, which describes the advantages and drawbacks of each sensor and how they could be used in different scenarios."
principal component analysis the pca allows the use of smaller databases and reduction of noise [cit] object-oriented classification it reduces salt-and-pepper effects commonly noted in pixel-based remote sensing image classification. [cit] texture analysis-based methods
"in this section, we divided the target vehicle trajectory into two parts: when vehicles are in a straight line or when they are in a turning scenario. in addition, we kept the gps error constant at 2 m. we also used two vehicles, and the distance between them was set at 5 m apart. thus, we can evaluate the impact of the vehicle trajectory regarding the accuracy of tested approaches in real-world scenarios."
"to evaluate the behavior of our proposed solution, we have used the simulation of urban mobility (sumo) [cit] for scenario construction, omnet++ [cit] along with veins framework [cit] for vehicles communication and python scripts for statistical computing. hence, it was possible to define all vehicle mobility and all vehicular network parameters according to the ieee 802.11p standard. the parameters used in the simulations are described in table 2 . concerning the network topology, we took into account that all vehicles are inside their communication range. thereby, each vehicle is capable of communicating with each other. thus, vehicles can exchange both their location information given by gps and the sensor distance information. when one vehicle receives this information, it can start the computation process by constructing the needed matrices and computing the proposed method covalid."
"as we can see in figure 10a -c the vloci and gps had the same value as explained in section 4.3.1. furthermore, we can notice that the covalid had its best performance regarding the x-axis in the downtown scenario, while in the highway, it performed with accuracy almost constant, as well in neighborhood scenario, except when increased the number of vehicles for 10. the best accuracy in the downtown scenario can be explained because, in a highway scenario, the vehicle velocity is higher. hence, the higher the velocity, the more affected is our proposed approach in the x-axis."
"the sensor readings are expressed as the measurement matrix h k . however, the relationship between the measurements and the state vector is required. to meet that requirement, we can observe two interesting points. first, the gps measurements have a linear relationship with the state vector, since gps provides the coordinates of both axes. second, the distance of the sensor measurements is gathered in polar coordinates, which means that we need to convert them from polar to cartesian coordinates in the matrix below."
"finally, analyzing the advantages and disadvantages of each sensor cited above, it is clear that it would be possible to combine the outputs of all three sensors in a data fusion approach to achieve a high level of accuracy since their efficiency depends on their field of view."
"when applied to images in which the different regions are clearly recognizable, the classification techniques mentioned above give good results. when neighbouring regions overlap, fuzzy classification methods should be used. they allow processing inaccurate, uncertain or redundant data."
"in this section, we kept the gps error constant at 2 m, while the number of vehicles was increased from 2 to 10. furthermore, both the distance among vehicles and vehicles' velocities were set randomly. to evaluate the performance of each technique, we took into account both the rmse values regarding the x-axis, the y-axis, as well the average between both axes."
"where (x) and (x ) are respectively the perfect and estimated vehicles' coordinates, while the latter varies between gps, vloci, and covalid. in equation (22), we compute the mae for one axis to simplify the explanation. however, it is suitable for as many axes as necessary."
"in this work, we propose a novel location data fusion technique, called cooperative vehicle localization improvement using distance information (covalid), that cooperatively gathers gps and distance information from nearby vehicles to improve their locations. our covalid solution is an extension and improvement over our previously proposed bound algorithm [cit] . in this current work, we are using a weighted average model in gps positions to put more confidence in distance information provided by vehicles closer to the target. thus, we take advantage of these extra sensors to propose a distance-based data fusion technique to improve the localization provided by gps. also, we have applied a set of equations based on the concept of congruent triangles. these equations work with information about the difference between both the sensor and the gps distance. to perform data fusion, we use an extended kalman filter (ekf) that is fed by results from these equations. also, we used road constraints to adjust the positions of the vehicles on the road by using only a single anchor node. thus, we can estimate the new vehicle position through the proposed ekf model."
the water line is the interface between the body of water and the slope of the beach. it refers to the limit of the foam of the swash (the rush of seawater up the beach after the breaking of a wave).
"if the lighting is not uniform or the different objects in the image have different luminance values, global thresholding is no longer appropriate. for these images, local or adaptive thresholding is better suited. unlike the global methods that consider the value of the pixel, the local methods take into account the value of neighbouring pixels for the calculation of thresholds."
"the cameras can also be used as devices that provide reliable distance information since its accuracy is around 0.01 m, and also they have higher update rates when compared to the other sensors. although, it is a range-limited sensor that operates in a range of up to 10 m, as described in table 7 . furthermore, its accuracy can be affected when it is exposure to lights, mainly sunlight, and even at night when the other vehicles' lights can interfere in the camera's performance. in our simulation results, we can notice that cameras can have good accuracy since the distances among the target and the neighbors are up to 10 m. also, the drawback situation with lights was not tested."
"also, y-axis overall, we noticed that in both downtown and neighborhood scenarios, the vloci behavior was affected similarly as covalid, whereas in highway scenario the vloci kept its accuracy almost constant due to the distance measurement model used in vloci algorithm along with vehicles' skewed position treatment. another interesting point is that in downtown scenario was also the worst covalid performance as expected since the buildings and other obstacles can affect the sensors' measurements. the rmse values on average of both axes, seen in figure 15a -c can summarize the behavior of the tested approaches. overall, we can notice that the best accuracy was reached in the highway scenario that is due to its characteristics: a scenario with no buildings or obstacles, and mostly a straight-line scenario. moreover, results suggest that the ekf works well using the velocities of the vehicles in this scenario. using the straight-line trajectory, we can notice that, according to figure 16a -c, in both downtown and highway scenarios, the covalid improved its performance due to two reasons. first, because of the trajectory characteristics (a straight-line). second, because the ekf deals well with noises in distance information in these scenarios along with higher velocities, as seen in the highway case. however, as expected, in the neighborhood scenario, the covalid had the worst performance due to the lower vehicles' velocity. from trajectories with turns, we can observe, according to figure 17a -c, that covalid presented the same behavior as in the highway scenario, improving its performance. whereas, in the downtown scenario, its accuracy was affected by the scenarios' characteristics such as obstacles, lower vehicles' velocity, and sensors' field of view. on the other hand, in the neighborhood scenario, the covalid kept rmse values almost constant. it can be explained due to the combination of lower vehicles' velocities and scenario characteristics."
"it is the line of maximum light intensity. like all virtual reference lines, this line is a digital reference line resulting from image processing [cit] shoreline extracted from colour and luminance distinction on colour averaged video images these features represent an average position of the instantaneous shoreline for about ten minutes. [cit] skeleton of beach it corresponds to the median line of the form described by the contours of the beach circumscribed by the vegetative limit or the foot of the dune and the wetting line of foreshore or \"visible high seas\" [cit] ."
the advantages of this technique are simplicity of integrating two images and good for highlighting urban features. its drawback is that it does not retain the radiometry of the input multispectral image [cit] .
"the use of a median filter window smoothens the image while enhancing the edges because some pixels have high grey-level values compared to their neighbours. it is effective in removing white noise, while preserving sharp shoreline edges."
watershed transform is fast in computation time but often provides a very large number of regions that will be merged to obtain a correct segmentation of the objects in the image. [cit] wavelet transform
"in this paper we show a compilation of shoreline detection methods based on optical remote sensing images. first, we list the indicators presented in the literature and then the different methodological approaches used are presented before conducting a comparative analysis of these different approaches."
"remote sensing is the technique of obtaining information about objects or areas from a distance, typically from aircraft or satellites [cit] . it is increasingly used in coastal monitoring insofar as it"
"another interesting point is that when increasing the number of vehicles, the covalid performance improves due to the use of the weighted average method of nearby vehicles' positions. also, the results suggest that covalid can be used as a solution for localization problem aided by gps in all tested scenarios, except when the number of vehicles is increased to 10 in neighborhood scenario. in this particular case, the rmse values, as seen in figures 10c, 11c and 12c, show that covalid had the worst performance due to the 10th vehicle being farther to the target and as a consequence, its distance information become noisy, since in neighborhood scenarios there are only one-lane streets and sometimes the 10th vehicle is not even in the same street as the target vehicle. however, on average of both axes, as seen in figure 12a -c, results suggest that our proposed solution is suitable for all tested scenarios. however, in the neighborhood scenario, covalid presented limitations on its performance, when used with 10 vehicles. number of vehicles (highway scenario) (b) rmse in x-axis-highway scenario. number of vehicles (downtown scenario) (a) rmse in y-axis-downtown ccenario. number of vehicles (highway scenario) . rmse values in y-axis in downtown, highway, and neighborhood scenarios-regarding the increase in the number of vehicles. number of vehicles (downtown scenario) number of vehicles (highway scenario)"
"in this work, we are taking into consideration that near vehicles have related gps errors. although the different brands of gps receptors do result in different errors, it is known that they are spatially auto-correlated, which means that vehicles in similar locations have similar errors [cit] . however, it is worth to mention that real-world errors were introduced in our simulation environment to model the difference in gps receivers brands."
"moreover, all simulations in this section were conducted using 10 vehicles with both distance and velocity set randomly, 2 m of gps error, and the scenarios were divided into random, straight-line, and trajectories with turns."
"we can note that the shoreline detection problem has still not been adequately solved since there are no algorithms that can be used regardless of the application or type of image. moreover, for a particular application, the choice of an algorithm is problematic because there is neither a theory established for this purpose nor an index of comparison between the different methods. an existing method is often adapted to a particular application. these are then \"ad hoc\" methods whose performance is difficult to evaluate outside their specific application."
"the thresholding methods are simple and fast, but they have often been developed to treat the particular case of segmentation of panchromatic (pan) images into two classes and are not sufficient for multispectral (ms) and hyperspectral (hs) images where the complexity of the information cannot be summarized by a grey-level histogram without information loss. for this reason, many algorithms using other segmentation techniques such as classification can be found in the coastline detection process."
"this profile can change from one coast to another. for this reason, there is no indicator that can be used for all types of coast; functional indicators depend on the coast profile and the monitoring objectives."
where the x and y are the residual values that must be used to estimate the new vehicle coordinates. the equations (6) and (7) can be derived in:
"also, we can notice in our previous work [cit] that the vehicles farther away from the target can provide less accurate distance information than closer vehicles. the main idea in this work is to put more weight in the distance information given by neighbors closer to the target and less weight for the ones that are farther. in the vloci algorithm [cit] the authors compute the weighted average for the target's gps position received from its neighbors using equation (12) ."
"to analyse coastal change, a shoreline indicator is required. because of the dynamic nature of this borderline, the chosen indicator needs to take into account the shoreline in a spatiotemporal sense and must consider the dependency of this variability on the time scale [cit] ."
ammophila arenaria and agropyrum junceum are plants used to stop coastal dunes movements in tempered zones. [cit] upper limit of algae or marine lichen on the walls of rocky cliffs
"detecting the different indicators, shown in the table above, allows coastal monitoring. these indicators should be able to show the environmental modification in the beach area, but they should not be so sensitive to fluctuations in local conditions."
"pre-processing methods can be grouped into two kinds, which are noise reduction and image correction. irrespective of the processes used in a shoreline detection method, it naturally begins with preliminary steps that aim to reduce noise and improve image quality. the quality of a detected shoreline partially depends on the quality of the pre-processing methods applied. by using a pre-processing method that is better adapted to the data set, it is possible to improve the detection result. for a better recognition of the shoreline, it is crucial to reduce noise while preserving edge information. for this reason, adaptive filters, such as the median filter [cit], are widely used. they are particularly suitable when the noise is composed of thin lines or isolated points, which are scattered in the image."
"we can see that in the one-meter gps error scenario, the covalid, vloci, and gps trajectories are almost the same as the ground truth, as shown in figure 5a . however, our proposed solution is slightly better when compared to the other techniques. when the gps error increased to 5 and 10 m, respectively, both covalid and vloci could still reduce and improve the gps localization. besides, our proposed solution, covalid reached its best performance in 10 m of gps error scenario, minimizing it on average of both axes in 58% when compared to gps, and 51% when compared to vloci. it is worth mentioning that when the gps error increases, the trajectory of vloci algorithm is quite different than the ground truth, as seen in figure 5b, while the covalid maintained its trajectory similar to the ground truth. in tables 4-6, we can notice that our proposed solution obtained the least rmse values in all cases when compared to both gps and vloci. we can also notice that albeit vloci had improved its performance when the gps error increased from 1 to 10 m, the algorithm depends on the gps accuracy, in other words, the more accurate the gps device is, the more efficient vloci can be. our proposed solution demonstrated similar behavior since it is also a gps assisted approach. however, covalid shows to be efficient in all evaluated scenarios."
"to assess the impact of the number of vehicles in all tested approaches, we kept the gps error constant at 2 m, while the number of vehicles was increased from 2 to 10. furthermore, we maintained the distance constant among all neighbors regarding the target vehicle in 30 m. to evaluate the performance of each technique, we took into account both the rmse and mae values regarding the x-axis and y-axis, separately, as well as the average between both axes. all graphs presented in this section were plotted with 95% confidence interval."
"terrestrial surveys can be achieved using landmarks, global positioning system (gps), terrestrial light detection and ranging (lidar) or tridimensional (3d) scanners [cit] . in general, in situ measurements are difficult to achieve. in this case, other survey methods such as remote sensing are recommended [cit] ."
"some solutions to reduce localization errors and overcome the gps limitations have been proposed in the literature [cit] . in this section, we divide these localization approaches into gps free and gps assisted solutions [cit] ."
"in this work, we tested and analyzed three different sensors that are capable of providing distance information that is used in our proposed approach to improve the vehicles' position estimation. as seen in table 7, we used radars, lasers, and cameras as sensors."
"sometimes the gps position may not be on the same line as the one formed by the true positions of vehicles a and b, as shown in figure 3 . in these cases, we can compute the gps distance (d\") between a and b\". also, we still have distance information from both the gps and the sensor (d), so we can use covalid. thus, we assume the gps position is in the same line as the one formed by the true positions of a and b . thus, the distance value d\" is equal to d, which may not be true in the real world. however, the less the angle ∂, the closer d\" will be regarding d. hence, we performed our algorithm as the gps position was in the same line of the sensor's position, as seen in figure 2 ."
"isodata classification is an improved version of k-means. due to its simplicity of implementation, k-means is the most used classification algorithm. to modify the number of classes during the iterations, the isodata model introduces new parameters, allowing it to divide a class into two, when the sum of the variances of the grey-level pixels belonging to a class becomes greater than a fixed threshold, or to merge classes, when the distance between the centroids of two classes becomes less than another threshold."
"the rise and fall of the tides along the coast is a complex process that influences the establishment of a shoreline indicator. the tidal datums refer essentially to high tide or low tide. different tidal data are used successfully as shoreline indicators. we can cite the mean sea level, the mean high water line, and the mean spring high water line, among others. [cit] mean high water line [cit] mean spring high water line, mean high water spring tide [cit] mean higher high water line [cit] mean low water line [cit] mean low water spring tide mark [cit] lowest astronomical sea level [cit] virtual reference lines"
"this profile can change from one coast to another. for this reason, there is no indicator that can be used for all types of coast; functional indicators depend on the coast profile and the monitoring objectives."
"radar sensors are capable of measuring both the relative distance and speed of a target in short, medium, and long-range, with ranges up to 20 m, 100 m, and 250 m, respectively. in vanets, the radars commonly used to address localization problems are long-range sensors. in this section, we tested four different radars, all of them of long-range. since most radar sensors have no moving parts, the across-track accuracy is affected. it can be verified through results shown in the previous section, wherein all tested scenarios, the accuracy was more affected in the y-axis than in the x-axis. however, these sensors work well even in challenging environmental conditions, such as rain, dust, and fog."
"in the y-axis, according to rmse values described in figure 11a -c, our proposed method had better performance when compared to both vloci and gps in downtown, highway, and until 9 vehicles in neighborhood scenario. however, when the number of vehicles increased to 10, it can be noticed that covalid had its performance significantly affected. it is explained because, in scenarios with turns, it is more challenging to apply the similarity of triangles concept, since the communication can be affected by obstacles, such as buildings, and houses. another interesting point is that contrarily to the x-axis, we can notice is that the higher the velocity, the less affected is our proposed approach in the y-axis. also, in both downtown and neighborhood scenarios, we can notice that vloci had the worst performance, it can be explained because vloci was developed and tested in straight-line scenarios that is one of the characteristics of highway scenario, where vloci can overcome gps accuracy."
"a compilation of the different shoreline indicators that have been reported in the literature is provided. then, a summary of shoreline detection approaches in remote sensing is proposed, extending from image pre-processing to segmentation and edge detection."
"many edge detection methods have been successfully used for shoreline detection. heene [cit] showed results obtained using the canny edge detector together with two masking steps, an additional edge focusing and closing step as an input for an object-oriented matching process."
it defines the variations of the width of the range between an upstream limit and a downstream limit. the upstream limit is set at the foot of the dune or the lower limit of vegetation whereas the position of the downstream limit varies according to the authors.
"definition 5. since the gps can provide noisy coordinates, we can compute the difference of the distances between both that information given by the sensor and the one given by the gps. hence, we denoted distance error as:"
"in this section, we compare the results of our proposed solution to the initial gps inaccurate coordinates, to the vloci algorithm, and also to the perfect position of vehicles. for this, we plotted graphs with vehicles' positions as a result of each cited approach. in these graphs, the yellow circle represents gps position, whereas the cyan cross, the red cross, and the blue line denote, respectively, covalid, vloci, and the ground truth position."
"there is a correlation between the instantaneous high water line and the mean high water line, but the mean high water line it is not quite a tide datum because its definition takes into account other criteria that include, among others, the vegetation limit."
"on the other hand, laser sensors can measure the distance of an object. however, they are not able to measure the relative speed of a target with a single scan. for that purpose, lasers need successive scans. these kinds of sensors can be slightly more accurate than radars, although its accuracy is significantly affected by environmental conditions, and their prices are still higher when compared to radars. in this work, we tested two different laser sensors, and results show that the impact of the use of lasers is not significant in terms of accuracy. however, all applied approaches were not tested in adverse environmental conditions, and it is known [cit] that it can affect the accuracy of distance information given by lasers."
"base of the scree the base of the scree is an indicator that may be chosen when the cliff is affected by mass movements. [cit] contour of the tear scar like the base of the scree, the contour of the tear scar may be used in case of cliff mass movements [cit] in case of a protected seafront"
"the region growing methods are fast and conceptually simple, but they are very sensitive to the disposition of the objects in the image. the table 6 summarizes the most commonly used segmentation methods in shoreline detection."
"the high tide wrack line is the line of debris left on the beach by high tide. it is usually made up of eelgrass, or others kinds of litter."
"the lee sigma filter replaces each pixel with the mean of all diagonal values in the moving kernel that fall within the designated standard deviation range, in which the pixels beyond the standard deviation range are regarded as speckle-contaminated and hence are not used to calculate the mean. it takes into account, for estimation of local statistics, only the pixels within a certain range of radiometric values [cit] ."
"this work is outlined as follows. in section 2, we present the related work. in section 3, we describe our proposed solution. in section 4, we show the methodology, performance evaluation, and obtained results. finally, section 5 provides our conclusions and future work."
"our proposed solution is also based on the cooperative exchange of gps data. the covalid algorithm is a localization technique capable of adjusting the vehicle gps coordinates based on inter-vehicular distance data. it relies on accurate distance information that is given by measurement devices such as lidar, radar, or cameras. to perform the data fusion of gps and distance information, we applied an extended kalman filter (ekf), which is different from the vloci. another noticed difference is that covalid is designed to improve gps coordinates in both axes, whereas vloci can improve just in one axis. therefore, the main contribution of this work is that we can reach a high level of accuracy of the estimated positions using only gps and distance information, which has a low computational cost due to the kalman filter recursive call."
the state of the art also shows that almost all shoreline detection methods use commercial tools and do not rely on the development of automated process.
"noise reduction is an important task in shoreline detection; it allows improving image quality, which contributes to a better detection of the shoreline. however, it is not the on-off pre-processing step applied to satellite images. in the case of a multispectral analysis, it is necessary to perform radiometric and geometric corrections. to minimize the effects of weathering on the radiometric values generated by interpolation during the geometric correction, radiometric corrections must precede geometric corrections [cit] . geometric corrections can be of two kinds; rectification and georeferencing [cit] . the rectification is an oblique image correction in order to obtain a vertical image corrected for all or most strains inherent in the shot and distortion produced by the environment [cit] . georeferencing is the application of a coordinate system to an image to put it to the real spatial scale [cit] ."
"in this work, we have proposed an improvement to the bound algorithm, named covalid, which improves gps position of nearby vehicles and minimize their errors through an extended kalman filter that performs the data fusion of both gps and distance information to provide a precise estimation for the vehicle's positions within the network area. also, our solution takes advantage of a weighted average method to put more confidence in distance information given by neighbors closer to the target. we evaluated and tested our solution through simulations in three real-world scenarios, such as highway, downtown, and neighborhood."
"the precision of these indicators depends on the quality of the material, the working conditions and the experience of the operator. therefore, this precision can vary from one operator to another, but also for the same operator, several results are possible depending on the working conditions. a good way of monitoring these indicators is the use of satellite remote sensing. there is a wide range of satellites whose products can be successfully used for coastal science, and particularly for shoreline detection using automated or semi-automated image processing techniques."
"boak and turner [cit] listed 45 coastline indicators used around the world for coastal monitoring studies. a sketch of the spatial relationship between the most commonly used shoreline indicators is shown in figure 2 . a compilation of these indicators, their description and some references where they are used are given in tables 1-5. the different kind of indicators represented in this table are organized into seven types: geomorphological reference lines, vegetation limits, instant tidal levels and wetting limits, tidal data, beach contours and storm lines. each of these types of indicators relates to different indicators that have been used for coastal detection and monitoring."
"as shown in figure 4a, our proposed solution was able to improve the gps positions. however, sometimes, those estimations still put the vehicle outside the road. so, we apply the road constraints, as described in section 3.5, in our data fusion solution, resulting in a more accurate estimation, as seen in figure 4b . also, it is noticeable that the trajectory of the vehicle using covalid + rc is similar to the ground truth. according to table 3, the covalid + rc, called just covalid from now on, is capable of reducing x-axis and y-axis gps positioning error on average in 62% and 22%, respectively. another interesting point in table 3 is that the vloci algorithm had better performance when compared to covalid without road constraints (rc). it can be explained due to the fact that we made some adjustments in the vloci original approach, and one of them was to use road constraints. so, vloci was already using rc, while covalid not. we noticed, for this scenario, that the vloci algorithm improved its accuracy when compared to the results presented by farhan [cit] due to the adjusts that we made. it is important to mention that the vloci approach assumes that vehicles are traveling in one lane and in the same direction. hence, the values in both, rmse and mae are the same in the x-axis for vloci and gps techniques. so, when comparing our covalid solution to the vloci, in terms of accuracy in the y-axis, our approach outperforms vloci by at least 11%, reducing the error from 1.85 m to 1.64 m. we can also observe differences between values when the axis changes. it can be explained due to the fact that we assume the error in both axes is proportional, which may not true in real-world scenarios."
"another satellite whose data are also commonly used is spot, since it is one of the oldest satellites with a wide coverage. as for high-resolution satellites data, they are used in a fewer number of publications due to the high cost of these products."
"in recent years, image classification has become an active research topic in the field of pattern recognition and computer vision [cit], and several pixel-based classification methods have been used for shoreline extraction."
"it is noticed that the problem described in the matrix above is non-linear, so we can apply the ekf to linearize it. for that purpose, the jacobian (partial derivative) is used to estimate jh k (the jacobian matrix of h k ), and jh t k is its transpose. the measurement's noise is given by v, which is assumed to be zero-mean gaussian white noise with covariance r k . hence, the kalman gain can be calculated by:"
"in the case of unsupervised classification, one can use a principal component analysis (pca) to determine the class number. for coastline detection, pca can be a powerful tool that allows researchers to obtain uncorrelated pixels with high variance for a better classification [cit] ."
"in this section, we describe the details of our proposed solution, the covalid (cooperative vehicle localization improvement using distance information) algorithm which is an extension and improvement over our previously proposed bound algorithm [cit] . we also explore and discuss some of its challenges and real-world implementation."
"where c is the constant of proportionality, h, a and b are the sides of the triangles. due to the demonstrated property above (similarity of the triangles), it supports that the ratio of two sides in one particular triangle is equal to the ratio of two sides in another similar triangle. from equation (3) we can formulate:"
"equation (2) gives us the distance information using the gps coordinates. whereas, equation (5) provides the difference between both the gps distance and the sensor distance. figure 2 shows that d is the sensor distance information. as explained in the next section, we are using the weighted average information. here, d is the distance computed based on the gps positions of both vehicles a and b. once we have this information, we can calculate the difference (d − d) of the distance between the vehicles, the coordinates of vehicle b, centering in-vehicle a, are given as x and y. moreover, using the concept of similarity of triangles, we can notice that the β angle is the same in both triangles acb and triangle ac b . hence, covalid can adjust the vehicle's position based on the difference between the sensor and the gps distances. with all the needed information, we can utilize the concept of similarity of the triangles to estimate the new vehicle position, through equations (6) and (7)."
"it is worth mentioning that we assume the error in both the x-axis and the y-axis is proportional, which might not be accurate in some real-world scenarios. moreover, covalid can be used in real-world scenarios, despite its use of straight lines. for instance, if two vehicles are on the same road (straight line), the sensors can collect distance information even if they are not in the same lane which is a fair assumption since both highways and downtown scenarios are common scenarios."
"in a random trajectory scenario, the results presented in the x-axis show that covalid had similar behavior for all three tested scenarios. we can observe that the higher is the distance information error, the worse is the covalid performance. the same behavior can be seen in the y-axis, and as a consequence, on average of both axes. however, the covalid accuracy just decreased its performance around 32 cm in the downtown scenario."
"the growing region method is part of so-called object-oriented segmentation, which has started to gain momentum in recent years and is being successfully used for coastline detection."
"two classification types can be distinguished, the pixel-oriented classification and the object-oriented classification. each of these two types of classification has been successfully used in the context of shoreline detection."
"the concept of similarity of the triangles as demonstrated in our previous work [cit], states that if two triangles share congruent angles, they are similar, as shown in figure 1 . hence, the ratios of the corresponding sides of any two triangles are equivalent, no matter the hypotenuse length."
"in areas with sharp cliffs, with no notches, regularly beaten by the waves and cleared of fallen materials, the base of the cliff is an optimal alternative to the cliff top. [14, [cit] in case of scree at the cliff's toe"
"remote sensing is the technique of obtaining information about objects or areas from a distance, typically from aircraft or satellites [cit] . it is increasingly used in coastal monitoring insofar as it contributes to the fullness of the radiometric information and allows automated or semi-automated extraction of the shoreline by image processing."
"deep learning can achieve state of the-art accuracy for many applications by taking the advantages of the larger data set (big data) that we are now able to analyse. it is the object of intense research in image processing applications. however, according to our knowledge, a shoreline detection method using deep learning has not yet been proposed."
"coastal dunes dune crest line the dune crest is the highest elevation peak, where the slope changes sign from positive (landward facing) to negative (seaward facing) [cit] . [18, [cit] cliffs and backed beach bluff top, cliff top, top of the cliff the bluff top (cliff top) refers to the top edge of the cliff [14, [cit] base of the bluff, cliff toe, bluff toe"
"[ 94, 95] crest of washover terrace washover terraces are deposited where beaches are highly erosional and adjacent ground elevations are lower than the highest storm surges. the crest of the washover terrace forms the highest beach elevation and is the best indicator of shoreline movement for these types of beaches [cit]"
coastline indicators must have the ability to represent schematically but correctly the overall coastal state from the point of view of its sedimentary evolution [cit] .
"in this section, we used a simple intersection scenario to evaluate the performance of our proposed localization solution. in this scenario, vehicles can move in a straight-line road. furthermore, we used rmse and mae to assess the accuracy of the vloci, covalid, and gps regarding the impact of gps error, the increasing of the number of vehicles, and distance among vehicles. then, both the results and discussion about them are presented."
"it is estimated that there are about 504,000 km of shoreline worldwide, and more than 50% of the world's population lives within 100 km of the sea [cit] . detecting and monitoring shorelines are consequently of significant economic and social importance, especially if we know that climate change has devastating effects on coastal areas. the shoreline marks the transition between land and sea. ideally, it is defined as the physical interface of land and water [cit] . in some references, the shoreline is expressed as an intersection of coastal land and water surface showing water edge movements as the tides rise and fall [cit] . in fact, the shoreline is flexible depending on sea level, swell, tides and near-shore currents."
"the rmse and mae values show that our proposed method had better performance in all evaluated scenarios when compared to both vloci and gps regarding the x-axis. this result is expected since in x-axis both have the same values, as presented in figures 6a and 7a. number of vehicles number of vehicles another interesting point is that when the number of vehicles increases to 3, mae values demonstrated that the vloci algorithm could overcome covalid regarding the y-axis, as shown in figure 7b . however, according to rmse values, the vloci algorithm overcomes our proposed method only when the number of vehicles is increased to 4, and maintained its better performance for the remainder of the tested scenarios, as seen in figure 6b . it suggests that when the number of vehicles increases, better accuracy is achieved in the y-axis by vloci. also, it is worth pointing out that although our solution was overcome by vloci when the number of vehicles increased, our method maintained rmse and mae values almost constant. number of vehicles figures 6c and 7c show the average error of both axes. we can notice that our proposed method had better results in all tested scenarios when compared to both vloci and gps. it can be explained by the fact that covalid uses distance information to minimize the gps error in both axes, while the vloci algorithm only improves the error in one axis."
"for coastal management purposes, it is necessary to know the evolution of the shoreline according to the associated time scale. in order to analyse these changes, a definition of the shoreline must be given. the definition of the shoreline that theoretically is supposed to represent the linear boundary between the maritime and land domains is very challenging because of the wide variety of indicators (key) that can be based on geomorphologic aspect, tidal level, or the configuration of the vegetation, among others [cit] ."
"the k-mers represent a protein family, but not a specific organism from that family. however, most families only contain a few proteins (the median size of the protein families is only six), and thus most protein families only come from a handful of species and very few genera. to assign taxonomic groups to sequences, we identify the last common ancestor of the organisms whose proteins make up a family from their taxonomy. this is an approximation that provides for a rapid assessment of the members of the community."
"the core of our algorithm regards the estimation of the target position, based on the otdoa measurements. however, if other sources, such as gps, car sensors and smartphone sensors are available, the system is able to use them in order to improve the accuracy of the estimate. to make this possible, it is necessary to make the measurement model in (4) time variant in order to accept at each time a different set of sources. consequently, the dimensions d y and d n change over time according to available measurements. for instance, if at time k only the position estimated through the otdoa measurements is available, the measurement model will be"
"the architecture of the convnet is illustrated in fig. 1, where conv, mp, and fc represent convolutional, max-pooling and fully-connected layers, respectively. in general, the convnet is considered as a hierarchical feature extractor, which extracts features of different abstract levels and maps raw pixel intensities of the crack patch into a feature vector by several fully connected layers. all parameters are jointly optimized through minimization of the misclassification error over the training set via the back propagation method [cit] ."
"out of the generated samples from the above steps, 640,000 samples are used as the training set, 160,000 samples are used as the validation set for cross-validation when training the convnets, and 200,000 samples are used as the testing samples. the numbers of crack and non-crack patches are set to equal in all three data sets."
"the sensitivity and specificity of the k-mer approach was measured using synthetic metagenomes constructed using genomes that were not included in the figfams build ( supplementary fig. 1 ). the sensitivity [tp/(tp þ fn)] measures whether genes that are there (i.e. in the complete genome annotation) are found; for very short dna sequences, most genes that are there are missed."
"k-mer searches were on average 860 times faster than blastx because the k-mer approach neither extends the matches nor calculates alignment statistics for the resulting matches. dna sequence annotations using k-mers are as sensitive, precise and accurate as blastx searches, especially for shorter reads. the main disadvantage of using unique k-mers is as the read-length increases the blastx sensitivity exceeds that of k-mers."
"in this section we present simulation results for validating the performance of the proposed algorithm. the road network is loaded from the free database openstreetmap [cit] . vehicle trajectory is generated according to the map and on the bézier curve model [cit] and vehicle position measurements are simulated. two scenarios have been evaluated. in the former, in fig. 4, the vehicle is assumed traveling along the roads shown in fig. 3, while, in the latter, a critical situation is emulated (the vehicle skids off the road). the following assumptions are valid in all the simulations unless specified differently:"
"the results in figs. 5 and 6 show that both algorithms, pf and ukf, improve the accuracy w.r.t single point position measurements, and map-matching applied to pf clearly outperforms the other solutions reducing the error by roughly 45% with both cv and ctrv models. table ii summarizes the average positioning errors for the two different dynamic models and tracking systems: the cv model in (7) shows better results in our scenarios. this effect becomes even more clear with ukf, where the average error achieved in the cv case is 2.46 m. notice that, with the ctrv model, the error is 3.47 m, which is approximately 30% higher, and this is less evident with pf. the reason of this difference is that the scenario is well described by the linear cv model in (7) . the non-linear crtv model in (8)"
"is a generic function of the state x k−1, while v k−1 is an independent and identically distributed (i.i.d.) process noise sequence, d x, d v are dimensions of the state and process noise vectors, respectively. the state vector x k includes a set of variables that describe the status of the system, and it is the variable we want to track. it evolves at discrete times k as a result of the assumptions made for the dynamic state model in (3) . then, the purpose of tracking is to determine recursively the probability density function (pdf) of x k from the measures"
"keeping roads in a good condition is vital to safe driving and is an important task of both state and local transportation maintenance departments. one important component of this task is to monitor the degradation of road conditions, which is labor intensive and requires domain expertise. recently, computer vision and machine learning techniques have been successfully applied to automate road surface survey [cit] . in this work, we focus on detecting cracks on the pavement surface, because they represent the most prevalent type of road damage and exhibit strong texture cues. a large number of recent literature in crack detection and characterization of pavement surface distresses clearly demonstrates an increasing interest in this research area [cit] ."
"we proposed an automatic detection method based on deep convolutional neural networks in which the features are automatically learned from manually annotated image patches acquired by a low-cost sensor, i.e., smart phone. to the best of our knowledge, this is the first study that applies deeplearning based method to road crack detection problem. in the future, we will optimize the proposed detection method and build an integrated low-cost system for real-time road crack detection."
"dna sequences from metagenomes are assigned functions based on the figfams that match. to search the k-mers, the dna sequence is translated in all six frames, and an exact matching algorithm is used to find identical amino acid strings. requiring either multiple independent k-mer matches from a single family or a minimum number of k-mer matches over a minimum sequence length is used to adjust the sensitivity of the match. the search reports the first and last positions in the query sequence where the k-mers match, and the number of k-mers that match that region. these matches can be combined into subsystems [cit] . the last common ancestor of the organisms in each family is also identified."
"as shown in supplementary figure 1, the length of the k-mer has a strong impact on the precision, sensitivity and accuracy of the search. based on these data and empirical observations, we recommend that users require at least two k-mer matchers per sequence (but generally no more than four k-mer matchers), and eight-or nine-amino acid k-mers. these parameters provide reasonable estimates of metagenome composition."
"on the other hand, the impressive performances for many medical imaging and computer vision tasks have evidently showcased the effectiveness of deep features learned by deep neural networks [cit] which are likely to replace the conventional hand-crafted features [cit] . restricted boltzmann machine (rbm), autoencoder and their variants are popular for unsupervised deep learning when the number of labelled examples is small, while deep convolutional neural networks (convnets) are popular for feature learning and supervised classification [cit] . such promising results motivate the application of deep learning techniques into the crack detection problems."
"to validate whether the k-mer approach could be used to annotate metagenomes, simulated metagenomes were made from 70 different microbial genomes representing a diverse selection of organisms that had been annotated using rapid annotation using subsystems technology (rast) but had not been included in the figfams (supplementary table 1 ). the metagenomes were constructed with grinder [cit] and were designed with median dna fragment lengths of 30, 50, 75, 100, 250 and 500 bp. metagenomes were annotated by searching for genes using the k-mers, and blastx searches of either the seed-nr database or the database of proteins used to generate the k-mer library. not every protein in the seed-nr is included in a figfam (e.g. singleton proteins are not members of a family)."
"we compared two algorithms for tracking, ukf and pf. the former is a parametric estimation of the posterior pdf and it is based on the so-called \"unscented transform\", which samples the pdf in determined points in order to track the mean and covariance of the state variable passing through non-linear transformations [cit] . the latter is a non-parametric estimation, which represents the required posterior pdf by a set of random samples of the state-space x (i) k (the i-th particle at time k) with the associated weights w [cit] . in terms of computational complexity, ukf outperforms pf since it has to track only a limited number of parameters while pf tracks n s particles, where n s depends on the application and the target accuracy. moreover, pf suffers from the curse of dimensionality [cit], which means that the complexity increases exponentially with the number of variables in the state vector x k ."
"the proposed tracking system can be implemented in a mobile radio network, eventually as a cloud service. it can rely on different inputs, as illustrated in fig. 1 . the main source of information is the position estimation based on otdoa measurements from the radio access technology (rat) network and blade [cit] . other inputs might be used 978-1-5386-6358-5/18/$31.00 ©2018 ieee when they are available, like gps and car or smartphone sensors. the results of our trajectory estimation algorithm are the trajectory parameters, e.g. current position, speed, heading and turn rate. in this work, we assume that the current position is the key performance indicator of interest, and we compare different schemes through their respective position error distributions and average values. in the next subsection, we provide a brief review of the bayesian technique used."
"all convolutional filter kernel elements are trained from the data in a supervised fashion by learning from the labeled set of examples introduced in section 2.1. in each convolutional layer, the convnet performs max-pooling operations in order to summarize feature responses across neighboring pixels. such operations allow the convnet to learn features that are spatially invariant, i.e., they do not change with respect to the location of objects in the images. finally, fully-connected layers are used for classification. due to the mutually exclusive property of the underlying crack detection problem (crack or non-crack), a softmax layer is used as the last layer of the convnets to compute the probability of each class given an input patch."
"in this scenario, we compare our proposed soft technique with a hard map-matching technique, where the weight of a particle outside the street is strictly zero. notice, from fig. 7, that when the user does not follow the street, the hard mapmatching is not able to track it. in this scenario, we have obtained an average error of 3.17 m for soft map-matching and 14.05 m for hard map-matching. therefore, our proposed technique turns out to improve pf accuracy even in critical scenarios, e.g. when the vehicle is skidding off the road."
"given a pavement image, the objective of a crack detection problem is to determine whether a specific pixel is a part of a crack. to solve this problem, the proposed solution is based on a convnet, which is trained on square image patches with given ground truth information, for the classification of patches with and without cracks. for notational convenience, crack and non-crack patches are also referred to as positive and negative patches, respectively. in this paper, a patch whose center is itself a crack pixel, or is within the close vicinity of a crack pixel, is considered as a positive patch. otherwise, this patch is considered as a negative patch."
"the seed annotation servers (http://servers.theseed.org/) provide programmatic access to the k-mer annotation algorithm via an api. these servers support the assignment of functions to protein or dna sequences. detailed examples are provided at that page and at http://edwards.sdsu.edu/rtmg. the web interface was built to provide rapid interpretation of a metagenome sample. the samples are analysed in groups of sequences (currently the default is 10 000 sequences at a time), in a round-robin fashion. users see the results of their annotation as it is being performed. at any time, all of the raw data can be downloaded as raw text to import into any other analysis platform or package. in practice we use this system to assess the quality of the metagenome and visualize similarities to the sample, leaving more detailed and thorough analysis until more time-consuming comparisons are complete. however, the results of the k-merbased analysis are generally recapitulated in downstream analyses."
"the threshold used to re-estimate the final probability is determined such that it yields the largest f 1 score on the validation data set [cit] . in this study, the threshold t is set to 0.64, at which the f 1 score is maximized."
"the precision [tp/(tp þ fp)] reports whether too many genes are being called on a fragment. with very short fragments, both short k-mers and blastx overcall genes. however, longer k-mers result in more confident calls, regardless of fragment length. blast precision is improved by only using the set of confidently called proteins-those that are in families and used to make the k-mers. accuracy measures whether the genes that are identified are correctly annotated [cit] . this measure of accuracy is testing whether the function assigned by the best blastx hit or k-mer reflects of the 'true' function of the protein as annotated in the genome."
"metagenomics has revolutionized microbial ecology. the extraction, purification and sequencing steps have been trivialized by next generation sequencing approaches, and environmental samples are routinely processed from collection to dna sequence in a matter of days [cit] . the bottleneck in metagenomics approaches has become the analysis of the sequences. the computational comparison of sequences against all of the known proteins using blastx is limited by computational resources [cit] ."
"here, we describe a novel approach to analysing metagenomic sequences using unique signature k-mers that represent members of a protein family. limited additional computational resources are required when the size of the underlying database doubles, as in the best case the search is dependent on the length of the k-mer. we implemented an api and web servers to support the annotation of metagenomes using k-mers."
where h i is a non-linear function that describes the relation between the measured quantity and the measurements. the measurements from the different sensors are combined by adopting a bayesian sensors fusion approach [cit] at the likelihood level.
"determining the unknown state of a dynamic system using noisy and distorted observations is the subject of recursive bayesian theory [cit] . in our particular case, the dynamic system is a moving vehicle or a vru. the state variables are the position, velocity, heading, and turn rate. let us consider the evolution of the unknown state vector sequence x k of a tu given by"
"the goal of training a convnet is to increase the variation of the training data and to avoid overfitting analogous to the training data set. the dropout method is used between two fully connected layers to reduce overfitting by preventing complex co-adaptations on training data [cit] . the output of each neuron is set to zero with a probability of 0.5. the training of the convnet is accelerated by graphics processing units (gpus). further speed-ups are achieved by using rectified linear units (relu) as the activation function [cit], which is more effective than the hyperbolic tangent functions tanh(x) and the sigmoid function (1+e −x ) −1 used in traditional neuron models, in both training and evaluation phases. the convnets are trained using the stochastic gradient descent (sgd) method with a batch size of 48 examples, momentum of 0.9, and weight decay of 0.0005. less than 20 epochs are needed to reach a minimum on validation set."
"fellowship for the interpretation of genomes (fig) protein families, figfams, were constructed as described previously [cit] . to identify the signature k-mers that represent members of a protein family, all amino acid oligomers from 7 to 12 amino acids were identified that were (i) present in one of more members of the figfam and (ii) were not present in any other figfam. these oligos are unambiguous representatives of the family. a binary tree was built to allow rapid searching of the k-mers and to identify their cognate protein families."
"the k-mers are much more sensitive than blastx, finding almost 1/5 of genes present when the fragment lengths are only 30 bp. when the fragment length exceeds 50 bp, blastx performance improves rapidly as the high scoring pairs exceed the threshold for inclusion, and approaches that of the most sensitive k-mer searches. all approaches reach a sensitivity plateau once the sequence length exceeds 100 bp, and the best methods only find 70% of the genes on the fragments. for the longest fragment lengths (500 bp), the mean open reading frame length on each fragment was only 340 bp as genes may start and end off the fragment. in all cases, longer sequence reads resulted in more sensitive assignment of annotations to the dna sequences, as seen before [cit] ."
"map-matching can improve the tracking performance through a more accurate positioning by exploiting a-priori road-map knowledge. this is done by assuming that vehicles typically follow the road path. including road-map information within the common kalman filtering framework represents a hard challenge due to the fact that road-maps express a step function behavior, that is highly non-linear, non-derivable, and thus critical w.r.t. the typical assumptions of kalman filters. therefore, we limit the map-matching implementation only to pf, which is not subject to these constraints and is also able to capture efficaciously multi-modal pdfs. in our implementation, map information is embedded by changing softly the likelihood applied during the update phase of the pf. first, the state space is increased by a map flag s k that indicates in which area of the map the target is currently estimated, i.e.x"
"in this paper we implemented a data fusion framework in order to track road users applying recursive bayesian techniques. we proposed and validated a novel soft-map matching algorithm, which is able to track users even when they are not following the road, e.g. in emergency situations which are not considered in previous works. moreover, we achieve a 45% reduction in positioning error in the considered scenarios when map-matching is not applied. in future work, we will also consider the dependency of the reliability and availability of sensor measurements on the location of the tu. we will also focus on the optimization of the particle filter in terms of computational complexity as well as on the investigation of specific dynamic models for vehicles and pedestrians."
rest of the paper is organized as: section 2 reviews related work; section 3 formulates the problem; section 4 describes the research methodology used for evaluation; section 5 presents and discusses the results achieved and section 6 provides summary and potential future work.
"for arabic to english mt, [cit] present the comparison of google and babylon translators. the arabic sentences are categorized in four basic sentences: imperative, declarative, exclamatory and interrogative. they report that google translator outperforms babylon translator. their work is close to ours'. we perform comparative study of mt systems from urdu to arabic and they compare the mt systems from arabic to english."
"in the case of romanian, agreement between the syntactic components of a text is mandatory. for this reason, a generation system for texts in romanian must checks for compatibility of the generated text (subject-verb agreement, article-head number agreement, gender compatibility, word-order, etc.). for example, an adjective in romanian usually follows the noun it modifies and fully agrees with it in terms of number, gender, case, and definiteness."
"in this paper, we compare three machine translators (google, bing and babylon) for translating urdu sentences to arabic sentences by using three performance evaluation metrics (bleu, meteor and nist). the corpus used in this research contains three different types of 159 urdu sentences and their respective arabic sentences. our results show that google translator, on the average, outperforms bing and babylon by 15.74% and 28.55% in bleu technique, 13.74% and 3.28% in meteor technique, 20.83% and 3.91% in nist technique respectively. this study is helpful for those who want to use online machine translators for urdu to arabic translation."
"in this section, we exploit bleu score to compare the performance of each translator. the average results are also shown in fig. 2 . we can easily see that google outperforms bing and babylon. google translator, as per bleu evaluation measure outperforms 28.55% better than babylon and 15.74% then bing."
"as with parsing, we can represent the grammar in a labeled stratified graph (lsg) if we allow the lsg to become a transition network for which arc labels refer to word categories and word forms. in this representation, each grammar rule can be transposed in a sequence of labeled paths. the symbols of the set l 0 can denote terminals that have associated some categories from lexic (word forms in the considered language) or nonterminals that denote syntactic categories and which are not a priori instantiated with an element from lexicon (will be instantiated during the inference process)."
"many mt approaches have been proposed in literature for the translation of different languages. in the relevant literature, we could not find any published machine translation approach from urdu to arabic however some commercial machine translation systems like google, bing and babylon provide urdu to arabic translation. the users of these translators, while translating from urdu to arabic, do not know the quality (accuracy level) of their translations. the users may be interested to use the best translator but they might not know the best one."
"we will develop our own urdu to arabic machine translation system by exploiting hybrid technique comprising template based and rule based approach. we expect to have better results than the available online machine translators. in future, we will also build a large corpus for evaluation mt systems."
"we formally define our problem as: \"given the set of urdu sentences as input to three machine translation systems, compare the output of these translators (arabic sentences) by using multiple evaluation methods.\""
"the concept of stratified graph provides a method of knowledge representation. this concept was introduced in paper [cit] . the resulting method uses concepts from graph theory redefined in the new framework and elements of universal algebra. intuitively, a stratified graph is built over a labeled graph placing on top a subset of a peano algebra generated by the label set of considered graph."
"in this section, we exploit nist score to compare the performance of each translator. fig. 4 . accuracy of all online machines using nist technique. comparing results in all techniques bleu, meteor and nist, it is concluded that google always outperforms babylon and bing translators. fig. 5 shows the summary of all results of all translators w.r.t bleu, meteor and nist metric."
"in this paper we treat from the mathematical point of view the inference process based on stratified graphs by means of new concepts such as regular paths, structured paths and accepted structured paths. also we proposed a new mechanism for interpreting the relations encoded in stratified graphs. this interpretation, defined in order to be used for natural language texts generation, pays attention to the agreement conditions that have to be fulfilled between the text components."
"1) the first step we need to perform is to calculate the brevity penalty (bp) which is calculated by choosing the reference sentence that has the more common n-grams length, denoted by r."
"as mentioned earlier, there is no research work which targets the content to be translated from urdu to arabic therefore we here review some research works which are related to urdu or arabic but the translation is aimed for other languages. different comparative studies of mt systems from urdu to other languages and vice versa are available in the literature [cit] . same is the case of comparative studies of mt systems from arabic to other languages and vice versa [cit] . kit english sentences. according to their report, the fluent human translator accuracy is 100% and other's 80%. whereas systran got only 70% accuracy while it is faster than human by 195 times."
the bleu score is calculated by comparing each translated sentence and then comparing with the reference sentence. the average of these scores is computed by averaging them with the corpus size to find the translation accuracy. it is noteworthy that the evaluation does not take into consideration the grammar correctness of the translation. bleu technique is constructed and put in place to calculate the quality at corpus level. the use of bleu technique to evaluate the quality of individual sentences always gives an output that lies between 0 and 1. these values tell the readers how similar the reference and candidate sentences (translator output) are. words with values closer to 1 are closer to the reference translation.
"urdu is the national language of pakistan while arabic is a major language in almost 20 different countries of the world comprising almost 450 million people. among 7,105 languages spoken in different areas of the world, urdu is ranked at 19 th number. 4 in pakistan, urdu language is the medium of instruction in most of the public and private institutions. the main information sources such as newspapers and electronic media use urdu language [cit] . arabic is the main language in 20 different countries like egypt, iraq, saudi arabia, somalia, sudan, syria and the united arab emirates [cit] . arabic is also considered as a religious language of muslims, as the holy quran and hadith books are written in arabic language."
"another machine translation evaluation technique is known as \"metric for evaluation of translation with explicit ordering\" (meteor). it premises on the harmonic mean of the unigram precision and recall. this technique is different from the one mentioned above in the sense that it works on the segment level while bleu works on corpus level."
"pakistani and arab communities have many things in common like cultural heritage, religion, traditions, etc. these communities need to understand each other for many reasons. a large community of pakistani people works in arab countries. every year, a large number of pakistani people travels to arab countries to visit sacred places (makkah, madina), to get jobs and to promote their trade and businesses. the arab people also visit pakistan to get higher education and to promote their businesses. these communities need to understand each other, but there is a language barrier. machine translation systems can help them remove this barrier. the performance of online mt systems differs a lot. a user of these mt systems may not know the best one. we, in this paper, evaluate the performance of three online mt systems to help the arab and pakistani communities to select the best mt system."
"parsing and as well as natural language generation require a lexicon -a file of words giving their syntactic categories and lexical features, together with the inflectional forms of irregularly inflected words. base forms, also known as lemmas or ground form do not contain any morphological derivation of the word (such as gender, number, tense, and so on) opposite to word forms which are made from the word base by adding inflectional morphemes."
there are few commercial translators that provide this translation. the users of these translators need to know the accuracy level of these translators. if it is known the users will prefer the best translator.
"in meteor algorithm, the first step is to map an alignment between the reference and candidate sentences. this alignment is established according to the unigram technique. mapping is also considered to be a line between single word of one sentence with the others. every single word of candidate sentences must map to either zero or one in the reference sentences. if two alignments map on the same word, then we need to consider the one with the fewest one. the final alignment completed by unigram precision (p) is shown in (3):"
"in our case, bleu divides urdu sentences into various ngram sizes, for example, unigrams, bigrams, trigrams and tetra-grams. for each of the four gram sizes, the accuracy for various translators such as bing, babylon and google translator is computed. in the end, for every n-gram sizes, we calculate the n-gram scores of the sentence. the respective steps to calculate the score for all the ngram sizes are as follows:"
"in this section, we repot the results which are generated by our evaluation metrics (bleu, meteor and nist) for the corpus which we mentioned above. we compare the accuracy of each mt system according to three evaluation metrics under separate headings."
"all the approaches have their own pros and cons. no mt approach is the perfect in all scenarios and for all languages [cit] . in this paper, we use the terms \"translator\", \"mt system\" and \"mt tool\" interchangeably."
"there are two sides to natural language processing. on the one hand, work in natural language understanding is concerned with the mapping from some surface representation of linguistic material expressed as speech or text to an underlying representation of the meaning while maping from some underlying representation of meaning into text or speech is the domain of natural language generation [cit] . both are equally large and important problems, but the literature contains much less work on natural language generation (nlg) than it does on natural language understanding (nlu). graph theory is a well-studied sub-discipline of mathematics, with a large body of results and a large number of efficient algorithms that operate on graphs [cit] . despite the various existing linguistic theories, which lead to different ways of viewing sentence structure and therefore syntactic analysis, most linguists today agree that at the heart of sentence structure are the relations among words ( [cit] . these relations refer either to grammatical functions (subject, complement etc.) or to links which bind words into larger units like phrases or even sentences [cit] . a natural way to capture and process the connections between entities is by means of graph-based representations. the dependency graphs are well-known graph-based representations in which the syntactic and semantic features of sentences are depicted. in order to create dependency graphs, a sentence is processed by a dependency parser, which is based on the theoretical foundations of dependency grammar."
"let us suppose that we have a description of the syntax of a natural language. we are not interested here what is the method used for the description but we suppose that it contains at least two elements: non-terminals (denoting the syntactic classes of the text components) and terminals (i.e. word forms). to highlight the role of structured paths in a nlg system based on labeled graph representations, we consider the example presented in figure 1 . we relieved here two accepted structured paths: -one of them is denoted by (1) in order to explain in an intuitive manner the inference process defined for nlg we assign an algorithm to every arc symbol. for the example given in figure 1, the following algorithms are attached to each labeled relation represented in the graph:"
"starting from the classical rtn representations, in the following section we propose a mechanism for text generation based on labeled stratified graphs whose arcs are labeled with generation conditions and the nodes with arbitrary symbols used to mark the paths in the graph."
the reference sentences are considered to be correct as they are generated by human experts. to evaluate the score of the corpus we use different techniques which are discussed in performance measures section.
"the procedure to calculate the meteor score for the entire corpus is to get the values for p, r and p and then utilize the formula shown in (7)."
"in this section, we exploit meteor score to compare the performance of each translator. the average results of table iv are also shown in fig. 3 . google translator, as per meteor evaluation measure outperforms 13.74% better than babylon and 3.28% than bing."
"we compare three online machine translation systems (google, bing and babylon). we use urdu sentences as input while arabic is output of the mt systems. the output is compared with the corresponding reference sentences (arabic). the reference sentences are the true values or ground truth as they are manually translated by the language experts. fig. 1 depicts the framework of the proposed methodology."
"this technique is only applicable to the unigrams and not for larger segments. to evaluate the n-gram matches, penalty, p as shown in (6) is used to obtain alignment values."
the inference process developed in a stratified graph is based on the decomposition of an accepted structured path into two accepted structured paths. the resulted two components are subpaths of the initial path upon which a decomposition process is iterated until result only atomic accepted paths. a subpath defines a continuous path which consists of different kinds of elementary arcs of the initial path. the order induced by the structure of the accepted path and some meaning attached to every elementary arc are used in order to perform the inference.
"we exploit three evaluation measures (bleu, meteor and nist) to compare the performance (accuracy) of the three translators from urdu to arabic. as a rule, a machine translation that is closer to the reference translation is considered to be more accurate. this is the gist behind the machine translation evaluation methods."
"in the course of text generation, the agreement rules must indicate how the combination of the syntactic categories and values must be associated with the syntactic elements [cit] . the global task of nlg is to map a given formal input onto a natural language output to achieve a given communicative goal in a specific context ( [cit] . the generation process we propose allows obtaining a surface text from regular paths chains. the natural language constructions are obtained using an inference process based on binary relations composition by paying attention that the generated constructions are well formed. during the generation, the agreement rules must force some values of the syntactic categories associated to some elements in order to agree with other elements."
"in this work, we compare three online mt systems (google, bing and babylon). we evaluate these mt systems by three different evaluation measures bleu [cit], meteor [cit] and nist. 5 the results show that google translator is better than bing and babylon translators. to the best of our knowledge, our work is unique and the first instance of comparing the urdu to arabic mt systems."
"machine translation (mt) is a process of translating a given input or source sentence from one language to the other target language. now-a-days mt plays a significant role in different areas like education, business, medical and trade, etc. different mt techniques such as rule-based [cit], direct [cit], transfer [cit], statistical [cit], interlingua [cit], example based [cit], knowledge-base [cit] and hybrid machine translation [cit] (mt) are used to translate from one language to the other."
"as noted by competent linguists, romanian language is morphologically rich and relatively flexible word order language [cit] . the term morphologically rich languages refers to languages in which substantial grammatical information, i.e., information concerning the arrangement of words into syntactic units or cues to syntactic relations, are expressed at word level [cit] . some relevant word morphological attributes with respect to romanian expressed based on their part of speech data are: -verb: mood, time, person, number, gender -noun : number, gender, type -adjective: number, gender, degree -pronoun: type, gender, number, case -determiner: number, gender, type observation 1. prepositions, adverbs, numerals and conjunctions have no morphological data."
"nist stands for national institute of standards and technology. basically, this is a method devised to check the quality of the text. it is similar to the bleu metric, because it works on n-grams but, at the same time, it is different from bleu because it does not calculate the brevity penalty. it is similar, to some extent, to metor as it computes the precision."
"remark [cit] in a specific language, a word form is uniquely identified by its lemma and the corresponding morpho-syntactic information. the reciprocal is not true: to a word form can correspond more morpho-syntactic interpretations, which have to be disambiguated by the context."
"in regard to the postictal period, the results presented here are in agreement with current knowledge about seizure termination (11, 32) . the abrupt increase in the density of links facilitates the correlations between distant areas and decreases the average path length. these changes could provide a basis for seizure termination through not an excitatory but rather by means of an inhibitory mechanism. this increase in correlations among brain regions after seizure termination has also been observed in several previous studies (10, 11, 32) . however, the results presented here show that extreme values of density of links, average path length, and clustering coefficient are attained beyond seizure termination, although the underlying triggering mechanism begins during the seizure itself."
"to solve this problem, this paper proposes two types of models: static and dynamic models. in a static model, booking limits are set for each cruise product in the beginning of the booking process. whenever reserved booking limit for a product is reached, associated product is closed. a dynamic model sets the booking limit for each cruise product according to the actual bookings throughout the entire booking process."
"the ictal activity during seizures was evaluated, and the results are displayed in figures 3-6 . seizure onset and termination, as determined by two neurophysiologists (lvz and jp), are marked by vertical solid lines in each of these figures. the visual detection of seizure onset is a retrospective process, in the sense that the neurophysiologist first searched for ictal activity, which has the principal characteristic of faster, high-amplitude oscillations that appear simultaneously on several channels, and then looked back at the neurophysiological recording to determine at which point the epileptogenic activity actually began. in the cases shown here, seizure onset was determined by abrupt changes in the frequency domain instead of by changes in the signal amplitude. this fact explains why the seizure-onset times (vertical lines) appear to be distant from the high-amplitude ictal activity. four seizures were analyzed. the first three correspond to ps with sg (figures 3-5) . the last one is a ps without sg (figure 6) . in each case, excitability (as quantified by eq. 3), ls, ls ordering, and network measures were analyzed."
tests for multiple structural changes. a problem with the mean-w t and exp-w t tests is that they require computations of order o(t m ). consider instead the sup-wald test.
"the interictal small-world property behaves in a wandering fashion during the seizures extent. although in the first part of the seizures, for the case of ps with sg, the average path length increases, making the network more regular, it then decreases during the second part of the seizures until the postictal period, with a simultaneous increase in the clustering coefficient, making thus the network more random. for the ps without sg, no appreciable change exists during the preictal, ictal, and postictal periods, maintaining, therefore, the interictal small-word characteristic."
"as there are randomized variables d i and d j in the constraints, according to the thought of chance constrained programming, we set the confidence level as α i and α j for the third and fourth constraint of model 1 [cit] . the third constraint can be converted into the following chance constraint:"
"band spectral regressions and low frequency changes. [cit] consider the issue of testing for structural change using a band-spectral analysis. they allow changes over time within some frequency bands, permitting the coefficients to be different across frequency bands. using standard assumptions, the limit distributions obtained are similar to those in the time domain counterpart. they show that when the coefficients change only within some frequency band (e.g., the business cycle) we can have increased efficiency of the estimates of the break dates and increased power for the tests provided, of course, that the user chosen band contains the band at which the changes occur. they also discuss a very useful application in which the data is contaminated by some low frequency process and that the researcher is interested in whether the original non-contaminated model is stable. for example, the dependent variable may be affected by some random level shift process (a low frequency contamination) but at the business cycle frequency the model of interest is otherwise stable. they show that all that is needed to obtain estimates of the break dates and tests for structural changes that are not affected by such low frequency contaminations is to truncate a low frequency band that shrinks to zero at rate log(t )/t ."
"excitability is displayed in panel (a) in each seizure, that is, figures 3a, 4a, 5a, and 6a. the first three seizures correspond to a ps with onset in the mesial area and sg. in contrast, the last seizure, shown in figure 6a is a ps without sg. the ictal patterns of the first three seizures are very similar with respect to both extent and amplitude. in the ps without sg, the ictal activity started in the mesial area and lateral temporal lobe but did not propagate to the parietal lobe. moreover, its intensity was lower than that of the other three seizures. although in all four cases, the seizures began slowly and unevenly, all of them terminated suddenly. note that in all four cases, the seizure intensity reached its maximal value in approximately the second half of the seizure."
"the aim is to assess retrospectively whether a given forecasting model provides forecasts which show evidence of changes (improvements or deterioration) with respect to some loss function. since the losses can change because of changes in the variance of the shocks (e.g., good luck), detection of a forecast failure does not necessarily mean that a forecast model should be abandoned. care must be exercised to assess the source of the changes. but if a model is shown to provide stable forecasts, it can more safely be applied in real time."
"it, v it are idiosyncratic errors and some or all components of β(t 0 b ) differ pre and post-break. due to the correlation between x it and u it, least-squares for each cross-sectional regression could be inconsistent. [cit] that the break point t 0 b can be consistently estimated as both n and t go to infinity. [cit] who study estimation and inference with and without interactive fixed effects using lasso methods. [cit] by allowing a factor structure in the error terms. [cit] study structural breaks in a heterogeneous large panel with interactive fixed effects. they show the consistency of the estimated break fraction and break date under some conditions."
"gaussian random variables. [cit] consider structural breaks in conditional quantiles using the minimum description length principle. while the framework is quite general, the method cannot consistently estimate the number of breaks jointly with their location. [cit] consider estimation and inference of common breaks in panel data models with endogenous regressors. the regressors and errors are, however, restricted to be i.i.d. processes, a common feature in this literature up to now. allowing for general mixing regressors and errors has not, to our knowledge, been achieved. work is needed to achieve the level of generality available using standard procedures. nevertheless, it remains a promising approach, especially in the context of very large datasets."
"from a seizure semiology perspective, the findings presented here have a clear meaning. although there is no appreciable evidence of seizure anticipation from the functional network point of view, in line with recent findings (29), a suggestive change in network behavior show up just after seizure onset. this difference between ps with and without sg regarding modularity determines which type of seizure will develop. local analysis based in ls display identical patterns in both kinds of seizures; however, global network findings show the existence of a differentiated behavior just after seizure onset and thus forecast a sg. certainly, a central mechanism should be involved here. more difficult to explain is the potential relationship between the structural alterations in the cortical network due to the temporal gliosis found in this patient and the functional behavior. this alteration (figure 1) is located in the lateral side of the temporal lobe and encompasses a small subset of temporal electrodes (t9-t13 and t18-t21). whether a relationship between the high-ls area in electrodes t3-t4-t12-t3 and the gliosis area exists or not remains to be studied and evaluated."
"hence, the method is quite robust. [cit] filter. this work is related to a recent strand in the literature that attempts to deliver methods robust to low frequency contaminations. one example pertains to estimation of the long-memory parameter. it is by now well known that spurious long-memory can be induced by level shifts or various kinds of low frequency contaminations. qu (2007, 2010), [cit] exploit the fact low frequency contaminations will produce peaks in the periodograms at a very few low frequencies, and suggest robust procedures eliminating such low frequencies. [cit] provide a method applicable to various time series models, such as arma, garch and stochastic volatility models."
"in the mesial area, two observations are noteworthy. first, immediately before seizure onset, high-ls activity was found near electrode m2, followed by a displacement of this activity toward other mesial electrodes. in each case, the pattern was very similar from m1/m2 toward m8. although the interictal ls activity at electrodes m1/m2 was intense, as noted above, it was nonetheless intermittent during the interictal period, as evident in figures 2b,d; however, all seizures appeared to begin with high ls near electrodes m1/m2. in contrast, the ls displacement from m1 to m8 appeared to occur during the first part of the seizure, at least in the first three seizures. in these cases, when high-amplitude oscillations began to appear [compare panels (a) and (c) in figures 3-5 ] during the second half of the ictal activity, an abrupt jump in activity from m7/m8 back to m1/m2 was observed. this change was not the case for the ps without sg. in the ps without sg, the ls remained localized near m8 until seizure termination ( figure 6a) ."
"in bid-price control, threshold or bid prices are set for the resources (cabins and lifeboat seats) and a cruise product is sold only if the offered fare exceeds the sum of the threshold prices of all the resources needed to supply the product [cit] . the dual variables can be used as bid prices. the request for product i is accepted if"
"two remarks are noteworthy about the above observations. the first is related to the increasing global synchronization originating during the middle of the seizure for the case of ps with sg ( figures 3d, 4d, and 5d ). all changes in the global network measures that reached extreme values after seizure termination were initiated in the second part of the seizure. at that moment, www.frontiersin.org however, most of the measures had attained interictal/preictal values, except for the average path length and perhaps modularity. the average path length increased from the time of seizure onset, without being accompanied by appreciable changes in other measures. the only explanation for this situation would be a change in the functional topology without any accompanying change in the global synchronization levels. the weak increase in modularity also supports this assumption and suggests that some interregional links may be converted into intra-regional ones. note moreover that modularity reaches values close to 0.5, suggesting a highly modular network (8) ."
"the analysis presented here suggests two types of therapeutic approaches: on the one hand, a local procedure, whether by a resective surgery or non-invasively by gamma-knife surgery (33), which eliminates interictal local synchronization areas. this procedure would provide a way to abolish the stereotyped ls spread during the first part of the seizures, both with and without sg, because, as we have shown above, the spread of ls areas always begins in the more intense ls regions. the second target for a therapeutic intervention would be trying to block the sg onset. sg begins with an increase in both the average path length and the modularity. this behavior does not occur during the seizure without sg. thus, a possible anti-sg treatment could be the application of drugs that allow inter-region connections, that is, in the regions' boundaries, increasing thus inter-regions' connections and therefore blocking the increase in both the modularity and the average path length."
"the vast majority of tests considered in the econometrics literature imposes some trimming that does not consider the possibility of a break occurring near the beginning or end of the sample. this is not so in the statistics literature. such tests lead to a different limiting distribution, mostly inducing a log-log rate of divergence that needs to be accounted for (e.g., csörgő and horváth, 1997) . these tests usually have poor finite sample properties."
"y it which are the estimates of µ i1 and µ i2, respectively. define the sum of squared residuals for nonetheless, noting that a gaussian random walk and a standard wiener process have the same distribution at integer times, one can apply the change in variable argument leading to the same inference procedure as in the univariate case, which now only holds approximately, so that inference works as in the univariate setting. [cit] further considers the case of (possibly) simultaneous break in mean and variance and proposes a qml method."
"a representation of both eqs 1 and 2 is displayed on the grids in figure 1a, in which ls is represented by the size of the electrode; greater electrode size corresponds to greater ls. in addition,s i is represented by the color of the electrode, ranging from white (no excitation) to red (high excitation). no relationship between the two measures exists. some electrodes displayed high excitability with low ls, that is, with low levels of synchronization of the activity with its first neighbors. this phenomenon can be seen, for example, at electrode m7. the cortical area covered by this electrode displays irritative activity (orange color) but is actually disconnected from its first neighbor m8. in contrast, the area covered by electrode t4 is well connected with its neighbors but has rather low irritative activity. another feature of figure 1a is the representation of links between electrodes. a threshold of 0.5 (see network analysis below) has been used to represent functional connectivity between areas; i.e., only those areas having activities with a correlation coefficient (eq. 1) greater than or equal to 0.5 are connected by straight lines. moreover, the thicknesses of these lines are proportional to the strengths of these correlations. figure 2a shows ls activity, as calculated using eq. 2, during a 1-h interictal period. a stable, inhomogeneous pattern of ls appeared throughout the entire recording. to better display ls, we thus made one additional transformation; for each epoch, we organized the ls values in decreasing order of intensity and thereafter plotted only the first 10 most intense values, as shown in figure 2b, in which red colors imply more intense ls activity (higher values). for example, areas covered by electrodes t4 and t13 exhibit higher ls."
"the objective function is to maximize the revenue of the cruise line when there is a stochastic demand of customers from different market segments, including singles and families. the first constraint shows that the total number of reservations does not exceed the capacity of the mth type of cabin. the second constraint shows that the total number of guests does not exceed the lifeboat seat capacity."
"for those problems with stochastic variables, a traditional solution method is deterministic programming. the main difference between deterministic programming and stochastic programming is how to treat with the stochastic demand. deterministic programming directly replaces the stochastic demand variable with the mean value or expected value of demand. the deterministic linear programming model is as follows:"
"in this study, we analyzed in detail both interictal and ictal activities, which were recorded with subdural electrodes, from the point of view of functional connectivity and network methodologies. our results suggest that two mechanisms may be involved in seizure onset and evolution. the first mechanism is related to local activity. ls activity follows a stereotypical behavior from the start of a seizure and lasts until the middle of the seizure, regardless of whether sg occurs. if no sg appears, the seizure ends at that moment. in contrast, if sg develops, further ls behavior appears. during the second half of the seizure, high-ls areas are located at the same sites that exhibit high interictal ls. the second mechanism, inferred from the global network measures, appears just after seizure onset. from that moment forward, network dynamics follow two different routes. in ps without sg, the only appreciable change is decreasing modularity after seizure onset. in contrast, ps with sg displays a trend toward a generalized synchronization starting at the middle of the seizure, and the network parameters reach extreme values just after the seizure ends."
"the functional connectivity between the time series of every pair of electrodes, i and j, was calculated using the absolute value of the pearson correlation coefficient"
"additional remarks on factor models follow. [cit] analysis assumes a known number of factors. this is a strong restriction which future research should relax. [cit] developed an innovative adaptive group lasso estimator that can determine the number of factors and the break fraction simultaneously but it is valid only under large breaks in the loading matrix. therefore, joint estimation of the break points and the number of factors remains an open issue even from a computational perspective."
"in terms of functional network structure, the results presented here are not fully in agreement with the current knowledge. the small-world characteristic displayed during the interictal stage is in accordance with the more general literature of cortical functional networks (30) and of healthy subjects (31) but in contradiction with more specific studies (10) where a shift from a regular to a small-world architecture is reported during the preictal/ictal transition. the results presented here conversely are in accordance with an interictal small-world architecture with a slight shift toward a more regular network during the first part of the seizures with sg and a subsequent shift toward a more random network during the postictal period. this is not the case for ps without sg. in this last case, no appreciable changes occur whether in the ictal or in the postictal periods. in regard to seizures with sg, our findings would thus suggest a shift toward a more modular, instead of regular, network topology during the seizures, as compared with the interictal/preictal period. this change, however, is not observed in the case of the seizure without sg."
"forecasting. we first discuss the concept of forecast failure (or breakdown) and describe methods proposed to detect changes in the forecasting performance over time. second, we discuss techniques to compare the relative predictive ability of two competing forecast models in an unstable environment. it is useful to clarify the purpose of forecast breakdown tests."
"in temporal lobe epilepsy (tle), seizures are thought to originate in specific areas of the cortex, known as seizure-onset zones (soz), before spreading to other areas, known as epileptogenic zones (ez), some of which overlap with the soz. ez are essential for seizures to propagate (1) . resection or disconnection of these areas, principally of the ez, which is usually identified as the epileptic focus, from the rest of the brain is currently considered the best way to eliminate seizures in drug-resistant tle patients. this \"single-focus\" model has been challenged (2-4) in favor of a network model in which emphasis is shifted from the epileptic focus (or foci) toward the properties of the limbic network itself. fortunately, in recent years, there have been great advances in the development of mathematical and numerical methods that aim to uncover the hidden properties of complex systems with mutually interacting parts. the introduction of the functional connectivity concept (5, 6 ) and the development of complex network methodology (7, 8) in recent years have not only provided analytical tools for exploring the physiology of the brain in general (9) and many of its pathologies, including epilepsy (10) (11) (12) (13) (14) (15), but also allowed researchers to formulate new hypotheses about brain function/dysfunction. in the case of epilepsy, much work has focused on the properties of global and local cortical networks in recent years. local synchronization (ls) and inhomogeneities in synchronization distribution, in particular, have received much attention (16) (17) (18) (19) (20) (21) (22) (23) (24) . in these studies, the existence of an inhomogeneous distribution of functional connectivity among cortical areas in patients with tle has been examined extensively. although it is tempting to associate high ls with the traditional epileptic focus, whether a direct relationship can be established between them is currently unclear. however, these areas almost certainly play a role in seizure development because the excision of cortical areas that include high synchronization zones appears to abolish the appearance of seizures. moreover, it has been suggested (18) and shown (23) that these areas also exhibit behavior that is very stable during the interictal stage. however, all previous work on ls areas has focused on the interictal period. to the best of our knowledge, no study has examined ls evolution during seizures, although many other network characteristics, such as centrality measures, have been examined during this critical period in epilepsy dynamics (10) (11) (12) (13) ."
"we have a spurious regression and the parameters are not identified. to be able to properly interpret the tests, they should be used in conjunction with tests for the presence or absence of cointegration allowing shifts in the coefficients [cit] ."
"both types of seizures began in areas with high interictal ls. however, keeping in mind the above observations, ps without sg displays a pattern similar to the first parts of ps with sg. a central element during the onset of generalization appears to be the re-establishment of ls at both m1/m2 and t4, which is lacking in the seizure without sg. we will next explore this issue using global network measures. to obtain a global picture of seizure dynamics, we studied four network properties. these results are displayed in panels (d) of the preictal measurements were similar in the four seizures, and differences between ps with and without sg appeared just after seizure onset. at that time, modularity, in the case of ps with sg, suffered a brief drop, but it recovered to preictal values by the middle of the seizure. in the case of ps without sg, the initial drop after seizure onset was instead maintained throughout the seizure period and continued into the postictal period, with no appreciable changes in other network measures. in contrast, all changes during ps with sg appeared during the middle of the seizure, when the average path length was maximized. in the later parts of the seizure, a clear trend toward the postictal period appeared in every measure, producing extreme values, such as the lowest modularity, lowest average path length, highest density, and highest clustering coefficient that were attained after seizure termination. perhaps, the clearest picture of this trend can be seen in figure 5d (third seizure), although similar features are replicated to varying extents in the other figures. these changes clearly imply that a \"wave\" of increasing global synchronization with a temporal origin in the middle of the seizure."
"in a cointegrating regression with i(1) [cit] show that the estimated break fractions are asymptotically dependent so that confidence intervals need to be constructed jointly. if only the intercept and/or the coefficients of the stationary regressors are allowed to change, the estimates of the break dates are asymptotically independent."
"the third and fourth constraints mean that the number of requests of product that are accepted should be not more than the demand to prevent vacant cabins. as d i and d j are randomized variables, the third and fourth ones denote uncertain constraints. the fifth and sixth constraints are the non-negative integer constraints. therefore, model 1 is a stochastic integer programming model."
"estimating breaks one at a time. bai (1997b) [cit] show that one can consistently estimate all break fractions sequentially. when estimating a single break model in the presence of multiple breaks, the estimate of the break fraction will converge to one of the true break fractions, the one that allows the greatest reduction in the ssr. then, allowing for a break at the estimated value, a second one break model can be applied which will consistently estimate the second dominating break, and so on. [cit] shows that this result fails to hold for breaks in a linear trend model. bai (1997b) considers the limit distribution of the estimates and shows that they are not the same as those obtained when estimating all break dates simultaneously. except for the last break, the limit distributions depend on the parameters in all segments. he suggests a repartition procedure, which re-estimates each break date conditional on the adjacent break dates. the limit distribution is then the same as when the break dates are estimated simultaneously."
"note that if µ i2 − µ i1 were i.i.d. random variables with positive variance then the above limit with n −1 replacing n −1/2 should converge to a positive constant. thus, the condition does not require every unit to have a break. the estimation method involves least-squares."
"in this study, we investigated changes in high-ls areas during seizures, that is, the evolution of ls during the preictal, ictal, and postictal stages in such areas. we also calculated several standard global network measures. to generate as detailed an analysis as possible, we analyzed seizure evolution in a single patient suffering from tle with partial seizures (ps) with secondary generalization (sg) and ps without sg. we present an analysis of subdural recordings of four seizures, three of them with sg and the fourth one without sg; studying both types of seizures in the same patient allowed us to discriminate between local and global seizure mechanisms."
"any change in δ 0 imply a change in δ *, except for a knife-edge case when the change in the bias exactly offsets the change in δ 0 . hence, one can still identify parameter changes using ols and a change in δ 0 will, in general, cause a larger change in the conditional mean of y t ."
"we also evaluated community structures in the whole network. a commonly used community-detection algorithm involving the maximization of modularity (27) was used. modularity measures how well a given partition or division in a community in a complex network corresponds to a natural or expected sub-division. in our particular case, we have taken the \"natural\" community structure to be the one with three sub-networks corresponding to the parietal, lateral temporal, and mesial temporal electrode grids. thus, the modularity of a network is a measure of how www.frontiersin.org close the actual community structures calculated for each epoch are to the community structure composed of the three regional sub-networks."
"tests valid with i(1) regressors. with i(1) regressors, a case of interest is a system of cointegrated variables. [cit] considers the null hypothesis of no change in both coefficients and proposed sup and mean-lm tests for a one time change. he also considers a version of the lm test directed against the alternative that the coefficients are random walk processes. kejriwal and perron (2010a) provide a comprehensive treatment of issues related to testing for multiple structural changes at unknown dates in cointegrated regression models using the sup-wald test. they allow both i(0) and i(1) variables and derive the limiting distribution of the sup-wald test for a given number of cointegrating regimes. they also consider the double maximum tests and provide critical values for a wide variety of models that are expected to be relevant in practice. the asymptotic results have important implications for inference. it is shown that in models involving both i (1) and i(0) variables, inference is possible as long as the intercept is allowed to change across regimes. otherwise, the limiting distributions of the tests depend on nuisance parameters."
"optimal tests. [cit] consider a class of tests that maximize a weighted average local asymptotic power function. they are weighted functions of the standard wald, lm or lr statistics for all permissible break dates. using either of the three basic statistics leads to tests that are asymptotically equivalent and we proceed with the wald test. assuming equal weights are given to all break fractions in some interval [ǫ 1, 1 − ǫ 2 ], the optimal test for distant alternatives is the so-called exp-type test: exp-"
"with i.i.d. errors, maximizing the wald statistic is equivalent to minimizing the ssr, which can be solved efficiently and the wald test for k changes is:"
"panels. panel data studies have become increasingly popular including inference about breaks. the literature on estimating panel structural breaks can be categorized as assuming whether the parameters of interest are allowed to be heterogenous across units or not. [cit] for corresponding methods for homogeneous panels. [cit] it is argued that it can be relaxed without affecting the consistency result, though for the asymptotic distribution one requires the cross-sectional dependence to be not too strong. the assumption on the break sizes is lim"
"they propose a modified sup-wald test that has good size and power properties. note, however, that the sup and mean-wald test will also reject when no structural change is present and the system is not cointegrated. hence, the application of such tests should be interpreted with caution. no test is available for the null hypothesis of no change in the coefficients allowing the errors to be i(0) or i(1). this is because when the errors are i(1),"
"non monotonicity in power. [cit] for changes in a trend function. in more general contexts, the sup-wald and exp-wald tests have monotonic power when only one break occurs under the alternative. [cit], the mean-wald test can exhibit a non-monotonic power function, though the problem has not been shown to be severe. all of these, however, suffer from important power problems when the alternative involves two breaks [cit] . this suggests that a test will exhibit a non monotonic power function if the number of breaks present is greater than the number accounted for."
"second, the difference between the modularity behaviors just after seizure onset in two types of seizures is striking. a sustained decrease in modularity during ps without sg was the main characteristic of this type of seizure because no other changes occurred. this observation implies a reorganization of the network functional topology toward greater global connectedness without accompanying changes in global synchronization, i.e., toward more inter-regional and fewer intra-regional connections. these changes occurred immediately after seizure onset without sg, in contrast to the case of ps with sg. in ps with sg, the modularity underwent more changes throughout the course of the seizure."
"(1) mesial area: an increase in ls activity at seizure onset was observed at the lower mesial electrodes, m1/m2. the ls activity then shifted and moved along the other mesial electrodes until the middle of the seizure. a disruption of ls activity followed, with the activity moving from m7/m8 back to m1/m2. (2) lateral temporal area: stable ls activity was observed until approximately the middle of the seizure. a disruption in the ls activity was then observed, which was followed by a restoration of ls, most notably at electrode t4, until seizure termination."
"(1) mesial area: an increase in the ls activity at seizure onset was observed at the lower mesial electrodes, m1/m2. the ls activity then shifted and moved along the other mesial electrodes until the seizure ended. (2) lateral temporal area: stable ls activity was observed until approximately the middle of the seizure. the ls activity was then disrupted until seizure termination."
"for model (1) with i.i.d. errors, the lr and wald tests have similar properties, so we shall discuss the wald test. for a single change, it is defined by (up to a scaling by q):"
"one hour of interictal ls activity is represented in figures 2a,b . in figure 2a, ls activity is calculated using eq. 2 for each electrode location. as shown previously (23), high values of ls are stable over time. in this particular case, areas near electrodes t4 and t12, which are spatially continuous and both located on the lateral side of the temporal lobe (see figure 1a), have greater ls and more stable behavior as compared with other cortical areas."
"a small-world behavior of the interictal network shows up when comparing against random networks with identical number of nodes and links as the originals ( table 1) . during the interictal period, random networks have higher average path length -4.22 in random networks and 2.63 in the interictal one -and much lower average clustering coefficient -0.04 in random networks and 0.59 in the interictal one."
"focus is solely on linear models and deals with so-called off-line methods whereby one wants to retrospectively test for breaks in a given sample of data and form confidence intervals about the break dates. given the space constraint, our review is obviously selective. the aim is to provide an overview of methods that are of direct usefulness in practice as opposed to issues that are mostly of theoretical interest."
"for alternatives close to the null value of no change the optimal test is the mean-w t test: [cit] approach the optimality issue from a different perspective using the approximate bahadur measure of efficiency. they show that tests based on the mean functional are inferior to those based on the sup and exp (which are as efficient) when using the same base statistic. when considering tests that incorporate a correction for potential serial correlation in the errors: a) for a given functional, using the lm statistic leads to tests with zero asymptotic relative efficiency compared to using the wald statistic; b) for a given statistic the mean-type tests have zero relative efficiency compared to using the sup and exp versions, which are as efficient. hence, the preferred test should be the sup or expwald tests. any test based on the lm statistic should be avoided. such results, and more discussed below, call into question the usefulness of a local asymptotic criterion to evaluate the properties of testing procedures; [cit] ."
"testing for common breaks. [cit] consider testing for common breaks across or within equations in a multivariate system. the framework is very general and allows stationary or integrated regressors and trends. the null hypothesis is that breaks in different parameters occur at common locations or are separated by some positive fraction of the sample size. under the alternative hypothesis, the break dates are not the same and need not be separated by a positive fraction of the sample size across parameters. a quasilikelihood ratio test assuming normal errors is used. the quantiles of the limit distribution need to be simulated and an efficient algorithm is provided. [cit] extend this work to cover the case of testing for common breaks in a system of equations involving joint-segmented trends. [cit], are common [cit] ."
"model (9) is an observationally equivalent factor model, where the number of factors is doubled and the factor loadings are time invariant. under (9), [cit] . the estimates of the break points are the minimizer of"
"to evaluate the global network properties, several network measures were calculated for the interictal period. the means and standard deviations of these network measures are shown in table 1 . the measure corresponding to the community structure, i.e., the modularity exhibited less variation than other measures, with relative variations during the interictal stage of 10.32. this measure yielded a value close to 0.5, meaning that intra-community links within the three principal brain regions, the parietal, lateral temporal, and mesial temporal areas, were as abundant as intercommunity links. the densities of the links and the average path lengths were also calculated. the temporal stability of the density of the links was found to be weak, with this measure exhibiting a relative standard deviation of 50%. however, as will be observed below, its behavior is highly stable when comparing it during the ictal activity. finally, an average clustering coefficient over all the nodes of the network was also calculated (8), hereinafter referred to simply as clustering coefficient."
"the ls activity is displayed in panel (b) in each figure. as above, the ls activity during the first three ps with sg is displayed in figures 3b, 4b, and 5b. figure 6b shows the ls activity during the ps without sg. by ordering the ls values by intensity, we obtain figures 3c, 4c, 5c, and 6c. we observed similar patterns for each of the three cortical areas examined."
"simulations show that the tests have good sizes for a wide range of truncations. the exact truncation does not really matter, as long as some of the very low frequencies are excluded."
"as observed during the interictal periods, areas with high ls were observed, specifically near electrode p6 (figure 2b) . however, these areas of high ls lost their strength before, during, and after the seizure in every case."
"tests for structural changes in multivariate systems. consider a sup-wald test for a single common change in a multivariate system. [cit] extend the analysis to multiple structural changes. they consider the case where only a subset of the coefficients is allowed to change, whether in the parameters of the conditional mean, the covariance matrix of the errors, or both. the tests are based on the maximized likelihood ratio over permissible partitions assuming i.i.d. errors. they can be corrected for serial correlation and heteroskedasticity when testing for changes in the parameters of the conditional mean assuming no change in the covariance matrix of the errors."
"on the lateral temporal side (t1-t32), the ls activity remained stable, at least during the first part of the seizure. a disruption occurred at approximately the middle of the ictal activity, during which no ls activity was observed at electrodes t4 and t12, but this ls activity was restored in the second half of the seizure, most www.frontiersin.org notably at electrode t4. however, this restoration of ls activity near electrode t4 did not occur in the fourth seizure."
"estimation in a system of regressions. substantial efficiency gains can be obtained by casting the analysis in a system of regressions. consider estimating a single break date in multivariate time series allowing stationary or integrated regressors as well as trends. [cit] considers a segmented stationary var model with breaks occuring in the parameters of the conditional mean, the covariance matrix of the error term or both. [cit] who consider models of the form"
"until now, there are only a few literatures about cruise two-dimensional revenue management. [cit] formulates a deterministic linear program considering the lifeboat seat capacity constraint, but this model is a simplified version which assumes that each product consists of exactly one cabin and at least two guests [cit] present an effective solution for cruise inventory application that incorporate a nested class allocation (nca) model which is a modified version of emsr [cit] ) and a dynamic class allocation (dca) [cit] ."
"therefore, the aim of this paper is twofold. the first one is to show a detailed analysis in a single patient with several, although of different type, seizures, which otherwise may overlook important details in a more general and wide-ranging study. the second aim is to show the dynamical interplay between local and global network characteristics during the ictal stage."
"revenue management is the method and practice of controlling the availability and pricing of products or services in different booking classes to maximize expected revenues or profits [cit] . it works by attending to the particular, not the general [cit] ) [cit] . the cruise line industry grows very fast in recent years. the cruise line industry has typical characteristics for applying revenue management, including fixed capacity, perishable inventory, market segmentation, advanced booking, high fixed cost and low variable cost, large demand fluctuation. but one of the important differences between the cruise line industry and other travel industries is that the capacity has two dimensions, which are the number of cabins and lifeboat seats. there are different types of customers, such as families and singles. these characteristics are a little similar as the container shipping industry [cit] . if the cruise product is defined to be the attribute combination of a cabin category, fare class, and number of guests, then the cruise line faces the following problem: under the environment of demand uncertainty and the constraints of two-dimensional capacity, how to allocate the limited capacity to all kinds of cruise products, or how many booking requests for each kind of cruise product should be accepted to maximize the total revenue?"
"most of the work on structural breaks in panels focused on common breaks, in which case . one may infer that simply adding a cross-sectional dimension yields more information and precise estimates. this is misleading because the result crucially relies on the assumption that the break is common to all units. although this may be relevant in practice, the results should be interpreted carefully."
"of obtaining more precise estimates and more powerful tests. the method of estimation is standard least-squares (ols), i.e., minimizing the overall sum of squared residuals (ssr)"
the variable k μ represents the dual associated with the kth capacity constraint and l μ represents the dual associated with the lifeboat seat capacity.
"the rest of the paper is organized as follows. section 2 describes the selected origami design, kinematics, and results from the stiffness and durability tests. in section 3, the hardware architecture and design of the robot are presented, including actuation strategy, embedded sensors and electronics, and workspace analysis. the robot's capabilities for manipulating arbitrary objects, such as a shuttlecock, egg shell, paper cup, and cube block, were tested; the results are provided in section 4."
"d denotes a small displacement between the two plates caused by the thickness of the folded structure. using eq. (1), the relationship between θ and d can be achieved by"
"serve as the pseudonyms of node i. the private keys k p rv i enable node i to digitally sign messages, and the digital certificate validates the signature authenticity. when a node i receives a probe from another node, it controls the legitimacy of the sender by checking the certificate of the public key of the sender and the physical identity, e.g. bluetooth mac address. after that, i verifies the signature of the probe message. subsequently, if confidentiality is required, a security association is established (e.g., with diffie-hellman)."
"where e i (t) is the location privacy level when the location proof exchange request is accepted while e i (t ′ ) is the location privacy level at the next scheduled location proof updating cycle. the difference between the two indicates the privacy loss if this location proof exchange request is accepted. the location proof exchange request is only accepted when the privacy loss is less than a predefined threshold. the drawback of the user-centric model is that nodes may have misaligned incentives (i.e., different privacy requirement), which can lead to failed attempts to achieve location proofs. we use dummy proofs in algorithm 1 to deal with failed attempts. the detailed scheduling protocol for witness is presented in algorithm 2."
"in this section, we consider deployment feasibility for applaus, including the computation and storage constraint, power consumption, as well as the proof exchange latency. we also use simulations to compare the performance of applaus with a baseline scheme, and evaluate the privacy level against powerful statistical analysis attacks."
"for preliminary testing of the robot's manipulation capabilities, three different objects with different shapes and weights were selected, including a shuttlecock (0.011 lb), an egg shell (0.011 lb), and a cube block with varying weight (0.02 lb-0.17 lb), as shown in fig. 9 (top) . to increase friction for better grasping, each fingertip was covered by a 3d printed cover made of flexible polyester. in future experiments, the fingers may be covered with different fingertips using different materials and shapes. origamibot-ii was programmed to follow four steps: (1) ready to grasp, (2) grasp the object, (3) lift the object up and bend the arm while holding the object, and (4) release the object. the entire sequence took about 10 seconds."
"in applaus, mobile nodes communicate with neighboring nodes through bluetooth interface, and communicate with the untrusted server through cellular interface. based on different roles they are playing in the process of location proof updating, they are categorized as prover, witness, location proof server, certificate authority or verifier. the architecture and message flow of applaus is shown in figure 1 ."
"more specifically, we are considering two-level crossvalidation from both location proof server and ca point of view and then integrating the results to ensure accuracy. the server-level validation is performed on individual location proof based on its timestamp and location information, where all concurrent and co-located location proofs from other pseudonyms are used to verify the reliability of the target location proof. for example, when a pair of location proofs by pseudonyms p a and p b are uploaded, the server checks if there are concurrent and co-located location proofs from other pseudonyms, which do not have any interaction with p a and p b . if there are such location proofs, p a and p b are suspicious to be colluders, and we then assign an appropriate trust level for the location proof. future research issues lie in how to design trust level metric appropriately for each location proof, and evaluate an optimal server-level threshold to rule out the bogus location proofs."
"in this paper, we propose a privacy-preserving location proof updating system (applaus), which does not rely on the wide deployment of network infrastructure or the expensive trusted computing module. in applaus, bluetooth enabled mobile devices in range mutually generate location proofs, which are uploaded to a untrusted location proof server that can verify the trustworthy level of each location proof. an authorized verifier can query and retrieve location proofs from the server. moreover, our location proof system guarantees user location privacy from every party. more specifically, we use statistically changed pseudonyms at each mobile device to protect location privacy from each other, and from the untrusted location proof server. we use user-centric location privacy model in which individual users evaluate their location privacy levels in real-time and decide whether and when to accept a location proof request based on their location privacy levels. extensive experimental and simulation results show that our scheme, besides providing location proofs effectively, can significantly preserve the source location privacy."
"there are many kinds of location-sensitive applications. one category is location-based access control. for example, a hospital might limit access to patient information by doctors or nurses unless they can prove that they are in a particular room of the hospital [cit] . meanwhile, one class of popular location-aware applications works only when users are able to prove their history locations [cit], such as auto insurance quote in which auto insurance company might provide discounts to drivers who can prove that they take high-safety routes during their daily commutes, fraud reduction on ebay in which location proofs from the seller can serve as additional evidence that the seller's account has not been compromised by an attacker, police investigations in which police forces are interested in finding ways for people to be able to provide efficient and trusted alibis, and location-based social networking in which a user can ask for a location proof from the service requester and accepts the request only if the sender is able to present a valid location proof."
"to actuate the arm, four sg-5010 motors were installed on the top of the manipulator. each of these motors pulls (or releases) a nylon cable rolled on a 3d-printed pulley, providing 9.55 lbf-in. the radius of the pulley is 2 and the maximum force to pull the cable is 4.85 lbf. for the gripper, three sg-90 motors were installed on the gripper plate for bending each of the three fingers. can pull the cable with 1.56 lbf-in. the cable is connected at a 1.18 inch distance from the servo axis so that each finger is bent by a 1.32 lb pulling force. arduino uno r3, used as the main processing board for motor control, provides an easy programming and debugging environment for sensor-based feedback control of the motors. raspberry pi requires an external analog to digital converter (adc), such as mcp3008 or ads1115, for analog sensing and thus requires a logic converter to use 5 v processing modules. in addition, arduino uno r3 collects the analog data and performs the motor control by using the pre-installed library and built-in functions."
". origamibot-ii: three-finger origami manipulator using the origami twisted tower for constructing the arm and fingers. four servo motors, installed on the top of the frame, control the arm and three mini-servos control the three fingers for grasping an object."
"the rest of the paper is organized as follows: we first introduce the preliminaries of our scheme in section ii. after that, section iii presents our location proof updating scheme. section iv presents the source location privacy analysis and how to deal with colluding attacks. the performance of our scheme is evaluated in section v. finally, we describe the related work in section vi and conclude the paper in section vii."
"the privacy property of our protocol is ensured by the separation of privacy knowledge: the location proof server only knows pseudonyms and locations, the ca only knows the mapping between the real identity and its pseudonyms, while the verifier only knows the real identity and its authorized locations. attackers are unable to learn a user's location information without integrating all the knowledge. therefore, compromising either party of our system does not reveal privacy."
"to study the feasibility of our scheme, we have implemented an experimental prototype of applaus based on the technique presented in the previous sections. the prototype has two software components: client and server. the client is implemented in java on top of android developer phone 2 (adp2), which is equipped with 528mhz chipset, 512mb rom, 192mb ram, bluetooth and gps module, and running google android 1.6 os. this device can communicate with the server anytime through at&t's 3g wireless data service. the server component is implemented in c++ on a t4300 2.1ghz 3gb ram laptop. it stores the uploaded historical location proof records and manages corresponding indices using mysql 5.0. we use two android phones to communicate with each other to test the practicality of our scheme."
"definition 3: a system has the property of pseudonym unlinkability if any pseudonyms p 1, p 2, · · ·, p m of an identity i presented in the location proof records cannot be inferred from one to another: ∀i, j, ∀o, prob("
"to estimate the stiffness and force capacity as well as dynamic characteristics such as fatigue over time, a twisted tower made of construction paper was tested by an instron servo-hydraulic testing machine (model 8501). 1 this machine has a maximum displacement of 4 inches, so a six-layer twisted tower with maximum 3-inch stroke was used for testing. a total of 1000 sinusoidal cycles were modeled with 1 hz frequency and 1.5-inch amplitude (3-inch stroke). a 250 lb capacity force transducer was used for the test. as shown in fig. 4 (left), the average stiffness for 1-inch contraction to 3-inch contraction was 8.61 pound/inch (lb/in) for loading and 5.81 lb/in for unloading at the first cycle. another observation was the fatigue of the structure by repetitive motion. figure 4 (right) shows that the stiffness changed gradually over time. it is anticipated that the stiffness and fatigue properties would vary depending on (1) the number of the layers, (2) physical size, and (3) material used to fold the structure."
"as commonly assumed in many networks, we consider an online certification authority (ca) run by independent trusted third party that pre-establishes the credentials for the devices. in line with many pseudonym approaches, to protect location privacy, we assume that prior to entering the network, every mobile node i registers with the ca that pre-loads a set of m public/private key pairs k"
"this paper proposed a privacy-preserving location proof updating system, called applaus, in which co-located bluetooth enabled mobile devices mutually generate location proofs, and upload to the location proof server. we use statistically changed pseudonyms for each device to protect source location privacy from each other, and from the untrusted location proof server. we also develop user-centric location privacy model in which individual users evaluate their location privacy levels in real-time and decide whether and when to accept a location proof exchange request based on their location privacy levels. to the best of our knowledge, this is the first work to address the joint problem of location proof and location privacy. extensive experimental and simulation results show that our scheme can provide location proofs effectively while preserving the source location privacy at the same time."
"now we look at how an adversary may reveal location information by analyzing the location proof history. suppose the attacker has sufficient resources (e.g., in storage, computation and communication). first, the attacker may simply monitor deny location proof exchange request 4: else 5: accept location proof exchange request 6: end if and examine the content of a record that may contain the user's identity and location. second, even if the user's id is encrypted or pseudonymized, it is easy for the adversary to trace back all the location activities related to the same id once its pseudonym is discovered. third, even though the user's pseudonyms change periodically, it is still possible for the adversary to infer this user's other pseudonyms from one pseudonym if these pseudonyms change at similar time or locations. moreover, the attacker may perform more advanced traffic analysis including rate monitoring and location correlation. in a rate monitoring attack, the attacker tries to monitor and correlate location proof updating rates from different pseudonyms. in a location correlation attack, the attacker may observe the correlation in updated location between a node and its neighbor, attempting to deduce a relationship."
"our study can be applied to networks where mobile nodes are autonomous entities equipped with wifi or bluetooth enabled devices. for example, it could be a pervasive communication system (a mobile ad hoc network) such as a vehicular network [cit], a delay tolerant network [cit], or a network of directly communicating hand-held devices [cit] in which mobile nodes in proximity automatically exchange information. in this paper, we focus on mobile networks where bluetooth enabled devices such as cellular phones communicate with each other in short-range bluetooth protocol. given its appropriate range (about 10m) and low power consumption, bluetooth is a natural choice for mutual encounters and location proof exchange."
"after analysis of many origami designs, the twisted tower design by mihoko tachibana was selected. 1, 2 this design was used to build a robotic arm with three fingers. the twisted tower is made of identical origami segments that are connected in an octagonal pattern and stacked to form a tower. to begin creating the tower, a single piece of rectangular paper is selected and folded following a specific sequence. the size of the rectangular piece determines the diameter and height of each octagon layer, and therefore must be determined based on the desired size of the arm and workspace. 1 the twisted tower requires 24 origami segments for the first octagon layer and 16 segments for each additional layer. any number of octagon layers can be added to form a tower with a desired height. figure 2 shows extending, contracting, and bending motions realized by the twisted tower. the relative orientations of the top and base layers are dependent upon the twisting direction in each layer. the twisted tower behaves like a spring, while the overall diameter remains the same during extension and contraction. in addition, the \"modular\" origami design, referring to an assembly of multiple origami segments, keeps it more stable and durable than single-paper origami patterns. 11"
"we assume that an adversary aims to track the location of mobile nodes. an adversary can have the same credential as a mobile node and is equipped to eavesdrop communications. we assume that the adversary is internal, passive and global. by internal, we assume that the adversary is able to compromise or control individual mobile device and then communicate with others to explore private information, or individual devices would collude with each other to produce false proofs. by passive, we do not assume the adversary can perform active channel jamming, mobile worm attacks [cit] or other denial-of-service attacks, since these attacks are not related to location privacy. by global, we assume that the adversary can monitor, eavesdrop and analyze all the traffic in its neighboring area due to short range communications."
"despite the potential physically demonstrated by origamibot-ii, the construction papers and plastic materials used for the arm and fingers are relatively fragile and susceptible to fatigue caused by repeated folding and unfolding at creases. while the current robot runs reliably without any noticeable fracture, several structural problems in the mechanical design were identified during repeated experiments, including (1) pulleys for the cables in the arm directly attached to the servo motor without any structural support to keep the pulleys vertical; (2) cables for the fingers directly connected to the mini servos causing uneven friction and stress in the cables; and (3) hooks used to secure the cable in each finger is along the bending direction where direct physical contact is expected during object manipulation, interrupting the proper grasping of the object of the fingers and creating multiple points of contact. manufacturability also limits the potential use of such a novel mechanism. an origami design often involves a complex sequence of folding and bending. some designs may also involve assembling multiple origami segments. this process is labor intensive and time consuming when performed by human hands. in addition, automation of such a process is not an easy engineering problem to solve. the existing origami folding robots can only perform \"simple\" folds, such as valley and mountain folds. 23 nevertheless, this system implies that the folding process can be at least partially automated."
"another experiment was performed to analyze the exerted force from the gripper to the object at the points of contact. for comparison, a three-finger gripper made of rigid plastic parts with a comparable physical size was built. this rigid gripper also used a cable-actuated mechanism with the same mini servos as the origami gripper. force sensitive resistors (fsrs) were attached at the fingertips of origamibot-ii and the rigid gripper to measure the consumed current, which is correlated to the actuator torque. to adjust the backward tension in the rigid gripper, which has a free joint at each finger, a rubber material was used. while load cells may be a better option for precise force measurement, fsrs were selected for their light weight. the relationship between force and resistance is provided by the manufacturer, while additional calibration was performed in our laboratory. fig. 10 . current consumption versus stroke (left) and fsr value vs. stroke (right). fig. 11 . the rigid gripper broke the egg shell at around 18 mm (0.7 in) stroke of pulling cable, while the origami gripper in origamibot-ii was able to deform the fingers wrapping around the egg shell without causing any damage."
"1) the prover broadcasts a location proof request to its neighboring nodes through bluetooth interface according to its update scheduling. the request should contain the prover's current pseudonym p prov, and a random number r prov ."
"2) the witness decides whether to accept the location proof request according to its witness scheduling. once agreed, it will generate a location proof for both prover and itself and send the proof back to the prover. this location proof includes the prover's pseudonym p prov, prover's random number r prov, witness's current timestamp t witt, witness's pseudonym p witt, and their shared location l. this proof is signed and hashed by the witness to make sure that no attacker or prover can modify the location proof and the witness cannot deny this proof. it is also encrypted by the server's public key to prevent from traffic monitoring or eavesdropping attacks. 3) after receiving the location proof, the prover is responsible for submitting this proof to the location proof server. it will also include its pseudonym p prov and random number r prov in the message. 4) an authorized verifier can query the ca for location proofs of a specific prover. this query contains a real identity and a time interval. the ca first authenticates the verifier, and then converts the real identity to its corresponding pseudonyms during that time period and retrieves their location proofs from the server. 5) the location proof server only returns hashed location rather than the real location to the ca, who then forwards to the verifier. the verifier compares the hashed location with the claimed location acquired from the prover to decide if the claimed location is authentic. in order to prevent the ca from knowing locations of a real identity, we have location proof server calculate the hash of each location and only send the hashed locations to the ca in step 5. in this way, the following property can be achieved."
"to address a particular challenge of manufacturability, we are currently investigating 3d printable origami designs, including the twisted tower. while further investigation is needed to identify appropriate materials to be used for the \"rigid\" surface and \"flexible\" hinges, we have printed a nearly identical structure, with the same twisting and bending motions, as a single 3d object. this early-stage work will be a pathway toward 3d printable origami structures while structural and geometrical properties are preserved."
"as illustrated in this paper, origami has great potential in addressing several design challenges in robotics, because it takes advantages of space creation (from 2d to 3d) and coexisting structural properties of flexibility and rigidity. however, little to no effort has been made to establish a theoretical framework for systematic selection of an origami design and its translation into a functional robot. such selection and translation requires considerations from multidimensional perspectives, which may include materials, geometry, mathematical modeling, novel actuation design, and control strategies. we are still at an early stage in this long journey, beginning with physical demonstrations and implementations of origami designs to build fully functioning robots."
"the transformation from the frame attached at the bottom center to the frame at the top center of the octagon layer is determined by following the sequence listed below: https://doi.org/10.1017/s0263574717000340 downloaded from https://www.cambridge.org/core. ip address: 54.70.40.11, on 08 [cit] at 18:41:50, subject to the cambridge core terms of use, available at https://www.cambridge.org/core/terms. the corresponding rigid-body transformation is calculated as follows:"
"this paper presented the design, construction, and preliminary evaluation of the cable-actuated origami manipulator, origamibot-ii. building on preliminary investigation of the twisted tower design for potential robotics applications, 1 the contributions of this paper are in (1) the physical construction of a novel functional manipulator where both the arm and fingers use the same origami designs in different sizes, (2) kinematics and workspace analysis; and (3) strating potential applications of this robot. in particular, experimental results showed the potential use of origamibot-ii for manipulation of fragile and/or irregularly shaped objects, which is still one of the most challenging engineering problems. in addition to potential engineering benefits, origamibot-ii can be built with a relatively small cost. the total cost for constructing origamibot-ii was $134.95 (detailed information is provided in table iii ) making it suitable for an educational and research platform."
"in this section, we discuss the security property in terms of source location privacy and colluding attacking, as well as the countermeasures for these threats."
"in practice, the adversary can thus be a rogue individual, a set of malicious mobile nodes, or may even deploy its own infrastructure by placing eavesdropping devices in the network. in the worst case, the adversary obtains complete network coverage and tracks nodes throughout the entire network, e.g., it is possible that the untrusted location proof server might be compromised by the adversary and the location information can then be easily inferred by examining the records of location proofs. therefore, we need to appropriately design and arrange the location proof records in the untrusted server so that no private information related to individual users would be revealed even after being compromised by adversary. hence, the problem we tackle in this paper consists of collecting a set of location proofs for each peer node and protecting the location privacy of peer nodes from each other, from adversary, or even from the untrusted location proof server to prevent other parties from learning a node's past and current location."
"as stated in the previous section, our location proof updating system has the property of pseudonym unlinkability and statistically strong source location unobservability; i.e., the source privacy of location information can be well preserved. in this subsection, we evaluate the robustness of applaus in defending against powerful traffic analysis and statistical test."
"the knowledge of the privacy information is separately distributed to the location proof server, the ca, and the verifier, respectively. each party only has partial knowledge."
"origamibot-ii supports real-time teleoperation through internet. the hardware configuration of the overall system consists of the manipulator and a remote computing device (e.g., a computer or a joystick) that are wirelessly communicating with each other. figure 8 shows the overall system architecture. in general, communication between two computers uses a transmission control protocol/internet protocol (tcp/ip) socket or the user datagram protocol (udp). the connection between the origami manipulator and a remote control device utilizes raspberry pi as a main processor to send collected h264 encoded video using real-time protocol (rtp) over udp. http-based image streaming have been attempt, but the cpu usage was up around 70% and the transferred image did not scale as well as h264. video streaming using real-time streaming protocol (rtsp) had been used over tcp but the streaming video was very slow. a 5-megapixel video camera captures each frame (15 fps) and sends them to the raspberry pi using the csi interface allowing fast and reliable high-resolution data transfer (1080 p). while streaming the video by udp to the remote pc, the raspberry pi also communicates with the arduino using a universal asynchronous receiver/transmitter (uart) to allow motor control and force feedback. direct control from client (remote pc) is enabled via 802.11 (i.e., wi-fi) over tcp. a remote pc accesses to the ip address and command packet to control the servo motors and receives the force feedback. a cc3000 wi-fi module, attached to the arduino, interfaces with the remote pc. on the arduino's side, \"node.js\"-based script has been programmed, which is an open-source, cross-platform runtime environment for server-side networking applications written in javascript. a graphical user interface (gui) written in visual c# provides real-time video streaming, motion script generation, force limit set-up and feedback, and motion running based on script file."
"proof 1: without losing generality, we assume the mobile node changes its pseudonyms in the order of p 1, p 2, ..., p m ."
"in this section we introduce the location proof updating architecture, the protocol, and how mobile nodes schedule their location proof updating to achieve statistically strong location privacy in applaus."
"an egg shell was used for testing exerted forces from the fingertips of the rigid gripper and the origami gripper. when the pulling force increased, the forces at the fingertips of the rigid gripper showed linear increase, while the forces at the origami fingers tended to converge to a certain threshold ( fig. 10 ). after the origami fingers reached this threshold, deformation was observed in its structure that makes the fingers wrap around the egg shell. figure 11 shows an experiment performed simultaneously for an egg shell using the rigid gripper and origamibot-ii. the three mini servos were programmed to continuously pull the cables until they reached the maximum stroke. when the stroke exceeded around 0.7 in, the egg held by the rigid gripper broke, while that in origamibot-ii was remained intact. the same results were observed in three repeated tests."
"our client code consumes only 80kb of data memory. when running, less than 2.5% of the available memory is taken. we measure the cpu utilization of our client code using a monitoring application, which allows one to monitor the cpu usage of all the processes running on the phone. when our application is in standby, the cpu utilization is about 0.5%, indicating that listening to incoming bluetooth inquiries requires very low computation. the cpu utilization goes to 3% and 5% respectively when communicating with another device and with the server, due to different communication interfaces. we observe that the cpu utilization reaches the highest level of 10% when a location proof packet is generated, in which heavy computations such as authentication and encryption/decryption are involved."
"mobile devices, such as smartphones and pdas, are playing an increasingly important role in people's lives. locationbased services take advantage of user location information and provide mobile users with a unique style of resource and services. nowadays more and more location-based applications and services require users to prove their locations at a particular time. for example, \"google latitude\" and \"loopt\" are two services that enable a user to track his friend's location in real-time. as location proof plays a critical role in enabling these applications, they are location-sensitive. the common theme across all these applications is that they offer a reward or benefit to users located in a certain geographical location. thus, users have the incentive to lie about their locations."
"as the ca has the knowledge of mapping between pseudonyms and real identity, as well as the trust levels of all individual location proofs for a real identity which are received from the location proof server, it is able to identify the trust level of a node in continuous time serial. the ca can calculate the average and variance of trust levels from individual location proofs, or the ratio of legitimate proofs over all the proofs, to determine the trust level of a node."
"another threat exists when two nodes collude with each other to generate bogus location proofs. when a malicious node c 1 needs to prove himself in new york city, he can have another colluding node c 2 to mutually generate bogus location proofs for him, with location tag of new york city. generally, such attacks can be identified by using threshold based solution or by looking into the location traces. in threshold based solution, the system can require the prover to obtain a threshold number of witness nodes, and hence can deal with some colluding attacks. however, since it is hard for the prover to always find enough number of witness nodes, we also use the following solution. the server has information about the numbers of pseudonyms at particular time and location. this information can be used to estimate whether a prover lies about not finding enough peers or always finding the same peer based on some statistical techniques."
"we also measure the performance with two metrics: proof exchange time latency and power consumption. figure 4 shows the latency to complete a location proof exchange process for different sizes of public/private key pair. the key size determines the number of bits in the encrypting keys as well as the size of the pseudonyms. larger key size can provide better security level, but involves more computation and storage resources. it can be seen from the figure that 128-bit key is the best choice since it can provide adequate security level, without introducing too much delay. the distance between the two devices when they exchange location proof also has some effect on the latency, where longer distance means longer delay due to the transmission signal strength. as has been established in earlier studies [cit], more than 80% of contact durations are less than 10 seconds, and thus there is no problem for our proof exchange process to be finished within the contact duration. figure 5 measures the power consumption under different bluetooth status. there are three status: inquiry, standby and proof exchange. the inquiry status is used to discover other bluetooth devices which are within communication range, and also send out proof requests. the inquiry process continues for a pre-specified time, until a pre-specified number of units have been discovered or until it is stopped explicitly. bluetooth devices that only listen to inquiry messages are in standby status. in our system, inquiry and standby are mutual exclusive at any time. the device enters the proof exchange status when it exchanges location proofs with others. the most frequent status is standby, which consumes only less than 0.1mw of power with any communication distance. the proof exchange status consumes the most amount of power and deteriorates with increasing communication distance; however, it will not appear until the next location proof updating cycle."
"workspace of the twisted tower is affected by the number of layers, the number of cables used for actuation, and how these cables are routed. subsections of the tower may be independently actuated by individual cables, which may be routed in different patterns. in origamibot-ii, four cables were routed in a zig-zag pattern, generating linear and bending motions without a twist. pulling and releasing the cables cannot cause buckling in this case. the kinematic workspace of origamibot-ii was estimated by the reachable positions of the end-effector, i.e., the center of the bottom plate of the suspended manipulator."
"to detect if two pseudonyms belong to the same source, the attacker can check whether two probabilistic distributions of proof message time intervals from the two pseudonyms are identical. for the attacker, the hypotheses of the test are:"
"in the simulation, we make the following assumptions: 1) all nodes have unit noise power; 2) than power values p 1 and p 2 are 10 w; 3)ĥ i ∼ cn (0, i),"
"in the next-generation wireless communication systems, smart relaying is one of the key technology that is being investigated to extend the cell coverage and to increase system capacity. for example, relaying will be standardized for the long term evolution-advanced (lte-a) wireless systems, in perticular two-way relaying has drawn a great amount of attentions where multiple terminal nodes utilize common relays to perform two-way information exchanges [cit] . since the relays usually operate in the half-duplex mode where they cannot transmit and receive simultaneously, the required physical spectrum resource (in terms of the number of time slots or frequency bands) is usually increased. fortunately, by mixing the data at the relay and exploiting the underlying selfinterference property in two way relay system [cit], we could reduce the number of required transmission time slots and thus improve the spectral efficiency."
"abstract-this paper presents the design of a robust beamforming scheme for a two-way relay network, composed of one multi-antenna relay and two single-antenna terminals, with the consideration of channel estimation errors. given the assumption that the channel estimation error is within a certain range, we aim to minimize the transmit power at the multi-antenna relay and guarantee that the signal to interference and noise ratios (sinrs) at the two terminals are larger than a predefined value. such a robust beamforming matrix design problem is formulated as a non-convex optimization problem, which is then converted into a semi-definite programming (sdp) problem by the sprocedure and rank one relaxation. the robust beamforming matrix is then derived from a principle eigenvector based rankone reconstruction algorithm. we further propose a hybrid approach based on the best-effort principle to improve the outage probability performance, which is defined as the probability that one of two resulting terminal sinrs is less than the predefined value. simulation results are presented to show that the robust design leads to better outage performance than the traditional non-robust approaches."
"besides the feasibility-caused outage, we have another source of outage if the above principle eigenvector based rankone approximation leads to a solution that does not satisfy the sinr constraints in (13). this happens more often when the largest eigenvalue is not dominating the others. to reduce the overall outage probability, we propose the following hybrid approach :"
"in this paper we proposed the robust beamforming approach under realistic channel estimation conditions (i.e., with estimation errors) for a two-way relay system with analog network coding. by using the s-procedure, the original constraints with infinite dimensions are converted to several lmis and then the relaxed sdp is solved by applying rank-one relaxation. a principle eigenvector based rank-one reconstruction approach is proposed to reconstruct the solution. to reduce the outage probability, we further proposed a hybrid approach that incorporates the non-robust approach when robust problem formulation is infeasible. simulations are conducted to show that the proposed hybrid approach leads to significant improvement in terms of outage probability over the non-robust one."
"first, we show the relationship between the overall outage probability and the error bound. we compare the hybrid approach against the non-robust approach by assuming that the number of relay antennas is four. as we see in fig. 2, a significantly smaller outage probability is achieved with the robust approach. in particular, when the error bound 1 and 2 are large, the outage probability almost remains the same as the error bound changes. when the error bound decreases to a certain critical point, the outage probability decreases significantly with the error bound. to further understand this phenomenon, we decompose the outage probability into two parts. one is the probability that problem q 3 is infeasible, denoted as p 1 and shown by the solid curve with squares in fig. 3 . the other is the conditional outage probability p 2 given that problem q 3 is feasible, shown by the solid curve in fig. 3 . as we see, p 1 increases when error bound increases, since it is more difficult for the robust approach to find feasible solutions given larger channel uncertainty. when the problem is feasible, the conditional outage probability p 2 is low compared to the nonrobust approach. interestingly, we see that p 2 first increases and then decreases when error bound decreases. the overall outage probability is given as p 1 + p 2 − p 1 p 2 ≈ p 1 + p 2 . as we see from fig. 3, the sum of p 1 and p 2 almost remains constant when the error bound is larger than 0.1. as a result, the overall outage probability almost remains unchanged till the error bound decreases to some critical point, after which the outage probability decreases significantly with the error bound. in our future work, we will further investigate this interesting behavior of p 2 and find out the conditions under which the rank-one solution exists. in addition, in fig. 4, we show the relationship between the outage probability and the number of relay antennas. the error bound is now fixed to be 0.01. as we see, the outage probabilities for both the robust and non-robust approaches decrease. this is due to the fact that higher spatial dimension offers a higher degree-of-freedom."
"in the optimization problem q 1, the number of constraints is essentially infinite, since δh 1 and δh 2 are of continuous complex values. as a result, problem q 1 cannot be solved directly. therefore, we need to transform the original problem into an effectively solvable problem of finite dimensions by applying the following theorem [cit] ."
"the rest of paper is organized as follows. in section ii, we present the system model and formulate the problem. afterwards, we transform the original non-convex problem into a sdp problem in section iii with the help of the sprocedure and rank-one relaxation. in section iv, we propose the principle eigenvector based rank-one reconstruction approach to obtain the beamforming matrix. in section v, simulation results are presented to show the performance improvement. in section vi, we conclude the paper."
"(2) considering the different impacts of drivers and significant variables on a given decision problem, the tsam method will assign different strengths to drivers and significant variables respectively. for each driver d"
"to define the gain compression factor to fit the recovery time. the gain dynamics associated with the intraband phenomena are associated with a fast recovery time immediately after the injection of a short pulse, with the gain compression increased under increasing pulse energy and bias current. although the tmm model focuses on the interband phenomena, the ultrafast intraband phenomena have been considered through the use of a power-dependent gain compression coefficient [cit], which allows tuning the fast recovery time of the intraband phenomena. in this strep, typical pump probe measurements are required with a weak continuous wavelength signal with average optical power in the small signal gain fed to an soa along with a high-power ultrashort return-to-zero pulse with a full width half maximum duration of 5 ps, which sweeps all the available carriers."
"on the basis of the above research, we implemented a simple real-time multimedia communication system. this system supports peer-to-peer connection between browsers after signaling negotiation. multimedia data can be real-time transmitted. users can perform a real-time video communication. furthermore, this system is mobile internet-based, and supports multimedia real-time interaction between the smart mobile devices. the system's screenshots is shown in figure 7 . fig.7 . system screenshots system implementation method. in the browser side, the system is built mainly based on the html5 and javascript and call the webrtc and websocket interface. in the server, the http server is built based on node.js express module, the websocket service is supposed by ws module. for stun server, configured to use google's public test server. as space is limited, the system building process cannot be explained detailedly here."
"in this step, managers present observations of significant variables, which are the evaluation of those managers on a situation. in the customer churn prediction example, observations of customers' \"age,\", \"monthly fee,\" \"call type\" and other variables generate a firsthand perception of the decision problem. thereafter, the first-hand perception is processed by the personal individual mental model and the individual sa is obtained. the collected observations are the input of processing in next step."
"in this paper, we analyzed the core framework and related core technologies of webrtc, and put forward a webrtc signaling exchanging mechanism based on websocket as the signaling exchanging data carrier. it realized the initial negotiation of multimedia session between browsers and the establishment of the interactive connections. finally, using this signaling exchanging mechanism implemented and webrtc api, designed and implemented a peer-to-peer multimedia real-time interactive system in the mobile internet (the peer can be pc or smartphones)."
"existing studies indicate that linguistic method is a powerful technique to handle qualitative information [cit] . applying linguistic method to handle individual awareness of a qualitative nature in the customer churn prediction problem is an alternative means to improve managers' team awareness in a dynamic situation. based on this consideration, this paper presents a team situation awareness measure (tsam) method. in the tsam method, the semantics of a set of linguistic terms are clarified by a semantic utility function; an individual awareness is expressed by a linguistic possibility distribution on the potential states of a given decision object; the team's domain knowledge and experience about the decision problem is expressed by a multi-level variable-driver hierarchy and a set of reasoning rules; and the team's awareness in a situation is obtained through approximate reasoning and aggregation on individual awareness."
"audio input and output devices: they are used to collect and play multimedia information. network connections: in the online video communication, there is a large number of data between peers need to be transmitted. it requires a stable and reliable network connection as the guarantee of data transmission."
"team decision-making cannot be conducted without communication and cooperation among team members. in this step, team members' individual impact on leaf drivers, which is the primary aspect in their duties, is determined. the impact is expressed by linguistic terms such as \"very important,\" or \"unimportant,\"(shown in table 1 ) which indicate to what extent a manager's observation or individual sa should be adopted when measuring a team's sa."
"this step distinguishes the managers' preference on used linguistic terms, which include the strengths of key drivers, the strengths of significant variables, and impact factors of managers, as well as linguistic possibilities. the utility functions clarify the semantics of those linguistic terms, which can be determined by experiments on statistic data and discussion among team members."
"following the estimation of the photonic densities for all signals travelling through each sector, we update the carrier density n i according to the well-known rate equation [cit] :"
"team decision-making allows managers in different functional departments and geographical locations to share collected information and their individual awareness of changes in a situation. inevitably, each manager's individual awareness of the situation is incomplete and unbalanced. to narrow down the gap between individual awareness and a real situation, individual awareness aggregation is conducted. in the course of this, managers' impacts on the decision problem are addressed. managers' impacts on the decision problem have two noticeable features. first, different managers may have different impacts on the same aspect of a decision problem. second, a manager may exert different impacts on different aspects. based on these two features, the tsam method assigns a set of impact factors to each manager."
step 3: let the variables be those shown in figure 2 and suppose each variable's possible states are shown in table 5 . step 4: let the connections between drivers and variables be shown in figure 2 and the strengths of variables be listed in table 6 table 6 example of strengths for drivers and variables.
"based on the team sa on leaf key drivers, the team sa on non-leaf key drivers can be obtained. the primary process is similar to that in step 9."
"vagionas and bos [cit] presented a novel multigrid solver for the dynamic response of the soa, relying on the wideband steady-state gain coefficient of eq. (2). introducing multigrid techniques in soa modeling enabled extending the accurate time domain modeling of the tmm model, allowing for the development of an efficient solution supporting implicit time discretization schemes. implicit schemes in turn enable accuracy-instead of stabilityrestricted time discretization of the signals. this implies a different discretization scheme, where sampling of the signals is optimized for certain accuracy in the solver instead of certain time step restriction. this allows lifting the limitations of an equidistant spatiotemporal grid for the representation of the incoming signals adopted by traditional explicit soa models, releasing an adaptive step size-controlled solver for the dynamic soa response with dense time sampling under a rapidly varying soa signal output and scarce time sampling when negligible changes are observed. adaptive times stepping adds one more degree of freedom to computational efficiency of accurate soa modeling, which is of crucial importance when evaluating large input patterns for statistical signal analysis independent of the bitrate or large circuit networks with multiple soa-based components."
"in addition to the definition of the material properties, feedback from a series of experimental characterization measurements on real soa devices can be incorporated in the model through the following three steps:"
"based on the decision problem, decision object, and key drivers, significant variables can be identified. these variables provide support information to solve the decision problem. in order to determine the possible states of each variable, the decision object's characteristics should be considered. for example, the \"normal\" state of \"monthly fee\" for customers who signed a $49 cap plan is completely different from customers who signed a $99 cap plan. in general, the possible states of a variable can be obtained through divisions on its universe of discourse."
"where the operator l, although simple in appearance, is the quite complex, as it involves solving the forward and backward propagation equations for both the signals and noise photon fluxes. by applying the implicit midpoint rule, we obtain the following equation:"
"on the way to develop complex devices with enhanced processing capabilities, semiconductor optical amplifiers (soas) have been a powerful building block that provides high-speed alloptical switching operations [cit] by featuring large optical gain, optical gating function, controllable performances by injection current, compactness, fast response, and ease of integration with other functional semiconductor devices [cit] . soas are utilized in telecom [cit] and datacom [cit] pics either as single soa travelling waveguides, supporting crossgain modulation (xgm) phenomena, or arranged in soa-mach-zehnder interferometer (soa-mzi) configurations, supporting cross-phase modulation (xpm) phenomena. the reason lies in the maturity of the soa technology to a point where commercial devices are readily available either as bulk single chip elements [cit], in arrays of certain pitch [cit], or packaged in discrete fiber pigtailed components [cit] for use in optical communication systems. moreover, with recent research achievements on mid-board flip-chip bonding of an soa array to a silicon-on-insulator (soi) platform [cit] or the development of temperature stable soa [cit], soas are expected to play a pivotal role in future pics."
"the developed multigrid v-cycle, including the above five steps, is schematically represented in figure 5 (ii), showing four grids of different granularity. each coarse grid comprises half the grid points compared with the higher/finer grid level, while transitions rely on the i h h restrict and i h h refine operators. the smooth operation exist of second-order distributive jacobi relaxations has been considered, that is, for each, ν ι h,κ, an update is calculated as follows:"
". for convenience, let v d be the set of significant variables connected to driver d and d x the drivers set which the significant variable x connecting to."
"refines the coarse grid functions to a finer grid. in order to generate an intermediate fine point between two coarse grid points, an inverse operator has to be considered compared to the restrict operation, which in our case is the linear approximation. in this way, a multigrid"
"to improve team decision-making efficiency and accuracy, effective cooperation is the most important issue. team members often express their awareness in forms with various natures. hence, how to effectively integrate multiple natures of awareness will be studied, which may include the perception of semantics in different expressions as well as reasoning based on those semantics. moreover, a correct mental model is the basis of team decision-making which will affect the generation and measurement of individual and team sa. therefore, how to construct an appropriate and flexible mental model will also be another task in our future study."
"with the rapid development of mobile internet and more and more intelligent mobile devices, the research of multimedia communication systems which focus on mobile user terminal (such as smartphones) has become a focus of people's concern. web multimedia applications that based on webrtc have got more and more attention because of higher development productivity and ease of use. web real-time communication (webrtc) is a collection of standards, protocols, and javascript apis, the combination of which enables peer-to-peer audio, video, and data sharing between browsers (peers). instead of relying on third-party plug-ins or proprietary software, webrtc turns real-time communication into a standard feature that any web application can leverage via a simple javascript api [cit] . webrtc greatly reduce hardware costs and technology costs of web real-time multimedia interactive systems development. currently, the latest version of the pc chrome, firefox browser and the android chrome have already support webrtc. the official standard of webrtc that include browser api, data transmission protocol etc. is still in process of formulating."
"a human being's mental model in a complex dynamic decision context is the core of their perception of the external environment, the generation of appropriate awareness, and timely decision making. a real mental model is very complicated. to reduce its complicity, a mental model can be simplified by a variety of core elements. for instance, in a customer churn prediction problem in telecoms, a real mental model can be simplified by drivers and variables, and the relationships among them, as well as relevant responses."
"in this paper, a tsam method is presented to measure team sa in a complex and dynamic context using identified drivers and significant variables which are used as components of individual and team mental models. the method uses utility functions to capture semantics of linguistic terms which are used to describe the strengths of drivers, variables, the impact of managers. utility functions can be used to create unbalanced and asymmetric representation of linguistic terms. individual and team sa is expressed by a linguistic possibility distribution on the decision object's possible states, which can be used to express awareness more naturally and easily. a simple example in telecoms customer churn prediction is presented to illustrate the main steps of the method."
"key drivers and their strengths can be determined once the decision problem and decision object are defined. for example, \"customer demography,\" \"bill and payment,\" \"call detail\" can be selected as key drivers for the decision problem about customer churn prediction in a telecom. those key drivers' strengths can also be determined according to profiles of that segment of customers. when the customer segmentation is changed, the strengths of those key drivers is also changed. table 3 main steps of the tsam method."
"complete sdp and ice information exchange. after ensuring that multiple communication peers can be identified each other, the next step is exchanging signaling information. the signaling information mainly includes the sdp information and ice information of each peer. these signaling information is serialized using json and transmitted by websocket."
"this section overviews the proposed tsam method, explains individual and team mental models, discusses semantic utility functions, and develops an algorithm to measure team sa."
"analysis. this system, which is running on the normal http server, has low-cost hardware costs. in the whole communication process, the server's function is negotiated the signaling before communication, the multimedia data transmit from one peer to another peer directly, server load is reduced greatly. websocket which suppose the bidirectional data communication used for signaling transmission channel, it is also well suited to webrtc usage scenario"
"step 6: in the illustrative example, there are two kinds of linguistic term sets, i.e., the linguistic weights w for the strengths of drivers, variables, and impact factor of managers; and the linguistic possibilities t . for the linguistic weight w, let the utility function q w be defined as"
"take the \"monthly fee\" for example. suppose the possible spends interval for \"monthly fee\" is [$39-$200], then we can define states \"normal,\" \"slightly high,\" \"high,\" and \"very high\" as shown in figure 3 to describe the observation of a customer's monthly spending."
"the light propagation through a single sector can then be described by the simple tm, which relates the incoming and outgoing amplitudes of an elementary waveguide sector. for the current analysis and without loss of generality, we assume a signal es propagating to the right direction and a second signal counterpropagating toward the left side, ec. by applying the tm of sector i at time t for both the externally injected lights and the spontaneously emitted photons over a wide optical spectrum of 1500-1600 nm, we obtain the following equation:"
"using the developed soa numeric modeling tools, we numerically evaluated two soa-based circuits predominant in all-optical signal processing: (i) an all-optical flip-flop architecture that exploits coupled soa waveguides, operating on xgm phenomena, and (ii) an all-optical xor gate that exploits an soa-mzi configuration and xpm phenomena. the numerical results are compared with the results experimentally obtained, showing very good agreement."
"studies above shown that almost all of the peer-to-peer multimedia transmission technologies are encapsulated in webrtc api. but the signaling mechanism and transmission channel that are not defined in webrtc need to research and implement by developer. therefore, this paper will presents a complete webrtc signaling mechanism via websocket, and uses this mechanism and webrtc api to achieve a peer-to-peer real-time multimedia interactive system. websocket service design. as the signal transmission channel, websocket is the foundation of signaling. first, it is necessary to build a websocket service and then establish a websocket bidirectional connection between a browser and a server. this connection establishment process is shown in figure 4 . the specific method is integration websocket service in http server, opening the websocket port and listening to the request. in the browser, a websocket object need to be created according to the server address and port. the websocket connection will be created after handshake. finally, the browser and the server are able to push signal data to each other."
"using the mental model, the solution of the decision problem is evaluated by means of the information process step by step along the links between key drivers and significant variables. for instance, to predict customer churn, figure 2 describes the relationships among key drivers and significant variables. observations on variables and awareness of key drivers can then be integrated in terms of the hierarchy."
"the decision problem and team members make the final decision cooperatively. team decision-making can efficiently reduce bias and incompleteness within the observation, inference, and other information processes in the personal decision-making procedure, and can improve the reliability of a decision [cit] . in an implementation of dynamic team decision-making, team members are required to collaborate with others by sharing their individual situation awareness so that team situation awareness of the decision context is developed [cit] ."
"the normalized traces are plotted of the xpm operation imprinted on the probe signal are illustrated in figure 4 (i) and the xgm operation in figure 4 (ii). it is obvious that in both cases, a π phase shift was introduced due to xpm operation. however, the probe signal is suppressed more in the case of the copropagating pump pulse, reaching normalized power values down to 0.1, while for the counterpropagating pump pulse the gain suppression is less deep. this owes to the increase pump-probe light interaction along the copropagating direction through the carrier rate equation. in addition, to the recovery time in case of a counterpropagating control pulse is shorter (after the suppression of the gain a local minima), however, the tradeoff is a slower response/fall time observed with a delay in the deep (minima of the output power) when the control pulse is injected in the counterpropagating direction. this owes again to the travelling time and the time required for the counterpropagating pump pulse to absorb the available carriers. the findings for xgm effects and xpm effects for π-phase shift of a gaussian control pulse are shown in figure 4 and are in full compliance with the ones found in the literature [cit] ."
"this book chapter aims to provide a holistic simulation methodology approach for efficient and validated numerical modeling of soas and soa-based circuits in the time, which may form the basis of further developments in soa modeling. this chapter is organized as follows: a short review of established soa modeling approaches in the literature will be presented in section 2, along with an overview of some recent research efforts. in section 3, a description of an experimentally validated time domain soa model relying on the tmm will be presented [cit], relying on explicit modeling techniques, followed by a gain parameterization procedure for tailoring the tmm model to characterization measurements of other soa devices in section 4. having developed an experimentally validated soa model, we numerically investigate the impact of the external current injection, the soa length, and the light propagation direction in the gain recovery using pump-probe techniques to qualitatively study the shortening of the gain recovery time toward achieving high bandwidth switching operations. section 5 presents a newly introduced efficient solver in the time domain, relying on implicit schemes and multigrid concepts. implicit schemes allow developing an adaptive step sizecontrolled solver for the wdm soa response [cit], which extends the validated tmm model by lifting the limitations of spatiotemporal grids and releasing adaptive sampling at the soa output. toward application in realistic system-level scenarios, the last section presents experimentally validated numerical results for two predominant soa-based all-optical signal processing circuits, such as a coupled soa xgm flip-flop arrangement and an xor gate based on soa-mzi xpm configuration. the former acts as memory for sequential logic processing circuits, while the latter forms a basic building block of combinational logic signal processing circuits. conclusions are addressed in the final section."
"then a conclusion about the decision object with respect to the driver d is obtained and denoted by t d h d . however, if"
"the remaining parts of the paper are organized as follows. in section 2, related work on situation awareness, linguistic method, and customer churning prediction is reviewed. section 3 describes the tsam method. a case study of customer churn management illustrates the effectiveness of the tsam method in section 4. finally, conclusions and our future work are discussed in section 5."
"based on the structure of the mental model, team sa is measured by a four-stage information process through observing significant variables, inferring decision object states, and aggregating individual awareness."
"customer churn management is one of the primary business strategies of service providers that is widely used in the wireless telecoms service industry [cit] . in a typical five-stage customer churn management framework, customer churn prediction is a vital stage which works with the other four stages, i.e., identification of the best data, data semantics, feature selection, and validation of results [cit] . the most-used techniques in the development of predictive models include genetic algorithm, regression, neural networks, decision tree, markov model, clustering analysis, etc [cit] . although these techniques are successful in some applications, practices indicate that they are weak in handling qualitative information. business surveys have identified various reasons for customer churn which cannot be quantitatively expressed in accurate data but are qualitative expressions. effective qualitative information process is, then, required to meet the demand of measuring team sa in dynamic decision-making."
"in this section, we report on a gain parameterization procedure and a methodology toward tailoring the tmm model and producing simulation results close to the experimental measurements obtained from a commercial soa device. the methodology is in principle generic and compatible with the other soa devices. the target is to identify a set of material parameters and cross-sectional characteristics by incorporating feedback from experimental measurements such as the bandgap shrinkage to fit the experimental gain spectrum in a spectrum analyzer, the recombination rates to match the gain profile of gain-vs-incoming power measurements, and the gain compression factor for the recovery time at pump probe."
"the core function of webrtc is to support the network multimedia communication. overall, establishing a multimedia connections between browsers needs audio technology, video technology and network transmission technology."
"further to step 7, the observations of variables reflect the perception on the decision problem and decision object. the first-hand perception must be analyzed and processed before it is used to obtain awareness of a decision problem and the related decision object. this step implements the process. in this step, each manager's observations are transferred into individual awareness of a special aspect of the decision problem and decision object by approximate reasoning on the basis of a personal mental model. this procedure is illustrated below."
"in the both peers of session or data transmission, one of the fundamental requirement is the ability to locate and identify each other on the network. in the trivial case, where both peers are located on the same internal network without any firewalls or nats between them. to establish the connection, each peer can query its operating system for its ip address. but usually, there are many firewalls or nats between peers, so ice agent which colligates stun, turn, etc. is used to penetrate firewalls and nats in webrtc [cit] ."
the second stage is an observation stage. observations on variables are collected and expressed by linguistic possibility distribution on possible states of those variables at this stage.
"soa numerical modeling has progressed on multiple fonts during the last decades, and various simulations approaches have been developed [cit] . soa models can be roughly divided in two categories: (i) the material models that target optimization of the emission properties, before and during the development-fabrication process of an soa device and (ii) the circuit models, which account for the carrier density, the interband, and the intraband phenomena for simulations of light-matter interaction in realistic system-level application scenarios."
"to meet the demands of customer churn prediction in competitive and changeable markets, this paper develops a tsam method using utility functions to support team managers to develop better awareness of a segment of customers' potential behaviors. in this method, a utility function is used for the semantic perception of qualitative information, i.e., linguistic terms. a general procedure and information flow of the tsam method is shown in figure 1 ."
"this step will determine 1) what the decision problem is; 2) in the problem, what the decision object is; and accordingly 3) the possible states that the decision object may have. take the customer churn prediction problem as an example. the decision problem is to predict \"whether customers will churn\". because customers may be divided into different segments, the problem may only focus on a particular segment of customers to answer that question. thus the concerned customer segment is the decision object. accordingly, the potential behaviors such as \"leave quickly\" and \"under contract\" can be chosen as the possible states."
"based on the individual sa and utilities of managers, the team sa on each leaf driver is obtained. suppose the used aggregation operator is the weighted-sum, then the team sa on driver d 1 is"
"in this regime, soa numerical modeling has attracted a lot of research attention during the last decades, as mathematical models are required to aid in the design of soas and to predict their operational characteristics [cit] . during the development process and prior to fabrication, soa models that take into account the semiconductor properties of the active material and cross-sectional dimensions are employed when optimizing the amplification gain and output power over a wideband steady state and under various external current injections. in addition, during the system-level performance study of soa-based circuits, time domain simulations are required for an accurate evaluation of the circuit response with multiple signals propagating bidirectionally along the waveguide. with growing complexity of circuits, system level designs call for experimentally validated yet efficient solvers that support multiple signals at different wavelengths propagating in multiple directions."
"the first stage is a preparation stage. at this stage, the decision problem, key drivers, significant variables, individual and team mental model, as well as utilities of linguistic terms, are identified and determined."
"customer churn prediction is a key component in customer churn management [cit] . traditional predictive models for customer churn are mainly established on statistics and machine learning techniques, such as logistics regression, decision tree, time-serial analysis, artificial neural networks, genetic algorithms, and data mining techniques [cit] . practices indicate that those models are successful in handling data of a quantitative nature: for instance, data stored in a company's internal databases. however, these methods have limitations in applications due to the absence of consideration of managers' qualitative awareness. a manager's individual awareness of customer churn is generated based on their domain knowledge and experience with collected observations from various sources, including public media, and public data sources, as well as internal databases. qualitative nature is a remarkable feature in managers' individual awareness. improving team situation awareness in a quickly changing market requires a method to handle qualitative awareness with various forms such as probability estimations and linguistic judgments. this paper takes qualitative awareness expressed by linguistic terms as the main subject and discusses how to measure team situation awareness from those linguistic terms."
"for ice information, ice agent of each peer automatically begins the process of discovering all the possible candidate after multimedia communication, whenever a new candidate is discovered, the information of this candidate will be sent to others peers by server's forwarding. the other peers configure its remote ice candidate once received information."
"the multimedia audio and video communication process is shown in figure 1 . real time data transmission technology based on webrtc. the data transmission of the traditional b/s systems is carry out between the browser and the server. the browser sends a request to the server, and then the server respond corresponding data according to the request parameters. webrtc achieves peer-to-peer real-time data transmission between browsers. in the delivery of real-time data, timeliness and low latency can be more important than reliability. therefore, webrtc uses udp at the transport layer."
"websocket enables bidirectional, message-oriented streaming of text and binary data between client and server. any side can send data to another side any time [cit] ."
"to obtain individual and team sa, approximate reasoning and aggregation are used. approximate reasoning is applied because observation of the variables and awareness of the drivers may not completely match the antecedents in f d and g d . aggregation is used for integrating observations from different managers and on different variables."
"after transmission channel be implemented, next, it is supposed to exchange the session description information. webrtc uses session description protocol (sdp) to describe the parameters of the peer-to-peer connection. sdp describe the session profile, which represents a list of properties of the connection: types of media to be exchanged (audio, video, and application data), network transports, used codecs and their settings, bandwidth information, and other metadata [cit] . the process of sdp exchange between peers is as follow.  the initiator (user a) creates an offer, and set it as his local description of the session. then, he sends the generated session offer to the other peer (user b)  once the offer is received by user b, he sets user a's description as the remote description of the session, generates the answer sdp description, and sets it as the local description of the session. then he user b sends the generated session answer back to user a.  once user b's sdp answer is received by user a, user a sets user b's answer as the remote description of his original session. rtcpeerconnection api introduction. in webrtc, the above mentioned data transmission, session mechanism and other functions are encapsulated in rtcpeerconnection api. it is responsible for the management of full ice workflow for nat traversal, sending stun automatic keepalives between peers, keeping track of local streams and remote streams. developer can use it to generate offer, receive answer, manage ice status and so on."
"managers' domain knowledge and experience is abstracted to be relationships among drivers, variables, and decision object's states. the tsam method mainly depicts three kinds of relationships."
"data encoding, decoding, transmission and display: after collected video by input devices, the data need to be encoded, transmitted. the other peer of connection need to accept data, decode and display."
"for sdp information, the initiator of communication sends the websocket offer signal to others peers by server's forwarding. the socketid and the sdp data of initiator are carried in signaling. the other peers will respond the answer signal according to received information."
"in the last two stages, approximate reasoning and aggregation are used. approximate reasoning implements two kinds of information transformations: 1) from observations on variables to awareness of leaf drivers; and 2) from awareness of leaf drivers to awareness of nonleaf drivers. aggregation conducts two kinds of information integration: 1) integrating multiple observations from managers; 2) integrating awareness of drivers."
"multigrid methods employ a series of coarser grids to obtain grid independent convergence rates. drawing from the finest grid of traditional longitudinal division of the soa into cascaded sectors, coarse and coarser grids with less number of soa sectors are adopted in order to represent the spatial free carrier density distribution by less and less grid points (carrier density samples). this is schematically illustrated in figure 5 (i), where 4 grid levels have been employed. the 4th level is the finest level, including an soa longitudinal discretization into 16 sectors, equal to the discretization employed in the tmm model. however, by applying multigrid concepts, the carrier densities of two neighboring sectors can be represented by a single sector in the coarser 3rd-level grid, resulting in half grid points. equivalently, the 3rd-level can be again restricted to coarser grids, with each transition halving the number of grid points. (ii) graphic representation of the five step multigrid v-cycle schedule. the initial finest approximation is smoothed (step 1), restricted to the coarsest grid (step 2), solved with coarse granularity (step 3), refined to the finest grid (step 4), and finally smoothed again (step 5)."
impact factor v 1 absolutely unimportant v 3 unimportant v 3 slightly unimportant v 4 medium v 5 slightly important v 6 important v 7 absolutely important
"is used to indicate the rule about a non-leaf driver d and the states of the decision object. therefore, in the tsam method, the relationship set r is denoted by: almost impossible u 3 slightly possible u 4 quite possible u 5 possible u 6 highly possible u 7 absolutely possible"
"first, this paper researched webrtc architecture and analyzed the core technology. next, for the part of signaling management that is not been defined in webrtc, we proposed a solution that used websocket as signaling transmission channel and built a signaling server to forwarding signaling information. finally, we built a real-time multimedia communication system based on the mechanism. the system is running well. the research provides an academic and practical foundation for webrtc signaling work. what is most important is the system can running in mobile internet, the mobile smart devices can communicate with each other. in the next step of the work, multiple browsers communication framework will be researched due to the low communication efficiency because of too many connection."
"the procedure includes a number of steps clustered into two sections: first, the definition of the material and waveguide properties and, second, the feedback from the experimental characterization of the soa device under study."
"a holistic methodology approach on time domain numerical modeling has been demonstrated, targeting to address accuracy and efficiency. accuracy is addressed through the development of an experimentally validated numerical model and a gain parameterization procedure. following the development of a validated numerical model relying on the tmm analysis technique, qualitative results are presented so as to investigate the gain dynamics and the recovery time of the soa during pump-probe measurements. efficiency is sought through the development of a newly introduced time domain soa modeling technique based on the multigrid concepts to introduce adaptive time stepping."
"moreover, a mental model in the tsam method also includes three other components, i.e., a given decision problem or given decision object; possible solutions of a given decision problem or possible states of a given decision object; and domain knowledge and experience for relationships among solutions of the given decision problem, its drivers and significant variables."
"having updated the carrier density, we can proceed to the estimation of the imaginary part of the propagation constant, which accounts for the phase shifting term of the amplitude. the estimation of the effective refractive index n eff in the imaginary part relies on the plasma effect, according to which the change of the refractive index δn pl is linearly dependent on the change of the free carrier density in the active region δn [cit]"
"numerical soa modeling has so far relied on explicit schemes for solving the associated system of coupled ordinary differential equations (ode), comprising the spatial discretized carrier density rate equation given in eq. (6) combined with the material gain coefficient in eq. (2). in the literature, odes are classified in two categories: the stiff and nonstiff odes. the latter can be solved efficiently by explicit time stepping schemes. the former requires many steps with explicit schemes as warranted by the smoothness of the solution, as has been the case for the traditional soa modeling and is schematically illustrated in figure 2 (ii) for the tmm model, where each node of the plot represents a small time step. an interesting alternative option would be to deploy implicit schemes, which could alleviate the problem of the many unneeded, as far as accuracy is concerned, time steps at the cost of having to determine the jacobian for the set of odes and inverting a matrix, at each time step. implicit schemes have shown to be more efficient for many problems [cit], with multigrid methods known to be among the most efficient solvers for many partial differential equations (pdes) [cit] ."
"situation awareness (sa) [cit] has received considerable attention from the human factors and ergonomics community during the past two decades and is widely used in complex dynamic systems where human factors are involved, such as nuclear power plant operations [cit], air traffic control [cit], and emergency response [cit] . improving team situation awareness (team sa) requires team members to share perceptions (i.e., individual sa) and comprehension of a situation among team members. effective communication and cooperation are two primary aspects for developing team sa. moreover, three other demands are required to improve team sa [cit] . first, the increasing complexity of dynamic contexts provides a great challenge to decision makers for the timely recognition of the external environment and to conduct ongoing analysis. second, correct awareness is the basis for appropriate decision-making; in particular, when team members are working at different geographical locations. finally, team sa should be appropriately measured."
"through the 10-step algorithm, the team sa is thus measured. in the next section, a simple case is studied to illustrate the main steps and effectiveness of the algorithm."
"considering the numerous interrelated phenomena that affect the performance of an soa, determining the optimum parameters that match all system-level parameters, such as the gainpower curve, the spectrum, and the recovery time, implies a certain difficulty. defining a certain methodology for parameter extraction ensures accuracy of the model for direct applicability in realistic use case scenarios and demonstrations. after developing a certain gain parameterization procedure, the use of multiobjective genetic algorithms supporting an automated iterative procedure can also been adopted [cit] toward a best-fit criterion."
"∥ . when using the solution of the previous time step as initial approximation to the solution at the current time step, k, o(log(1/h)) cycles are needed to solve the problem to the level of the truncation error. this can be reduced to o(1) by using the coarser levels to generate an initial approximation accurate to within the level of truncation of grid, h [cit] ."
". this step requires static power gain measurements using a continuous wavelength light source close to the gain peak wavelength and under varying average optical power in order to estimate net gain of the soa, and thus defining the small signal gain and the saturation point of the soa."
"to accommodate the increasing demand of data transfer and high-speed optical telecommunication networks with terabit transmission capabilities and high bandwidth switching functionalities, there has been a growing interest in increasing the recovery time of the soa. high-speed nonlinear soas are used to perform either xgm or xpm modulation between two input signals, i.e., a weak cw probe signal and a high-speed data signal with a short pulse width, acting as pump. increasing the external current injection or the elongating the soa active region have been shown to shorten the recovery time and thus support higher speed switching operations. after developing an experimentally verified time domain model, we numerically investigate the shortening of the recovery time during pump probe measurements under increasing current or increasing soa length. figure 3 (i) illustrates the simulated gain recovery and the normalized output power of the soa for various external currents. the external currents were tuned from 185 to 350 ma, with the 300 ma being the nominal current operation. the plots reveal a shortening at higher currents, stemming from the increased current density injected to the soa. the measured 1/e gain recovery times were plotted versus the supplied current in the inset, demonstrating a shortening from 45 ps under 185 ma down to 20 ps under 350 ma."
"to define the recombination rates for the gain curve versus the incoming optical power. this includes the c1, c2, and c3 parameters that are associated with the linear recombination coefficient at defects (current leakage), the spontaneous recombination, and the auger recombination rate, respectively. in practice, these three mechanisms exhibit complicated dependences that together account for the overall recombination rate given by"
"each webrtc connections object contains an ice agent. ice agent is responsible for gathering local ip, port tuples and queries an external stun server to retrieve the public ip and port tuple of the peer. furthermore, it is responsible for performing connectivity checks between peers and sending connection keepalives to stun server. if configured, ice agent appends the turn server. if the peer-to-peer connection fails, the data will be relayed through the specified intermediary [cit] . this complete process is shown in figure 3 . before session negotiation, it must determine whether the data can be successfully transmitted to the other peer and whether the other peer ready to establish a connection. thus, the initial peer of session need to send an offer signal, the other peer need to respond an answer signal. webrtc does not define the standard of transmission channel and protocol, this allow interoperability with a variety of other signaling protocols powering existing communications infrastructure, such as sip, jingle, isup and so on [cit] . in this paper, the transmission channel is implemented using websocket."
"step 5: suppose a team of four managers from a marketing department (e 1 ), net service support department (e 2 ), financial department (e 3 ), and customer service department (e 4 ) respectively. the impact factors of these four managers are assigned as shown in table 7 ."
"linguistic methods (or linguistic approaches) in an uncertain information process community refer to a kind of technique that is mainly developed to process information expressed in natural or artificial languages [cit] . linguistic methods have been successfully used in management [cit], industry [cit], and decision-making [cit], as well as the social sciences [cit] . most linguistic methods follow a three-step solution scheme [cit] : 1) choose linguistic terms; 2) choose aggregation operators for linguistic information; and 3) choose the best alternative by aggregation and exploitation. in the solution schema, qualitative information is represented by linguistic terms, which are elements in a designed processing framework, and is processed by the computation model in that framework. for example, fuzzy sets and fuzzy logic are the most used processing frameworks, in which fuzzy sets are used to interpret the semantics of linguistic terms, and computation and inference algorithms for fuzzy sets are used to implement the syntactic process of linguistic terms. automatic process of qualitative information can partly be implemented in linguistic methods."
"step 4: identify knowledge and experiences, and express them by rules between key drivers and significant variables as well as determine the strengths of those variables with respect to key drivers."
(3) the tsam method defines a set of rules which link the observations of variables as well as the awareness of drivers and the states of decision objects. below is an example of those rules.
"currently, existing team sa measure models are mainly based on statistics and inference established for operator training in industrial and military control procedures [cit], which cannot be directly applied to team decision-making due to the difference between control procedure and decision-making. the difference lies with two issues. on the one hand, the subject in a control procedure often has several normal states which play the roles of potential standards for measuring team sa, i.e., team sa can be measured through the deviation between the subject's running state and normal state. however, it is hard to define such a standard for measuring team sa in decision making problems. decision problems seldom regulate a subject's behavior and states to a fixed standard; rather, they change their decision targets to fit the change of subject accordingly. hence, team sa measure in a decision problem is a dynamic procedure. on the other hand, the information in control procedure is often quantitative nature and is processed at the technical level. however, information used in the decision-making process is often of a qualitative nature which is processed not only at the technical level but also at psychological, conceptual, and behavior levels. therefore, team sa measurement in decision-making problems is required to deal with more qualitative information."
step 8: assume that the individual sa of these four managers on the four drivers is obtained from observations of those variables as shown in table 8 .
"the explosive growth of information traffic and the concomitant increase of bandwidth hungry applications have contributed to a growing demand for communications networks offering greater bandwidth and flexibility at lower cost [cit] . this has led to a series of technological advances in high-speed backbone networks and telecommunications, where integrated optical systems delivering low-cost, low-power, high-bandwidth transmission, and high-speed switching elements are increasingly required [cit] . toward meaningful and functional subsystems, high-speed all-optical elements such as all-optical signal wavelength conversion [cit], on/ off keying modulation [cit], header recognition [cit], optical buffering [cit], and signal regeneration [cit] have been developed, targeting to perform basic functionalities in the optical domain. meanwhile, photonic integration technologies have steadily matured over the last decade, achieving improved yield fabrication [cit], allowing the fabrication of complex single-chip photonic modules with advanced functionality. this has resulted in impressive demonstrations of functional complex circuits and switching architectures [cit], making firm steps toward medium-scale (ms) photonic integrated chips (pics)."
"customer churn management is a typical dynamic team decision-making problem in many businesses [cit] . service providers such as insurance companies, telecoms and commercial banks face an increasingly changeable market which has been saturated with cheaper services and powerful competitors. thus, retaining potential churning customers becomes a challenging issue in their business strategies. to achieve a retention goal, team members, who are responsible for different duties and are distributed in various geographical locations, should efficiently share their individual awareness. based on team awareness of market divisions, the company's service performance, customer potential behaviors, technique progress in relevant support systems, and other related aspects, the team can take corresponding timely reactions. hence, improving the situation awareness of a management team can help to support appropriate business decisions and business strategies."
"in the tsam method, individual or team mental models adopt a similar structure, i.e., they are composed of a set of significant variables and a multi-level hierarchy of key drivers. these drivers are certain important factors related to a decision problem, and significant variables are indicators whose values can reflect the current state of a situation. in the customer churn prediction example, significant variables may be customer-related attributes, such as the monthly payments, bill and payments, call details, and customer care and services as shown in figure 2 [cit] . the terms \"driver\" and \"variables\" are both taken from the telecoms community."
"x 21 very important x 22 very important x 23 important d 3 x 31 important x 32 important d 4 x 41 important x 42 medium some rules for the driver \"bill and payment\" for this problem can be expressed such as -high monthly fee ⇒ churn; -high billing amount ⇒ churn; -often due payments ⇒ churn;"
"similarly, figure 3 (ii) illustrates the simulation results for the normalized soa power output and the gain recovery after pump probe measurements, while the external current injection was maintained constant at nominal values of 300 ma. the plots reveal a shortening of the recovery time when elongating the soa from 0.5 to 1.7 mm. the obtained 1/e recovery time was measured and plotted in the inset, indicating a shortening from 63 ps for a 0.5-mm-long soa down to 20 ps for a 1.7-mm-long soa. the effect of the propagation direction of the pump signal was also investigated using the tmm soa model. figure 4 depicts the simulated time traces of the normalized output power and the phase shift of the probe light, obtained after pump probe measurements with the pump control pulse being fed in a co-or counterpropagating direction. the cw probe signal featured an average power of 50 μw, which lies in the small signal regime, while the peak of the control pump pulse was tuned, so as to induce a π phase shift at the cw signal for either propagation direction."
"identification among connections peers. in consideration of browsers need to identify each other by server after building the formal webrtc peer-to-peer connection. since a 'chat room pattern' (multiple users visit a same website url) was designed to achieve identification among connections peers. the process of identification is as follow.  user a visit the appointed chat room address (e.g. http://webrtc/chat1), the page will establish a websocket connection with server after loaded the page. browser sends a websocket message to server. the message name is 'join_in' and data is the chat room identification (chat1). then, user a keeps a wait state.  once the 'join_in' message is received by server, it identifies whether there is a chat room with the same names (chat1). it will create a new chat room if not exist.  user b visit the same chat room address with user a, and sends the same websocket message to server.  once the message from user b is received by server, the same name chat room and existed user will be identified. next, server broadcast all of the users socketid (every websocket client has his own unique socketid) to users.  once all users received the socketid that is broadcast by server, they save it in local environment. now, the identification work among users has been completed. if user a want to negotiate webrtc connection with user b, he just need to send negotiate message and user b's socketid to server. the server will select the user b by socketid and forward the message from user a. the answer from user b to user a has the same procedure."
"as previously mentioned, the tsam method uses the status of an significant variable as an observation value on that variable. due to the potential overlap between two statuses of a significant variable, multiple statuses may be presented. on the consideration of that phenomenon, an observation about variable x in the tsam method is defined as a linguistic possibility distribution by"
step 2: let the key drivers be those shown in figure 2 . the strength of each driver is given in table 4 . table 4 example of strength of identified drivers.
"the proposed solver employs the rate equation (eq. 6) and the multigrid techniques to solve the carrier density distribution along the soa in coarser and coarser grids, while the propagation and amplification of the signal is still based on the connelly material gain coefficient employed in the tmm model [cit] . adopting the previous gain coefficient ensures equivalent steady-state results, such as the optical spectrum and the net gain, and tailoring of the soa parameters with experimental measurements. on the other hand, incorporating an implicit time discretization scheme and adaptive time sampling suggests that computational efficiency is exploited based on the adaptive time sampling in order to benefit from long bit patterns or small pattern changes at the input bit-streams."
"the solve operation can be implemented recursively, while the solution of the coarse grid problem is used to correct the fine grid approximation. high-frequency components owing to the interpolation of the correction cycle can be removed by the final smooth operations. the presented cycle features a grid-size-independent rate of reducing in the error by"
"spatial correlations were similar for all the four dates considered (see si4). [cit] (the most recent date here selected as representative), four out of the 15 es pairs were significantly correlated ( fig. 3 left), two positively (p with a and n) and two negatively (c with a and p)."
"along with the hash table of positions, the sequence needs to be continuously preserved in memory (both in compression and decompression). to minimize its representation in memory, we pack each dna symbol into two bits, instead of the common 8 bits. this approach allows for decreasing to a factor of four the memory associated with the representation of the sequence. notice that sequences with length 100 mbases would require 100 mbytes of ram just to be represented. with the packing approach, only 24 mb are needed."
"while toxicity profiles (as expressed by ld 50 values) are often predicted by correlative analyses using structural and physicochemical descriptors [cit], such molecular properties have been rarely used to predict the ability of a given substrate to generate rms, except for a few studies based on similarity descriptors [cit] . this lack can have a double explanation. on the one hand, the various classes of structural alerts seem to be better related to the ability of a given compound to yield or not reactive metabolites. on the other hand, there may be doubts that a given compound should possess structural features able to forecast the involvement of toxication reactions."
"production frontiers represent the set of efficient configurations, defined as landscape configurations that bring efficient supply of all es according to the pareto criterion. although production frontiers can be used with more than two es or dimensions [cit], applications to pairs of es are the most common because they are theoretical simple, easily displayed graphically, and a first step before analyzing multivariate relationships [cit] . the slope of the frontier at a point represents the marginal es2 loss when es1 increases, or vice-versa . any combination located inside the frontier (rather than on the frontier) is sub-efficient regarding both services considered [cit] ."
"for the whole period (1986 [cit] ), 8 of the 15 possible pairs were significantly correlated ( fig. 3 right), five positively (a-p, a-s, n-p, n-w, and p-s) and three negatively (a-c, c-p, and c-s). for the shorter time intervals, fewer significant correlations were found (see si5). a positive temporal correlation indicated that the pair of es changed in the same direction in the same places, while a negative correlation showed they changed in opposite directions for given places. for example, the negative correlation between a and c ( fig. 3 right) [cit], carbon storage decreased, suggesting a tradeoff between these two es, a finding which concurs with method 1. in contrast, nitrogen and phosphorus retention were positively correlated by this method: in other words, places with increasing nitrogen retention also increased phosphorus retention, suggesting a synergy between these two es, also in agreement with the first method (table 1) ."
"we also explored the possiblity of appending checksum rows to matrices prior to hss factorizatation. this approach does provide an invariant condition, but the invariant is preserved only within the interpolation error of the hss factorization. preliminary experiments indicated that these error bounds were not tight enough to permit efficient error detection."
"the repeat models are combined using the same methodology in the context models. for each repeat, there is a weight which is adapted according to its performance. in this case, the decaying (γ m ) is very small since the weights need to be quickly adapted."
"the method is based on a competitive prediction between two classes of models: weighted context models and weighted stochastic repeat models. as depicted in figure 2, for each prediction, the probabilities are redirected to an arithmetic encoder. the context models (at the left of figure 2 ) are combined through a weighted set of context and substitutional tolerant context models [cit] using a specific forgetting factor for each model, while the weighted stochastic repeat models (at the right of figure 2 ) use a common forgetting factor. an architecture example of a competitive prediction between five weighted context models (at left, represented with prefix c) and three weighted stochastic repeat models (at right, represented with prefix r). each model has a weight (w) and associated probabilities (p) that are calculated according to the respective memory model (m), where the suffix complements the notation. the tolerant context model (cw5, cp5) uses the same memory of model four (cw4, cp4), since they have the same context. independently, the probabilities of the context models and repeat models are averaged according to the respective weight and redirected to the competitive prediction model. finally, the probabilities of the model class with the highest probability (predicted) are redirected to the arithmetic encoder."
"the evaluation metrics for any fault-tolerant algorithm should include the following: the runtime overhead incurred by the resilience mechanism when no fault occurs (failure-free overhead), the runtime overhead when errors occur, as well as the memory cost introduced for fault handling. for runtime analysis, we divide the runtime of our algorithm into three states. a working state indicates normal work (forward progress) of our algorithm, without error checks. failure-free overhead is represented by the checking state, which indicates that the algorithm is performing preventive action (checking), but no error was detected. we account for fault handling time in the recovering state, which includes both time spent detecting an error, and the time recovering from failures. we use the fraction of time spent working as our main performance metric."
"with method 3, es relationships inform us about the tradeoffs society must consider when preferring one efficient es combination over another, and show which combination of pairs of es are in fact impossible (everything above and to the right of the green line in fig. 4) . the revealed tradeoffs lead to reflection on societal preferences regarding what is efficient and desirable. the fact that most observed scenarios are far from pareto efficient combination(s)"
"the exhaustiveness of the random sampling is encoded by the number of sampling cycles performed to generate each model. this parameter is equal to 12 by default; but, as shown in table 1, model generation involved doubling or halving the number of sampling cycles. the calculations seem to be modestly influenced by this parameter and show roughly constant results. however, it should be noted that a lower number of cycles speeds up calculations but unavoidably increases the randomness of the results, thus reducing their reproducibility. hence, the proposed default value appears to be a reasonable compromise, which can be cautiously lowered when generating classifiers including either many variables or involving very extended dataset of descriptors to reduce the computational cost."
"one of the main achievements of this paper is to combine weighted context models with weighted stochastic repeat models using a competitive prediction model. in order to test the impact of the inclusion of both repeat models and competitive prediction model, we include a very repetitive sequence (exogenous from the benchmarking dataset). this test has the underlying idea that repetitive regions are better modeled with weighted stochastic repeat models than by weighted context models. the test sequence is the assembled human y-chromosome downloaded from the ncbi."
"adding the above two, we find the total checksum memory for the v components is: n + 2rn/b. going down the tree for the b components, at each level l, need 2 l−1 vectors of size 2r each, for a total of"
"other studies have found that static approaches (method 1) detect fewer relationships than dynamic approaches [cit] . for four es pairs, method 3 detects es relationships (tradeoffs with c-n and c-w, synergies with a-n and n-s), while other methods do not. these results suggest that, because production frontiers are based on a large number of simulated scenarios, including combinations not observed in past to current landscapes, they can detect more es relationships than approaches based on observed landscapes."
"in this paper, we aim to expand on the techniques and ideas described in this section to provide new tools that can be used by medical practitioners to support early detection of neurodegenerative diseases. the new approach will allow medical practitioners to view real-time results from datasets containing symptomatic information collected from patients at risk or showing early signs of neurodegenerative disease. in the following section, we describe a possible scheme for achieving this. using a multidisciplinary approach, the successful advances presented above are incorporated into a classifier fusion strategy. the proposed scheme illustrates how new perspectives can be applied to early detection procedures in neurodegenerative diseases, by combining state-of-the-art classification algorithms."
the results show that the proposed method attains a higher compression ratio than state-of-the-art approaches using a fair and diverse benchmark. the computational resources needed by the proposed approach are competitive. the decompression process uses approximately the same computational resources.
"real landscapes (black dots in fig. 4) were far from pareto efficient combinations (green curves or dots) for most es pairs. the only exception was with the a-c pair: almost all observed landscapes were bordering the section of the production frontier with high carbon values and low agricultural production. the levels of individual es were in general lower in observed landscapes than in simulated scenarios (grey dots in fig. 4), except in the case of carbon sequestration (mostly high c in observed landscapes) and water yield (some high w in observed landscapes)."
"in this paper, we presented a reference-free lossless data compressor with improved compression capabilities for dna sequences. the method uses a competitive prediction model to estimate, for each symbol, the best class of models to be used before applying arithmetic encoding. the method uses two classes of models: weighted context models (including substitutional tolerant context models) and weighted stochastic repeat models. both classes of models use specific sub-programs to handle inverted repeat subsequences efficiently."
"the computation is directly proportional to the number of features considered in the dataset. figure 4 demonstrates a scatter plot using only three selected features and shows the complexity of classification of gait patterns for each subject. in this instance, feature 1 and feature 2 are associated with 'right and left foot movement signals' while feature 3 represents 'age', which is considered an important factor in disease progression. using the defined feature set, several classifiers have been evaluated for consideration in the final classifier fusion strategy. the principle goal is to use classifiers that perform the best. the classifiers considered are the, linear discriminant classifier (ldc), quadratic discriminant classifier (qdc) and the quadratic bayes normal classifier (udc) for density-based classification. for linear classification, an additional four classifiers are selected, which are the logistic linear (loglc), fisher's (fisherc), nearest means (nmc) and the polynomial (polyc). a linear classifier predicts the class labels based on a weighted linear combination of features or the pre-defined variables. the parzen (parzenc), decision tree (treec), support vector machine (svc) and k-nearest neighbour (knnc) classifiers have been selected for non-linear classification of our datasets. the results produced by all eleven classifiers are illustrated in figure 5 . the results illustrated in figure 5 were evaluated using a confusion matrix table to determine the performance of each classifier. in this instance, the confusion matrix technique was used to determine the distribution of errors across all classes. the estimate of the classifier is calculated as the trace of the matrix divided by the total number of entries. additional information that a confusion matrix provides is the point where misclassification occurs. this shows the true positive, false positive, true negative and false negative values. diagonal elements show the performance of the classifier while off-diagonal elements represent the errors. the accuracy of each classifier is represented as a percentage and is illustrated in figure 6 ."
"building on the previous set of results described previously, this section considers the three best performing classifiers for inclusion in the fusion classifier strategy. from the eleven classifiers tested the linear discriminant classifier (ldc), quadratic bayes normal classifier (udc) and the parzen classifier (parzenc) provide the best results with their accuracy in percentage being 62.5%, 65% and 60%, respectively. these base classifiers were selected and included in the fusion strategy. figure 6 describes the scenario and illustrates the simulated results obtained during the evaluation of gait signals using the eleven base-level classifiers. the results obtained from the three best performing classifiers are stored in a single array, and their error rates are computed. the mean error rate for the three classifiers is 0.42. the same three classifiers are then combined into a cell array using six different rules. the mean error rate for the combined classifiers is 0.40, which is slightly less than the mean of the base-level classifiers."
"a smart campus generally appears as a dynamic and heterogeneous scenario. the system may include fixed sensors deployed by the network administrator and external devices (think for example to mobile terminals of students, professors, technical workers, etc.) that temporally expose their resources. to conclude, technical details of parameters stored in the global security levels"
"the tool includes several default running modes from 1 to 15. apart from some exceptions, lower levels use less computational resources (time and ram) and are more prone to shorter sequences, while higher levels work better in larger sequences. nevertheless, specific model configurations can be manually set as parameters to the program."
"licitus has been conceived as a distributed framework: starting from a set of cryptographic materials and configuration variables stored by the manufacturer or updated by the system administrator, each node is able to autonomously bootstrap security services and negotiate a link-level key with its neighbors without requiring the intervention of any remote and centralized server. therefore, it natively promises good levels of scalability, also in high loaded scenarios."
"the security level of the default key is strictly related to the specific hashing algorithm used to generate it. without loss of generality, licitus may potentially adopt any kind of hash function for generating the 128-bit digests."
the same reason explains the synergy between nitrogen and phosphorus retention: lulc with high retention capacity for nitrogen had usually high retention capacity for phosphorus.
"this section has provided a brief discussion on the most common neurodegenerative diseases, more details can be found in the following references (a.s. [cit] ."
"our main goal is to handle sdc, which must be detected before it can be corrected. the methods we introduce, based on algorithmbased fault tolerance (abft), provide the ability to detect faults and handle errors in software. since these error-correction techniques based on data encoding provide limited recovery from possible errors, we combine encoding techniques with a software rollback scheme to achieve full recovery. we have designed several resilient algorithms for hss matrix-vector multiplication, which differ in the points during the computation at which they detect and recover from errors. our implementation of the rollback scheme uses containment domains (cds) [cit], a hierarchical recovery mechanism that corresponds to the hierarchical nature of hss algorithms. we introduce a finite automata model to describe our performance, and an approximate markov model for comparison. we analyze our results with these models, and compare our results across error rates to determine their effectiveness. the main contributions of this paper are the ft-hssmv algorithms, an analysis of their costs, and measurements of their effectiveness."
"similarly to virtual screening metrics, the ability of a classifier to correctly recognize the relevant compounds can be described by two kinds of parameters: firstly, enrichment factors account for the capacity to focus the correct compounds on the top of the ranking without considering what happens in the remaining part of the ranking; secondly and in contrast, the metrics variously based on receiver operating characteristic (roc) curves evaluate the reliability of the entire ranking but fail to parameterize how many substrates are correctly classified in the first (best) positions (the so-called early recognition problem)."
"replay and mitm attacks are generally performed to compromise the kmp protocol and the mutual authentication between communicating nodes. to further validate the resilience of the proposed framework against these issues, the effectiveness of licitus has been tested through a widely accepted automatic cryptographic protocol verifier developed at inria: the proverif tool [cit] ."
"a new approach in classification research, that has not been fully explored, is the idea of fusing classifiers together. estimates of posterior class probabilities are improved when multiple classifiers are considered in parallel [cit] . combining classifiers in a treelike structure, using weighted averages [cit], is useful for analysing real-time datasets. fusing classifiers together in this way has already been successfully used within other domains, such as the identification and classification of remotely sensed images [cit] . clearly, these studies show that the accuracy and computational time of individual classifiers can potentially be improved when classifiers are combined [cit] ."
"in order to really realize multi-hop connections in both chain and binary tree topologies, devices have been placed sufficiently far, in order to ensure that nodes not directly connected at the layer-2 do not interfere with each other. particular attention has also beed dedicated to the realization of the energy measurement system, used for estimating the amount of the current drained by"
"in this section, we describe two methods for detecting and recovering from errors during the multiplication of an hss matrix with a dense vector (hssmv) these methods are easily extensible to matrix multiplication with a dense matrix (hssmm). hssmv and hssmm are indispensable operations when hss factorization is used as a preconditioner in iterative solvers. hssmv and hssmm are also used in the randomized sampling algorithm for hss construction [cit] . in section 2.1, we discuss the advantages and disadvantages of resilience by preserving data to safe storage relative to methods based on checksum encoding. we review the hss factorization and an hssmv algorithm for matrix-vector multiplication in section 2.2, followed by a description of resilient hssmv algorithms: ft-hssmv by preservation-restoration in section 2.3 and ft-hssmv by encoding in section 2.4."
"skke registers worse performance for two reasons. first, the key negotiation scheme requires the exchange of an higher number of messages. second, the entire procedure is coordinated by a central entity, the trust center, that introduces an additional latency. furthermore, it is evident that skke presents serious scalability issues: the amount of time required to configure security services drastically grows when the network size increases (see, for instance, the chain of 16 hop counts and the binary tree with 10 hop counts). as a result, skke does not scale with the number of nodes in the network. only after that node at hop 9 completed the same procedure with respect to node at hop 8."
"in the following analyses, the size of the cluster will be defined as roughly equal to the number of rm-yielding substrates so that rm 1 encodes the capacity of the model to discriminate between substrates yielding or not reactive metabolites. although maximizing the distance between \"active\" and \"inactive\" instances is the primary objective of the classification algorithm, the quality function also includes the asymmetry index to assure that the frequency of rm-yielding substrates decreases when moving towards the bottom of the ranking. in this way, the entire resulting ranking encode for a sort of probability score, which can be associated with each binary prediction and can become particularly insightful for the doubt cases."
"due to high inter-subject variability between neurodegenerative patients, from mild-to-moderate and from moderate-to-severe, it is difficult to determine the appropriate features to classify data accurately. this problem is further exacerbated when a large number of patients are used. [cit] have addressed this issue by analysing eeg signals using multi-way array decomposition (mad), which is a supervised learning process for evaluating multidimensional and multivariate data like eeg [cit] . the mad approach analyses time, frequency, and electrode signal domains simultaneously. [cit] in studies on epileptic seizures. the parallel factor analysis (parfac) model has also been used to extract the multilinear interaction between groups, frequency, and space in eeg signals [cit] . the parafac model is associated with the multilinear version of the bilinear factor models [cit] . this technique is useful for analysing spatial-frequency characteristics for correct classification of subjects."
"as we have seen, several classification algorithms are used to identify symptoms and determine correlations between behaviour and neurodegenerative diseases. these include csp algorithms, mlp, pnns among others. these algorithms are used to mine data contained in large datasets relating to patients suffering with neurodegenerative diseases. they also help medical practitioners to elicit information about particular features of the symptoms associated with particular diseases. classification algorithms are important vehicles for achieving this however, clinicians are vital to support the final diagnosis and decision-making process. the goal is to provide a sophisticated and potentially powerful tool that improves the early detection, diagnosis and treatment of neurodegenerative diseases."
"we focus on two different ft schemes for these algorithms: one is based on preservation-restoration, and the other is based on checksum encoding. preservation-restoration consists of the following components:"
"we next speculate about what our results imply for making parallel hssmv algorithms [cit] resilient. in a parallel hss construction, the hss blocks are distributed across processors. the steps in algorithm 2 remain essentially unchanged. the coarse method, for example, only requires collective agreement of the checksum conditition at the last step, even though pairwise exchanges are required at the non-leaf nodes to propagate the results between levels. as a result, we expect the error detection and recovery methods described here to be directly applicable. further, we expect the computational and preservation costs for sdc detection to be similar, and the total rate of all (parallel) sdcs to be of the same order, a few seconds, as the rates described in this paper."
"for other es pairs, multiple pareto efficient combinations were identified. when cloud analysis revealed a tradeoff, the shape of the production frontiers provided information about the intensity of the tradeoff: intense (convex curve, also called concave upward or convex downward) or moderate (concave curve, also called concave downward or convex upward). for example, the a-c production frontier had negative slope and a convex shape, which suggested a strong tradeoff: from an efficient configuration with high carbon (c) and low agricultural production (a), increasing a would strongly decrease c (and vice-versa with high a and low c: increasing c would strongly reduce a). in contrast, the concave shape of the c-n production frontier suggested a moderate tradeoff, because increasing one es would only moderately decrease the other."
"earlier work with abft for dense linear systems includes huang's checksum encoding schem for error detection and recovery for ma- trix multiplication [cit] . chen used distributed checksums to enable tolerance of fail-stop failures on distributed systems [cit] . matrix encoding techniques were extended to lu and qr decompositions by luk and park, who used low-rang updates to acheive fast error recovery [cit] . this was applied to distributed parallel lu decomposition by du [cit] . ft algorithms have also been developed for two-sided decompostions such as hessenberg reduction [cit] and bidiagonal reduction [cit] ."
"with reference to the kmp depicted in fig. 2, the formal model developed for the proverif tool 4, contains the following main functions:"
"the primary goal of such algorithms is to extract meaning from potentially huge amounts of data. in other words, to characterise features associated with particular neurodegenerative diseases. this has led to a great deal of work in feature extraction within medical datasets. one example of this is the discrete cosine transform (dct) algorithm that decreases the number of features and the computation time when processing signals [cit] . dct is used to calculate the trapped zone, under the curve, in special bands. these are described as features and used to evaluate different classifiers for neurodegenerative diseases like huntington's disease, parkinson's disease and als. the results show that the quadratic bayes normal classifier is better at identifying different neurodegenerative diseases compared to others. however, they have only evaluated this approach using two feature datasets [cit] ."
"there are eight features in each class, which include signals for the right foot, signals for the left foot, age, height, weight, bmi factor, time, and walking speed. for each variable, the minimum and maximum values are calculated. then random pseudo numbers between these values are generated to produce 20 equal patterns for each class."
"we obtained fifteen scatterplots, based on 468 observations (i.e. 13 sub-watersheds in 36 landscape configurations). in each scatterplot, the production frontier consisted in the set of efficient es combinations identified using the pareto dominance criterion and joined by a line (see si3 for mathematical details). the shape and orientation of the point cloud in each scatterplot, and the proximity of points to each other also provide graphical information on the existence, strength and nature of a relationship [cit] . to describe scatterplots, we adopted an approach similar to the graphical analyses conducted by the envelope of cloud of points was computed using alpha-shape, a computational geometry algorithm that draws straight-line graphs around points and is a generalization of convex hulls [cit] . this envelope was graphically represented in each scatterplot. the portion of the envelope that also corresponded to the production frontier was not represented in case it involved non pareto efficient combinations. in the cases where a linear pattern of association was detected with the shape index, the orientation of the scatterplot informed on the nature of the relationship: synergy for scatterplots oriented from lower left to upper right and tradeoff for higher left to lower right orientation [cit] ."
"after the computation, we proceed with the following three steps to handle potential errors. first, we compute the column sum of ab and compare this sum vector with the last row ofccs, (i.e., [e * a]b). if the two vectors exceed a prescribed threshold, we conclude some entries ofã are incorrect. second, given the correctly precomputed row checksum ae and column checksum e * a, steps 1-4 of algorithm 3 correct up to one error per row (or column) of a."
"to extend our measurements to lower, more realistic error rates, we analyzed our timing data using a markov model. during the runs described in figures 3 and 4, we logged the value of gettimeofday for each error injection event or transition between work, check and recovery states of the fsm. we then post-processed this log to determine the total time in each state and the transition rate between states. figure 5 compares the fraction of the total time spent in the working state as measured by the log files to the fraction predicted by the markov model using the transition rates from the runs with 10 −3 errors/second. agreement between the two is excellent at low error rates. at higher error rates, errors may be injected during the recovery state and, when detected, cause reentry into the recovery state. this relationship between the recovery rate and the error rate is not accounted for by the three-state markov model and explains the diminishing fidelity of the model at higher error rates."
"in this way, it is possible to enable the join of external nodes not supporting security capabilities or not in possess of the right initial credentials. in fact, when a node without security capabilities or initial credentials wish to join a network (that is already operating in a secured fashion), it has just to send a beacon request in clear. its corresponding coordinator processes the request and switches to the hybrid secured configuration only if the flexibility feature is enabled. from that moment on, the considered node may join the network, all the broadcast messages are transmitted in clear, and nodes supporting security capabilities still continue to exchange protected unicast packets with their parents. of course, this feature is optional and it can be enabled or disable by the network administration according to the target design criteria."
"as a first step, a security analysis is presented for demonstrating the resilience of the proposed framework against different kinds of attacks. then, the communication overhead incurred during the setup of a secure domain is evaluated. finally, an experimental analysis is presented for assessing, in different topologies, the time and energy required to establish a secured domain."
"in the future, we will develop fault-tolerant algorithms for the other hss matrix operations, including hss construction and ulv factorization. since all these algorithms follow the same hss tree structure, our hierarchical style preservation-restoration can be employed in a similar fashion, but new invariant conditions must be developed for effective error detection."
"at a basic level, it allows individual patterns or features within the data to be explicitly associated with particular diseases. for example, abnormal and chaotic body movements caused by damage to neurons can be associated with huntington's disease. at a more advanced level, the similarities between different neurodegenerative diseases need to be clearly defined. this will allow a patient's unique needs to be considered when deciding on appropriate treatments. while, having similar characteristics, different neurological diseases cause atrophy in different parts of the brain; huntington's disease causes damage to the caudate, parkinson's damages the substantia nigra, and als damages the lower motor and pyramidal neurons, resulting in severe damage to body movement. furthermore, there is a need to take into account other features directly related to diseases such as age, gender and so on. clearly, focusing on a single correlation is unlikely to identify a particular neurodegenerative disease. solutions that are designed to make correlations between multiple patterns or features within the data are likely to be particularly effective in identifying specific neurodegenerative diseases."
"even though there is limited research on es relationships in central america [cit], tradeoffs between erosion control and biodiversity have been found in our study site [cit] and synergies between carbon sequestration and water-related services have been identified at the national scale [cit] . we did not observe a clear relationship between carbon and water, which might suggest that scale has an effect on the nature and intensity of the relationships detected."
"now, without loss of generality, we can consider two devices willing to negotiate a layer-2 key: node a and node b. for instance, a could be a child node directly connected to the network coordinator and b could be the network coordinator. the kmp supports both anonymous and certified dh schemes. it consists of six consecutive steps (see figure 2 )."
"to sum up, when the master key is secret, licitus is resilient against any attacks aiming at compromising the mutual authentication (like men-inthe-middle, replay, etc.). furthermore, when the secrecy of the master key is compromised, the mutual authentication is still guaranteed only in the case the kmp makes use of x.509 certificates."
"in addition to retrogenesis, toxic proteins and gait abnormality, as mentioned earlier, there are some other potential features that can play a significant role in the early detection of neurodegenerative diseases. for instance, alzheimer's disease has three major effects on electroencephalogram (eeg): slowing of the eeg, reduce complexity of the eeg signals and perturbations in the eeg signals [cit] . different synchrony measurement techniques are substantially employed to detect any perturbation in eeg of alzheimer's patients and healthy subjects. some of these techniques are pearson correlation coefficient [cit], magnitude and phase coherence [cit], granger causality [cit], phase synchrony [cit] . along the same lines, other imaging modalities can also be considered for the early detection of alzheimer's. for instance, magnetic resonance imaging (mri) is used to measure the volume of specific brain area, such as hippocampus [cit] . diffusion tensor imaging (dti) is an emerging non-invasive technology to visualise subcortical fibre tracts. transcranial magnetic stimulation (tmi) is used to evoke electrical response in the brain which ultimately helps to examine the degree and progression of dementia [cit] ."
"2 l ). this is similarly defined for block diagonal matrices v (l) and b (l) . one key advantage of the hss structure over other non-hierarchical structures is the use of nested bases. that is, at any intermediate node τ in the hss tree, the actual basis u big τ is not stored explicitly, but only represented as the unevaluated product of the bases of the children (ν1 and ν2) and the node τ 's (small) basis uτ, as follows:"
"show that the introduction of security services generally incurs a not negligible computational effort, which worsens communication latencies and energy efficiency. however, they also demonstrate that licitus is able to overcome the skke protocol of the zigbee ip security architecture by speeding up the configuration of security services (up to 120%) and ensuring relevant energy savings (larger than 50%)."
"in what follows, a summary is proposed to draw the main features of the leading approaches proposed so far to cover the empty spaces left by the ieee 802.15.4 standard."
"the third test assumes that the key agreement protocol is based on the certified dh algorithm and the attacker knows the master key. as reported in fig. 3(c), proverif demonstrates that a malicious node is not able to compromise the right execution of the kmp. therefore, the mutual authentication is still guaranteed. in fact, it is registered that: node b completes the kmp when the procedure is really initiated by node a, and node a completes the kmp when the procedure is really initiated by node b."
"one possible approach is to build on the advances made in ehealth systems to improve the detection, diagnoses and treatment of such diseases to support disease management and integrated care strategies [cit] . this will allow physicians to incorporate information and communication technologies into the decision-making process to enhance the diagnosis of such diseases and inform treatment strategies [cit] . the research agenda is timely, given that conclusive diagnosis of these diseases is currently only possible posthumously, by direct examination of the affected brain tissue after the death of a patient [cit] . compounding the problem further, obvious symptoms of neurodegenerative diseases are only visible during the advanced stages of the illness (i.e., gait impairment) when no possible cure is available. this often leaves the patient in a miserable condition awaiting his or her death. clearly, new approaches are required to detect the early onset of symptoms associated with such diseases to either prevent or mitigate disease progression [cit] ."
"future work will consider the application of the proposed novel technique on large datasets with more significant and promising set of features. although, this research work has successfully substantiated the theory of gait relation with neurological disorders, great deal of work is required to prove the conjecture of neuronal destruction by some noxious proteins. a plausible augmentation is the multi-way analysis of eeg signals from spatial-spectral and temporal perspectives."
"with these encouraging results in hand, the last part of the study applied the efo approach to uci datasets which are routinely used to test new machine learning algorithms as collected in http://archive.ics.uci.edu/ml/index.php. these analyses had two primary objectives since they were planned to test the efo performances when using (1) non-cheminformatics data and, more interestingly, (2) roughly balanced datasets. among the available datasets, attention was focused on two balanced datasets chosen because they were recently used for benchmarking analyses in a study to validate a new method for generating training and test sets and thus the here obtained results can be easily compared to the published models [cit] . moreover, the two chosen datasets involve categorical predictions based on categorical, integer and real attributes. in detail, the first dataset comprises various health data for heart patients and the predicted attribute refers to the occurrence of heart disease in the collected patients [cit] . the second dataset involves sonar signals and the predicted attribute is the discrimination between metals or rocks based on a pattern of 60 frequency-modulated signals in the range 0.0 to 1.0 [cit] ."
"we modeled the six es using estimates of prices and yields for each agricultural product for the provisioning service, and the invest software for the regulating es. invest consists of a suite of spatially-explicit models that use lulc maps, biophysical and economic data to quantify and map various es provided by landscape in biophysical or economic terms [cit] for static spatial correlations, we calculated the pearson correlation coefficients between pairs of es (sub-watershed level) at the four dates. for the temporal correlations, we calculated the variations of each es between two consecutive dates (1986 [cit], 1996 [cit], 2001 [cit] ) and between the start and end of the whole period studied we graphically represented the production possibility set of each pair of es by plotting es values in all landscape configurations (four observed and 32 simulated) against one another."
"the remainder of this paper is structured as follows: section 2 discusses the major neurodegenerative diseases that are affecting older people. this includes huntington's disease, alzheimer's, parkinson's disease and als. section 3 describes classification algorithms (mathematical models for extracting patterns and features from the data) and their current use in neurodegenerative disease research. section 4 describes the classifier fusion strategy posited in this research work. section 5 analyses the potential of the approach discussed in the paper and makes suggestions for future work before concluding the paper in section 6."
"the standard imposes to use the ccm* algorithm and a 128-bit key to protect mac frames. at the same time, the ccm* algorithm assumes that each key must be used for a specific number of block ciphers (i.e., until the frame counter associated to a given communication reaches its maximum value)."
"for method 1 (spatial correlation), es levels depend in many cases on shared underlying factors, such as lulc. for example, the observed strong tradeoff between agricultural provision and carbon sequestration results from the fact that invest models for carbon sequestration and agricultural production are based on simple look-up tables with lulc with high agricultural production having low carbon sequestration and vice-versa (except for coffee agroforestry systems). as a consequence, those es cannot be observed at the same time in one given lulc."
"we assume that the memory model starts with counters all set to zero. through all the computation, the memory model is updated according to the outcomes. therefore, the prediction of each context model is set along with the training. notice that, in figure 2, the models four and five share the same memory model (cm4) because model five is an stcm with the same k as in model four."
"password-guessing attacks try to extract keys by means of dictionary-attacks [cit] . licitus is extremely robust with respect to this kind of attacks because nodes do not use passwords for computing layer-2 keys. in fact, default and link keys are obtained from the master key and dh parameters, respectively."
"diffie-hellman key agreements [cit] . security analysis, instead, is done by using the dolev-yao model, i.e., a baseline procedure reproducing many operations that could be done during an attack (i.e., capture and modification of the stream of messages exchanged between two devices over an unsecured channel). starting from a set of initial assumptions (for instance, the attacker only knowns the algorithm, the attacker also knows the master key, and so on), the tool verifies if a malicious node is able to successfully force the kmp procedure, thus compromising the mutual authentication property, or not."
"accordingly, all the files used in this article have been losslessly decompressed using the same machine and os (linux ubuntu). regarding different floating-point hardware implementations, we have only tested one sequence compression-decompression (drme) between different hardware and os version, namely compressing with one (server) machine and with a specific os and, then, decompress with a different (desktop) machine and os version. although it has worked in this example, we can not guarantee that it stays synchronized on machines if they have different floating-point hardware implementations."
"the three methods are increasingly sensitive for detecting es relationships in the order of their presentation here: spatial correlations, temporal correlations, production frontiers. for most es pairs, if the first method leads to a specific interpretation about es relationships, the same interpretation is found with the second and third methods; but the first method leads to fewer interpretations on synergies or tradeoffs than subsequent methods (table 1) . for tradeoffs, production frontiers enable a precise description of tradeoff intensity, which correlations do not allow."
"our initial (uncompressed) matrix was a square h-matrix with side lengths of 20,000, generated for each off-diagonal block with rank of 5% of its size. the hss construction used 1,000 random sampling vectors. the relative error in the matrix l ∞ norm due to compression of 10 −10 . we then performed 10,000 iterations of the hssmv algorithm, each with a different random vector. figure 3 shows the runtime overhead for each ft algorithm when no errors are injected. encoded+coarse is a hybrid method that uses matrix encoding as its primary fault tolerance mechanism and resorts to coarse-grained restoration when ft-gemm is unsuccessful. note that these timings include only the hssmv iterations and exclude a) the time to construct the hss matrix (167 s), b) the time to store a nonvolatile copy of the hss matrix (0.2 s), and c) time to precompute the checksum arrays (0.2 s). in every case, the additional costs for data preservation and error checking are less than 2% of the runtime without ft (labeled \"none\" in the figure). runtime overhead for the fine-grained cds is higher than the others, but is nevertheless small-nearly within the natural performance variation of the system. figure 3 also lists in parenthesis, the total memory used by each algorithm. memory use doubles when cds are used because an additional copy of the matrix required for recovery. the fine-grained matrix encoding algorithm requires only 1% more memory than the non-resilient algorithm, but is somewhat less robust than using cds (see section 2.4)."
"in parallel, welfare economics and production theory have inspired frameworks to describe relationships between es [cit] . in these frameworks, the set of production possibilities describes all combinations of multiple es levels that can be accommodated within a landscape given its structure, natural capital and management inputs (human labor, technology, etc.) [cit] ) . [cit] ) (fig.1) . such combinations often called \"pareto optimal\" or \"pareto efficient\" [cit], although strictly speaking the pareto criterion applies to people, not to services or goods [cit] . the terms \"pareto optimality\" and \"pareto efficiency\" are used interchangeably in the literature, even though the former is often used as a normative criterion indicating desirable situations, while the latter implies a more neutral description in positive economics [cit] . for this reason we use \"pareto efficiency\"; often just \"efficiency\" for conciseness, acknowledging that we always talk of allocative (pareto) efficiency and not productive efficiency (i.e. production at the lowest cost). the boundary of the set is known as the production possibilities frontier, also called the \"efficiency frontier\" [cit] ) . for conciseness, we refer to this as the \"production frontier\" in the following."
"similarly to the previous case, when the anonymous dh scheme is selected, a prime number, p, and the corresponding primitive root, g, are firstly extracted from the prime numbers table ( by following the aforementioned procedure). then, the public key, p b,b, is computed from the randomly generated private key, p v,b, according to the dh algorithm:"
"the adoption of frame counters makes the ieee 802.15.4 technology resilient against replay attacks. since the proposed approach fully integrates ieee 802.15.4 security features, it also inherits robustness to these threats."
"our data shows that ft-hssmv algorithms effectively handle errors at high error rates, about one error per second. since the failure-free overhead of our algorithms is small, the overall runtime overhead when failures do occur remains small until the time between errors approaches the recovery time of the algorithm. we showed that the additional runtime costs of medium-grained and fine-grained error checking are small. our analysis of these results with the markov model shows that we can identify the limits of these algorithms' effectiveness by performing measurements at low error rates, and extrapolating these results to higher error rates. finally, our experience developing these algorithms shows that the preservation methods are robust and straightforward, at the expense of requiring a form of safe storage, whereas the encoding techniques require no safe storage, but required changes to data structures and modifications to the algorithms to ensure that the additional encoding information is maintained."
"the analysis of the amount of energy consumed by each node of the network, reported in figure 7, fully confirms all of the comments reported above."
"our set of scenarios includes land-use configurations that are probably not acceptable to stakeholders. another study on es and landscape scenarios suggested that a full-restoration scenario may be of limited relevance to decision-makers, since it is regarded as unfeasible, but remains scientifically relevant as a benchmark to assess conservation efforts [cit] . the production frontier approach relies on the way we define plausible hypothetical scenarios and the possibility of including socially unacceptable scenarios."
"almost all obtained models include a proper combination of physicochemical and stereo-electronic parameters even though the relevance of the latter is here clearly less pronounced than in the previous classifiers, a result particularly evident for the descriptors featuring the homo/lumo energies. this finding can be explained by considering that homo/lumo energies and their derived parameters are particularly informative in predicting chemical reactivity, while here physicochemical properties are more convenient in describing the recognition between substrates and enzyme. clearly, some stereo-electronic parameters are also included in these last models where they reasonably account for the covalent phases of the enzymatic reactions as seen for electrophilicity indices when predicting substrates undergoing nh/noh oxidation (mod. 8), as well as the lumo energies for substrates undergoing csp 2 /csp oxidation (mod. 7)."
the three methods all concluded that the relationship between agricultural production and carbon sequestration shows a tradeoff. this result is consistent with other studies analyzing relationships between those two es [cit] . other es showed either no relationships with agricultural production (for example water yield) or synergies (for example phosphorus retention). this can be explained by the models we used to quantify es: the simplified representation of water and nutrient processes in
"in this case, the energy spent by the network coordinator, the leaf child, and the intermediate node (namely parent in figure 7 ) have been measured. the coordinator, which is in charge of handling the most of tasks in the network, always incurs the highest energy consumption. on the contrary, child devices experience the lowest energy consumptions due to the lower number of operations they manage during the time. however, slightly larger energy consumptions are registered for intermediate nodes that, differently form the leaf child, have to manage key negotiation mechanisms with their parent and child nodes."
"as a final consideration, we remark that the time instant when the default key is computed is different for network coordinator and child node. in particular, while the first device can generate the key as soon as it becomes the coordinator of a given portion of the lln, the child node should firstly receive the beacon messages (i.e., association phase), to extract the parameters needed for the computation of the default key. note that this task can be handled without any problem because such parameters are stored in the mac header and, hence, transmitted in clear."
"in the following two subsections, we describe our approaches to making algorithm 1 resilient to failures. we expect the hssmv algorithm to be used repeatedly in an iterative solver. errors may occur in this long stretch of computation."
"the predictions reported below involved both substrates yielding rms in the first-generation as well as those giving rms in any generation. indeed, one may suppose that the properties of a given substrate might someway anticipate the reactivity of metabolites formed directly from it, while such properties should be less effective in predicting the reactivity of metabolites, which are indirectly generated in the subsequent generations. however, the predictions reported here involved also substrates giving rms in any generation, by considering that an optimal model should be able to predict the capability of a given molecule to yield reactive metabolites regardless of the involved metabolic generation. moreover, and focusing on first-generation rms, specific models were also developed by separately considering the reactive metabolites produced by some specific metabolic reactions."
"neurodegenerative diseases are chronic, irreversible, life threatening and incurable diseases. accurate recognition of diseases in relation to patterns is still a big challenge in the field of brain informatics and neuro computing. moreover, the advanced symptoms of almost all neurological diseases are the same. for example, gait abnormality is common across most neurodegenerative diseases. consequently, during the later stages of a disease it is important to make correct correlations between symptoms and the type of disease to tailor treatments. in this paper, we begin to address this problem using an automated non-invasive fusion classification technique with different combining rules. eleven classification techniques were initially evaluated using a confusion matrix and the best performing classifiers were selected and used in the classifier fusion strategy. using roc analysis, we compared the base-level classifiers (linear discriminant classifier, quadratic bayes normal classifier and parzen classifier) and the fusion classifier strategy posited in this paper. the results show that our approach performed better than base-level classifiers. furthermore, the results show that the classifier fusion strategy generates better results when combined using the 'voting rule'."
"finally, at bottom level for d, the total storage is n. adding all the above, the total size of the checksum vectors for the fine-grained approach is 2n + 5rn/b."
"the development of efficient dna sequence compressors is fundamental for reducing the storage allocated to projects. the importance is also reflected for analysis purposes, given the search for optimized and new tools for anthropological and biomedical applications."
"researchers suggest that classification algorithms might provide a platform for achieving this [cit] . statistics [cit], large-scale data analysis [cit] and visual analytics [cit], have changed the way we view the instrumentation of human behaviour and the environments we live in. it is now possible to measure many more aspects of human behaviour that includes human physiology and gait. these rich data sources provide the basis for understanding long-term changes and correlations between dependent and independent variables associated with medical and healthcare outcomes, including the well-being of individuals. a great deal can be learnt from advances in data processing that have already been made within many other domains and how they can be applied to the early detection of neurodegenerative diseases."
"the first test assumes that the key agreement protocol is based on the anonymous dh algorithm and the attacker does not know the master key. as reported in fig. 3(a), proverif demonstrates that a malicious node is not able to compromise the right execution of the kmp. thus, the mutual authentication is always guaranteed. in fact, it is registered that: node b completes the kmp when the procedure is really initiated by node a, and node a completes the kmp when the procedure is really initiated by node b."
"preliminary models were developed with a view to identifying the first-generation rms generated by specific metabolic reactions from among all considered substrates, but these initial analyses proved unsuccessful (models not shown). along with the above-mentioned problem of the unbalanced datasets which here appears to be particularly exacerbated, such a failure can be explained by considering that such models in fact involve two distinct predictions, namely which substrates yield rms and which substrates undergo a specific metabolic reaction. reasonably, these two distinct features can depend on different (and maybe contrasting) molecular properties thus justifying the unsatisfactory results."
"multi-layer perceptrons (mlp) and probabilistic neural networks (pnns) have featured widely in research to process and analyse medical datasets [cit] . mlps are feed-forward networks that work with back-propagation learning rules [cit] . pnns are similar to mlps in that they are feed-forward networks that consist of three layers; an input layer, radial basis layer, and a competitive layer. this type of feed-forward network operates using the parzen's probabilistic density function (pdf) [cit] . in terms of overall performance, pnn networks perform slightly better than pml networks."
"even though endogenous protective mechanisms (many of them based on the marked scavenging effects of glutathione (gsh), are able to detoxify these reactive species, the generation of rms should be minimized or, better, essentially avoided in new drug candidates, especially considering that gsh levels are often markedly lowered under several pathological conditions such as oxidative-based diseases [cit] ."
"for some es pairs, there was a single pareto efficient combination, where both es had their highest levels. this situation generally occurred for es pairs that were in synergies according to the point cloud analysis (e.g. n-p), but not always (cf si6 for a graphical explanation). for example, the a-w pair had one single pareto efficient combination but no clear and strong es relationship according to the cloud shape. in contrast, some plots showed an extended production frontier where the shape of the cloud suggested synergy (e.g. a-n) or noninteractive es (e.g. p-w)."
"there are many file formats to represent genomic data-for example, fasta, fastq, bam/sam, vcf/bcf, and msa, and many data compressors to represent specifically these formats [cit] . all of these file formats have in common the genomic sequence part, although in different phases or using different representations. the ultimate aim of genomics, before downstream analysis, is to assemble high-quality genomic sequences, allowing for having high-quality analysis and consistent scientific findings."
"alternately, the correlation can be performed on the difference in es supply at two times, called \"the spatial correlation of temporal variation\", or the \"change-over-time approach\" [cit] or \"correlation analysis between the amounts of changes in es\" [cit] . for conciseness, we refer to it as \"temporal correlation\" in the rest of this paper (fig. 1 ). it can also be computed using either pearson or spearman correlation coefficients."
"since in the cpcm the context order (k) is the crucial parameter, we assessed the impact of the variation of the context order according to different modes for different genomic sequences. figure 5 depicts this assessment using the hosa, enin, aeca, and yemi sequences (in decreasing order of sequence length). the remaining plots for the other sequences in the dataset can be found at the code repository. generally, there is a relation between the context order of the cpcm and the length of the sequence (according to the respective redundancy), where longer sequences require a higher context order, and shorter sequences stand for lower context orders. as an example, the hosa sequence (largest) is better compressed (in level 12) with a context order of 16, while the yemi sequence (shortest) is better compressed (in level 2) with a context order of 5. the described competitive prediction model runs in high-speed using reasonable accuracy. the accuracy of the model can improve with the development of a prediction based on multiple models, namely through weighted context models. however, this creates a trade-off between accuracy and computational time, which may be very high for the gains that it may produce."
"the fine-grained error checking provided the best performance at high error rates. figure 4 shows that the runtime of the coarse-and medium-grained ft algorithms increased steeply when the error rate exceeded the hssmv iteration rate. at the highest error rate tested (1 per second), performance of the coarse-grained algorithm is roughly half that of the fine-grained cd and encoding schemes, which are nearly the same speed as the unprotected algorithm."
"the production frontier approach provides information about es relationships beyond that yielded by either static or temporal correlations. it shows how current configurations differ from the efficient ones or could be improved, and therefore it can be used to discuss stakeholder preferences for different efficient landscape configurations [cit] . although [cit] ) . [cit] . it can be identified by combining the production frontier with indifference curves that represent the preferences of a given stakeholder or social group for es (i.e. how much they would trade off one es in exchange for another) [cit] . some challenges have been raised regarding the application of production possibility framework, such as the difficulty of identifying plausible landscape configurations and es combinations, and the rather abstract nature of discussions about landscape optimality and efficiency [cit] values at two times or more for method 2) and different landscape configurations (one observed landscape configuration for method 1; two configurations of the same landscape at two dates for method 2; and a large number of hypothetical landscape configurations for method 3). all approaches describe different aspects of es relationships (tradeoffs and synergies), that can be compared. thus, one should not expect the three methods to come to the same conclusions. for instance, in the simple example shown in fig. 1, methods 1 and 3 suggest tradeoffs between es, whereas method 2 suggests synergies. the three methods might result in different patterns of es relationships."
"to bridge this gap, this work designs, implements, and experimentally evaluates (for the first time) an integrating framework, namely lightweight scheme for iot secure communications (licitus), that harmonizes and orchestrates, the functionalities of consolidated approaches to security configuration, bootstrap, and key negotiation phases. the resulting system offers: (i) a rich set of possible security configurations that can be enabled also in heterogeneous scenarios where protected communications may coexist with unsecured ones; (ii) an effective methodology for easing the initialization of secure ieee 802. 15.4 networks; (iii) a lightweight key management protocol (kmp) for negotiating a layer-2 key between a node pair; (iv) resilience to several security attacks, and (v) a full compatibility with ieee 802.15.4 technology."
"regarding the effect of cluster size, table 1 reveals that the two monitored enrichment factors (efs) show contrasting trends: indeed, the ef in the top 1% improves when cluster size is reduced yet becomes worse when increasing the cluster size. in contrast, the ef in the top 10% parallels the cluster size, reaching a maximal value in clusters with a size equal to that of the considered top 10%. as discussed under methods, these preliminary analyses confirm that the cluster size must be almost equal to the number of positive compounds included in the used dataset. based on these results, the following preliminary analyses were carried out by constantly considering the cluster size to be equal to 100."
"as mentioned earlier, the predictions were repeated by simulating the substrates in their neutral and ionized forms. in detail, the used database includes a significant amount of ionizable molecules (315 out of 977 among which 215 basic compounds and 100 acid substrates); notably the ionizable molecules which yield rms represent the 13.7% (43) a percentage truly superimposable to that seen in the whole database thus suggesting that the ionization characteristics do not influence the propensity to yield rms. the relation between ionization state and predictive power should reveal whether molecular ionization can bias some relevant stereo-electronic properties (such as those derived by homo/lumo energies) rendering them less efficient in properly accounting for chemical reactivity. stated differently, if neutral molecules provide encouraging results, focusing only on neutral state might be advisable to avoid incorrect protonation states for molecules with complex ionization equilibria."
"as mentioned in the introduction, the study involved 977 compounds, the 3d structure of which was either generated manually using the vega software or, when available, automatically retrieved from pubchem. the molecules were simulated both in their neutral form and in their preferred ionization state as existing at physiological ph. their conformation and atomic charges were optimized and refined by pm7 [cit] which also allowed the calculation of a relevant set of stereo-electronic descriptors including, among others, the homo/lumo-based reactivity indices and the delocalizability descriptors. the minimized conformations were then used by vega to calculate an extended set of geometrical and physicochemical descriptors by discarding highly correlated variables. in this way, a set of 28 descriptors was collected and used in the development of the predictive models as described below. the computed descriptors were directly used in the study without scaling, weighting, or normalization procedures. the dataset used in the predictive analyses for neutral substrates is collected in table s1 ."
"as proposed in a previous study [cit], cluster analysis can afford a graphically intuitive way to evaluate the overall reliability of a classifier by monitoring how many rm-yielding compounds are included in each cluster. a suitable model should be able to place most of the rm-yielding substrates in the first cluster, with their proportion progressively decreasing in the following clusters. in contrast, models which randomly distribute the rm-yielding substrates in all clusters should be considered unsatisfactory regardless of how many correct molecules are placed in the best clusters."
supplementary materials: the following are available online. scheme s1: pseudocode for here proposed efo algorithm; table s1 : metabolic data and molecular descriptors used as learning set to develop the predictive models; table s2 : computed scores for each substrate and for the four best classifiers as reported in table 4 .
"the security configuration manager can be directly configured by the manufacturer or by the administrator before the deployment of the network. to ensure the protection against tampering attacks, specific software-based and/or hardware-based mechanisms may be used for preventing the physical access to all the stored variables [cit] ."
"for example, in the above reported analyses, the compounds classified in the top 10% are considered as yielding rms, and mod. 1 places 48 true positives in this best cluster. this means that mod. 1 shows a sensitivity of 0.48, meaning that if a molecule falls in the top 10% it has a probability of 48% to yield one or more rms. nevertheless, this probability is not constant but depends on the computed score and indeed if a given molecule has a score that brings it in the top 2% the probability of giving rms raises to 63%. similarly, if a substrate falls in the remaining 90% it is predicted as non-reactive and mod. 1 has a specificity equal to 0.9, meaning that this predicted probability is equal to 90%. however, if a given molecule falls in the bottom 10% this probability increases to 97%. in other words, the proposed method allows a score-based probability to be assigned to each prediction. this can represent a crucial advantage compared to most available classification approaches."
"as reported by the message sequence chart in figure 4, the child node sends a first request to the trust center, which replies both to child and parent nodes with an initial shared secret (i.e., the master key in the skke protocol language). then, this secret is used to protect the following 6 messages exchanged between the nodes, for finalizing the key agreement mechanism. now, considering the size of each logical message and the constraint on the mtu, 9 different mac messages are required to complete the protocol. indeed, as a first general comment, it is possible to observe that the skke protocol integrated in zigbee ip specifications does not support an authenticated key agreement mechanism. however, supposing to use the anonymous dh scheme as the key negotiation algorithm, it can be immediately noted that the number of mac packets required to establish a secured communication in"
"furthermore, there is a close relationship between neurodegeneration and toxic proteins [cit] . the accumulation of pathological neurofibrillary plaques and tangles develop in the entorhinal cortex and hippocampus parts of the brain. these proteins play a pathogenic role in the progression of neurodegenerative diseases, which results in neuron degeneration and cognitive impairment. neuroscientists argue that damage between the entorhinal cortex, and the hippocampus leads to memory loss [cit] ."
"from one side, the standard describes, with a high level of accuracy, procedures and parameters to handle secured mac frames. from another side, it does not clarify some crucial aspects, such as the initialization of a secure ieee 802.15.4 domain, the generation and the exchange of keys, the configuration of security-related parameters at the mac layer, and the definition of how the entire system may react when a new device (that does not support security capabilities, or is not able to synchronize itself with the existing secure domain)"
"as well known, resource-constrained devices are unable to perform complex algorithms and protocols in a limited time, so that it is mandatory to implement simple and effective key agreement protocols [cit] . for this reason, we develop a lightweight approach with limited computational and bandwidth requirements."
"as mentioned above and reported in table 2, the second part of the study aimed at comparing optimized classifiers able to predict: (a) substrates giving first-generation rms; (b) substrates giving rms in any generation; substrates giving first-generation rms through specific metabolic reactions such as (c) oxidations of csp 2 and csp atoms, (d) oxidations to quinones or analogues and (e) oxidations of nh or noh moieties."
"since these preliminary analyses have the primary objective to calibrate the algorithm parameters, the model generation was performed for simplicity on the entire dataset avoiding validation procedures. among the user-defined parameters able to influence the proposed classification algorithm, attention was focused here on four key parameters, namely (1) the size of the cluster by which the quality function is calculated (see equation (1)), (2) the exhaustiveness of random sampling, (3) the filtering cut-off in the top 5% enrichment factor below which a variable is discarded, thus influencing the number of considered descriptors, and (4) the number of variables included in each model. all these initial analyses were performed by predicting the substrates which gave rms in their first generation, and by considering the compounds in their neutral state. table 1 shows the results of these calibration analyses obtained by monitoring the performance of the generated classifiers as parameterized by three relevant metrics: the average values for the 20 considered models of the substrates giving rms in the top 1% and in the top 10% of the corresponding rankings, and the highest number of substrates found in the top 10%. these two percentages were chosen because the enrichment factor as computed in the top 1% encodes the ability of the method to concentrate \"active\" molecules in the top of the ranking, a feature which is particularly relevant in typical virtual screening campaigns, while the top 10% corresponds to the percentage of the ranking which is particularly relevant in these preliminary analyses since it roughly corresponds to the number of substrates giving first-generation rms. indeed, the dataset includes 138 rm-yielding substrates out of 978 compounds. these numbers imply that the rm-yielding compounds represent about 10% of the dataset. the performances, as encoded by the mean percentage of substrates giving rms in the top 1% and top 10% as well as highest number of \"positive\" substrates in the top 10%, are evaluated by exhaustively varying the cluster size, the sampling cycles, the number of included variables, the cut-off of the preliminary filtering to discard uninformative descriptors (in parenthesis, the number of discarded descriptors when lowering the threshold cut-off value) and the ionization state of the substrates. notice that in these analyses the mean and best top 10% correspond to the mean and best model sensitivity. n and i stand for substrates simulated in their neutral and ionized forms, respectively."
our work is distinguished from other ft algorithms by its focus on the hss matrix format and the use of cds to perform finegrained error correction within the hssmv operation.
"to provide a further insight, we also investigated the impact that such energy consumption has on a network operating in the long run. the study, however, demonstrated that the amount of energy consumed during the initialization of security services when both licitus and skke are used has a limited impact on the network lifetime. in fact, it is always less than 1% of the whole battery capacity. nevertheless, in the case the application scenario requires that security services must be periodically renewed, the energy saving reached by licitus may further amplify its advantages, thus increasing the network lifetime. anyway, being out of scope of our contribution, we leave a deep analysis of this specific aspect as a future work."
"the study proposes a novel classification algorithm based on linear combinations of descriptors, which are generated through enrichment factor optimization (efo). even though this approach could find many insightful applications in virtual screening campaigns, it is here presented by considering its potential as a general classification approach in predicting substrates yielding rms. the study takes advantage from a previously collected and reported metabolic database and reveals that even though most hitherto published predictive models are based on the occurrence of well-defined structural alerts, the capacity of a given substrates to form rms (at least in the first-generation metabolism) can also be predicted by using physicochemical and stereo-electronic descriptors with the latter playing a key role in parameterizing the intrinsic reactivity of a molecule and its metabolites. as an aside, the study also comprises classifiers able to recognize the kind of metabolic reaction a molecule can undergo, and these preliminary results open the door to the use of this approach in metabolism predictions. more generally, the predictive models developed here emphasize the potential of using highly curated metabolic datasets and suggest that the exploited database can provide reliable learning sets for developing various metabolic predictive models. moreover, and for simplicity, the here proposed models were developed focusing on the lowest energy conformation even though one may argue that monitoring more than one representative geometry might improve the models especially for very flexible molecules."
"the arrival of high throughput dna sequencing technology has created a deluge of biological data [cit] . with the low sequencing costs of next-generation sequencing [cit], metagenomics [cit], ancient genomes [cit], and biomedical applications [cit], the number of available complete genomes is increasing widely. most of the data are discarded and, when classified as crucial, compressed using general or specific purpose algorithms. additionally, with the increasing of ancient sequenced genomes, the quantity of data to be compressed is now achieving a higher magnitude [cit] ."
"moreover, the second prediction type has the added benefit of involving clearly less unbalanced datasets since here the positive compounds are 39, 23 and 24 out of the 138 first-generation rms as produced by oxidation reactions of csp 2 and csp atoms, to quinones or analogues, and of nh or noh moieties, respectively. based on the previous results and focusing on the randomly generated training set, the cluster size is equal to 25 for the first reaction type (mods. 5 and 8, table 2 ) and 20 for the other two cases (mods. 6, 7, 9 and 10, table 2 ), while the initial filtering of the variables based on their enrichment factor on the top 5% was rendered less stringent (the required ef value equal to 1.0 instead of 2.0) to avoid an excessive reduction in the number of descriptors considered. table 2 compiles the best classifiers as generated by considering either neutral or ionized substrates. as a trend, the obtained models show truly satisfactory statistics as emphasized by the corresponding mcc values always greater than 0.5. conceivably, these remarkable results benefit from using markedly smaller and less unbalanced learning sets compared to the previously used datasets and these results suggest also the here proposed approach is influenced by the composition of the learning sets even though additional tests involving very unbalanced datasets (as used, for example, in virtual screening campaigns) should be required to precisely assess the performances and limitations of the efo method. more importantly, these notable models bear witness to the possibility of successfully predicting the specific metabolic reaction(s) a given substrate may undergo, considering only physicochemical and stereo-electronic descriptors. we note that such type of prediction could find more general applications in predicting the metabolism of xenobiotics."
"in this paper, abnormality in gait is the key feature considered. neurophysiological changes associated with aging affects the locomotor system's ability to generate stride-interval correlations [cit] . therefore, analysing correlations between stride intervals and neurological functions provides an important marker for detecting the onset of neurological diseases. force sensitive switches placed inside each subject's shoe are used to measure the stride intervals [cit] . the output received from these switches provides a measure of the force applied to the floor. the signal was sampled at 300 hz and stored in a lightweight, ankle-worn recorder. subsequently, the recorded signal was automatically analysed to determine initial contact time and, hence, the stride intervals (the time from initial contact, typically heel strike, to the next initial contact of the same foot) for each gait cycle of the walk."
"in the case of adoption of the anonymous dh, a prime number, p, and the corresponding primitive root, g, are firstly extracted from the prime numbers table by considering the latest n p bits from the output of the following hash function:"
"many types of faults occur in computer systems [cit] . the impact of these faults and the techniques for detetecting or recovering (or not recovering) from the errors they induce depend on the timing and location of the fault. permanent and intermittent faults cause frequent failures at the same location and are mitigated by replacing hardware after uncorrectable permanent errors are detected [cit] . transient faults, such as particle-induced bit flips, are more likely to be random and not reproduceable. many transient faults can be detected and corrected in hardware by ecc and memory scrubbers. rollback recovery, based on checkpoint restart is commonly enployed on parallel systems to recover from uncorrected faults that lead to fail-stop errors. when undetected, \"soft\" (i.e. transient) faults can modify data without causing the program to halt or providing other error notification, leading to so-called \"silent data corruption\" (sdc) that can propagate through a calculation and result in incorrect output. in future hpc systems, silent faults are expected to occur at higher rates than fail-stop errors [cit] ."
"for method 2 (temporal correlation), es relationships are interpreted as the consequences of underlying processes, such as land-use changes. in the study site, lulc changes are dominated by changes in forests and agricultural areas. as forests have no agricultural production and high carbon sequestration (and agricultural areas have the opposite), those es show opposite trends over time [cit] . other es pairs show surprising results: for instance, agricultural production and sediment retention are positively correlated using method 2, even though they are un-correlated with method 1. because the drivers of the spatial distribution of es (hydrological connectivity, altitude, climate, etc.) can be different to drivers of their temporal evolution (lulc change, urbanization), the approaches can identify different es relationships."
"apart from for the models developed in the first part, which involved the entire dataset, the dataset was randomly subdivided into a learning and a test set (by default 70% and 30%). this was done by using a specially developed vega script (training and test set creator.c), the models being generated using only the learning set, followed by their validation using them to predict the molecules yielding rms included in the test set. to minimize the influence of randomness, this task was repeated 5 times. the results below describe the best predictors obtained by this approach."
"the best of authors' knowledge, no contribution explored so far the opportunities and subtleties that could arise from a joint integration of existing solutions, in a standard compatible way."
"several in silico approaches have been proposed with a view to minimizing the risk of rm generation [cit] . to predict the metabolite's reactivity, two different scenarios can be figured out. firstly, and when all major metabolites a given drug candidate can generate have been characterized, several computational approaches mainly based on stereo-electronic descriptors can be used to assess their reactivity [cit] . secondly, and when the metabolic profile of a new compound is still unknown, its potential to yield reactive metabolites can be estimated by considering the occurrence of functional groups (the so-called structural alerts) which are known to generate rms based on mechanistic studies on known drugs associated with idiosyncratic reactions or other toxicity profiles. in drug development strategies, these structural alerts should always be avoided regardless of the advantages they would offer [cit] . clearly, this second scenario is more frequent because toxicological screening is usually performed in the early phases of drug development when the number of drug candidates involved is too high to permit extended experimental metabolic studies [cit] ."
"learning from imbalanced datasets is an important and controversial topic that is addressed in this paper. these kinds of datasets usually generate biased results [cit] . for instance, imagine a medical dataset with 50 true positive values (majority class) and 20 true negative values (minority class). if half is selected for training and the remainder for testing (25 healthy and 10 sick persons), we find that the accuracy is 90%. the result suggests that the classifier performs reasonably well. however, what happens, when all the positive values are accurately identified and only five out of the ten negative values. in this situation, the classifier is more sensitive to detecting the majority class patterns but less sensitive to detecting the minority class patterns. this is caused because the training data is imbalanced. in other words, the classifier concludes that five out of the ten unhealthy people are healthy when this is not the case. these kinds of results ultimately cause more destruction if data comes from real time environments, such as biomedical, genetics, radar signals, intrusion detection, risk management and credit card scoring [cit] ."
"for a compression method to be considered lossless, all the compressed sequences must be decompressed exactly to the original sequences. the current compression and decompression methodologies are symmetric. this symmetry means that both weighted context models, weighted stochastic repeat models, and competitive prediction model are synchronized in the same order using the same characteristics. additional side information is included in the compressed file (in the beginning) in order for the decompressor to use the same characteristics. for example, in the weighted stochastic repeat models, the seed is passed in the header to ensure the exact beginning in the stochastic process."
the medium-grained approach adds two checks to the coarsegrained version. the two additional checks verify that the intermediate quantitiesxτ are computed correctly and are performed after
"the two first methods, both based on observed landscape configurations lead to similar conclusions for most pairs of es (table 1) . the exceptions are the four pairs for which method 1 does not show significant correlations, whereas method 2 does. for example, there is no significant spatial correlation between a and s; but temporal analysis shows that places where agricultural production increases also show increased sediment retention, suggesting a synergy."
"the hypothetical nature of the landscapes scenarios generated in order to construct production frontiers explains some differences between findings: given that landscape changes are usually slow and in continuity with previous configurations, observed landscapes are generally similar one to another (i.e. the black dots in fig. 4 were clustered). therefore, methods relying solely on observed data offer only a narrow glimpse of the full range of potential es values. in contrast, production frontiers consider a broad range of es levels, revealing unsuspected relationships between es. several publications observe that in order to be useful to decisionmaking, scenarios must account for the uncertain nature of the future, and incorporate surprises and discontinuities [cit] . this requires large sets, with as complete a range of configurations as possible."
the memory overhead for this coarse-grained error check is smallwe need only one vector c of size n. the drawback is the long time to recover from errors; this is especially important on systems with high error rates.
"given the well-known limitations of the common classification algorithms in providing satisfactory results when, as in this study, the learning set is markedly unbalanced, a classifier method based on logistic regressions as driven by an enrichment factor optimization (efo) has been developed and included in vega zz package as the automatic model builder.c script. such a method predicts a categorical dependent variable (the rms generation) by developing linear combinations of continuous independent variables (the molecular descriptors). since the developed equations produce continuous output values and not the expected binary outputs, during the learning phase the n best score compounds are considered as positives and the remaining (t − n) compounds are considered as negatives (where n is the number of positive compounds included in the training set and t is the total number of instances). for example, the prediction of substrates generating first-generation rms assumed the 138 compounds with the best scores as yielding rms, while the remaining 839 molecules are considered as non-reactive substrates. the resulting classifiers were thus evaluated by considering their capacity to place the rm-yielding molecules within the 138 top positions in the ranking. accordingly, the score computed for the 138th compound, namely the last compound considered as positive, represents a threshold value, which will allow the discrimination between positives and negatives in the validation phase and, more in general, when applying the obtained model to external compounds."
"the clouds of points showed a wide diversity of shapes over all es pairs (fig. 4) . all pairs involving water yield (w) showed dispersed clouds of points, with shape indices over 0.75."
"in more detail, the models compiled in table 2 allow for some interesting observations. apart from the models obtained to recognize the substrates undergoing oxidative reactions at csp 2 and csp atoms, the ionized substrates perform better than the neutral ones. this finding is in contrast to the results so far reported and can be explained by considering that here the best performing ionization state should correspond to that concretely involved in molecular recognition by the relevant metabolizing enzymes, while the previous predictions mostly depend on the intrinsic reactivity of a molecule that is less influenced by the simulated ionization state as evidenced in the previous sections. on these bases, the obtained results suggest that the ionized forms play a role in quinone formation and more markedly in nh/noh oxidation where the ionization equilibria can directly affect the sites of metabolic attack (compare mods. 7 and 10)."
"none of the three methods really deal with es interactions; they reveal es relationships from which interactions can be inferred but not proven. in addition, the observed es relationships are partly explained by how invest models the services, rather than by real-world es interactions. establishing es interactions requires a better understanding of their underlying causes and the relationships between es and global drivers (land-use change, climate change, etc.). by using associations or correlations as proxies for causal relationships, we fall into the fallacy that correlation proves causation. there is a risk of suggesting active interaction where the correlation is either spurious or due to a common underlying driver . [cit], but elaborated to consider the shape (convex or concave) as well as the slope of the relationship (positive or negative), can be applied to suggest the mechanisms and possible interactions behind the reveal relationships. rigorous establishment of causal mechanisms for interactions will require experimental work and innovative approaches, not just correlative studies ."
"as depicted in figure 2, licitus always requires 4 logical messages. considering that ieee 802.15.4 specifications impose a maximum transmission unit (mtu) equal to 127 bytes, such messages are mapped into 4 mac packets when the kmp is based on the anonymous dh scheme. instead, when the certified dh approach is adopted, the number of mac packets becomes 24. in fact, by storing the public key in a x.509 certificate [cit] of 864 bytes 5, the first two logical messages defined in the kmp scheme need to be fragmented in multiple mac packets."
the comparison of the generated best performing model with the classifiers developed by using different classification algorithms implemented in the weka software reveals that the proposed approach compares with the best available approaches and shows two crucial advantages since it involves a limited number of descriptors and provides a score-based probability which allows a critical evaluation of the obtained prediction.
"a thorough evaluation of the licitus framework is reported in this section, along with extensive experimental comparisons with respect to the zigbee ip security architecture."
"as schematized in figure 1, the algorithm proposed here is composed of several logical units starting from a preliminary data filtering which allows the selection of the most informative descriptors. in such an initial process, each descriptor is filtered based on its capability to place the rm-yielding substrates in the top of the ranking by simple ef analysis. only descriptors with an ef value as computed for the top 5% greater than a user-defined threshold (by default equal to 2.0) were selected for the model generation."
"notably, linear discriminant analysis (lda) and the functional linear discriminant analysis (flda) [cit], which are very popular approaches to predict categorical features. using continuous variables, perform markedly worse than the here proposed algorithm even though the model obtained with flda shows an mcc (2.6) close to the mentioned threshold of 0.3. among the algorithms which surpass this threshold, the k-nn classifiers (ibk) [cit] and the randomizable classifiers [cit] generate models with performances very similar to those of mod. 1 in terms of both the mcc value and the number of true positives. finally, tree algorithms offer the best performances among the methods included in the weka software: in detail, the classifier based on the pruned j48 algorithm [cit] shows a comparable mcc value and a higher number of true positives compared to mod. 1, while the random forest method [cit] yields the highest mcc value but a lower number of true positives."
"conceivably, the performances of the generated models increase with the number of included variables, even though table 1 shows that marked statistical enhancements are seen up to 4 variables, while the inclusion of additional variables induces more limited improvements probably due to overfitting problems. thus, classifiers including five or (at most) six variables should represents an optimal balance between computational time, predictive power, and robustness of the classifiers. in contrast, the generation of models with more variables requires a computational cost, which is not justified by the marginal increase of the corresponding performances as clearly witnessed by the models including 8 or 10 variables. table 1 shows that the influence of ionization state on model performances is clearly limited; ionized substrates afford slightly better results when considering classifiers with few variables, while more complex models show almost identical performances regardless of the substrate's ionization state. when considering that the abundance of ionizable molecules within the dataset, the role of ionization state deserves further investigations and therefore the following models will be generated considering in parallel neutral and ionized substrates. based on these preliminary analyses and to speed up the model generation, the following predictive analyses were carried out by considering: (a) a cluster size roughly equal to the number of \"positive\" substrates; (b) sampling cycles equal to 12; (c) filtering cut-off equal to 2.0; (d) classifiers including six variables."
"from one side, it is suggested to use robust hash functions, thus making the entire framework resilient against attacks. from another side, instead, it is even important to adopt hash functions that do not require high computational capabilities (that are generally scarce in constrained nodes forming a ieee 802.15.4 network)."
"adding or removing one scenario in the plot could transform a production frontier into a single pareto efficient combination or vice-versa. for this reason, the short production frontiers of some es pairs (e.g. a-n; a-p or a-s) could be artefacts of scenario selection and should not be over interpreted (see also si6). interpreting production frontiers is only robust when it is supported by a similar interpretation of the cloud. this underlines the importance of using a large number of scenarios to build production frontiers and applying sensitivity analysis by adding or removing hypothetical landscapes and observing how results change."
",k . endfor endfor endfor only one error per row or column. 4 however, both high efficiency and strong resilience could be achieved by hybrid methods that first attempt to recover using algorithm 3 and resort to restoration when necessary. table 1 summarizes the memory requirement for various hssmv algorithms presented in this section, including the extra storage needed for the ft algorithms. the second column is the baseline of the size of hss matrix a itself. the next two columns contain various checksum vectors and the data size in the safe store."
"when the certified dh approach is used, the public key is not computed from scratch; it is instead directly delivered by the certificate associated to the child node."
"as stated before, the security configuration manager controls the level of security, imposes the corresponding minimum requirements, and supports all the procedures handled by other components of the framework. to this aim, the following parameters and initial credentials are stored:"
"a growing volume of work incorporates ft into iterative solvers with sparse matrices. bronevetsky and de supinski combined abft for encoding and detecting errors with checkpoint based recovery to tolerate errors in sparse iterative methods [cit] . shantharam developed a checksum encoding scheme to detect errors in sparse matrix-vector multiplication and triangular solve operations, and used these to construct a ft pcg solver [cit] . zhang combined an inner-outer solver (that is relatively insensitive to faults during inner iterations) with preservation-restoration techniques to build a ft iterative solver [cit] ."
"where i i, v, and ∆t are the i-th sample of the measured current, the voltage of the batteries (i.e., 3 v), and the sampling time interval (i.e., 50 µs), respectively. with their parent as soon as they complete the association phase. in the zigbee ip security architecture, the skke protocol starts immediately after the reception of the beacon message. in the proposed security framework, instead, the kmp is executed only after the generation of the default key by the bootstrap manager."
the kmp is implemented in a distributed manner: a couple of communicating nodes can negotiate a layer-2 key without needing to interact with any remote and trusted node. the resulting approach is potentially scalable and it does not brings to huge latencies in complex networks.
"in this paper, we ask the following question: do different assessment methods lead to different interpretations and conclusions about es relationships? we apply three different methodsspatial and temporal correlations between es pairs and production possibility frontiers -for assessing es relationships in the upper part of the reventazón watershed in costa rica, compare their outcomes and implications, and discuss the assumptions and applicability of each of the methods."
"the discussion so far has highlighted a number of ways to classify features within datasets. while these do provide obvious benefits, there is still scope to improve the overall accuracy of classification. our proposed method provides a possible scheme that explains how the design goals have been incorporated within the scheme and highlights the novelty of our approach."
"we tested our ft hssmv algorithms by injecting errors into the hss factors at random times during the hssmv operation. before the hssmv iterations begin, we register every u, v, b and d block with an errorinjector class. when matrix encoding is used, the entire encoded matrix is registered. the error injection module schedules 1,000 errors with frequencies sampled from an exponential probability distribution and memory locations uniformly distributed across the registered arrays. during the iterations, errors injection is triggered according to the schedule by comparing the current time to the schedule after each preserve call (or blockmv when matrix encoding is used). a new schedule is generated whenever the previous schedule is completed."
"the study was organized in two parts: the first part involved a set of calibration analyses aimed at investigating the effect of the key parameters influencing the here reported classification approach as well as the role of ionization state in developing the predictive models. based on these preliminary results, the second part will involve the generation of optimized predictive models that will be then compared with those which can be generated by applying a set of well-known classification algorithms as implemented in weka 3.8 software [cit] ."
"licitus, combined with the security capabilities already integrated within the ieee 802.15.4 technology, is robust with respect to the most important and critical security issues, as cryptanalysis, tamper attacks, password guessing, replay attack, and mitm."
"where i(t) is the data retrieved from the data source, that is mapped to some signal s(t) and the inherited noise found in the signal is defined as n(t). consequently, the filtered value can be defined as the signal s(t) -the noise value n(t). as a preprocessing step, relevant features are extracted from integrated data. after completing this stage, all extracted features are meaningful and ready for classification. in this study, 3,000 'motion vector' values for left and right foot strides of each subject were extracted over a 10-second period. the mean values obtained from the motion vectors are used to eliminate erroneously recorded data."
"the dataset containing the eight features described in the previous section provides the feature sets required to diagnose neurodegenerative diseases accurately. more specifically this dataset is used to select a classifier, train it, test it and finally evaluate the result to determine if the correct classification is performed."
"as a general comment, it can be observed that the configuration of security services always brings not negligible computational efforts. in fact, it emerges that the higher the network size, the higher the amount of time required to initialize security services and to negotiate layer-2 keys. however, obtained results clearly demonstrate that the proposed approach always ensures the lower airtime consumption, thus emerging as a promising, efficient, and scalable solution."
"note that the sum over ν is not over all descendents of τ, but only a set of descendants at the same level l . algorithm 2 is the medium-grained ft hssmv algorithm. the text in blue highlights the error detection/recovery procedures. after steps 1 and 2, the checks are performed using the precomputed check vectorsw. the third check, after step 5, uses the precomputed c check vector introduced for the coarse algorithm."
the rest of the paper is structured as it follows: sec. 2 presents some of the most important techniques nowadays available for securing layer-2 communications in iot systems. the proposed licitus framework is presented in sec.
"interpreting production frontiers leads to new insights on es relationships, but at the price of complexity, since several lines of evidence must be simultaneously considered. production frontiers are also sensitive to outlier es combinations. for example, some es pairs show either a single pareto efficient combination (e.g. a-w) or a concave production frontier (e.g. s-w);"
"is the concatenation of allxτ on level of the hss tree. suppose we introduce a weight vector w. (in practice, w can be a vector of all ones.) we pre-compute the intermediate check vectors at each level l:"
"the construction of production frontier is a two-step procedure [cit] . first, es are assessed across a set of management options (as large and diverse as possible) [cit] ) . second, efficient combinations for each es pair are identified using pareto-dominance criteria or statistical estimators [cit] . analyzing all possible management scenarios is in most cases practically impossible because of data and computational requirements, particularly if es models are not automatically connected to the computer tools for building scenarios [cit] . even though production frontiers depend on scenario selection and too few scenarios could lead to ambiguous conclusions [cit], using a set of a limited size is acceptable if it includes sufficiently diverse and contrasted scenarios close to the putative frontier (i.e. adding more scenarios to the analysis will not improve substantially the production frontier) [cit] . another approach uses smart sampling strategies to improve scenario selection, for example constraint optimization [cit], latin hypercube sampling [cit] or genetic algorithms [cit] . however, such sampling strategies often ignore that some scenarios are biophysically or socioeconomically unrealistic."
"with this hierarchical, unevaluated product representation, the stored basis matrices are asymptotically smaller than u big τ, and the compression results at the children are reused at the parent node, hence also reducing operation count asymptotically."
"the method enables setting any number of context models and repeat models, as long as at least one model is used. this setup permits very high flexibility to address different types of dna sequences and creates space for further optimization algorithms."
"despite the complex nature of neurodegenerative diseases, the starting point is retrogenesis. this stage shows early signs cholinergic system malfunction in the basal forebrain. as the disease progresses, it eventually affects the entorhinal cortex and the hippocampus part of the brain causing damage to short and long-term memory [cit] . further atrophy in the affected area of the cerebral cortex leads to speech loss, damage to sensory neurons, and the inability to reason. symptoms may include enhanced memory loss, attention loss, difficulties recognising family members, getting dressed and movement."
"memory is needed for the check vectorsw at all levels of the hss tree. denote r as the hss-rank, and b as the block size at the finest level partition. at each level l, there are 2 l vectors of size r each. the memory required for the check vectors is: output:"
"jarvis shows and improvement of 1.1% and 0.9% to geco and geco2, respectively. the computational time is competitive with geco and slighty more than geco2. regarding the second-best tool in compression ratio (xm), jarvis improves the compression to approximately 0.6%. in addition, it is faster 5.7 times more than xm. regarding ram, jarvis used a maximum peak of 7.12 gb in the largest sequence. these are competitive memory values with geco/geco2 and, at least, half of the ram needed by xm. figure 6 shows the compressed size and speed, where the mean of speed values for all datasets is calculated to obtain the average speed for each method. as depicted, jarvis shows the best compression rate since the compressed size is the lowest. on the other hand, geco, geco2, and xm seem to have very similar performance, while paq8 and lzma are not so efficient in genomic data. regarding the speed, jarvis is approximately at the level of lzma and geco2, showing that the trade-off between computational resources and precision is minimal. additionally, jarvis can run with other modes. in figure 7, we include a comparison of all the fifteen modes in jarvis for the three largest sequences. for example, running jarvis with level 12 in hosa sequence achieves 38,280,246 bytes (1.6139 bps). this result is an improvement of 1% over jarvis in mode 7. the trade-off is computational time and ram, however still less than xm. therefore, jarvis is flexible and can be optimized to achieve considerably better compression ratios. the optimization, besides the choice of the best model, can be applied in a specific combination of the number of models, depths, estimator parameters, among many others. table 2 because we rerun the tool. each number, corresponding to the blue dots, stands for the mode/level used in jarvis. we recall that additional levels or specific configurations can be set."
building on the dataset defined in the previous section the dimensionality of the matrix is reduced to produce a set of feature vectors. this is commonly referred to as feature extraction. this process is crucial to classify signals accurately. this process also removes erroneously recorded signals caused by sensor malfunction and noise that can have a negative effect on signal classification. this process can be defined using the following mathematical formula and the process is illustrated in figure 2 .
"their decisions often involve tradeoffs between es, deliberate when they reflect explicit choices or unintentional when knowledge is lacking [cit] . recent publications highlight the different uses of es knowledge in decision-making processes and distinguish between decisive, technical and informative uses [cit] . with the two former uses, knowledge about es relationships contributes to defining and evaluating policies. for example, it may help allocate financial and human capitals to the land management in a way that improves multi-functionality and reduces competition between services now and in the future [cit] . informative use of es relationship knowledge is also important to raise awareness about environmental problems and foster dialogue, debate and negotiation between stakeholders [cit] ."
"as depicted in figure 8, all the modes from jarvis compress better than the best mode from geco2. jarvis (mode 12) achieves a compression 5.413% better than geco2 (mode 15) using approximately the same computational time. this example shows a substantial improvement using both weighted stochastic repeat models and the competitive prediction model."
"secondly, our research work has focused on eight variables (features) as input for our classifiers unlike previous work where only left and right feet signals are considered. neurodegenerative diseases are more common in males as compared to females and they are closely linked to the age of the person. therefore, we have also considered gender and age variables to produce results that are more reliable. moreover, we have also considered the stage of the disease, patients walking speed and time that are other important input variables."
"finally, regarding the mitm attack, it could be launched by either an internal device (i.e., a malicious node that knows secured secrets shared among the rest of network devices) or external nodes (i.e., that do not know the value of variables stored within the security configuration manager component) for compromising the right execution of the key negotiation phase. the designed kmp uses well-known approaches already adopted in the past for other protocols, as transport layer security (tls). hence, its security can be demon-strated by using existing analysis. in the first part of the protocol, the dh algorithm is used to negotiate the shared key. here, mitm attacks can be avoided using x.509 certificates, used to uniquely bind the public key to its owner. the mutual authentication scheme implemented in the second part of the protocol, instead, protects the entire process against replay attacks. the aim of the two latest messages is inspired to functionalities provided by finished messages in the tls protocol [cit] . similarly to tls, these packets carry an authentication field that is computed by considering all values, included random numbers, exchanged with the first couple of messages. thus, the security proof related to the proposed protocol is as for the tls protocol [cit] ."
"abnormality in gait is another symptom that has a strong correlation with cognitive impairment. [cit] argue that a healthy gait pattern requires input not only from the neurological system associated with motor and sensory neurons but also from cortical processes, such as judgment, planning and a spatial awareness. in other words, it is generally agreed that gait disturbances are closely related to disturbances in cortico-cortical and cortico-subcortical connections, e.g., frontal connections with the parietal and frontal lobes and the basal ganglia, respectively [cit] ."
"ieee 802.15.4 specifications [cit] allow to protect mac packets by means of symmetric-key cryptography techniques, based on the well known aes-ccm* algorithm. moreover, to support several security options, the standard also specifies eight possible configurations:"
the number of logical and mac messages that a pair of devices have to exchange for establishing a secure communication represents the first term of comparison between our proposal and the zigbee ip security architecture.
"the tool is accompanied with the appropriate decompressor, which uses slightly less time to decompress than to compress, and approximately the same ram. the decompressor is approximately symmetric. all the sequences that we tested have been losslessly decompressed."
"the present study investigates the feasibility of predicting the ability of a given molecule to yield reactive metabolites by using physicochemical and stereo-electronic descriptors. the predictive models were generated by using a purposely developed classification algorithm based on enrichment factor optimization (efo) and implemented in the vega suite of programs [cit] . the study takes advantage from the already reported database which includes metabolic data as collected by manually curated analysis of the primary literature as published in the years [cit] . in detail, the database contains 1171 substrates (drugs and xenobiotics) which yield 6767 metabolic reactions and includes information about each included metabolite being (or not) a reactive molecule [cit] . the present study is focused on the substrates giving reactive products in the first metabolic generation, namely in the metabolites directly deriving from parent compound (138 molecules out of 977, i.e., 14.1%) as well as in all generations (217 molecules out of 977, i.e., 22.2%). in detail, the first-generation rms are primarily produced by the oxidation of unsaturated carbon atoms which represent the most abundant function (28%), followed by oxidations of nitrogen atoms (18%) and quinone formations (17%)."
"the second test assumes that the key agreement protocol is based on the anonymous dh algorithm and the attacker knows the master key. differently from the previous case, the malicious node can successfully compromise the right execution of the kmp. in fact, as reported in fig. 3(b), proverif realizes that when node b completes the kmp, it is not possible to ensure that the procedure was really initiated by node a. indeed, it could be initiated by the malicious node. similar considerations can be argued in the case a is the node that completes the kmp."
"anyway, apart these preliminary comments, the time and the amount of energy needed to configure a secure system are strictly influenced by the network load, packet losses, de-synchronization issues, and complexity of each task, as discussed in the rest of the section."
"other algorithms, such as dissimilarity-based classification techniques, have proven to be very useful for analysing medical datasets. for example, algorithms, such as the k-nearest neighbour classifier (k-nn), and linear and quadratic normal density-based classifiers, have been extensively used to classify seismic signals [cit] . nonetheless, the results have shown that bayesian (normal density-based) classifiers outperform the k-nn classifier, when a large number of prototypes are provided."
"step 5: the node b verifies the validity of the received t a parameter. in affirmative case, it computes the authentication parameter, t b, sending it to the node a:"
"this section presents the results for experiments performed on the fusion classifier strategy. in this paper, the multiclass receiver operating characteristic (roc) analysis [cit] technique is used. this technique is useful for analysing several different classes, in our case four different classes. first, the classifiers are evaluated in matlab using the 'testc' routine, which provides several performance estimates for a trained classifier on a test dataset. the mean value produced by the test results for individual classifiers is 0.42, which is an error rate. in comparison, the mean value for combined classifiers is 0.40, which is obtained by combining different classification rules. this has clearly shown that the combined classification technique works better than the individual use of classifiers. moreover, the results depict that the voting combination rule works more efficiently than other combining rules used. using the voting combination rule, the prediction of the base-level classifiers is combined according to a static voting scheme, which does not change when changes to the training set are made [cit] . figure 11 shows the results of the roc analysis for the base-level classifiers, where the 'quadratic normal bayes classifier' shows the least error rate compared to all other classifiers. in this case, error i represents the 'false positive' values, while error ii presents the 'false negative' values that show the system's failure to predict any disease and label the objects as healthy persons. as it can be noticed from figure 11, the uncorrelated quadratic bayes normal classifier generated less errors and produce better classification when benchmarked with the bayes normal-1 and parzen classifiers. this is because quadratic bayes normal classifier (bayes normal-u) uses uncorrelated variables [cit] . figure 12 shows the results when classifiers are combined using various combining rule algorithms that include the product, the mean, the median, the maximum, the minimum and the voting combining rules. as shown in figure 12, the best result that produces the least error is the 'voting combiner' with a value of 35.0%. this is closely followed by the 'product combiner', 'mean combiner' and 'maximum combiner'. while other rules like 'median combiner' and 'minimum combiner' are showing 45.0% and 47.5% error respectively. most of the literature surveyed only considers skewed datasets, where the number of healthy and diseased persons is not equal. this ultimately generates a biased result due to the dominating effect of the majority class. even the roc curves are hard to compare using different classifiers for different misclassification costs and class distribution. we have analysed an equal number of objects for each class to avoid misclassification."
"the capacity of drugs and other xenobiotics to generate electrophilic reactive metabolites (rms) is an unwanted property that should be carefully avoided during the design and development of drug candidates [cit] . this is easily explained by considering that rms can couple with nucleophilic sites within endogenous molecules forming stable covalent adducts endowed with clear toxic effects [cit] . even though detailed molecular mechanisms of toxication are not always well understood, and probably rms are not the only trigger factor, a direct link between their formation and idiosyncratic adverse drug reactions (iadrs) is widely accepted [cit] . moreover, rms are also involved in drug-induced liver injury (dili) along with other factors such as a marked lipophilicity and high daily drug doses [cit] . not to mention that, when covalent modifications target dna, rms generation can result in mutagenicity [cit] ."
"at the end of calculations, the resulting output files comprise: (i) a file containing the results for the selected best models, (ii) a log file including the details of the performed calculation, (iii) a file compiling the computed scores for each molecule and for each model and (iv) a reduced input file including only the best performing descriptors. this last file can be used as input to speed up the calculations in which models including several variables are generated and for which an exhaustive model generation could become too time-consuming. the pseudo-code of the entire algorithm is included in the supporting information (scheme s1)."
"alzheimer's is the most well-known of the neurodegenerative diseases and the one that poses the greatest growing challenge among the aging population [cit] . the results from a recent survey showed that while cancer and heart disease have typically been the top priorities in healthcare, alzheimer's has become just as important during recent years [cit] . alzheimer's is a disease that affects memory, thinking and behaviour. the changes within the brain that accompany these symptoms are toxic proteins (amyloid beta -aβ) referred to as 'tangles' and 'plaques' [cit] . these pathological neurofibrillary tangles accumulate in the entorhinal cortex and hippocampus parts of the brain, used for short and long-term memory [cit] . neuroscientists have researched that in order to keep the memory alive the communication between these two parts is essential, were any compromises often leads to memory disturbance and eventually memory loss [cit] . the occurrence of tangles has so far proven incurable and irreversible. the exact cause is unknown and there is no evidence to suggest whether the disease or the build-up of proteins is the root cause."
"the objective of this study was to compare different methods for assessing es relationships, using an example in costa rica. the methods we selected imply different assumptions about es relationships and their quantification. two methods (spatial and temporal correlations) relied on observed landscape configurations, and one (production frontiers) on simulated landscapes. the three methods showed different levels of sensitivity in detecting es relationships. interpreting spatial and temporal correlations is apparently straightforward, but the interpretation of production frontiers is more complex since it relies on several features of es pair-plots: the shape, orientation and dispersal of the cloud of points, and the slope, shape and length of the frontier. all methods described similar tradeoffs between agricultural production and carbon sequestration. some synergies between agricultural production and other services were also observed, suggesting that a general pattern of tradeoff between provisioning and regulating services should not be assumed without caution. our analysis provides useful guidance on how to interpret production frontiers. as the three methods provide different contributions to decision-making on es, it is recommended to choose methods in accordance with the decision context or to combine methods and compare their implications for decisionmaking."
"in this section, we benchmark the proposed compressor against state-of-the-art data compressors. the dataset proposed for this benchmark contains 15 genomic sequences [cit], with a consistent balance between the number of sequences and sizes. moreover, it reflects the main domains and kingdoms of biological organisms, enabling a comprehensive and balanced comparison. the dataset contains a total dna sequence lenght of 534,263,017 bases (approximately half a gigabyte)."
"the classifier fusion strategy posited in this paper incorporates several distinct processes; data gathering, feature extraction, and feature evaluation, as illustrated in figure 1 . combining these processes provides a system for processing gait data to support the early detection of specific neurodegenerative diseases. industry led datasets with toolsets designed for processing large biomedical data are used to provide a solution for the early detection of neurodegenerative diseases that performs better than several well-known approaches. using this unique configuration, new toolsets are provided for real-time symptomatic data analysis of neurodegenerative diseases to support diagnosis and treatment strategies. working with good datasets is a key requirement within the classifier fusion strategy. it allows data about real people, who may or may not have a neurodegenerative disease, to be used to train and test the approach. for example, gait data collected from normal people and those suffering with alzheimer's can be used to define features and test the success of detection using a combined dataset that contains the two sets of data. processing industry led biomedical datasets has proven to be very successful in computational data analysis with current medical conditions [cit] ). however, many of these tools only consider individual algorithms. addressing this limitation is important to allow comprehensive studies across different data using a combination of algorithms to increase predictive capabilities at the detection stage. the following sections describe the data collection, features extraction and feature evaluation processes in more detail."
"besides offering a graphical evaluation, the distribution of substrates which yield reactive metabolites can be used to derive a quantitative parameter based on the asymmetry index (ai), a measure of deviation of the cluster distribution from a normal curve which can be computed based on the pearson's moment coefficient of skewness [cit] . thus, the greater the ai value (namely the more right-skewed the distribution) the better the model. moreover, and to also optimize the early recognition, the quality function optimized in the model generation is calculated as the product of the asymmetry index and the percentage of rm-yielding substrates included in the first (best) cluster (rm 1 ). this is reported in equation (1) where n is the number of clusters, rm i is the abundance of substrates yielding rm in the i cluster and rm m is the rm average."
we would like to thank roberto cordone of dipartimento di informatica at the università degli studi di milano for the valuable support in code engineering.
"the key negotiation manager implements all the functionalities for the definition of the link key used for layer-2 unicast communications. it is made up of three entities: the kmp engine, which implements the key negotiation scheme to derive a pre link key; the command handler that translates logical messages of the kmp in mac messages; and the key generator that generates all layer-2 keys, starting from the pre link key."
"finally, the last analyses focused on non-cheminformatics data emphasize the general applicability of the efo method which provide satisfactory results even when using balanced datasets regardless of the type of included variables. specifically, the efo application on the heart dataset afforded a highly performing model, which allows a very easy and successful prediction of the occurrence of heart disease."
"hereafter) and five regulating es -carbon storage (\"c\", capacity to store carbon and thus mitigate climate change), water yield (\"w\", quantity of freshwater runoff per year), nitrogen and phosphorus retention (\"n\" and \"p\", contribution of the ecosystem to retaining nutrients on the land rather than allowing them to be carried off in runoff) and sediment retention (\"s\", capacity to prevent soil erosion). these es are of particular importance in the study area because of the economic importance of agriculture and water-related activities (such as hydropower production) and the susceptibility of soils to erosion. cultural services are also important in this landscape, but we did not have data to assess them."
"huntington's disease is a type of dementia related to alzheimer's disease. huntington's disease is less common than alzheimer's and parkinson's, and was first discovered by george huntington in 1872. it is a devastating degenerative neuropsychiatric disorder [cit] ) that affects 8 out of 10,000 people in caucasian populations [cit] . the polyq part of the brain contains the huntington's gene with 11-34 repeated sections of glutamine -responsible for the production of a cytoplasmics protein called huntington. when the polyq region generates more sections of glutamine, a mutant huntington protein is produced, which is the actual cause of huntington's disease [cit] . this disease is an incurable hyperkinetic motor disorder. the primary symptoms of this disease are jerky and shaky movements called chorea [cit] . so far, no preventive measures have been discovered for this fatal disease."
"for every leaf node, τ : cd_preserve( vτ ) 3. for every leaf node, τ, calculate: markov chain, can be used to estimate the performance of such systems, by associating a transition rate with each allowed transition between states. the system remains working state for an average time t, after which it performs error-related work. errors are discovered at a rate of 1/m. in the markov model, m represents the mean time spent in the working state before an error is found. it takes a time c to test for an error (check), and a time f to recover from (fix) an error. the time f includes any time for reloading data, performing rework, or correcting errors. when multiple recovery trials are necessary, perhaps due to errors during recovery, we incorporate these effects in this model by increasing f . if we use these rates in a markov model of our finite-state machine, and solve for the steady-state probabilities, then we find:"
"taken together and although the compared weka models were developed by adopting the included default parameters (namely without optimization procedures), the comparison described above reveals that the proposed method shows performances comparable to, or only slightly worse than, the best available classification algorithms. moreover, it should be noted that the tree algorithms generated slightly better models by including all available descriptors, while mod. 1 involves only six descriptors, a difference that should avoid overfitting issues rendering mod. 1 more robust and more extensively applicable. again, the proposed approach affords a score-based instead of a simple binary prediction and this means that the better the score, the higher the probability that a given substrate generates rms."
"spatial correlations could be interpreted in terms of es relationships: for example, the a-c negative correlation showed that places with high agricultural production had low carbon storage, and vice-versa, suggesting a tradeoff between these es. in contrast, n and p were positively correlated, in other words, places with high nitrogen retention had also high phosphorus retention, suggesting a synergy between them."
"more homogeneous models should predict which substrates form rms through a specific metabolic reaction either from among all substrates undergoing the same specific reaction or from among all substrates generating first-generation rms. the first type of prediction still involves the recognition of substrates yielding rms, and thus resembles those already developed in the previous sections even though focused on a specific subset of all simulated molecules. in contrast, the second type of prediction appears conceptually different compared to the previous ones, since it predicts the susceptibility of a given molecule to undergo a specific metabolic reaction. hence, the following analyses will be focused on the second prediction type both for its novelty and because its results can be combined with the previous models offering a kind of predictive procedure by which one may first predict which molecules can yield rms and then through which reaction(s) they can be generated."
"while these approaches provide obvious benefits, current applications for classifying medical data are still lacking consistency in terms of revealing hidden significant information, especially from real-time clinical data. the main limitation with the approaches described is that they only consider a small number of classifiers. furthermore, many of them fail to include relevant and important features, such as age and gender that can have a significant impact on results. moreover, overall accuracy depends on a single set of variables while other variables could potentially have more impact on the performance evaluation [cit] ). the approach posited in this paper considers all renowned classification algorithms and uses a large-scale feature set. each variable in the array has its own significant relationship with the progression of specific diseases. moreover, rather than relying on base-level classifiers, a new strategy is described based on the fusion of classifiers. in this way, it is possible to explore any new dimensions that may emerge from the results."
"we selected three bivariate methods for assessing es relationships, commonly used in the literature [cit] : (1) static spatial correlations; (2) spatial correlations of temporal variations; (3) two-dimension production possibility frontiers."
"but it might be irrelevant for assessing how managing the land to increase the provision of one es will affect other es [cit] . there is a need to clarify which types of issues each method can help to resolve, taking into account their range of application and underlying hypothesis [cit] . few studies have done so explicitly (but see: [cit] ."
"furthermore, the reported investigation clearly demonstrates, once again, that licitus is able to configure security services in a consistently less time than the zigbee ip security architecture, while guaranteeing more than the 50% of energy savings."
"invest and the absence of model validation may limit confidence with which we can interpret es relationships [cit] . although most studies on es identify tradeoffs between regulating and provisioning es [cit], our results point out that correlative associations between food production and regulating services should not be automatically identified as interactions, or generalized to other landscapes without caution [cit] )."
"compared to the preservation-restoration methods in prevision section, this ft-gemm has the potential to correct a limited number of errors more efficiently than recomputing the entire result. the full-checksum encoding approach requires both row-and columnchecksums and therefore requires roughly twice as much memory for storing checksums as the fine-grained preservation scheme described above, but does not require preserving a second \"safe\" copy of the input matrix."
"3. the experimental evaluation, the comparison with respect to the zigbee ip security architecture, and the security analysis are discussed in sec. 4 . finally, sec. 5 closes the paper and draws future works."
"regarding cogi, the method provides a small factor of compressibility, better than gzip 2.3% (although not present in the table, gzip in the best option achieve 150,794,589 bytes). nevertheless, cogi is the fastest method. on average, cogi is faster than jarvis 28 times, although jarvis achieved 31% higher compression ratio. cogi is more suitable for industry-orientation purposes."
"as a preliminary analysis, the resilience of licitus against password guessing, replay, and man-in-the-middle (mitm) attacks has been proved through the for handling the key agreement mechanism [cit], has been also provided. note that the considered zigbee ip security configuration has been chosen as a valid benchmark strategy for comparison because it shares many features with the proposed framework and, in particular, the ability to establish a secure network domain in which node pairs negotiate symmetric keys. experimental results"
"zigbee ip (i.e., 9) is higher than those needed by the proposed solution (i.e., 4). in addition, in zigbee ip some messages (i.e., the second, the third, and fourth in figure 4 ) are exchanged with the trust center that may be some hops away from the nodes involved in the kmp. as it will be demonstrated in the following section, this will negatively impact on energy consumptions and latencies. it is worth to note that, the designed kmp protocol requires an higher number of mac messages when the certified dh scheme is used (that are equal to 24 packets as discussed before). this is the cost to pay for offering also the authentication of devices; but, this is an important feature not supported by skke."
"hence, the selected descriptors were systematically combined to generate classification models according to the user-defined number of independent variables. the coefficients of the resulting equations are calculated by applying the hooke-jeeves optimizer for non-continuous functions, the goal being to optimize the ranking position of the substrates yielding rms. moreover, a random sampling algorithm is applied to evade local minima thus better optimizing the resulting classifiers. the performances of each resulting model are evaluated by a purposely defined quality function. besides offering a graphical evaluation, the distribution of substrates which yield reactive metabolites can be used to derive a quantitative parameter based on the asymmetry index (ai), a measure of deviation of the cluster distribution from a normal curve which can be computed based on the pearson's moment coefficient of skewness [cit] . thus, the greater the ai value (namely the more right-skewed the distribution) the better the model. moreover, and to also optimize the early recognition, the quality function optimized in the model generation is calculated as the product of the asymmetry index and the percentage of rm-yielding substrates included in the first (best) cluster the yellow box indicates the input, the green box comprises the initial variable filtering; the blue boxes define the main tasks performed by the algorithm; the red box displays the obtained results. the brown boxes include the computational approaches by which each generated classifier is optimized by maximizing the corresponding quality function."
"the repeat model, also known as a copy expert from the xm compression method [cit], is a model that stores into memory the positions relative to the sequence that has an identical k-mer identified in the past of the sequence. the positions are stored, using a causal processing paradigm, usually in a memory model as a hash-table. the model is used after a k-mer match occurs and is switched off after a certain threshold of performance is reached. figure 3 depicts an example of a repeat model with k-mer size of 8 while figure 2 (at the right side) represents the architecture. the positions of where the subsequence occurred in the past are stored in the hash table. in this example, two positions are identified, namely 14,251 and 14,275. if we used only one position, this would be similar to the green implementation [cit] . however, we use the information at most from rpn models (rpn are the maximum number of repeats models which are shown in figure 2 ). when the rpn is higher than the available number of positions, the number of actual models is bounded by the maximum. these repeat models are called stochastic because, to start a new repeat (after a k-mer match), any position given the same k-mer (in the hash table) has the same probability of being used. if we used the sequential order, the initial positions of the sequence would be more used, given the number of repeats being upper bounded by rpn. therefore, the stochastic nature enables uniform distribution of the repeats to start in different positions along the sequence. another advantage is the absence of indexes to represent the position of the repeat being used under the positions vector. as such, the stochastic nature allows decreasing the memory inherent to the representation of the hash table."
"advances in medicine and healthier lifestyle choices are allowing people to live longer [cit] . however, as this shift continues, so does an increase in age-related neurodegenerative diseases, such as alzheimer's and dementia [cit] . currently, treating neurodegenerative diseases, places considerable pressure on national healthcare systems [cit] . many believe that significant increases will be unsustainable [cit] . the solution is not obvious; however, approaches centred on early detection, and management is likely to yield some interesting results. nonetheless, early detection of neurodegenerative diseases is still a major unresolved and significant area of concern for national healthcare services, globally. neurodegenerative diseases are one of the leading causes of death, even in developed countries. a progressive central nervous system disorder leads towards severe neurodegenerative diseases like alzheimer's, parkinson's, huntington's, and amyotrophic lateral sclerosis (als). due to the insidious onset and gradual progression of pathological changes, it is crucial to divide the evolution of neurodegenerative diseases into different stages in order to detect symptoms earlier."
"this technique is used in ft-scalapack [cit], for dense matrix operations, such as mm, lu and qr factorization. checksum encoding methods provide self-recovery -the (computed) encoded data provides sufficient information for error dection and correction. there is no need for a safe store, therefore no other software support is required. the main drawback of these approaches is that only limited error patterns can be corrected. for example, in huang's mm row/column checksum scheme, only one error per row or column can be corrected. in some cases, the algorithm can detect more errors, but cannot recover. in order to tolerate more errors, more encoded data is needed, which may be costly both in memory and in runtime. a second drawback is that the checksum encoding, detection and recovery methods are specific to particular algorithms. a new ft scheme needs to be designed and proved mathematically for each new operation. the hss structure of a matrix a can be represented as a recursive structure through a telescoping factorization [cit] :"
"in this paper, a complete security framework for the ieee 802. it has been demonstrated that licitus, compared to a benchmark protocol within zigbee ip specifications, always guarantees lower computational efforts and more than 50% of energy saving. future research activities will cover also the study of the licitus feasibility in more capable devices and the analysis of its impact on the quality of service offered to real applications properly conceived for future iot systems."
"several methods have been proposed and used for detection of neurodegenerative diseases. most approaches focus on cognitive decline, biomarkers, and direct analysis of metabolites or genes [cit] . however, in recent years, early detection and neuroimaging techniques [cit], including genetic analysis, are techniques that are commonly used [cit] ) to detect potentially life-threatening diseases like cancer, cystic fibrosis, and neurological diseases [cit] . mini-mental score evaluation (mmse) and symptom's quantification are other well-known techniques commonly used to diagnose neurodegenerative diseases [cit] . nonetheless, the use of computer algorithms and visualisation techniques are considered fundamental to support the early detection process. one example of this is the common spatial patterns (csp) [cit] that has been successfully used to study some of the diseases for example alzheimer's. csp is one of the algorithms which belongs to an adverse class of algorithms known as blind source separation (bss) [cit], incorporates significant properties of class labelling and dimensionality reduction. moreover, this classification algorithm performs signal separation to rank and order the relevant separated components found within the data [cit] ."
"to carry out experimental tests, licitus and the security mechanisms de- details about rom and ram footprint of the considered security solutions have been summarized in table 2 . from reported values it is possible to observe that the proposed approach guarantees the minimum memory demands, thus becoming very suitable for any kind of constrained device. three different network topologies have been considered:"
"x, y respectively represents the horizontal and vertical coordinates of a certain pixel point in the picture. g x (x,y) and g y (x,y) are for representing respective gradient value of this point in the direction of x and y."
"in table 2 we give the bleu scores of our bidirectional models using fine-tuning and mixed-fine tuning. we obtained these bleu scores on the non-blind test set which was provided along with the training data. we did not use this test set for training or tuning. the bleu scores are obtained using sacrebleu [cit] . we can see that while the performance of japanese to english slightly degrades (not statistically significant), english to japanese translation improves by approximately 2 bleu points. as such mft is either comparable to or significantly better than regular fine-tuning and was the reason why we chose it for the final submission."
"the performance of the classifier is determined by the selected features and learning algorithm, so detection effect can be improved only by the zoom images before detection. based on the multi scale images detection, non maxima suppression technique is employed to choose the optimal detection window in detection targets."
the final detection experiment results are shown in fig. 5 . it can be seen from fig. 5 (a) to 5 (c) that the testing system can detect pedestrian target well.
"combining with hog extraction and svm training, the flow chart in this paper is showed in fig. 1 . there are three parts in this flow chart: features extraction, training and detection. figure 1 ."
"stability of our hog and original hog these correlation coefficients are utilized for measuring the stability of two kinds of characteristics. it can be seen in the extraction fig. 2 that most improved hog correlation coefficients are concentrated in 0.55 to 0.65, the original hog correlation coefficients are concentrated in 0.5 to 0.6. the correlation coefficient is larger the similarity among features is greater, and the extracted features are to be able to characterize more information of the pedestrian."
"in this paper we have described our primary japanese↔ [cit] . in general, we found that bi-directional modeling and mixed-fine-tuning (mft) work reasonably well for this task although mft is the main reason behind the improvements. however, these techniques only partially address the problem of training nmt models that are robust to noise. mft is a robust training approach and does not actually deal with different sources of noise. in the future we will consider applying better pre-processing mechanisms, domain adaptation techniques and data augmentation techniques for even more robust translation systems."
"pedestrian detection is not only an important branch of target detection, but also a hot and difficult point of studies in the field of computer vision. its applications can touch upon intelligent transportation, safety monitoring, autonomous driving, human computer interaction and so on [cit] . in view of the nonrigid factors such as the light, posture and color of clothes, pedestrian detection is still a difficult problem."
"b. comparison of the classifier performance the method employed by dalal & triggs and that of this paper are to be compared. in dalal's technique, hard samples are trained repeatedly. this procedure cannot have an effect on the comparison of these two features, so this procedure can be omitted. 7,308 negative samples and 2,416 positive samples are extracted from the 1218 negative image samples in the inria library. the linear svm detectors are obtained by the training. fig. 3 shows the comparison of the classifiers trained by two kinds of samples. it can be seen from the fig. 3 that when the fppw (false positive per window) is comparatively small, the miss rate of this paper is much smaller than that of dalal. lower fppw is usually required in the practical application, and this paper can keep comparatively high detection rate in the situation of low fppw (1 -miss rate)."
"in fig. 4, some examples about detection results are given. it is clear that most pedestrian targets can be detected utilizing the technique in this paper. for example, for the five pedestrian targets in 4(b), the pedestrians in the complex backgrounds where there is a juncture of strong light and soft light are not detected and three miss-detections also occur. however, the pedestrian targets whose postures are changed to a certain extent can be detected effectively."
"because the model had converged sufficiently by 150,000 iterations. we then used this model to perform mixed-fine-tuning (mft) which uses a combination of the out-of-domain and in-domain corpus. mft is done for 50,000 iterations on 1 [cit] words."
"neural machine translation (nmt) [cit] has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (pb-smt) [cit] . nmt performs well in resource-rich scenarios but badly in resource-poor ones [cit] ."
"i is used to stand for a picture and i(x, y) stands for the grey level of the image in the pixel point (x, y). the specific calculating process of the extended hog is showed in the following part."
mixed-fine-tuning is a simple but effective way of performing domain adaptation via fine tuning where one does not have to worry about the possibility of quick over-fitting.
"in this section, the detection system is tested on the inria data set and makes a comparison with the existing technique. it is realized in the vc and opencv developing environments."
"multi-scale detection examples in practical application, the detection speed is also an important measure to an algorithm. the detection speed of dalal algorithm is 1.2 times of that of ours. although the algorithm in this paper is a bit slower than dalal's, it greatly enhanced detection accuracy."
"kindly refer to the task overview paper [cit] for additional details about the task, an analysis of the results and comparisons of all submitted systems which we do not include in this paper."
"a. comparison of the correlation coefficient of hog five hundred images are chosen among the 2416 pedestrian images and the correlation coefficient is calculated using original hog and the improved hog respectively. 124,750 correlation coefficients are obtained by 500 samples in all, as shown in fig. 2 ."
"considering the above problems, multi-scale detection is adopted in this paper. the main idea is to set the detect-waiting images in zoom mode according to certain proportion firstly. moreover, the windows whose overlap ratio exceeds certain proportion in the detection results are regarded as the multi-windows for the same target. then, non-maxima suppression technique is adopted to choose the window whose score is the highest as the position of the target."
"based on the previous studies, further improvement is made in this paper. firstly, in consideration of the fact that dalal'hog cannot extract the body local features in comparatively large image region [cit], this paper extracts more gradient information and thus get more description operators which can indicate more information about pedestrian features."
"svm is a kind of pattern recognition technique based on statistical learning theory [cit] . kernel function is adopted to map the data in the input spaces to a high-dimensional feature space. then, in this high-dimensional space, the generalized optimal classification face is calculated. thus, the linearly inseparable data in the original space can be separated linearly in the high-dimensional space [cit] . the general expression is:"
"feature and learning algorithm are most important in pedestrian detection based on machine learning. we presented refinements leading to a significant improvement on an existing pedestrian technique. the traditional hog can not describe pedestrian body detail in larger image region. considering this point, this paper extends original hog features, but maintains the original computing system. experiments shows the improved features can achieve satisfactory detection accuracy, higher than original hog. but to solve the slower detection speed is an important point of our next work. we will endeavor other techniques (e.g. integral image) to solve this problem."
"at present, there exist many technical methods for pedestrian detection, most of which are based on machine learning. the detection technique possesses two important aspects. the first one is feature descriptor which can represent detection targets. the other one is the chosen learning algorithm. features commonly used include edgelet [cit], harr-like wavelet feature set [cit], local binary pattern [cit], histogram of oriented gradient [cit] and so on. all these features are used to extract information about marginal changes and contour shape. the learning algorithms mainly include cascade connection adaboost and svm [cit] . adaboost is a kind of iterated algorithm. it trains repeatedly on the same train dataset and then combines different weak classifiers according to the weight of the last samples to obtain ultimate strong classifiers. the main advantage of cascade classifier is its excessively high detection speed. svm maps the sample set to the linearly separable higher dimensional space by kernel function. in the higher dimensional space, dot product operation is only needed to get the discriminant results. its main advantage is the robustness if the target mode changes."
"one such resource-poor scenario is the translation of noisy sentences which are often found on social media like reddit, facebook, twitter etc. there are two main problems: (a) the type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) scarcity of training data to capture all noise phenomena. one of the first works on dealing with noisy translation led to the development of the mtnt [cit] test suite for testing mt models that are robust to noisy text. fortunately, the problem of noisy text translation can be treated as a domain adaptation problem and there is an abundant amount of japanese-english text that be leveraged for this purpose. in this paper, we describe the systems for japanese↔ [cit] under the team name \"nict\". in particular our observations can be summarized as follows:"
"therefore, the proposed algorithm issa in this paper is suitable for function optimization, and the established models issa-svm and issa-dml are fit for grade classifications of air quality and the doa estimation, respectively. these give us indications that in a future work, we will propose new or improved swarm intelligence algorithms and apply them to optimize the parameters of machine learning for classifications and estimation in the real world by being combined with other approaches."
"these swarm intelligence algorithms inspired by nature have been applied into various areas. for example, in the reference [cit], sca is improved and applied to optimize the parameters of bp neural network for predicting the direction of stock markets with google trends. in the reference [cit], alo mimics the hunting mechanism of antlions in nature, whose algorithm needed to be performed consist of five main steps of hunting prey such as the random walk of ants, building traps, entrapment of ants in traps, catching preys, and re-building traps, and has been used to perform function optimization, constrained optimization and ship propeller design. in the reference [cit], gwo inspired by grey wolves has been utilize to realize function optimization and the classical engineering design problems."
"the updated approach in issa is performed by reobtaining the squirrel on every tree from the offsprings of the squirrel on every tree, which shows the diversity of population. besides this approach, the remaining of issa is the same as ssa. according to figure 5, although the diversity plot of iwo is decreasing with the iterations, the fluctuation of diversity plots of issa and ssa is similar. but the reproduction of issa updates the population constantly, which makes issa have the longer execution time and the larger computational complexity. in addition, the size of population and the number of iterations have an influence on issa."
"holds, which is achieved by an invariance control law with a fixed parameter γ i . since we concentrate on chattering reduction in this paper, we make the following assumption."
"from table 4, the average function values of issa on functions f 1 (x) − f 3 (x), f 5 (x), f 6 (x), f 10 (x), f 12 (x) − f 14 (x) that are closer to their minimum values than those of ssa, iwo, pso, da and alo, and iwo gives the better optimization results for functions f 4 (x), f 7 (x)−f 9 (x), f 11 (x) than issa, ssa, pso, da, alo. therefore, table 4 shows that issa is superior to ssa, iwo, pso, da and alo. the same result is obtained from figure 7. and from figure 7, it is shown that the average function values of functions f 1 (x) − f 14 (x) by performing issa are all nearest to the minimum values within 100 iterations."
"recent years have witnessed a growth in the sensing and actuation capabilities of control systems. these technological advances enable addressing a wide range of engineering applications, such as the smart grid [cit], biological networks [cit], and automated highways [cit] . these applications commonly rely on efficiently coordinating the decision making of multiple interacting agents, which only have partial information about the internal variables of the overall system. the lack of full information often presents itself as structural constraints on the controllers' parameters and motivates the field of distributed control."
"and da has a better result on function f 27 (x). therefore, table 5 and table 6 show that issa is superior to ssa, iwo, pso, da and alo. from figure 8 and figure 9, the same results are obtained, and the convergence curves of functions f 15 (x) − f 36 (x) by performing issa trend to their minimum values within 100 iterations."
"we proceed with addressing question 2) stated at the end of section 2 about the feasibility of p t,r n−1 . it turns out that the feasibility of p t,r n−1 is closely related to the existence of a quadratic lyapunov function for the closed-loop system, which is separable in the sense defined below."
"in fig. 3a )-c), the trajectories generated by the three control laws almost coincide. the figures show that the boundaries are overall followed by each control law. it can also be observed, that the trajectory starts and ends in the same point in all three cases. this result highlights the stability of the controlled system (proposition 1)."
"in the experiments, the number of iterations and the size of population in alo-dml,da-dml, pso-dml, iwo-dml, ssa-dml and issa-dml are set to be 100 and 50, respectively."
"in the reference [cit], doa estimation is an important research field in array signal processing, and has been widely used in various regions such as radar direction and mobile communication. the doa methods include the conventional beamforming (cbf), multiple signal classification (music), estimating signal parameters via rotational invariance techniques (esprit), maximum likelihood(ml), weighted subspace fitting (wsf) algorithm and their improvements. ml is divided into stomatic maximum likelihood (sml) and dml. in this paper, issa and dml are combined to be a hybrid method issa-dml for doa estimation."
"invasive weed optimization (iwo) is a novel numerical stochastic optimization algorithm inspired from colonizing weeds [cit], which was proposed by a.r. mehrabian and c. [cit] ."
"our contributions are as follows. first, in section 3 we introduce the notion of sparsity invariance to characterize a novel class of convex restrictions that is based on imposing appropriate sparsity patterns on certain matrix factors. as a result, we generalize previous approaches [12, [cit] and we improve their feasibility and performance. moreover, we show that convex restriction approaches based on sparsity invariance cannot be generalized further. second, we provide necessary and sufficient conditions for the feasibility of these convex restrictions, in terms of the existence of a corresponding separable lyapunov function for the closed-loop system (section 3). third, we suggest a computationally tractable procedure to design favourable structures for the lyapunov matrix to achieve high performance in section 4. this procedure highlights that increasing the performance is closely linked to loosening the degree of separability we force on the lyapunov function. we validate our results through numerical examples both in section 3 and section 5."
"in discrete time implementations of invariance control the invariance function is no longer continuously evaluated in time, but only at the sampling instants. as a result, the switch between nominal and corrective control does not happen immediately, resulting in high-frequency oscillation at the boundaries. this causes a deviation from the desired behavior at the constraints and decreases the controller performance."
"element-wise satisfaction of (29) and a non-positive value of each element of z c − z no is sufficient for (32) to hold. therefore, condition (29) is sufficient for stability, if"
"invariance control monitors states and outputs of a system with respect to predefined boundaries. the basic structure of an output tracking problem with invariance control is depicted in fig. 1 . the nominal controller processes the output values and determines a control signal. for the invariance control to be stable, the nominally controlled system without invariance control is assumed to be stable [cit] . the invariance controller checks the system states, whether a boundary is about to be violated. if this is the case, it emits a corrective control output, which ensures the adherence to the boundaries. otherwise, the output of the nominal controller is passed through. therefore, at a distance from the boundaries, the system follows the commands of the nominal controller and whenever it closes in on the constraints, no violation occurs. in this paper, we consider a nonlinear, control affine mimo system of the forṁ"
"every flying squirrel in ssa is only on one tree in the forest and every flying can have its offsprings in nature, which gives a inspire that the reproduction in iwo can be introduced into volume 7, 2019 the flying squirrels in ssa. thus a novel hybrid algorithm based on ssa and iwo is proposed, written as issa. the proposed improvement of issa is based on three cases of generating new locations in ssa, as follows:"
"the fitness values of all flying squirrels are sorted in ascending order, and the tree with the minimal fitness value is declared to be the hickory nut tree. the next three trees with the next three minimal fitness values are considered to be the acorn nuts trees. the remaining trees are the normal trees. thus the corresponding flying squirrels are on the hickory nut tree, the acorn nuts trees and the normal trees."
"holds, then the tracking error is stabilized in the sense of lyapunov. if (3) holds with strict inequality, then the tracking error is asymptotically stabilized in the sense of lyapunov. proof: using (1) and (2), condition (3) transforms into"
"which equal zero right on the constraint and are negative within the admissible set. these output functions y i are used to derive the invariance control law. there is no upper limit for the number of defined constraints. the entire admissible set h of the system is given by the set of those state vectors, for which all output functions are non-positive"
"we refer to the property (8) as sparsity invariance. in order to address question 1) stated at the end of section 2, we give a full characterization of sparsity invariance in the following theorem. its proof is reported in the appendix."
"1. matrix r t is computed according to the two-step procedure (12), (13). 2. the graph g(r t ) has the minimal number of connected components, thus minimizing the degree of separability forced on a lyapunov function for the closed-loop system."
"where fs l and fs u are lower and upper bounds in j th dimension of i th flying squirrel, respectively, and u (0, 1) is a uniformly distributed random number in the range [cit] ."
case 2: fs nt may move towards acorn nut trees to fulfill their daily energy needs. the new location of squirrels can be updated as follows:
"wherer is the estimation of the covariance matrix, tr() represents the trace of the matrix, and a + (θ) is the pseudo-inverse of matrix a(θ). then eq. (24) and eq.(25) are introduced into eq.(23) and the dml of the variable θ is obtained as follows:"
"in this paper, the penalty parameter c in eq. (20) and kernel function parameter γ in eq. (21) are need to be optimized in svm. in issa, the population is created and every individual is composed of two parameters: the penalty parameter c and the kernel function parameter γ of rbf in svm. the function"
"in this paper, we take dataset on the air quality and temperature conditions of taiyuan province, shanxi, china (whose location is depicted in figure 10 ) from according to air quality index (aqi), the condition of air quality evaluation is classified into six grades: excellent, good, light pollution, medium pollution, heavy pollution, and serious pollution, whose corresponding grade of aqi evaluation is signed to be grade 1, grade 2, grade 3, grade 4, grade 5 and grade 6, respectively, shown in table 7."
"in the following sections, the performance of issa is verified by the combination with svm for challenging an actual application for the grade classification of air quality and with the dml algorithm for the doa estimation of mems vector hydrophone, respectively."
"in this section we address the two questions raised above, by providing conditions for p t,r to be a feasible restriction of p k ."
"in this section, we first introduce some notation on sparsity structures, and then present the problem statement of distributed optimal control. we highlight its non-tractability and introduce the class of convex restrictions under investigation."
"in this work, we consider a novel control law for invariance control with chattering reduction. it is analyzed with respect to stability, invariance and applicability with different sets of constraints. we derive a stability condition and additionally, we analyze the controller output with respect to its compliance with the constraints. we show that with linear constraints on the states and/or outputs, the control output renders the system controlled invariant, bounding the states and/or outputs to the admissible set. simulations illustrate the theoretical results. these findings encourage the use of invariance control a variety of control applications. a remaining challenge is the consideration of time-varying constraints."
"issa is proposed by introducing the reproduction of iwo into ssa, which is compared with alo, da, pso, iwo, and ssa in the whole paper. in this paper, issa is utilized to perform on 36 benchmark functions for the minimum optimization and is applied to be combined with svm (issa-svm) for the grade classification of air quality and with dml (issa-dml) for doa estimation of mems vector hydrophone."
"the svm, alo-svm, da-svm, pso-svm, iwo-svm, ssa-svm and issa-svm models are applied to classify the air quality dataset into six classes and are performed ten times independently, respectively. the average accuracy rate of classifications are obtained and shown in table 8. from table 8, it can be seen that the accuracy rate of issa-svm model is up to the maximum value 87.91971%, which illustrates that issa-svm ourperforms the svm, alo-svm, da-svm, pso-svm,iwo-svm, and ssa-svm models. we also observe that the order of models is issa-svm, iwo-svm, pso-svm, ssa-svm, da-svm, svm and alo-svm models according to the decreasing accuracy rates of these seven models. table 9 is obtained by performing these seven models: svm, alo-svm, da-svm, pso-svm, iwo-svm, ssa-svm and issa-svm only once. from table 9, the number of correctly classified tested samples and accuracy rate of issa-svm model reach the maximum values,484 and 88.3212%, respectively. figure 13 shows the classification results of these seven models. it is shown that issa-svm model is superior to the other models: svm, alo-svm, da-svm, pso-svm, iwo-svm, and ssa-svm for the grade classifications of air quality."
"this alteration makes the probability of dropping a seed in a distant area decrease nonlinearly at each time step, which results in obtaining the new population superior to the pre-population according to the fitness values. thus r-selection mechanism is transformed into k-selection mechanism."
"in order to fulfill both conditions c1 and c2, the minimal value from (24) and (25) is used for corrective control. the final law for the corrective control input is given by"
"where p is the number of signal sources, n mc is the number of independent experiments, θ(k) is the actual angle of the kth signal source andθ i (k) is the estimated angle of the kth signal source in the ith experiments. we take snrs of the gaussian white noise 0db, −5db and 5db, respectively. then the average incident angles of doa estimation and rmse are obtained, shown in table 10, table 11 and table 12 ."
"many problems in the real world can be attributed to optimization problems. with the complexity of problems increasing, it is obvious that the need for optimization techniques becomes more and more. initially, mathematical optimization techniques used to be the only tools for optimizing problems. and then heuristic optimization techniques appear."
"with z c1,i and z c2,i from (24) and (25), respectively. as an example, the corrective pseudo input for a relative degree 2 system is given by"
"discussed here is regarded as the fitness function of issa, where n is the number of training samples,y j i and y j i are the actual output value and the predicted output value of the i th input sample, respectively, d is the dimension number of the output. therefore, issa is utilized to optimize the penalty parameter c and kernel function parameter γ . thus the optimal c and γ of svm are obtained. then svm is used to perform the grade classification of aqi. therefore, based on issa and svm, the hybrid model is established, named by issa-svm."
"based on the above parameters, the average function values and the corresponding average standard derivations (std) are listed in table 4, table 5 and table 6 . the convergence curves of the average function values on these 36 benchmark functions are shown in figure 7, figure 8 and figure 9."
"we also choose that alo, da, pso, iwo and ssa are combined with dml to become the models alo-dml, da-dml, pso-dml, iwo-dml and ssa-dml for being compared with issa-dml. in the platform of matlab, we use alo-dml, da-dml, pso-dml, iwo-dml, ssa-dml and issa-dml for performing the doa estimation."
"in this section, we present an illustrative example to validate our results on improving the performance with respect to previous approaches. all instances of problem p t,r n−1 were solved using sedumi [cit] and yalmip [cit], on a computer equipped with a 16gb ram and a 4.2 ghz quadcore intel i7 processor."
"we run these six models alo-dml,da-dml, pso-dml, iwo-dml, ssa-dml and issa-dml 30 times independently. we take the root mean square error (rmse) as the indicator of evaluating a model, which is defined as follows [cit] :"
"the variable z i is called the pseudo input of the linearized system. if the output functions are chosen such that the systems is not completely linearized, the remaining zerodynamics are assumed to be stable for the controlled system to be stable."
"in figure 14, the uniform linear array l consists of m array elements and the distant between two adjacent array element is d. we consider p narrowband far-field signals from distinct directions [θ 1, θ 2, · · ·, θ p ] impinging on the m −element uniform linear array. we suppose that the narrow-band far-field signal is incident by plane wave mode."
"in this paper, we present a novel invariance control approach for nonlinear, control affine mimo systems, which addresses the problem of chattering reduction for an output tracking problem. based on tools from lyapunov theory, we provide a condition for stability of the invariance controlled system with chattering reduction. furthermore, we investigate admissible configurations for the set of constraints. the results presented here generalize and improve our earlier work [cit] by allowing systems of arbitrary relative degree. the efficacy of the proposed control approach is demonstrated in simulations on a robotic application example."
case 3: fs nt which already consumed acorn nuts may move towards hickory nut tree in order to store hickory nuts in case food scarcity. the new location of squirrels can be updated as follows:
"in this paper, the reproduction of weed in iwo is introduced into the reproduction of ssa, thus the hybrid algorithm issa is proposed. firstly, issa is used to perform function optimization on 36 benchmark functions and is compared with alo, da, pso, iwo, ssa for evaluating the function performances. then issa is utilized to optimize the penalty parameter c and the kernel function parameter γ of support vector machine (svm), thus a new model issa-svm is built. further, issa-svm is used to classify the grade of air quality and is compared with the other six models: svm, alo-svm, da-svm, pso-svm, iwo-svm and ssa-svm. finally, issa is combined with the deterministic maximum-likelihood (dml) algorithm, thus the new model issa-dml is established. issa-dml model is applied to perform the direction of arrival (doa) estimations of the simulate signals with two kinds of incident angles by comparison with alo-dml, da-dml, pso-dml, iwo-dml and ssa-dml models."
the distribution of 1829 data according to the grade of air quality is shown in figure 11 and the box visualization of aqi dataset is shown in figure 12.
"model predictive control (mpc) [cit] or receding horizon control [cit] are alternative, optimization-based approaches, which may, in addition to state and output constraints, also consider input constraints. they are, however, computationally expensive, which renders the application to highdimensional nonlinear systems with real-time requirements difficult. additionally, the behavior within the admissible set is difficult to predict. as another alternative, the reference governor approach [cit] explicitly considers disturbances of the system. however, determining the so-called \"safe\" sets of the state space requires extensive numerical simulation. the use of barrier lyapunov functions allows asymptotical tracking of a reference while bounding all signals, but considers only a single system output [cit] . invariance control, in contrast, is capable of handling multiple outputs and utilizes analytic functions to define admissible configurations."
in order to illustrate the findings from the previous sections a simulation is carried out in matlab/simulink. we compare the proposed novel method to the standard invariance controller [cit] and the chattering reduction method presented in our earlier work [cit] for different sets of boundaries.
"with the aim of improving feasibility and performance of approaches based on computing a blockdiagonal lyapunov function for the closed-loop system [12, [cit], we characterized a generalized class of feasible convex restrictions of the static optimal distributed control problem based on the concept of separable lyapunov functions. we validated our main results through numerical examples."
"there are many improvements on ssa to be needed, such as initialization, the updated locations, seasonal monitoring condition and random relocation at the end of winter season. in addition, more than one swarm intelligence algorithm is employed to be combined with ssa to establish the new hybrid algorithm."
"while [cit] assumes the relative degree to equal 2, we consider an arbitrary relative degree. we use (9) and (10) to determine the invariance function in the (k + 1)-th time step. as an example, for a system with relative degree 2, it evaluates to"
"the remaining of the paper is organized as follows. the basic ssa and iwo are described in section ii, and section iii shows a novel hybrid algorithm issa based on ssa and iwo. the proposed algorithm issa is used to solve 36 benchmark functions in section iv. section v and section vi show that issa is combined with svm for grade classifications of air quality and with dml for the doa estimation, respectively. section vii gives the analysis of results and discussion. the conclusion is in section viii."
"as shown in figure 1, lift-to-drag ratio or glide ratio of a flying squirrel gliding at steady speed is defined as follows [cit] : the lift (l) results from downward deflection of air passing over the wings, defined as:"
"then two models issa-svm and issa-dml are built for performing the grade classifications of air quality by combining issa with svm and estimating the angles of doa on simulation experiments by combining issa with dml, respectively."
"is the moore-penrose pseudo inverse of a k [cit] . however, due to the finite sampling time in real systems, a system, which is controlled using this approach, shows distinct chattering effects at the boundaries."
"assumption 2: the admissible set is non-empty. extending the admissible set, the invariant set takes the system dynamics into account to determine the set to which the states need to be confined. an invariance function φ i (x, γ i ) of each constraint determines this set. the invariant set of the entire system is the set of state vectors, for which each invariance function takes a non-positive value"
"let d g be a random gliding distance, r 1, r 2 and r 3 be the random numbers belong to the range [cit], fs ht be the location of flying squirrel that reached hickory nut tree, fs at be the flying squirrel squirrels on the acorn nut trees moving towards hickory nut tree, fs nt be the flying squirrels on normal trees moving towards acorn nut trees to fulfill their daily energy needs and t denotes the current iteration. and g c is the gliding constant in order to keep the balance between exploration and exploitation."
"is the fitness function value of the i th flying squirrel, which depicts the quality of food source searched by the i th flying squirrel i.e. optimal food source (hickory tree),normal food source (acorn tree) and no food source (flying squirrel is on normal tree)."
"in some applications, limits on system states and outputs are desired, for example to impose certain performance specifications, safety margins or to guarantee (practical) stability. one application domain is human-machine interaction with examples in driver assistance, rehabilitation, physical training and assistive robotics in domestic and industrial settings, cf. [cit] . this requires a control scheme, which is applicable to nonlinear systems, has real-time capabilities and influences the system behavior only to the extent necessary to ensure adherence to limits. invariance control is such a control scheme. it provides a straightforward approach for imposing constraints on states and outputs of a nonlinear system. the control mechanism switches between nominal control, whenever constraints are satisfied, and corrective control, when constraint violation is likely. since a switch to corrective control only occurs on absolute necessity, the system is mostly under nominal control for the desired control task execution."
"is used for determining the corrective control input. resulting from (8) and (12), the system is positively invariant with respect to the invariant set (7) of an active constraint i, if"
"proposition 3: consider the invariance controlled, nonlinear, control affine system (1) with the nominal control u no and let assumptions 1-6 hold. further assume that the set of active constraints k (28) consists of p linear constraints (34), for which assumption 7 holds. then, the corrective control (17)"
"the remainder of this paper is organized as follows: section ii gives the necessary background of the control strategy. the novel control scheme is introduced in section iii and a sufficient condition for stability is derived. in section iv, possible restrictions on boundary definitions are analyzed. the results of a numerical example are presented in section v. conclusions are drawn in section vi."
"the squirrel search algorithm (ssa) [cit] was a novel nature-inspired algorithm for optimization, where there are four simplified assumptions in the search process of flying squirrels:"
"in fig. 3d )-f), we observe the differences between the control schemes. the novel control law almost eliminates the chattering effect and the trajectory follows the boundaries reference trajectory y des standard approach [cit] y chattering reduction [cit] almost exactly. it also shows no violation of the constraints, emphasizing that the system is made controlled invariant (proposition 2). the standard control scheme and the chattering reduction method from our earlier work, show chattering effects, even despite of the reduction and do slightly violate the constraints. while the standard invariance controller ignores the discrete time implementation of the controller, in our previous work, we use euler's method to approximate the invariance function in the following time step. here, we explicitly consider the effects of the discrete implementation of the controller in combination with a continuous system, which yields a more accurate solution and achieves a better controller performance."
"a constraint is active and requires a corrective control action if its invariance function φ i (x, γ i ) has a positive value. if no constraints are active, nominal control is applied. therefore, only the set of active constraints"
the system (1) is input-output linearizable with stable zero-dynamics. the corresponding invariance function to each output function depends on the output function and the respective relative degree r i [cit]
"since then, many swarm intelligence algorithms have sprung up. especially, seyedali mirjalili himself or he and his co-authors have proposed many swarm intelligence algorithm and applied them to solve different problems, such as ant lion operator(alo) [cit], sine cosine algorithm (sca) [cit], moth-flame optimization (mfo) algorithm [cit], whale optimization algorithm (woa) [cit], dragonfly algorithm (da) [cit], grey wolf optimizer (gwo) [cit], multi-objective ant lion optimizer (moalo) [cit], multiverse optimizer (mvo) [cit] . in addition, there are more swarm intelligence algorithms proposed, such as the artificial tree (at) algorithm [cit], artificial bee colony (abc) algorithm [cit], fruit fly optimization algorithm (foa) [cit], bat algorithm (ba) [cit], invasive weed optimization (iwo) [cit], and squirrel search algorithm (ssa) [cit] ."
"the stability of the controlled system does not guarantee adherence to the constraints. it remains to show that the novel control law (17) with (26) also renders the system controlled positive invariant. first, we consider the single constraints."
"the simulation is carried out three times for each control law, once for every set of constraints. the results are shown in fig. 3. fig. 3a )-c) depicts the complete trajectory in x 1 -and x 2 -coordinates, the corresponding reference trajectory and the boundaries. in fig. 3d)-f), the behavior of the system in the right upper corner of the admissible set is shown in more detail. in order to illustrate, that the configuration of the boundaries does not impair the invariance as long as the active constraints are linearly independent, the constraints are chosen such that three significantly different angles occur in the upper right corner."
"the hypotheses in ssa are that there is only one squirrel on every tree. in this paper, every squirrel is regarded to have its offsprings. the number of offsprings of every flying squirrel on the hickory nut tree, or the acorn nut trees, or the normal trees according to figure 3 of iwo is determined, respectively. then the population of the flying squirrels is redetermined by the way of the ascending fitness values, shown in figure 4, which makes the population diversity. the flying squirrels are redistributed on the hickory nut tree, or the accord nut trees, or the normal trees. thus the flying squirrel on the hickory nut tree is kept to be the optimal in every iteration."
"in this paper, we simulate array signal processing of mems vector hydrophone, and utilize it to perform the doa estimation. here, we take the uniform linear array signal model l into account, whose structure is shown in figure 14 ."
"in the experiment process, tsds only reserves those features whose weights are greater than δ. table 2 shows the comparison results of different thresholds δ in the accuracy. the experiments indicate that each classifier can gain the highest average classification accuracy when the threshold δ is set as the mean value. thus, the mean value is adopted as δ."
the feature selection process is described as follow: for process is related to two essential problems: how many features can be obtained and which features can be gained.
"in order to obtain the closed form of the pc with (7), we perform the fourier transform (ft) on s rb (t, t m ) in (4) and s ref (t) in (7) alongt axis"
"the main drawback of the approach based on information entropy is that biases towards the feature with more values. to making up the bias of information entropy, the symmetrical uncertainty is employed to estimate the degree of association between features and class labels [cit] ."
"a procedure for contrast estimation assumes that the expected differences between performance of different algorithms are the same across datasets [cit] . in this paper, for every pair of eight algorithms in experiment, the formula which is utilized to calculate the difference between the performances of the two algorithms in each of the seven datasets is given as"
"it is easy to prove that v a exhibits a decreasing trend along v r . since v r is unknown, we assume the maximum value of v r as v r,max, which can be determined according to practical applications. then, we have the searching interval volume 7, 2019 v step, which can be expressed as"
"feature selection is one of the data optimization techniques in machine learning and data mining. if the dataset can be compressed effectively by utilizing feature selection, we can obtain potential valuable information, and further improve the performance of the classifier models. in this paper, a novel feature selection ensemble learning algorithm is proposed based on evidence theory. the key contribution of tsds is to improve correlation criterion and ensemble fusion strategy. the improved symmetrical uncertainty based on tsallis entropy and forward sequential approximate markov blanket help us to gain more relevant informative features in the original feature space. ensemble fusion strategy makes an algorithm better to obtain optimal global feature selection approximately. a study of all experiments demonstrates that tsds can obtain relative optimal feature subset to construct a classifier, which can gain a higher classified accuracy than other comparing feature selection algorithms. to select the threshold to control the number of selected feature effectively and automatically and to enhance fusion efficiency will be the focus of further research."
"for the wideband radar echo model, if the matched filter for the narrowband echo model (mfnbem) is still used, a serious pulse compression (pc) distortion (pcd) will happen. the pcd is caused by the mismatching between the mfnbem and the wideband radar echo model. the pcd mainly includes the peak energy loss and the shift of the peak location. in summary, both the pcd and aru are two key problems for hypersonic vehicle detection."
"feature selection focus on obtaining the approximate optimal feature subset which can preserve the discrimination ability of the original data. the pseudo-code of our proposed algorithm of feature selection with ensemble learning is elaborated in algorithm1, which is divided into two major parts. one is to select the feature subset. each execution of the body of the feature selection is an iteration. it adjusts the parameter q of tsallis entropy automatically in each iteration. because each iteration is independent, it guarantees that each feature subset is diversity (the detailed is shown in algorithm2). the other is to fuse f evidence which obtained from the previous task (the detailed is shown in algorithm3). we can fuse each evidence according to the equation (16) after the weight of each evidence has been calculated. furthermore, normalization of feature weight can ensure computational efficiency. f .weight ← weight su"
"motivated by the above analyses, considering the pcd and aru effects of the hypersonic vehicle, we firstly mathematically analyze the wideband radar echo model and obtain the mathematic relationship among the scale effect, speed, and time-bandwidth product. thereafter, based on this mathematic relationship, we construct a generalized matched filter (gmf) and propose a long-time coherent integration algorithm for the hypersonic vehicle detection. icompared to the conventional detection algorithms, this proposed algorithm can eliminate the pcd and achieve a better anti-noise performance with a lower computational cost. through numerical simulations and mathematical analyses, we verify the effectiveness of the proposed algorithm. the aforementioned mathematic relationship can work as a criterion to avoid the pcd. basing on this criterion, the wideband radar echo model for hypersonic vehicle is translated into conventional narrowband radar echo model and the hypersonic vehicle detection algorithm can be studied more widely."
"therefore, we can make the conclusion that, the proposed algorithm can obtain nearly the same anti-noise performance as wsrft with much lower computational complexity. compared with the ckt, the proposed algorithm can obtain a better anti-noise performance with a little more computational cost. this is to say, the proposed algorithm achieve a good balance between anti-noise performance and computational cost, and is appropriate for hypersonic vehicle detection."
"for simplicity, the noise is ignored in these formulas. with (14), the seipd has been eliminated, however, the aru still occur. the item −2v r t m c in (18) denotes that the trajectory of a hypersonic target can span over multiple range cells, i.e., the aru. to focus the energy and estimate motion parameters, the aru should be corrected."
"high speed of the hypersonic vehicle can cause the seipd and aru, which will deteriorate the detection performance and motion parameter estimation. in this paper, we first deduced the mathematic relationship among the scale effect, speed and time-bandwidth product for wideband radar echo model. thereafter, basing on this mathematic relationship, we propose a coherent long-time integration algorithm for the hypersonic vehicle detection. compared to the full parameter space searching algorithm, this proposed algorithm can obtain nearly the same anti-noise performance with much lower computational complexity. we verify the effectiveness volume 7, 2019 of the proposed algorithm by mathematical analysis and synthetic simulations. above all, we should point out that the aforementioned mathematic relationship provides a theoretical criterion for the design of the wideband matched filter. thus, we can surmount the seipd and transform the wideband radar echo model to narrow band radar echo model. based on this, the detection or imaging for the hypersonic vehicles can be studied widely."
"the filter approaches mentioned above do not take into account sample diversity in sampling the training set. they obtain only one feature subset in the feature selection process. therefore, our work differs from the above-mentioned approaches in the evaluation criterion for feature selection and the ensemble fusion strategy based on dempster-shafer evidence theory aspects."
"as seen from figure 6.(a), the proposed algorithm can achieve about 3db higher output snrs than the ckt when the input snr is greater than -50db, and its input-output snr curve is almost overlaps with the ideal mtd curve when input snr is higher than -52db. it's easy to figure out that the 3db snr loss of the ckt is mainly caused by the pcd (as aforementioned, about 0.26 db is caused by straddle loss). the input-output snrs of the proposed algorithm are nearly the same as the wsrft, because both of these two algorithms can eliminate the pcd and the aru, and coherently integrate the energy of the pulses. the mtd cannot handle the aru, therefore, it has a very bad energy focus ability for hypersonic vehicles."
"in order to avoid the heavy computational cost caused by the full searching in parameters space, a criterion to determine the step for radial velocity searching should be studied to eliminate the pcd. based on the criterion, we can avoid the heavy computational burden. thereafter, the existing coherent long-time integration algorithm can be exploited to realize hypersonic vehicle detection."
"as we all know, the mtd is widely used in radar signal processing, for the reason that it can be fast implemented with fft, and obtain good performances in low speed scenes., and x denotes rounds x to the nearest integers towards infinity. for this proposed algorithm, its main implementation procedures include the searching for the parameter v a, the searching procedure of the unknown doppler ambiguity integer and the fft-based chirp-z transformation. that is to say, except for the searching for v a, the remain procedures of this proposed algorithm are the same as ckt. therefore, the computational complexity is"
"over the last few years, many researchers pay more attention to ensemble learning in classification tasks, which combines the consequence of multiple base classifiers. similarly, the principle of ensemble learning can also be utilized for feature selection. it can effectively incorporate ensemble learning into feature selection. there are two different integration strategies, as shown in fig.2 . one is to employ ensemble learning for feature selection (elfs) [cit] . it obtains an approximate optimal feature subset by combining multiple feature subsets based on the nature of ensemble learning [cit] . the other one is to utilize feature selection for ensemble learning (fsel) [cit] . it utilizes different feature subsets to construct an ensemble of a diverse-based classifier [cit] . at present, the output of the feature selector is partitioned into two general types: feature weighting, feature subset. for the former one, a weight is assigned to each feature after feature selection process. and then, the average weight of each feature is calculated in all base feature selectors. for the latter one, the cumulative number of each feature appeared in the output of all base selectors is listed in descending order, just as maximum majority voting."
"in sections ii and iii, by analyzing the wideband echo model for hypersonic vehicles and the gmf, a coherent long-time integration algorithm is proposed for hypersonic vehicles. theoretical analyses demonstrated that this proposed algorithm can handle radar echoes with seipd and aru. in this section, the computational cost and the anti-noise performance, which play important roles in hypersonic vehicle detection, will be analyzed. to illustrate the characteristics of this algorithm more intuitively, several algorithms including. moving target detection algorithm (mtd) [cit], wsrft and ckt, are chosen as references."
"to facilitate experimental comparison, the number of features of other approaches is uniformly set to that obtained by tsds. some experiments have been conducted to verify the tsds algorithm. the detail of experiments in different algorithms are shown in table 3 -5 (boldface represents the highest accuracy for each dataset in the classification algorithm). the details illustrate that tsds can obtain the highest accuracy for 3,4,3 datasets in cart, svm, bayes, respectively. fig.5 presents tsds can achieve the highest average classification accuracy among the former ones in the cart, svm, and bayes, respectively. the performance analysis of a new method is a crucial task to carry out in research. furthermore, contrast estimation and friedman test are used to estimate the performance of the given algorithms. contrast estimation based on medians can be employed to evaluate the performance difference between two algorithms. friedman test is a multiple comparison test approach which is employed to detect the significant differences between two or more algorithms. thus, contrast estimation is employed to estimate the differences between tsds and the former ones. the results in table 6 -8 demonstrate that tsds can outperform the former ones in terms of classification performance. then, in the following experiment, friedman test is used to estimate tsds and the latter ones. in table 9, the statistical analysis results illustrate that tsds would be more effective than qis and handi. the detailed experimental comparisons are shown in fig. 6-8 . meanwhile, from fig. 6-8, it is obvious that tsds can obtain better overall performance than the latter ones."
"to prove the effectiveness of feature selection, the experiments have been performed on the seven uci datasets to compare the performance of classifiers constructed by dataset with tsds, disr, cmim, jmi, mrmr, spfs-lar, qis, and handi. need to pay attention to it, neither qis nor handi can process the connect dataset with running out of memory. therefore, these algorithms are divided into two parts: 1) disr, cmim, jmi, mrmr, and spfs-lar. 2) qis, and handi. for each dataset, the 10-fold crossvalidation is utilized to estimate the classification accuracy. meanwhile, to make the comparative analysis between different algorithms more balanced, this process is repeated ten times for each dataset. furthermore, to evaluate the performance of different feature selection algorithms in an experiment, we introduce three metrics including classification accuracy, contrast estimation, and friedman test."
"due to the particularity of the hypersonic vehicles, we cannot obtain the raw data currently. in this section, the experiments with the synthetic data are performed to validate the effectiveness of this proposed algorithm. in this experiment, the radar parameters are the same as those in the example 2. two targets are considered, and their motion parameters are listed in table 4 . the maximum radial velocity v r,max is set as 20mach and v step ≈ 1666.52m/s. the synthetic data is contaminated by complex white gaussian noises. figure 7 shows the simulation results. the integration result for t2 via the proposed method is shown in figure 7.(a) as a comparison, the ckt is also applied to process the synthetic data, the energy focused results are shown in figures 7.(c) and (d). the two peaks are located at the range gate 576 and 1063, respectively, and their radial velocities are estimated as -2206.05 m/s and 5772.45m/s, correspondingly. apparently, the ckt cannot obtain the correct range of the targets."
"the remainder of this paper is organized as follows. in section ii, the signal model for hypersonic vehicle with scale effect and intra-pulse doppler is built. the proposed hypersonic target detection algorithm is illustrated in section iii. the analyses of the computational cost and the anti-noise performance are given in section iv. the experiments with the synthetic data are performed in section v to validate the effectiveness of the proposed algorithm. section vi gives the conclusion."
"the following experiments are conducted to evaluate the anti-noise performance and target detection ability of this proposed algorithm. the point target t1 is used in the following experiments. radar parameters are the same as which in table 2. the proposed algorithm is designed for the hypersonic vehicle in the air and we only consider the additive stationary zero-mean complex white gaussian noise in this paper. complex additive white gaussian noise are added to the echoes, and the snrs are [-67:1:-45] db, 200 times monte carlo trials are done for each snr value in figure 6.(a) and (b) ."
"in this section, we make a comparison of the proposed method with the other existing approaches. three different classification algorithms are employed to evaluate the performance of all feature selection methods. the classifiers include support vector machine (svm), decision tree (cart), and bayes. nine datasets are used in the experimental analysis. these datasets are divided into two classes: seven standard datasets from the university of california irvine (uci) machine learning repository and two gene expression datasets with high dimension and minuscule sample from arizona state university (asu), as shown in table 1 . the goodness of given approaches cannot be only measured in terms of the improvement for the average classification accuracy. therefore, we utilize the friedman test [cit] and contrast estimation [cit] to evaluate the significant differences between different algorithms. all algorithms are executed in python and run in the hardware environment with core i7-7500 2.7ghz and 32.0gb ram."
"evidence support degree indicates the support degree of an evidence which is supported by other evidence. the higher similarity with other evidence, the higher support degree it is, and vice versa. according to matrix conflict, the following formula is utilized to calculate the similarity degree between m i and m j ."
"according to the radar principle [cit], the received signal s r t, t m during a coherent processing interval (cpi) without any assumptions can be written as"
"a candidate feature subset is constructed from the original feature space according to feature searching strategy in the subset generation process, whose efficiency is evaluated and compared with the previous one according to the evaluation criterion. subset generation and subset evaluation are repeated until a given stopping criterion is satisfied. fig.1 shows that the evaluation criterion is one of the key factors in feature selection. according to the evaluation criterion, feature selection algorithms are divided into wrapper, embedded, and filter methods. wrapper feature selection approaches utilize a predefined classification model to evaluate the selected feature subset. embedded feature selection models focus on embedding the feature selection process into the classifier construction [cit] . a general drawback of these schemes is high computational complexity in wrapper models and embedded models. besides, the selected feature subset obtained by these models is associated with the learning algorithm as well. compared with these two methods, filter feature selection algorithms analyze the characteristics of data and evaluate features independently of any specific classifier. features are ranked based on given criteria, and then the features with the highest ranking are employed to construct classification models [cit] . in filter models, the evaluation functions are of four broad categories: consistency [cit], distance criterion [cit], dependency criterion [cit], and information metrics [cit] ."
"for information entropy, the larger the uncertainty, the smaller the weight it is. on the other hand, the smaller the information entropy, the larger the weight it is. the method mentioned above can be used to reduce the weight ratio of the evidence with higher indeterminacy in the fusion process. therefore, the weight of each evidence is defined as"
"machine learning aims to acquire knowledge from data. in practical application, the data increases both in scales of samples and features. these large-scale data may include hundreds even thousands of features, which result in the curse of dimensionality. because of containing a mass of redundant attributes, the performance of classification has a great impact on machine learning. therefore, elimination of insignificant information is increasingly being recognized as a key element in extracting potential useful information. feature selection or attribute reduction is an important data optimization technique for dimensionality reduction, which focuses on eliminating irrelevant and redundant attributes from a dataset. the main purpose of feature selection is not only to find a suitable subset from original feature space, but also to retain high classification precision and preserve original meaning of those features after reduction. a typical feature selection process (called selector) can be divided into three steps: subset generation, subset evaluation, and stopping criterion [cit], as shown in fig.1 ."
"feature selection has attracted a lot of attention in data preprocessing, for example, data optimization in machine learning, pattern recognition, and so on. in general, prior works on feature selection can mainly be categorized into three classes: wrapper, embedded, and filter methods."
"according to markov blanket, it is easy to obtain the redundant feature in the feature space. however, in case of high dimension and minuscule sample, the cardinality of the markov blanket gives rise to overfitting [cit] ."
"to retain the nature of the original data, we propose a novel feature selection ensemble learning algorithm based on tsds. the process of tsds is divided into two parts: 1) single feature selection model. 2) evidence fusion model, as shown in fig. 3 . in this section, we describe these two parts in detail."
"obviously, (16) has the same form with the pc result of narrowband radar echo [cit], that is to say, we have transform the wideband radar echo model into narrowband echo model."
"suppose that the feature subsets generated in the previous chapter are independent, tsds allows the fusion of information coming from different feature subsets. therefore, the evidence combination rule is utilized to combine different weighted feature subsets in a manner that is both accurate and robustness."
"where a 3 is the amplitude. here, the item −2v r t m c in (17) has been eliminated, which implies that the aru has been calibrated in (23"
"example 2: in this example, we consider the mono-target t1. its motion parameters are listed in table 1. radar parameters are listed in table 2. figure 4 shows the simulation results. it is necessary to point out that the velocity of t1 is set as 5908.95m/s to enlarge the deviation between the target velocity and v a . (21), the linear coupling between t m and fˆt has been eliminated, therefore, the energy distributed as a horizontal line in figure 4.(c). in this case, the aru has been eliminated, and all the echoes are calibrated to the same range gate, as shown in figure 4.(d) . at last, the ft along t m axis is taken to realize energy accumulation, and the focused energy peak can be seen in figure 4.(e) ."
"where a 1, s t t, t m,t, k 0, τ and t m denote the backscattering coefficient, transmitted signal, fast time, scaled factor, time delay and slow time, respectively."
"conventional feature selection algorithms execute sampling only once, which cannot maintain the diversity of samples. on the contrary, from the above descriptions, with multiple ensemble learning based on random sampling, tsds can maintain the diversity of samples effectively. after feature subsets generated, it utilizes the ensemble fusion strategy based on dempster-shafer evidence theory to ensemble all feature subsets. the above experiments prove that tsds can enhance the classification performance effectively in low-dimensional datasets."
where a am is the amplitude. it should be noted that the pc and velocity ambiguity compensation can be done at the same time in order to avoid unnecessary computational burden in real applications.
"the basic idea is as follows: 1) we utilize the modified symmetrical uncertainty and the forward searching approximate markov blanket to measure the distinguishing ability of each feature with class label based on tsallis entropy, which can effectively eliminate irrelevant and redundant feature to preserve optimal feature subset approximately. 2) we employ fsel to gain the feature subset from different aspects."
"in what follows, the computational complexity of mtd, the wsrft, the ckt and the proposed algorithm are analyzed. assume that n t m, nˆt, m am and n v denote the numbers of coherent processing pulses, range gates, searching times for doppler ambiguity integer and velocity searching times in wsrft, respectively."
"basing on (10), we obtain, if (10) holds, the pcd caused by the scaled effect can be ignored. that is, (10) can determine the interval of v a as"
"3) by combining the credibility degree and tsallis information entropy, the dempster combination rule is used to realize information fusion to gain the final feature subset."
"radar high-speed target detection has been receiving a growing attention and significant research efforts in the modern radar field due to its importance for target imaging and exploration of space resources [cit] . the high-speed target, especially the hypersonic vehicle, shows characteristics of far-range, low-observable and strong noise. therefore, the long-time coherent integration is necessary to improve the output signal-to-noise ratio (snr). unfortunately, the longtime integration and high radial velocity make the across range unit (aru) happen."
"to illustrate the scalability of tsds, another series of experiments are performed. two datasets, colon and leukemia from different application domains are employed in this part. in the experiments, these two datasets contain thousands of features, and many of them could be highly correlated with others, as shown in table 1 . similarly, we employ the 10-fold cross-validation to evaluate the classification accuracy in all datasets, and the process is repeated ten times for each dataset to ensure the comparability. according to the intension of feature selection approaches, approaches for comparison are divided into two categories: 1) disr, cmim, jmi, mrmr. 2)spfs-lar, qis, and handi. then, the tsds algorithm will compare with these two categories of feature selection algorithms in detail, respectively."
"from the simulation results, we can conclude that the proposed algorithm can overcome the seipd and aru effect, and achieve a good coherent integration result."
"to validate the effectiveness of tsds algorithm, we compare our algorithm with the following seven feature selection approaches: 1) disr [cit] : it relies on a measure of variable complementarity to evaluate the additional information. disr criterion combines two properties of feature selection. one is feature complementarity, which means that a combination of feature can obtain more information than the sum returned by each feature individually. the other is the computation of a lower-bound on the information of a feature subset expressed as the average of information of all its subaggregate. 2) cmim [cit] : it mainly utilizes maximization conditional mutual information. the feature that can obtain additional information about the predicted class is selected. in other words, in the process of cmim, it does not select a feature similar to each one which has been picked to the selected feature subset. cmim takes the tradeoff between independence and discrimination into account. 3) jmi [cit] : it is the model-independent approach for feature selection based on joint mutual information. it is found to be better in eliminating redundancy than simple mutual information. 4) mrmr [cit] : it obtains feature subset by utilizing minimal redundancy and maximal relevance measure criterion. this scheme avoids the difficult multivariate density estimation in maximizing dependency. meanwhile, mrmr can be combined with other evaluation criterion such as wrapper to obtain a very compact feature subset at a lower cost. 5) spfs-lar [cit] : spfs-lar utilizes similarity preserving feature selection framework to preserve feature. the regularized sparse multiple-output regression formulation is used in this framework to enhance its effectiveness. the advantage of spfs-lar is that it does not require parameter tuning in the feature selection process. 6) qis [cit] : qis pay attention to the distinguish ability of each feature which can be used to distinguish a given sample with other samples. the maximumnearest-neighbor is employed to discriminate the nearest neighbors of samples. to address the problem of neighborhood parameter selection, the margin of the sample is utilized to set the neighborhood parameter value. 7) handi [cit] : when adding a new feature to the current feature subset, handi utilizes the conditional discrimination index to calculate the increment of distinguishing information. the proposed discrimination index is computed by the cardinality of a neighborhood relation. to compare with the above algorithms, there are some parameters to be predefined in terms of original papers. the first four algorithms are related to information entropy, thus no additional parameter setting is needed. since spfs-lar is related to similarity preserving framework, it does not need additional parameters. for qis, the neighborhood size is set as 0.1. according to the original paper, the parameter is set as 0.001 for low-dimensional data and 0.01 for highdimensional data for handi."
"dempster-shafer evidence theory was firstly presented by dempster [cit] . it is an effective uncertainty reasoning method to combine multiple information sources. the researches indicate that the synthetic consequence of conventional dempster s combination rule is frequently contrary to the reality in the practical applications [cit] . two major approaches are presented to enhance the accuracy of synthetic consequence. one is to amend the combination rule. the other is to alter the original evident resource. in this paper, we focus on the latter one."
"in this work, we focus on an extensible set of tools and hardware cores to enable a hardware designer to insert a minimally invasive performance monitoring infrastructure into an existing design, with little effort. the monitors are used in an introspective capacity, providing feedback about the design's performance under real workloads, while running on real devices. this paper presents our hardware performance monitoring infrastructure (hwpmi), which is designed to ease the identification, insertion, and retrieval of performance monitors and their associated data in developed systems."
"rna sequencing (rna-seq) has become the preferred technique for transcriptome-wide analysis of gene expression. however, estimating expression from short sequence reads poses unique problems such as accurate read alignment in the presence of sequencing errors, measurement bias depending on library preparation methodology, and complexity in estimating the expression of distinct mrna transcripts with shared exons. as a result, rna-seq analysis is still rapidly evolving, with a wide number of tools available for each of the major processing steps, and many combinations in which these tools are commonly implemented. as such, the optimal workflow for a given application remains a subject of intensive investigation."
"the results as seen in fig. 3 show that the overall segmentation accuracy of the proposed method has an average 7.54%, 22.72% and 56.12% higher f measure than idtracker, optical flow and sift flow, respectively. the proposed approach also exhibits an 8.74% and 21.30% higher similarity index compared to idtracker, and optical flow, respectively, indicating an improved performance in relation to missing or occluded objects. in particular, the proposed method is more robust against challenging background environments, such as unclear zebrafish well containers with labels (as illustrated by seqs. 8 and 9). the robust segmentation accuracy as seen in fig. 3 across the 10 videos under variant background conditions evaluated with the proposed system further shows that the segmentation performance does not depend on video input tested."
"the dataset consists of 10 video sequences with 3056 frames in total, with various durations and imaging conditions as summarized below (the detailed sequence information and the code to facilitate the ground truth generation are freely available online 14 ):"
"where c l, w l, and g l are the achievable capacity, bandwidth, and channel gain of link l, respectively; s 2 is the ambient gaussian noise power. accordingly, when maximum transmission power is used, the corresponding maximum capacity c max l is c max l"
"tracking accuracy evaluation. figure 4 summarises the tracking accuracy using the motp and mota metrics 21 evaluated over the 10 video sequences, where the raw tracking accuracy for each video is provided in supplementary table s1 . seq. 1 has the clearest background, seq. 2-6 each have one type obstruction (well edge shadows, particles, particles, labels on well, and well edge shadows, respectively), and seq. 7-8 each have two types of obstruction, and seq. 10 has the most complex container background conditions. both the proposed tracking fig. 4 . however, when the video conditions increasingly degrade from seqs. 5-10, the proposed system is more reliable than the existing systems as illustrated by the consistent performance of the proposed system compared with the existing systems as measured by both metrics from seqs. 5-10 in fig. 4 . further, the proposed system exhibits the smallest position detection error, with a decreased overall error of 25.61 and 44.49 pixels using motp compared to idtracker and lolitrack, respectively, and an increased accuracy of 31.57% and 27.2% using mota compared to idtracker and lolitrack, respectively."
"tools. tools of some form are needed to help the designer manage the complexities associated with hardware design, such as timing requirements, resource limitations, routing, and so forth. fpga vendors provide many tools beyond synthesis and implementation that reduce development time, including component generators [cit] that build mildly complex hardware cores, that is, single-precision floating point units and fifos. system generator tools like the xilinx base system builder (bsb) wizard [cit] and the altera system-on-programmablechip (sopc) builder [cit] help the designer construct a customizable base system with processors, memory interfaces, buses, and even some peripherals like uarts and interrupt controllers. there are even tools that can help a designer debug running hardware in the same manner as with logic analyzers in a microelectronics lab [cit] . however, these virtual logic analyzers do not provide runtime feedback on the system's performance, and furthermore require specialized external equipment. in the case of chipscope, each fpga must be connected to a computer through jtag. this limits its use in large scale designs, where debugging hundreds to thousands of fpgas is a daunting task. while jtag can support multiple devices in a single chain, there is additional latency as the number of devices in the chain increases. of course, if jtag were the only option, hwpmi could be inserted into the jtag chain; at this time no such integration has been performed."
"we next evaluated performance by examining the specific recall and precision for individual workflows. recall across the workflows was highly correlated with the number of genes identified (fig. 5a, b) . this was true regardless of which of the reference datasets was used for comparison (additional file 5 and additional file 6). furthermore, the relative rankings of the workflows, ordered by absolute recall value, tended to be consistent across reference datasets (additional file 6). for gene-level predictions, a subset of workflows using samseq exhibited the highest recall values; for transcript-level predictions, workflows using bayseq and nbpseq exhibited the highest recall (fig. 5a, b) . however, there were exceptions to these rules, depending on the choice of read aligner and expression modeler (fig. 5 and additional file 6)."
"in this paper, the routing, rate control, and power allocation are investigated in multi-hop energy renewable wireless mesh networks and the problem of networkwide energy consumption minimization under network throughput constraint is formulated in a form of minlp. to address the uneven routing problem which may incur some severe performance issues, fairness is taken into account and the min-max fairness model is applied to address this problem. in addition, solving the minlp problem would time prohibitive, and thus an energyaware routing algorithm eara is proposed to deal with the joint control of routing, rate, and power in practical multi-hop er-wmns. a weighted dijkstra's shortest path algorithm is applied to search an optimal routing. furthermore, the concept of unit flow is proposed such that our dijkstra-based algorithm can support the multipath routing. extensive simulation results are presented and analysed to show the performance of the proposed schemes. xia xie received the phd degree in computer science from the huazhong university of science and technology, china. she is an associate professor at the school of computer science and technology, huazhong university of science and technology, china. her current research includes data mining, performance evaluation, and parallel and distributed computing."
"to test the opportunity to characterize clinical controls at the cell level with a rapid and robust procedure, we applied a protocol of immunofluorescence staining to breast cancer cell lines with the microfluidic tissue processor and, subsequently, analyzed the fluorescence images with an automatic processing algorithm. we have chosen to characterize both her2+ (sk-br-3 and bt-474) and her2− (t-47d, mda-mb-231, mda-mb-468, and mcf-7) cell lines to find robust parameters for quantification that could be used as clinical controls when staining patient tissues. based on previous results reporting the importance of both her2 and ck assessment for potential breast cancer diagnostics, 10 we stained and quantified both biomarkers. the on-chip protocol was very simple and fast: delivery of the primary ab cocktail (rabbit antihuman her2 igg and mouse antihuman ck igg) for 12 s; incubation for 2 min, washing for 30 s, delivery of the secondary ab cocktail (goat antirabbit igg tagged with a596 and goat antimouse igg tagged with a647) for 12 s, incubation for 2 min, and washing for 1 min. the entire protocol, including slide removal and coverslip mounting, takes only 5 min. the result of this procedure is shown in fig. 2 . the membrane staining of her2 is sharp and visible for her2+ cell lines, as well as the ck in the cytoplasm, whereas only the latter is visible in her2− cell lines. this is expected because, according to previous research, 7,10 microfluidic immunofluorescence imposes a fast, well-controlled staining that limits nonspecific adsorption of antibodies."
"the remainder of this paper is organized into the following sections: in section 2 the background and related works are discussed. section 3 details hwpmi's design and implementation, while the results and experiences of integrating the system into three applications are discussed in section 4. finally, in section 5 the conclusion and future work are presented."
"tracking evaluation metrics. to enable the objective evaluation of tracking performance on the database, this paper employs the widely utilized standard multiple object tracking (mot) metric: classification of events, activities and relationships (clear mot) 21 ."
"to evaluate the segmentation approach in the proposed tracking system, the proposed segmentation approach is compared with the segmentation method within idtracker 5, and the well-known motion feature based optical flow 22 and sift flow 23 methods. the overall tracking accuracy of the proposed system is then compared with idtracker 5, and the widely used commercial lolitrack system 6 . all evaluation experiments were performed using the zebrafish larvae segmentation and tracking dataset presented in this paper, annotated with manually generated segmentation and tracking ground truth."
"unlike conventional approaches where a designer must manually create and insert monitors into their design, including the mechanisms to extract the monitored results, this work analyzes existing designs and generates the necessary infrastructure automatically. the result is a large repository of predesigned monitors, interfaces, and tools to aid the designer in rapidly integrating the monitoring infrastructure into existing designs. this work also provides the designer with the necessary software infrastructure to retrieve the performance data at user defined intervals during the execution of the system."
"1. the problem of network-wide energy consumption minimization under network throughput constraint in multi-hop er-wmns is investigated. we formulate it as a mixed-integer nonlinear program (minlp) that is in general np-hard. moreover, to solve this problem, a scheme that jointly considers the routing, flow rate, and power allocation is proposed in this paper. 2. fairness is also taken into account in the proposed scheme to address the uneven routing problem which may lead to some severe performance issues, e.g., some nodes frequently enter the sleep mode due to their low residual energy level, in multi-hop er-wmns compared to traditional multi-hop"
"hwpmi focused on parsing designs written in vhdl and inserting monitors directly into the vhdl source. the advantage of this approach is the portability of the monitored design. however, by leveraging tools such as torc, the insertion can be performed at the netlist level. the hwpmi tool flow has now been augmented to support inserting the monitoring infrastructure into either the vhdl source or into synthesized netlists, based on a user parameter. continued work is underway to perform the netlist profiling with torc instead of relying on the hdl parsing tools. specifically, torc inserts the monitors into the edif design. figure 9 illustrates how torc is currently used in the hwpmi flow to avoid modifying the vhdl source. the hwpmi flow remains identical throughout the initial stages, but once the monitors have been selected for insertion, torc is used to merge them into the synthesized netlists. torc provides a fast and efficient mechanism to generate modified design netlists with the monitoring infrastructure inserted."
"finally, we present the resource utilization of hwpmi. our goal is to be minimally invasive both in terms of processing overhead and resource utilization overhead. listed in table 2 are some of the performance monitor cores that have been used in the three applications. this includes varying sizes of the monitors to show the overall scalability. while the individual monitor's utilization is heavily dependent on the function of the monitor, we show that with very low overhead hwpmi can be added to a design. especially when compared to a design that requires the addition of a bus interface to a hardware core for performance data retrieval, hwpmi offers an attractive alternative. furthermore, the overhead of the hardware monitor interface, which is 34 slice ffs and 74 4-input luts on the v4fx60 fpga, makes the standard monitor infrastructure of hwpmi very appealing compared to custom monitoring cores."
"aligners and estimators generally did not follow any specific trends, consistent with our observation that their influence is overshadowed by that of the differential expression analysis tool. however, two exceptions stood out. first, using bitseq as the expression modeler tended to result in identification of large numbers of differentially expressed genes, but only in combination with differential expression tools that used an underlying negative binomial model for expression data (bayseq, deseq2, edger, and nbpseq); ebseq was the one exception, with the number of differentially expressed genes within range of workflows using differential expression tools that model other distributions (ballgown, bitseq, limma, and noiseqbio). we note that bitseq was unusual in that its most prevalent estimated expression count value was between 1 and 2, rather than less than 1 fig. 6 comparison of performance metrics. a, b precision and recall for each workflow, with top (shaded) and balanced (white) performers labeled. c, d plots as above, with points colored by tool for each step as most expression modelers estimated; this likely explains why these expression data were poorly modeled by a negative binomial distribution. second, using star as the read aligner, most notably with ballgown as the differential expression tool, led to some of the highest performance workflows having a balance of precision and recall. interestingly, these best performing workflows are not combinations of aligner and estimator that are suggested by the ballgown authors, demonstrating the utility of broad, empirical exploration for uncovering improved workflows. overall, there are multiple workflows that exhibit excellent performance, and, the relationship between recall and precision among the differential expression workflows that track along the inverse linear relationship likely reflects differential calibration of these methods with regard to the tradeoff between sensitivity and specificity, rather than any fundamental difference in statistical or algorithmic performance."
"another well-established cad tool for reconfigurable computing is vpr [cit], a place-and-route tool that remains widely used more than a decade after its inception, and now forms the base of the broader vtr [cit] . however, vpr has traditionally not supported edif, xdl, or actual device architectures. some of those pieces-edif in particular-are available from brigham young university (byu), albeit in java rather than c++. more recently, byu developed an open-source java tool named rapidsmith that fully supports xdl and xilinx device databases [cit] ."
"(1) plb ipic signals figure 6 : sample output of hwpmi system analyzer tool on the collatz design, identifying the components, registers, statements, and interfaces to the core."
"to support xilinx platform studio (xps) ip core development, the hwpmi parse pcore tool is used to parse xilinx pcore directory files: the microprocessor description (mpd), peripheral analysis order (pao), and black box description (bbd) files, along with any xilinx coregen (xco) project files. this enables a designer to migrate profiled cores to other xps systems with minimal effort. furthermore, monitors can be written based on the xilinx coregen project files to provide monitoring of components such as generated fifos, memory controllers, or floating point units, if so desired."
"as shown in (17), the power consumption and residual energy are contributed to the weight. since the value of w l can reflect the consumed energy and residual energy, the energy-aware routing problem with the objective of minimizing network-wide energy consumption under network throughput constraint can be transformed into finding a weighed shortest routing problem in multi-hop er-wmns."
"the frame-to-frame organism assignment based on the cost matrix is performed using the muncres implementation of the hungarian algorithm 19, which searches for unique assignments i.e., assigns source object i to only one target object j in the secondary frame. the assignment is based on the global minimum of the smallest sum of squares distance amongst all of the possible associations, and allows ≠ n m in case of organism detection failure or larvae occlusion. water impurities and well edge shadows may still remain in the binary bitmap, and to avoid these remaining noise fragments the maximum value dist max( ) gt of organism displacement extracted from the tracking ground truth is defined as the distance threshold 19 . in the cases where d o t, i j is greater than the distance threshold, the value of d o t, i j in the cost matrix d is set to inf before mapping association. in the cases where zebrafish larvae fail to be detected or segmented in one frame but reappear in subsequent video frames, a 'gap' in the moving trajectory of this object will appear at the frame where the zebrafish larvae detection initially failed, with a resulting new trajectory created from the frame where the zebrafish reappears. scenarios of multiple object occlusion 7 and misdetection of long-term stationary larvae objects can generate such trajectory 'gaps' . thus, in the proposed tracking system a 'gap bridging' stage is performed using the nearest neighbour algorithm 20 to connect trajectory fragments and improve the inter-frame organism association. however, the trajectory gap will not be connected if the squared distance calculated between the two frames of trajectory fragments is greater than the distance condition calculated as . ⁎ dist 1 2 max ( ) gt 2 . the ratio of 1.2 is extended by 20% beyond unity to set a margin for rebound, similar to the threshold ξ in the gap filling stage of 7, with the trajectory at that frame recorded as an error."
"we sought to empirically assess performance characteristics of rna-seq analysis workflows applied to patientderived clinical samples, which integrate multiple sources of variability that are not well represented in typical benchmarking datasets. we began by generating a test set of rna-seq profiles from purified human leukocytes. specifically, we isolated cell populations from cryopreserved pbmcs collected as part of a study of malaria exposure in ugandan children [cit] . from these samples, we isolated"
"synthesis where the original hardware design is synthesized prior to any insertion of performance monitors. the purpose of synthesizing the design at this point is to retrieve additional design details from the synthesis reports including subcomponents, resource utilization, timing requirements, and behavior. this leverages the synthesis tool output to supplement the static hdl profiling stage by more readily identifying finite state machines and flip-flops in the design. all of the configuration information and synthesis results are available for performance monitor recommendation/ insertion. three tools have been developed to specifically support the designer in the component synthesis stage. these tools automatically synthesize, parse, and aggregate the individual component utilization, resource utilization, and timing information data for the designer. the first tool is the iterative component synthesis tool which runs the synthesis scripts for each of the components in the design. the second tool is the parse component synthesis reports tool: it runs after all of the system components have been synthesized, and collects a wealth of information about each component from the associated synthesis report file (srp). this information includes the registers, fifos, block rams, and finite-state machines (fsm), in addition to all subcomponents. the third tool is the aggregate system synthesis data tool which is used to aggregate all of the data collected as part of the parse component synthesis reports tool. these tools collectively identify the system's interconnects, processors, memory controllers, and network interfaces, in addition to the designer's custom compute cores."
"this work also leverages torc to provide netlist manipulations quickly and efficiently, in place of the original hdl modifications [cit] which were limited to vhdl and were less efficient. future work will integrate torc more fully into the tool flow, replacing the static hdl analysis in favor of netlist analysis. in addition, hwpmi is being prepared for an open-source release which includes the tool flow and hardware ip core repository of both the monitoring infrastructure and performance monitor cores."
"as fpga resources increase in number and diversity with each device generation, researchers are exploring architectures to outperform previous implementations and to investigate new designs that were previously limited by the technology. unfortunately, a designer trying to exploit the inherent parallelism of fpgas is often faced with the nontrivial task of identifying system bottlenecks, performance drains, and design bugs in the system. the result is often the inclusion of custom hardware cores tasked with monitoring key locations in the system, such as interfaces to the network, memory, finite-state machines, and other resources such as fifos or custom pipelines. this is especially true in the rapidly growing field of high performance reconfigurable computing (hprc). there are several research projects underway that investigate the use of multiple fpgas in a high-performance computing context: ramp, maxwell, axel, and novo-g [cit] . these projects and others, like the rcc spirit cluster, seek to use many networked fpgas to exploit the fpga's potential on-chip parallelism, in order to solve complex problems faster than before."
"the distribution probability density model parameters for the gmm are calculated according to the mixing weight and minimum message length (mml) criterion 13, and the improved adaptive gmm introduces an exponential decay envelope shaped by the constant factor α to adapt to background changes. the decay envelope factor α strongly weights the pixel samples representing the temporary movement of these water impurities and illumination changes, to minimise the influence of this temporary movement and enable fast adaptation to background changes. therefore, the zebrafish larvae regions in the video frames are distinguished as moving objects and segmented by subtracting the calculated background model from the original video frames. that is, in the proposed system there are no requirements on the video images to have a clear background, use transparent containers without edge shadows, or have high intensity contrast between the zebrafish larvae and the background."
"because absolute recall and precision values are influenced by the repertoire of analytes that can be measured by a given platform, we first filtered each reference and rna-seq gene set to include only features measurable both by rna-seq (i.e., present in the grch37 genome release) and by the microarray (i.e., a probe targeting the feature was present on the microarray platform) within a given comparison. all gene set counts are reported based on these filtered numbers, as are all estimates of recall and precision. recall was calculated as the number of significant genes in the intersection of the test rna-seq dataset with the reference dataset, divided by the number of genes identified as significant in the reference dataset. precision was calculated as the number of significant genes in the intersection of the test rna-seq dataset with the reference dataset, divided by the number of genes identified as significant in the test rna-seq dataset."
"reads were aligned to release grch37 of the human genome. reads were aligned with bowtie2, hisat2, kallisto, salmon, sailfish, seqmap, star and tophat2 [cit] . gene and transcript expression was estimated with bitseq, cufflinks, htseq, isoem, kallisto, rsem, rseq, sailfish, salmon, star, stringtie and express [32-35, 37, 39-45] . the isoem code was modified to increase the maximum available memory. expression matrices for differential expression input were generated using custom scripts as well as the prepde.py script provided at the stringtie website. differentially expressed genes or transcripts were identified with ballgown, bayseq, bitseq, cuffdiff, deseq2, ebseq, edger exact test, limma coupled with vst or voom transformation, nbpseq, noiseqbio, samseq and sleuth [33, 39, 40, [cit] . of these, all but ballgown, bitseq, nbpseq, samseq, and sleuth used intrinsic filtering or recommended extrinsic filtering of genes or transcripts prior to testing. for sailfish and salmon, outputs were converted to a sleuth-ready format using wasabi [cit] . for kallisto, sailfish, salmon, and bitseq, transcript-level values were condensed to gene-level values using tximport prior to evaluating gene-level differential expression [cit] . for all differential expression analyses performed at the transcriptlevel, significant transcripts were converted to the corresponding gene for performance evaluation, such that if a single transcript was called as differentially expressed, the corresponding gene was also called differentially expressed. we note that because of this unavoidable difference between gene-level and transcript-level comparisons, quantitative comparisons of recall and/or precision between a gene-level and a transcript-level workflow should be avoided. rather, we recommend evaluating the relative performance of a given workflow as compared with other workflows with matched gene-level or transcript-level estimation. when possible, differential expression was assessed using multiple expression units (counts, fpkm, tpm) and performance metrics are reported separately for each unit. in general, all software was run with default parameters; specific runtime parameters are listed in additional file 1, along with software versions, and scripts for running all code are available at https://github.com/cckim47/kimlab/tree/master/rnaseq. further information about implementation is available upon request. all software was run at a detection level of alpha of 0.05, fdr of 0.05, or pplr in the most extreme 0.05. abbreviations used throughout the figures are a sixletter code represented as aabbcc, where aa denotes the read aligner (ra), bb denotes the expression modeler (em), and cc denotes the differential expression (de) analysis tool. all tools and codes are shown in table 1 ."
"in the frame t + 1: where n is the detected number of zebrafish larvae in the frame t, and m is the number of zebrafish larvae segmented in the successive frame t + 1. the element d o t, i j in the matrix denotes the cost to connect the i-th object in the frame t, to the j-th object in the frame t + 1. the value of d o t, i j is calculated as the euclidian distance from the source object to the target object based on the centroids of the segmented regions in cartesian coordinates o i (x i, y i ) and t j (x j, y j ), as given by:"
"in the field of breast cancer research and diagnostics, the human epidermal growth factor receptor 2 (her2) receives major clinical interest since this membrane protein is targeted with an fda-approved drug, namely trastuzumab. 1 to assess the status of the her2 protein and to decide about the application of an her2-targeted therapy, clinical guidelines have been set. 2 they are based on the outcome of a standard experimental technique for the evaluation of protein overexpression, which is called immunohistochemistry (ihc). 3 it consists of using antibodies and enzymatic reactions to stain a tissue slice obtained from a tumor biopsy of the patient. the optical readout consists in colored membranes of which the intensity indicates the overexpression level, scored as 0, 1þ (negative), 2þ (ambiguous), and 3þ (positive). according to the guidelines, for which 3þ patients can benefit from her2-targeted therapy, 2 a unique score is attributed to the entire tissue, but previous work was proposed to score each cell individually, 4 which is advantageous to study intratumoral heterogeneity and its consequences in disease prognosis. 5 a recognized problem in standard ihc is the difficulty to obtain robust and quantitative assessment. 6 therefore, classification of the her2 overexpression levels using ihc staining depends on the experience of each evaluator. previous attempts to solve this issue used microfluidics to perform immunofluorescent staining. 7, 8 the crucial point was to decrease the incubation time required for the staining of the cancer tissue using a microfluidic tissue processor for a fast and local delivery of reagents on the sample. such well-controlled microfluidic staining helped to decrease the number of ambiguous (2þ) cases compared with manual procedures. another study used image processing on immunofluorescence staining to characterize the ratio between the her2 expression and the cytokeratin (ck) expression (a biomarker used to define the tumor region in patient biopsies) 9 and showed that this parameter helps with estimating the her2 gene amplification in patient tissues. 10 nevertheless, those studies used a quantification method that involved a standardization with respect to a positive (3þ) patient introduced in each test, which can result in a normalization bias because (i) the status can vary from case to case (patient specificity) and (ii) each tissue slice from the same patient may have a slightly different score depending on its location within the tumor (intratumoral heterogeneity). moreover, the biomarker quantification was based on an average value calculated on all the cells within the image, which may hide interesting information on the individual cells."
"the sensitivity of the segmentation accuracy due to the tuning factor α was examined in 12, where the range of α values evaluated showed a consistent and reliable segmentation performance. in turn, the robust segmentation accuracy seen in fig. 3, which illustrates the 10 videos under variant background conditions evaluated with the proposed system shows that the segmentation performance does not depend on the video input tested. further, optical flow, sift flow and idtracker respectively exhibit 1.95%, 2.45% and 0.47% more variance in all of the evaluation metrics studied, which suggests that the segmentation results are less reliable across the complex video sequences evaluated."
"if the number of linear segments is enough, the linear approximation error will be very small, but the resulting computational complexity would be very high. theoretically, the number of segments can be obtained when an approximation error of each linear segment is prescribed. by using (16) to replace the nonlinear constraints in (10), the three minlp problems, opt, min-max, and opt-f, can be transformed into milp problems that can be efficiently solved by an off-the-self solver such as cplex [cit] . hence, after obtaining the results, the routing, flow rate, and power can be determined accordingly."
"blood was collected from ugandan children as part of the program for resistance, immunology, surveillance & modeling of malaria in uganda study using previously described methods [cit] . peripheral blood mononuclear cells (pbmcs) from a total of 18 individuals were isolated on ficoll gradients, counted, and immediately cryopreserved and stored long-term in liquid nitrogen. samples were thawed in the presence of dnase and immediately stained in facs buffer with antibodies specific for the following targets: cd7 (clone 4h9), hla-dr (clone l243), cd16 (clone cb16), cd14 (clone 61d3), cd19 (clone hib19) from ebioscience; and cd177 (clone mem-166) from biolegend. for flow cytometry, classical monocytes were identified as cd177 . both monocyte subsets were isolated to high purity using two consecutive rounds of sorting on a facsaria, using an event rate no higher than 5,000 events/s and sorting directly into an rna preservative buffer on the second sort. a total of 67 -3149 cells were sorted per sample. each sample represents a single individual, and both nonclassical and classical subsets were sorted from each individual. sorted cells were immediately snap frozen on dry ice and stored in a −80°c freezer until the time of rna isolation."
"we also evaluated performance of the workflows by calculating recall (intersecting significant genes divided by total number of significant reference genes) and precision (intersecting significant genes divided by total number of significant genes identified by rna-seq), using the microarray datasets as references. in order to further examine the influence of each stage of the workflow on the prediction of differentially expressed genes, we computed the absolute difference in recall and precision in all possible pairwise comparisons of workflows differing in only one component. similar to the impact on the number of genes identified, for both precision and recall, the largest effects were observed in workflows differing in the statistical analysis of differential expression, as indicated by the increased medians of differences for this step (fig. 4) ."
"included in hwpmi are the specific monitors, such as timers, state-machine trackers, component utilization, and so forth, along with a sophisticated monitoring network to retrieve each monitor's data all while requiring little user intervention. to evaluate hwpmi, three use cases show how existing designs can utilize hwpmi to quickly integrate and retrieve monitoring data on running systems. moreover, hwpmi is flexible enough to support both high performance reconfigurable computing (hprc), running on the spirit cluster as part of the reconfigurable computing cluster (rcc) [cit] project at the university of north carolina at charlotte, and embedded reconfigurable systems running on a single board with limited compute resources. several interesting results are discussed which support the importance of such a monitoring infrastructure. finally, the tools and hardware cores presented here are being prepared for an open-source release in the hope of increasing and diversifying the types of monitors and the systems that will be able to utilize hwpmi."
"it can be observed from fig. 7 that the network lifetime is gradually decreasing with the increase of the throughput. this is because more energy will be consumed to deliver the information over multi-hop er-wmns when the throughput required by users is increasing, resulting in that more nodes will enter into sleep mode and the network lifetime will be reduced. in addition, among opt, opt-f, and eara, opt algorithm can achieve the highest network throughput. the reason is that the energy consumption by applying the opt algorithm is lowest due to no consideration of the balance of energy consumption and residual energy. moreover, this figure shows that the curve of eara is close to opt-f as well."
"since the quality of service (qos) for each user needs to be guaranteed in multi-hop er-wmns, the data rate of each session should be met in the design of the joint routing, rate control, and power allocation scheme."
"as hardware designers develop custom cores and assemble systems-on-chip (socs) targeting fpgas, the challenge of the design meeting timing, fitting within the resource constraints, and balancing bandwidth and latency can lead to significant increases in development time. when a design does not meet a specific performance requirement, the designer typically must go back and manually add more custom logic to monitor the behavior of several components in the design. while this performance information can be used to better understand the inner workings of the system, as well as the interfaces between the subcomponents of the system, identifying and inserting infrastructure can quickly become a daunting task. furthermore, the addition of the monitors may change the original behavior of the system, potentially obfuscating the identified performance bottleneck or design bug."
"to achieve data transmission between a source node and its corresponding destination node, the multi-path routing scheme is applied in this study. hence, each session can be split into multiple flows, and traffic is delivered through multiple paths. let r l ðfþ denote the flow rate attributed to session f ðf 2 fþ on link l. we use l"
"beyond the information on the biomarker expression, we explored the possibility to use the cell size as another indicator of the cancer status (fig. 5) and found that the performance in the discrimination of cells is much less effective (i.e., lower informedness), which confirms the similarity in size of her2+ and her2− cancer cells identifiable in cancer tissues. 4, 10 finally, to quantitatively compare to what was observed previously on tissue samples at a tile level, 10 we computed the average cell her2/ck ratio of the different samples and showed it together with the her2 signal of the same samples (fig. 6) . when using the her2 signal alone [ fig. 6(a) ], the 95% confidence intervals of her2− and her2+ samples overlap, which show the indistinguishability of the two types. whereas, when adjusting for ck expression at single-cell level [ fig. 6(b) ], the two types are much more separated."
"one of the barriers to validating analysis workflows is a paucity of real-world rna-seq samples for which reference datasets are available for comparison. here, we describe an rna-seq dataset generated from human classical and nonclassical monocyte subsets isolated to high purity. differential gene expression analysis between these subsets has been analyzed in multiple transcriptome-wide microarray and beadchip studies [cit], providing us with gene sets that have been validated by multiple independent laboratories using multiple gene expression analysis platforms. therefore, these gene sets provide a reference estimate of biological 'truth'. using the sequence reads from our monocyte subset dataset, we evaluated commonly used differential expression workflows for their performance, as assessed by their agreement with these references. we find that different rna-seq analysis workflows differ widely in their performance, as assessed by recall, or the proportion of reference-identified genes that were also identified by the given workflow, and precision, or the proportion of genes identified by the workflow that were also identified by the reference. many workflows perform equally well, but are calibrated differently with respect to favoring higher recall or precision, with an inverse relationship between these parameters. based on our observations, we recommend that the selection of a given approach be guided by the tolerance of downstream applications for type i and type ii errors. used in conjunction with the previous microarray and beadchip studies, these rna-seq data provide a real-world test set for guiding the development of improved software and workflows."
"supplementary table s2 summarises the total number of individual identities swapped across each tested video. the proposed system exhibits the smallest identity swapping rate, with 24.32% less identity swap than the idtracker system. in addition to the proposed segmentation method exhibiting a consistently higher accuracy segmentation than idtracker as shown in fig. 3, applying the proposed segmentation method to idtracker reduces the zebrafish larvae misdetection and false positive rates, as shown in fig. 4 . however, the identity swapping rate is doubled as shown by supplementary table s2, due to the generated binary foreground images providing limited intensity information for idtracker to generate the required fingerprint."
"we performed on-chip staining, fluorescence imaging, and image processing on four cases of her2+ cell lines and four cases of her2− cell lines. among the two biomarkers, none of them defines a clear separation between her+ and her2 − status, and their distributions partially overlap [ fig. 3(a) ]."
"several studies have previously explored gene expression differences between cd14 hi cd16 − classical monocytes and cd14 lo cd16 + nonclassical monocytes using microarray or beadchip analysis [cit] . similar to our rnaseq dataset, these studies all represent monocytes from healthy donors. however, given that the data originate from labs in singapore, the united states, and germany, it is likely that there is some bias in genetics across the studies. it is also likely that these microarray data do not reflect the same genetic makeup and environmental pressures present in our data, which are obtained from ugandan children with a high degree of malaria exposure. it should also be noted that recent studies have differentiated between three, rather than two, monocyte subsets [cit], and several reference datasets were produced prior to this advancement and thus might not represent the same degree of purity in their nonclassical monocyte subset [cit] . despite these differences, in aggregate, these datasets provide a strong reference of biological 'truth' for comparison, as individual datasets can be evaluated as independent assessments of a given rna-seq analysis workflow. because differentially expressed gene lists were not available for all studies and statistical criteria differed between studies, we have made our re-analysis of these publicly available datasets available as supplementary data (additional file 2). overall, the four datasets identified 4069 unique genes. of these, 572 were shared among all 4 datasets, and 2755 were shared between at least two datasets. the wong dataset showed the least overlap with the other datasets, contributing approximately half of the genes unique to a single dataset (fig. 2) . with these four datasets as our references for performance comparisons, we focused our evaluation on rna-seq analysis approaches that have gained wide adoption due to their performance, availability, documentation, and/or ease of implementation. we evaluated 9 read aligners, 12 expression modelers and 13 methods for identifying differentially expressed genes and transcripts (table 1), in all possible combinations. exceptions included cases in which the output of an earlier stage was incompatible as the input to a later stage due to file format or expression units, or difficulty with software execution. in total, including comparisons made at the gene level and transcript level, and comparisons using expression data reported in counts, tpms, or fpkms, we evaluated 495 unique workflows (additional file 4). we note that some of the workflows were not intended to be used in the resulting combinations by the original authors of the software. despite the aforementioned heterogeneity in the microarray and beadchip analysis results, we found that performance of various rna-seq workflows was remarkably consistent across all four reference datasets. we note, however, that these reference datasets are also subject to the inherent biases of the experimental and computational methods used to produce them. here, we have depicted our results using performance metrics averaged across all four references; however, we have also made available the performance estimates for each individual reference (additional file 5 and additional file 6), and an interactive visualization to explore the relative performance of the tools in more detail (additional file 7)."
"segmentation evaluation. figure 3 shows the average f measure and si scores presented with the 95% confidence intervals for each of the 10 video sequences in the dataset, presented in the order that the first sequence has clearest background and the 10th (last) sequence has the most complex background."
"cryopreserved sorted cells were thawed, and rna was isolated using an rnaqueous micro kit (thermofisher, waltham, ma) following manufacturer recommendations with the following modifications: lysis buffer/cell aliquots were initially mixed with 180 μl of 200 proof rnase-free ethanol; the flowthrough was reloaded onto the column to capture additional material with a second binding step; and the purified rna was eluted twice with 6 μl 55°c rnase-free water following a 2 min incubation. isolated total rna was vacuum concentrated to 1 μl and converted to pre-amplified cdna libraries using template-switching reverse transcription [cit] as implemented in the smarter ultra-low input kit (clontech, mountain view, ca). two samples failed to yield cdna and were thus excluded from further processing. fragmentation was performed enzymatically using a nextera xt dna kit (illumina, san diego, ca), and barcoded samples were multiplexed, pooled, and purified using agencourt ampure xp beads (beckman coulter, brea, ca). libraries were quality-controlled for size distribution and yield using a bioanalyzer 2100 with high sensitivity dsdna assay (agilent technologies, santa clara, ca), and sequenced as 51 bp single-end reads on 4 lanes of a hiseq 2500 (illumina) running in high-output mode at the ucsf center for advanced technology (san francisco, ca). reads were demultiplexed with casava (illumina), and read quality assessed using fastqc [cit] ."
"reference datasets were prepared from four published studies conducted on microarray or beadchip platforms (gse25913, gse18565, gse35457, gse34515) [cit] . an additional reference set (gse16836 [cit] ) was considered, but excluded due to inter-sample variation precluding identification of differentially expressed genes. significant differentially expressed genes between classical and nonclassical monocytes were identified for each dataset. in brief, series matrix files were downloaded from the ncbi gene expression omnibus, log 2 transformed if necessary, full-quantile normalized [cit], and analyzed for statistically significant gene expression between classical and nonclassical monocytes. to reduce bias introduced by a single statistical method, we employed two approaches: significance analysis of microarrays (sam) [cit] with a false discovery rate of 0.05, and limma [cit], with a bh-adjusted p-value of 0.05. performance of the workflows against both sam and limma were compared to one another and found to exhibit good reproducibility regardless of the statistical method used to generate the data (additional file 2 and additional file 3); as such, we chose to use the genes at the intersection of the two methods for our final reference gene sets."
"line and can specify a specific vhdl source to evaluate. the parser identifies the entity's structure in terms of ports, signals, finite-state machines, and instantiated components. the parser works by analyzing the vhdl source file and uses pattern matching to decompose the component into its basic blocks. the parser is only responsible for the identification of the vhdl component's structure. the results are then passed into a python pickle for rapid integration with the remaining tools. next, the hwpmi system analyzer tool iteratively parses the design to identify the different interfaces, such as bus slaves, bus masters, direct memory access, and xilinx locallink. this is done at a higher level than the hwpmi core parser which more specifically analyzes individual ip cores. figure 6 shows the output of the hwpmi system analyzer for one of the systems evaluated in this work, the collatz design. more commonly a designer would use the system analyzer because it can support iterating through an entire design, once given a list of all of the source files. on the command line the designer invokes the tool with a project file that lists all of the vhdl source file locations. the user also specifies the top-level entity for the design."
"we also plotted the cell-by-cell status in the her2 versus ck scatter plot [ fig. 3(b) ]. we observe that the cells belonging to a particular cell line cluster together in the biomarker expression plot, as expected. moreover, duplicates of her2+ cell lines (sk-br-3 and bt-474) are also close to each other. this indicates that the experimental and analytical procedure reliably quantifies the expression of the biomarkers despite the potential technical variations of the staining and the imaging steps (e.g., excitation light intensity, washing efficiency, and temperature) between a slide and another, which can explain the small differences between duplicates of the same cell line. furthermore, we observe that her2+ and her2− cell lines likely cluster apart following a straight line with positive slope. this can provide a robust method to distinguish between positive and negative cases."
"it is important to note that the specific workflows highlighted above are at the extremes of one or another performance metric. as would be expected, the prediction of more or fewer significant genes results in a tradeoff between recall and precision. for example, the workflows employing noiseqbio that exhibit the highest precision were also among those with the lowest recall ( fig. 5 and additional file 6). an investigation of the relationship between precision and recall revealed that this tradeoff generally persisted throughout, with many workflows following an inverse linear relationship between precision and recall (fig. 6a, b) . this held true for both gene-and transcriptlevel analysis, was true regardless of the expression estimation units, and was also consistent across reference datasets (fig. 6a, b, additional file 7, and additional file 8)."
"wild zebrafish embryos (danio rerio) were incubated at 28 °c in a petri dish filled with an e3 medium. any debris and unfertilised embryos were manually removed three hours post-fertilization (hpf). five days post-fertilization, the larvae were obtained from hatched zebrafish embryos. for data acquisition, zebrafish larvae were transferred to poly (methyl methacrylate) (pmma) housing wells. low frame rate videos were recorded with a dino-lite ad7013mt microscope at frame rates of 14 or 15 fps. high frame-rate videos were captured by an imaging development systems (ids) ui-3360cp-c-hq microscope, with a high resolution 12.5 mm focal lens."
"suppose a set of f active unicast sessions in the considered network scenario. let sðfþ and dðfþ denote the source and destination nodes of session f ðf 2 fþ, respectively. moreover, rðfþ represents the data rate of session f. therefore, for the network with jf j active session flows, network throughput u is the sum of data rate of all sessions, i.e.,"
"three applications are used to demonstrate our hwpmi tool flow: single precision matrix-matrix multiplication, a hardware implementation of the smith/waterman flocal align(), and a hardware implementation of the collatz conjecture core. this section will highlight different use cases of hwpmi in these applications."
"this section studies how to jointly control the routing, rate, and power so as to achieve the network-wide energy consumption minimization under the network throughput constraint in multi-hop er-wmns. this problem is motivated by the scenario where users have strict network throughput limit. hence, given the network throughput constraint, the optimization problem is to minimize network-wide energy consumption by virtue of jointly controlling the multi-path routing, rate for each session, and power on each link."
"t he increasing demand for ubiquitous network access leads to the rapid development of wireless access technologies. the multi-hop wireless mesh network (wmn), as a promising solution for low-cost broadband internet access, is being used on the last mile for the enhancement of internet connectivity for mobile users for its provision of high data rate [cit] . a multi-hop wmn is usually constructed by wireless mesh nodes that are wireless mesh routers or gateways. one of features of wmns is that mesh nodes are rarely mobile and powered by power grid. mobile users access internet service through gateway and information is always delivered by virtue of multi-hop relaying."
2. all remaining regions were sorted by mean dapi intensity and the bottom 5% was excluded; this step filters out the background regions (low dapi signal).
"it is important to emphasize that the hwpmi flow does not intelligently insert a subset of monitors when the available resources are depleted. future work is looking into ways to weight monitors such that hwpmi can insert more important monitors; however, presently hwpmi recommends the available monitors that can be inserted into the design and it is up to the designer to choose the subset that will provide the best feedback versus resource availability trade-off."
"precision was highly inversely correlated with the number of genes predicted across the workflows (fig. 5c, d ). like recall, rankings were generally consistent regardless of which reference dataset was used, as was the overall relationship between significant genes and precision (additional file 5 and additional file 6). for gene-level predictions, a subset of workflows using noiseqbio exhibited the highest precision, whereas for transcriptlevel predictions those with the highest precision used several different combinations of tools, with the most prevalent being ballgown and noiseqbio. strikingly, when used on transcript-level data, the commonly used combination of tophat2, cufflinks and cuffdiff exhibited one of the highest precision values, coupled with the second lowest number of differentially expressed genes identified (fig. 5 and additional file 5)."
"in particular, each node is powered only by renewable energy in multi-hop er-wmns and the solar is considered as the energy source. a large-sized solar panel is used to obtain the solar energy that is then transformed into electrical energy. the structure of a node is shown in fig. 1, where a solar panel connects the access point. the electrical energy will be stored into the battery via a charging controller that controls the charging process. each node consumes the energy from the battery because it can supply the energy continuously. once the energy contained in the battery is lower than a threshold, the charging controller will immediately shut down the power supply and the node enters sleep mode. since the energy production rate is low in practical energy systems, the energy replenishment rate is less than the energy consumption rate when a session is delivered through a node. usually, the node cannot work well when the residual energy of a battery is lower than a threshold b outage and meanwhile the battery has a maximum capacity limitation b max . therefore, the residual energy of a node varies between b outage and b max ."
"microfluidic methods were also implemented to enable multiplexed immunofluorescence staining on tissue slides. 11 the approach used fluorescence spectroscopy on quantum dots to detect multiple epitopes on the same sample using three parallel microfluidic channels. the same multichannel approach was then used to standardize the staining conditions by exploiting a christmas-tree design for linear gradient generation. 12 a main advantage of such a method is the ability to provide a semiquantitative assessment on robust controls, such as cell lines. conversely, its main drawback is the use of a multichannel microfluidic design for multiplexing, which, while providing information on different biomarkers in different locations of the same sample, prevents coexpression or correlation analysis of cells expressing more than one biomarker."
"examples of the result of this procedure are shown in fig. 2 . the regions in which the image is partitioned by this algorithm define a cell in the majority of cases, but for those that correspond to the background or to only a part of a cell, we excluded them from all subsequent analysis using robust filtering methods:"
"compared to the tracking of adult zebrafish in microscopic videos, the dynamic 'bursty' locomotive characteristics and complex video imaging conditions of zebrafish larvae due to their small size relative to background imaging artifacts poses many different challenges for the tracking of multiple zebrafish larvae. this paper proposes a zebrafish larvae tracking system for both single and multiple zebrafish larvae under complex video conditions, applying an adaptive gmm probability density model, median filter and morphological operations to segment larvae objects from the background, and hungarian assignment for tracking. comparisons with existing state-of-the-art biological small organism tracking systems illustrated the accuracy and efficiency of the proposed system, where the proposed system also removes the strict limitations on input video imaging conditions to enable the testing of unconstrained experimental videos. further, the proposed background subtraction and segmentation approaches applied alone as pre-processing to existing tracking systems (such as idtracker) improve the multiple organism tracking accuracy by up to 32%. this is due to decreased zebrafish larvae misdetection and false positive rates; however, the identity swapping rate may increase if the identity is generated using intensity variance information, such as the approach used in idtracker. the immediate future work is in evaluating the proposed tracking system for other biological organisms, including adult zebrafish. together with the increased size and intensity contrast, the continuous swimming movements of adult zebrafish provide consistent motion features that can be easily captured by the adaptive gmm model and subsequent object tracking."
"many automatic single and multiple tracking systems have been recently developed for adult zebrafish 1, [cit], such as the state-of-the-art based on deep learning 7, particle filtering 9, and the well-known idtracker 5, reporting outstanding tracking performance for adult zebrafish. however, the locomotive characteristics of zebrafish larvae are dramatically different from adult zebrafish. adult fish are continually swimming, whilst zebrafish larvae can display little or no movement over time 8, 10, thus their dynamic responses can be imbalanced. zebrafish larvae can exhibit a mean proportion of activities less than 0.075 over time, according to the statistics reported in 8 . this is the first primary cause of tracking failure in these systems and traditional statistical tests based on movement features to track and analyse larvae behaviour. moreover, the intensity contrast between adult fish with the water background is also greater than that for zebrafish larvae, due to the transparent larvae body peripheral. however, both the adult zebrafish tracking systems in 5, 7 are based on the assumption of high intensity contrast, which is another common and required imaging condition constraint for existing zebrafish tracking systems."
"although routing, rate, and power can be determined through solving minlp problems, much time is still needed to solve large-scale problems [cit] . this section presents an energy-aware routing algorithm with the consideration of flow rate and power allocation. this algorithm can achieve the joint control of routing, rate, and power with no need of solving minlp problems. the computational complexity of the proposed eara is controllable, and can be adjusted according to the accuracy requirements. as a result, the algorithm can be well applied in the practical multi-hop erwmns. moreover, the consumed energy and residual energy are simultaneously considered and a balance between them can be attained. it is noteworthy that a multipath routing is considered here and the split flows meet the rate balance."
"1. all the regions were sorted by area, and both the bottom and the top 5% were excluded; this step filters out the regions corresponding to parts of cells (small areas) and large background regions (large areas)."
"figs. 4 and 5 depict the changes of total residual energy and the number of sleep nodes along the time in opt, opt-f, and eara algorithms. a 15-node multi-hop wmn is considered in these two examples and several sessions are perpetually delivered over the network. in order to avoid the selected source node and destination node entering into sleep mode, the sessions occur randomly and periodically change. fig. 4 shows that the total residual energy is continually degrading along the time. the reason is that the energy is consumed to deliver two sessions while the energy replenishment rate is much less than the energy consumption rate. in addition, the opt algorithm has the highest residual energy among these three algorithms. this is because the opt algorithm can achieve the minimal energy consumption whereas opt-f and eara can obtain the balance between the residual energy and energy consumption owing to the fairness constraint factor. moreover, it can be seen that the two curves of opt-f and eara are much close. it means that the similar performance can be achieved by the proposed algorithm, eara, compared with opt-f. this phenomenon shows the effectiveness of the eara algorithm. fig. 5 illustrates that the number of sleep nodes is increasing along the time. meanwhile, the number of sleep nodes of the opt algorithm is more than that of the opt-f algorithm and the eara algorithm at the first stage. this is because opt can achieve the optimal energy consumption. moreover, the network lifetime, i.e., the duration of delivering sessions, if using the opt-f algorithm or the eara algorithm is less than that using the opt algorithm. in this paper, the lifetime is defined as the network operation time until the residual energy of the whole network is less than 30 percent. the reason is that much more energy is consumed by the opt-f and eara algorithms, so some nodes can easily enter sleep mode after a duration."
"the structure and fabrication of the chip were reported previously. 7, 10, 14 first, a 4-in. silicon wafer with a 2.5-μm wet-oxide layer was taken, on which 5-μm az9260 photoresist was spun, exposed with the channel mask, and developed to form the channels. the oxide underneath was etched by reactive ion etching (601e; alcatel, france). the resist was stripped, and an additional lithography was realized by spinning 5-μm az9260 photoresist (microchemicals gmbh, germany) and exposing the latter to a second mask. after the lithography, the front side was etched in two steps by deep reactive ion etching (drie; 601e, alcatel, france) to form channels and vertical access holes with different depths. first, the wafer was etched to a depth of 100 μm and the resist was stripped. thereafter, the channels were etched via the patterned hard mask realized in the first step. the etch depth varied between 50 and 200 μm, depending on the design. subsequently, the silicon wafer was bonded to a 2-μm parylene-c-coated pyrex wafer using a low-stress parylene-c bonding procedure. after bonding, additional lithography of the glass/silicon-bonded stack was done on the silicon side using a 8-μm-thick az9260 resist. then, one more step of drie was performed from the backside until the access holes were reached, a process used at the same time to generate notches for o-ring incorporation. subsequently, the resist was stripped and oxygen plasma was applied to clean the device. fabrication was finalized by dicing the glass/silicon micromachined structures into their final shapes [ fig. 1(a) ]."
"the constraints on the input imaging conditions as required by existing systems are largely due to poor object detection and segmentation results from the input videos. thus, improving the segmentation method will remove the need for input imaging constraints, where it has been shown that improving the segmentation accuracy can result in more reliable tracking performance 11 . however, this assumption of improving segmentation accuracy to enhance tracking performance has not yet been examined. this paper investigates the novel adaptation of advanced computer vision techniques and multiple object tracking algorithms to develop an automatic, accurate and effective multiple zebrafish larvae tracking system using microscopic larvae videos, without any constraints on the input video imaging conditions. the proposed system is designed to segment and track the 'bursty' movement characteristics specific to zebrafish larvae, where the resultant tracking trajectories generated by the proposed system can then be used for further study, including the analysis of larvae movement characteristics. the performance of the proposed system is evaluated based on segmentation and tracking accuracy using a zebrafish larvae dataset also presented in this paper, and compared with the current state-of-the-art idtracker system 5 and the off-the-shelf commercial lolitrack system 6, which allows for both single and multiple zebrafish larvae tracking. for reproducible research, the dataset generated for evaluation and the software for the proposed zebrafish larvae tracking and evaluation methods are publicly available online. figure 2 outlines the proposed automatic zebrafish larvae tracking system, which consists of multiple stages: background subtraction, zebrafish larvae segmentation, association or matching of the larvae between successive frames, followed by bridging any remaining gaps amongst the trajectory fragments. (c) frame example with larvae occlusion, which will not been seen when the larvae are separated in petri dish plates; (d) idtracker 5 required frame input with clear tank edges, and large size ratio between adult fish and the container; (e) frame example with labelling as indicated by the red arrows, water bubbles as highlighted by the red circles, and larvae with low intensity contrast between the well edge shadow as shown by the red rectangle; (f) frame example with small water particles as shown by the red triangles."
"the proposed similarity index (si) metric in equation (6) accounts for the number of correctly segmented objects by penalizing missing objects or object occlusion. num miss and num gt are the number of objects missed, and objects detected in the ground truth, respectively. the recall and precision metrics estimate under-segmentation and over-segmentation, respectively. the f measure is a weighted calculation of the precision and recall."
early versions of hwpmi interacted with vhdl source to identify and insert performance monitors and the necessary infrastructure into existing designs. torc is being added to expand beyond vhdl and to ensure that the original source remains unmodified after it has been profiled and evaluated-only the resulting synthesized netlists are modified. another reason for migrating to torc is its efficiency and scalability: a plot of edif read and write performance is provided in figure 1 . using torc to interact with edif has been shown to be far more efficient than using vhdl parsing and insertion tools.
"the proposed segmentation approach is applied as pre-processing to the idtracker system to determine the effect of the proposed segmentation approach on the overall tracking accuracy. the result of both the motp and mota values improved by 32.00% and 22.91%, respectively, compared with the original idtracker system. the proposed background subtraction and segmentation processing also removes the need to constrain the input zebrafish larvae video imaging conditions, and enables the testing of videos under realistic experimental conditions using idtracker. that is, researchers who already use idtracker can apply the proposed segmentation method as pre-processing to obtain tracking results of higher accuracy using the existing idtracker system, with video data in unconstrained imaging conditions. figure 5 is a visual example of the tracking trajectory obtained for seq. 4 by lolitrack, idtracker and the proposed system. it can be seen that the proposed tracking system exhibits the most complete tracking trajectories estimated for realistic experimental conditions. in contrast, lolitrack (fig. 5a ) detects the well edge shadow as zebrafish due to their similar intensity values, whilst idtracker system (fig. 5b ) produces many trajectory gaps primarily caused by the false detection (as shown by the light blue line) of larvae objects due to their 'bursty' locomotive characteristics and small size differentiation with impurities inside water. the resulting identity estimated from idtracker is therefore also not reliable, with an estimated reliability of identity of 63% calculated according to the trajectory analysis of idtracker as shown by fig. 5b ."
"each worker node is running an application-specific soc which includes the hwpmi hardware cores. these cores can be seen in figure 4 as the system monitor hub, hwpmi interfaces, context interface, performance monitor hub, and performance monitor cores. the system monitor hub acts as an intermediary to decode incoming requests for performance data. each hardware core connects to the system monitor hub via a software-generated context interface (cif). the cif connects to the performance monitor hub which in turn aggregates all of the performance monitor core data within the hardware core."
"in the past years, researchers largely concentrate on the channel assignment, routing, and rate allocation problems in multi-hop wmns [cit] investigated the channel assignment problems in multiradio wmns, and designed centralized and distributed algorithms for the channel allocation with the objective of minimizing the overall network interference. [cit] addressed the radio resource assignment optimization problem in wmns, where routing, scheduling and channel assignment were jointly considered. passos and albuquerque [cit] considered the routing and rate adaptation problem in wmns, and proposed a joint automatic rate selection and routing scheme to provide best routing and rate. in addition, since energy consumption is becoming a very important problem in the world for the rising of greenhouse gas emission, energy efficiency has been attracting much attention [cit] considered the energy efficient scheduling scheme in wmns and an enhanced pseudo random access scheme was proposed to improve energy efficiency. the power control problem in wmns was also investigated to reduce interference and energy consumption [cit] . although some work has been done to reduce energy consumption, the benefit is essentially marginal."
"although some related research work has been conducted on problems in er-wmns, the high interdependency of of routing, rate, and power, and their significant influence on energy consumption in multi-hop wmns have been little studied. to fill in this vacancy, this paper proposes a scheme that jointly considers routing, rate control, and power allocation to minimize network-wide energy consumption with network throughput constraint in multi-hop er-wmns. the key contributions of this paper are summarized as follows:"
"to solve the limitations of the previous approaches and increase the robustness of the her2 status assessment, we studied the possibility to apply cell-based biomarker assessment on breast cancer cell lines for two biomarkers on the whole sample. we developed high-throughput fluorescence-based cell recognition and signal quantification for cell pellets stained using a very simple microfluidic design. six different types of human breast cancer cell lines, with different her2 expression status, 13 have been studied. the goal was to evaluate an experimental and analytical pipeline to define robust cell line-based controls and diagnostic criteria for future clinical her2 assessment in breast carcinomas."
"the hardware performance monitoring infrastructure (hwpmi) presented in this work expedites the insertion of a minimally invasive performance monitoring networks into existing hardware designs. the goal is to increase designer productivity by analyzing the existing design and automatically inserting monitors with the necessary infrastructure to retrieve the monitored data from the system. as a result of hwpmi the designer can focus on the development of the hardware core rather than trying to include front-end application support to monitor performance. toward this goal, a collection of hardware cores have been assembled, and a series of software tools have been written to parse the existing design and recommend and/or insert hardware monitors directly into the source hdl. hwpmi also integrates with an existing sideband network to retrieve the performance monitor results in high performance reconfigurable computing without requiring modifications to the original application. embedded systems can leverage hwpmi through a dedicated system-on-chip controller which reduces run-time overhead on existing processors in the system. this work demonstrated hwpmi's capabilities across three applications, highlighting several unique features of the infrastructure."
"p l is transmission power of node i. as shown in (13), the transmission power of each node is constrained by its residual energy, network-wide residual energy, and network-wide power consumption. therefore, the network-wide power consumption is balanced by leveraging the residual energy. in particular, the maximum allowed transmission power is proportional to the residual energy of the transmitting node. the fairness constraint factor a can be derived through solving min-max, and then is used for solving opt-f to obtain routing, rate control, power allocation."
"it is obvious that min-max and opt-f are also minlp. so far, some techniques have been proposed to address general minlp problems, e.g., branch-and-bound [cit], outer approximation method [cit] . however, these can only handle small-sized problems. in addition, as shown in opt, min-max, and opt-f, only (10) involving log function is a nonlinear equation. in general, solving a mixed-integer linear programming (milp) problem is much easier than solving a minlp problem. this motivates the efforts to transform the log function into linear functions as conducted in the literature [cit] . the idea is to use a series of piece-wise linear functions to approximate the log function and replace it as shown in fig. 2 ."
"the time interval, t, is the length of samples and is determined as per the work in 13, where the first 500 frames of videos are used to estimate the gmm model parameters so as to obtain a consistent background model. in practice, however, there are many short microscopic zebrafish larvae videos where the number of frames is less than the required time interval, t. as a solution, duplicate video frames are added at the beginning of short videos to allow gmm model background estimation. this process is explained and illustrated in supplementary note in the dataset 14 ."
"from the static hdl profiling and component synthesis stages, six performance monitors were identified for inclusion into the smith/waterman hardware core. figure 10 shows a high-level block diagram of the performance monitors in their locations relative to the smith/waterman hardware core. control reg 0 186 0 186 core status 29540 0 29540 0 aa1 0 2095745 0 2095745 n1 0 2095838 0 93 n0 0 2095838 0 93 gg 0 2095838 0 93 hh 0 2095838 0 the first performance monitor identifies the number of writes to the software-addressable registers in the smith/ waterman hardware core via the plb slave interface (plb slv ipif), the results of which are listed in table 1 . in addition to the register name, number of reads and number of writes, table 1 also presents these reads and writes when run in original and modified modes. the performance monitoring data indicated that several software registers were being written to unnecessarily, and the modified version of the application eliminates these extra writes. this demonstrates the benefit of hwpmi for debugging: the results quickly revealed that the software application was missing a guard, as shown in figure 11 ."
"also identified by hwpmi were additional performance monitors to evaluate the plb master interface (plb mst ipif), which identified that only off-chip memory transactions were performed by the core. moreoever, the offchip memory transactions were 118,144 read-only requests, which is a significant number of transfers and warrants the evaluation of a dma interface. the designer could also adapt the core to leverage an alternative interface, such as the native port interface (npi), to reduce memory access latency, increase bandwidth, and reduce the plb contention. an fsm profiler performance monitor was added that provides feedback in the form of a histogram, to identify the percentage of time each fsm state is active. figure 12 presents the breakdown of the time in each state. this shows that build is the longest running state, accounting for 27.67% of the execution time. the next four states, build score, read ssj, build switch, pop pwaa, each occupies ∼13.8%. thirteen of the remaining states account for less than 1% each, and have been group together in the others category. overall, this profiling data should more quickly focus the designer's attention on the build state, to determine if there is a more efficient way to implement this state. another useful feature of hwpmi is its ability to evaluate designs with different resource utilizations. designers often find themselves adding buffers or fifos into designs without knowing a priori how large they should be. in these cases, hwpmi can collect run-time data as a designer modifies the fifo depth. sometimes these modifications can reveal interesting design tradeoffs, such as those shown in figure 13, where a design utilizing smaller parallel buffers runs more efficiently than one using fewer larger buffers. in order to collect this data, a designer simply modifies the fifo depth, and hwpmi collects the information at user defined intervals."
a. the her2 and ck signals were measured around the border of each cell by making a band of 10 pixel width centered in the border. b. each cell was shrunk by 5 pixels and the mean dapi intensity was measured.
"in this algorithm, a weighed dijkstra's shortest path algorithm is exploited to find the optimal routing. since the energy consumption and residual energy should be jointly considered, the weight should have the following properties: 1) it can reflect the energy consumption relating to the flow rate and channel quality, and 2) it should be inversely proportional to the residual energy of the transmission node and receiving node. the weight of link l, represented by w l, is defined as follows:"
"though the widely used lsrtrack and lsranalyse 1, videohacking 4 and the state-of-the-art approach in 8 explored zebrafish larvae tracking, the video input must be under strict constraint imaging conditions. as reported in 1 and 7, even small impurities inside the water (as shown in fig. 1a ) and lighting reflections (as shown in fig. 1b ) will affect the tracking result. in addition, the small size difference between the zebrafish larvae and the petri dish (as shown in fig. 1c ) and that of adult zebrafish with the fish tank (as shown in fig. 1d ) causes water impurities such as water bubbles (as shown by the red circles in fig. 1e ), excretion, small particles (as shown by the red triangles in fig. 1f ) that are not usually detectable in adult fish experiments but inevitably affect the detection of zebrafish larvae. strict input imaging conditions are impossible to maintain in practice: for example, even if a clean environment is originally used to house the organism, excretions produced by the organisms during the experiment can render it impossible to maintain a completely clean and transparent container background during long-term organism observation. idtracker 5 even explicitly defined the smallest acceptable size ratio between the zebrafish and the tank for creating the clear background environment required for the video data. in addition, these larvae tracking systems 1,4,8 use a petri dish plate to separate individual zebrafish larvae, allowing only one zebrafish larvae in each petri dish to avoid overlapped and swapped trajectories that can result from multiple zebrafish larvae housed in one container (as shown in fig. 1c ). however, limiting experiments to one zebrafish per dish strictly constrains the research application as interaction and grouping behaviour cannot be studied."
"in this work, we presented an ultrafast experimental and analytical method to robustly discriminate her2+ and her2− breast cancer cells at single-cell level. this technique based on microfluidic immunostaining with fluorescence imaging coupled to high-throughput cell segmentation offers a quantitative assessment of the cell-by-cell her2 and ck status in breast cancer cell lines. using the her2-and ck-expression scatterplot, we achieved a strong discrimination of different cell types of various her2 status in a highly sensitive and specific manner, paving the way for standardized her2 protein assessment for diagnostic use. given the current lack of a methodology to precisely evaluate the her2 overexpression in breast cancer patients with robust clinical controls, we anticipate that our quantitative cell-based approach will have an important impact on the creation of clinical standards. in the future, we can apply this method to breast cancer tissues from patients. the clinical results can be thus benchmarked with the cell line staining for an accurate her2 classification, eventually in a high-throughput manner. this study is the first step to realize this aim, giving the opportunity to disentangle the technical variability of the method itself from the biological variability usually present in tissue samples. moreover, its potential coupling with diverse experimental techniques for breast cancer assessment (e.g., fluorescence in situ hybridization, assessment of estrogen/progesteron receptors) and with advanced statistical modeling can provide unprecedented detailed information on key features, such as intratumoral heterogeneity at the cell level."
"on the other hand, the power grid infrastructure, which provides electricity to multi-hop wmns, has been experiencing a dramatic change from the traditional electricity grid to the smart grid where renewable energy is integrated [cit] . renewable energy is usually extracted from renewable resources (e.g., solar and wind) so that no fossil fuel is burn and thus no greenhouse gas is produced. obviously, the use of renewable energy, to some extent, can alleviate the greenhouse gas emission problem. therefore, renewable energy will play a vital role in future wireless network infrastructures (e.g., wireless mesh networks)."
"fig . 6 illustrates the effect of the energy replenishment rate on network lifetime in multi-hop er-wmns. the energy replenishment rate varies in this simulation experiment and other parameters are set the same as described above. fig. 6 depicts the change of the network lifetime with the increasing of the energy replenishment rate, and compares the network lifetime with different algorithms. as shown in this figure, the network lifetime is increasing when the energy replenishment rate grows. owing to the increasement of the energy replenishment rate, the residual energy of each node increases so that the network lifetime can be extended. in addition, opt can achieve the highest network lifetime because the energy consumption of opt is the lowest among these three algorithms. it can also be seen that the curve of eara is close to opt-f, indicating that the proposed algorithm can work very well. fig. 7 reveals the effect of the throughput required by users on network lifetime in multi-hop er-wmns. in this simulation experiment, the throughput requirement varies and other parameters are set the same as described above."
"clear mot consists of two metrics: multiple object tracking precision (motp), which estimates the location precision of all detected objects compared to that of the manually labelled zebrafish larvae positions in each frame (known as ground truth); and, multiple object tracking accuracy (mota), which measures the accuracy in tracking object trajectories (producing exactly one trajectory per object), and the ability to consistently label objects over time. mathematically, the motp and mota metrics are represented as:"
"the motivation for the creation and evaluation of this infrastructure stems from the inherent need to insert monitors into existing designs and to retrieve the data with minimal invasiveness to the system. over the last several years we have been assembling a repository of performance monitors as new designs are built and tested. to increase a designer's productivity, we have put together a suite of software tools aimed at profiling existing designs and recommending and/or inserting performance monitors and the necessary hardware and software infrastructure. this 2 international journal of reconfigurable computing work also leverages existing open-source work by including torc (tools for open reconfigurable computing) to provide an efficient backend for reading, writing, and manipulating designs in edif format [cit] ."
"however, the weighted dijkstra's shortest path can only support single-path routing rather than multi-path routing. thus, this problem should be addressed to make this algorithm support multi-path routing. in order to deal with this problem, the concept of unit flow is proposed in this algorithm. the unit flow is an atomic flow with a constant flow rate, and cannot be split further when it is delivered in erwmns. the selection of constant flow rate depends on the accuracy and computational complexity requirements. let d denote the unit flow rate and a session is usually composed of several unit flows. for session f with nðfþ unit flows, its flow rate rðfþ is by introducing the concept of unit flow, the multi-path routing problem for a session has become a single-path routing problem for multiple unit flows. the weighted dijkstra's shortest path algorithm will be executed to find a routing for a unit flow. therefore, for session f with nðfþ unit flows, the weighted dijkstra's shortest path algorithm should be executed nðfþ times. as shown in (17), in order to get the weight of each link l, transmission power p l should be calculated firstly. through transforming (10), p l can be obtained as"
"to analyze the biomarker expression at the cell level, we developed an algorithm to segment the cells in the images. the procedure are as follows:"
"the result of solving min-max is consistent with that of solving opt when all nodes have same residual energy. compared with opt, the objective of opt-f is to minimize the network-wide energy consumption while making sure that each node i has a minimum power allocation among all maximum allowed power. therefore, solving min-max firstly to obtain a and then solving opt-f can provide a min-max guaranteed power, and corresponding routing and rate."
"zebrafish larvae video dataset. largely due to the time and manual labour required to generate ground truth segmentation and tracking, standard datasets for benchmarking moving objects in video sequences are still emerging. for this work, the authors have not yet discovered any publicly available zebrafish larvae video segmentation and tracking datasets. thus, this paper generated a dataset with segmentation and tracking ground truth annotated per frame, labelling the zebrafish and background for segmentation and tracking accuracy evaluation."
"the performance monitor data is collected with the assistance of hwpmi. in addition to the utilization and interface performance monitors, an additional monitor was added that yielded a highly beneficial result. this is an example of supplemental data collection. when a designer needs to collect additional information, hwpmi offers the ability to add custom monitors without the need to augment how the system will retrieve the data. this can be especially useful for quick one-off data that might only be useful for a short period of time. rather than manually adding the infrastructure to the original core, only to remove it later-or retain it and waste resources-hwpmi can collect the data quickly and efficiently. figure 14 shows a histogram of the number of steps taken for each input number to be reduced to one. another interesting monitor is the processor interrupt monitor. for latency sensitive applications with processorto-hardware core communication, an interrupt is often used. however, configuring the interrupt or optimizing the interrupt service routines is critical. in the case of collatz the time for the processor to respond to a single interrupt was measured as ≈11.12 µs."
"initial hwpmi development was targeted to support high-performance reconfigurable computing systems, such as spirit. however, the tools and techniques are also easily adapted to support more traditional embedded system development with fpgas. in fact, the sideband network can be replaced with a bus interface to give an embedded system access to its own performance monitoring data. while this introspective monitoring does add to the runtime overhead of the system, designers can now specify the interface mechanism to hwpmi. powerpc 405, powerpc 440, and microblaze systems are supported through interfaces with the processor local bus. an embedded system example is shown in figure 5, where a separate soc provides independent monitoring of hardware cores. in this case, resources are required for the extra soft-processor, buses, and peripheral ip cores, in addition to the monitoring infrastructure. the benefit of this approach is that no modifications are necessary in the original device under test (dut)-no changes to the software running on the dut's processorso the performance overhead is minimized."
"when a performance monitor is created there is a set of criteria that must also be included to allow the recommendation to take place. for example, there is a plb slave interface performance monitor which specifically monitors reads and writes to the hardware core's slave registers. all signals are identified during profiling, but until these signals are matched against a list of predetermined signals, there is no specific way to identify when those signals are being written to. another example considers finite-state machines: once an fsm has been identified by the system, it is trivial for the respective performance monitor to be recommended for insertion. the actual insertion of the performance monitors is done at the hdl level. each performance monitor's entity description and instance are automatically generated and inserted in the hdl."
"7. the thresholded image was inverted and segmented based on the voronoi method, which partitions the image by lines of points having equal distance to the borders of the two nearest particles."
"at this point the design has been analyzed in order to recommend specific performance monitors for insertion. the designer can choose to accept any number of these recommendations, from a report like that shown in figure 7 the monitors are stored in a central repository which can be augmented by the designer if a specific monitoring capability is not available. the monitors all are encapsulated by a performance monitor interface, shown in figure 8, which connects the monitor to the performance monitor hub and includes a finite-state machine to retrieve the specific performance monitor data and forward it to the hub. to aid in the insertion of hwpmi into existing hardware designs, a software tool has been developed, the hwpmi system insertion tool. the purpose of this tool is to insert the system monitor hub and sideband network interface cores into the top-level design. the tool inserts the monitors directly into the vhdl source, prior to synthesis, map, and par."
"in general, when a session is delivered on a link in multihop er-wmns, the energy is mainly consumed due to data transmission and reception. the receiving power is denoted by p rec, which is considered as a constant in this paper. the transmission power is represented by p l when link l is active. it is obvious that transmission power is a variable parameter that is related to the quality of the link and rate allocation. let x l be a binary variable indicating whether link l is active or not, i.e.,"
"wmns. the min-max fairness model is proposed to address the tradeoff between the power consumption and residual energy of each node. 3. due to the high computational complexity to solve the minlp formulation, an energy-aware multi-path routing algorithm (eara) is proposed to practically deal with the joint control of routing, rate, and power in multi-hop wmns. this algorithm uses a weighted dijkstra's shortest path algorithm to search an optimal routing, in which the weight is defined as a function of the power consumption and residual energy of a node. moreover, the concept of unit flow is proposed to address the issue that the weighted dijkstra's shortest path algorithm cannot support multi-path routing. the remainder of this paper is organized as follows. section 2 describes the network model, including the considered network scenario, session flow model, and power and energy consumption model. the network-wide energy consumption under network throughput constraint is formulated as an minlp problem and fairness is also considered to address the uneven routing problem in section 3. section 4 proposes an energy-aware routing algorithm that can be used in practical multi-hop wmns. extensive simulation results are presented and analysed for performance evaluation in section 5. finally, section 6 concludes this paper. orthogonal channels are used by all links so that the interference can be avoided. it is noteworthy that the number of channels is as many as the number of active links because a channel can be reused spatially. in addition, the time-division system is considered here, where time is divided into slots with equal length t, and t refers to the t-th discrete time period. we assume that no new session occurs during a time slot so that the routing, rate control, and power allocation cannot be affected."
"as zebrafish (danio rerio) larvae have emerged as a vertebrate and mammal model for many biomedical applications including screening for biochemical abnormalities 1 and behavioral science investigations 2, the tracking of the larvae has emerged as a challenge. typical manual tracking approaches are tedious, commonly requiring significant periods of manual observation and labelling of the image features to represent the activity for a single experimental task 3 . furthermore, as a subjective manual task, the results are difficult to reliably repeat and reproduce. recent research attention has therefore focused on the development of automatic multiple zebrafish larvae tracking systems due to the increased availability of digital microscopy and video storage systems."
"hwpmi is a hardware-accelerated implementation of the smith/waterman algorithm, commonly used in protein and nucleotide sequence alignments [cit] . the particular implementation on the fpga was developed as a proof of concept [cit] for accelerating the flocal align() function of the ssearch program-an implementation of the smith/waterman algorithm from the fasta35 code package [cit] ."
"in recent decades, technology has had a huge development by allowing the design of more complex programs, by using new software development techniques, new data mining algorithms, and more efficient software architectures. likewise, the integration of the emotional state of the user with technologies like neural networks, genetic algorithms, and fuzzy logic produces in the case of educational software a new way of permeating awareness among students and software."
"next the system uses a fuzzy logic engine to obtain a new user´s level by combining the current level, exercise´s variables, and the emotional state variables. the process to obtain a new level is performed every time the user completes an exercise."
"the ats starts with a step of login for authentication and registration of users. in case of a new user the system proceeds to realize a diagnostic test, which consist in a set of multiple choice questions. the result of the diagnostic test is the initial difficulty level of the student. the next step presents an exercise for programing in the java language."
"given the growing demand for learning programming languages, it is necessary to use new tools using modern techniques such as emotion recognition, which motivate and facilitate student learning. at present, supporting tools for teaching programming languages only take into account cognitive aspects of the student, ignoring other behavior features like user´s emotions. studies have shown that emotions are closely related to cognitive processes such as learning [cit] ."
the main contribution of this work is the use of a bimodal emotion recognition (facial and spanish textual recognition) inside of an intelligent tutoring system for learning java.
"the system uses a set of fuzzy rules to identify the complexity of the next exercise where six aspects are considered: student´s level (beginner, basic, intermediate, and advanced), validation of last exercise, number of compilations, identified facial emotion, time spent in exercise and identified text emotion. we used a java library named jfuzzylogic for implementing the fuzzy system and a new semantic algorithm (asem) for textual dialogue. the evaluation allows the ats to identify the complexity level of the next exercise which can be lesser, greater or equal to the current level. an example of a fuzzy logic rule is shown next (figure 2)."
"during the training phase, we used an interaction which allows the communication with student, where some processing text is recorded. next the emotion is generated according to the existing words found in the original corpus, showing the parameters used in the emotion generation, and also those words which could not be found in the corpus (they are also added to the corpus newwords). for each new added word an expert define the pfa. this first phase is realized in an iterative form until the new corpus has found the expected success level."
the rate of success of the emotion recognition was 80%. whenever the student comments had more than 3 words that couldn't be found on the corpus sel the emotion recognizer produce an incorrect results (incorrect emotion). the emotion recognizer achieves better results by adding those words in the corpus sel. finally whenever there was no match between the facial and textual emotion we choose the textual emotion as the correct one because most of the time the text emotion recognizer produce better results.
"as we explain before in section 1, a set of variables related to the solving of the problem define the new level of the student and the new exercise (see figure 5) . once the student solve the exercise, a fuzzy inference engine defines the next exercise using a set of variables previously mentioned. the main interface of the java zenzei is shown in figure 6 with some brief explanation given in gray boxes. when a student ends an exercise a small windows is displayed asking to add comments related to that exercise (see figure 7) ."
"a recent version of autotutor named affective autotutor [cit] adds capabilities to recognize the affective state of a student by using facial expressions, body language (posture) and textual dialogue recognition. it detects when the student is bored, confused or frustrated and intervenes through a pedagogical agent."
"this work integrates diverse technologies into the affective tutor named \"java zenzei\", such as the use of a neural network (using weka) and a feature extractor for recognizing facial emotions, as well as a semantic algorithm (asem) to extract emotions from textual dialogue. this paper is divided as follows. the second section describes related work. the third section mentions the effort done in the development of a bimodal recognizer that was used in our research. the fourth section describes the tutoring system operation. the fifth section shows results of the proposed work. finally, the sixth section presents conclusion of the work."
"there exists a lot of research about multimodal recognition to detect emotions taking into account different aspects. for example, soleymani [cit] combines eeg signals with eye tracking that can be considered bimodal recognition. there are also a lot of work with multimodal recognition like mert [cit] that works with face, voice, and body motion."
"-there are many words that were not found in the original corpus (2,036 words) from a total of more than 300,000 of the spanish language. -most students don't use accent in words."
"at the beginning intelligent tutoring systems used to be implemented with traditional principles of behaviorism, moving later to other more interactive and dynamic learning theories, in which the subjects interact in virtual learning environments. affective tutoring systems (ats) are intelligent systems that incorporate the ability to recognize the emotional state of the students allowing the user to interact with exercises that stimulate their emotional state [cit] ."
"we perform some initial test with graduate students from the instituto tecnológico de culiacan. the exercises were designed for students with basic knowledge of java language. in the test there were 9 students who finished the exercises in a short time and with few errors. that allowed students to move faster to more complex exercises. table 1 shows the results obtained from the test. at table 1 we can see the student e06 for example, started at an intermediate level, had a short time solving the exercises, got just 1 compilation error, and the average emotion found was happy, so eventually the its promoted him to advanced level."
"the system has a log in component which identifies and authenticates the user. there's a component to get the user´s current difficulty level by searching at the database, in case the user is new, a diagnostic test module is executed, which shows a questionnaire to be solved, to determine the starting level of the user. the system shows an exercise based on the user´s current level. when the exercise is solved, the exercise´s variables (exercise validation, amount of compilations and time required) are obtained, also the emotional state variables (facial and text emotion) are extracted by taking a photo of the user and asking a question about the exercise."
"as future work we expect to add more words to corpus semantic, so we can increase the rate of success of the recognizer. another upcoming work is to integrate more exercises with different rates of complexity with the goal of having a more complete ats."
"-if the semantic word is found in the corpuses, the word features (pfa and emotion) are extracted. -the emotion in classified according to the features of the word. -the emotion with greatest intensity is produced. figure 3 shows the asem algorithm that can recognize the emotion from text. to get the emotion we use pfa and eiv (enhancer/inhibitor) values for every word found in the text written by the students. the whole process is explained next (figure 4): -the next semantic word is obtained. -this word is sought in corpus semantic. if found, the emotion and pfa value are extracted. -if the word is a negative adverb, the emotion of the next word is switched (e.g. happy to sad). -if the word is a superlative adverb, we identify the word as a pre-adverb or a post-adverb. in case of being a pre-adverb we enhance the pfa, and eiv of the next semantic word. in case of being a post-adverb we enhance the pfa and eiv of the previous last semantic word. -finally we use the sum of each found emotion in the text dialogue, and then we determine the emotion by the greatest value."
"-the text is normalized: the accented words, numbers, and special characters are removed and uppercase letters are converted to lowercase. -non emotion words like he, she, the, etc. are removed by using the corpus stopwords. -semantic words are sought in corpuses semantic and improper words. if semantic words are not found in the corpuses the new words are added to corpus newwords."
the emotion recognition is a complex task and to date the rates of most recognizers do not get 100% of success. so using bimodal recognition is a way to increase the rate of success in the recognition. next we explain the structure of the java zenzei tutor.
"based on the obtained results we stablish that combining both emotion recognizers (facial expression and text dialogues), add precision to the work of identifying the emotional state of a student. we found that the precision of the text-dialogue recognizer mostly depends on the number of words stored in the corpus semantic."
"for emotion recognition from text, we implemented a semantic algorithm (asem) that allows identifying emotions from text dialogues using semantic labels [cit], where the user writes different comments for answering questions received in a random form. this semantic algorithm is based mainly in a word corpus called sel [cit] which was built by experts with an output emotion and a probability factor of affection (pfa). the asem algorithm incorporates semantics rules that allow emotion detection with a rate of success greater than 80% according to different test performed with graduate students. the rate of success increases by adding new words together with the corresponding pfa which must be computed by an expert. the emotion recognition is made in two phases: training and application."
"-there are words with no emotion (neutral emotion). these words integrate a corpus named stopwords (e.g. the, he, or she) -improper words are eliminated from the text but they are considered as having a negative emotion. -the emotions identified by the text recognizer are joy, surprise, neutral, sadness and anger. -words that deny a sentence (e.g. no or never) change its meaning (valence) and they are already stored in the semantic corpus. -words which define the intensity of the words (e.g. very, few, nothing) are already stored in the semantic corpus."
"the recognition method for emotions works with two recognizers, where the first one is a method for emotion recognizing through face detection. in this process, the system take a picture of the student when he/she is solving the exercise. we use a feature extractor for the face which was implemented with opencv library and a neural network implemented in weka for classifying emotions. the emotions identified by the facial recognizer are: joy, surprise, sadness, anger and neutral emotion."
"in this paper we presented a novel intelligent tutoring system for the support of learning the java language. this system uses two emotion recognizers that work through text and facial expressions. we made different tests of the ats with graduated students of computer science, producing the next results: the tutoring system based on facial expression is able to recognize emotions with a success rate of 80%. the emotion recognizer based on text dialogues achieved a success rate of 85%. the fuzzy system always produced the adequate exercises according to stablished parameters. for example, if the student was boring, had few errors, had few compilation and executions errors, and had a short time to solve the problem, the system increase the complexity of the exercises. on the other hand, if the student was not boring, had few errors and a greater time of solving the problem, the complexity of the next exercises remains the same. but if the student had too many errors and spent a lot of time solving the exercise, then the system decrease the complexity level of the next exercise."
"the ats structure (figure 1) presents a clear idea of how the system works. persistence of the information required by the tutor like student´s information, exercises, answers for each question, information of each exercise to be executed, is stored in an object-oriented database (db4o)."
"principle 9. the birth sequence of nodes in the tree is from an upper layer to a lower layer (for a subtree). that is, the growth of the tree is from upper to lower. thus, users are added into the tree from upper to lower sequentially."
"(2) f2ac can manage access control such as merge, delete, and retrieve user or privileges in a lightweight manner via a proposed access control model, directed tree with linked leaf model."
"(1) it is required to separate the authentication for logging in to cloud and the authentication for access control, namely, user name in ucl. it will simplify the implementation and management of user authentication at cloud. the security of original authentication system of cloud will not be damaged. it is also easy for user to understand (for better user experience) and conduct user addition for file accessing."
"(4) each node links to one or multiple properties, called nodelink. each link has two related properties. one is accessfileset; the other is oneprivilege. accessfileset property is a set consisting of one or more files; oneprivilege property is one privilege for files in accessfileset. each node may have more than one accessfileset, but each accessfileset has one corresponding oneprivilege (namely, the largest privilege). that is, each node links to one or more sets that represent access files, and each set has one bit to represent the privilege for this set. we look on one or multiple properties as linked leaf (leaves) for this node. the model is thus called a tree with linked leaf (leaves). note that linked leaf is not a leaf node; leaf node is a node that has no child, but linked leaf is a property for every node."
"currently, mobile storage cloud services impose a typical security problem, access control for distributed users who will access the shared file in cloud servers. in particular, the related access control policies should be determined by user themselves, which result in sophisticated requirements. moreover, many service providers simply assume that the other users have almost the same access privileges as the original user who creates and uploads files. it can simplify management logics at cloud but raise the risks of file leakage at client. that is, the other users can arbitrarily read, modify, and update uploaded files, which usually imposes security risks in mobile scenarios."
"(5) 3 can be edited by (by herself, as 3 is written by and will be reported to manager ). note that, hereby 3 can be edited by and cooperatively."
"there exist at least three major entities in file storage cloud: an entity who uploads files and shares mobile information systems 3 those files, an entity who is asked to access these files, and a storage server who is at cloud side. simply speaking, uploads files into . gives login account information to . logs in and accesses files according to her privilege that is set by ."
"in this paper, we proposed a lightweight, fine-grained, and flexible scheme, called f2ac, for access control in multipleparty file editing and sharing in mobile cloud computing. f2ac can support dynamically adding and deleting users in an ad hoc group, privilege self-defining as a creator's proxy or team leader, transitively authorizing privileges for members in subteams, transitively revoking privileges, and separating of access authentication from system authentication. the directed tree with linked leaf (leaves) model is proposed for lightweight implementation and verification. the leaf merging and node merging are described for lightweight storage and fast retrieval of privileges. the future work could be the evaluation of linked leaf model in some mainstream cloud services such as apple icloud, baidu cloud, and alibaba cloud."
"when the number of uploaded files and shared users increases, the fine-grained access control of those files for those users is mandatory. it is worth to note that because access control policies are not determined by the administrator of cloud servers, control methods should be so easy that ordinary cloud users can understand and conduct straightforwardly. it should be flexible in that access control policies are user-centric and can be defined on demand via operational interfaces provided by cloud servers. moreover, the access control should be lightweight; otherwise, the response delay upon accessing will not be endured and user experiences will be worse to damage the qos (quality of service) of storage cloud services."
principle 6. the merging set of accessfileset in leaves on a child of a node (not root) is upper bounded by accessfileset in the node's leaf whose oneprivilege is authorize. it is similar to principle 2.
"(2)separation of tv logo region. according to the information given by tv logo template, the logo region is separated into the master region and sub region."
"step 11 (other users: login and present token). after user logs in to cloud, she will be asked for showing her token by cloud. after user responds with her token, cloud will conduct processes similar to step 10."
"step 5 (cloud: store records into ). after user uploads all files and sets up all options for access control, cloud will store corresponding records into acl."
"in this paper, we propose a scheme called f2ac (lightweight, fine-grained, and flexible access control) to tackle the above challenges. we present and analyze the design rationale in an incremental way for better understanding. some formal presentations are presented for better clarity and rigorous generality."
"principle 10. usually, a physical person has one token (and corresponding username). our scheme has the flexibility that one physical person can be given more than one token. there is another way to achieve the functionality that more tokens are held by one physical person. that is, usually tokens in ⟨, ⟩ are distinct for different username, but same token for different username will achieve the goal of more tokens at a physical person. (the reason is that token is used for the authentication of access control.) principle 11. one node cannot have two parents (fathers). that is, if one username with corresponding token is created and assigned, the username will not be reused by other nodes for assigning. this principle will simplify the tree. nonetheless, the f2ac can also achieve that one node has more than one father. the method is that you add the node at first and let two children's token be the same."
"after initialization, mutation operator is applied to generate the mutant vector v i,g en for x i,g en . this step is the core operator in de, which aims to increase the diversity of population and avoid plunging into local optimum. the mutant vector is the vectorial sum of a scaled difference of two individuals and a third individual:"
"for the case of 0.0 db target net gain, the power evolutions of the signals and pumps along the fiber are shown in figs. 7 and 8. fig. 7 shows the signal powers of the selected 6 channels gradually decrease and then increase to the original level. fig. 8 shows the power evolution of four backward pumps with different pumping wavelengths. it can be observed that the powers of the four pumps present an overall decreasing trend from 25 km to 0 km. this is because the pumps lose power to the signals due to raman interactions between pumps and signals. in addition, the pump with the shortest wavelength always loses power to other pumps and signals, and its power attenuates quickly along the fiber length. and the pump with the longest wavelength suffers from very small attenuation initially and then its power decreases after some distance, because it can receives some power from the other pumps at the start and it still contributes the great amount of power to amplify the signal power. through the assignment of the wavelengths and power for the four pump, good flatness of raman gain can be obtained. fig. 9 presents the changes of the values of net gain and gain ripple in iterations of the de. de is used to optimize the net gain and gain ripple simultaneously. we can see that the net gain converges steadily to 0.0 db level and the gain ripple can quickly reach the optimal level. the algorithm is rapidly converged at about 60th generation."
"for example, user uploads file, but user is willing to let user be a proxy of her for managing the file editing. policy 4 (transitive of ℎ privilege and additive users). this one will be most complicated and flexible. it combines functions in policies 2 and 3. that is, the user with authorize can assign authorize privilege to others, and the user with authorize privilege can also add more users for current shared files."
"in most tv programs, the backgrounds of tv logo have the following features: the background is simple and there is no complicated interference; complicated background of tv program surrounds the region of tv logo but do not cover the region; tv logo is located in the complicated background, and the hue of background differs from that of tv logo; tv logo is located in the complicated background and the hue of background is similar to that of tv logo."
"(3) the lightweight property requires that computational overhead should be managed in cloud side, as the total number of users in storage cloud is always huge. thus, cryptographic algorithms should be avoided as least as possible. the computational overhead at mobile clients should be as least as possible for better performance in energy consumption and user experiences."
"(1) acl is a step 1 (user: firstly login and upload). for the first time login, user can upload one or more than one files into cloud ."
"principle 12. the deletion of a user can be done by the deletion of a node. if the node has children (namely, not a leaf node), the deletion of the node will let the node's father be the node's children's father. it seems to replace the team leader. if the node has no children (at lowest layer), the deletion of the node will be done directly. certainly, the deletion of a node will remove the node's linked leaves together."
"model. policy 4 is the most flexible (or powerful) one in all policies for file sharing in a dynamic group or a group with a large number of members. intuitively, someone may suspect that this policy may result in some inconsistency due to the complexity of user addition, deletion, and transitive authorization. to make it clear, we propose a concept and implementation model for f2ac, which presents as a directed tree with linked leaf (leaves)."
"is the pump power vector, n is the number of pumps. the function is the rfa model to be constructed. once the mapping is known, the gain can be calculated quickly without time-consuming integration of the raman coupled equations. in this way, the entire calculation process becomes a black box and the main task is converted to solve a mimo regression problem. to develop a fast and accurate method, we employ elm to construct the model of rfa."
"in contrast, if user logs in with user 's account, instead of user 's, user will be regarded as user 's file sharer and be requested for showing the token that is set by user ."
"in this paper, we propose an efficient hybrid optimization approach for rfa design by integrating machine learning algorithm called extreme learning machine (elm) with the differential evolution (de) algorithm. to our knowledge, this study is the first attempt to introduce randomized neural network technique [cit] to optimize multi-pumped rfas. elm, one of the randomized neural network technique [cit], has outstanding advantages of extremely fast learning capability, high generalization performance and free of parameter tuning. to solve the problem of low efficiency, elm model for raman amplifier is established to replace the time-consuming integration of raman coupled equations. in the hybrid method, de algorithm, a simple yet powerful evolutionary algorithm [cit], is applied to optimize the pump wavelengths and pump powers with the desired flat gain spectrum. the elm model is incorporated into the evolution of the de algorithm and accelerates the search process. the hybrid approach exploits both the global search ability of de and the high computation efficiency provided by elm model. this combination can be a very effective way to enhance the speed and stability of the optimization for multi-pumped rfas."
"step 7 (cloud: store records into ucl). after that, cloud stores corresponding records into ucl. for example, ⟨, ⟩, ⟨, ⟩, ⟨, ⟩."
"recently, access control in storage cloud has attracted more and more attention [cit] . however, in those methods a fine-grained, flexible, and lightweight solution has not been thoroughly explored. we make the first attempt to solve it in this regard. however, such an access control scheme poses three challenges as follows:"
"(2) each nonroot node has one parent (namely, father). anyone of these nodes has a directed edge pointed to its father. the father is the user who adds a child node or children nodes in the tree. only users who have authenticate or create privilege can add a child or children. the father of a child can be fetched from acl by looking up bywhom column."
tv logo implies semantics about tv channel name and program tendency and is one of the important semantic sources for video analysis，comprehension and retrieval. it is widely used in television monitoring and media assets management.
"with the pervasive usage of mobile handheld computing devices such as mobile phones, tablets, and laptops, mobile business processing becomes possible and grows largely during commercial traveling. as storage services in cloud, such as google, alibaba, and baidu, are freely provided, users may rely on these services to share and edit business files with others remotely and cooperatively. for example, after one user creates or uploads a file into cloud servers, the others can access the file remotely and edit the file cooperatively."
"step 10 (cloud: retrieve ucl and acl). after user responds her token, cloud will retrieve ucl to get the user name, namely user . cloud lists all files for user by retrieving acl according to her privileges."
"4.1. basic setting. there exist two tables for facilitating control mechanism at cloud. we denote them as cloud & ⟨, ⟩, where & can be looked as \"is defined to.\" acl is used for access control; ucl is used for user authentication."
"principle 14. for simplicity and better understanding, we do not include the further accessing conditions, denoted as, in the design of linked leaves. indeed, it is without loss of generality. the can be considered as an extension of, as specifies more requirements in accessing policies. therefore, nodelink can be extended into ⟨, v, ⟩ if required."
"(1) users are organized in a tree structure. each user presents as one node at the tree. the root of the tree is the user who uploads all current sharing files, namely, one who has privilege for those files. all users in the tree share the same login account information as the root, but they are distinguished via their tokens."
"(2) in step 2.2, one privilege is already enough, as relationships between privileges are subset (or superset). the highest (or largest) privilege is set for one user aiming at one file."
"the rest of the paper is organized as follows. section 2 gives an overview on relevant prior work. in section 3 we discuss the basic assumption used throughout the paper. section 4 provides the detailed description of our proposed models and analysis. finally, section 5 concludes the paper."
"the experiments are performed with intel(r)core(tm)i7-2600cpu@3.40ghz and the software platform is windows7 system, vc6.0, opencv1.0. the video sequences are captured from the broadcasting tv programs in china. the size of image is 640*475 and the number of tv logo models in data base is 53. table 1 gives the recognition rate of 12 different tv logos. in this paper, we select cctv and some provincial tv station logo as experiment sequences. there are total 3900 frames and the threshold of the number of keypoints for sift is 3. we can see from table 1 that the recognition rate can achieve approximately 95%. in addition, this method is also effective for transparent logo. in table 1, the logos of cctv1, cctv2, cctv3, cctv5, btv-1, btv-2 and btv-3 are all transparent and the recognition rate for these logos are also high."
"note that, for consistence, if a none-leaf node is deleted, the node's all children will replace their fathers by the node's father. (all children's bywhom will be replaced by the node's bywhom.) (2) a user's privilege for a file can be modified by anyone who has create or authorize privilege for this file. for better understanding, we can look users with creat and authorize for a file as team leaders for this file. other users with privileges such as update, modify, or read for this file will be considered as group members for this file."
"m(x, y) is the gradient magnitude, and θ (x, y ) is the orientation. l is the convolution of a gaussian mask to image in the first stage. from the gradient orientations of the sample points within a region around the feature point, the orientation histogram, which has 36 bins covering the 360 degree range of orientations can be computed. with the gradient magnitude and a gaussian-weighted circular window each sample is weighted before inserting them into the histogram. 4) creating the feature point descriptors --sift feature vectors."
"principle 13. the modification of user's privilege can be done by modifying the oneprivilege on the linked leaves of the node. update, modify, and read can be changed into each other among them. update, modify, and read can be changed into ℎ . we reserve the flexibility that ℎ can also be changed into update, modify, and read, but it may need to delete subtree of this node for the consistence of principles."
"where l is the transmission length. p s (0) is the input signal power, p s (l ) is output signal power at the end of the fiber. in general, the analytical solutions of the raman coupled equations cannot be calculated directly owing to its complexity. numerical methods, such as runge-kutta method, the shooting algorithm and the average power analysis method, are widely applied to get the approximations of this problem. however, the calculation process is quite time-consuming, which directly restricts the search efficiency of the algorithm for optimum design parameters of pumps. in this paper, a novel idea to solve the model of rfa is presented by using the technique of extreme learning machine. under specified system parameters, the multi-pumped rfa can be regarded as a multi-input multi-output (mimo) system, where the wavelengths and powers of multiple pumps are the input of the mimo system,and the net gains of signals are the output. this input-output relationship of the rfa model can be described as"
"we consider a backward-pumped rfa design, where the pumps transmit in the opposite direction of the signals. multiple pumping scheme is used to achieve gain flatness for the rfa. the schematic diagram of applying the extreme learning machine (elm) technique to a backward multi-pumped rfa communication system is shown in fig. 1 . the system consists of forward signals, backward pumps, optical spectrum analyzer (osa), dual port wdm analyzer and the proposed elm model. for the multi-pumped rfa, the selection of the pump parameters (i.e. pump wavelength level and pump power level) directly determines the gain performance. therefore, the goal of multi-pumped rfa design is to search for the appropriate wavelengths and powers of pumps in order to meet the desired gain response and guarantee the amplification performance. as an important part of design procedure, the mathematical model of rfa is the foundation of realizing the optimization."
"elm is an efficient learning method for single hidden layer feed forward networks (slfns), which has been successfully applied to handle a broad range of regression problems. the attractive features of elm are its extremely fast learning speed and its strong generalization performance [cit] . different from the traditional neural network methodology, elm is a one-pass algorithm. it randomly assigns the input weights and the biases of hidden nodes instead of adaptively tuning them, and analytically determines the output weights by matrix operations without iterative calculation. here, elm is utilized to model the nonlinear functional relationship between the pump parameters and the corresponding net gain of rfa. as a supervised learning algorithm, the training process is required to establish the desired mapping from the input to the target output by means of learning from the training samples. for multi-pumped rfa, the wavelengths and powers of pumps are adopted as training input, and the net gain is the training target. each training sample of input-output pairs is collected by randomly setting the pump parameters used to generate the net gain spectrum. the detailed training process of elm is described below."
"in this paper, a method of tv logo recognition based on sift is proposed which uses the structure and color features of tv logo and combines the region separation and sift matching. experiments results show that this method has a good performance in tv logo recognition, especially for transparent logo."
"generate the mutant vector v i,g en according to (13) . 7: /*crossover*/ generate the trial vector u i,g en according to (14) . 8: /*selection*/ evaluate the fitness of trial vector u i,g en ."
"(3) f2ac can permit users to define various access control rules as they demand and separate the access control for system users and file users, which simplifies user experiences and management flows in cloud."
"where h † denotes the moorepenrose generalized inverse of matrix h. in summary, the training process of elm is presented as algorithm 1. after training process, a well-trained elm model for rfa will be obtained. we can use the welltrained elm to directly compute the net gain of rfa for arbitrary pump wavelength and pump power entered the model. the elm approach for calculating net gain is powerful in computation efficiency because it can avoid the time-consuming solving process of the raman coupled equations and also can save the subsequent optimization time. then, we incorporate the well-trained elm model into the optimization process of de algorithm. the optimization process will be described in the next section."
"where a i is the weight vector connecting the input neurons and the i th hidden neuron, and b i is the bias of the i th hidden neuron. g(x) is the activation function and the sine function is used here. β i are the output weights connecting the i th hidden neuron and the output neurons. we can rewrite (4) in the following form:"
"(3) usually, create can be only assigned to the user who uploads the file into cloud, by cloud automatically in step 2.1. in the following sections we will extend it for more flexible functions in more complicated application scenarios."
(6) acl and ucl can be constructed by the above tree model. the retrieval of acl and ucl can be accomplished by underlying tree data structures and algorithms.
"according to the master region classification, the first step is to match the master region template, and then match the corresponding sub region. the image in which the number of keypoints is larger than threshold 0 q is the best matching image."
"(3) each node has three properties, namely, username, token, oneparent. username and token are assigned by node's father. the property oneparent points to node's father."
"the optimization design of rfa is the process of determining the optimal pump wavelengths and pump powers that can yield the desired flat gain spectrum. in this study, two performance metrics are taken into account: net gain level and the gain ripple. our objective is to minimize the average error between the target net gain and the actual output gain with the minimal gain ripple. according to the above-mentioned elm model, the output gain can be written as"
"here we used 16 orientation histogram aligned in a 4x4 grid. each histogram has 8 orientation bins. this 4x4x8 elements is combined as the sift feature vector. after being constructed, this 128-element vector is then normalized to unit length. finally, a normalized 128-element float type vector is created as the sift keypoint descriptor."
"the method of tv logo recognition based on sift is proposed in this paper. it successfully suppresses the noises and complex background. moreover, associated with tone matching and blocks, it can narrow down search of sift and reduce calculation amount. in the experiment, the method adapt to changing background of tv logo, with advantages of rapid and accuracy. in the future, we will focus on the development of rough matching and try to look for a method to minimize the search of sift."
"(2) the fine-grained property requires that user privileges should be easily defined, changed, revoked, verified, and managed via various rules and policies. and especially, those can be determined and controlled by cloud user themselves. the access control mechanism should smoothly make it possible for users to edit shared files remotely, cooperatively, and securely."
step 6 (user: assign ucl). user sets up corresponding token for each user in acl including herself by checking options (in user interface) provided by cloud.
"step 9 (user: secondly login and present token). after user logs in to cloud at the second time, she will be asked for her token by cloud. the reason is that user is in ucl at this time."
"(2) 1 and 2 are two files that will be edited by a team led by ( is the leader of team 1, e.g., technical team)."
"where a, b, β are the weight, bias and the output weight of the elm network, respectively. s is pre-determined according to the rfa design problem. the gain ripple in band is defined by"
"(1) each user has only one possible bywhom in acl. once a user is added into acl, the bywhom field will be set. for better understanding, if we consider bywhom as a parent (namely, father) and consider each user as a node, user graph will be considered as a directed tree. the direction is from a child or all children to the father. recall that, for user deletion, only the father can delete directed child or children in the tree (if user 's bywhom in acl is user who currently logs in to cloud, user will be presented an option on whether deleting or not in user interface)."
"separation of tv logo region plays an important part in this method. the master region usually represents the tv station and sub region represents different channels belonging to the tv station. in other words, for a certain tv station, almost all channels have the same master region. figure 3 gives an example of the separation of tv logo region. the region of tv logo in figure 3 is separated into two parts. the part of \"btv\" represents beijing tv station and is the master region. the part of \"财经\" represents finance and economics channel and is sub region. in order to introduce less background interference and noise, the edge of the region should be as far as possible close to the logo. 1) constructing the scale space of an image, detecting the scale-space local extrema. to efficiently detect stable keypoints locations in scale space, lowe [cit] has proposed the scale space, the difference-of-gaussian (dog) function convolves with the image:"
"definition 1 (original user). this is the user who applies for an account from a cloud storage service for uploading files, for example, alibaba's aliyun. she uploads files into the cloud server. she is denoted by user in further examples."
"next, the design results of 4-pump rfa optimized by the proposed hybrid approach that combines de algorithm and elm model are compared with the results of other methods in previous work, where particle swarm optimization (pso) and ant colony optimization (aco) are chosen as the optimization algorithm, sha and apa are applied to calculate the gain,respectively. for all algorithms, the population size was set to be 15 and the maximum generation was set to be 100. comparative experiment was conducted under the same pc environment and the parameters of rfa also remained the same. we used these four methods to optimize the pump parameters of the same type of 4-pump rfa with the target of 0.0 db. the results of net gain, gain ripple and the total optimization time are listed in table 3 . from table 3, the net gain values are all close to 0.0 db, which meets the system requirement. the proposed method obtains the gain ripple of 0.3793 db within 0.1865 s. the gain ripples of pso+sha, pso+apa, and aco+apa are 0.8742 db, 0.6221 db, 0.6318 db. and the corresponding time costs are 37368 s, 4235 s, 10502 s. it can be seen that the proposed method achieves the relatively smaller ripple with the shortest time. compared with other methods, the proposed method yields good gain performance and meanwhile greatly enhances the optimization speed by several orders of magnitude. it indicates that the proposed method combines well with the characteristics and advantages of de algorithm and elm model. it has made a significant improvement in the global searching ability and computation efficiency."
"the experimental setup of the backward multi-pumped rfa is shown in fig. 1 . we apply the proposed method to design the gain-flattened rfa in c+l band with the following physical system parameters. the wavelengths of signals range from 1530 nm to 1620 nm in 1 nm spacing, and the initial values of the signals powers are 0.1 mw. the wavelengths of the pumps can be adjusted within the range from 1420 nm to 1520 nm, and the wavelength interval between any two pumps is allowed to be 1 nm spacing. the powers of the pumps are within the range from 10 mw to 500 mw. the fiber length is 25 km. [cit] environment, and all calculations were performed on a 64-bit computer with intel core i7, 3.4 ghz processor and 8 gb of ram. to validate the effectiveness of the proposed method, three parts are numerically studied in this section. first we need to choose the number of hidden nodes of elm and then train the elm model for the rfa. next, the optimization process is performed for the cases of 4-pump rfas. finally, some performance comparisons are presented to evaluate the proposed method."
"(1) the flexible property requires that the proposed method must tackle complicated situations such as highly dynamic ad hoc groups, in which accessing users can join and leave groups conveniently. moreover, all users may not be in the same layer (in terms of file sharing relationships). that is, a leader of a group can be authorized by the initiator and the leader can further assign and revoke privileges to other group members. a subgroup (or sublayer) can be formed in a group or a layer, so the authorization can be conducted in an iterative manner."
"in this paper, an optimization method based on the combination of elm model and de algorithm is presented for the gain-flattened multi-pumped rfa design. the proposed method inherits the computational advantages of these two advanced techniques. elm technique provides a fast, direct, and accurate way for the calculation of gain spectrum without solving the complicated raman coupled equations. de algorithm is utilized to determine the optimal pump wavelengths and pump powers to meet the gain requirement and minimize the gain ripple. we incorporate welltrained elm model into the evolution of de to further improve the overall optimization efficiency. the design results shows that a flat gain is obtained covering c+l band with only 4 pumps and the gain ripple is lower than 0.5 db. meanwhile, comparisons of the proposed approach with other related methods are also undertaken. the results demonstrate that the elm method is far superior to shooting algorithm and the average power analysis method in term of the calculation speed. compared with other evolutionary algorithms, such as pso and aco, the proposed method yields better gain performance and faster convergence speed. the whole optimization procedure takes only less than 0.2 seconds, which enhances the efficiency by several orders of magnitude. the proposed technique can be extended to be applicable to the dynamic and flexible pump control for rfas."
"step 4 (user: upload more files and assign more users and their privileges for more files). if user uploads additional files, for example, file 2, file 3, steps 2 and 3 will be reconducted iteratively for those files by cloud and user ."
"definition 2 (accessing user). after original user logs in to cloud and uploads files, she will be asked to assign access privileges to some accessing users for those files. original user also needs to specify accessing user's account information for logging in to cloud systems and token information for distinguishing different users for file accessing."
"(6) is the team leader. she adds two users into her team, namely, and . assigns to edit 1, but cannot update 1. the modification on 1 by can be reviewed by and updated by ."
"de algorithm is a simple, fast and robust stochastic search technique that has shown superior performance in solving global optimization problems [cit] . it has been successfully applied to a wide range of fields due to its strong global search capability, easy implementation and quick convergence. the most distinct characteristic of de is that it mutates individual by adding a weighted difference vector between other random individuals in the population [cit] . the implementation of evolutionary process includes three important operators: differential mutation, probability crossover and greedy selection. the flowchart of de algorithm is shown in fig. 3 . de process starts from an initial population of n p individuals, which is randomly generated from a uniform distribution. each individual in the population of current generation is represented by a real-valued vector:"
"(3) once a user is added into acl, she will be added into ucl. the same user name presents only once in ucl. for better understanding, all users in ucl can be considered as a group whose members share the same login account information (namely, login user name and password) in cloud as the original user who uploads shared files."
"(3) the condition could be extra accessing rules on demand, which is set \" * \" in the description for simplicity. indeed, it can be set as loc, mac, and so on. hereby, & ⟨ *, loc, mac⟩. can be considered as the extension of . (4) step 8 is usually accomplished offline or out of the channel, which is out of the scope of the scheme."
"(4) for each file, only one user can have create privilege. the user who has create privilege is the original user (the one who uploads the file into cloud). all users in ucl will share the same account information with this user who has create privilege."
"for a raman fiber amplifier with multiple backward pumps, the amplification behaviors are governed by a set of nonlinear raman coupled differential equations, where the interactions of the pump-to-pump, signal-to-signal, and pump-to-signal, as well as the amplified spontaneous emissions (ase) and rayleigh scattering components are described. the complete raman coupled differential equations have the following form:"
principle 7. the merging set of accessfileset in leaves at all children of a node (not root) is upper bounded by accessfileset in the node's leaf whose oneprivilege is authorize. it is similar to principle 3.
"step 11, if a user (e.g., user ) logs in via not using user 's account (including user name and password), cloud will respond that this user (namely, user ) is not a file sharing user related to user . it is an independent login, not being related to the file sharing of user (i.e., this login may be used for user 's file uploading and sharing)."
"scale invariant feature transform (sift) was introduced by lowe [cit] and is invariant to rotation, translation and scale variation between images and partially invariant to affine distortion, illumination variance and noise. apostolos psyllos [cit] presented an interesting approach based on an enhanced scale invariant feature transform (merge-sift or m-sift) for vehicle logo recognition. in paper [cit], sift has already been used for recognizing faces in controlled situations. in paper [cit], a new vehicle tracking algorithm based on ukf and sift is proposed."
"step 2 (user: assign one user and one privilege for one file). once user uploads a file successfully (e.g., file 1 ); cloud will add two records into acl as follows."
"the design of f2ac should tackle the following situations: (1) original user and team leader can authorize privileges for a highly dynamic group in which users can be added or removed. (2) those privileges can be flexibly managed by original user or team leader in an iterative way and accessing hierarchical layers can be diverse and arbitrary. (3) the access control can be conducted in a lightweight manner at cloud, especially when system access control is separated from file access control, which is a realistic deployment requirement in storage cloud."
"in the above abstract model, the major steps for f2ac are described. in this setting, only user that uploads the file can obtain create privilege and can authorize privileges to others. it can simplify the management of privileges, but it may not be convenient when the file is shared in a dynamic group whose members are changed frequently, or a group with a large number of users or different layers."
"definition 6 (authorize privilege). original user and team leader can have authorize privilege, who can assign privileges to other users, and add or remove accessing users."
"(1) f2ac can create, add, and delete users in a group (and a subgroup iteratively), authorize a user as a group leader who can authorize privileges to other group users iteratively, and revoke privileges for a user in the group."
"step 3 (user: assign more users and their privileges for one file). user continues to set up more users and more privileges for this file. cloud add more records in acl correspondingly. that is, step 2.2 will be conducted iteratively for file 1 for assigning more users and privileges."
"through the above operators, the new population is generated and then will continue further evolution operations until the maximum number of generation is reached. the implementation procedure of de algorithm is summarized as algorithm 2."
(3)sift matching. the keypoints obtained from the master region are used to identify which categories the logo belongs to and the keypoints from the sub region are used to make a final judgment according to the number of matching points.
the flow diagram of algorithm is shown in figure 1 .this algorithm can be divided into three steps: (1)pre-judgment of tv logo. the tv logo information is obtained by applying a special mask image to the captured video frames. then the coarse matching is performed on the hsv color space to select the similar tv logo templates.
"where g t is the pre-specified target net gain, max(g net ) and mi n(g net ) are respectively the maximum and minimum of all the signal net gains given by (10) . then, the rfa design can be formulated as a minimal optimization as follows:"
". in accessing users, some are team leaders who can assign privileges for shared files in this team. team leader can also add and remove accessing users in this team."
"step 9), user will be requested for her token. if this time no file is uploaded, ucl will not be created. that is, once a file is uploaded by a user, ucl and acl will be created at cloud."
"(5) the file set (usually consisting of multiple files) at the father node should be the superset of that (namely, file set) at a child node or children. it is nontrivial to be aware of but can be understood for the reason that father's files are reassigned to a child or children to edit. thus, a merging set of a child's or children's accessfileset is upper-bounded by the father's accessfileset."
"2) accurate keypoints localization. at each candidate location, a detailed model is fit to determine location and scale. boundary influence is reduced using trilinear interpolation to distribute the value of each gradient sample into adjacent histogram bins. the figure 4 gives the accurate keypoints in tv logo. in this stage, the sift algorithm assigns orientations to each keypoint location base on local image gradient directions 2 2 )) 1,"
"comparison of reconstructed activity in the anesthetized and awake animal. a: reconstruction of all 522 units from the anesthetized rat sensory motor-cortex, session 42 in the kampff data base. b: same as in a but for the 471 units from the awake recordings from mouse sensory-motor-cortex from the data base of steinmetz. c: rise triggered average of reconstructed activity (left). the lag is relative to the average (across units) threshold crossing time point. the activity was smoothed across neighboring units (see methods). latency of the half-maxima for each unit (right). bars indicate the standard deviation of the bootstrapped latencies. note that the correlation statistics is for the latency as a function of cortical depth (not vice versa). d: same as c but in the awake animal. e: summary of the latency as a function of the cortical depth (mean±standard error). f: distribution of cross-correlation values for all unit-pairs in the anesthetized animal (left), and for the awake animal (right). g: average cross correlation in anesthetized and awake animal (the standard error is smaller than the printing resolution)."
"here we apply the algorithm for reconstruction of membrane potential on a dataset of extracellular activity recorded with the neuropixels probe [cit] from the somatosensory cortex of the rat that was acquired in the kampff lab [cit] b . depending on the session, between 50 and 200 units were identified using the kilo-sort spike sorting algorithm [cit] ). an additional (target) unit was extracted from the simultaneous intracellular recording. only intracellular recordings with a certain signal-to-noise ratio were included in the analysis (see methods). the subthreshold activity of the target unit was reconstructed using the algorithm and was compared against the ground truth, the simultaneous current clamp or voltage clamp. since the algorithm reconstructs subthreshold activity without knowledge of the spiking activity in the target neuron it is fair to estimate the correlation between reconstructed activity and the intracellular activity using all time points (including those where there are spikes). the algorithm can reconstruct the membrane potential of a whole cell recording during pronounced down/up-states with a correlation of 0.87 (figure 2c) . for a more desynchronized intracellular activity in voltage clamp the algorithm reconstructs peaks and troughs in the absence of spiking activity with a correlation of 0.4 ( figure 2d) . for a third cell the more complex down/up state dynamics in a whole cell recording could be reconstructed with a correlation of 0.61 (figure 2e) . overall the subthreshold dynamics could be reconstructed with an average correlation of 0.62±0.22 (figure 2f )."
"large scale spiking recordings is a universal technique that has been applied in all species ranging from c-elegans to humans, anesthetized to freely moving animals. here we present an algorithm that uses large scale spiking data to reconstruct subthreshold dynamics of individual neurons. we have verified the algorithm directly using in-vivo intracellular data. moreover, analyzing the reconstructed subthreshold activity from awake and anesthetized recordings, we have further confirmed the validity of the reconstructed data by means of verifying already known opposing subthreshold dynamics across the cortical laminae for two discrete states, i.e. awake vs anesthetized."
"the reconstruction algorithm implicitly relies on correlations between neurons to extract a continuous subthreshold activity from discrete spikes. previously, correlations have been used to reduce dimensionality in order to estimate a smoothed firing rate using poissonian or gaussian statistics [cit] . such an approach is, however, suboptimal for reconstructing subthreshold activity since the nonthresholded poissonian or gaussian statistics will cause the activity to vanish in the absence of spikes. subthreshold activity may, however, be arbitrary large as long it remains below the threshold. therefore, we have devised a thresholded non-poissoninan spike timing error function which is not correcting subthreshold activity in the absence of spikes. due to the nature of dimension reduction techniques they typically need one or more parameters to set the number of dimensions and to control how the dimension reduction should be done for the data at hand. the reconstruction algorithm, on the other hand, is \"plug-n-play\" in the sense that there are no explicit parameters (it was not tuned to the particular data-sets used in this study). furthermore, in contrast to dimension reduction techniques the reconstruction algorithm allows each neuron to have a unique nonlinear mapping to the other neurons. thus, the algorithm could be defined as a dimension conserving technique for subthreshold reconstructions."
"spike repetitiveness and -rate for a unit a prerequisite for the reconstruction is that there always is at least one neuron whose firing can give information about the subthreshold activity of another neuron. the more neurons that spike while the unit, ui, is silent the better the reconstruction will be for unit ui. the −rate gives the instantaneous summed firing rate across all neurons at a lag relative to the unit that minimizes this summed firing rate. the rationale for the minimum operation is that any minimal instantaneous firing rate will constitute a bottleneck for the reconstruction."
"here we have reconstructed the membrane potential of neurons from both anesthetized and awake recordings and studied their subthreshold laminar dynamics. we have used the nick steinmetz data set for the awake activity and the kampff-lab data set for the anesthetized activity. to study the latency differences between cortical layers, the subthreshold activity was reconstructed using 1 ms resolution (figure 6a and b) . to study the latency across layers we detected all low to high transitions in the data set and averaged those (figure 6c and d) . the latency of a given unit was the time point when the activity crossed the 50% of the peak activity. in general, the latency decreased for deeper layers ( -24±7ms/mm, m±se) for the anesthetized case and increased (37±7ms/mm, m±se) for the awake case ( figure 6e) . this was also true for the 10% threshold, but not for the 100% threshold (figure s3a and b) . for the awake data there was not a significant correlation between the cortical depth and the 100% latency indicating a later synchronization when the activity becomes suprathreshold."
the repetitiveness can be described in general terms with the −rate (see methods). it is maximal (average neuronal firing rate times number of neurons) for independent firing and it is zero when spikes cluster at a certain lag between the target neuron and input neurons (synchronization).
"a correlation index between the spiking data and the reconstruction was used to estimate the correlation between the reconstruction and the membrane potential. first the spike times were used to divide the time in intervals − to + for each spike si (see formula above). for this interval the mean, minimum, maximum and value at the spike bin was calculated from the reconstruction."
the variability of the latency was estimated using bootstrapping. to this end threshold crossings were sampled with replacement to conserve the number of threshold crossings and the latencies was calculated for the corresponding threshold triggered average of the reconstructed activity. this procedure was repeated 100 times and the standard deviation was calculated for the resulting latencies (figure 6c) . to minimize the influence of outliers in this estimate we also smoothed the reconstructed activity across neighbouring units (figure 6d) . the filter used was:
"for the awake-mouse data from nick steinmetz in the lab of kenneth harris and matteo carandini in the university college of london spikes were sorted automatically by kilosort [cit] and manually by n. steinmetz using phy [cit] ). we choose to analyze data from the sensory-motor cortex. the activity in the sensory-motor cortex is typically more desynchronized than in other primary sensory areas, which on the one hand makes the reconstruction more difficult, but on the other hand gives more reliable correlation estimates (see -rate below)."
"moreover, the −rate estimates the number of input spikes that is available for each time bin. for example, a -rate of 1000hz corresponds to at least one input spike for each reconstructed millisecond."
"although we have focused on the sensory-motor cortex of the rat and mouse, we have shown that the reconstruction framework encompasses a large spectrum from repetitive to random firing in both simulations and in-vivo data. therefore, we are confident that the reconstruction framework can be applied to other data sets. nevertheless, further verifications can be done using both whole cell patch recordings and imaging techniques. whole cell patch recordings offer the unique advantage of providing a direct measure of the membrane potential [cit] and it is possible to record the simultaneous subthreshold activity in multiple neurons in the anesthetized animal [cit] and the awake animal [cit] . imaging approaches using genetically encoded voltage indicators can record up to 4 to eight neurons simultaneously invivo [cit] . without simultaneous intracellular recordings it is, however, difficult to say how much of the recorded fluorescence fluctuations origin from background-, or out of focus, fluorescence from other active neurons in-vivo [cit] . finally, although imaging below 300-500 micrometer requires invasive insertion of a prism, or aspiration of brain tissue, this is irrelevant for the verification since the damaged circuitry is the same for the imaging and the reconstruction. therefore, the reconstruction algorithm has the potential to leverage those experimental methods to address the simultaneous subthreshold dynamics in hundreds of neurons, in multiple areas, in behaving and freely moving animals with the minimal invasiveness offered by the state of the art extracellular recordings [cit] ."
", where s is the total number of spikes for the unit and is the smallest nonnegative value allowed by data type. then the spike times are used to divide the time into regions proximal to each spike. for each spike such a region is defined by − to + ."
"this signal to noise ratio criterion resulted in 7 cells. one cell-attached current clamped, four whole cell, and two cell attached voltage clamped. although cell attached recordings has been used to access the membrane potential and currents in in-vitro and in-vivo [cit], the disadvantage of cell attached recordings is that a small rseal leads to a slower time constant [cit] ). this was not an issue here since the rate of change of the fluctuations of the cell attached recordings were similar to those of the reconstructions."
"the non-repetitiveness in turn suggests that the reconstruction algorithm works best for nonovertrained animals or well-trained animals performing many different conditions. importantly, this does not mean that the experiment becomes less interpretable. on the contrary, first, the functional connectivity between two signals, being it sensory-neuronal, neuronal-neuronal, and neuronalbehavioral is more reliably estimated if one of the signals (or both) is experimentally randomized [cit] . this is because randomization typically cancels out long term correlations that would otherwise bias the estimate. second, the brain will not \"adapt\" to a stereotypical behavior by changing the locus of representation to a another brain area [cit] . nevertheless, the algorithm has been able to reconstruct both synchronized and desynchronized activity which suggests that there typically is a \"noise\" component in-vivo that makes the spiking non-repetitive."
"as a result of the dimension conservation the algorithm can only be applied if there are enough neurons and/or spikes for making the reconstruction (see figure 5c ). although, here we have used data from the neuropixels probe to record an adequate number of neurons, more neurons could potentially be sampled for a longer time using thin-film electrodes [cit] or with calcium imaging [cit] ). in the case of a brain area with small volume and few neurons it may become necessary to put additional electrodes in functionally related areas. note that the algorithm is insensitive to whether the input area is sending axons to, or receiving axons from, the target area. since the algorithm is acausal it can tune the reconstruction for either case or for both . overall although the algorithm is dependent on a large-scale-recordings we expect the algorithm to be less restrictive as to where those recordings should be done in relation to the target neurons."
"to calculate the latency across cortical layers we first calculated the time points where the average reconstructed activity across units, a(t) passed a certain threshold. for the anesthetized recordings where there were clear transitions between down and up states, we used the temporal average of a(t) to define the threshold. for the awake recordings where we did not have clear state transitions, we calculated a bandpass filtered version using a low and high pass filter according to the following formula:"
"network weights (node 15 in figure 7), separate weights (node 11 in figure 7), were initialized with rectangular distribution 0.1+-0.01, 0+-0.1, respectively. network gating biases (node 3 and 7 in figure 7), and bias in node 12 were initialized with normal distribution 2+-0.1, and -0.1+-0.01, respectively. the network was trained with adam with a learning rate of 0.001 [cit] ."
"we have used a backpropagation network for reconstructing the subthreshold activity ( figure network) . the network and its utilities were programmed in the freeware octave. the network output is supervised by the spikes of the unit that should be reconstructed, i.e. target unit, and the network input is the spikes of all other units, i.e. source units. the feedforward propagation from input units to output/target unit does not need the target spikes, which in turns means that the reconstruction, (t), is done without the knowledge of the target spikes. the feedback/backpropagation sweep needs the target spikes for training the network and optimize the weights and biases."
"the correlation structure between neurons has been shown to depend on the state of the animal. overall, the correlation between units was larger in the anesthetized state than in the awake state (figure 6f and g) . for both types of states, the correlation decreased towards infragranular layers (figure s3c and d) . this trend could also be seen in terms of the dimensionality of the population activity in each layer and by means of the visualized population trajectories (figure s3e and f) . together this suggests that the latency of the subthreshold activity is different for the awake and the anesthetized state, but that the relative complexity between the layers remains the same for the two states."
"finally, the algorithm was verified by comparing the reconstructed dynamics from the awake and the anesthetized state. according to previous studies the reconstructed activity is strongly correlated across layers and within layers during endogenous slow fluctuations in the anesthetized animal [cit] . in contrast, in the awake animal, we see an average correlation that is three times smaller than that of the anesthetized animal. unfortunately, since there is no study to the authors knowledge that quantifies correlations between paired intracellular recordings in the motor cortex of the awake animal the best verification that we can find is from the overlapping sensory cortex [cit] . accordingly, during paw movements and during sensory stimulation, the subthreshold activity in the sensory cortex in the awake animal shows low correlations [cit] . moreover, the awake and anesthetized state differed in terms of the subthreshold dynamics. the onset of the slow fluctuations is earliest in the infragranular layers and progressively delays towards supragranular layers. only towards the pia there seems to be an earlier onset. this pattern is common to the latency patterns shown three earlier studies. first, in the auditory cortex there is a strict monotonic increase in latencies from infragranular to supragranular layers [cit] ). in a second study it is shown how the current source density has an additional earlier sink towards the pia [cit] . in the third study the onsets of slow fluctuations in intracellular recordings during anesthesia can be described by an early onset in deep layers, followed by an onset in superficial layers, and finally followed by a late onset in middle layers [cit] . the origin of such a progression might be the top-down projections that end in layer i and vi [cit], as has been proposed by multiple authors [cit] . indeed, optogenetic inhibition of axons from m2 to s1 decreases the component in s1 with the layer progression suggesting that it is manifested by a top down mechanism [cit] . in contrast, in the awake animal, we see the earliest onset in supragranular layers followed by the infragranular layers. this pattern resembles that in the hindlimb sensory-motor cortex [cit] and only partly that of the auditory cortex [cit] ). intriguingly, some 100 milliseconds later towards a suprathreshold activation the latency difference across the layers diminishes (figure s3b ). this latency \"cancelation\" could indicate a late influence of feedback signals with the opposite latency pattern. such a late synchronization pattern can be seen for the membrane potential for layer ii/iii and v neurons in the awake mouse [cit], their fig 4d. one millisecond resolution reconstruction was used for the laminar latency estimations. such a resolution is only possible with a larger number of neurons since there will be more input spikes per reconstructed millisecond. the high temporal resolution might be especially beneficial for understanding the fast activity changes during for example sensory stimulation or experimentally induced perturbations. when a perturbation hits the brain the resulting activity can take many paths through the extensive neural network. like throwing a stone in a small water pond, the water will be reflected from the uneven sides of the pond and after a very short time there will be a complex interference pattern. given known connectivity and connection latencies a perturbation can spread across the entire brain already after 100 ms [cit] ) . this potential explosion of indirect mechanisms gives little time to record a clean effect of the perturbation. only during the first 5 ms of a brain perturbation the effect remains in the local microcircuit. to this end it is intractable to quantify the response using spiking activity since the average firing rate of the neuron might be in the order of 1 hz. therefore, an instantaneous readout such as the membrane potential will enable sampling the response before it has spread outside the recorded area. overall, large scale recordings will most likely be instrumental for studying the fast interactions that governs genesis of new brain activity in the healthy and in the diseased brain."
"a single metric that predicts the reconstruction reliability. a: relation between −rate, condition count, and correlation error. note that for a specific condition count the −rate has been averaged across all noise ratios (see figure 3 and s2). b: relation between −rate, noise ratio, and correlation error. note that for a specific noise ratio the −rate has been averaged across all condition counts (see figure 3 and s2). c: relation between correlation index and true correlation for different degree for different −rates. the data is pooled for all combinations of conditions (1, 16, 256, and inf) and all noise ratios (0, 0.027, 0.05, and 0.37). d: same as in c but as the relation between −rate and correlation error. e: relation between correlation index and true correlation for different −rates for in-vivo data. f: same as in e but as the relation between −rate and correlation error."
"to be able to approach large scale membrane potential recordings we propose to use large scale neuronal spiking activity to reconstruct the membrane potential of individual neurons. in fact, the physiology of the brain facilitates this thanks to the many nuances of redundancy, being it branching axons [cit], gap junctions [cit] ), ephaptic input [cit], or recurrent/mono/poly-synaptic input. because of this redundancy, a sub threshold event will occur in multiple neurons, some of which will generate a spike because of this event and others because of distinct input (figure 1) . in line with this redundancy it is possible to predict the spiking activity of one neuron given the spiking activity of other neurons [cit] )."
"here we test a framework for reconstructing subthreshold activity for a specific unit from a set of spike times and their unit identity (figure 2a) . we have developed a cost function that only penalizes the reconstruction when it is above the threshold at a time point where there is no spike (figure 2b) . therefore, anything can happen below the threshold without it being corrected for. the reconstruction is done by an artificial neuronal network that detects spatiotemporal patterns in the population spiking activity. the framework is available at https://github.com/david-eriksson/sublab. using data from simultaneous neuropixel and intracellular recordings we show that the algorithm can reliably reconstruct the subthreshold dynamics. then we show that the reconstructions are most reliable for de-synchronized spiking data. finally, we use the reconstruction to reproduce some differences in membrane potential dynamics for anaesthetized and awake animals."
using spatiotemporal spike patterns to reconstruct subthreshold activity. an artificial neuronal network is trained to find spatiotemporal patterns that are associated with the spikes of the reconstructed unit. during the reconstruction phase those associations are used to fill in subthreshold activity.
"the spike times are binned separately for each of the u units with 10ms or 1ms time bins. the spiking data (node 16) of the target unit that we want to reconstruct is assigned to the backpropagation node (node 17) and the source spiking data is assigned to node 1 (figure 7) . there is a dropout after node 1 with 0.1 dropout probability. to produce a continuous signal each spike is convolved using a gated unit with one sigmoidal time constant (adjusted via backprop) in each temporal direction (forward, nodes 7-10, and backward, nodes 3-6, in time). the input units are repeated 2 times (two independent time constants) which means that there are 4 times the number of units dimensions in node 1 and throughout the network to node 11. this enables each temporal direction to be modelled by two exponentials and therefore it enables the formation of delays. delayed filters have successfully been used to predict spike trains [cit] . the output of node 11 has 4 dimensions and contains the weighted sum (adjusted via backprop) separately for each of the four node duplicates. each of the 4-dimensional output of node 11 is added to a bias (adjusted via backprop) and the resulting four dimensions are individually passed to a hyperbolic function (tanh). finally, the reconstruction, (t) is the weighted sum (adjusted via backprop) of the four inputs from node 14."
"indeed, for an independent noise process there is also an underestimation of the true correlation ( figure s2) . all in all, both simulations and in-vivo recordings suggest that, for an accurate and reliable reconstruction, the spikes should not form repeating spatiotemporal patterns."
"corman routing protocol. however, corman does not give efficient performance during communication, because corman does not make better channel reuse. by combining the better channel reuse method in corman we can improve the performance of manet. this paper focus on new proposed protocol called icorman, the new protocol in manet which makes the efficient channel reuse by using ant colony optimization algorithm. also better channel reuse method based on aco is presented in this paper. the practical analysis of proposed work is possible using java in order to claim the efficiency of proposed routing protocol against the aodv & corman routing protocol which shows that, icorman improves the performance of manet as compared to aodv & corman routing protocols in terms of pdr, throughput, end to end delay and total pdr"
"to test how the reconstruction depends on the repeatability of the spike trains we generated conditions for which all the trials within a given condition had an identical synaptic input (but noise was different between each trial). we tested 1, 4, 16, 256 conditions. each trial was 4 seconds long and the trials were appended back to back."
"the error calculation is as follows. the threshold is defined at 0. first a spike probability, p(t), is created for each time bin by distributing the total number of spikes (s) to those reconstruction values, (t), in node 15 that are above 0."
"to test how the reconstruction accuracy scales with the number of input units we pseudo-randomly divided the units in groups of 2, 4, 8, 16, 29, 58, 117, 235, and 471, 3, 7, 15, 28, 57, 116, 234, and 470 ."
"in order to reconstruct the subthreshold membrane potential, we rely on the naturally occurrin g asynchronicity of large-scale neuronal activity. this temporal jitter is captured by the −rate which is high when spikes are not occurring at a certain lag between two neurons. a large −rate indicates that there are enough \"independent\" data samples for the algorithm to find the relation between subthreshold and spiking events. the −rate is maximal when the neurons are firing independently of each other in which case the −rate becomes equal to the average neuronal firing rate times the number of simultaneously recorded neurons. a large −rate renders the reconstruction more accurate both for ground truth in-vivo data and for simulation data. for the in-vivo data, however, the correlation-index underestimates the true correlation. this could suggest that the spikes are not purely driven by the membrane potential of the neuron. there could for example be a noise component that is independent of the membrane potential, and independent between the neurons. such a noise signal could be averaged out when taking multiple neurons into account as is done for the reconstruction."
"when comparing the averaged correlation indices across the two largest groups (the groups with 234 and 470 input units) we noticed that the average correlation did not increase from the smaller to the larger group as would be expected. this could be because the algorithm did not get enough data (spikes) to fit its parameters. therefore, we tested if the average correlation would increase if we only reconstructed units with a high spike count:"
if p(t) is not above 0 at any time in the spike interval the error is the negative average of ∆ − and ∆ + . this scenario is typically the case initially before the reconstruction converges. since the error is proportional to the inter-spike interval the error density will be constant. this in turn will make a single spike as important as a burst. hence the algorithm will not only try to fit high firing frequencies but also single spikes and their timing are accounted for.
the mean reconstructed activity was subtracted from respective unit. then a singular value decomposition (svd) was calculated from the sum-of-squares-and-cross-products matrix (summed across time points) for the normalized reconstructed activity for the units in each layer. the dimensionality for the layer was then taken as the number of eigenvalues that are larger than 10% of the largest eigenvalue. to get the variability of this dimensionality we applied bootstrapping. the svd was applied for a new set of units taken with replacement in order to conserve unit count. this procedure was repeated 100 times and the standard deviation was calculated for the resulting dimensionalities.
"in the future more neurons can be recorded at any given time. therefore, we run the reconstruction algorithm with different number of neurons ( figure 5a) . the data is from the motor cortex during spontaneous behavior of the awake mouse recorded by dr. nick steinmetz at the ucl in the lab of matteo carandini and kenneth harris. we have used both single and multi-units for the reconstruction ending up with 471 units. on average the correlation scaled linearly with the logarithm of the unit count ( figure 5b ). for the last step from 234 to 470 units the increase in correlation depended on the number of spikes, suggesting that the network is overfitted with fewer spikes (figure 5c ). thus, although we choose this data set because it has the longest recording time it still does not seem to be enough data (spikes) to fit all the parameters for 471 units."
"ideally one would like to measure the membrane potential of hundreds of neurons in multiple different areas and different layers in the brain, and in a freely moving animal of any kind. such large-scale recordings of membrane potential are so far impossible. to measure the membrane potential there are electrophysiological methods using glass pipettes and imaging methods using fluorescent indicators [cit] . electrical recordings using a glass pipette are regarded as \"the gold standard\" since they measure the membrane potential directly. such recordings have been done in anaesthetized animals [cit], head fixed awake animals [cit], and even in freely moving animals [cit] . to patch multiple cells in vivo is difficult and may in the future be facilitated by patch \"robots\" [cit] . imaging methods might be more suitable for addressing the membrane potential of multiple cells simultaneously [cit] ), but may still represent a smoothing of activity, owing to limitations on scanning and shuttering speed when compared to electrophysiological sampling rates. there is a strong effort to develop fluorescent indicator constructs that approximate the membrane potential as close as possible [cit], and an equally strong push in developing imaging tools to be able to image deep in the brain and even in the freely moving animal [cit] ."
"in order to do a fair comparison between the instantaneous population firing rate and the reconstruction algorithm we smoothed the binned firing rate, r(t), with the following kernel:"
"a heuristic was developed for a spike-time based reconstruction error. the heuristic describes how much one gain/loose in spike time precision by moving the reconstructed activity below or above threshold. in short, if the reconstruction comes above the threshold far away from a spike then this causes a large positive error. on the other hand, if the reconstruction is below the threshold where there is a spike this causes a negative error. the spike timing balances periods of slow firing rate with periods of high firing rates without having to change or assume a specific temporal smoothing."
"in any species and in any neuronal circuit the membrane potential plays a key role in transforming and distributing neuronal signals. the membrane potential integrates incoming information, it influences the function of ion channels so they can modulate subsequent input, and, whether it crosses a threshold or not, it transmits this new information to other neurons [cit] . in addition to its physiological importance the membrane potential is a continuous signal that is richer than a discrete signal such as a train of action potentials. finally, the membrane potential can reveal fast changes in neuronal physiology which cannot be seen with a smoothed spike train. these can be the fast changes that occur during normal brain function or in response to an experimentally controlled perturbation."
"for the simultaneous neuropixels-and patch data from the kampff lab in the sainsbury wellcome centre in london we spike sorted the neuropixels data using kilosort [cit] ). the intracellular recordings were done in sensory-motor cortices in lister-hooded rats of both sexes, aged between 6 weeks and 8 months, anaesthetized with urethane. the intracellular recordings where resampled to 30 khz, i(t), and aligned to the neuropixels recordings (sampling rate 30khz). to remove the spikes from the intracellular signal the spikes were detected after high pass filtering the patch data:"
"the automatic detection and classification of arrhythmias in electrocardiography-based signals (ecg) has been widely studied in the last years in order to aid the diagnose of heart diseases. one way to perform this type of test is to conduct a long-time recording of the cardiac activity of an individual in his/her normal routine in order to obtain a reasonable amount of information about the individual's heartbeats. however, the posterior task of analyzing such data may be tiresome and more prone to errors when interpreted by human beings, since there is a huge amount of information to be processed."
"since the detection and segmentation of beats in ecg signals is not the main goal of this work, we have employed precomputed annotations of r waves provided by the database in order to accomplish the signal segmentation. in addition, 4 records derived from patients that make use of pacemakers that were discarded, following the recommendation of ansi/aami standard ec57 [cit], which also recommends to group the 15 classes reported in the database's annotations into 5 classes (table 1 ). figure 1 depicts some ecg signals for each class, being class q represented by 10 signals, and the remaining ones represented by 100 signals. the signals were randomly picked up from the database."
"potential of the technology along with applications to production and facility planning [cit] and ability for extensive visualization techniques for machine operators [cit] . studies applying vr and ar technology to energy analysis is limited. pelliccia [cit] discuss the use of vr to the visualisation of energy paths of a milling centre, however analysis was limited to electrical energy for one tool, with no consideration of other flows such as compressed air or heat, nor enabled multi-level analysis across multiple machines in a production chain or the manufacturing facility. herrmann discuss the use of ar for simulating multiple inputs and energy flows of a grinding machine, along with consideration of the full factory floor, however again, analysis was limited to one machine tool, neglecting interdependabilties between multiple machines and the manufacturing process chain, along with the surrounding building environment."
"soon after finding prototypes, the opf algorithm is used, which essentially aims at minimizing the cost of every training sample. such cost is computed using the f max path-cost function, given by:"
"due to the novel nature of the concept of digital twinning, current literature is limited, with very few studies applying the use of a digital twin to production systems and manufacturing environments for industry 4.0. studies mainly focussed on introducing the concept of digital twinning, highlighting its potential and importance to improve accuracy and capabilities of the manufacturing production system [cit], as well as proposing methodologies for data acquisition and exchange between systems [43, [cit], rather than analysis of the manufacturing process and integration with other dependable features such as production chains and the facility environment. no studies have been found which use the concept of digital twinning for energy analysis and integration of physical systems with the surrounding environment and material flows. further work is required to investigate areas for application of the digital twin and benefits it can bring along with industrial case studies, and integrated analysis with the surrounding environment in real or near-real-time."
"in order to cope with such problem, several works have been carried out arrhythmia classification in eeg signals by means of machine learning-oriented techniques [cit] . however, regardless of the classification algorithm used, some processing steps are crucial to design a reasonable approach to detect arrhythmia. the quality of classification when dealing with ecg signals is directly dependent on the preprocessing phase, which aims at filtering noise frequencies that might interfere with ecg signal [cit] . after preprocessing, it is required to detect and segment each heartbeat of the ecg signal. in order to perform this task, an important step is the detection of the qrs complex (three deflections from ecg signal), specifically the r wave, since most part of the techniques for the detection and segmentation of heartbeats are based on the location of such deflection. because of the steep angular coefficient and amplitude of the r wave, the qrs complex becomes more obvious than any other part of the ecg signal, being easier to be detected for later segmentation."
"mousavis and kohls were the only studies to capture dynamic behaviour of the manufacturing process. analysis of energy consumed in the study by solding was based on a constant consumption rate, similarly, the simter tool didn't consider time dependent effects in the energy context and wilson used average power consumption data to calculate energies. seow considered energy during operational state only. likewise, addition of tbs modules and consideration of all energy flows including compressed air, steam and heat transfer along with electrical energy is required within further research"
"in this section, we evaluate the performance and the computational time of the opf classifier using six distance metrics. 2 the evaluation is performed considering the classification according to five [cit] and three classes [cit] ."
"similarly, [cit] integrated the manufacturing production line with the hvac system, using the concept of the energy opportunity window in which allows machines to be turned off without loss of throughput. the thermal model was built using energyplus, which allowed for consideration of changes in environment such as air temperature, internal heating loads and solar radiation. the production line was modelled using matlab's simulink and included random effective processing time to account for random downtime, as for ease of analytical analysis, a continuous flow model was used which resulted in cycle times being assumed constant and downtime events not considered. this addition aids in the capture of the effect of stochastic production systems and processes [cit] . it was concluded that opportunity windows could be coordinated with times of high energy demand and hvac systems to optimise facility energy use and reduce cost. further work is required to develop a model able to optimise costs of both production and hvac systems, along with multi-zone hvac systems. integration of compressed air and water within the model would also be beneficial. the study was highly focussed on reducing energy costs through efficient scheduling, rather than accurate determination of energy consumption and analysis of all relevant material flows and interdependabilites within the manufacturing plant."
"also, based on a similar analysis to the one carried out with the data in table 14, it could be confirmed that also in the three-class problem, opf-scs is the most appropriate to identify the pathological classes, i.e., the ones with greater clinical interest. [cit] considered only the euclidean metric and obtaining highest accuracy rates of 90.7 and 90.9 % in the 3-and 5-class problems, the best values are indicated in bold 7 libsvm implements the one-against-one method for multi-class tasks."
"the framework was applied to the manufacture of two products to provide a decision support tool, assessing energy demand, peak load and lead time. however the study focused on machine and process level analysis with no consideration for tbs or the building shell. furthermore, indirect energy demands eg. compressed air, was also neglected from the study despite being present in the framework. no validation of the model was carried out, with accuracy effected by a large number of model assumptions. reliable data about energy and resource demands were unavailable due to novelty of the components in production. the tool can provide a decision support tool, with the aim to support production planners. further integration of additional modules and consideration of tbs is required for a holistic approach."
"this study has highlighted the value of des for analysis of manufacturing facilities, accounting for changes in machine state whether idle or working, task scheduling and production planning as well as resource flows and non-continuous and stochastic processes."
"the final step is the classification of ecg signals, which is usually accomplished in a supervised fashion. support vector machines (svms) [1, 7-9, 13, 27, 29, 31] and artificial neural networks (anns) [cit] are among the most used machine learning techniques for this purpose. other approaches such as linear discriminant analysis [cit] and a hybridization of support vector machines and artificial neural networks [cit] are also applied for heartbeat classification. however, one of the main shortcomings related to the aforementioned pattern recognition techniques concern with their parameters, which need to be fine-tuned prior to their application over the unseen samples (test set). svms are known due to their good skills on generalizing over test samples, but with the cost of having a high computational burden when learning the statistics of the training data, since each different kernel has its own parameters to be set up. anns are usually very fast for classifying samples, but its training step may be trapped in local optima, as well as it is not straightforward to choose a proper neural architecture."
"a lack of case studies, validation and applicability of models to industrial cases was noted. with a large number of assumptions, the accuracy of discussed tools is unknown. the use of des coupled with ems and mes by michalowski resulted in the need for a large amount of unknown data such as equipment loads, energy losses, expected process energy use and heat generation. the analytical tool presented by sun assumed constant heat capacity and outdoor temperature, and could not model convective or radiative heat transfer from machines. brundage also disregarded environmental conditions, assuming max temperatures in the middle of the day to correspond with max time of use charge."
"however output data from simulations included time, utilisation of machines, product quantity flows, supply levels and product travel distance rather than analysis of material flows and energy. analysis was performed on a process level, neglecting individual machines and the surrounding environment. although the model was verified using historic production data, the input data had to be manually modified in excel due to inaccuracies and inconsistencies, and real-time data implementation in the model was not possible."
"for any node t 2 d 2, we consider all edges connecting t with samples s 2 d 1, as though t were part of the training graph (fig. 2d) . considering all possible paths from s ã to t, opf finds the optimum path p ã ðtþ from s ã and labels t with the class kðrðtþþ of its most strongly connected prototype rðtþ 2 s ã (fig. 2e) . this path can be identified incrementally evaluating the optimum cost c(t):"
approaches for analysis of energy analysis in manufacturing are summarised in table 1 . [cit] coupling manufacturing with building energy analysis using des [cit] coupling manufacturing with building energy analysis using des for decision making and scheduling [cit] energy visualisation in manufacturing using vr and ar [cit]
"paradigm a used conventional des tools, with separation between simulation and evaluation, allowing for extensive, low effort modelling with good transferability. however this disregards energy dynamics and interdependabilities between systems. paradigm b introduced the complexity of interactions between des modelling and additional simulation (eg tbs). this allowed for a more realistic analysis but increased simulation run time, reduced computational performance and reduced transferability. in contrast, paradigm c suggests integrating dynamic energy analysis with tbs and evaluation methods to provide a single data model. based on paradigm b and c, the authors developed an energy oriented, scalable and modular simulation tool for use in manufacturing environments, with the ability to model dynamic energy flows of all factory subsystems in a holistic approach."
"no studies have been found linking concepts of industry 4.0, such as digital twinning, virtual factories and automated model creation, with analysis of energy or material flows with a multi-level holistic approach. these novel concepts have been focused towards applicability and potential within industry 4.0, as well as introducing multiple concepts of increased automation and flexibility within smart factories. with these technological advancements, increased need for high cyber security and increased automation results in an increased energy demand, at a cost to industry. therefore, highlighting energy use of manufacturing processes in the shift towards industry 4.0 is of upmost importance."
"mousavi [cit], seow [cit] and kohl [cit] however, saw the importance of machining states and the dynamic nature of production, providing energy profiles for each process. likewise, [cit] measured the energy behaviour of systems during the off/on switching of actuators. however mousavi, seow and cataldo provided an isolated view on energy use in manufacturing facility, accounting for energy use of a single product or machine rather than a manufacturing facility or production line."
"six feature extraction approaches (associated with dataset a-f) were chosen based on the work of luz and menotti [cit], which performed a comparison among some of the most used approaches for such purpose, mainly: discrete wavelet transform (dwt), independent component analysis (ica), principal component analysis (pca), as well as information about rr range/interspace, which is the distance between peaks of two successive r waves in an ecg signal. for each dataset, the following methods were considered in this work:"
"tools of integrating hvac with the manufacturing production line requires further work, with integration of the building shell, real time data and detailed modelling of machine processes. inclusion of material flows such as compressed air, water, lighting and heat transfer is mentioned briefly in some but not all tools. furthermore, the focus of studies is around determination of optimal scheduling and production planning for the reduction of peak loads and energy costs, rather than the accurate determination of energy demand and resource flows for individual processes as well as on a holistic level for the manufacturing facility."
"where hsi is a trivial path, hs; ti is the arc between the adjacent nodes s and t such that s; t 2 d 1, d(s, t) denotes the distance between nodes s and t, and p s á hs; ti, is the concatenation of path p s with the arc hs; ti. one can note that f max ðp s þ computes the maximum distance between adjacent samples in p s when p s is not a trivial path. roughly speaking, the opf algorithm aims at minimizing f max ðp t þ; 8 t 2 d 1 ."
"current applications of vr and ar in the manufacturing industry is highly swayed towards product design and development [cit] . a similar trend was identified in academia, with many studies highlighting the fig. 7 . energy visualisation method based on a particle system [cit] . fig. 8 . use of augmented reality to determine environmental impacts of a manufacturing process [cit] ."
"the framework provided an estimate of the energy required to manufacture a single product during operational state only, and therefore cannot obtain an accurate value for overall energy consumed on a daily or weekly basis for the manufacturing facility, or provide analysis on influential factors or interaction amongst levels. the isolated use of des also neglects the continuous nature of some energy utilities. furthermore, no case study or method of validation was performed using the proposed framework."
"the optimum prototypes are the closest nodes of the mst with different labels in d 1 (i.e., samples that fall in the frontier of the classes, as highlighted in fig. 2b ). removing the edges between different classes, their adjacent nodes become prototypes in s ã . the opf algorithm can define an optimum-path forest with minimum classification errors in d 1 (fig. 2c) ."
"here, we present the results considering the experimental dataset divided into five classes. table 5 displays the recognition rates obtained by opf using each distance metric 3 in the datasets defined by each feature extraction approach."
"a systematic, all-inclusive integrated approach for the analysis and optimisation of energy and material flows, encompassing manufacturing processes on a multitude of levels including machine tool, production system, process chain along with the surrounding environment, complete with lighting, hvac, and influences of external weather conditions, building location and geometry as well as the building shell and effects of thermal bridges is required. a number of tools have attempted to bring these aspects together, however often neglect interdependabilities between layers and neglect detail, and often full layers completely. such attempts at analysis are narrow and inaccurate, with focus towards qualitative production planning and optimisation of existing production chains and systems, rather than holistic quantitative analysis."
"although attempts have been made for holistic analysis of manufacturing facilities, the simulation methods only focus on one level in detail, often neglecting others completely or neglecting detail and interdependencies between levels [cit] . tools offering analysis over multiple levels often lack detail, thus are considered suitable for estimations and as a decision support tool, aiding in efficient production planning and reducing bottlenecks rather than accurate energy analysis. to accurately reflect behaviour of the facility, continuous time based simulation, required for analysis of technical building services (eg hvac), and discrete event simulation, required of the manufacturing processes and process chain, is to be coupled."
"for reduction in lead time or c0 2 emissions. comparison of data with a previous production plan saw a deviation of 2%. the authors stated that the tool can provide a rough estimation suitable for quick assessment or analysis of individual processes deemed of low importance to the overall system. however the tool offers no consideration of technical building services, heat flows or analysis of energy at machine level. methodologies of data collection throughout the literature were seen to be inaccurate, and tools provided limited perspective of all relevant layers, with focus predominately on the manufacturing process and or production chain. studies mentioned are therefore considered effective tools for understanding interactions between equipment for more efficient process planning [cit], analysis of peak loads and bottleneck reductions to increase efficiency is production system and aid in planning [cit], and decision making [cit] . further work is required to accurately quantify energy from manufacturing processes."
"des's ability to simulate and optimise production lines, process chains and shop floor layouts has resulted in the development of novel tools utilising des alongside data analytics for additional insight into facility operation."
"it has been highlighted that the top contributors of electricity consumption in a manufacturing environment are the manufacturing system and hvac system [cit], however tools and frameworks which consider hvac energy models along with manufacturing are very limited. in the cases of industrial processes such as automotive paint shops, temperature control, and therefore hvac, is vital for production quality, and therefore should be considered alongside manufacturing processes."
"use of vr in manufacturing has the potential to reduce time and costs, and lead to increased quality, reduction of design errors and improved efficiency and well-being of operators [cit] . it's been used in industry for training, assembly and disassembly of products, manufacturing, design and virtual prototyping [cit], as well as improvements in ergonomics of the workplace [cit] ."
"attempts at achieving holistic analysis of manufacturing facilities through coupling simulation tools, as well as the development of novel numerical and analytical models were presented, however these presented challenges, such as availability of adequate and accurate data, as well as use of averaged model parameters and need for model simplification. furthermore, emphasis was on model structure through use of multiple levels or hierarchies, rather than input parameters and analysis of all relevant resource, thermal and energy flows. such work is limited due lack of energy strategies within organisations [cit] report stating that over a third of manufacturing companies do not set energy efficiency targets nor have no means of measuring improvements [cit] . furthermore, energy costs are generally not accounted for by production managers, and are considered indirect costs to maintain facility operation [cit] ."
"simter provided a factual based output for decision making rather than a point solution. estimated energy use lacked accuracy due to dependency upon equipment data provided by the manufacturer and number of process uncertainties and unknown input parameters, which, for large complex systems, resulted in the need for analysis on a sublayout level, with re-simulation until a satisfactory set of potential solutions was obtained. time dependent effects were also disregarded in the context of energy flows [cit] . the model can however provide an order of magnitude of energy use and cost index for comparison of machine investment and labor. energy hotspots were identified, allowing for opportunities to reduce peak loads through load shifting and use of mixed energy sources when available."
"the concept was tested on a grinding machine, implemented with a power meter, compressed air, coolant, pressure and temperature sensor. the full factory floor was presented, with in and outflowing material and energy data displayed along with measured energy data, in terms of energy usage or c0 2 impact. the tool was able to determine the greatest contributor to energy expenditure, but further steps are required in order to test and simulate further possible improvements, along with abilities to alter process parameters and investigation of various scenarios. furthermore, the ability to monitor multiple machines along an integrated process chain would allow for a more in-depth analysis of energy expenditure of a manufacturing facility."
"in this paper, a detailed study about the performance and computational time of supervised classification algorithms regarding the task of arrhythmia detection in ecg signals was presented. the main contributions of this work are: (1) to evaluate the opf classifier in the task of arrhythmia detection, (2) to evaluate six distances with opf, among which the best accuracy rates were obtained by the manhattan metric, while better generalization (i.e., the accuracy achieved per class) was attained using squared chi-square distance, (3) to test six feature extraction techniques and investigate which one leads to better recognition rates and generalization, (4) to compare opf against support vector machines and a bayesian classifier, being found that opf was the less generalist, while the svm classifier was the most accurate, and, finally, (5) to find that opf achieved the best trade-off between computational load and recognition rate. opf being less generalist with respect to classes v and s, which are of great clinical significance regarding class n, one can conclude that this classifier is more appropriate for the classification of arrhythmias in ecg signals than the svm and bayesian classifiers."
"virtual reality (vr) is an environment in which the user is fully immersed in a simulated virtual world, which may or may not hold any resemblance to the real world [cit] . augmented reality (ar) is vr placed over the real world, but with the provision of additional information. an enriched real world is represented rather than one that replaces it [cit] . in this way, both technologies are considered linked (fig. 6) ."
"in the test phase, the best computational time was obtained by svm-rbf (6.7 s), being almost 8 times faster than opf-l1 (53.3 s), both with feature extractor d. the third fastest technique was opf-scs (131.3 s), while bc, despite being the fastest in the training phase, took 173 s to classify the samples. in resume, svm-rbf was the fastest in the classification phase, followed by opf-l1, opf-scs and bc. usually, svm is fast for classifying samples, since it only considers the support vectors for such purpose, while opf may need to evaluate a considerable number of training samples for that. however, if we consider the total time, opf-l1 was the most efficient technique, which may lead us to consider it as a very suitable classifier concerning the trade-off between low computational time and high recognition rate. table 14 presents the confusion matrix related to svm-rbf classifier in the five-class problem for the dataset a [cit] . it can be noted a confusion of class sveb with class n, where only 37 (2 %) samples were classified correctly for class sveb. however, using the opf-scs classifier with dataset c [cit], the amount of samples correctly classified in the same class was around 43 %. thus, to detect cardiac arrhythmia, also known as cardiac dysrhythmia or irregular heartbeat, the accuracy over class sveb is usually considered most important. as such, the opf-scs accuracy obtained for this class, which is much higher than the one of svm-rbf, is of greater clinical relevance."
"the industrial sector is responsible for 55% [cit] . with the uk's goal of achieving a 60% [cit], manufacturing industries are faced with the challenge of reducing energy usage without negatively impacting profits and productivity. determining and understanding energy use at every stage of the manufacturing process is critical for optimising manufacturing processes and facility management in order to reduce energy consumption. however manufacturing systems and plants differ considerably across companies, with the need for varying parameter considerations with no blue-print for achieving energy optimisation. manufacturing processes and systems involve complex interactions between resources, water, compressed air, heat and energy, all of which is dependent on the process and state and well as control and operation. interaction between these individual processes, the manufacturing production line, the building environment as well as management and personnel is required to fully understand the operation of the facility, of which requires a computationally efficient, complete and accurate model for analysis by simulation. a study has shown that investing in energy-efficiency technologies and adopting technology to intelligently control energy uses can reduce energy consumption by 50% as opposed to making operational improvements, of which can reduce this by only 10-20% [cit] discrete event simulation (des) is often adopted in the manufacturing industry as an effective method of evaluating various strategies for process operation, optimisation and management, and hence enhance performance of systems with regards to bottleneck locations, queue times and task scheduling [cit] . as opposed to technical buildings services (eg ac controls) which are more suited to the continuous paradigm. manufacturing systems are dynamic, with states which change at discrete point in time, for example, the non-continuous nature of milling and turning processes, order schedules and batch sizes. simulation has been highlighted as the most appropriate method to model dynamic material and energy flows in a manufacturing environment due to the complexity of process interactions and large volume of variables [cit] . therefore, at machine level, des is an effective method of analysing material flows and can be used to analyse energy use in machining operations. however, des cannot simulate thermal building energy performance, and is consequently analysed in isolation to technical building services (tbs), despite the significant interdependabilties between parameters."
"remark 2. the definition of a compatible projection can be abstractly viewed through the lens of category theory, mirroring the idea that one proves a property by mapping a system to the \"simplist\" type of system that has that property [cit] . for safety, these are dynamical systems defined on the entire real line, with the safe set being the positive reals. thus h π is a compatible projection if the following diagram:"
"with increasing advancements in sensor technologies and their importance within the developments towards industry 4.0, the use of real time monitoring and system analysis within new modelling tools is of high interest. 'real time control of energy usage' is believed to have projected savings of 280 trillion btu/yr [cit] . the tool presented by brundage allows for real time system monitoring, using readily available plant floor data. whereas analysis is limited to past data for methods discussed by michaloski and sun, of which limits accuracy and tools cannot capture the extremely random behaviour of hvac and manufacturing systems."
"input-to-state safety describes how the safe set c changes in terms of the disturbance as it appears in the state dynamics (see definition 3 in section ii). this description does not easily permit analysis of how safety degrades when the disturbance is more easily characterized by its impact in a barrier function derivative. this limitation motivates projection-to-state safety (pssf), which enables a characterization of safety in terms of a projected disturbance."
"this paper discusses methods of energy analysis at the machine level using des, as well as efforts at combining manufacturing level analysis with that of the built environment to achieve a holistic understanding of energy flows and consumption. manufacturing facilities are often considered as having a multi-layer hierarchical structure, of which is utilised in developing tools and frameworks. there exists a broad range of analysed manufacturing processes, from cnc machining to casting, with conflicting aims of analysis including bottleneck reduction, efficient factory layout and decision making. furthermore, with the increase in automation and intelligent systems driving industry 4.0, increased complexity in systems requires a new outlook on optimising and analysing manufacturing procedures and processes which is presented in this paper."
"we presented a novel method for assessing the impact of disturbances on safety in a project environment via projectionto-state safety, and considered how it can be utilized in conjunction with learning to mitigate the impact of model uncertainty on safety. we demonstrate the ability of learning to improve the guarantees endowed by pssf in simulation and experimentally on a segway platform. future work includes developing data-driven methods for quantifying the worst case projected disturbance, and synthesizing data-aware controllers that reduce the projected disturbance."
"in this section we consider a structured form of uncertainty in affine control systems. we discuss the impact of this uncertainty in a cbf time derivative, and on the pssf behavior of the system. we demonstrate how learning can be used to mitigate the resulting impact on safety."
"similarly, [cit] presented a multilevel simulation framework (fig. 4 ) for coupling models with life cycle analysis data, using des models for the process chain, predicting energy demands for single processes and the comparison of environmental impact impacts of design alternatives."
"in order to compare the performance of opf over traditional classifiers (svm-rbf 4 and bayesian classifier), we considered only the two best distance metrics found in the previous section, i.e., manhattan and squared chi-squared distances. therefore, we can summarize the techniques to be compared as follows:"
"the move towards a higher level of digitalisation with the increased demand for an interconnected, automated and fully flexible approach has shaped industry 4.0, the 4 th industrial revolution, of which encompasses cybersecurity, augmented reality, big data, robotics, additive manufacturing, cloud computing, internet of things, digital twinning and simulation. at the heart of industry 4.0 is the smart factory, described as 'a manufacturing solution which provides flexible and adaptive production processes to solve problems arising on a production facility with dynamic and rapidly changing boundary conditions in a world of increasing complexity' [cit] . with an increase in digitalisation, incorporated within industry 4.0 and the smart factory is the use of cyber physical systems (cps), which allows interoperability and communication between a set of physical devices, objects and equipment with a virtual cyberspace [cit] . the concept of a digital twin, a near-real-time digital image of a physical component, product or system enriched with sensor obtained production and operation data, shows potential for increasing accuracy of manufacturing system analysis. the digital twin is seen as the next step in model based systems engineering and this 'communication by simulation' allows assistance along the full life cycle of the operation with monitoring and control of the physical system which continuously updates the virtual model [cit] ."
this section provides a review of control barrier functions (cbfs) and input-to-state safe control barrier functions (issf-cbfs). these tools will be used in section iii to define the notion of projection-to-state safety.
"in the presence of disturbances, a controller k synthesized to render the set c safe for the undisturbed dynamics (2) may fail to render c safe for the disturbed dynamics (8) . to quantify how safety degrades, we consider the notion of input-to-state safety [cit] :"
"a number of studies have focused on the use of des in the manufacturing environment, modelling material flows with energy and resource flows [cit] . notably, solding and thollander [cit] investigated the electrical energy consumption of an iron foundry through the use of des simulation, with the aim to combine material flow analysis with energy and resource flows. however accuracy of simulation results were effected as energy consumption was considered at a constant rate, neglecting dynamic behaviour of a single system [cit] . this was considered suitable for allowing the reduction of peak loads and costs as well as efficient production planning, which was highlighted as the main drive for the research, rather than analysis of complex machine tools and accurate determination of all relevant energy flows and consumption. energy data was used as an input, with simulation methods used to analyse processes in order to improve system performance with respect to energy consumption."
"in the context of safety, if a set c ⊂ r n is defined via a continuously differentiable function h as in (3)-(5), a compatible projection h π for the function h with respect to π defines a corresponding set c π ⊂ r k :"
"due to its flexibility, it is increasingly popular to incorporate learning into safe controller synthesis [cit] . many of these approaches seek to provide statistical guarantees on the safety via assumptions made on learning performance. in practice however, limitations on learning performance arise due to factors such as covariate shift [cit], limitations on model capacity, and optimization error. thus, it is critical to understand the relationship between learning error and what safety guarantees can be ensured."
"in contrast to the definition of issf which enlarges the safe set in terms of the disturbance d, pssf quantifies how the safe set enlarges in terms of the projected disturbance δ. to utilize safety guarantees implied by issf-cbfs for analyzing pssf behavior, we require the following definition:"
"journal of manufacturing systems 51 (2019) [cit] number of components made the diagram unclear. similarly, a particle system technique was mentioned along with the use of vr, which allowed for visualisation of dynamic changes of energy consumption over time, as well as highlighting the direction of energy flows (fig. 7) . a cad model of the machine was developed, however cad geometries are generated without concerning energy consumption, and the methodology to which it was created is unknown, and therefore accuracy of the model could not be determined. the methodologies discussed are based on a fixed framework and are related to a single configuration of machine axes. development of a dynamic model with multiple modes of operation, along with real time capabilities, as well as multi-machine considerations would allow for a more accurate analysis of the manufacturing process."
"we can observe that opf with manhattan distance obtained the best recognition rate with dataset d (91.21 %), and that is approximately 0.35 % higher than the second best result obtained with the canberra distance metric (90.88 %), as well as 0.5 % higher than the result obtained with the squared chi-squared metric (90.75 %). additionally, the results using dataset d were the best for all employed distances, suggesting that the method proposed by yu and chen [cit] might be a good feature extractor to be used together with opf. in addition to the recognition rate, we also computed the sensitivity (se) and specificity (sp), as well as the harmonic mean (h) of these two parameters (table 6 )."
"simulation results were validated through interviews with foundry staff, and through comparison between outputs from the model and real system, along with energy mappings. it was concluded that the model was valid, but performance of any statistical tests was not mentioned. the surrounding facility environment was not taken into consideration, with focus solely on electrical energy demand of the casting process. due to methods of data collection, lack of detailed production data and difficulties in deciding boundaries between operating states led to an inaccurate simulation model, with the inability to produce detailed daily production planning. adding automation to data collection methodologies and management was identified as an area for further study, along with the integration of the model with existing energy management systems to improve accuracy of results."
"since we observed that opf and svm-rbf were the most accurate classifiers, our future works will be guided to explore the synergy between these classifiers in order to build an ensemble of classifiers aiming at increasing the the standard deviation is also displayed. the lowest times are indicated in bold"
"under the assumption that the introduction of the estimators does not violate the cbf condition, such that there exists a state-feedback controller k satisfying (35) with ḣ defined as in (37), we may define the projected disturbance as:"
the assumption on local lipschitz continuity of f and k implies that f cl is locally lipschitz continuous. thus for any arxiv:2003.08028v1 [eess.sy] 18 [cit]
"the best values of h considering class n were obtained using canberra (0.78) and squared chi-squared (0.78) distances and feature extractor c. the combination of squared chi-squared metric and extractor c resulted in the best value of h for class s (0.60). in regard to classes v and f, euclidean distance has provided the best results with feature extractor f. as to class q, opf did not classify any sample properly due to the following main factors: the nonconcentrated distribution of samples from that class and the low representation of samples in the training and test sets ( $ 0.00015 % of the total number of samples)."
"in this section, we present the experimental results concerning the effectiveness and efficiency of each pair classifier/feature extraction technique employed in this work. first of all, the opf classifier is evaluated considering six distance metrics: euclidean, chi-square, manhattan, chisquared and squared bray-curtis. after that, a comparison among opf with the best metrics, support vector machines with radial basis function (svm-rbf) and a bayesian classifier (bc), is then presented."
"the framework was applied to a case study for a manufacturing process in the automotive industry, coupling two continuous models, a discrete event, a hybrid and a state based model. energy demand, costs and environmental impacts were assessed along with measurements for improvement, however the study only developed a theoretical framework, with no application to industry or model validation mentioned, or comparison with measured data. focus was on water-energy nexus, rather than holistic detailed analysis of all levels and all relevant interlinked energy and material flows."
"we refer to c as an input-to-state safe set (issf set) if such a set c d exists. this definition implies that though the set c may not be safe, a larger set c d, depending on d, is safe. if d ≡ 0, we recover that the set c is safe. c can be certified as an issf set for the closed-loop system (8) with the following definition:"
the following theorem allows us to extend issf properties of the projected system on c π to pssf properties of the original system on c.
"our paper is organized as follows. section ii provides a review of control barrier functions and input-to-state safety. in section iii we define projection-to-state safety (pssf) and discuss how pssf enables quantifying degradation of safety in terms of a projected disturbance. section iv defines a broad class of model uncertainty and explores how learning can be used to mitigate the impact of this uncertainty on safety. lastly, in section v we present both simulation and experimental results using pssf to quantify the impact of learning error on safety guarantees for a segway system."
"generic definitions of process inputs and outputs allowed for flexibility in terms of level of model detail, as averaged information in the lci database could be used rather than individual specification of parameters for each simulation. however this also reduced accuracy of the model due to model simplification and potential exclusion of energy relevant flows and processes. unavailability of certain parameters within the database may also require system monitoring and metering for each process, leading to inconsistency in model accuracy between simulations. consumed resources such as energy, material and water were required as inputs to the module, rather than being determined by the model."
"two case studies were carried out which focused on electrical consumption of an aluminium die casting process chain, with compressed air and steam assessed in the simulation of a weaving mill. required variables were collected via measurement prior to the system simulation which showed consistency of above 95% in comparison to simulation data. statistical tests t-test showed statically significant results. various scenarios were tested, with determination of efficiency, production output, energy consumption and yearly electrical energy savings. rather than a focus on simulating energy demand on the process and component level, the study was directed towards energy orientation, with the study of dynamic interactions of processes and auxiliary equipment for efficient planning of manufacturing systems. the impact of unit processes was excluded due to use of rated power for energy consumption predictions [cit] . waste heat emissions were also neglected."
"studies on novel tools developed for the vison of industry 4.0 are predominantly based around process improvements to increase throughput and productivity, by increased automation and connectivity, such as multi-level analysis [cit], rather than that of improving energy efficiency in manufacturing processes and systems. the m4 tool was based on analysing factory performance with routing and product flexibility, while rodič focused on optimising machine layout with no mention of material or energy flows or integration of manufacturing processes with the surrounding environment, nor quantitative analysis of manufacturing processes. tools presented by jain and terkaj focused on increased automation of the simulation process. the adoption of the digital twin in manufacturing facilities holds great potential for analysis of energy and material flows due to the capabilities of accurate machine representation and real or near-real-time process analysis. further work is required to develop the technology with expansion of analysis into the manufacturing production chain with consideration of the surrounding facility environment, along with relevant material and energy flows."
"ensuring safety is of significant importance in the design of many modern control systems, from autonomous driving to industrial robotics. in practice, the models used in the control design process are imperfect, with model uncertainty arising due to parametric error and unmodeled dynamics. this uncertainty can cause the controller to render the system unsafe. as such, it is necessary to quantify how the desired safety properties degrade with uncertainty."
"multiple tools neglected dynamics or interdependency between all layers, with focus on energy associated with a single unit process or creation of an individual part. analysis of production processes from machine level through to process chain with interaction of machines within the production line would be advantageous. furthermore, inclusion of material flows and thermal effects would aid in understanding of the facility, as heat transfer from machining processes can have a significant impact on energy consumption due to additional demands required from hvac systems to maintain a certain environmental working condition. a considerable number of presented models and frameworks lack validation, which possess uncertainty with regards to the accuracy of the methodology as well as applicability to industry. current technologies involved in developing the smart factory were discussed, with digital twinning showing great potential for increasing the accuracy of simulation, with the ability to replicate individual facilities and processes with the addition of real time data rather than having to rely on a database of generic manufacturing processes or modelling assumptions and simplification during model development. studies show the use of virtual and augmented reality can aid in understanding of energy flows for less experienced workers along with providing education on how to control processes for more energy efficient production. it can provide a visualisation tool rather than a method of energy analysis. however due to the novel nature of these technologies, studies are limited and provide a basis of understanding as well as highlight their potential within the manufacturing industry."
"with the aim of addressing the need for a holistic approach, [cit] presented a co-simulation approach allowing for sub layers to be modelled using the most appropriate simulation environment or analytical approach with the use of middleware. a case study was studied, which calculated electrical power demand and heat losses for a metal cutting manufacturing facility. however, the model could not be validated, as most parts of the simulation were non-existent at the time of validation. authors assumed the model was accurate and valid only due to validity of process sub models. co-simulation presented a difficulty of conflicting time discretization resolution between models, resulting in numerical errors and stiff systems of differential equations. a solution was the reduction of model complexity, focusing on the most energy intensive parameters and characteristic base models, which however, may result in the inaccurate capture and neglect of potential energy intensive processes. a number of assumptions are made due to model complexity, with presupposed sub model requirements. the proposed tool was considered suitable for energy prediction during early planning phases, with the ability to identify further potential for energy savings."
"emphasis has been predominately on tools providing outputs for effective decision making and process planning rather than accurate analysis of energy flows or tools for energy optimisation. further work is required to develop a tool capable of accurate determination of relevant energy, resource and thermal flows associated with manufacturing processes, along with a methodology of enabling interaction between different levels within the manufacturing production environment. furthermore, further work is required for the development of novel techniques such as digital twinning and its use in the manufacturing industry as a tool of providing computationally efficient and accurate models."
"for the use of ar in manufacturing, operators can carry out machine operations and also be provided with real time data and process information simultaneously without leaving the work piece [cit] . environmental impacts of the operation can also be viewed in this way, with users able to take immediate action dependent upon feedback. its use in collision detection on sorting lines [cit] and use in robot control [cit] allows for visualisation of scenarios and process risk assessment. ar has also be used for factory layout planning and maintenance and is considered a valuable teaching tool, especially in product assembly [cit] . nee and ong [cit] discuss the use of ar in aiding operators in cnc machining, with reflection of dynamic tool movements providing the user with real time information on cutting parameters and cnc programs, as well as alarms and errors in machining."
"in practice, the system dynamics (1) are not known during control design due to parametric error and unmodeled dynamics. instead, a nominal model of the system is utilized:"
"control barrier functions (cbfs) have become increasingly popular [cit] as a tool for synthesizing controllers that provide safety via set invariance [cit] . safety guarantees endowed by a controller synthesized via cbfs rely on an accurate model of a system's dynamics, and may degrade in the presence of model uncertainty. the recently proposed definition of input-to-state safety (issf) provides a tool for quantifying the impact on safety guarantees of such uncertainty or disturbances in the dynamics [cit] by describing changes in the set kept invariant."
"however, a high recognition rate not always reflects a satisfactory performance in terms of classes separation, once that only class n (patient without cardiac arrhythmia) represents % 90 % of all dataset. for instance, let us consider the case of chi-square metric, which presented the best accuracy rates for feature extractor b (table 5 ). the the most accurate result is indicated in bold good results of such metric did not lead us to a satisfactory performance in terms of classes separation, since it presented low values for sensitivity and specificity for all classes, except for class n. this is due to the misclassification of most samples of classes s, v, f and q, as belonging to class n, leading to a low harmonic mean (2 %). in order to clarify this, the confusion matrix related to feature extractor b and squared chi-square metric was built, table 7 . from the data obtained, one can verify that the dataset is dominated by class n, which clearly influ- b 002-100-001 000-000-100 000-000-100 001-001-100 000-000-100 c 015-098-008 002-001-100 017-009-098 000-000-100 000-000-100 d 004-100-002 000-000-100 004-002-100 000-000-100 000-000-100 e 004-100-002 000-000-100 004-002-100 000-000-100 000-000-100 enced all other classes. this can be confirmed by analyzing the results obtained for classes s, v, f and q that had the majority of the samples misclassified as being from class n (first column of table 7 ). also, it is important to stress that the accuracy calculated in this work do consider unbalanced datasets [cit] ."
"social media is seen as an extremely important developer resource that provides updatable documentation and high quality answers as well as dialogue opportunities . having improved response time significantly compared to traditional communication methods ), social media is unavoidable for software developers. the outcome is a shift in work processes, and the wish to communicate face-to-face is challenged by many-to-many communication . however, whereas some perceives social media to provide a larger user community than accessible by traditional communication tools [cit], concerns are also raised regarding information overload and quality assessment [cit] . thus, social media is creating novel preconditions and expectations for software developers to adhere to."
"after recording the data from the studies, quantitative and qualitative analyses were conducted. from these analyses, characteristics and findings were identified according to the objectives and research questions of this review."
"one of the major research gaps on knowledge sharing via social media identified in this review is a lack of focus on the individual. more than 85% of the studies focus at the organizational or group level. for a more comprehensive and adequate understanding and knowledge of the field, it is necessary to concentrate more research on the impact of social media use on the individual."
"we organize the rest of the paper as follows. in the next section, we discuss the tandem approach in speech recognition. in section 3, we describe the multi-stream and coupled hmms. we present how a coupled hmm can be represented as a stream-tied mshmm in section 4. section 5 gives the details about the experimental framework and the architecture of the system. results of the experiments are presented and discussed in section 6. finally, we present our conclusions in section 7."
"the vast majority of the identified studies in this review used an empirical approach (see figure iv) . roughly a quarter of the studies were purely analytical, and only four literature reviews were identified. as mentioned prior, research within the field of social media for knowledge sharing is still developing, especially given more recent additions and developments. therefore, there may not be a comprehensive body of data for conducting a literature review at this time, which is suggested by the fact that only four review were identified in this study."
"in peer-to-peer (p2p) networks, each peer can be used as the service provider and the visitor, and also can provide recommendations at the same time."
"small gait cycle durations allow animals to kick the ground frequently for acceleration and achieve high-speed locomotion [cit] . because short flight durations induce small changes in the com height, we investigated the com height changes. from (7), we obtained the com height changes in the first flight ℎ 1 and second flight ℎ 2 by the difference between the maximum height at apex and the minimum height at foot contact in each flight as follows:"
the aim of the research is to explore and map the current state of studies on knowledge sharing via social media related to software development.
"social media has introduced a dramatic change to the software developer landscape and old assumptions are challenged . today, software developers openly contribute to online knowledge creation and the recognition of social connectedness is gaining increasing attention )."
"although originally proposed for audio-only speech recognition, the idea can be used for video based features as well by applying the same process to extracted video features. in this work, we employ the tandem approach for video data, using multiple classifier outputs in addition to regular observation features. we model all streams of data using a mshmm where each stream comes from different sources; one for audio features, one for video features and two sets of features extracted from tandem classifiers."
"this review follows a five-step approach [cit] ). this approach allows for comprehensive coverage of the literature and ensures auditability and repeatability for future searches. figure i illustrates the five steps, which the following sections detail."
"recommendation each peer in the trust model metrust has the unique credibility of recommendation, reflecting the credibility of a peer in the p2p system, and embodying the peers' \"reputation\" in providing recommendation. in the trust model, malicious peer can be identified by updating credibility of recommendation, which can ensure the security of p2p systems. firstly, we give some related primitives and corresponding semantics as below: 1）the trust evaluation algorithm that peer i compute the totol turst of response peer j is as follows: 2）after calculation, peer i take the interaction with responder peer j with the maximum trust value and need to perform the evaluation process, evaluation and update algorithm is as follows:"
"one extension to mshmm is coupled hmm in which independent transitions of streams are allowed. in this model, the probability of transition for one stream depends on previous states of both streams:"
"generally, the trust computation in p2p networks is as follows, which was also referred in other paper [cit] :, where uses a function based on cosine similarity to describe the degree of similarity between two peers comparing difference of peers' weights."
"in our work we model a mshmm system, where one stream is audio data, and other three streams are visual data. since asynchrony is a problem only between data acquired from different sources, we take it into consideration between one audio stream and the group of three video streams."
another absent theme is implementation. social media has been recognized for its communicative and collaborative potential and is expected to increasingly expand from the private sphere into companies.
"conventional speech recognition with hidden markov models (hmm) [cit] processes audio data using hidden state machines with markovian transitions and gaussian mixture emissions. since audio channel noise is an important factor that affects recognition accuracy negatively, audio data may be processed so that it is less sensitive to noise or supported with visual data to increase accuracy."
"social media is constantly developing, and therefore research struggles to keep up with its progress. thus, there is a need to be more prospective and to decrease the duration of the review processes. furthermore, observational studies and research of implementation of social media will add to existing research and improve available knowledge in the field."
"a summary of the information contained in each paper was prepared in spreadsheet format organized in terms of descriptive, methodological, and thematic categories (table iv) . the descriptive and methodological analyses were more deductive in nature and focused on the categorization of the identified studies by year, journal, title, paper type, etc. a more inductive approach was taken for the thematic analysis, which analyzed and mapped the discussion in research studies of knowledge sharing via social media in the field of software development. the goal of this was to identify emerging constructs related to different social media tools for knowledge sharing, as well as to identify gaps in the literature and future research agendas. two authors coded the data independently to ensure inter-coder reliability [cit] ."
"given, that software development is one of the most mature and advanced fields in terms of social media usage for work purposes, it offers a comprehensive foundation for the analysis and comparison of novel technologies. this slr is part of a research project that is exploring how social media is used to share knowledge within the field. corresponding with the main research topic of the project at large, the primary inquiry of this review relates to how social media can support knowledge sharing activities."
"we investigated this dynamical effect based on a model which incorporates θ as the pitch angle of the whole body, as shown in fig. s1 . in this case, the foot contact does not necessarily occur simultaneously between the fore and hind legs."
"in order to increase audio feature robustness, hmm's generative modeling of the observation data can be supported with discriminative classifiers where outputs of the classifiers are used as observation features, resulting in a tandem hmm system [cit] ."
"in the first step of this review's approach, the research questions to guide the review were formulated. a clear research question is critical to provide focus and direction to an slr. the main research question was established via discussions between the authors and colleagues within the knowledge sharing field. the research question was formulated using the following cimo logic [cit] ."
"as noted prior, implementation is not discussed in any of the identified papers in this review. thus, more research on the successful introduction of social media for knowledge sharing in companies in needed to advance an understanding and knowledge of the field."
"the outputs of the classifier are then considered in the hmm model as observation vectors. usually the values are directly used, so for a tandem classifier trained for a number of c classes, hmm observations are vectors of length c."
"the results clearly show that, as the snr decreases (i.e. noise increases) the weight combinations that emphasize visual data tend to give better results (can be seen by comparing audio only results with video only or audiovisual results). this is a well known result in audio-visual speech recognition, since due to audio noise, contribution of audio channel to the accuracy decreases and eventually becomes zero. also as proposed, contribution of tandem data to the accuracy can clearly be seen since for each model (whether regular mshmm or stream-tied mshmms) tandem stream employed results are better from audio-visual results without tandem data at most of the snr levels. the increase in accuracy achieved by visual tandem streams evidence the improvement of the proposed method over conventional observation only based audio-visual speech recognisers, which are state of the art in audio-visual speech recognition."
"papers related to topics where the focus is not on knowledge sharing and social media many papers focus on other ways to share knowledge, but the focus in this study is on knowledge sharing via social media."
"many of the case studies use content log analysis or something similar, but it appears advisable to complement existing research by taking a step back and concentrate some additional efforts on understanding the scattered practices. however, this could interfere with the goal of being prospective."
"to simulate noisy recording conditions, we have added noise in different snr levels to the audio signal, and trained models using only clean audio. the noise is the volvo 340 car noise obtained from the noisex database [cit] and the snr levels are determined using the \"audio voltmeter\" program from the g.191 itu-t stl software suite [cit] ."
"this review investigated how the different identified studies were conducted. the adopted research methods of these studies include a great mixture of quantitative and qualitative methods, as well as case studies. in addition, many studies have combined quantitative and qualitative methods to complement each other. this indicates a great mixture of research on knowledge sharing via social media, which ensures a more broad and accurate picture of the field (see figure iii) . one particular method that appears to be overlooked in the studies is that of observations. while the many case studies all include activity logs and log content analysis, there is a lack of observational studies, which could provide new findings on the relationship between social media and knowledge sharing."
"the rest of this paper is organized as follows: section ⅱ discusses related work on trust models. section ⅲ introduces our trust model based on preference and algorithms for computing the trust value. simulation-based experiments are showed in section ⅳ. in the last section, we present conclusions."
"our mshmm consists of four streams; (1) audio, (2) visual, (3) visual tandem using svm classifier and (4) visual tandem using nn classifier. the contribution of each stream to the decoding process differs on each snr; we examine different stream weights between 0 and 1, in steps of 0.1. we use stream-tied mshmm model equivalent to a chmm as proposed in section 4. first we model the phones with three states, and train an audio-only hmm. to obtain an audiovisual mshmm, we concatenate visual data to audio data and train the multi-stream model using single-pass retraining from the audioonly hmm with only one iteration. next, to create an initial model for the chmm, we couple the states by adding the hybrid states during which we take audio stream alone and couple it with the remaining streams (since all of them are derived from the same visual channel) thus resulting in nine states for each phone. then using the state coupled regular mshmm model as the initial model we apply two different training methods as discussed in section 4.1 to train stream-tied mshmms equivalent to the chmm. stream weights are given as equal values during stream-tied training."
"as a popular distributed computing paradigm, service-oriented computing makes software applications even more attractive due to it provides a lot of benefits like scalability, loose-coupled and on demand business applications in heterogeneous environments. however, along with these benefits, web service also raises some concerns especially how to ensure the correct running of service applications [cit] . a critical challenge is how to build high-reliable composite service applications in the large-scale and complex computing environment."
"in recent years, p2p (peer-to-peer) technologies [cit] have been widely used in the vast field for its unique advantages, for example, file sharing, collaboration, instant messaging, communication, e-commerce systems, etc. p2p overlay networks can be divided into structured systems, unstructured systems and other systems with new structures to improve the efficiency of searching in the p2p network [cit] . p2p technologies allow people to interact with each other directly via the internet, which makes it easier to communicate on the network. however, some disadvantages of p2p systems, such as anonymity, autonomy, and other characteristics have also led to some security issues affecting the quality of service of p2p networks. for example, some users suffered because of the malicious deception from illegal seller and no longer do online shopping; online bank accounts of some users were stolen and suffered heavy losses; some users cannot buy desirable commodity due to the lack of experience. some researches on the trust model [cit] in p2p systems show that using trust model can identify malicious peers effectively. trust models can measure the credibility of the peer in various aspects, which has significance in improving the quality of service in p2p systems."
"an automatic search of electronic databases was conducted using a broad and comprehensive search string. the search string included a combination of three sets of keywords that related to social media, the subject, and the discipline. these three sets of keywords were used to construct a search string with the boolean operators and and or."
"the individual is surprisingly absent in the identified studies of this review, which primarily focus on general impacts and benefits at the organizational level. increased attention should therefore be given to the effects of knowledge sharing via social media at the individual level so as to gain a better and allencompassing understanding of how these technologies can be effectively utilized."
"we ignored the pitching movement of the whole body in our model (fig. 2) because the com vertical and spine joint movements are more important for determining galloping dynamics, compared with pitching movements. this assumption induced simultaneous foot contact between the fore and hind legs."
"an organizational focus is most commonly used in the selected studies. more than half of the entire set of studies investigate the impact, potential, etc., from an organizational perspective (see figure vii) . in these studies, the focus is, for example, on how organizational processes are affected by social media, or how social media can contribute to improve them. the main portion of the remaining studies focus from a group perspective on the processes, for example, within a single development team or department. hence, these studies are more narrowly defined."
"stability analysis 156 when we found periodic solutions, we computationally investigated the local stability from the 157 eigenvalues of the linearized poincaré map around the fixed points on a poincaré section. we 158 defined the poincaré section by the state just after the second foot contact. because our model 159 is energy conservative, the gait is asymptotically stable when all the eigenvalues except for one 160 eigenvalue of 1 are inside the unit cycle (these magnitudes are less than 1)."
"where q i t represent stream states, qt is the joint state and α1 and α2 are the stream weights. the transition probabilities between hybrid mshmm states are related to the chmm model by the following formula:"
"we take a subset of training data as validation set and for each noise level find the best audio-visual stream weight combination on this validation data. for \"clean\" snr we take audio stream weight as one and visual stream weight as zero since our models are trained on clean audio. these stream weights are presented in table 2 . for recognition results on test data, we train models as proposed in section 4.1 using whole training data and for each type of mshmm and snr level we present four different recognition accuracy rates in table 1 . for the columns labeled \"audio\" and \"video\", only audio or video stream is active by giving zero weights to the unused streams and one to the used stream. for the \"audio visual\" column, the audio and video weight combination found from validation set is utilized, with zero weights for tandem streams. for the \"audiovisual and tandem\" column, video weight is distributed to visual and tandem streams such that; half of video weight is given to visual stream and other half is equally divided to the tandem streams."
"here, we derive the relationship (3) between the states immediately prior to and immediately following foot contact in the model. we assumed elastic collision for foot contact that involves no position change and energy conservation. we define ∆ p as the impulse at foot contact from the ground in the vertical direction. ∆ p ϕ is the change in the angular momentum caused by the impulse. the relationship of the translational and angular momentum between immediately prior to and following the foot contact gives"
"in animal galloping, the pitching movement of the line connecting the root of the neck and the 101 hip is relatively small. in our previous study [cit], we used a physical model 102 composed of two rigid bodies and two legs, which was able to perform a pitching movement as 103 well as vertical movement. the simulation results revealed that the vertical movement of the com 104 and the spine joint angle between the bodies were significant determinants of the dynamic char-105 acteristics of bounding gait, compared with pitching movement. in the current study, we neglected 106 the pitching movement of the model, making the com vertical positions of the fore and hind bod-107 ies identical. furthermore, even when we also ignored the horizontal ground reaction force in 108 our previous work [cit], the principal dynamic characteristics in bounding gait 109 remained unchanged. therefore, we also neglected the horizontal ground reaction force of the 110 model, which allowed us to ignore the dynamics of, and assumed that the leg bars were always 111 vertical to the ground. we discuss the effects of our assumption about the pitching movement on 112 the galloping dynamics in more depth in supplementary information s1. 113 during the flight phase, the equations of motion for and are given by"
"the systematic literature review (slr) presented in this paper aimed at providing a comprehensive overview of studies focused on knowledge sharing via social media, as well as identifying gaps in the literature to assist future research agendas."
"comparing results across different models can give information about using regular mshmm or two different training strategies for stream-tied mshmm. the regular mshmm model is used as an initial model to generate stream-tied models and since state coupling, stream tying and parameter (whether only transition or both transition and emission probabilities) training changes the structure of the models, the results do differ for audio-only and video-only columns across models. the increase in the accuracy for tandem employed models between regular and stream-tied mshmm trained with first method shows the benefit of taking asynchrony into consideration together with tandem streams. comparing two different training strategies for stream-tied mshmm show that training emission parameters together with transition probabilites yields better results than transition-only update method in lower snr values. however for higher snr values, the transition-only update method seems to work better. in our future studies, we plan to investigate these results on different datasets."
"using posterior probabilities of a classifier as a feature vector is a well known technique in speech recognition [cit] . the idea in this \"tandem approach\" is adding a classifier layer after feature extraction. the class definition for the classifier can be chosen parallel to the hmm, such that each class can be one of words, sub-words, phones, monophone states or context-dependent phone states. for example, consider a monophone hmm model for digit recognition with ten words (one for each digit), around twenty phones (depending on the language) and a total of sixty monophone states. the tandem classifier may be trained to discriminate one of these units of the model."
"because the first and second terms of the right-hand side are positive and negative, respectively, the sign ofφ + (τ 1 ) and the type of the second flight depend on ψ 1 and c 1 . when"
"in this work we propose architectures of multi-stream and coupled hmms, which uses both direct audio-visual observation features and visual tandem features extracted from support vector machine (svm) and neural network (nn) classifiers. neural networks, particularly multilayer perceptrons were successfully used in tandem speech recognition studies before [cit] . svms were also employed with success in speech recognition as well [cit] . these two different classifiers have strong and complementary properties which makes them good candidates for extracting separate streams of posterior probabilities. according to the best of our knowledge, it is a novel idea to use multiple classifier posteriors as separate streams in speech recognition. conventionally, classifier combination may be performed at the frame level by decision fusion of individual classifiers' posterior probabilities, however we show that one can have much higher improvement by using model-level fusion through the use of multiple streams. we have only used visual tandem streams since for our problem of interest we have not had much improvement with additional audio tandem streams. however, audio tandem streams may also be incorporated to the system to improve accuracies in general. in addition, we implement the chmm which allows asynchrony of audio and visual streams with a modified multi-stream tied hmm model [cit] in this work. this implementation enables efficient initialization and training procedures for the chmm and yields much improved results in accuracy."
"the main standard to verify the correctness and usability of web service composition is that the composite service applications can run correctly and achieve the users' goals successfully [cit] . therefore, it is critical to analyze and verify web service composition before the implementation of composite service applications. at present, there exits three formal methods for verifying the web service composition: petri nets, finite automation and process algebra [cit] . the behavior theory of pi-calculus provides a good theoretical basis for the verification of web service composition [cit] . so, we use pi-calculus to model and verify the behavior compatibility of web service composition in this paper."
"the rapidly growing interest in social media across various research fields necessitates an overview of the current research on the topic. however, research on knowledge sharing via social media related to software development is still in its early stages and initial engagement with the literature in this field revealed it to be confusing and scattered. there are many disparate conceptualizations of social media [cit], and the literature is often vague and dispersed [cit] . despite the field's lack of an overview ), a growing interest in social media and their potential offerings was evident in the literature. the literature showed identifications that the field could be facing rapidly gamechanging development and many studies popped up in different and scattered settings. as the introduction of social media to the general discussion on computer supported cooperative work (cscw) is somehow still novel and emerging, the field has not reach a mature state with common references and a general valid background knowledge database. therefore, this paper conducts a systematic literature review (slr) of existing research literature on knowledge sharing via social media in relation to software development. this review can be used to identify possible gaps in the literature so as to inform future research agendas and generally improve research within the field."
"the field of social media is experiencing increasing attention. however, it is still an emerging field, and excluding works-in-progress such as conference papers and working papers could leave out important research."
"however, while im is widespread within the field of software development, the fact that this specific type of social media is the focus of only two of the identified studies in this review shows that im is not considered a viable tool for knowledge sharing."
"when identifying studies within the same body of research or the predecessors of another identified study, only the most complete or newest version was retained. the removal of duplicates, and subsequently, those studies on the first screen (based on the relevance of the title to the research question) resulted in the retention of 565 studies for a more in-depth review of the abstract. selection was then carried out by first reviewing a study's abstract and afterwards reviewing the full papers that were selected based on their abstracts. a total of 459 studies were rejected predominantly due to a review of their abstracts, with some rejections based on a review of full papers, leaving a total of 106 studies remaining. table iii shows the selected databases and the corresponding number of identified objects based on the search string."
"this review also investigated how research within social media for knowledge sharing is being presented. figure v shows that the preferred way to publish research within the field is via conference papers. certain of these studies may have only qualified as conference papers, and thus were not developed further into journal papers. however, there might be another explanation as well. as mentioned prior, the technology within social media is rapidly changing and developing. in combination with a comprehensive and time consuming review process of journals this makes a very bad match. therefore, sometimes it is preferred to use conferences to present ideas and research and it is not seen as worthwhile to go into the process of making a paper publishable for a giving journal. this is a challenge that needs to be dealt with within the research community. as social media research often focuses on the potential for fast and easy knowledge sharing, immediate collaboration across spatial and temporal boundaries, etc., perhaps some of these ideas and methods are suitable for advancing the publication process as well. moreover, a considerable amount of the studies place focus on km activities and knowledge sharing."
"firstly, the credibility of recommendation of each peer is showed in figure 11 . the number of peers is 100, and 1-80 acts as good peers, the other acts as malicious peers. 50% of malicious peers, that's 85,86,87,89,91,94,95, 96, and 98,100, compose em class peers, and others are im class peers. figure 11 shows that the trust model can identify em class peers. then figure 12 compares success rates of four trust models with 30% malicious peers when the proportion of em class peers changing from 0% to 50%. as showed in figure 12, metrust, the trust model proposed in this paper, can better control this kind of malicious behavior. this paper presents a trust model in peer-to-peer networks. each peer in the system has a unique credibility of recommendation. analysis and simulation show that the trust model can evaluate the peer's trust value with the smaller overhead, and identify malicious peers in p2p networks, which improve the quality of service in p2p networks effectively."
"when the foot touches the ground, it receives the grf. because the com vertical positions are identical between the fore and hind bodies, the foot contact of the fore and hind legs occurs simultaneously. this condition is given by"
"however, the question of how companies should prepare and act in order to ensure the successful implementation of social media is not present within the studies identified in this review."
"we presented a new method for audio-visual speech recognition which uses multiple visual tandem features in parallel with regular audio and video features in a multi-stream hmm framework. we experimented with synchronous and asynchronous hmm methods using multi-stream hmm and coupled hmm. it is shown that by using visual tandem features an improvement in recognition accuracy is obtainable. in addition, asynchrony modeling brings additional improvement in recognition accuracy. we believe this method is an attractive approach in audio-visual speech recognition and there are many potential areas for improving the method such as using different classifiers, utilizing an increased number of tandem streams, better roi and visual feature extraction techniques and employing better initilization and training methods for the coupled hmm which we plan to pursue as future work."
"we examine the proposed model with the m2vts video database [cit], which consists of videos of 37 different people recorded in five sessions and arranged in five tapes. we have used first four tapes as training data and the fifth tape (excluding one video due to occlusion on the chin) as testing data, since first four tapes are recorded under similar conditions and for each subject fifth tape has some visual differences (e.g. glasses, hat) that add extra difficulty to the test set. using only fifth tape for testing is different from the jack-knife approach on previous works [cit] and may add challenge and cause decrease in visual recognition accuracy. on the videos, the speakers say ten french digits so we have ten words (digits) and 19 phonemes."
"general knowledge sharing [ww11], while im is often used as a real-time glue between different communication and collaboration channels [dg11] . moreover, social media raises awareness of and simplifies knowledge sharing through lightweight tools ."
"together, these four categories account for more than 60% of the entire set of studies (see figure vi) . a great distribution is seen in the remaining themes, including motivation, general usage, and information sharing as the most represented. the diversity of themes helps to develop a more comprehensive understanding of the field. however, further studies are required on more minor topics, such as barriers, innovation, and success factors, to establish broad conclusions. as social media is known to be vital to socializing, it is surprising to find that only a few studies have investigated themes such as trust and socialization processes."
"however, one problem in this assumption is that, real world data are not always in perfect synchrony. in an audio-visual system, this asynchrony can be due to the nature of the speech generation process or because of small delays in audio-visual data acquisition and processing. an example is in generation of plosives like the phone \"p\" where the lip position changes before the hearing of the plosive sound. so, one should consider modeling the difference of \"generation timing\" across the modalities."
"the criterion of effects in our experiments is the success ratio of transactions, which is the percentage of the number of successful transactions versus the number of total downloads in a query cycle."
"this systematic review aims to locate, select, and appraise as much of the relevant research as possible related to the research question [cit] ). this second step of the review's approach involves the creation of a comprehensive search string to ensure the identification of all relevant studies."
"then figure 10 shows success ratios when the probability f changing from 0 to 0.8 with 30% malicious peers. metrust, proposed in this paper, is superior to other trust models, which can deal with dm class peers."
"overview of studies figure ii shows the type of social media discussed in the studies identified in this review. it clearly shows that wikis are the most comprehensively researched type of social media, seconded by social networks."
"the periodic solution gives an important criterion to determine the solution type; the signs oḟ− anḋ+ are different for one type of flight (solutions of types c, e, cc, and ee) while they are identical for two different types of flights (solutions of types ce and ec). the criterion gives a hypothesis: while the effect of grf is too small to change the direction of the spine movement in the gallop with two different types of flights, the effect is so large that the direction changes in the gallop with one type of flight. we evaluated this hypothesis as follows. the difference oḟ+ anḋ − is given by"
"in p2p network, each peer not only can provide services, but also can obtain services, besides they also can give recommendation of some peers. this paper takes the p2p file-sharing network as an example, and describes the trust relation."
"has also been suggested that cheetahs achieve high-speed locomotion by extended flight, so that 360 the touchdown of the forelimbs does not decelerate in the horizontal direction (bertram and gut-361 [cit] ). we would like to improve our model to investigate these effects in future research. 362 our model included a torsional spring connecting two rigid bodies. previous animal measure-363 ment data suggest that animals use their bodies as elastic structures, such as the tendons in the 364 torso [cit] . however, trunk muscles are also effectively used as actua-365 tors to produce energy for acceleration [cit] . finally, 366 we also intend to investigate the effect of trunk control on locomotion speed and energy efficiency 367 in the future."
"to be able to get recommendation effectively, this paper proposes a trust model based on recommendation in p2p networks. in the trust model each peer has a unique credibility of recommendation. experiments and simulation results show that the trust model proposed in this paper can identify malicious peers in p2p networks, and can improve the quality of service effectively."
"trust models have been widely used in e-commerce, distributed computing, recommender systems [cit] . trust models compute the trust value in the peer mainly through the quantitative evaluation system, to forecast the capability of providing service in this peer. a peer's trust value can be the gist of other peers to decide whether they will get service from this peer, and after interactions other peers can update their opinions on the peer providing the service, such as ebay. currently trust models based on recommendation [cit] in p2p systems mainly compute the trust value of the service provider from its own interactive experience and recommendations from other peers. some characteristics of p2p network such as heterogeneity, autonomy increase the complexity of the trust evaluation, and some researches [cit] put forward their trust models to solve certain security problems. however, trust evaluations in these trust models don't consider this condition refers to different preferences exist in different peers. different peers getting the same service from the same peer will have different views to the peer providing the service who will receive unfair judges, which impacts the quality of service in p2p network."
"therefore, certain tools are only seriously researched after they become mainstream . the findings of this review show that researchers are only beginning to recognize the need to focus on newer aspects of social media and their specific functions, rather than offer comprehensive general discussions. gamification, q&a sites, and microblogging, along with many other aspects of social media, have been part of the field's toolbox for many years now. such features are already well implemented and used in the everyday life and the work processes of companies throughout the world. despite this, however, research is only beginning to focus on these areas. due to the rapid technological development within the field of social media, tools are constantly updated and quickly outdated, putting research in danger of being outdated even before it is published."
"inconsistency of peer evaluation criteria will lead to unfair assessment of the service provider, the trust model proposed in this paper take evaluation criteria differences into consideration to solve this problem. peers need to determine their own evaluation criteria, and provide to the other peers as the interaction reference. the evaluation criteria of the peer weights are realized with n-tuple."
"the ahp is a structured technique for helping people deal with complex decisions. rather than prescribing a \"correct\" decision, the ahp helps people to determine one. an ahp hierarchy is a structured means of describing the problem at hand. it consists of an overall goal, a group of options or alternatives for reaching the goal, and a group of factors or criteria that relate the alternatives to the goal. the ahp hierarchy for preference is showed in fig. 1 . the ahp hierarchy for preference definition 1. declaration of preference: after computation by ahp, each peer has a n-tuple for weights of his preference which will be declared in p2p network: preference. a peer can declare his weights in accordance with his preference freely."
"here, we show the mechanism by which the symmetry condition (12) forces the third and fourth rows in (11) to be satisfied. the substitution of (12) into the first row of (10) giveŝ"
"we first train a regular mshmm model and then form hybrid states from them. the emission probabilities of the hybrid states are initialized using the original mshmm states' emission probabilities. since the original mshmm describes transitions only between the original q states, the transitions involving the new states cannot be inferred directly from them. one can initially give equal probabilities to transitions:"
"the last group of studies focus on the individual. however, these amount to as little as 14% of the entire set of studies. these studies focus on, for example, participation in online communities [cta09], how usability information is shared [bhc08] and discussions on q&a sites [tbs11], [byh14] . the majority of the studies in this group focus on how social media is used, and hence do not examine how social media affects the work processes of a single individual or their accompanying impacts. very few studies touch upon factors such as motivation [hl07], [pts13], [vsdf14], trust [avhpm11], or work performance and value [cvglg12] . this review reveals a tendency in research within knowledge sharing via social media toward investigating the subject from a general perspective, with a lack of focus on the impacts at the individual level. this conclusion is supported in [stdc10] where they proposed ten research questions that seek to understand the implications of social media related to software development. however, only one of these questions addresses impacts on an individual level (i.e. interruptions and information overload)."
"research on knowledge sharing via social media is characterized by a general approach. therefore, additional studies on specific tools and processes will help to achieve a more comprehensive understanding of these different tools/functions and their related benefits and challenges."
"2) malicious peer group (cm): malicious peers form a group to carry out joint fraud. peers in this class have the function as im class, and also exaggerate the members of the same group and denigrate other good peers;"
"firstly, the credibility of recommendation of each peer with existing im class peers is showed in figure 6 . as can be seen from figure 6, in all 100 peers, peer 1 to peer 90 are good peers, and peer 91 to peer 100 are malicious peers, 50% among malicious peers(peer93、96、97、99、 100)consist the cm class, and the remainder is im class peers. in figure 6, the credibility of recommendation of cm class peers is a bit low; while that's of other peers providing honest recommendation have no significant change. then the changing trend is given that incredible downloads of cm class peers over cycles of four models with 20% malicious peers. as can be seen from figure 7, the trend is similar to the figure 3, and not goes into details here. in the case of the presence of different scale cm class malicious peers, the download success rates of four models are compared. proportions of malicious peers in all peers are 0%, 10%, 20%, 30%, 40%, 50%. as showed in figure 8, eigentrust model has no action to cm class peers, and malicious peers in cm class will be easier to obtain higher credibility with the increasing of cm class peers; and eigentrust model does not have punishment mechanism on this kind of false recommendation which causing good peer download success rates decrease significantly. peertrust model has little difference with eigentrust model with cm class peers existing. metrust model can judge false recommendation and cm class malicious peers can be identified, so the result is better. in most cases, metrust model is superior to peertrust model and eigentrust model, when the proportion of malicious peers reaches 30%, download success rates of good peers can reach more than 75%. with 40% malicious peers, loads of each peer in the four models are showed in figure 9 . the condition is similar to the above situation with im class peers, and not repeats them here."
"social media is typically used for collaboration, km, communication, and knowledge sharing. the majority of published papers within the field are empirically-based and in the format of conference papers. wikis, social media in general, and social network portals are the most studied tools. research addressing more specific and new types of social media for knowledge sharing will not only provide more information on the field, but will also increase support available for software developers who currently use or are considering using such tools."
"3) swing peer group (dm class): in addition to constitute cm class, the malicious peer can provide other peers with trusted transactions and normal feedback by probability f in order to accumulate trust to do some malicious acts; 4) camouflage peer group (em class): in addition to constitute cm class, some malicious peers in em class acts as good peers so as to get high credibility of recommendation, and give high trust to other malicious peers in cm class."
"the class of each frame is determined using alignment done from an hmm trained on only clean audio data of the videos since clean audio is the most reliable data and we use it as a baseline for our experiments. input to tandem classifiers are obtained by splicing 9 consecuive frames resulting in 540 features. we take phones as classes; since using words result in a small number of complex classes and using phone states would result in too many classes. so tandem classifiers discriminate 19 classes, and generate feature vectors of length 19 where each dimension corresponds to the output of the classifier for each class. we use stacked generalization with four-fold cross-validation to train the tandem classifiers and extract posterior probabilities [cit] ."
"3) metrust trust model is a local trust model. peer in the network has a unique credibility of recommendation. each peer determines the extent of adoption of recommended peers according to the difference of the evaluation criteria, and update recommended credibility after the interaction."
"firstly, the credibility of recommendation of each peer with existing im class peers is showed in figure 2 . as can be seen from figure 2, because im class peers provide honest recommendation, the credibility of recommendation of each peer does not reduce in metrust model. in the case of the presence of different scale im class malicious peers, download success rates of four models are compared. proportions of malicious peers in all peers are 0%, 10%, 20%, 30%, 40%, 50%. as showed in figure 4, metrust model, peertrust model and eigentrust model are better than the random model and can inhibit the im class malicious peers. in random model, the download success rate is reducing with increasing proportions of malicious peers. metrust model with im class peers in network is little difference with peertrust model, and download success rates of these two models have a decreasing trend with increasing proportions of malicious peers. eigentrust model have a certain number of high trusted peers, however this assumption in practice is unreasonable and difficult to operate, with the increasing proportions of malicious peers, and high trusted peers play the more important role, so that when the malicious peer reach the ratio of 30% and 40%, eigentrust model is also able to achieve the higher download success rate. when malicious peers reach the half of all peers, that's 50% of the system, download success ratios of three models are all low. metrust model and peertrust model have little difference when im class malicious peer exists, but metrust model has obvious advantages compared to other trust models when there are few more cunning malicious peers will be discussed below. in figures 5, we compare the load of each peers in four models when the proportion of malicious peers is 40%, i.e. the number that providing unreliable service as a service peer. in all 100 peers, peer 1 to peer 60 are good peers in figure 5, and peer 60 to peer 100 are peers in the im class, and eigentrust model selected peer 56 to peer 60 as high-trusted peers. as can be seen from figure 5, im class peers don't provide reliable services, and they will not become download sources, so the number of providing reliable services is 0. but in eigentrust model, when malicious peers reach a certain size, the number of providing services from high trusted peer 56-peer 60 is far higher than normal peers. as in eigentrust model, there are a certain number of pre-high trusted peers which cumulate their trust with increased interaction cycles and play a more and more important role in the interaction with the increasing scale of malicious peers. the more malicious peers and the less good peers exist in networks, the more dependence of high trust collective will happen, which cannot achieve the load balancing; the download success ratio of eigentrust model is strongly dependent on high trust peers, which might make high trusted peer overload. while there aren't global trust values in metrust model and peertrust model, each peer selects the service peer based on their own preferences, and different peers will select different service peers due to their different evaluation criteria, which will not bring peer overload."
"addressing the above stated areas will not only increase knowledge within the field, but also aid in enhancing the available support for both researchers and practitioners."
"periodic solutions and comparison with measured data 221 to determine the periodic solution in (7), we obtained 1, 2, 1, 2, 2, 2, 1, and 2 from (8)-(12) as functions of 1 and 1 as follows:"
"limitations and future work 345 in the current study, the results revealed that our simple model has six types of solutions and that involves only collected flight, the measured data were located in the stable solutions of type e but 352 not type c (fig. 6d ). to overcome this limitation, several improvements are needed. for example, 353 our two rigid bodies need to be asymmetric because horses have different physical properties be-354 tween the fore and hind parts of the body, and bend their backs around the sacrum rather than 355 in the middle of the spine [cit] . in addition, the torsional spring in the spine joint 356 should be asymmetric in the extension and flexion directions because the spine of the horse is dif-357 ficult to bend [cit], particularly in the extending direction [cit] . 358 furthermore, we neglected the dynamics in the horizontal and pitching direction in our model. it"
"first, the software was used to eliminate duplicates. then, a screen based on the relevance of the title of the studies to the research questions identified papers relevant for a more in-depth review of the abstract, and subsequently, the full paper. relevant studies were selected using explicit inclusion and exclusion criteria (table ii) ."
"however, aside from wikis and social networks, there is a current lack of research on specific social media tools for knowledge sharing. we are currently witnessing a more widespread focus of research within the field, but there seems to be a demand for even further investigation of the potential of more specific social media tools."
"from the slr, 4235 studies were initially retrieved. the slr was performed on the 8 [cit], with the initial set of studies limited to papers being published at that time. as mentioned prior, the removal of unrelated studies resulted in the final retention of 106 studies, which constituted the final set of studies analyzed in this review. a complete reference list is available in appendix a."
"research within this field is struggling to keep up with rapidly changing technological development, with findings potentially becoming outdated before being published. thus, there is a need for more prospective research. furthermore, changes may be necessary to the extensive process of submission to publications, such as increasing the speed of this process, or the establishment of completely other ways of publishing academic material. perhaps certain aspects of social media could be used in this process."
"this simple example demonstrates how asynchronous transitions of streams can be handled by deriving a new model from an existing one. to generalize the idea, a mshmm with q states and s streams/channels can be converted to a model behaving like a chmm having a total of q s states. typically, we allow asynchrony inside an hmm model only, so in a large vocabulary the increase in the number of states is proportional to the average number of states in a model (such as the model of a single phone) and not to the total number of states. figure 1 (b) also visualizes the process using state transition diagrams."
"where q the difference between mshmm and chmm can be seen visually by comparing the graphical models of the mshmm and chmm in figure 1 (a), where squares represent hidden states and circles represent observations."
"additionally, as the popularity and importance of social media has increased, research has begun to focus on the general use and impact of social media. with the ability to reach out to others for quick answers to specific problems during a development activity."
"for audio data mel frequency cepstral coefficients (mfcc) [cit] with 13 static plus δ and δδ features are used for each window. visual region of interest (roi) is extracted as a square area using active shape models (asm) [cit], where weight center of the lip is taken as roi center and length of a side of roi is taken as one and a half times of the distance between the eye centers. after the roi is extracted for the whole video, principle component analysis (pca) is applied on frames, extracting top 30 principle components for each frame. combined with first derivatives over time dimension, a frame is represented with a vector of 60 dimensions."
"in a big global study knowledge management (km) experts stated that research in the relationship between km and technological enablers often is retroactive and tries to make km fit into a former defined theme . this study seems to confirm this in relation to social media. that is, as social media becomes implemented in companies, researchers begin to change their focus toward these tools."
"parameter dependence of solutions 329 the type of periodic solutions depended on the relationship between and (figs. 5 and 6) . in . schematics to explain the mechanism under which while cheetahs involve two different types of flights, horses involve one type of flight. δ̇s are not so different between cheetahs and horses, and 1 of cheetahs is much larger than that of horses. therefore, while signs oḟ− anḋ+ are identical for cheetahs, they are different for horses."
"we obtained the xml data dump (as of 01.07.2016) 7 provided by wikimedia containing information about each of the editing actions done by contributors. we parsed the data, transformed it into csv data, and imported into a memory-based database (memsql). for each edit, we kept information about the editor who did the edit, the timestamp when the edit was completed in wikidata's database, the item where the edit was done, and the comment that mediawiki automatically generates to annotate the changes in the database."
"given that we look at lifespan and volume of edits, we consider 4 types of contributors: (a) contributors with long lifespan and high volume of edits, (b) contributors with short lifespan and high volume of edits, (c) contributors with long lifespan and low volume of edits and (d) contributors with short lifespan and low volume of edits. the contributors with the highest impact on the system are contributors who contribute extensively, and for a long time (group a). groups (b) and (c) are also valuable contributors. for example, a contributor supervising recent changes to revert and correct malicious edits, a few times a month; she might do only a couple of edits a month, but if she does it for a long period of time, her contribution can help wikidata in terms of data quality. yet, ideally one would like to have as many contributors as possible in group (a). predicting the lifespan and volume of the contribution of editors, we are able to classify existing contributors into one of these groups, and we can consequently, decide whom to address and how to do it."
"if these hypotheses are confirmed, these dimensions will help us predict the class to which contributors will belong (i. e. power or standard contributors in terms of lifespan and volume of edits)."
"to the best of our knowledge, there is no previous work analyzing the evolution of editing behaviour in these terms as a predictor of the volume of edits and lifespan in wikidata. the research around wikipedia, older than wikidata, has examined the edit history in terms of edit quality, editor interaction, editor participation, as well as emerging information cascades (see section 3 for a detailed description of the related work). even if both systems share commonalities, wikidata has features that could, in theory, encourage people to work with different patterns than in wikipedia. besides that, there is no published work about the intersection of both communities; so, it should not be assumed that contributors in wikipedia and wikidata work exactly in the same way. it is, thus, important to collect empirical evidence and study the behaviour of contributors in the wikidata environment, too. anyhow, other works have not used the trend in the evolution of contribution, participation and diversity as predictive factors of lifespan and volume of edits. therefore, not only do we contribute to the state of the art by studying contributors in wikidata, but we also contribute with a method. [cit] grouped editors based on the types of tasks they focus on, and as an extension (cuong and müller [cit] ) observed the extent to which wikidata editors change between roles. [cit] surveyed wikidata editors to understand differences between novice and expert users, examining how motivations, goals, usage of interfaces and type of actions differ. while these works help to understand the fundamental differences between some groups of editors, and the way editors change the type of actions they work on as they become more experienced, these works fail to (i) provide a method that helps to predict the extent to which editors will be engaged in terms of time and contribution, and they (ii) do not explain how power and standard editors change or maintain their contribution, participation, and diversity of type of edits."
"bottom-up modeling approach for the quantitative estimation of parameters in pathogen-host interactions opportunistic fungal pathogens can cause bloodstream infection and severe sepsis upon entering the blood stream of the host. the early immune response in human blood comprises the elimination of pathogens by antimicrobial peptides and innate immune cells, such as neutrophils or monocytes. mathematical modeling is a predictive method to examine these complex processes and to quantify the dynamics of pathogen-host interactions. since model parameters are often not directly accessible from experiment, their estimation is required by calibrating model predictions with experimental data. depending on the complexity of the mathematical model, parameter estimation can be associated with excessively high computational costs in terms of run time and memory. we apply a strategy for reliable parameter estimation where different modeling approaches with increasing complexity are used that build on one another. this bottom-up modeling approach is applied to an experimental human whole-blood infection assay for candida albicans. aiming for the quantification of the relative impact of different routes of the immune response against this human-pathogenic fungus, we start from a non-spatial state-based model (sbm), because this level of model complexity allows estimating a priori unknown transition rates between various system states by the global optimization method simulated annealing. building on the non-spatial sbm, an agent-based model (abm) is implemented that incorporates the migration of interacting cells in three-dimensional space. the abm takes advantage of estimated parameters from the non-spatial sbm, leading to a decreased dimensionality of the parameter space. this space can be scanned using a local optimization approach, i.e., least-squares error estimation based on an adaptive regular grid search, to predict cell migration parameters that are not accessible in experiment. in the future, spatio-temporal simulations of whole-blood samples may enable timely stratification of sepsis patients by distinguishing hyper-inflammatory from paralytic phases in immune dysregulation."
"it should be noted that, even in the case of lowdimensional parameter spaces, the estimation of parameters for abm generally turn out to be computationally intensive. this is a consequence of the fact that abm simulate the interactions between thousands of agents in continuous space as stochastic processes. to simultaneously facilitate an increase in model complexity and a decrease in computational expense for parameter estimation, we applied the local optimization algorithm adaptive regular grid search. this algorithm compares abm simulations by evaluating the least squares error (lse) regarding the experimental data of the whole-blood infection assay. stochastic effects of the abm were investigated by comparing simulation results for a fixed set of parameter values with varying number of in silico replicates. using 100 in silico replicates as a reference for the mean value of the lse, we generally observed for relevant parameter sets, i.e., parameter sets that yield reasonable agreement with the experimental data, that relative variations in the mean lse were already well below 10% for 30 in silico replicates. therefore, we set the number of in silico replicates to 30 throughout the whole parameter space."
"when we observe the timestamp of the first and last seen edits of editors, we can analyze for each year in our dataset the number of editors who joined, as well as the number of editors who were seen last that year, and the number of editors who joined but were seen last that year. as it can be observed from figure 4, [cit] this number decreased. exactly the same behavior is observed for the last seen edit and the people whose first and last edit is seen in the same year."
"the state-based model pi-sbm opens the possibility to study the dependence of the immune response against c. albicans on the number of pmn and monocytes in blood. simulating the virtual infection scenario with the previously estimated parameters (see supplementary table 3), we considered various cases of immune cell deficiencies. the model predictions at 4 h post-infection and for gradual decreases in the immune cell numbers are presented in figure 7 for the cases of monocytopenia and neutropenia separately."
"we take into account the indicators used in the wikimedia community 10 and we extend them with others defined by us. in particular, we look at three dimensions: (i) editor contribution, (ii) editor participation, and (iii) diversity of the edits accomplished by editors."
"between the two prediction problems that we set up (i. e. predicting the lifespan range and predicting the range of volume of edits), predicting the lifespan has a higher priority, because it gives us the key information about when we should address standard editors that will become inactive. therefore, being able to predict lifespan more accurately than the volume of edits is a positive result."
"editing behaviour indicators goal of analysis useful signal to discriminate different behaviours between these groups of editors, at least visually (in section 8, we will see that even such indicator will be useful for prediction)."
"in order to confirm our intuition about the relationship between lifespan/edit count and the indicators, we use a random forest regressor with the whole dataset as training data and visualize the predicted values on the indicator space."
"while this work is leveraging a very large dataset of activity logs, in the future we plan to complement our work with a qualitative study performed by surveying and interviewing representative wikidata editors sampled from the different categories we looked at in this work. as future work, we can also include the quality of edits as a feature which can be measured with the revert rate as a proxy. furthermore, as a follow-up work, we plan to work on the implementation of the solution for attrition and retention management proposed in section 9, in collaboration with wikimedia."
"given that editing behaviours may show different type of patterns when using tools, we decided to focus our analysis on the set of edits done by registered users without tools. the data used by default for all the results presented in all the following sections is therefore referring to this set of 35+ million edits done over 7+ million items of the knowledge base."
"the distribution of the lifespan shows that there a couple of thousands of people who have been contributing for almost the 4 years that we studied, which is priceless. again, having this descriptive statistic can help configure the retention / reactivation strategy. currently, wikimedia encourages editors after they reached editing milestones (e. g. editors get congratulated via the wiki after they achieve their 100th edit). so, similarly, we could think of acknowledging people for being in the project for a particular amount of time. the fact that the relationship between lifespan and volume of edits is not linear indicates that there are some people who make a contribution of for example a thousand of edits in a short time -even less than one month. probably events such as hackathons and editathons with a specific focus (e. g. to enter the description of women swiss scientists in wikidata) stimulate editing activity of participants, but in some cases it might mostly during the event. moreover, data providers might also edit in bulk for a short time to ingest one single data set into wikidata."
"the wikidata community acknowledges the value of both (a) editors who contribute with a high number of edits, and (b) editors who contribute for a long time."
"to solve these two prediction problems, we use supervised learning methods. more specifically, we use binary classifiers that take as input the information about the slope, intercept and r2 obtained by applying the ransac algorithm on the multiple indicators-based measurements, across sessions and across months. to define the thresholds for high / low volume of edits and long / short lifespan, and create the 4 classes (i. e. power and standard editors as for lifespan, and power and standard editors as for volume of edits), we observe the distribution of both volume of edits and lifespan, and decide that 15 months and 100 edits are suitable figure 22 . plots comparing the f1-score for each class (power and standard) obtained by two classifiers (a logistic classifier and the random forest classifier) when predicting the volume of edits that editors will make. the first plot shows the f1-score evaluation using 100, 200, 300 sessions of edit history per editor as training data, while the second plot shows the evaluation using 3, 5, 10 months of edits per editor as training data. numbers to empirically distinguish between the different classes of editors that we define in terms of volume of edits and lifespan respectively."
"wikidata (and wikipedia) editors often have breaks (i. e. periods of time when they do not edit). for this reason, the wikimedia foundation introduced the notion of active and inactive users, to differentiate between users who are present and who users who \"having a pause\". a user is defined as active if in the last 30 days she did 5+ edits. however, a user that is within a period of inactivity has not necessarily abandoned the project. to measure the lifespan of editors, we need to distinguish between being temporarily inactive and gone. therefore, we look at editors as being in one of three possible status: \"active\", \"inactive\" and \"gone\". the distinction between active and inactive is given by wikimedia, to distinguish those who are gone, we need an additional definition. instead of defining an arbitrary threshold (e. g. of one year) of time after which we label editors as gone, we calculated it empirically. in order to do so, we analyzed the length (in months) of inactivity periods for all wikidata editors. surprisingly, the longest gap is of 16 months, which means that there were editors who, after 16 months without editing, came back to wikidata and edited again. looking at the percentiles we decide to use 9.967 months as a threshold in our data set to define editors that are gone and editors that are still in the system (either in active or inactive mode). once we labeled the dataset according to this threshold, we encountered that from the total of 140,330 editors (who edited items) 77,698 [cit] . that is, around 55% of the editors have abandoned the project."
"based on our findings, we identify three major areas where the wikidata community could focus: -increasing the crowdsourcing ratio per item: in the same way that there are indicators to draw attention to the relative (in)completeness of items (see the recoin tool https://www.wikidata.org/wiki/user:ls1g/recoin), there could be indicators measuring the plurality of item descriptions and the number of editors involved in the description (as a crowdsourcing ratio). the goal of showing such information would be to request the help of further editors and improve the plurality of the item description -however that is defined. -acknowledging long lifespan and high activity periods: the retention literature conveys that it is better to interact with users before they leave because the probability of making them go back to the system once they drop-out are lower. hence, we encourage the wikidata community to add lifespan and high activity recognition to the acknowledgments that are implemented in the wiki. the system could congratulate editors for being in the system for a long time or for having a high activity peak. the message could report historic statistics or even show flashbacks about a significative edit done by the editor a while ago. as a reward, the community could grant these editors a special privilege in their \"wikidata-versary\". -encouraging a behavioral change that makes standard editors become power editors: with wikidata's increasing size and complexity, it is becoming more and more challenging for editors to master the variety of tools to edit, query and visualize wikidata's data, and find things they can contribute to. in our work, we found out that a very high rate of editors have a very short lifespan, and their evolution show that their contribution and participation is not constant. that is, there are many editors with the potential to become more active. usually, early dropouts are motivated by any of the multiple possible phenomena such as people lacking the conviction for free knowledge, people not finding the way to contribute and wikidata not being able to develop a sense of \"addiction\" 13 . organizing training and dissemination events to educate people about the value of free knowledge is the solution to (i) and that is something that wikimedia already does. for (ii) and (iii), wikidata needs to provide a methodology and tools that allow these editors find valuable work to do and develop a reinforcing editing habit. previous works have shown that suggesting things to edit in wikipedia can be useful both for the users and the system [cit], and providing article feedback can also help readers transition into editors . given that the dynamics defined in the wikipedia community is similar to the wikidata community, we believe that there is space to develop a solution that can guide wikidata editors and help them become more active and transition close to power editors (if they are willing to do so). we propose to design a system that helps standard editors find their editing mission. missions could be defined individually or collectively (i. e. shared with other editors). to encourage a change in their behaviour, it would be important to consider behavioral change theories [cit] suggest that change is more effective when the person frames intentions and goals, has the chance to self-regulate him/herself, and can freely select from available choices. so, as a design principle, the system would let them define what they would like to achieve, and decide what they finally would like to work on let, rather than impose or assign work to do. the system, still, would need to reduce the number of editing possibilities to a manageable and attractive set of options. and such an algorithm would need to exploit the main difference between wikidata and wikidata: the structure in the data and the fine granularity of traceable actions. another key feature of such a solution could be a tight collaboration between power and standard users. standard editors would define their intentions (e. g. editing for the city of zurich) and identify themselves with roles (e. g. someone would like to become a quality ninja, but she does not know how yet). power editors would set calls for actions and define data needs. the system could enable a 1:1 contact between power and standard editors, so as to share and disseminate best practices, recommendations on habits and know-how. wikimedia currently enables mentorship 14 . our system would aim at systematizing some actions in this process. the predictive models defined in this paper would be useful to estimate the amount interaction (and help) that editors would need, as well as to compute the time when the system interacts with the editor, whose behaviour we would like to improve (in terms of engagement). 15"
"-i5: diversity of types of edits. we use a standard measure for diversity, the the shannon-entropy [cit], to calculate the diversity of types of edits as"
"we then classified the edits based on the (i) type of editor, (ii) the type of thing edited, (iii) the means used to edit, and (iv) the type of edit carried out."
"a timely stratification of sepsis patients in different phases of immune dysregulation requires spatio-temporal simulations of whole-blood samples. to achieve this goal, an abm of the wholeblood infection assay was established that builds on the pi-sbm and incorporates spatial properties of the blood sample in a threedimensional continuous representation. in particular, in the abm c. albicans cells as well as monocytes and pmn are agents that can migrate in the environment and interact with each other. apart from the model parameters associated with the migration of cells, the abm was based on the transition rates of the pi-sbm after appropriate conversion. this procedure strongly reduces the number of a priori unknown parameters of agents to the subset of migration parameters. the latter can be estimated using the computationally cheap grid search algorithm and enables the prediction of the migration behavior for the different immune cell types that are otherwise not directly accessible in experiment. the interrelations between the different modeling approaches are schematically shown in figure 1 demonstrating that results are re-used across different modeling approaches to simultaneously facilitate an increase in model complexity and a decrease in computational expense for parameter estimation. our step-wise computational biology approach avoids typical limitations of realistic models by focusing parameter estimation on those parameters that arise at the next level of model complexity."
"since we are interested in observing and comparing the evolution of these indicators over time, we first compute each of these indicators at several points in time and second, we fit a linear model using the ransac algorithm [cit], a robust linear model estimator able to deal effectively with outliers. for each editor we obtain the values for their slope, intercept and r2. these values allow us to understand the general trend over time of an indicator for an editor."
"in order to make such a prediction, we analyze the evolution of editing behaviour. we analyze this information from two different perspectives: we run a (i) session-based analysis, and we also study the editing progress (ii) on a monthly basis. the two perspectives are complementary: with the former, we aim at understanding the extent to which editors change their behaviour as they gain more experience and do more edits in each session they spend editing; while with the latter, we perform a time-sensitive analysis. when we analyze the behavioural evolution throughout the editors' lifetime (in sessions and in months) we measure indicators related to the editors' productivity, editors' participation and the diversity of the types of edits (cf section 7)."
"-i4: number of seconds between the first and last edit in the timeframe being analysed. if there is only one edit, we decide to define i4 as 0 seconds. note that this indicator is only relevant for a session-based analysis."
"-i1: number of edits. it looks at any kind of edit (from creation, to update and deletion). -i2: average number of edits per item. it identifies any kind of edit done to items (either items of the schema-or the instance-level). -i3: number of items edited."
"our work has two major limitations: first, we do not provide a qualitative analysis of the edits. while we distinguish among different types of edits (see indicator i5 section 7.2), we focused on contribution, participation and diversity of the edits without observing features of the items (e. g. topics and categories of the items), or the quality of the actual edits (e. g. whether the new statement created by an editor is semantically accurate). clustering items by topics would be useful for computing further diversity indicators, whereas understanding the quality of the edits would help us categorize editors in different ways. labeling the quality of edits can be beneficial for filtering out editors whom we may not want to retain -people who intentionally provide incorrect, hence, disruptive edits -and consequently can help us design more accurate measures against attrition. yet, predicting the lifespan is a useful information by itself, because it gives a hint about the moment when we need to intervene to encourage behavioural change. likewise, the prediction of the volume of edits gives an indication about the magnitude of the editor's work. the fact of knowing if they are primarily good or bad edits will only change the interpretation and the way we will proceed (e. g. in the case of malicious editors, the shorter the lifespan and the lower the volume of edits, the better; and in the case of helpful editors it is exactly the opposite) second: we do not reveal the reasons that lead to editor attrition. understanding the issues that standard editors face is crucial for designing a solution to the attrition problem. that is why, we plan to run a survey in the wikidata community, to address this question."
"different behaviours: figures 9 to 15 show several scatterplots, where each depicts the slope (x-axis) and intercept (y-axis) for a particular indicator for each editor, in one particular evolution granularity. figure 9 for example shows the plot for indicator i1, with a session-based evolution. we decide to plot these two dimensions along the x and y axis, ignoring r2 because it does not provide a table 2 . for different options to study the evolution of editing behaviour. i1 is the number of edits. i2 is the average number of edits per item. i3 is the number of items edited. i4 is the number of seconds between the first and last edit in the session -only valid for session-based analysis. i5 is the diversity of types of edits."
"given this result, we can compare the distribution of lifespan and edit count of different slope intervals. figures 16 and 17 show a valuable example, comparing the histogram for the lifespan of editors with a slope with an absolute value smaller or bigger than 0.2, for indicator i1 (number of edits per session). there is a clear difference in the distribution of both histograms: the editors with absolute slope value smaller than 0.2 (and therefore closer to zero), have bigger lifespans and the frequency of bigger lifespan is also higher than for the editors with absolute value bigger than 0.2."
"for example, when we fit a linear model on the different observations for i1, we can identify if a user is increasing, decreasing or maintaining the number of edits over time by looking at the slope of the fitted linear model. the intercept provides information about the scale and r2 indicates the error between the fitted model and the actual shape of the i1 time-based indicators."
note that the ransac algorithm removes from the data set those editors who have an insufficient number of observations to draw any conclusion on the evolution of the behavior.
"the results referring to the differences between power and standard editors suggest that power editors have habits in terms of their contributions -edits done each month -and the participation -the time spent in sessions. the existence of habits reflects not only the conviction to contribute, but it also shows that these editors successfully manage to find work they can do. from conversations we had with editors, we know that some power editors add information about the film they have watched, or edit information about the person they heard about in the news. others look everyday through the list of recent and unpatrolled changes. there are different events that trigger editing action, and power editors follow them regularly. one could argue that if contribution and participation are both constant, then there is no clear signal of a learning effect that leads to editors becoming faster in accomplishing their tasks. we assume that the explanation is that power editors have a learning effect mostly in the beginning and later their efficiency becomes stable. since they typically have many edits, it is possible that this initial learning effect is invisible to the linear model fitted. the increasing trend for the diversity of the types of edits present in power editors is aligned with the fact that intrinsically motivated people tend to \"seek out novelty and challenges, to extend and exercise one's capacities, to explore, and to learn\". [cit] ."
"as described in section 2.2, probabilities for state transitions in the abm of the whole-blood infection assay can be derived from the interaction rates of the pi-sbm. this reduces the space of parameters that has to be searched in the process of parameter estimation, leaving only two migration parameters-i.e., the diffusion coefficients d m and d g, respectively, for monocytes and pmn-to be calibrated. however, even for a reduced parameter search space, there still is need for a calibration strategy that keeps the number of abm simulations within limits, because simulating stochastic processes requires sufficient numbers of repetitions in order to obtain numerical results that are statistically sound."
in this paper we have performed a longitudinal cross-sectional analysis of wikidata's editor community behaviors aiming at understanding how different types of editing behaviors lead or not to high volume and long-term engagement with the community.
"we also looked at the extent to which items are crowdsourced, by computing the number of distinct editors who have edited each item. again there is a clear a longtail in the distribution, as there are many items that have been edited by few editors, while there are few items that have been edited by many editors. the median is 1 editor per item, while the standard deviation is 3.1 editors per item ( figure 2) ."
"next, we compared the abm simulation results for the absolute lse minimum with those of the pi-sbm. these are plotted together with the experimental data of the whole-blood infection assay in figure 6 and in supplementary figure 4 for the time evolution of c. albicans sub-populations. thus, we found that both modeling approaches, the non-spatial sbm and the spatial abm, yielded excellent agreement with the experimental data. furthermore, we found that our simulation results obtained from the stochastic abm were robust, which can be seen from the line thicknesses in figures 6c,d representing the standard deviations obtained from 30 abm simulations."
"we further decrease computational costs associated with parameter estimation in the abm by system scaling. thus, similar to the procedure applied for the state-based model pi-sbm, we first scan the parameter space with a small system of 1/5 µl blood and subsequently re-scan the relevant parameter region with the system of 1 µl blood as defined in section 2.2."
"finding 4: editors with long lifespan have a constant contribution over months, while editors with short lifespan do not. as for the way the participation evolves over sessions, we can see in figure 12 that editors with a higher total volume of edits have a constant participation, while editors with lower volume of edits have a rather increasing or decreasing evolution. when it comes to comparing the participation over sessions of editors with long and short lifespan figure 13, the separation between the two groups is less clear (because some power users also have a non-constant participation evolution), but it is still visible. figure 11 . scatter plot indicating for each editor the slope and intercept obtained by the ransac algorithm for indicator i1 (number of edits), in a session-based analysis of evolution, depicting the editors' edit count. finding 5: editors with a high volume of edits have a constant participation over sessions, while editors with low level of volume of edits do not."
"we observe that there are the editors with constant slope (i. e. they perform according to past sessions/months), while others have positive or negative slope, with different values for intercept (negative, zero and positive). this finding confirms that there are different evolutions of editing behaviours present among the community, where some people increase the number of edits over time, other people show a decay in the time they invest, and other editors show a constant performance."
"we analyze the evolution of editing behaviour from two different perspectives: first, we look at how editors behave as they get more experienced after doing batches of edits. to group edits, we define editing sessions, a common technique in web-based systems. second, we observe the way editors edit over the natural months in which wikidata has been online. note that the latter allows us to account for the exact occurrence of inactivity periods."
"the choice of an appropriate mathematical modeling approach strongly depends on the questions to be answered and the hypothesis, as well as the characteristics of the underlying experimental data with regard to temporal and spatial information. a wide range of modeling approaches exists that differ by their computational complexity and can be classified depending on the degree of spatial representation as well as the internal degrees of freedom attributed to the model entities. the computationally cheapest modeling approach for dynamic systems is represented by ordinary differential equations (ode), where biological entities are assumed to be present in high numbers and spatial information is not required such that they can be collectively represented by a homogeneously distributed concentration variable. state-based models (sbm) resolve the biological entities as individuals that occupy states and are able to perform transitions between states representing dynamic processes. in contrast to ode, this approach allows modeling discrete events for any entity number in a biological system. however, sbm are in turn limited in that they do not represent spatial aspects. individual-based models (ibm) such as cellular automata (ca) and agent-based models (abm) do simulate discrete entities in space and time [cit] . in a ca simulation, these entities can undergo state changes associated with their internal degrees of freedom as well as positional changes on a pre-defined spatial grid of computational cells [cit] . the discrete number of individual entities as well as the spatial representation of the environment result in increasing computational costs in terms of run-time and memory. even more computationally expensive but biologically more realistic simulations can be performed by the abm approach. here, biological objects are represented as individual entities, so-called agents, that are able to move in space and can act as well as interact with other agents according to individual properties. [cit] and pollmächer [cit] . in particular, the abm developed by pollmächer [cit] simulates the detection of a. fumigatus conidia by macrophages in a to-scale representation of human alveoli and predicts the requirement of a chemotactic signal guiding the phagocytes to the spatial positions of conidia."
"to understand the influence that different users have in the system, we computed the total number of edits made by each user and plotted the histogram. as it can be seen in figure 1, there are many users who make a low number of edits and few users who make a high number of edits, as expected. the median of the edit counts is 2 edits, while the standard deviation is 7223 edits. this kind of behavior is actually similar to what we observe in other crowd-powered systems. in the context of paid microtask crowdsourcing, participation is typically dominated by few workers who complete most of the workload [cit] . similarly, in citizen science projects (e. g. galaxyzoo) very few users perform many tasks, while the vast majority tags less than 30 images each [cit] ."
"here, ǫ c is adjusted as to fit each combined unit comparably well to the experimental data. the same values for ǫ c were used in the pi-sbm and the abm and are given in supplementary table 1 . next, the parameter set p is randomly varied within a pre-defined neighborhood of 10% variation, leading to a new set of parameter values, p ′, as indicated in figure 4a, step 2. subsequently, the simulation of the pi-sbm is performed again for parameter values p ′ and the corresponding score e[ p ′ ] is calculated. whether the new simulated data will be accepted or rejected is decided by applying the mmc scheme that is depicted in figure 4a, step 3. the probability to accept worse parameter values is influenced by τ (f ), representing the \"inverse system temperature\" in a sa process. the simulation of the annealing process involves a gradual decrease of the system temperature with progressed fitting, i.e., τ (f ) is increased with the number of performed fitting steps f (see supplementary information 2.1). after performing a total number of fitting steps, the fitting algorithm is repeated starting from a newly chosen random parameter set. this is done for a certain number of runs and the set of parameters with the minimal fitting error ( p min ) is saved from each fitting process. the mean values of the parameter values and their standard deviations are computed over all runs to determine the robustness of the estimated parameters."
"item edits are edits done to create, update or delete an item in the knowledge base (e.g. an entity, a class). non-item edits are edits done in other kinds of pages such as project and user pages. we only focus on edits done on items which are part of the knowledge graph. -means to edit: there are various interfaces to edit wikidata (e.g. the wiki, wikidata games, etc.). we differentiate between edits done using tools and edits done without tools, because in the former case users do not decide what item to work on next, nor the type of edit to do. to classify edits into these two groups we use the oauth tags database provided by wikimedia and scan the edit comments for any other trace left by tools listed in wikimedia directories (including gadgets. user scripts and external tools). -type of edit: we use the list of actions registered in wikidata's backend 8 to distinguish between the major actions (e. g. set a label, update a claim, delete a claim or add a reference)."
"the bottom-up modeling approach presented here may be extended in various ways. for example, the implementation of a hybrid abm could be envisaged where molecular interactions, e.g., as mediated by antimicrobial peptides, are not simulated in a rule-based fashion but in an explicit way by a molecular diffusion solver. other directions of future research include (i) focusing on conditions of immune dysregulation, (ii) comparing the impact of different pathogens, and (iii) including other types of innate immune cells. furthermore, it is conceivable to combine modeling approaches with microscopy experiments of infection scenarios in vitro in an image-based systems biology approach [cit] . first steps into this direction have recently been made, e.g., by establishing algorithms for the automated image analysis of phagocytosis assays [cit] and for the automated tracking and classification of pmn from time-lapse microscopy [cit] that was applied in the context of comparing c. albicans and c. glabrata infection [cit] ) . in the future, we expect that a systems medicine approach exploiting the predictive power of virtual infection models will play an important role in the context of infectious disease diagnosis."
"in our work we also look at contribution and participation, but using different measures (i. e. number of edits per month / session, number of edits per item, number of items and seconds invested in the session) and observing the trend -increasing, decreasing or constant-of the measurement over time. furthermore, we compare power and standard users, without clustering them."
"habits are related to commitment and effectiveness [cit], and often the users who do not develop habits are related to churn. we set up three hypotheses, focusing on contribution (e. g. number of edits per session / month), participation (e. g. time invested in a session) and diversity (e. g. type of task variability)."
"the probability is defined as the frequency of type t. when we compute i5, we normalize the entropy based on the number of edits done in the session/month analyzed."
"on every encounter between a monocyte and a c. albicans cell. this correspondence of event probabilities for the two modeling approaches imposes a condition on the spatial dynamics of cells, i.e., on the values of the diffusion coefficients in the abm and by that on the time-step t abm (see equation 6 ). for optimal migration parameters, i.e., parameters that result in good agreement with the experimental data, it is expected that measurement of the associated phagocytosis rate in the abm coincides with the corresponding rate from the pi-sbm."
"in order to improve the situation and reduce editor attrition, the community (and especially wikimedia as the main manager) needs to design and implement methods that stimulate a change in the behavior of these contributors who become non-active. the literature in marketing research has extensively studied the problem of customer churn (or customer turnover) and designed churn management strategies that aim at increasing customer retention [cit], because losing customers can endanger a business -having fewer customers often means obtaining a lower revenue. while in wikidata (and in general in any wikimedia project) the motivation to keep editors contributing to the knowledge base is not economic, there is attrition and participation inequality. therefore, even though the concrete actions to engage further contributors are different from scenarios with economic transactions and customers, it makes sense to use assumptions and techniques from this field of research."
"our results are relevant to the wikidata community, because they shed some light on the way power and standard editors in wikidata evolve differently over time. having these insights is useful -especially now that there is still limited knowledge about the way the community progresses-to design methods that encourage standard editors to contribute more, and hopefully also longer, as they experience progress. additionally, our results and observations may be of use to other crowdsourced knowledge curation and maintenance projects with similar characteristics to better engage their communities of contributors."
"for comparison between the model predictions and the experimentally determined kinetics in the whole-blood infection assay, we introduce specific combinations of states, referred to as combined units, that are measurable and useable for the parameter estimation. these comprise all extracellular c. albicans cells c e,"
"as we can see along the figures of section 7.2.2, the difference between editors with long / short lifespan and editors with high / low volume of edits, in terms of the way they evolve is in some cases obvious and in others not. we can confirm hypothesis 1, because we see that editors with high lifespan show a constant contribution, while other do not. the same applies to high volume of edits, although with less strength (cf. finding 4). we can also confirm hypothesis 2 in the case of lifespan, because we see that people with longer lifespan maintain a constant participation, while editors with shorter lifespan do not. however, when it comes to volume of edits, the measurements do not help differentiating the two groups of editors as clearly. hypothesis 3 is rejected, because we see that both in long and short lifespan, and in high and low volume, editors tend to increase the diversity of the type of actions they accomplish. hence, evolution in contribution and evolution in participation are good indicators to differentiate standard and power editors in (especially in terms of lifespan), while diversity alone it is not sufficient."
"before addressing the topics raised in the definition of hypotheses, we obtain descriptive statistics that allow us gain a better understanding of the data set. out of the complete set of 350+ million edits, 1.5+ million edits are done by anonymous users and 261+ million edits are done by bots. we exclude both sets of edits from our analysis, as we are interested in studying human editing behaviour. we limit our analysis therefore to a raw data set of 87+ million edits."
"the main purpose of the exploratory phase of our analysis is to identify differences in the evolution of behaviour between groups of editors, mainly editors with high volume of edits vs. low volume of edits, and editors with high lifespan editors vs. low lifespan editors."
"to better understand the distribution of editors lifespan, we decided to analyze only editors who are gone (because only in that case we can be sure that we are looking at completed lifespans). figures 5 and 6 show the histogram for the editors lifespan. we can see that there are many users with a very short lifespan, and only few are long-lasting editors. there are editors who have been editing for almost 3 years. when comparing this drop-out ratio to other systems, we find similar results. user participation in different types of online collaborative platforms shows similar patterns to the ones we have observed in wikidata. generally speaking, in online communities about 90% of the participants are inactive content consumers while about only 10% actively contributes content for long periods of time [cit] . other examples include participation in moocs being very skewed with numbers of participants completing the online course varying between 5% and 10% [cit] . figure 7 relates lifespan and number of edits done within the observed time frame between first and last edit. obviously, with bigger lifespan editors may have bigger edit counts. however, interestingly the behavior is not linear, meaning that there are still people who are either slower or less committed, who have longer lifespan but the same number of edits."
"here, figure 3c, we present a schematic overview of processes that occur according to defined rules associated with certain probabilities. it is important to note that, due to the spatial aspects that are captured by the abm but not the pi-sbm, we have to distinguish between processes that are contact-dependent and contact-independent."
"because our goal is to understand and predict who will and who will not thrive as volunteer, we support our research in past studies and related theories that highlight differences in the behavioural evolution of effective and non effective people -related to the volume of edits -and committed/persistent and uncommitted people -related to lifespan."
"wikidata, in contrast to the vast majority of knowledge bases on the web of data, has a strong focus on human intervention across its complete data management process. tools and bots operate on the data to some extent, but the data is primarily curated and maintained by a community of volunteers. the community -editors, developers, data providers and researchers -discusses and collaborates to decide how to model, ingest and patrol information. this human-driven process enables wikidata to diminish the data quality problems that appear in (semi-)automatically generated knowledge bases, including entity misclassification, inconsistencies, semantically inaccurate links, and outdated data [cit] ."
"marketing analysts and researchers highlight the importance of targeting customers to improve retention [cit] . however, it is important to provide tailored solutions based on the customers' behavior, especially since loyal customers need a different attention than likely-to-drop-out customers. that is why traditional retention strategies first tend to predict whether a customer will be a churner or not, 5 and then implement actions to convert likely-to-be churners into non-churners [cit] . for these actions to work effectively, the prediction needs to be done as early as possible. the subfield of survival analysis [cit], often used in the context of retention management, estimates the \"time left\" until an event (e. g. time until death in the medical domain, time until drop out in the context of online shopping). this is useful to act against retention and intervene before customers leave. hence, having a method to estimate the time that customers will spend in the system is a prerequisite to design a retention management solution."
"the focus of our work is primarily on the editors' engagement patterns, independently from wikidata's content because we would like to first understand if there are editors with different levels of commitment and different habits. this is high importance to the community, because a project like wikidata greatly benefits from unconditional contributors who provide knowledge, no matter the status of the knowledge base."
"for this reason, we consider that users can be power users or weak users in two different ways: (i) in terms of the volume of their contribution and (ii) their lifespan. we set this two-fold goal as the focus of our empirical analysis, and examine the evolution of editing behaviour following the 4 different configurations listed in table 2 :"
"participation inequality [cit] ) is present in a vast amount of web systems, where often only the 1% of users contribute heavily, 9% of users contribute sporadically and the remaining 90% are so-called lurkers, who consume information (reading and observing) but do not actively contribute. hence, the skewed distribution of edit counts is an expected behaviour. still, it is a relevant descriptive statistic that can be used to understand the order of magnitude of the set of editors whom should be addressed, and define parameters of the editor retention / reactivation strategy (e. g. an upper bound for the number of expected edits to be improved). the (also) skewed distribution of editors per items indicates that not many item descriptions are really crowdsourced, which in the case of wikidata might be especially detrimental, considering that the knowledge base is meant to be the aggregate of what primary web sources state. features like qualifiers, ranks and references in statements allow wikidata to portray existing plurality, and therefore, the more people involved in curating information about an item, the higher the chance to capture this plurality and the lower the risk of having certain kind of bias during the data collection."
"as we see in figure 7, these two dimensions are not necessarily always related. the optimal case is to have editors who contribute with many edits and stay a long time in the system. however, each of these dimensions separately is useful. the former implies that the knowledge base may grow or improve (depending on the concrete edits), whereas the latter means that there are editors who could eventually be available in a call for participation."
"finding 6: editors with a long lifespan tend to increase the diversity of the type of their edits, while editors with short lifespan can either increase or decrease it over the months."
"to estimate the values of transition rates in the pi-sbm that yield the best fit to experimental data, i.e., the fit with the smallest least squares error (lse), we applied the method of sa-mmc scheme (for details see section 2.3.1). in figure 5, the resulting transition rates of the pi-sbm are compared with those previously obtained within the p-sbm (for a quantitative comparison see also supplementary tables 3, 4 ). the direct comparison between the p-sbm and pi-sbm revealed that most transition rates are quantitatively similar in the two models."
"that are either alive (c ae ) or killed (c ke ) cells in extracellular space as well as cells resistant against killing and/or phagocytosis that are either alive (c ar ) or killed (c kr ). next, the combined units c m and c g refer to c. albicans cells that are phagocytosed, respectively, by monocytes"
"according to the results, we obtain a higher f1 when we predict lifespan than we predict volume of edits. figure 23 . plot comparing the f1-score for each class (power and standard) obtained by two classifiers (a logistic classifier and the random forest classifier) when predicting the lifespan that editors will have, using 100, 200, 300 sessions (first plot) and 3, 5, 10 months (second plot) of edit history per editor as training data."
"as expected, the number of edits done with tools exceeds the number of edits done manually (see table 1 ). the number of distinct editors editing manually (without tools) is higher than the ones using tools. an explanation to this fact might be that there are sporadic editors who make a few edits in the wiki to try out the system or to maintain specific targets, but do not get involved with tools like the wikidata game or quickstatements."
"state-based models (sbm) do not account for any spatial aspects. for example, cells in the pi-sbm do not actually migrate during the immune response and, therefore, do not have to get into contact before a phagocytosis event can take place. in contrast, agent-based models (abm) do capture spatial aspects in a defined environment. applying a bottom-up modeling approach, we implemented an abm that is-apart from its spatial aspectsthe exact analog of the non-spatial pi-sbm. as depicted in figure 1, all transition rates that were previously estimated for the pi-sbm were fed into the abm (see section 2.2 for details). the only parameters left to estimate were those related to cell migration, which in the dense cell system of the wholeblood assay resembles a random walk. in particular, while the diffusion coefficient associated with the passive movement of c. albicans cells could be inferred from the stokes-einstein equation to be d c ≈ 1 µm 2 /min, the active migration behavior of immune cells requires a rigorous parameter estimation of the diffusion coefficients d m and d g for monocytes and pmn, respectively."
"the colour of each dot indicates whether the editor represented by the dot has a high or low volume of edits or long or short lifespan (depending on the particular instance). as it is clearly differentiable in figures 9 and 10, power editors (having long lifespan), seem to have a constant behaviour in terms of the output they produce over the months (vertical green cluster in the figures). in contrast, weak editors tend to change their behaviour, having either an increasing or decreasing evolution of their contribution (diagonal white cluster in the figures). in the case of session-based evolution, most of the editors with constant contribution are editors with long lifespan or high volume of edits, but the differentiation between the two groups is not as perfect as with the monthly evolution ( figure 11 )."
"in figures 18 and 19, we visualize a projection on the slope and intercept of the predicted lifespan for i1 and i2 respectively of the month based indicators. the predicted values corroborate the intuition about the absolute value of the slope as being a good indicator of high lifespan: for i1, a slope close to zero (constant behaviour) corresponds to high lifespan; for i2, we see that the second and fourth quadrant are associated with low lifespan, following the pattern visualized in figures 9-15 . the corresponding projection for the session based indicators (figures 20 and 21) are less clear, presumably because the interaction between all different indicators are more involved, and a two-dimensional projection is not able to simply visualize them. we verify the prediction capabilities of such model in the next section."
"-the number of new editors joining the wikidata community has been increasing over time, but it decreased recently. -the distribution of contributions is very skewed with few editors contributing most of the edits and many editors performing just a few edits. -to correctly define an edit session, it is necessary to consider the intrasession edits distribution (as well as intra-session differences and intersession differences), obtaining a threshold of 4.3 hours, around 4 times longer than in wikipedia. -there is no linear relation between editors' lifespan and the volume of edits they provide. -power editors tend to show a constant contribution over the months and a constant participation over the sessions, while standard editors show instead an increasing or decreasing tendency. -power editors tend to increase the diversity of the types of edits, but this dimension alone is not clearly separating the two sets of editors, because some standard editors also increase their diversity over time. -despite the unbalanced nature of the data (i. e. few editors with many edits or long lifespan), it is possible to automatically predict the future the volume of edits and lifespan duration of an editor based on the available edit history of wikidata editors. we are able to obtain better prediction results than a naive classifier in each of the 4 tested configurations. we are able to predict lifespan better than volume of edits (with an average f1 score above 0.9) in both session-and month-based evolution."
"-we empirically define new sessions after 4.37 hours of inter-edit time, around 4 times longer than in wikipedia. -editors with long lifespan have a constant contribution over months, while editors with short lifespan do not. -editors with a high volume of edits have a constant participation over sessions, while editors with low level of volume of edits do not. -editors with a long lifespan tend to increase the diversity of type of their edits, while editors with short lifespan can either increase or decrease it over the months."
"having shorter times between edits than in wikipedia is coherent with the nature of wikidata, because it allows people to edit structured data -that being adding references, updating a date or a quantity, or creating a link between two items. in wikipedia, people can also correct a typo, add a citation or a link, but the main task is to write a text (either a complete page or a paragraph), and the edit is registered when the text is saved, and not after each word has been edited. in wikidata, if statements are edited within the same item, and the editor is proficient it is very much feasible that two edits are accomplished within one or few seconds. the high number of consecutive edits done in or under 5 seconds can be due to the fact that some actions, like merging items and deleting items (only granted to special editors) can be executed one after the other in a very short time. furthermore, labels in various languages can be edited all at once. it can also be that some editors edit multiple items in parallel (e. g. in several tabs open at the same time) to perform similar kind of actions, like for example geolocate, or add descriptions. another explanation for the high number of very short times between edits is that we are able to identify edits by tools that leave a register trace (e. g. # petscan), but it is not possible to filter out api calls executed from tools that other developers might have implemented and did not tag or even advertise."
"in this section we discuss the findings highlighted in sections 6 and 7, and present some of the implications of our research results for the wikidata community."
this work was financially supported by the deutsche forschungsgemeinschaft (dfg) through the excellence graduate school jena school for microbial communication (jsmc) and the crc/tr124 funginet (project b4 to mtf and project c3 to ok).
"-language and culture: wikipedia is divided by language and by extension culture. in wikidata editors from all these languages and cultures come together to work together on the same data. -notability: wikidata serves all wikimedia projects and therefore needs to cover more concepts than any of the individual wikipedias. -large scale editing: wikidata, as a result of its virtue of being machinereadable and editable, is seeing considerably more edits done with the help of tools and bots than the wikipedias. indeed, it currently accumulates one third of all edits across wikimedia projects. -editing interface: wikipedia offers a text-editor like interface as well as a wysiwyg editor. wikidata offers a form-based interface as well as a large amount of special-purpose tools (e g. wikishootme, wikidata game)."
"measuring the diversity of tasks accomplished by editors to measure the diversity of of tasks that editors do, we distinguish between the different types of actions registered by wikibase 11 :"
"in this section we report about the exploratory analysis that we have done to understand the evolution of the editors in various dimensions. the goal of this analysis is to understand the trends that appear over time, and to find different groups of editors that should be addressed differently when designing engagement strategies."
"in summary, with this work we found that: -there is a skewed distribution of edit counts. -there is a skewed distribution of editors per item."
"a constant contribution over time is a signal of power editors but not of standard editors. if editors develop their editing habits, they are likely to schedule them regularly in their agendas, and the longer a habit runs for, the more established it becomes [cit] . so, in terms of the time spent while contributing, we hypothesise: hypothesis 2: [cit] found surveying editors, that indicates that editors take more responsibility and do different tasks over time (when they grow from novice to proficient), makes us hypothesise that an increasing trend in the diversity of tasks over time differentiates power from standard editors, assuming that a standard editor will have a lower probability of crossing the line from novice to proficient editor. hence, we formulate the third hypothesis as: hypothesis 3: an increasing diversity in the types of tasks done is a signal of power users but not of standard users"
"wikidata is a freely-available openly-editable knowledge base. [cit] . more than just human contributions, wikidata serves as a data integration hub where other knowledge bases (e. g. viaf, europeana, dbpedia) link to or are imported in. 6 wikidata is set apart from many of the other available knowledge bases in several ways: -community curated: wikidata's initial primary goal was to support wikipedia editors by providing them with a central knowledge base that holds data to be shared between all wikimedia projects. in order for this to happen and in the spirit of its sister projects, wikidata is open for editing by anyone and maintained by an open community of editors. -multilingual: in order to fulfil its initial primary goal of supporting wikipedia editors, wikidata needs to provide one central place to collect and maintain the same data that is then shared between all wikimedia projects. all editors must work on the same data independent of their language. wikidata achieves this goal by identifying its items and properties (the basic building blocks of wikidata used to describe concepts in the real world) with language-neutral identifiers. for example, the property \"instance of\" is identified by \"p21\" and the concept \"earth\" is identified by \"q2\". -knowledge diversity: wikidata is built as a secondary database. this means it is not meant to record raw facts. instead, it collects statements from other sources and references them. with this model, different views can be recorded on controversial topics and the consumer of the data can investigate further and judge which of the sources they accept. this is crucial for wikidata in order to cater to the many different cultures in wikimedia projects. it also aligns with wikipedia's ethos of referencing information and making it possible for the reader to dig deeper into a topic."
"the consolidation of social computing has led to multiple studies that examine human behaviour in various systems, including major volunteer crowdsourcing projects like wikipedia and open street maps. we review related work focusing on methods and findings about the contribution of wikidata's and wikipedia's volunteers, user attrition in web systems and the evolution of user behavior in volunteer systems."
"-we run a quantitative analysis about the volume of edits and the lifespan of editors (section 6). -we perform a longitudinal study over the wikidata history and identify the trends in the evolution of editing behavior of different groups of editors, mainly standard and power editors (section 7). -we define supervised classification methods to predict the range of months that an editor will be contributing to wikidata, and the range of edits that an editor will do in wikidata (section 8). -we highlight a set of implications that our findings may have in the wikidata community section 9."
"measuring user engagement in volunteer projects is key to understand who the most valuable users are and to design mechanisms that decrease attrition. a work related to the effect of wikipedia edit activity on engagement is [cit], which defines a function of edit probability where the more a user has edited the more likely she is to edit in the future. [cit] study how users join and leave an online community focusing on the linguistic aspects of their contributions. they provided a method for predicting the range of time when users would drop out of the community based on their use of words over time when writing posts in a beer-related community. their empirical research showed that the linguistic evolution stabilises after a while staying stable until drop out. as the authors showed, in the data set they analysed, this phenomenon is relative to the lifetime of the user, and not to absolute to a biological frame. [cit] measured the activity ratio and the daily devoted time by users in the citizen projects of galaxy zoo and the milky way project, grouping people into profiles like the hardworking, spasmodic, persistent, lasting and moderate users. in both data sets, they found that the majority of users are classified as moderate, and only a few are persistent users."
"we select two different classifiers: random forest 12 and a logistic classifier to compare their performance in terms of precision, recall, f1-score, and support. we evaluate the classifiers in different settings: (a) for a session-based evolution, we run the prediction evaluation with training data of size 100, 200, and 300 sessions. (b) for a month-based evolution, we pick training data of size 3, 5, and 10 months. the data that we use at this point is the complete set of edits done by humans without tools over items. we considered filtering out editors whose status is not 'gone', as in such case we cannot really state their lifespan with certainty. however, we noticed that after applying the ransac algorithm, there were no editors whose state is active and her lifespan is shorter than 15 months. the only editors who are not gone, have a lifespan longer than 15 months and therefore can also be labeled as having a long lifespan (no matter what the exact final value for the lifespan will be). figures 22 and 23 show the average f1-scores obtained for each class (power and standard) by the two classifiers, after running 10-fold subsample validation, for each training data size. in all cases, the random forest classifier outperforms the logistic classifier. if we compare the two plots of figure 22 to the two plots of figure 23, we observe that the former show stable f1-scores, even when augmenting the number of months or sessions in the training data. in the latter plots, there is a trend to increase the f1 with a bigger training data size."
"motivated by the findings presented in the previous section, we define our prediction tasks as follows: given an editor, and a set of observations based on the aforementioned productivity, participation and diversity indicators, we predict -whether the editor will be contributing with a high or level volume of edits and -whether the editor will contribute for a long or short lifespan."
"weaving a community of devoted people who are able to contribute duly of their own free will is one of the main challenges in wikidata. since wikidata's mission is to represent human knowledge in a structured way, the project needs the help of a large number of people. despite the positive response from thousands of volunteers, there is a clear need for attracting new contributors and growing the community, because there is still much data and many tasks to work on. 1 at the same time, it is important to retain contributors who already approached wikidata by encouraging them to contribute. to address this challenge of acquiring new people and maintaining the activity of existing community members, wikimedia organises events to advocate for free knowledge, disseminate the project, offer technical training for newcomers, establish social tights between members, as well as edit data and develop software together. there are also resources that aim to facilitate the editors' contribution. for example, the wikidata games 2 present editors with very simple edit requests (e. g. add the occupation or the gender of a person) that help to improve the completeness of the knowledge base. still, wikimedia's official reports indicate that there is a big number of the people who one day edited wikidata but are currently inactive. the latest statistics 3 [cit] the number of active editors 4 was between 7.8 k and 8.7 k. taking into account that wikidata registered more than three hundred thousand contributors during its history, the inactivity ratio is large. this fact could be detrimental for wikidata, especially when we consider that many of these inactive contributors did not edit for a very long time and might have, presumably, abandoned the project. this level of abandonment rate suggests that wikidata may not be maintained nor extended to its full potential. one may think that increasing editors retention in wikidata could lead to more item descriptions and more complete and up-to-date data."
"in this article, we work towards developing such a method in the context of wikidata. we aim at predicting the time that contributors will be in wikidata -lifespan, as well as the volume of edits they will contribute with, to understand the magnitude of their action in the knowledge base. therefore, our goal in this work is not to provide the solution to the retention problem designing a retention campaign but to build the prerequisites, providing a data-driven approach to the prediction task. in order to do so, first we carry out an exploratory analysis to understand differences between power and standard editors, and second, we provide and evaluate a predictive model that uses the insights of the exploratory data analysis."
"in this section, we describe the granularity of the evolution analysis, the crosssectional indicators that we obtain for each editor over their lifetime, and the two goals that we set to distinguish between power and weak users."
"regarding the diversity of type of edits, editors with longer lifespan tend to increase the diversity over the months, while editors with smaller lifespan can figure 13 . scatter plot indicating for each editor the slope and intercept obtained by the ransac algorithm for indicator i4 (seconds per session), in a month-based analysis of evolution, depicting the editors' lifespan. either decrease or increase the diversity (cf. figure 14) . as for editors with higher total volume of edits, they seem to keep the diversity of the types of edits, while editors with lower volume of edits increase it or decrease it (cf. figure 15 ). figure 15 . scatter plot indicating for each editor the slope and intercept obtained by the ransac algorithm for indicator i5 (diversity of type of edits), in a month-based analysis of evolution, depicting the editors' editcount."
"while these are natural extensions of our work, they are complex topics that are worth standalone articles. automatically identifying the quality of edits, for example, is a highly challenging task [cit] . in fact, the definition of data quality in wikidata is still under debate among the research and volunteer communities. 16 similarly, partitioning the knowledge base according to different topical domains could be done in many various ways and would require studying the application of specific techniques such as topic modelling in knowledge graphs. these two tasks are out of the scope of this study, which by design aims at understanding the differences in participation and dedication between power and standard editors, and predicting the group to which editors will belong in the future."
"in order to account for spatial aspects of the immune response, we extended the sbm to an agent-based model (abm), where cells are simulated as agents that can migrate in continuous three-dimensional space and can interact with each other on encounter in space. applying the bottom-up modeling approach, we made use of the rates that were determined by fitting the pi-sbm to the experimental data and estimated the diffusion coefficients of immune cells in blood (see figure 1) . due to high computational costs of abm simulations, applying the global optimization method sa-mmc was no realistic option and we chose the computationally affordable local optimization method adaptive regular grid search. this method searches for the optimum within a pre-defined parameter space, which in the present case was a two-dimensional space spanned by the diffusion coefficients for monocytes and pmn. in contrast, applying sa-mmc was beneficial in the case of pi-sbm for three reasons: (i) the parameter space was eight-dimensional, (ii) limitations of the parameter space would have been difficult to motivate biologically, and (iii) computational costs for repeated simulations were still acceptable due to the neglect of spatial aspects."
"knowledge bases have become key to enabling semantic search and exploration functionalities in a wide range of applications on the web. besides the general interest knowledge bases owned and managed by companies (e. g. google's knowledge graph), there exist openly available knowledge bases as a result of open data initiatives. for example, the linked open data [cit] ) consists of over one thousand structured data sets describing various topical domains, and published by different agents including universities, private companies and other organisations. wikidata is an open, free, multilingual knowledge base (vrandečić and krötzsch 2014) [cit] more than 37 million data items. while the creation of wikidata was motivated by the need of more efficient data management methods within wikipedia, wikidata has become an important knowledge base for many other systems and applications that reuse wikidata's item descriptions. moreover, hundreds of other external data sets such as viaf, the library of congress, europeana or facebook places created by libraries, governmental and private organisations have been integrated with wikidata, making wikidata the hub to explore a network of open knowledge spread over the web."
"the volunteering-based design in wikidata, akin to any other wikimedia project, encourages the contribution of intrinsically motivated people who believe in free knowledge and are eager to help with their expertise and cooperation. the system provides mechanisms for accountability and transparency (i. e. anyone can see who did what, and discussions are public), and strong contributions (and contributors) are openly acknowledged and recognised. the feeling of belonging to the community also drives volunteers in wikidata, like in wikipedia [cit] ."
"finding 1.1: there is a skewed distribution of edit counts (i. e. few editors with many edits and vice versa). figure 3 shows the boxplot with the counts of edits, by year in which editors started to edit. the median decreases with the years, which might be related to the fact that in the beginning there is a broader \"blank space\" to be edited."
-there is a slightly skewed distribution of lifespan. -there is not a linear relationship between the lifespan and the volume of edits done by editors. -in wikidata we find shorter times between edits than in wikipedia.
"for the reconstruction of haplotypes using haploreconstruct [cit] at least eight input parameters are needed. haplovalidate estimates the key parameters from the data, while others are not modified, but were chosen to fit to a broad set of data (see table 1 )."
"each set of haplotype blocks (with a given correlation cut-off) is tested for the independence of its clusters. allele frequency trajectories generated by the same selection target should be highly correlated even when separated into different clusters by a too stringent snp correlation cutoff. if this is the case, focal cluster correlations are significantly higher than the background cluster correlations. if there is no significant difference between focal and background correlations, we assume that all clusters on the focal chromosome are independent and represent different selected regions."
"the cut-off for determining whether snps are clustering or not clustering has a major influence on haplotype reconstruction; too stringent clustering splits regions that belong to the same haplotype block and too relaxed clustering combines independent regions into haplotype blocks [cit] . haplovalidate addresses this by generating haplotype blocks for different snp correlation cut-offs. the approach aims to identify the correlation cut-off which combines blocks that are not independent, but also separates independent blocks."
we benchmarked haplovalidate using 1000 selective sweep simulations on two entire chro- haplotype blocks containing multiple targets of selection can either be present at the beginning of the experiment or emerge in later generations due to a recombination event.
we evaluated the performance of haplovalidate using a range of sweep simulations with different numbers of targets and selection coefficients. we applied haplovalidate to two already published d. simulans e&r studies and compared our results to the original studies.
"the clustering of is based on clustering parameters that were crosschecked with experimentally generated haplotypes. hence, we were interested, whether it is possible to recover the same clustering with haplovalidate, without consulting evolved haplotypes. using the candidate snps from with haplovalidate (mncs 0.01) resulted in the identification of very similar clusters (see figure 7) . instead of 88 clusters, haplovalidate detected 87 haplotype blocks of which 85 overlap at least for 95% with regions from . vice versa, 81 regions detected by overlap at least for 95% with haplovalidate."
"as the landscape of a manhattan plot can differ dramatically between experiments, the same window-size is not suitable for all data. to use a window-size accounting for the peak landscape we calculate the median normalised cmh-score sum (mncs) as the median cmhscore sum per window normalised by the sum of all cmh-scores (see equation 1). we use this parameter to automatically choose the window-size. window sizes were varied from 0.1 to 10 mb in 0.1 mb steps and the mncs was calculated for each window-size."
"in addition to the snp correlation cut-off, we also varied the window size to fine-tune haplovalidate, as window-size determines the maximal length of reconstructed haplotype blocks. small windows result in shorter blocks [cit], which facilitates independent block separation. hence, while it is preferred to have small window sizes, this may result in too few snps in a window for a reliable estimate of the cluster correlations."
"here, we propose a new approach to define the haplotype reconstruction criteria, which avoids ad hoc choices of clustering parameters and does not depend on the availability of phased haplotype data. our approach takes advantage of the full genomic data to distinguish between statistically significant clustering, most likely caused by directional selection, and random associations. it is implemented in the r package haplovalidate."
step 1: haplotypes are reconstructed with minimum cluster correlations ranging from 0.9 to 0.3 in 0.1 steps. the reconstruction with most clusters is used as starting point to determine focal and background cluster correlations.
"nevertheless, our study also demonstrated the limits of a haplotype block-based analysis of the adaptive architecture. we noted that a high fraction of the reconstructed haplotype blocks contained multiple selection targets, leading to a substantial under-estimation of selection targets, if each haplotype block is considered the outcome of selection operating on a single target in this block. restricting the analysis to intermediate generations (up to f20 or f30) improved the resolution -significantly more single-target blocks were identified while fewer multiple-target blocks with a fewer selection targets are detected (see figure 5 ). interestingly, most multiple-target haplotype blocks were already present in the founder population, indicating that the initial haplotype structure is an important factor for shaping the genomic signatures of selection. more work is needed to understand how the haplotype composition of the founder population in combination with the number of founder haplotypes affects the power of e&r studies."
"moving from a snp-centric analysis to the identification of selected haplotype blocks provides a significant advancement of e&r studies . introducing haplovalidate, which reconstructs selected haplotype blocks from genomic e&r time-series data, we provide a tool, to make the reconstruction of selected haplotypes a routine method that does not rely on the availability of haplotype information, neither from the founder population, nor from evolved individuals. haplovalidate can be applied to a broad range of data, including few selection targets [cit] as well as many . we attribute this generality to the data-driven selection of two key parameters of the clustering procedure, the minimum correlation between snps constituting a cluster and the window size."
"simulans using using mimicree2 v206 [cit], simulating diploid sexual organisms and loci with constant selection coefficients. we mimicked an e&r experiment in drosophila simulans consisting of 10 replicates with a population size of 1200 each, evolving for 60 generations. we extracted allele frequencies for every 10th generation [cit] ) and haplotypes for the most evolved generation (f60). we restricted our analysis to the main autosomes of d. simulans (chromosome 2 and chromosome 3)."
"haplovalidate is an extension of the r package haploreconstruct [cit], which uses hierarchical clustering on pairwise correlations of snp allele frequencies at multiple time points. a cluster of correlated snps in a genomic window defines a reconstructed haplotype block that increases in frequency over time. haploreconstruct requires a variety of parameters for haplotype detection, which are typically defined ad hoc and tailored to detect selected haplotypes starting at low frequency. haplovalidate overcomes these limitations by estimating key clustering parameters from the full genomic data."
standard haplovalidate analyses use a window-size corresponding to a mncs of 0.01. in the case that data-sets do not produce a sufficient number of clusters because the windows were too small we used a mncs of 0.03 and the corresponding window-sizes.
"using only intermediate time-points resulted in many clusters not containing a selection target (56 % for f20 and 24 % for f30, see supplementary data s2). 74 % of these clusters are based on hitchhiking snps connected to a target of selection, but while the hitchhikers passed the significance threshold, the causative snp did not and was therefore not included in the clustering. hence, even when the causative snp is missing from the candidate snps, the correct selected region can be already identified in early generations (for an example see"
"selection targets that share a haplotype have highly correlated allele frequency trajectories . whereas the analysis of f60 results in one region, generation of f20 results in two separate regions, which correspond to haplotype groups present at these time points. selected snps show the same colours as in plot 2 and in the left panel."
"if not stated otherwise, all analyses were conducted with r version 3.6.0 [cit] and the r package poolseq version 0.3.1 [cit] and haploreconstruct 0.1.3_3 [cit] ."
focal cluster correlations can be only calculated if at least two clusters are present in a given window. windows containing only one cluster are not considered. [cit] randomly selected snps were used to increase computational efficiency.
"starting allele frequencies and selection coefficients were taken from . the corresponding number of loci was drawn from the set of 99 selection targets without replacement. in the case of 128 loci, 29 randomly chosen value pairs were used twice. snps matching the allele frequency were randomly chosen from the founder population. we used the d. simulans specific recombination map (dsim_recombination_map_loess_100kb_1. [cit] ). we generated \"pool-seq data\" with 50x coverage and added sequencing noise by binomial sampling based on the allele frequencies."
"we fixed the haplotype reconstruction parameters such that alleles starting from any frequency (starting allele frequency between 0 and 1) in at least 1 replicate are included. the allele frequency change threshold parameter aims to focus on snps changing more than expected under drift. because the χ 2 -test and cmh-test used here already account for drift [cit] we set the haploreconstruct threshold to 0. following, we required at least 20 snps for each cluster and only clusters sharing at least 4 snps could be merged."
step 2: we normalise allele frequency data by using arcsine-square-root transformation followed by centring and scaling. cluster correlations are computed by calculating the distribution of median allele frequency trajectory correlations between two clusters. these two clusters can be either clusters within the window used for haplotype reconstruction (focal cluster correlations) or clusters on different chromosomes (background cluster correlations).
"apart from experimental phasing, which requires living flies [cit], various methods have been suggested to statistically infer haplotypes from poolseq data. taking advantage of sequenced founder haplotypes, the haplotypes of evolved individuals have been determined by regression [cit], a hidden markov-model [cit], maximum likelihood [cit], and a system of linear equations [cit] . these methods rely on the complete knowledge of all involved founder haplotypes [cit] and are limited to a restricted window size because otherwise the error-rate is too high [cit] ."
"step 4: if a selection target is present in several haplotypes, they will be identified as independent, overlapping clusters. to identify the dominating cluster per selection target, we filter genomic regions with overlapping cluster for the cluster with the most significant allele frequency change (cmh-test/χ 2 - [cit] )."
"the founder population was created from 189 experimentally phased haplotypes originating from a natural d. simulans population . we generated the same number of simulations for 16, 32, 64 or 128 selected loci (equal number of loci on both chromosomes)."
"the challenge is to determine which blocks are independent. this is achieved by comparing the correlation of snps in the focal cluster with snps of other clusters on the chromosome (focal cluster correlations) of snps in the focal cluster with snps from clusters on the other chromosome (background cluster correlation). with no physical linkage between different chromosomes, background cluster correlations are an estimate of random associations."
"5) solve the following optimization function with attained η * i and µ * i using the toolbox yalmip and solvers mosek [cit], further get the optimal value γ * (κ *"
"the record as a common data structure is described as shown in figure 26 . the protocol of record is the direct accessibility to its fields, which implies that the i/o operations of records may be directly (randomly) conducted in any field of the record. in the record model, recordst, the element in each field, field(in) rt i is allowed in different types. two of the key control variables of the record adt are maxfieldsn and #fieldsn. the former denotes the maximum allowable fields of the record; and the latter denotes the current position of the last field in the record. the addresses and the order of all fields are parallel and arbitrary. the constraints on each field of recordst may be explicitly specified in the architectural model of the record."
the following theorem provides a sufficient condition of computing the mmdadt via hplfs. theorem 2: consider the continuous-time switched system (1) where the minimum value of modulus of (a i (κ i ))'s eigenvalue η * i is given by theorem 1. there exist controllers κ i . let us define the objective function
"in order to solve problem 2, a i and v i will be replaced by a i (κ i ) and v i (κ i ). hence hplf candidate in (25) may be expressed as"
"the above-described experiment is a simple split-plot design (spd). there are two kinds of these experiments. the first case considers a block design by superimposing one randomized complete block design (rcbd) on top of another rcbd. the first rcdb involves the whole-plot factor and the second involves the split-plots and the split-plot factor. in the second case, the split-plot design considers a completely randomized design (crd) to the first factor and a split-plot to the second. this second experiment is called between-and-within-subjects design (hinkelmann; [cit] ) . suppose in each case the factor a is the whole-plot factor with a levels and b is the split-plot factor with b. in the first case, the analysis of variance (anova) table is presented in table 1, considering r replicates. this kind of design is referred by spd(rcbd, rcbd)."
"on the basis of the work products, recordst. staticbehaviorspc, developed in the preceding subsection as a set of static behavioral processes, this subsection describes the dynamic behaviors of recordrecordst at run-time using the rtpa process dispatching methodology. recordst. dynamicbehaviorspc as shown in figure 33 models the event-driven behaviors of recordst, which establishes the relations between system events and the stack behavioral processes. the event-driven dispatching mechanisms also put recordst into the context of applications."
"an adt encapsulates a data structure and presents the user with an interface through which data can be accessed. it exports a type, a set of valid operations, and any axioms and preconditions that define the application domain of the adt. adts extend type construction techniques by encapsulating both data structures and functional behaviors. the interface and implementation of an adt can be separated in design and implementation. based on the models of adts as generic data structures, concrete data objects can be derived in computing."
"figures 20 through 25 describe a typical adt model, sequencest, in a coherent design and using a unified formal notation. with the rtpa specification and refinement methodology, the mechanisms, architectures, and behaviors of sequencest are rigorously and precisely modeled."
"thus, the switched system is globally uniformly asymptotically stable when average dwell time is not smaller than τ . according to lemma 2, the dimension of matrix exponential term is decreased to a one-dimensional exponential term, which implies that"
"there exist many situations where for a factorial experiment different types of experimental units are being used and where the levels of some factors are applied sequentially, necessitating separate randomization procedures (hinkelmann; [cit] ) . the simplest case consists of two factors where the experimental units of one size are randomized for the levels of one of the two factors. those experimental units are then subdivided into smaller experimental units to which the levels of the second factor are randomized. the precisions for the comparison among levels of each factor are different. also, the fact that each factor is associated with different types of experimental units leads to different experimental error variances associated with these comparisons. this is the main reason that makes this kind of experiment different from the factorial experiments."
"a set of seven behavioral processes such as create, initialize record, retrieve field, retrieve record, update field, update record, and release is designed in recordst.staticbehaviorspc. the following subsections describe how the static behaviors of recordst as specified in eq. 10 are modeled and refined using the denotational mathematical notations and methodologies of rtpa. because of the similarity of adt manipulation processes between recordst and stackst as described in eq. 4, only key processes, initializerecordpc, retrievefieldpc, retrieverecordpc, updatefieldpc, and updaterecordpc, of recordst will be formally modeled in the following subsections."
"in order to enhance the algebraic methods and reduce their complexity, rtpa is introduced [cit] . the rtpa methodology provides an explicit and easy-to-use algebraic approach for adt and system modeling. rtpa reveals two fundamental technologies for adt system modeling and refinement known as the unified data model and the unified process model [cit] ."
"before proceeding, several definitions are necessarily introduced to aid in an understanding of subsequent developments, i.e. [cit] the set of admissible switching rules with modedependent dwell time"
"the details in deriving the relations from (13) to (17) may be found in section 1.4 of ref. [cit] . in order to estimate whether a symmetric matrix polynomial is representable as a sos form, a general method is to establish square matrix representation (smr), as outlined below."
"if the closed switched system is stabilized by controllers κ * then proceed to step 7). otherwise, set the step size to a smaller one in step 3) and return to step 3). 7) the switched system (1) could be stabilizable by output feedback controllers κ * i . terminate the algorithm."
"the stack creation process of stackst, createstackpc, is formally modeled as shown in figure 5, which establishes a new stack in system memory and links it to a specified logical id. the input arguments of the process are the given name of the stack as well as its size and the type of element. the output result and status are the creation of the stack as well as the settings of related initial values in the udm of stackidst.architecturest. in order to create a physical stack and link it to the logical name of stackidst, the process calls a system support process, allocateobjectpc, for dynamic memory manipulation for adts as illustrated in figure 12 . when the given stack has already existed or cannot be established due to memory"
"a queue is a typical data structure for modeling the first-in-first-out (fifo) mechanism of an adt with a set of elements in the same type. the conceptual model of queues and its key control variables are introduced in this section. based on it, formal models of the queue adt in terms of its architectural and static/dynamic behavioral models are rigorously developed in rtpa."
"this section presents the proposed method in detail. subsection a presents basic concepts of the proposed method, containing the method to construct the hplfs. subsection b presents the method to pursuing mmdadt in light of the polynomially parameter-bounded condition. subsection c presents the method to determine the mode-dependent controllers. subsection d provides a summary of the overall proposed method."
"the stack as a common data structure is described as shown in figure 3 . the protocol of stacks is lifo, which implies that the i/o operations of stacks must be on its top or most recent elements as identified by the pointer of the current position. in the stack model, stackst, each elementrt shares the same type where rt represents the run-time type in rtpa. two of the key control variables of the stack are sizen denoting the maximum capacity of the stack and currentposp denoting the current top position of the stack. the addresses of elements grow from the bottom up with the relative base address at 0h. the top-level adt model of the stack, stackst, encompassing its architecture, static behaviors, and dynamic behaviors, can be specified in rtpa as follows:"
before run-time [cit] the following subsections describe how each of the seven behavioral processes of adt stackst as specified in eq. 4 are modeled and refined using the denotational mathematical notations and methodologies of rtpa.
"on the basis of the work products, queuest. staticbehaviorspc, developed in the preceding subsection as a set of static behavioral processes, this subsection describes the dynamic behaviors of queuest at run-time using the rtpa process dispatching methodology. queuest. dynamicbehaviorspc as shown in figure 19 models the event-driven behaviors of queuest, which establishes the relations between system events and the stack behavioral processes. the event-driven dispatching mechanisms also put queuest into the context of applications."
"experimenters and statisticians sometimes mistake a split block design with a split-plot design. this is especially true when one of the factors involves periods, say factor b, or other forms of repeated measurements through time, which they designate as split-plot treatments. whenever the times are calendar or clock times, the periods are crossed over the other set of treatments, say a. this means that they are in a split block arrangement. such experiments could be considered to be in a repeated measurement category but in many instances, each of the time periods and especially the interactions of factor a with time periods b are of interest to an experimenter, for example, the various pickings of a tomato or bean crop or the various cuttings of a hay crop. table 3 -simplified anova scheme for the split-plot design considering repeated measure (t -factor with t levels) in the spd(rcbd, rcbd)."
"figures 15 through 19 describe a typical adt model, queuest, in a coherent design and using a unified formal notation. with the rtpa specification and refinement methodology, the mechanisms, architectures, and behaviors of queuest are rigorously and precisely modeled."
"a record is a typical data structure for modeling an adt with a set of fields configured in different types. the conceptual model of record and its key control variables are introduced in this section. based on it, formal models of the record adt in terms of its architectural and static/dynamic behavioral models are rigorously developed in rtpa. it is recognized that record is one of the most powerful and widely used adts and data structures, because it allows flexible field structures and data types. the mathematical model of a record is a tuple, which forms a fundamental architectural modeling means in adt and software system modeling."
"this paper attempts to investigate a type of static output feedback controller, similar to that which is discussed in ref. [cit] for stabilizing continuous-time switched linear systems with time-dependent constraints. one of the major contributions of this paper is to find a new method to reduce the dependency of priori knowledge based on polynomially parameter-bounded condition and explore mmdadt for each subsystem, during control synthesis when the initial subsystems are not all stable. another contribution is the construction of hplfs depending polynomially on modedependent controllers, and deriving of the existence and solvability conditions in accordance with a set of sos. the remainder of this paper is organized as follows. section ii reviews some necessary concepts and definitions of timedependent switched systems, and further puts forward two basic problems. section iii establishes the iterative hplfs conditions for minimum mode-dependent average dwell time switched systems. examples are provided in section iv to verify the effectiveness of the developed methods. a conclusion of this work is given in section v."
"a stack is a typical data structure for modeling the last-in-first-out (lifo) mechanism of an adt with a set of elements in the same type. the conceptual model of stacks and its key control variables are introduced in this section. based on it, formal models of the stack adt in terms of its architectural and static/dynamic behavioral models are rigorously developed in rtpa."
"switched systems consist of a finite class of subsystems and switching signals which orchestrate the switching rules between each subsystem. switched systems are an important approach to analyzing and researching the issue of hybrid systems in engineering applications, such as power converter systems [cit], hybrid transportation systems [cit] and vertical take-off and landing (vtol) unmanned aerial vehicles (uav) [cit] . in stabilizing nonlinear systems, a method of approximation via polytopic linear switched systems [cit] may be used to pursue asymptotical stability, or the use of nonsmooth lyapunov functions to track performance of given parameter constraints [cit] . in this paper, the switched systems"
"figures 26 through 33 describe a typical adt model, record st, in a coherent design and using a unified formal notation. with the rtpa specification and refinement methodology, the mechanisms, architectures, and behaviors of recordst are rigorously and precisely modeled."
"it should be noted that the eigenvalues of (a i (κ i )) contain not only real numbers but also conjugate complex roots, furthermore, (a i (κ i )) is not always hurwitz matrix while searching for its minimum and maximum eigenvalue within"
"able for behavioral refinement and the modeling of dynamic behaviors of adts. another approach to adt specification is the algebraic methods, which treat an adt as an algebraic structure of sorts and operations on the sorts [cit] ). an algebraic model of an adt, stack, is illustrated in figure 2 [cit] . the advantage of algebraic models of adts is its abstraction and elegance. however, it is often too abstract for system implementers and users, especially for dealing with some non-trivial adts."
"is a generic architectural model of a software system as well as its hardware components, interfaces, and internal control structures, which can be rigorously modeled and refined in denotational mathematics as a tuple, i.e.:"
"regarding lemma 1, it is difficult to solve the timedependent switched systems function, since the third inequality contains the matrix exponential of a i (κ) which is not a convex optimization function. in order to deal with this nonlinear term, graziano chesi proposes a polynomial approximation of this term on the basis of taylor's expansions in ref. [cit] . unfortunately, the high-order error in approximation resulting from this method, requires a high-order extended matrix to approximate the third term. this undoubtedly increases the numerical data size, and hence operation time. therefore, it is necessary to introduce a more feasible and simpler method to reduce the complexity of the matrix exponential term. a feasible way to approach this problem would be to estimate a bound on the matrix exponential term as below."
"a type system specifies data object modeling and manipulation rules of a programming language, as that of a grammar system that specifies program syntaxes and composing rules of the language. therefore, the generic complex types can be modeled by abstract data types [cit], which are a logical model of a complex and/or user defined data type with a set of predefined operations."
1) express the switched system as in (1) and the closedloop system with controllers κ i as in (4). 2) choose the set k of allowed controllers.
"the sequence as a common data structure is described as shown in figure 20 . the protocol of sequence is fifo, which implies that the i/o operations of sequence must be at its tail and head, respectively. in the sequence model, sequencest, each elementrt share the same type rt. two of the key control variables of the sequence are sizen denoting the maximum capacity of the sequence and currentposp denoting the current tail position of the sequence. the addresses of elements grow from the head to the tail with the relative base address reserved at 0h. the head or the front element of the sequence, element 1 rt, is always located at address 1h based on the relative base address of the sequence basen."
"remark 6: although theorem 1 gives the compact set of η i, but it is a relatively conservative method to explore the upper-bound of η i . firstly, the value of ρ i is not an optimal bound for every controller κ i, so the traversing space is always larger than that which is needed. secondly, the step size 10 −q is also not optimal, because the requisite eigenvalue maybe exist between the step size 10 −q and 10 −(q+1) . moreover, during the process of traversal, the eigenvalues with positive real-part are also considered which could be treated as redundant data, hence the upper-bound of η i will probably deviate from the desired. therefore, the compact set of η i has relative conservativeness. in other words, perhaps some η i out of the compact set also could fulfill the constraints of theorem 2."
"on the basis of the udm model of queuest developed in the preceding subsection, the behaviors of the queue can be modeled as a set of upms operating on the udms and related input variables. the high level behavioral model of the queue adt is modeled by queuest.staticbehaviorspc as shown in eq. 6. the schemas of the queue can be further refined by a set of upms for each of the behavioral processes. a set of seven behavioral processes such as create, enqueue, serve, clear, empty test, full test, and release is designed in queuest. staticbehaviorspc. the following subsections describe how the static behaviors of queuest as specified in eq. 6 are modeled and refined using the denotational mathematical notations and methodologies of rtpa. because of the similarity of the adt manipulation processes between queuest and stackst as described in eq. 4, only two key processes, enqueuepc and servepc, of queuest will be formally modeled in the following subsections."
"then, suppose that there existsκ i holding for some κ i, let us define the functionγ * and mode-dependent average dwell time τ * i to pursue the minimum value of the total dwell time"
"the additional error in this model has the justification for its inclusion in the model due to the lack of randomization of the time factor along the blocks or levels of the factor under test (treatments) (a-factor). compound symmetry (cs) or huynh-feldt conditions are necessary assumptions, regardless of this additional error, for the validity of the f tests in anova. one way do deal with models of this kind is to perform a mixed model analysis, considering only the error effects in the model as random effects and with different covariance structures for the error terms. the mixed model analysis can deal with several other error covariance structures, but can not be done by sisvar."
"contrasting the behavioral models of queuest.behaviorspc in rtpa as developed in this section and that of predicate logic as shown in figure 1, advances of the rtpa method and notations are well demonstrated. among them, the most important merit is that an adt model in rtpa can be seamlessly transformed into code in any programming language when the formal models are designed and refined systematically."
"on the basis of the work products, sequencest.staticbehaviorspc, developed in the preceding subsection as a set of static behavioral processes, this subsection describes the dynamic behaviors of sequencest at run-time using the rtpa process dispatching methodology. sequencest.dynamicbehaviorspc as shown in figure 25 models the event-driven behaviors of sequencest, which establishes the relations between system events and the stack behavioral processes. the event-driven dispatching mechanisms also put sequencest into the context of applications."
"in the second case, the analysis of variance (anova) table is presented in table 2, also considering r replicates in a completely randomized design for the whole-plots factor. this kind of design is referred by spd(crd, rcbd)."
"a set of eight behavioral processes such as create, append, find element, retrieve, clear, empty test, full test, and release is designed in sequencest.staticbehaviorspc. the following subsections describe how the static behaviors of sequencest as specified in eq. 8 are modeled and refined using the denotational mathematical notations and methodologies of rtpa. because of the similarity of adt manipulation processes between sequencest and stackst as described in eq. 4, only key processes, appendpc, findelementpc, and retrievepc, of sequencest will be formally modeled in the following subsections."
"the architecture of recordst can be rigorously modeled using the udm technology of rtpa. the udm model of the record adt, recordst.architecturest, as shown in figure 27 provides a generic architectural model for any concrete record in applications with two key parameters, i.e., #fieldsn and filedtype(in)rt, where the constraints on each field are given in the right-hand side of the vertical bar. supplement to the key architectural attributes, there is a set of status fields in the record, which models the current operational status of the record in boolean type such as createdbl, initializedbl, fieldretrievedbl, recordretrievedbl, fieldupdatedbl, recordupdatedbl, clearedbl, emptybl, fullbl, and releasedbl."
"the queue as a common data structure is described as shown in figure 15 . the protocol of queues is fifo, which implies that the i/o operations of queues must be at both its tail and head, respectively. in the queue model, queuest, each elementrt share the same type rt. two of the key control variables of the queue are sizeofqueuen denoting the maximum capacity of the queue and currentposp denoting the current tail position of the queue. the address of an element is growing from head to tail with the relative base address reserved at 0h. the head or the front element of the queue, element 1 rt, is always located at address 1h based on the relative base address of the queue basen. the top-level adt model of queue, queuest, encompassing its architecture, static behaviors, and dynamic behaviors, can be specified in rtpa as follows:"
"dynamic behaviors of a system are run-time process deployment and dispatching mechanisms based on the static behaviors. because system static behaviors are a set of component processes of the system, to put the static processes into an interacting system at run-time, the dynamic behaviors of the system in terms of process dispatching are yet to be specified. with the work products, stackst.staticbehaviorspc, developed in the preceding section as a set of static behavioral processes, this subsection describes the dynamic behaviors of stackst at run-time using the rtpa process dispatching methodology. stackst.dynamicbehaviorspc as shown in figure 14 models the event-driven behaviors of stackst, which establishes the relations between system events and the stack behavioral processes. the event-driven dispatching mechanisms also put stackst into the context of applications. figures 3 through 14 describe a typical adt model, stackst, in a coherent design and using an unified formal notation. with the rtpa specification and refinement methodology, the mechanisms, architectures, and behaviors of stackst are rigorously and precisely modeled."
"problem 2: design controllers κ i to asymptotically stabilize each mode of the switched system (1) within set k, where k (including admissible controllers) is defined as:"
"a number of adts have been identified in computing and system modeling such as stack, queue, sequence, record, array, list, tree, file, and graph [cit] . a summary of the ten typical adts is provided in table 1 where the structures and behaviors of the adts are described. adts possess the following properties: (i) an extension of type constructions by integrating both data structures and functional behaviors; (ii) a hybrid data object modeling technique that encapsulates both user defined data structures (types) and allowable operations on them; (iii) the interface and implementation of an adt are separated. detailed implementation of the adt is hidden to applications that invoke the adt and its predefined operations."
"the main analysis module is the analysis of variance (anova) of linear statistical models. although very powerful, this option can only be used for balanced and orthogonal data in sisvar, except for the case of one-way anova design with one factor. for this specific model, when there are missing data, the user should simply omit the line corresponding to the missing values in the file. among the advantages that sisvar has over its competitor, statistical analysis systems is the ability to slice the interaction and nested effects among fixed factors of linear models. besides the analysis of variance, there is the possibility to apply multiple comparison procedures and contrast of the sliced means of one factor into settled levels of the other including the scott-knott test, absent in most of the competitor statistical programs. for quantitative factor effects, the sliced crossover or hierarchical effects could be done by regression analysis. several other feature of sisvar could be seen in ferreira (2011 ) or in ferreira (2014 . this paper presents a special capability of sisvar to deal with fixed effects models with several restrictions in the randomization procedure. these restrictions lead models with fixed treatment effects, but with several specifications of error structure. one way do deal with models of this kind is to perform a mixed model analysis, considering only the error effects in the model as random effects and with different covariance structures for the error terms. another way is to perform an analysis of variance with several error effects. these kinds of analysis, for balanced and orthogonal designs, can be done by using sisvar. the software leads an exact f test for the fixed effects and allows the user to apply multiple comparison procedures or regression analysis for the factor levels of fixed effect, and also in crossed and nested classifications."
"on the basis of the udm model of sequencest developed in the preceding subsection, the behaviors of the sequence can be modeled as a set of upms operating on the udms and related input variables. the high level behavioral model of the sequence adt is modeled by sequencest.staticbehaviorspc as shown in eq. 8. the schemas of the sequence can be further refined by a set of upms for each of the behavioral processes."
"in light of different switching rules, switched systems are usually divided into three main classes [cit] : those under arbitrary switching constraints, those under state-dependent constraints and those under time-dependent constraints. according to the first class, the state is able to switch from one subsystem to another with arbitrarily fast velocity or arbitrary trajectory. as for the second class, the switching event occurs at each switching surface when the trajectory crosses a given switching surface. for the third class, the switching should spend no less than an allowable time period in one subsystem, known as dwell time. in the study of stabilization of switched systems with time-dependent constraints, the dwell time is always a given constant [cit] rather than a minimum value, so it may dramatically reduce the efficiency of switched systems. therefore, the exploration of mmdadt for switched systems is significant, and it is more available in many practical cases."
"the number of parametric variables is 51, and the number of independent variables is 4. fourthly, the sought controllers κ i are computed in accordance with step 6) . we obtain"
"then, v(x) is an hplf of degree 2d hom for the system (1). for switched systems, the hplf candidate of the i-th subsystem is denoted as"
"therefore, in the light of above two examples, mmdadt switching is shown to potentially require far less dwell time to stabilize the unstable switched system, with better response performances. furthermore, the approach discussed in this paper can be put into practice to stabilize the switched system with fewer constraints and shorter dwell time. for example, vtol uav, which could transit between vertical and horizontal flight modes, is able to use the approach to design individual controller with mmdadt to improve the performance of mode switching process with more flexibility and less transition time."
"where is a free vector. the expression (19) is defined as a complete smr, and so the gram matrix method for polynomials may be extended to the expression of matrix polynomials. l( ) is a linear parametrization of the linear set l 2d hom, satisfying"
"in this section, two numerical examples of the proposed method will be presented. the approximation functions will be solved by matlab installed toolbox yalmip and solvers penlab and mosek on a universal computer, which equipped with windows 7, intel core i7, 2.5ghz, 12 gb ram."
the associate editor coordinating the review of this manuscript and approving it for publication was xudong zhao. are in the framework of linear systems and asymptotically stabilized by time-dependent hplfs constraints.
"using types to model real-world entities can be traced back to the mathematical thought the formal design models of a set of abstract data types (adts) of bertrand russell [cit] ( [cit] ) . a type is a category of variables that share a common property such as the kind of data, domain, and allowable operations. types are an important logical property shared by data objects in programming [cit] . although data in their most primitive form is a string of bits, types are found expressively convenient for data representation at the logical level in programming. type theory can be used to prevent computational operations on incompatible operands, to help software engineers to avoid obvious and not so obvious pitfalls, and to improve regularity and orthogonality in programming language design."
"abstract data types (adts) have been recognized as an important set of rigorously modelled complex data structures with pre-specified behaviors. this paper has introduced the formal rtpa modeling methodology for adt architectures and behavioral specification and refinement in a top-down approach. the architectures, static behaviors, and dynamic behaviors of a set of typical adts such as stack, queue, sequence, and record, have been comparatively studied. two generic methodologies of rtpa known as the unified data models (udms) for system architectural modeling and the unified process"
"the number of parametric variables is 28, and the number of independent variables is 3. fourthly, the sought controllers,κ i, are computed in accordance with step 6), we obtain"
"a sequence is a typical data structure for modeling an adt with a series of elements in the same type where their order information is important. the conceptual model of sequences and its key control variables are introduced in this section. based on it, formal models of the sequence adt in terms of its architectural and static/dynamic behavioral models are rigorously developed in rtpa."
"the main advantage to use sisvar is its capability to deal with split-plot type of design, including split-block and split-plot with repeated measures employing the simple anova method. also, it allows multiple comparisons analysis and regression on the interaction effects present in the model easily."
"on the basis of the udm model of recordst developed in the preceding subsection, the behaviors of the record can be modeled as a set of upms operating on the udms and related input variables. the high level behavioral model of the record adt is modeled by recordst. staticbehaviorspc as shown in eq. 10. the schemas of the record can be further refined by a set of upms for each of the behavioral processes."
"there are a number of approaches to the specification of adts. mathematically, the main approaches are of logical and algebraic, as well as their combinations. although each of these approaches has its advantages, there are gaps when applying them to solve real-world problems. the logical approach is good at specifying high-level static behaviors of adts in forms of their preconditions and post-conditions of operations. for instance, a specification of queue as an adt in predicate logic is shown in figure 1 [cit] international journal of software science and computational intelligence, 2(4), 72-100, [cit] 75"
"definition 4. the unified process model (upm) of a program ℘ is a composition of a finite set of k processes according to the time-, event-, and interrupt-based process dispatching rules, i.e.:"
"this is what sisvar called \"complex variance\" using the usual notation of specialized literature. someone can deal with this situation in sisvar performing all f tests and all multiple comparison procedures immediately and easily. several multiple comparison procedures are available in sisvar. among them there is the scott-knott test to which the results are some grouping of the treatment means where there are no ambiguous results as in the traditional multiple comparison procedures as tukey and snk. also, if the treatment levels are continuous variables (fixed effects), the appropriate analysis is to fit a linear regression model. the sisvar can fit several linear models, where the effects to be included in the model are chosen by the users."
"in addition an expression, denoted the power vector h(x, d hom ) is adopted, whose entries are monomials of degree equal to d hom without repetitive coefficients. generally, a typical power vector satisfies a recursive rule, so that"
"the architecture of sequencest can be rigorously modeled using the udm technology of rtpa. the udm model of the sequence adt, sequencest.architecturest, as shown in figure 21 provides a generic architectural model for any concrete sequence in applications with three key fields, i.e., elementrt, sizen, and currentposp, where the constraints of each field are given in the right-hand side of the vertical bar. supplement to the key architectural attributes, there is a set of status fields in the sequence, which models the current operational status of the sequence in boolean type such as createdbl, appendedbl, firstfoundbl, lastfoundbl, retrievedbl, clearedbl, emptybl, fullbl, and releasedbl."
"lemma 1 [cit] : for average dwell-time switching mode, there exists a class of hplfs and an average dwell time parameter τ, for switched systems, such as:"
"output feedback controllers κ i are designed to asymptotically stabilize a switched system whose open loop subsystems are not all stable, and the admissible set of controllers"
"the architecture of queuest can be rigorously modeled using the udm technology of rtpa. the udm model of the queue adt, queuest. architecturest, as shown in figure 16 provides a generic architectural model for any concrete queue in applications with three key fields, i.e., elementrt, sizen, and currentposp, where the constraints of each field are given in the righthand side of the vertical bar. supplement to the key architectural attributes, there is a set of status fields in the queue, which models the current operational status of the queue in boolean type such as createdbl, enqueuedbl, servedbl, clearedbl, emptybl, fullbl, and releasedbl."
"previous research has indicated that stability analysis is a fundamental issue in the study of switched systems. for an arbitrary switching rule, the method of common lyapunov functions (clfs) provides a basic tool for guaranteeing this type of switched systems' asymptotic stability properties. considerable efforts have been put into finding various clfs, e.g. common quadratic lyapunov functions (cqlfs) [cit], common stochastic lyapunov functions (cslfs) [cit], common hplfs [cit] and common homogeneous rational lyapunov functions [cit] (hrlfs). but this method is not only lacking of a switching mechanism, but also presents a dramatic increase of robust conservativeness. a variety of attention has been paid to researching stability under constrained switching, which mainly involves state-dependent switching and time-dependent switching. in the state-dependent switching case, the behavior of inactive subsystems has no influence on the whole system. with regard to switched linear systems, several sufficient stability conditions expressed by different techniques, such as the multiple lyapunov functions (mlf) [cit] or the piecewise quadratic lyapunov functions (pqlf) [cit], have the capability to analyze stability properties of the switched systems. for time-dependent switching, also called slow switching [cit], comprises dwell time [cit] (dt) switching, average dwell time [cit] (adt) switching, minimum dwell time [cit] (mdt) switching and mode-dependent average dwell time [cit] (mdadt) switching. on account that stability analysis of time-dependent switched systems needs to fulfill the exponential stability conditions, it is rather difficult to decouple between controllers variables and dwell time variables in exponential terms. hence the aforementioned time-dependent methods of stability analysis usually use a given dwell time constant or require a priori knowledge. therefore, a tactfully new approach to simplifying exponential terms to linear terms and a subsequent exploration of the lower bound of dwell time are necessary, while the switched system is stabilizing."
"computational operations can be classified into the categories of data object, behavior, and resource modeling and manipulations. based on this view, programs are perceived as a coordination of the data objects and behaviors in computing. data object modeling is a process to creatively extract and abstractly represent a real-world problem by data models based on the constraints of given computing resources."
"camera network localization john kassebaum, nirupama bulusu, member, ieee, and wu-chi feng, member, ieee abstract-for distributed smart camera networks to perform vision-based tasks such as subject recognition and tracking, every camera's position and orientation relative to a single 3-d coordinate frame must be accurately determined. in this paper, we present a new camera network localization solution that requires successively showing a 3-d feature point-rich target to all cameras, then using the known geometry of a 3-d target, cameras estimate and decompose projection matrices to compute their position and orientation relative to the coordinatization of the 3-d target's feature points. as each 3-d target position establishes a distinct coordinate frame, cameras that view more than one 3-d target position compute translations and rotations relating different positions' coordinate frames and share the transform data with neighbors to facilitate realignment of all cameras to a single coordinate frame. compared to other localization solutions that use opportunistically found visual data, our solution is more suitable to battery-powered, processing-constrained camera networks because it requires communication only to determine simultaneous target viewings and for passing transform data. additionally, our solution requires only pairwise view overlaps of sufficient size to see the 3-d target and detect its feature points, while also giving camera positions in meaningful units. we evaluate our algorithm in both real and simulated smart camera networks. in the real network, position error is less than 1 00 when the 3-d target's feature points fill only 2.9% of the frame area."
"vision-based localization has been well studied. many solutions opportunistically search all view overlaps for robustly identifiable world features [cit] then correlated these scene points between camera views and estimate essential or fundamental matrices, which decomposed provides a camera pair's relative position and orientation-the data needed for network localization [cit] . the appeal of opportunistic visual search methods is that they require image data only. yet they have a drawback in that essential and fundamental matrix estimation only provides relative camera positions up to an unknown scale factor, one that varies for each localized camera pair. to be able to realign all camera positions to a single, network-wide coordinate frame, some solutions require triple-wise camera overlaps, implying the need for densely deployed networks, while others show all cameras an led-lit rod of known length, providing the means to establish consistent scale [cit] ."
"like rod-based solutions, our 3-d target-based solution reduces the cost of feature point detection, eliminates the unknown scale factor problem, and reduces the number of required overlaps. however, our solution also eliminates the need to find world feature point correlations in shared camera views, and reduces the amount of required view overlap to only that needed for each camera to identify the target's feature points (which varies based upon target size, camera resolution, and lens focal length)."
"the accuracy of the camera positions and orientations provided by our solution depends solely upon the accuracy of the estimation of the camera's projection matrix-which in turn depends upon the accuracy of the detection of the pixel coordinates of the target's feature points. it also depends upon the accuracy of estimates of the camera's intrinsic and lens distortion parameters. the many experiments we have performed strongly suggest the need for improvement in the latter, and so this will be a focus of our future work."
"in the generate phase, the decoder predicts the next target word. we first combine the previous hidden state s i−1, the previous word y i−1 and the current context vector c i into a vectort i :"
"the graphs in fig. 10(a) show the position, coordinate, and orientation errors from the simulated network localization at four different maximum random detection noise levels. as can be expected, as noise decreases, accuracy increases. the sudden growth of position error when the target is smallest and noise is the highest occurs because randomly introduced pixel noise is essentially a scattering effect on the closely spaced pixel coordinates of the detected feature points. the graphs in fig. 10 show the individual coordinate and orientation angle errors in the simulated reproduction at a maximum random noise displacement of two pixels."
"our localization solution expands the advantage of the led-lit rod with a simple, 3-d feature point-rich target of known geometry. a 3-d target provides in one frame all feature points needed by one camera to determine its position and orientation relative to the target. fig. 1 shows a 3-d target we used to localize a small network. it has 288 feature points, more than necessary for accurate localization, and includes colored areas to facilitate detection and correlation of the feature points projected to an image."
"(2) finally, in the update phase, the decoder computes the next recurrent hidden state s i from the context c i, the generated word y i and the previous hidden state s i−1 . as with the encoder we use gated recurrent units (gru). table 1 summarizes this three-step procedure. we observed that it is important to have update to follow generate. otherwise, the next step's look would not be able to resolve the uncertainty embedded in the previous hidden state about the previously generated word."
"in fig. 11 we show the average position errors of 25 localizations of a large simulated network that required 20 linear camera realignments to bring the final camera into the global coordinate frame. all simulated cameras have wide aspect lenses and a maximum possible lens distortion displacement of 25 pixels, though the cameras averaged eight. we performed the localization at two different noise levels and with two different target positions: one in which the target appears in the bottom left or right corners of the frame, and second in which the target appears halfway between the center and left edge of frame or halfway between the center and right edge of frame. despite the fact that the target has a larger appearance size when seen in the corner positions, the localization is more accurate at the smaller appearance size closer to the frame center. this is due to estimated lens distortion model being less accurate for points near the edge of frame."
"using the simulator, we recreated the actual five camera network configuration. to closely approximate the lens distortion of each actual camera, we chose a set of 14 lens distortion parameters that produce the same two radial distortion coefficients in the separate intrinsic and lens distortion parameter estimation. table i compares the estimated actual and simulated parameters of the five cameras."
"using the 3-d coordinates of the target's feature points and their corresponding 2-d pixel coordinates in an image taken by a camera, we estimate the camera's projection matrix using the well-known dlt method [cit] . solving for requires a minimum of six 3-d and pixel point pairs in general position, as discussed in section iv."
"in section ii, we present an overview of extant smart camera network localization solutions. section iii presents our camera model and a brief overview of projection matrix estimation and decomposition. sections iv and v discuss design requirements for 3-d targets. section vi presents localization results for both actual and simulated camera networks."
"projects a 3-d point in the by scaling it to the image plane and translating it to pixel coordinates via the principal point. we treat the projected point as a homogenous point in order to get its 2-d pixel coordinates in the plane in our localization solution, target feature points are expressed in the 3-d target's coordinate frame, as shown in fig. 1 . before hitting target feature points with, they must be realigned via a rotation and translation to the, as shown in fig. 2(b) . we rewrite this realignment using 3-d homogenous coordinates (3) we can now write a camera's projection matrix as (4)"
"although a more complex scoring function can potentially learn more non-trivial alignments, we observed that this single-hidden-layer function is enough for most of the language pairs we considered."
"several researchers have proposed distributed smart camera network localization solutions that extract pixel coordinates of local features to estimate either essential or fundamental matrices between every pair of cameras with an overlapping view. these matrices express the epipolar geometry between two cameras in stereo configuration. the strength of epipolar geometrybased solutions is that they require only the network's available visual data; knowledge of the geometry of the 3-d world is not required. still, these solutions require solving the point correspondence problem, which means detecting and correlating in images from both cameras the pixel coordinates of a minimum of eight commonly viewed world features."
one advantage 3-d target-based localization has over opportunistic image search methods is smaller required view overlaps between cameras. this is because our solution requires view overlaps only large enough for both cameras to see and detect the 3-d target simultaneously. therefore we are interested in localization accuracy in relation to target size in frame.
"when a smart camera images the 3-d target and detects its feature points, it uses the well known dlt method [cit] to estimate a projection matrix. the camera then decomposes the projection matrix to get its position and orientation relative to the coordinatization of the target's 3-d feature points. a 3-d target is required for projection matrix estimation from a single frame. 2-d planar targets, like the checkerboard patterns used to estimate a camera's intrinsic parameters [cit], do not satisfy the general position requirement, as explained in section iv."
"in 2, the authors have proposed a new clustering algorithm that considers both node position and node mobility in vehicular ad hoc environments. the proposed algorithm intends to create stable clusters by reducing reclustering overhead, prolonging cluster lifetime, and shortening the average distance between chs and their cluster members. most important, this algorithm supports single and multiple chs. simulation results show the superiority of the clustering algorithm over the other three well-known algorithms. shortening average distance between chs and their cluster members could generate collisions. our contribution is to constraint this average distance."
"3-d target-based localization still requires the network to have a connected vision graph. it also requires moving the 3-d target through the network so it appears in each shared view overlap. but 3-d target-based localization only requires pairwise view overlaps because, despite multiple pairwise localizations, the target always defines the coordinate frame and so provides consistent scale. also, realignment of cameras to a single coordinate frame is straightforward because cameras that localize to two target positions (because they have two edges in the vision graph) can compute the rotation and translation between them and share this data with the other cameras it has localized with. note that our solution does not require tracking the target's movement."
"3-d target design considerations also include efficient feature point detection and correlation. on our six grid 3-d target, the green ball and colored areas bounding the left and top sides of each grid serve this purpose. the ball appears as a circle from any viewpoint which is an initial indication that the target appears in frame and is a starting point for finding the rest of the target. scanning along radii of the green circle we find the top row's middle (blue) grid to learn the orientation of the target which is not required to be upright. from the top middle grid we scan left and right (target orientation) to find the colored areas of the other two top row grids, then down to find the bottom row's colored areas. finally, we find edge fits to the grid-side borders of the colored areas which define a scan area containing the grid's feature points. we can now either use scan lines over the area to find edge fits to the rows and columns of white squares, or use a corner detection algorithm such as that provided in opencv. these steps to detecting the 3-d target are shown in fig. 5 ."
"in the case of german (de) source, we performed compound splitting [cit], as implemented in the moses toolkit [cit] . for finnish (fi), we used morfessor 2.0 for morpheme segmentation [cit] by using the default parameters."
"istributed smart camera networks consist of multiple cameras whose visual data is collectively processed to perform a task. the area viewed by distributed smart camera networks can be small, covering an object for reconstruction, or covering a room perhaps in a health care facility, or covering a very large area, like an office building, airport, or outdoor environment for documentation, surveillance, or security."
a novel user-oriented fuzzy logic-based k-hop distributed clustering scheme for vanets that takes into consideration the vehicle passenger preferences is proposed in 5 . the novelty element introduced is the employment of fuzzy logic as a prominent player in the clustering scheme.
"which we call alignment weights. the context vector c i is computed as a weighted sum of the annotations (h 1, ..., h tx ) according to the alignment weights:"
"in 6, the authors propose a distributed clustering algorithm which forms stable clusters based on force directed algorithms. they propose a mobility metric based on forces applied between nodes according to their current and their future position and their relative mobility. the force applied between the vehicles reflects the ratio of divergence or convergence among them."
"due to obstructions in the deployment area, such as walls or uneven terrain, hand-measuring camera positions and orientations is prone to error. gps is not accurate enough for vision-based tasks, nor can it provide camera orientation. it is possible to use a network's available visual data to accurately localize the network, but these techniques impose a deployment constraint: the network's vision graph-in which vertices are cameras and edges indicate a view overlap-must be connected. a connected vision graph implies that each camera's view overlaps one other camera's, but also that some cameras in the network have separate view overlaps with two or more cameras, i.e., have two edges in the vision graph."
"multi-hop clustering algorithms proposed for vanet 1 use the changes in the packet delivery delay to calculate the relative mobility between vehicles in multi-hop distance. however, calculating packet delivery delay requires very accurate synchronization among the vehicles, which is not feasible for such dynamic networks."
"localization of a smart camera network means to determine all camera positions and orientations relative to a single 3-d coordinate frame. once localized, distributed smart camera networks can track subjects moving through the network by deter-mining trajectories and triggering cameras that will next view the subject. if the localization method provides camera positions in meaningful units such as feet or meters, the network can measure sizes of subjects and objects, facilitating recognition and behavior interpretation."
"in all results, we report position error as the euclidean distance between the actual and estimated camera positions, and directly measure the difference between actual and estimated camera orientation angles."
"we test our 3-d target-based localization algorithm on a network of five smart cameras running the panoptes [cit] smart camera platform. nodes consist of cots webcams at 640 480 pixel resolution tied to crossbow stargates with 400 mhz intel pxa255 processors running linux. to facilitate measuring ground truth of camera positions and orientations, we disassembled the webcams and mounted their board cameras atop stands at various heights and angles, then positioned the camera stands on a flat, measured surface with our 3-d target, as shown in fig. 7 . due to the small size of the network, we used the linear realignment approach described in section v to maximize the number of realignments."
"cameras project the 3-d euclidean world to 2-d images, dictated by the camera's perspective point, as shown in fig. 2(a) . we adopt the standard camera model in which the camera's perspective point is the origin of a camera coordinate frame and the image plane is parallel to the 's -plane. the mapping of a 3-d point in the to the image plane is performed by the camera's five intrinsic parameters contained in calibration matrix (1) where and are the lens focal length in pixels and differ if the horizontal and vertical spacing of pixels on the imager differs. skew factor accounts for warping in images of other images [cit] . principal point are the pixel coordinates of the intersection of the optical axis with the image plane."
"in 4, the authors present a beacon-based clustering algorithm aimed at prolonging the cluster lifetime in vanets. they use a new aggregate local mobility criterion to decide upon cluster re-organisation. the scheme incorporates a contention method to avoid triggering frequent re-organisations when two chs encounter each other for a short period of time. however, nodes that have lost their cluster-head due to merging or mobility and cannot find nearby clusters to join, they will all become chs almost at the same time. these nodes will take a period where they will decide as to who will be the new cluster-head."
"in our submission, we used recurrent neural network language model (rnnlm). more specifically, let s lm i be the hidden state of a pretrained rnnlm and s tm i be that of a pre-trained rnnsearch at time i. the controller network is defined as"
"during the look phase, the network determines which parts of the source sentence are most relevant. given the previous hidden state s i−1 of the decoder recurrent neural network (rnn), each annotation h j is assigned a score e ij :"
"we presented a new 3-d target-based localization solution for smart camera networks that is a practical alternative to existing opportunistic image search-based localization solutions. generally, opportunistic image search-based solutions are most useful for deployments where large view overlaps are desired, such as in stereoscopic vision systems. our solution, though, requires only small, pairwise view overlaps, making it more suitable for larger networks deployed for human or environmental monitoring. also, because the 3-d target provides all feature fig. 11 . position errors in the localization of a large simulated network at two different maximum noise levels and with the target appearing either only in the bottom left and right corners of the frame, where lens distortion is the greatest, or just to the left or right of the center of frame. fig. 12 . localization errors for a simulated camera when using fewer target feature points for localization. the top and bottom graphs show the results at a maximum random detection noise of 0.5 pixels and 1.0 pixels, respectively. points needed for camera localization, the reduction in computation and communication costs compared to existing solutions makes our algorithm more suitable for battery-powered, processing-constrained smart camera platforms. our solution also gives camera positions in meaningful units, facilitating visual metrology."
"in recent years, vehicular ad-hoc networks (vanets) have presented an important field of research because they create many new applications such as dissemination of safety and traffic condition messages and control of vehicle flow formations. the development and improvement of vehicular safety technologies can significantly reduce the accident rate and improve road safety. in a typical vanet, there are two types of wireless nodes, mobile units and roadside units 1 . mobile units can be any kind of vehicles which are equipped with wireless antennas, such as buses, cars, trucks, etc. roadside units are some fixed wireless nodes installed on the roadside, and these units can provide wireless connections to the internet for mobile units 1 . usually, roadside units are access points which are provided by internet service providers (isps). as a result, there are two kinds of wireless communications in vanets, vehicle to vehicle (v2v) communications and vehicle to infrastructure (v2i) communications. using v2v communications, vehicles can communicate with other vehicles in ad-hoc mode; and vehicles can transmit emergency messages in this mode. conversely, using v2i communications, vehicles can access the internet and communicate with correspondent nodes through the internet 1,18 . clustering is a technique to group nodes into several clusters. each node in the cluster structure plays one of three roles: cluster head (ch), cluster gateway (cg), and cluster member (cm) 2 . the ch in a cluster plays the roles as coordinator and backbone. it is in charge of all the communications inside a cluster, managing medium access and allocating the resource to cluster members 3 . a cg is a border node of a cluster that can communicate nodes belonging to different clusters 2 . the clustering scheme has been well investigated in wireless ad-hoc networks in recent years. however, considering the characteristic of vanets, such as high speed, sufficient energy and etc., the clustering schemes proposed for conventional wireless ad-hoc networks are not suitable for vanets 1 . therefore, clustering schemes for vanets should be designed specifically 1, 18 . it is inevitable that the highly dynamic topology of vanets will disturb cluster formation and re-organisation, increasing the cluster instability 4, 16, 17 .for example, vehicles change their speeds and lanes rapidly and frequently 3 . the high-mobility nature of vanets will bring huge computation overheads to vehicles when they use highestdegree algorithm to elect the ch. therefore, a clustering algorithm must strive to maintain cluster stability and retain the cluster contents and structure for as long as possible, as otherwise, the frequent re-clustering processes will degrade the performance of communication 4 . the size of a cluster should be well controlled. in the case of a small cluster, the vehicle chosen as ch can rapidly leave the confines of the cluster, causing a recurrent algorithm of choosing new ch. in the case of a large cluster the vehicle chosen as ch stay there longer even though there might be another vehicle which could be more efficient as a ch. therefore, it is of great importance that the cluster size should neither be too large nor too small."
"as we move the 3-d target to a new view overlap, we lose its position and orientation relative to previous target placements. yet recall that a connected vision graph means some cameras have separate view overlaps with two or more other cameras. these cameras will localize to two different 3-d target fig. 6 . realigning a camera to a target placement that it did not see. camera 1 can only see the left target placement on the left and camera 3 can only see the right target placement. camera 2 sees both. (a) cameras 1 and 2 localize to the left 3-d target placement, then cameras 2 and 3 localize to the right 3-d target placement. (b) camera 2 uses its position and orientation relative to both target placements to compute the rotation and translation between the two target placement coordinate frames, and passes the transform data to camera 3. (c) using the data from camera 2, camera 3 transforms its position and orientation relative to the right target placement to be relative to the left target placement, which it could not directly localize to."
"lastly, we explore how many feature points are required for accurate single camera localization. we present results for single camera localizations with the 3-d target design shown in fig. 1 but with only 32, 24, and 16 feature points used per grid to estimate the projection matrix. note that fewer feature points on the target implies a physically smaller target, further decreasing the required overlap between cameras."
"to explore the effects of target appearance size, we performed the actual testbed localization three times, each with the 3-d target set further from the cameras. in the third localization, the target's feature points occupy less than 3% of any camera's frame area. the localization results are shown in fig. 9 . fig. 8 shows a sample of the feature point detection accuracy for each target distance. the graphs indicate that the appearance size of the 3-d target has no bearing on the localization accuracy because the position, orientation angle, and coordinate errors are all almost unchanged across the three localizations. we presume, without clear validation, that the growth of -axis orientation error compared to the nearly flat -and -axis orientation errors, and similar growth of the coordinate error compared to the and coordinate errors, may be attributable to a combination of flaws in our ground truth measurements and an incomplete lens distortion model. in the configuration of our testbed with the target's -axis pointing towards the cameras, flaws in the flatness of the table, the verticality of the camera stands, and the measurement of the tilt of the cameras may manifest as -coordinate and -axis errors. similarly, tangential distortion can be incorrectly modeled as a translation and tilt of the camera [cit] . these singular parameter errors do not appear in accurately positioned virtual cameras with well-modeled lens distortion curves, as shown in the next subsection."
"we then transformt i into a hidden state m i with an arbitrary feedforward network. in our submission, we apply the maxout non-linearity [cit] ) tot i, followed by an affine transformation. for a target vocabulary v, the probability of word y i is then"
"automated methods to localize sensor networks require the means to gather range information between nodes, or between nodes and mutually observed targets. noncamera-equipped networks consisting of resource-constrained scalar sensors can measure range information from ultrasound, radio, or acoustic signals [cit] . smart camera networks can infer range information from visual data. the visual data is gathered, in general, either by tracking the motion of subjects viewed by multiple cameras, or by finding the pixel coordinates of robustly identifiable features that either happen to appear or are deliberately placed in camera views."
"we also stated that errors in estimates of camera positions and orientations from single camera localizations are propagated in realignment data passed to neighboring cameras. the size of these errors depends upon the accuracy of the detection of the 3-d target's feature points. they also depend upon the accuracy of the estimated intrinsic parameters and lens distortion model used to constrain projection matrix decomposition and refine extrinsic parameters. because estimated camera models are more accurate near the center of the frame where the noise from lens distortion is least, we explore the propagation of localization error in a large virtual camera network with the 3-d target placed both near the center of frame and in the corner, while also varying the amount of feature point detection noise."
"results for single systems and primary ensemble submissions are presented in table 2 . 5 when translating from english to another language, neural machine translation works particularly well, achieving the best bleu-c scores among all the constrained systems. on the other hand, nmt is generally competitive even in the case of translating to english, but it not yet as good as well as the best smt systems according to bleu. if we rather rely on human judgement instead of automated metrics, the nmt systems still perform quite well over many language pairs, although they are in some instances surpassed by other statistical systems that have slightly lower bleu scores."
"any target design, though, must provide six or more feature points in general position for projection matrix estimation. the general position requirement applies to both the 3-d space of the target's feature points and the 2-d space of the image plane. the requirement exists because the dlt method solves (8) which is a linearization of the elements of with coefficient matrix comprised of coordinate values from point correspondences. must have rank eleven for (8) to be solvable, yet points not in general position produce linearly dependent rows. more precisely, three or more collinear target feature points provide only five linearly independent rows for, while four or more coplanar target feature points (no three collinear) provide only eight linearly independent rows. thus, a target designed for projection matrix estimation must have a minimum of either two noncoplanar sets of three noncollinear points, or four coplanar points with no three collinear and two other points noncoplanar with the first four."
"mathematical notations capital letters are used for matrices, and lower-case letters for vectors and scalars. x and y are used for a word in source and target sentences, respectively. we boldface them into x, y andŷ to denote their continuous-space representation (word embeddings)."
"checkerboard patterns and other planar targets commonly used for intrinsic parameter estimation obviously violate the general position requirement. these regular patterns, though, facilitate feature point detection and correlation with the target's known geometry. patterns also increase robustness to detection noise because collinear/coplanar feature points provide an oversampling of the constraints they represent. fig. 3 . single camera localization error for a virtual 5 mm camera with moderate barrel distortion localized to 3-d targets comprised of two long-edge adjacent grids. detection noise is simulated at 0.5 max pixel displacement. fig. 4 . single camera localization error for a virtual 5 mm camera with moderate barrel distortion localized to the target shown in fig. 1 using 2, 3, 4, 5, and 6 grids. detection noise is simulated at 0.5 max pixel displacement."
"lastly, in fig. 12 we show results of virtual single camera localization to our 3-d target design when using only either 16, 24, or 32 of the available 48 feature points per grid at two different maximum noise levels and when the 3-d target's feature points occupy 2.5% of the frame area. as expected, at the higher noise level there is greater localization error. yet the use of fewer points has almost no impact on accuracy. these results indicate that our 3-d target could be reduced to having only 16 feature points per grid."
"in our contrastive submissions for cs↔en and de↔en where we re-ranked 20-best lists with a 5-gram language model, bleu scores went up modestly by 0.1 to 0.5 bleu, but interestingly translation error rate (ter) always worsened. one possible drawback about the manner we integrated language models here is the lack of translation models in the reverse direction, meaning we do not implicitely leverage the bayes' rule as most other translation systems do."
"we can organize the realignment of cameras to a single, global coordinate frame in different ways. a linear approach chooses the first target placement as the global coordinate frame, then moves the target into the view overlap of an already globally localized camera and one of its unlocalized neighbors. both cameras localize to the new target placement and the neighbor is immediately realigned to the global coordinate frame. while straightforward to implement and requiring the least amount of communication, the linear approach is not ideal because single camera localization errors are propagated in passed transform data. a better approach minimizes error propagation by choosing for the origin of the global coordinate frame the 3-d target placement that results in the fewest number of realignments. we do this by finding a maximum spanning tree of the connected vision graph, finding the longest path in the spanning tree, and choosing for the global coordinate frame the middle 3-d target placement on the longest path. because the larger the 3-d target appears in a camera's frame typically means the more accurate the localization, we use 3-d target frame appearance size as the edge weight to determine the maximum spanning tree."
"in addition to the rnnsearch model, we train a separate language model (lm) with a large monolingual corpus. then, the trained lm is plugged into the decoder of the trained rnnsearch with an additional controller network which modulates the contributions from the rnnsearch and lm. the controller network takes as input the hidden state of the lm, and optionally rnnsearch's hidden state, and outputs a scalar value in the range [cit] . this value is multiplied to the lm's hidden state, controlling the amount of information coming from the lm. the combined model, the rnnsearch, the lm and the controller network, is jointly tuned as the final translation model for a low-resource pair."
"placements, one visible in each view overlap. from their positions and orientations in the separate target placement coordinate frames, these cameras can compute the transform relating the placements, as shown in fig. 6(a) . then passing the six transform parameters (3 rotation, 3 translation) to neighbors lets the neighbors realign their positions and orientations to be relative to a target placement they did not directly localize to, as shown in fig. 6(b) and (c) ."
"due to widely varying environmental and lighting conditions in deployments of smart camera networks, as well as variations in camera quality, lens focal length, and distance between cameras, it is unlikely that any one 3-d target design will be suitable for all possible networks. rather, the deployment conditions should dictate the size and type of target used."
"there are also other physical aspects of our solution to be explored, such as spatial resolution of the cameras and other possible target designs, such as spherical. we also plan to explore possible error refinement methods, such as performing bundle adjustments on pairwise localizations, as well as over the entire network configuration estimation."
"the determination of the pixel coordinates of projected 3-d target points is subject to noise from both detection inaccuracies and lens distortion. we perform iterative refinement of the extrinsic parameters using the levenberg-marquadt optimization. the cost function we minimize is (7) where is the number of point correspondences used in the estimation of, and are the pixel and target coordinates of each correspondence, and is the projection using the refined extrinsic parameters with fixed intrinsic parameters and two coefficients of radial lens distortion estimated separately using zhang's camera calibration algorithm [cit] ."
"when a camera sees and localizes to the 3-d target, it computes its position and orientation relative to the target's coordinate frame, which is defined by the coordinatization of the target's feature points. if all smart cameras in the network are oriented such that all can see and localize to the 3-d target simultaneously, the network can be localized with one target placement. more likely, though, all cameras do not all view the same area and localizing the network requires placing the 3-d target in several separate view overlaps."
"the analyser consists in exercises developed in order to determine the behaviour of an individual compared to another through the measurement of the student's abilities, such as reading, observation, among others skills, depending on the nature that is evaluated, see fig. 4 ."
"it is in the area of distance education where there have been major advances in the development of digital content more accessible and understandable to the student, among which is the use of various technologies, including online collaboration. the use of applications for the exchange of information, such as instant messaging or chat, email and social networks has produced an incalculable number of opportunities to ensure that education can be accessible from anywhere in the world. because of this, there is a gateway to information in which it is no longer necessary to be physically present in order to obtain large collections of data relevant to something someone wants to know."
"because students need activities to stimulate their interest in a certain topic and engage them in the learning process, it is necessary that these activities are according to the nature of behaviour they use to learn. many students like to read, but what happens to those who are not well suited for this activity. it is necessary to carry out activities in a distance education environment that can cover a wide spectrum based on the different learning styles, each student learns at their own pace and form, so everyone deserves that the information is presented in a form adequate to their needs. we've based on felder-silverman model as a tool for teaching courses in science, is currently used worldwide for issues as diverse as language learning and biology, see"
"the present study also showed a better adaptation at anterior margin than at middle section and posterior margin, and no significant difference in adaptation was found between middle section and posterior margin of the major connectors fabricated from both the intraoral and extraoral digital impressions. it indicated the results might be related with the technical characteristics of 3d printing. more errors could be found at those parts of the denture with steep slope and great curvature when manufactured by selective laser melting (slm) technique 28 . the 3d printing technique used in this present study, polyjet, could create models which was dimensionally similar to the virtual 3d stl image and showed adequate details with uniformly smooth surface 19, 21, 29 . but few researches have focused on its printing accuracy of objects with long span and large curvature. in preliminary experiments, the major connectors were printed from the top of the palatal plate to the occlusal rests, but there occurred to be deformations on resin frameworks so that a part of occlusal rests could not be fully seated on volunteers' teeth. thus, improvements were performed in the later experiments: adding supporting rods at the parts of large curvature on major connectors and applying a new policy that the major connectors were printed from the occlusal rests to the top of the palatal plate. the changes had solved the problem raised in preliminary experiments, and all the major connectors fabricated from digital impressions could achieve maximal stability and good seating on volunteers' upper jaws."
"dental cad/cam techniques also include another two major parts: designing by software and manufacturing by computer-controlled machines. the latest software such as cerec 4.4 of sirona and dental system of 3 shape have specific modes of restorations, implantology or orthodontics to help dentists and technicians to make treatment plan and design dental prostheses with great convenience. polyjet, a kind of three-dimensional (3d) printing technique specialized by stratasys corporation, jets and instantly uv-cures tiny drops of liquid photopolymer layer by layer on the tray to create 3d models with no post-curing needed 18 . it creates smooth and accurate models with microscopic layer resolution down to 0.1 mm 18, 19, which is able to reproduce more accurate anatomic details than other common rp techniques 20, 21 . to date, available studies on rpd frameworks manufactured by intraoral digital impression combined with cad/cam techniques are still limited 17, 22 . this in vivo study was designed as a self-controlled experiment. the objective was to compare the difference in intraoral adaptation between the 3d-printed major-connectors of rpds derived from intraoral and extraoral digital impressions. the null hypothesis was that: (1) it was feasible to manufacture the major connectors of rpds by intraoral digital impressions and 3d printing techniques; (2) there was no significant difference in adaptation measurement between the major-connectors fabricated from the two different digital impression data."
"obviously, there is a flexibility diversity of mucosae at different areas of palate. thicker mucosae are at the palatal rugae and the two sides of the palatal vault with better flexibility, while thinner mucosae are at the center of palate, also namely hard areas, with worse flexibility. vinyl polysiloxane materials are pressed onto jaws during the process of making conventional impressions, and oral mucosae at the two sides of the palatal vault will be oppressed more than the mucosa at hard areas. so, when further analyzing the difference in adaptation of cad/ cam major connectors at different areas, it was found that the adaptation of major connectors fabricated from extraoral digital impressions were better at the two sides of the palatal vault than at the midline palatine suture. on the other hand, intraoral digital scan has no direct touch with the oral structure and it captures both the hard and soft tissues in a static state. in the present in vivo study, the adaptation of major connectors fabricated from intraoral digital impressions were found better at the midline palatine suture than at the two sides of the palatal vault. this phenomenon could be more likely attributed to the pressure of the impression materials between the major connectors and the resilient soft tissues 26, 27, so the gap might be widened at the two sides of the palatal vault."
"relatively slow development of cad/cam rpds was also because of the higher demands with cad/cam equipments and techniques in order to meet the clinical requirements for large-sized oral prostheses fabrication. dental system cad software has specific modes for rpd designing which can simulate the conventional workflow in dental laboratory, that is: import of digital model data, surveying and determining the path of insertion, blockout of undercuts automatically, placing the retention grids, drawing the region for major connector, placing the minor connector and retainer (e.g., clasps and occlusal rests), sculpting (e.g., smoothing the surface of wax pattern and increasing or reducing the thickness of wax pattern), adding finishing line, placing the accessories (e.g., retentive posts, sprues and supporting rods), selecting the surface decorative figure of major connector, and export of designed frameworks data. the cad software is able to provide much convenience for dentists and technicians to finish the designing of rpd frameworks as well as increase their work efficiency, however, it still exists some deficiencies at present such as lack of relief at specific areas and beading line at posterior margin of major connectors. during the conventional designing process of major connectors, relief should be placed at such areas as incisive papilla, maxillary hard areas and torus mandibularis to prevent pain of pressure then mucous injury under the function of occlusal force. additionally, a 1mm-wide and 0.5 to 1mm-deep groove should be carved at the position corresponding to posterior margin of major connectors on plaster cast, forming a beading line to seal the boundary and reduce the foreign body sensation 14 . thus, cad software for rpds needs to be improved and equipped with relevant function to adapt to the clinical demands better. further development can also be based on the establishment of the 3d database for maxillary and mandibular soft tissues. when designing a rpd framework, the software can forecast the different flexibility extent of soft tissues at different regions and reduce the thickness of different soft tissues along with the normal direction in an automatic way, simulating the effect of pressure impressions."
"once the student has taken the test g-oal analyser, he/she can access to g-oals recognizer. the recognizer will obtain the learning styles preferences from the student's database and display only those educational materials whose representation matches the student's preferred style, see fig 7."
"each learning style corresponds to a teaching style, so this research proposes the following types of activities according to the combination of student learning styles, called dimensions. such dimensions are:"
"the present study applied the biocompatible resin material to print major connectors for the research purpose. in clinic, there are two main methods to obtain cam rpd frameworks: one is to print sacrificial patterns in wax or resin materials firstly and then cast to metal frameworks; the other is to directly manufacture metal frameworks by slm technique. direct manufacture is attempted with the aim of eliminating the time and material-consuming investment-casting process 16, but is still of high production cost as well as propose demanding requirements for the property of printing machines and printing materials. all make the application of slm technique in rpds fabrication is not as wide as that in the fields of single crowns and fdps."
it was also suggested to broaden the methodology to cover a wider range of courses and perhaps create an online community among professors to share learning activities through repositories.
"with 3d printing method. the results that the gaps under the major connectors fabricated from intraoral digital impressions were wider than those from extraoral digital impressions, which indicated that the conventional pressure impression was conducive to obtain better overall adaptation between major connectors and palatal soft tissues."
"the student must not adapt to the learning activity, the learning activity must adapt to the student. this adaptation implies the design of learning activities with the same learning objective but with different representation, according to the student's preferred style."
"the same principle applies to the other three dimensions (see fig. 6 ), all information is stored in a database to be used later use by the learning styles recognizer. fig. 6 . learning styles preferences of a student"
"with 44 exercises (see fig. 5 ), including verbal critical reasoning, diagrammatic abstract reasoning, verbal comprehension and numerical computation problems, g-oals determines the student's learning style preference using the following formula:"
"the vast majority of students who are part of a distance learning process suffer from a loss of interest in certain moments; these moments are marked by the continuous repetition of similar activities that become monotony. another factor that favours the loss of interest is that in many cases such activities are not attractive to the student; also, because each student has a different learning style, it is necessary to recognize and provide sufficient approach to understand the topics to be addressed in the course so that it suits everyone. for this it is important to establish what teaching style meets the learning style of the student in question and know the educational method with which to try to approach him."
"participants. the december 23, 2014 . the study was conducted in accordance with the declaration of helsinki. volunteers who gave informed consent and met the following criteria were enrolled into the study."
"the present study used a self-control method to compare the difference between adaptation of the major connectors fabricated from intraoral digital impressions and extraoral digital impressions respectively. the null hypothesis was partly rejected. it was feasible to manufacture the major connectors of rpds by digital impressions and 3d printing techniques. but there was a significant difference in adaptation measurement between the major-connectors fabricated from the two different digital impressions. it was found that there was space for a layer of light-body impression material under every major connector seated in mouth. the overall adaptation ranged from 159.87 μm to 577.99 μm of major connectors fabricated from intraoral digital impressions and 120.83 μm to 536.17 μm of major connectors fabricated from extraoral digital impressions. in one early study regard to conventional cast maxillary rpds, the adaptation of the metal palatal plate ranged from 0.11 to 0.93 mm in mouth and from 0.09 mm to 0.68 mm for on the cast 26 . diwan r. reported the mean adaptation was 0.64 ± 0.07 mm for the modified palatal plate and 0.56 ± 0.04 mm for the palatal strap on cast models, respectively 27 . there were inevitable gaps under major connectors for the reason of distortions of the wax patterns, dimensional changes in the refractory casts, contraction of the alloys and manmade errors of the technicians during the cast procedures of conventional rpd frameworks. the adaptation of major connectors found in the present study lay in the range of the results of the previous studies 26, 27, suggesting that the digital impressions of whole upper jaw could basically meet the clinical requirements for the adaptation of rpds, and it was feasible to fabricate the major connectors of rpds using intraoral or extraoral digital impression combined table 2 . adaptation of the major connectors at the midline palatine suture and the two sides of the palatal vault (unit: μm)."
". many researches have proven that single crowns and fdps manufactured from intraoral scanning data can deliver comparative marginal and internal fit with those fabricated from conventional impressions [cit] . comparing much progress of cad/cam techniques for teeth-supported restorations with digital impressions, development for removable partial dentures (rpds) is relatively slow. it is more complicated because rpds frameworks may not only cover a wider range of dentitions but also soft tissues that covered by major connectors. major connectors, one of the important components of removable partial dentures, are able to connect the parts on both sides into a whole integrally, delivery and distribute occlusal force to abutment teeth or adjacent supporting tissues, and meanwhile enhance the strength of the removable partial dentures 14 . as touched with soft tissues directly, whether the adaptation of major connectors is good or not will influence wearing comfort and the health of oral mucosa. a previous study reported that it was feasible to use the intraoral scanner to obtain digital impressions for whole upper jaws, including full dentitions and palatal soft tissues from the intraoral or extraoral digital impression datasets showed good adaptation as the results obtained by traditional casting method 16, 17, which supported that digital impressions and cad/cam techniques could be an alternative choice for the conventional fabrication of rpd frameworks. but few studies are available regarding the quantitative analysis on the adaptation of cad/cam rpds to guide the clinical application."
"it is feasible to manufacture the major connectors of rpd by digital impression and 3d printing technique. digital impressions of whole upper jaws can basically meet the requirements for rpds fabrication in clinic, and both the adaptation of major connectors derived from the two kinds of digital impressions are clinically acceptable. though the adaptation of major connectors derived from intraoral digital impressions is worse than that from extraoral digital impressions, intraoral digital impression has brought a new thought in the fabrication of rpds. further improvements in cad softwares and cam technique will lay the foundation of their application in the fields of rpds fabrication. feedbacks from patients about wearing comfort and usage satisfaction are also needed to instruct the clinical application in the future."
"(1) fig. 5 . calculation of learning style preferences: using the formula mentioned above, a value is calculated for each exercise. by comparing the results in the same row, the lowest value wins. the number of wins is counted and the difference is assigned to the winning preference. in this example, we would say that the student has a \"level 2 intuitive preference\"."
"the inclusion criteria were (1) volunteers from shanghai jiao tong university school of medicine; (2) aged at least eighteen years; (3) good oral hygiene; (4) complete maxillary dental arch except the missing third molar; (5) intact hard and soft tissues, including treated teeth decay and healed teeth extraction socket. the exclusion criteria were (1) undergoing orthodontic treatment; (2) with metal crowns and any other metal materials on teeth; (3) advanced periodontitis affecting gingival recession; (4) obvious teeth mobility (mobility degree higher than 1); (5) obvious dentition malalignment (malalignment degree higher than 1)."
"technology transfer and information sharing from around the world have become increasingly common, most notably in the field of education, where institutions have ventured in the use of digital content to inform and train people, facilitating the use of tools that allow remote access, such as the use of computers and mobile devices among others. this has led to great interest in undertaking new ways of making use of teaching/learning methods, preferably online."
"we propose the use of an application (named g-oals analyser), which consists of a series of exercises presented in random order; each exercise is designed to evaluate the ability of a student to understand the problems at hand, see fig. 3 . one of the methods for the effective deduction of learning styles and preferences is the use of a test [cit] ."
"with g-oals analyser, the student's preferences can be known and for the first time offer a viable alternative to the presentation of information that is understandable from the point of view of what the student wants to see and how he want to see it."
"the acquired meg signal is visually observed and then processed for head movement correction and compensation is accomplished with built-in software for example max filter in elekta/neuromag system, and commercially available other software like brain electrical source analysis (besa), asa, curry, emse. the environmental interference and constant or periodic artifact correlation is performed by applying single space separation (\"sss\") or temporal single source separation (\"t-sss\"). single space separation is a new method for compensation of external interference and sensor artifacts by decomposing the meg signal into inner source signal and outer noise. independent component analysis (ica) or template can also be used for rest of the biological artifacts like eye blinks. pre-processing ( figure 1b) includes the following steps: identifying nonfunctional (defective) channels followed by correction in head movement followed by software interference suppression [signal-space projection (ssp), signal-space separation (sss), t-signal space separation (t-sss)] and later artifact identification and rejection of environmental interferences and subject interference (cardiac activity, muscular activity; [cit] ) ."
"posterior-to-anterior information flow over the cortex in higher frequency bands in hc subjects with a reversed pattern in the θ band has been reported [cit] b) . the information flow from the precuneus and the visual cortex, toward frontal and subcortical structures, was found to be decreased prominently in ad [cit] b) . meg based multiplex brain network yields to an effective structure for the integration of the frequency specific networks [cit] ."
"update velocity vx, vy by equation (7) 17: update coordinate x, y by equation (8) 18: avoid the node out of monitoring area by equation (9)"
"the manuscript is divided into seven sections. after introduction, in section the schematic for meg signal acquisition and analysis, the comprehensive flow diagram of meg data acquisition and analysis is presented. section resting state and event-related response studies describes about resting state and evoked studies followed by description of source reconstruction (section source reconstruction). the cutting-edge research of meg analysis for ad involving single channel, connectivity and network analysis is scrutinized from existing literature in section meg data analysis. section machine learning approach for meg based analysis gives a brief idea about the application of machine learning algorithms on meg data in ad population. various methods of statistical analysis performed with meg data is discussed in section statistical analysis of meg data. section discussion outlines the overall implementation and experimental outcomes of meg studies. section conclusions summarized all the generally applicable procedures and points onto the scope for future research."
"(1) due to environmental noise and signal strength attenuation with larger transmission distance, the monitoring ability of sensor nodes shows some uncertainty. p ik is used to indicate the probability of monitoring target k covered by node s i, and computed as follows:"
"from the parameter named synchronization likelihood (sl), the strength of synchronization of two time series is evaluated based on state space embedding. nonlinear forecasting (nf) and cross mutual information functions (cmif) are measures for the predictability of one time series when a second series is known, have been implemented for fc analysis. predictability based on similarity (based on the amplitude of two time series) has also been evaluated from meg data using cross approximate entropy (cr-appen)."
it has been observed that the parts of brain responsible in the activation differs from resting state to task based conditions [cit] ). brain regions that correlate in resting state are found to be involved in co-activation during tasks however their roles are likely to get shifted in evoked study [cit] .
where w is the inertia weight of velocity; c 1 and c 2 are the individual acceleration and coherent velocity scale factor respectively; r 1 (t) is a random number on [cit] at the number of rounds t;
"copyright ⓒ 2016 sersc while the smaller value of w will enhance local development capabilities. it means that the inertia weight of velocity w can balance the local and global during the optimization process, and the values of w can be a fixed value, also can dynamically change with generation number. three methods of value selection are described as follows:"
it is observed that the integrated implementation of the high (millisecond) temporal resolution of meg and eeg can give better accuracy than fmri localization in measuring neuronal dynamics within well-defined brain regions and assessing the source localizing ability for identical stimuli [cit] .
"in this section, we analyze the running time of the algorithm in the case of the algorithm with and without coherent velocity, and the results are shown in figure 6 . it is not difficult to recognize the approximate proportional relation between the rounds of iteration and the running time; and then the coherent velocity introduced into particle swarm optimization for network coverage not consumes more time but saves running time. it is because the coherent velocity improves the ability of a node to find the optimum position in local area."
"the inertia weight of velocity selection is flexible as described above. the random adjustment method is simple and effective, but ignores the influence of the number of rounds iterations, so this method reduces the speed of convergence; the linear or nonlinear decreasing method significantly improves the speed of convergence, and the nonlinear decreasing method is more considering the difference of different iterative phase, to improve the ability of the algorithm to explore a better solution."
", ci v is the coherent velocity of node i. after each movement, the coordinates of node i is need to be updated. the change of position from the number of rounds t to t+1 is the velocity, so it can be computed by following equation at the number of rounds t+1."
"millions of people are affected worldwide by ad and this number is increasing every day [cit] . ad affects not only the individual but also it has serious impact the entire family of the patient as well as on society and economy. the actual cause of ad is not known yet, however, both clinical and laboratory research have revealed oxidative stress is related to ad. it is believed from available clinical data that hippocampal as well as frontal cortex regions, the glutathione level depletion is linked to the conversion from a healthy aged subject to mci [cit] . there are other associated features with hippocampal and frontal cortex texture, which also change in ad [cit] . at present, mci or ad are detected symptomatically by the clinicians and various neuropsychological tests like clinical dementia rating (cdr) [cit], mini-mental state examination (mmse) [cit] . seven minute screen (7ms) that consisted of four individual tests (orientation, memory, clock drawing, verbal fluency) [cit], functional assessment staging (fast) [cit] and montreal cognitive assessment (moca) test [cit] are used for screening of ad."
"statistical analysis of the network coverage in the case of the algorithm with and without coherent velocity is made, and the results are shown in figure 5 . from the figure, we can see that the network coverage gradually converge after 40 rounds iteration to stabilize with best coverage 0.8179 when using the algorithm without coherent velocity. on the other hand, the network coverage gradually converges after 70 rounds iteration to stabilize with best coverage 0.91697 when using the algorithm with coherent velocity. therefore, the addition of the coherent velocity significantly avoids the network prematurely into a local optimal solution, and improves the network coverage."
"changes in fc have been found in elders with scd as well as mci patients compared to healthy individuals in an investigation (lópez [cit] ) . meg data has been collected in resting state from 39 healthy control elders, 41 elders with scd, and 51 mci patients. fc has been evaluated based on source reconstructed meg data using plv."
"definition 1: in wireless sensor networks, the velocity of sensor nodes is mutual influence and interference. it means that any change of node in direction or speed may cause all neighbors movement adjustment, and these adjustments, in turn, inhibit or encourage the change. this velocity in direction or speed is called coherent velocity [cit] ."
"moreover, at this stage of preprocessing of meg data, filtering may also be applied to segment the meg data into different band waves (δ, θ, α, β, and γ). subsequently, the exclusive sensor space time series meg data ( figure 1c ) distributed over all the brain regions, which is processed further for single channel analysis ( figure 1i ) and for connectivity estimation ( figure 1j ) is generated."
"meg is a powerful technique for the recording of changing activities of brain functions. from single channel analysis, a patterned and consistent slowing of brain oscillations has been observed in ad. fc studies revealed decreased connectivity in ad than controls. ec studies must be implemented further to conclude anything. as a whole, association of parietal and temporal areas has been reported with the advancement of ad in comparison to hc subjects. involvement of the hippocampus has also been demonstrated but more investigation is required."
"in equation (13), i v is the average velocity of local neighborhood nodes for node i with the number of n s, and it can be computed as follows:"
"in evoked studies, association of any specific brain region with the task was correlated for probable ad and hc group [cit] . the instant reduction of the signal amplitude after opening the eyes was found to be diminished in ad, but functional connectivity (fc) remained unaltered for eye opening or closing [cit] . multiple regions of brain activity are localized and also their time courses are characterized from somatosensory and visual activity task using multistart analysis of meg data [cit] ."
"the pattern changes of connectivity in ad and mci have been shown year wise from literature as compared to hc subjects in figure 3 . all the parameters (e.g., coherence, sl etc.) are listed in the vertical axis. most of the studies have accomplished fc analysis of meg data for ad and reported a decreased connectivity in all bands (δ, θ, α, β, and γ). mostly the variations were observed in α and β bands. although majority of the investigations inferred reduced connectivity than hc subjects, but variability of research outcomes are observed in different research groups. [cit] which also detected less connectivity for ad and mci."
"meg provide means to uncover ad related deviations in brain oscillations. in-depth analysis of meg application in ad can be useful to identify plausible biomarkers to detect the early stages of this disease. this section entails thorough explanation about the outcomes of single channel, connectivity, network analysis of meg data in ad and mci."
"for each individual, the optimal position marked as b i (t) is the best position for node i located from the beginning to the number of rounds t, then the individual best position of node i at the number of rounds t+1 is computed as follows [cit] :"
"meg data is useful to get insight for brain functionality and the role of large-scale network is studied by approximation of neuronal interaction at source level. to accomplish this objective, reconstruction is performed for source time series. for source reconstruction, the forward and inverse modeling of meg sensor data needs to be implemented with the help of mri data. the forward problem is specified as the calculation of the magnetic field vector which is acquired outside the head. inverse problem is involved in calculating the current density of the source that produce magnetic field vector. for source reconstruction of meg data, the most commonly applied techniques found in the literature are: equivalent current dipole (ecd) [cit], beamforming [cit], and minimum norm estimation (mne) (hämäläinen [cit] ) . minimum current estimation (mce) is also popularly applied for source estimation and localization of meg data [cit] ."
single channel analysis is based on per channel local analysis and studies have been performed for the assessment of ad and control subjects from individual time series meg data.
"the position movement of node i from the number of rounds t to t+1 is depend on a certain velocity marked as v i (t+1). it is formed by inertial velocity, acceleration and coherent velocity, and it is computed as follows:"
"in this paper, on the basis of research on spatial neighborhood, the node coverage and area coverage models are analyzed, then an optimal coverage algorithm of wireless sensor networks based on particle swarm optimization with coherent velocity is proposed. the algorithm regards the network coverage maximization as fitness function of coverage optimization, and inertial velocity, individual acceleration, coherent velocity are comprehensively considered to achieve node moving to the optimal position for optimal coverage of the monitoring area [cit] ."
"minimum-variance pseudo-unbiased reduced-rank estimation (mv-pure) framework has been proposed for better reconstruction of source activity from meg data [cit] . in addition, multilayer network analysis approach has also been applied to integrate multiple frequency bands in a single framework and results reported disruption of hub regions in ad [cit] ."
this review focuses on the meg analysis techniques and modulation of neuronal rhythms in ad brain. motivation of the review is to provide an outline for the application of meg for connectivity analysis and its application combining other neuroimaging modalities that can help for the identification of early diagnostic biomarker for ad.
"various open source software and toolboxes are available for meg data analysis. brainstorm [cit], eeglab [cit], fieldtrip [cit], mne [cit], nutmeg [cit], openmeeg [cit], spm, emegs (electromagnetic encephalography software) [cit] etc. are commonly used. commercial software packages are also available dana and curry [cit] . though these tools keep updating on a regular basis but there remain some complications associated with meg data processing and analysis."
"reports suggest that in mci and ad difference in these regions are observed as compared to healthy subjects. in case of frequency as well as fc analysis, these regions have shown slow brain activity and decreased connectivity, respectively."
"the optimal coverage algorithm of wireless sensor networks based on pso is achieved by many rounds iteration. the conditions of iteration terminate can be pre-set maximum number of rounds iteration, and also a certain network coverage, as well as no change in the coverage within a certain number of rounds iteration. the first condition is selected in this algorithm. in the each round iteration of optimization procedure, multiple steps are operated, such as the inertia weight of velocity computation, spatial neighborhood construction, local neighborhood coverage computation, best position update, coherent velocity update, velocity and position of node update etc. the algorithm 4 describes the coverage optimization procedure. compute the inertia weight of velocity by equation (10) or (11) or (12) 5:"
"the advantage of non-parametric statistical test is that it gives complete freedom of choosing the experimental conditions for comparison. this independence delivers an up-front approach to explain the multiple comparisons problem (mcp) [cit] . mcp is a commonly found problem in statistical analysis of meg. this problem initiates since megdata are multidimensional. the signal is sampled at multiple channels and multiple time points; hence meg-data becomes multidimensional. moreover, during meg data analysis the effect of interest (i.e., a difference between experimental conditions) is evaluated at an extremely large number of (channel, time)-pairs which in turn gives rise to mcp."
"pkm conceptualize the idea, literature search, and manuscript writing. ab literature search, writing, and figure preparation. mt literature search and writing manuscript. as literature search and writing."
"in the ideal condition, the uncertain monitoring radius error r e is 0, so the monitoring radius is a fixed value r. according to the equation (5), the local neighborhood coverage combined by s 1, s 2, s 3 is maximal in the case of seamless coverage topology, shown in figure 3(a) . in other words, the limit of zero for the overlapping coverage area o of three nodes s 1, s 2, s 3 leads to optimal coverage. the distance relationship between communication radius and monitoring radius is shown in figure 3 (b), and it could be computed as follows:"
"second, the node and area coverage models are provided as optimal objective function. third, a pure local coverage optimization algorithm of wireless sensor networks is detailed description using particle swarm optimization."
"when the communication radius is about 3 times of monitoring radius, the network coverage trends to achieve maximum, and the value is 0.91697 when communication radius is 18. table 3 tells us that the number of neighborhood nodes has influence to the network coverage while using the spatial neighborhood constructed by certain number neighbors. too large numbers of neighborhood nodes not only reduce network efficiency, but also further reduce the network coverage. the following experiments analyze and compare the network coverage of initialization and after 100 rounds iteration using the spatial neighborhood constructed by fixed radius while the value is 18m, and the results are shown in figure 4 . from figure 4, we can see that the network nodes of initialization leading to low network coverage are randomly distributed, and there are a lot of monitoring blind spots and overlapping areas. the distribution of nodes after 100 rounds iteration changes uniform with less monitoring blind spots and overlapping areas, so the network coverage has been greatly improved."
"the changes observed in single channel analysis of meg data using different parameters are represented in figure 2 . the increasing and decreasing measures of all the parameters, e.g., absolute power, approximate entropy etc., (listed in the vertical axis) for all the frequency bands (δ, θ, α, β, γ) have been shown in chronological order. most of the studies have reported results common for all the frequencies in ad whereas some investigations informed significant difference in specific band waves. for example, relative power (rp) was reported increased explicitly in δ band but decreased in β band. majority of the parameters implemented for single channel analysis of meg data were reported reduced in ad compared to hc subjects though in few cases, metrics were found increased. the synthesis of single channel analysis of meg studies, it is inferred that a slowing pattern of oscillations is visible in ad and mci patients in frontal, parietal, temporal, and occipital brain regions."
"from table 2, we can see that the communication radius of the nodes has influence to the network coverage while using the spatial neighborhood constructed by fixed radius."
"in this algorithm, the maximization neighborhood coverage r i of node i is regarded as the optimization goal. it is a function of the number of rounds iteration t, marked as f(t). each sensor node is seen as a particle individual, and its coordinates change with t [cit] . in order to facilitate the presentation, there does not distinguish between the dimension of node coordinates, and entirely mark the coordinates of node i at the number of rounds t as"
"in this algorithm, each node is an independent individual that records the information obtained and used for coverage optimization. these information include coordinate of x/y-axis, spatial neighborhood, local best coverage, velocity and coherent velocity etc.. the structure of node attributes is shown in table 1 ."
"meg co-ordinate system is computed before meg signal acquisition with above explained digitization process. on the other hand, mri co-ordinate system ( figure 1d ) is generated after the mri image acquisition. raw meg data ( figure 1a2 ) is acquired from the subjects in a magnetically shielded room either in resting condition (no task) or during certain tasks as per the study requirement. processing scheme of meg data are the same for resting state as well as with evoked potential conditions, however, only the execution scheme will vary. in resting state category, the subject needs to be fully awake as drowsiness may introduce noise. for event-related studies, the subject is asked to perform certain tasks in reaction to a given stimulus (e.g., visual, auditory, or somatosensory). simultaneously with the recording of meg signals, brain's electrical activity, heart's electrical activity and eye movements of the subject are captured by means of electroencephalogram (eeg), electrocardiogram (ecg), and electrooculogram (eog), respectively."
"according to the previous work, the optimal solution can be improved by varying the value from 0.9 at the beginning to 0.4 at the end for most problems."
"the meg is useful to bridge other electrophysiological measures like eeg, local field potential (lfp), fmri, pet, and brain stimulation. it opens opportunities for the cross validation of the research findings from various modalities. moreover, meg in combination with appropriate modalities can be helpful to understand the nature of changes of the neurotransmitters like gamma-aminobutyric acid (gaba), glutamate etc. [cit] . meg has the capability to open up new avenue for clinical research in ad."
"in ad, neural complexity was found lower in low frequencies [cit] . low clustering coefficient and short characteristic path length suggested a more random configuration of network in ad than hc subjects [cit] . decrease in eigen vector centrality reported the loss of a known temporal hub in ad and also fewer modules with weaker connections [cit] b,c) ."
"meg findings reported a lower fc in ad than normal supporting the concept of ad as a 'disconnection syndrome' [cit] . these observations coincide with the results of pet, eeg, or fmri studies [cit] . combining different modalities [e.g., eeg, pet, fmri, magnetic resonance spectroscopy (mrs)] with meg will be interesting [cit] . it has been reported that integrated approach of eeg and meg gives more accurate connectivity estimation than individually [cit] ."
involving various neurotransmitters with altering brain oscillations will open a new research domain for clinical application and it is an important thrust area in our laboratory.
(2) linear decreasing a linear approach for w can be used to decrease from the beginning of larger value to a smaller value [cit] :
"in memory based task, a relation between hippocampal atrophy and the degree of neurophysiological activity in the left temporal lobe has been demonstrated . a reduced meg response during the retention period of a working memory task was observed in ad and the findings were in line with mrs data [cit] . task based evoked potential analysis can also differentiate between ad and hc subjects but the clarification of changes may be difficult depending upon task performance and level of concentration of the subjects during experiment [cit] ."
"(11) where n t is the maximum number of rounds iteration executed by the algorithm, and w(0) and w(n t ) are the initial and final inertia weight of velocity, as well as w(t) is the value of inertia weight at the number of rounds t."
"(3) non-linear decreasing on the other hand, a non-linear approach for w can be adopted to decrease from the beginning of larger value to a smaller value [cit] :"
"the complex analysis procedure for meg signals consists of various stages including co-registration with mri images, forward and inverse problem as well as applicable steps for data analysis (figure 1) . subjects participating in meg study; also undergo mri for 3d anatomical images."
"optimal coverage of wireless sensor networks is one of the most fundamental problems for constructing efficient perception layer network. it not only achieves network coverage maximization, but also avoids coverage of blind spots [cit] . sensor nodes are divided into static nodes and dynamic nodes. static nodes are easy to achieve coverage optimization in the case of artificial deployment, while redundant deployment and sleep scheduling of nodes are adopted to achieve maximize coverage optimization in the case of random deployment. mobile robot or simple activity node is generally used as dynamic node to take on environmental monitoring and data gathering in a special area, so the mobility is fully utilized to expand the monitoring range, reduce blind spots, and enhance the network coverage [cit] ."
"modulation of gamma oscillations is a widely established mechanism in a variety of neurobiological processes, but its neurochemical basis is not fully known yet. in addition, research reports suggest that γ oscillation properties depend on gabaergic (gamma-aminobutyric acid) inhibition [cit] . the link between gaba concentration and gamma oscillations is a thrust area of research. a direct relationship between the density of gaba receptors and γ oscillations in human primary visual cortex (v1) has been established by an investigation by combining flumazenil-pet (to measure restinglevels of gaba receptor density) and meg (to measure visually induced gamma oscillations; [cit] ) ."
"non-parametric test, on the other hand, does not make any such assumption and can be performed even without any information regarding population or have a small population. non-parametric two-tailed mann-whitney u-test was carried out using spss software for evaluation of statistical significance of classification between ad and normal individuals [cit] ) . mann-whitney u-test has also been used with leaveout cross-validation to measure the ability of median frequency and spectral entropy to differentiate ad from hcs . kruskal-wallis test was performed for each channel pair between mci and hc [cit] . for evaluation of statistical significance wilcoxon signed rank test was performed on simulated data [cit] ."
"in table 1, we have listed some of the meg studies, to gather information regarding the efficacy of meg data analysis methodologies statistically. accuracy, sensitivity, specificity and area under the region of convergence (roc) curve (aurc) are briefed in this table. different studies have implemented different parameters for meg data analysis. it is global analysis in most of the cases and not region specific. in addition, frequency bands are also varying in the experiments. aurc values reported in literature within 0.529 and 0.912 range for different studies. from overall observation we noticed that some common parameters like mean frequency (mf), sample entropy (sampen), and lempel-ziv complexity (lzc) have exhibited accuracy ranging between approximately 77-85, 58-70, and 61-83%, respectively. although high sensitivity and specificity have been accomplished in most of the studies, but it is difficult to draw any exact conclusion from these results due to the heterogeneity and diversity of the metrics. hence, more region specific, frequency band specific studies are required to be executed to infer more useful information from meg data analysis."
"in order to avoid the neighborhood nodes at the same time moving towards a same position, resulting in a local optimum rather than search for a better solution, a definition of coherent velocity is introduced, and then it is used to explore better solution by the exclusion mechanism between the nodes to force the movement direction diverge out."
"the cover range of sensor node s i is the circular area of its own coordinates (x i, y i ) as the centre and monitoring distance r as the radius. the euclidean distance between the sensor node s i and the monitoring target k with coordinates (x, y) is d ik, and computed as follows:"
"any monitoring target k may be covered by multiple sensor nodes, so the combined coverage p k is formed by all the nodes cover probability, and computed as follows:"
both scd and mci groups exhibited a very similar spatial pattern of altered links: a hyper-synchronized anterior network and a posterior network characterized by a decrease in fc. this decrease was more pronounced in the mci group. these types of fc alterations may work as a key feature to provide a useful tool to characterize the early stage and predict the course of ad.
"as observed from figure 5, α band frequency in mci changes, similarly in case of fc we observe the reduction in connectivity pattern for scd and mci from hc subjects. also, from table 1, it is clear that all these alterations in every parameter take place in ad as well as mci. henceforth, further research hit is required to unhide the concrete cause behind these variations. though meg studies found slowing down of brain rhythms in ad, but these findings cannot specify ad as majority of brain diseases show similar pattern of brain oscillations. therefore, from single channel analysis studies it is inferred that a pattern is seen in ad irrespective of applied method. however, uniform distribution of slowing throughout the brain is not seen. these alterations in brain oscillations, the left parietal, occipital and temporal areas were found to be most frequently affected."
"in order to avoid the node moving out of the monitoring area, a reverse ricochet strategy is used after collision, and the distance of reverse ricochet is calculated as follows:"
"the interfering factors to be considered in meg data analysis are field spread (fs) and volume conduction (vc). because of the topographical representation of magnetic field beyond the source, signal can be picked up at some distance and it is termed as field spread [cit] . due to fs, a signal from one underlying source can be present in multiple time series. this creates error in the estimation of statistical dependencies between time series data which in turn hamper the evaluation of fc [cit] ."
"single channel analysis studies have been executed to observe a distinction between healthy individuals and ad by timefrequency analysis of meg data. parameters reported in literature related to single channel analysis is classified into four groups: (i) spectral analysis, (ii) signal complexity, (iii) signal regularity, and (iv) signal predictability."
"analysis of variance (anova) and two-tailed t-test were applied to test the group differences. spearman's bivariate correlation test were used to access the connection between cognitive status and network derived measures [cit] ). use of systat software for windows has been reported to calculate huynh-feldt-corrected p values for meg data [cit] . they stated application of a two-way repeated measure analysis of variance with ad and hc group as an inter subject factor and 117 meg channels as intra subject factor. pearson correlation coefficient has been used to correlate meg and mri volumetric variables (relative left and right hippocampal volume) to distinguish ad and hc group . the ad vs. hc analysis has also been done using multivariate analysis of variance (manova) where they choose parameters correlation dimension and neural complexity [cit] . a three-way repeated measures anova was used to compare power values between groups. to compare activation within rois and peak frequencies also t-test was carried out [cit] . anova with greenhousegeisser correction has been utilized for the qualitative statistical classification among ad, mci, and control groups [cit] ."
"because of the advantages of deployment flexibility, environmental adaptability, network self-organization and easy to extend, wireless sensor networks have attracted tremendous research interest in academia and industry recently. in the internet of things era, all objects and information access networks, so wireless sensor networks is particularly important as an ubiquitous information perception network. it is flexible and fast to construct perception layer include monitoring nodes and aggregation nodes, then a variety of environments and monitoring targets information is acquired and aggregated by multi-node collaboration to achieve seamless integration of the real physical world and the virtual information world [cit] . it completely changes the interact manner between humans and nature."
"meg data is usually recorded in altered investigational environments, having a spatiotemporal configuration, sampled at several sensors and multiple time points. researchers aim to recognize the alteration among the data observed in these conditions. the processing and analysis procedure varies in research studies as can be observed from figures 3, 4 . some studies work on sensor space data and some on source space which introduce the major alteration in processing pipeline and also the frequency bands considered for the study differs as per requirements and anticipated outcomes. moreover, in most studies the conditions vary with respect to the configuration of stimulus being presented instantaneously before or during the registration of the signal. in other studies, the conditions change with respect to the type of response given by the experiment subjects. another aspect in this regard is that ad population is diverse with respect to gender, age, demography, disease progression etc. are difficult to be found similar among studies. modality integration also plays an important role in experimental design for ad research. various modalities such as eeg, fmri, pet, dti have been integrated with meg to gain complementary information. data availability for ad research is also another major research avenue to investigate for ad research."
"both the local and global optimization will benefit solving some kinds of problems. there is a trade-off between the global and local optimization for different problems. considering of this, an inertia weight of velocity w is introduced. the w, which reflects the memory of movement history, plays the role of balancing the global and local optimization, controls the influence of previous velocity on next movement. the larger value of w is conducive to exploration and increase the diversity of possible solutions,"
"in particle swarm optimization, mutual exchanges could be achieved through the exchange of successful experience between the particles in the same neighborhood. in wireless sensor networks, it is not suitable for global optimization by whole network communication in the condition of nodes with limited communication capability and energy. in order to get the trade-off between convergence speed and optimal solution, the nodes organize a certain number of neighbours to construct a spatial neighborhood, in other words, they decompose the global optimal solution into multiple local optimal solution of spatial neighborhood [cit] . there are two methods of construction: (1) neighborhood with fixed radius, in which the number of nodes in each neighborhood may be different; (2) neighborhood with certain number, in which the node should dynamically adjust the communication radius to communicate with neighbours [cit] ."
"although, in some studies both parametric as well as nonparametric statistics have implemented for comparison, but they have not applied mcp correction [cit] c; [cit] ."
"to properly describe the fact that most active regions have been observed with the most abundant changes in ad, an activity dependent degeneration hypothesis has been proposed [cit] ) . in a recent meg study, changes in the activation of prefrontal brain area has been observed in early stage ad [cit] ."
"from coherence analysis, decreased neural connectivity of multiple brain regions including the right posterior perisylvian region and left middle frontal cortex correlated with a higher degree of disease severity has been reported [cit] . insufficiency in executive control and episodic memory is correlated with reduced fc of the left frontal cortex, whereas visuospatial impairments is correlated with reduced fc of the left inferior parietal cortex [cit] . these results suggested that reductions in region-specific α-band resting state fc are strongly correlated with specific cognitive deficits in ad spectrum [cit] ."
"mci patients showed a network comprising four links where fc values were significantly lower compared to scd fc values. this network with reduced fc connected temporal, parietal and occipital regions of the brain, and comprised both intra and inter-hemispheric links (figure 8) ."
"because of the limited ability of sensor nodes, it is rather complex for optimal coverage of the whole network, and the coverage calculation within the local neighborhood of node i is more suitable. the local neighborhood coverage what is marked as r i is the ratio of"
"spatial neighborhood is a prerequisite for local coverage optimization. the neighborhood of a node changes constantly, so the neighborhood should be updated in each round iteration of coverage optimization. as described in section 1.1, the construction of spatial neighborhood could be made by two methods, and algorithm 1 shows the procedure of construction. algorithm 1. neighborhood with fixed radius rc"
"after analyzing all the findings reported for meg based connectivity studies, a decrease in meg based functional connectivity in the higher frequency bands are observed. increase in connectivity has mainly been observed in the parietal and temporal regions as well as between the parietal and occipital regions and here fc was not frequency dependent. it is concluded from all these connectivity analyses that parietal areas along with left frontal and occipital areas are associated with the decrease in long term connection and for short term connection involvement of the frontal and parietal regions of right hemisphere was reported."
"these methods are in fact independent of signal models and it makes them attractive for application in meg analysis for rapid evaluation of data [cit] . such approaches were utilized to joint multimodal processing of meg and fmri data. processing of meg data with the outputs of a deep neural network obtained from and trained on the same visual categorization task has been performed [cit],b) . with the help of these multimodal approaches, new principles of brain function, generalized to functional systems and patient population, are modeled [cit],b) . there are certain neurochemicals (n-acetyl-aspartate (naa) and myoinositol (mi)) alterations in ad. concentration of naa decreases and mi increases with the progression of ad. integrated study of meg with mrs has been performed to associate altered brain oscillations with neurochemical changes [cit] . for a working memory task, the ad group showed a reduced number of activity sources over left temporoparietal areas in meg analysis. in another mrs study increase in creatine, mi, and in the mi/naa ratio was observed in bilateral temporoparietal region. these results were correlated with mmse score and 65% of the variance was found [cit] ."
"hence, majority of the meg based brain connectivity studies using same experimental pipeline in ad research cannot directly be compared. it necessitates the demand for open science in meg research by generalizing a specific pipeline for meg data acquisition, processing and analysis in clinical setting. research groups should be interested in making research accessible for all to share their materials and data, others can use and analyze them in new ways, potentially leading to new discoveries. this will reduce the so-called \"reproducibility crisis.\" meg data being available will be scientifically beneficial for researchers that can open new avenue in this realm of ad research."
"in this paper, starting from the point of multi-node collaboration in the perception layer of internet of things, an optimal coverage algorithm of wireless sensor networks is proposed based on particle swarm optimization with coherent velocity. the experiments verify the effectiveness of the algorithm; in addition, the coherent velocity plays an important role to improve network coverage. the main contributions are as follows:"
"co-registration of meg data with anatomical 3d mri image ( figure 1e ) enables source localization of the meg data ( figure 1g ). this source reconstructed meg data, which is called source space data (figure 1h ), is used for connectivity estimation among the different selected regions of interests (roi) as well as single channel analysis. source reconstruction is performed for all the meg sensors/broadband activity. afterwards, source space time series is separated into different frequency bands."
"the anterior network presented higher fc in the mci group compared to hc subjects in three links connecting anterior regions, including left inferior temporal gyrus, left paracingulate, and left anterior cingulate. the posterior network exhibited lower fc in the mci group, and comprised 14 links between connecting posterior cortical structures such as: temporal medial structures (both hippocampi and right parahippocampus), parietal (left postcentral gyrus, both supramarginal gyri), and occipital areas (left frontal pole, both superior occipital cortices, right inferior occipital cortex, right lingual cortex) (figure 6) . scd subjects showed increased fc values respect to hc subjects in the same regions described in the previous comparison. scd subjects also showed decreased fc in 11 links. those links connected both intra and inter-hemispherical areas between posterior regions (as shown in figure 7) . interestingly, all the links affected in the scd group, were also disrupted in the mci group in a similar manner."
"coherence is one of the widely used for fc estimation. coherence is the degree of similarity of frequency components of two time series (of simultaneous values or leading and lagging relationships). mathematically, coherence is the frequency domain equivalent to the time domain cross-correlation function. its squared value quantifies, as a function of frequency, the amount of variance in one of the signals that can be explained by the other signal, or vice-versa, in analogy to the squared correlation coefficient in the time domain. the coherence coefficient is a normalized quantity bounded between 0 and 1."
"equivalent current dipole (ecd) has been used to analyze the magnetic counterparts of p50 and mismatch negativity for a meg based study with passive oddball paradigm of ad patients. in comparison to hc subjects, larger cortical activation of standardevoked m50 was observed in ad [cit] . ecd has also been implemented for meg based dipole density estimation of δ and θ band in ad ."
"in order to verify the influence of network coverage in the case of spatial neighborhood constructed by different methods, different communication radius and different number of neighborhood nodes, the corresponding experiments are made, and the results are shown in table 2 and table 3 ."
"minimum current estimation (mce) has been utilized to identify cortical sources of spontaneous brain oscillation from meg data for mci and ad. in comparison to hc group, oscillatory abnormalities in the alpha source distribution were clearly visible in ad whereas for mci significant changes were not observed [cit] ."
"however, such allocation approach generates extra overhead associated with channel sounding which may considerably increase the scheduling complexity as well as the total latency. the tradeoff between sinr enhancement and channel-sounding overhead becomes an essential metric for scheduling mainly in the context of huge number of terminals. a detailed assessment exercise by means of simulation and analysis of actual traffic patterns has been performed to assess the performance of lte release 10 ul air interface. the exercise was carried out for a massively crowded deployment scenario in saudi arabia. results are summarized in fig. 6 . for a small number of connections and/or frame arrival rates, channel-dependent ul resource allocation is indeed feasible. this is particularly true when the majority of connections experience quasi-static channels. however, as the number of connections and/or frame arrival rates increase, ul channel sounding overhead becomes intolerable. the common practice is to resort to pseudo-random frequency hopping which was proven in our study to be suboptimal."
"generally, in our model, workers are more restricted than in market-oriented crowdsourcing platforms. still, such an approach provides adequate flexibility for workers by allowing them to choose the time periods in which they are willing to work. thus, assuming a strong competition among workers, the model will be feasible. constantly increasing interest in crowdsourcing platforms [cit] indicates that such competition is quite realistic. also, sla-enabled services are higher valued, therefore, the monetary compensation, and, thus, the competition can be higher. the feedback provision from the consumer is of his/her own interest, as it positively affects the result quality. it can be provided not for all results but selectively. in this paper we don't discuss the cost of the work and payments in detail. although it is an important factor, in our vision, it can be seamlessy integrated into the platform. one design solution could be that the workers specify the minimal cost for their work and the consumers would pay as much as they want as in a traditional crowdsourcing platform. thus, the more the customer is willing to pay, the more workers would be considered for assignment, and, as it is sensibly to assume, more suitable workers could be found. however, such a design will not change the basics and the algorithms of our platform substantially. thus, for the sake of simplicity, we assume that all the jobs cost correspondingly to their specified duration."
"afterwards, a value proposition description for each iot application in terms of the business impact as well as potential cost saving and/or revenue growth should be developed. finally, the surveyed iot applications should be classified according to mandate, i.e., open-loop (monitoring) vs. closedloop (actuation and control), level of privacy and security required, deployment mode (indoors, outdoors, building faade, light poles, etc.), and communications mode (convergecast vs. peer-to-peer)."
"following, a descriptive framework capturing the high-level interactions between the five kpms should be developed for the different classes of applications. the limitations or in contrast advantages of the underlying networking paradigm (cellular-based, infrastructure-assisted d2d, or multihop) must be researched in terms of the five metrics. finally, the performance and qos requirements will be mapped to the identified classes of applications."
"recently, business processes need to be adapted or extended more frequently to the changing market. companies often lack the new capabilities or knowledge required. to tackle those changes, either new personal needs to be hired, or rather, the new process steps are outsourced. the work in this paper is based around a recent and attractive type of outsourcing called crowdsourcing [cit] . the term crowdsourcing describes a new web-based business model that harnesses the creative solutions of a distributed network of individuals [cit] . this network of humans is typically an open internet-based platform that follows the open world assumption and tries to attract members with different knowledge and interests. large it companies such as amazon, liveops, or yahoo! ( [cit] have recognized the opportunities behind such mass collaboration systems [cit] for both improving their own services and as a business case. the most prominent platform they currently offer is the amazon mechanical turk (amt) [cit] . requesters are invited to issue human-intelligence tasks (hits) requiring a certain qualification to the amt. the registered customers post mostly tasks with minor effort that, however, require human capabilities (e.g., transcription, classification, or categorization tasks [cit] )."
"the most pressing challenge of iot systems seems to be how to meet the performance aspirations of end users given the connectivity limitations as well as the underlying device heterogeneity. rephrased slightly differently, it is a prime goal of researchers to achieve the best attainable performance from the perspective of the application in light of the restrictions and limitations inherent to iot systems. consequently, the design of iot systems in reality mandates the solution of a complex set of constrained optimization problems."
"a key theme addressed in this paper relates to the mode of operation given multiple iot ads. there is growing evidence that operation in complete isolation, i.e., in silos, deprives end users from the great potentials that collaboration and resource sharing may offer. the iot vision pursued in this paper is one whereby iot devices belonging to multiple ads join forces to deliver the best end user experience. this is indeed to be attained while respecting privacy, security, and qos constraints."
"in this paper, we investigated the challenges and techniques that would be implemented in future internet-of-things (iot) systems to push intelligence to front-end devices. two interesting iot trends showing the need to develop new mechanisms to face the massive iot challenges are discussed, namely bandwidth-hungry and delay-intolerant applications and device heterogeneity. based on these challenges, we proved qualitatively and quantitatively the importance of front-end intelligence in the enhancement of iot applications performance. afterwards, we proposed and discussed three design perspectives that can be simultaneously integrated within a single smart iot device. the first design perspective: connectivity enables better wireless and spectrum resource management by exploiting the high network density by performing efficient routing and, at the same time, coping with network congestion and capacity deficit by designing underlay coordination techniques. the second technique is to focus on developing more application-oriented strategies achieving tradeoff between the applications' key performance metrics and the computational and implementation complexity. finally, the third design perspective exploits the co-existence of multiple administrative domains in the same geographical area to push devices to collaborate and socialize such that resources, and environment and neighborhood information are efficiently shared and exploited to enhance devices' output. incorporated all together within software-driven architectures, these techniques allow iot devices to be more independent from the back-end platform with the ability to make in-situ decision, to ensure seamless and reliable connectivity, and to be more productive from an iot application point of view."
"the foreseen ''capacity deficit'' on lte ul networks can be also circumvented by means of advocating the use of multihop communications. in fact, multihop networking is a legacy yet well-developed tool from m2m and wireless sensor networks (wsn) contexts. both domain are perceived as precursors to the iot [cit] . subsequently, it is then inevitable to apply what has been learned there in terms of scalability under energy constraints. to that end, multihop networking has been often advocated as a viable tool meeting the design objectives. examples include smart utility/grid [cit] and vehicle-to-vehicle networks [cit] whereby multihop networking has been incorporated into a standardized protocol stack. as a matter of fact, an industry-leading work-group, thread, release last year a wireless mesh (multihop) networking protocol tailored for the iot of the future [cit] ."
"various schedulers. to demonstrate the advantage of skill-based assignment, a scheduler which mimics a market-like platform was compared with the greedy scheduler and the heuristically-enhanced greedy scheduler (see sect. 4). in market-like scheduling, the assignment followed the logic that randomly chosen workers were picking the most suitable for them active tasks. the results are shown in fig. 3(a) . in tests with high schedule density (about 0.8 or more), market-like assignment performed better than in tests with low density, because workers had more tasks to choose from. however, about 15% of task deadlines were violated in these tests, because workers aimed to fulfill their own preferences rather than the goals of the system. for the rest of the tests, the average quality was 1.5 times better for skill-based scheduling in the large. this clearly shows the benefit of skill-based scheduling. the heuristics did not improve the greedy algorithm substantially, and for some tests even impaired it."
"crowd workers. the workers are assigned for jobs and return the result of job processing. each worker has the claimed skills that s/he initially reports to the platform, and the real skills. the real skills are generated randomly with normal distribution with 0 mean and variance of 0.3. then, the reported skills are initiated as real skills with injected error (normally distributed with mean value equal to the real skill and variance of 0.2). the crowd size in experiments was 1000 workers. this size is big enough to enclose the diversity of workers, but still allows for fast simulation. we tried to use 10000 instead, but the results did not change substantially. workers can be unavailable at certain periods."
"the crowd workers' profile management is of major importance in our assumptions. as mentioned before, the success of the scheduling algorithm partially depends on the accuracy of the profile monitoring. at the beginning of her/his membership at the crowdsourcing platform a user registers with a profile representing the skills. usually this information is not very accurate because users tend to over-/underestimate their skills. hence at runtime, a monitoring module must run on-line and manage the profiles by updating the provided information. it is necessary to avoid conflicts with the promised quality agreements (c.f., sect. 3). this is a major challenge. the task processing results and the expected quality outcome must be used as a reference for the real skills of a worker. the quality expectations on the tasks result are often detailed in the task description. at the amt, for example, the result feedback contains usually only a task accept or reject. at our platform, with an agreement requiring the customer to give a feedback on the quality, the feedback contains crucial information for the algorithm 2 that can be used to update the skills of the reported worker profiles. as the scheduler requires skill knowledge the profile update is twofold. if the worker only provided a low quality the update depends on the difference (lines algorithm 2. profile monitoring."
"all these potential gains that can be obtained thanks to collaboration of smart iot devices require the establishment of mechanisms to model how beneficial relationships can be created among different ads. for instance, a graph-based relationship model, incorporating perceived benefits and cost of collaboration, as well as trustworthiness, can be considered. in addition, centralized and distributed mechanisms for neighbor discovery in light of heterogeneity of radio access and networking technologies could be proposed while taking into account the protocol overhead associated with neighbor discovery particularly in terms of bandwidth and energy consumption."
"in light of the developed techniques for iot device intelligence, the aim is to develop various iot deployment profiles, each having distinctive characteristics and attributes. accordingly, devising network design parameters at the device levelrather than the infrastructure level -are necessary for the operation of each profile. another aspect that needs to be considered is to focus on the development of mechanisms and protocols enabling out-of-the-box software-defined and self-configuration device capabilities in addition to the optimization of the design of existing over-the-air (ota) reconfiguration protocols such that network connectivity is minimally affected, and collaboration policies are respected."
"addressing the collaboration problem is a non-trivial task given the discrepancy in device form factors and computational capabilities on one hand, and application objectives/ constraints on the other hand. therefore, there is a need to develop frameworks, that address the heterogeneity of iot devices in rather a pragmatic manner, and methodologies that embraces differences between devices. we do not call for the homogenization of technologies. instead, we advocate the exploitation of resources across ads at the edge of network, i.e., at the device level."
"in this paper we extend this simple model and focus on crowdsourcing platforms that deal with task groups consisting of manifold similar jobs provided by consumers. most current public crowdsourcing platforms with market-like operation chain announce received tasks at their portal as a first step. next, the worker chooses among the assorted mass of task those s/he likes to process. the selection is motivated by her/his personal preferences. thus, the following assignment is initiated by the worker, and as a consequence, it hardly allows the system to have an influence upon assignments and to leverage the skill heterogeneity of involved workers."
"while our vision and understanding of the iot matures, a few key trends and features are quickly erupting to the surface. the foremost trend is scalability in terms of iot device count and density [cit] . not only is the iot large-scale by design but it is also home for devices belonging to multiple ''administrative domains'' (ad). within the context of this paper, an ad encompasses all iot assets, including front-end devices and back-end platform resources, which fall under the jurisdiction of a single commercial or operational entity."
"reliable and high-bandwidth iot connectivity is quite essential to meet end user aspirations. nonetheless, a big fat/fast pipe does not tell the whole story. iot systems also entail quite a few operational challenges the most noteworthy of which are battery management, device maintenance, and software upgrades. as such, there is an inherent tradeoff between application qos requirements versus operational constraints. this paper aims to provide insights and examples to achieve such tradeoff while taking into consideration five kpms, namely: energy, latency, throughput, scalability, and reliability."
"as shown in fig. 2, features were identified from the accelerations. the features that were sensitive to changes in mobility status included acceleration y, standard deviation (std) (eq. 1) in x to z accelerations, range of y (range-y) (eq. 2), sum of ranges (sr) (eq. 3), signal magnitude area (sma) of sum of ranges (eq. 4), difference of range (diffsr) (eq. 5), range of x and z (rxz) (eq. 6). these features were entered into a decision tree (fig. 3 ) to determine if a cos occurred. single or double thresholds, with threshold values modifiable in the setup menu, were used to identify cos. status values were also calculated for each feature. these status values were imported into a second decision tree for activity classification (fig. 4) ."
"however, as shown in sect. 5, such extensions do not give a substantial improvement. we believe that the reason of such a weak improvement is the size of the crowd: if a worker cannot be assigned to a due task, in most of the cases a good enough replacement for the worker can be found. thus, we conclude that a greedy algorithm is generally sufficient for scheduling in a crowdsourcing environment. the refinement of the algorithm can be done according to the particular crowd characteristics that can be estimated only when the system is used by real users in the commercial operation."
"the concept of ads is further illustrated in fig. 2 . the iot back-end platform acts as the interface between iot front-end assets, i.e., wireless devices, and the end user. the iot platform may be owned and managed by the end user. alternatively, the iot platform can be offered as a service by an external party. in this specific illustration, there are five ads, rather than four, since one of the four end users is running its applications using two different iot platforms. this paper aims to exploit the coexistence of these multiple ads by proposing innovative solutions that enlarge the potentials of the iot applications. effective collaboration schemes across the ads will certainly offer more flexibility to end users. in fig. 2, collaboration may be initiated across domains: 1) within the same iot platform, or 2) via a special communications interface across iot back-end platforms."
"in this section, we first focus on the main connectivity challenges that are faced by large-scale iot namely the issues with cellular networks and the spectrum management."
"a few other examples of bandwidth-hungry delay-intolerant iot applications come from industry process management. conscious to safety and operational integrity, many gas plants are installing tomography cameras which stream live characterization of the plants atmosphere [cit] . coupled with arrays of gas sensors, tomography cameras help gas plant operators detect anomalies in the underlying physical/chemical process and react in a timely manner."
"all the plots show the average job outcome quality for the resulting assignment density which is calculated as total number of periods for all workers while they were either unavailable or busy, divided by the difference between the first task submission time and the latest deadline. apart from where explicitly specified, performed experiments contained no task deadline violations."
"the primary objective in the context of this key perspective is to unleash the potential of cross-domain collaboration while observing end-to-end performance targets as well as privacy and security constraints. this is envisioned to be achieved by means of establishing a front-end marketplace, i.e., by empowering iot devices with the capability of socializing with each other for the sake of bargaining and auctioning resources. it is necessary to evaluate the gains that can be achieved due to the cooperation of different devices belonging to the same and/or different ads as well as i.e., application-related decisions to be taken by the smart iot device often without reverting to the back-end platform? in-situ decision-making shall be assessed in terms of accuracy, timeliness, probability of false-alarm decisions, and probability of missed detection (failing to detect an important event). what is the cost saving brought forward by means of cross-domain collaboration, in terms of battery size, sensory circuitry, radio front end, etc."
"sum of ranges was more sensitive than distinguishing mobility states (fig. 5) . the extracted features can also help to movements; such as, brushing teeth (bt (ch), washing hands (wh), drying hand dishes (md), moving a kettle (mk), toas preparing a meal (pm), and washing dishes in fig. 6 . a combination of static status a to identify these small movements. movements are easily distinguished; howe missed for a continuous series of small example, brushing teeth and then combing h the sma of sum of ranges curves was s sum of ranges curves. the smooth sma o (sma-sr) curve easily defined thresho climbing stairs and walking (fig. 7) . better activities classification results were achieved when using both acceleration features and video clips, as compared to using the accelerometer only (table iii), with the exception of sitting. acceleration-only had 4% greater sensitivity than acceleration-and-video for sitting because one cos was missed, which resulted in no associated video data. the relatively low accelerometer sampling rate with blackberry os 5 is a challenge for wmms activities classification. at less than 10 hz, fewer accelerometer signal processing options are available and the loss of accelerometer data during video recording limits the ability to detect cos within the video recording period. however, by combining and weighting the range, sum, and covariance statistics, good activities classification was possible for standing, sitting, lying, preparing a meal, and brushing teeth. walking, climbing stairs, and riding an elevator had high sensitivity, but the specificity of cos identification and activities classification could be improved by adding additional sensors or increasing the accelerometer sampling rate. the classification of other small movement activities requires further research to increase sensitivity and specificity. higher accelerometer sampling frequencies (above 20hz, and ideally above 50 hz) could help reduce walking false positives and help to classify walking-related activities correctly (level ground, inclines, stairs, etc.). further research on calibration methods to set appropriate thresholds for each individual could also help decrease false positives."
"proof: unconstrained l allows for an unlimited number of retransmissions before a link is successful. similarly, unconstrained e allows for unbounded increase in transmit power. as such, r is also unbounded. therefore, arbitrary targets for q k andq k can be set independent of ρ."
"skill requirements are generated so that each skill with approximately equal probability either equals 0 which means that this skill is not required for the task, or is in ( the feedback that a consumer provides for a job is generated using the real skills of the worker which were assigned for this job. in contrast to the estimated skills, these real skills are unknown to the platform and are only used to simulate the real outcome quality (by calculating the suitability with these skills). this quality is thus reported as the feedback."
"to demonstrate the efficiency of skill update mechanism, we compared the regular simulation which implements the logic described in sect. 4 (\"regular\" series) to upper and lower bounds. the series named \"no feedback\" represents the lower bound and only the initial information on the profiles is used for scheduling. an upper bound to the algorithm is shown by the series of \"real skill-aware\". in this case the exact skills of a worker are known to the system. the improvement of the skill update mechanism over the lower bound is evident and keeps performing better at any scheduling density. in the experiments of fig. 3(b) the improvement over no feedback remains between 10-15%. as sect. 4.4 explains, the reason why it is never reaching real skill awareness is twofold. first, the scheduling strategy need some input right from start when only few feedback is available. second, the feedback is a single value that describes the performance depending on ten different skills. also, a skill value greater than required calculates the quality with the lowest value required."
"by means of incorporating random access channel (rach) resources into the frame's preamble. iot devices at a given hop extract position information encoded by the nodes of the previous hop, and accordingly are able to locally make their routing decisions. it is also interesting to note here that although rach collisions may indeed occur, such collisions do not hinder the forwarding process but rather have an unwanted impact on overall energy consumption [cit] . fig. 8 details the basic operation of a contention-free scheme and illustrates."
"in this study, we consider a scenario of a large-scale multihop iot setup where a few exchanges of information must take place before an actuation event is executed by a given iot device. it is assumed a device will execute an action after a decision is made based on vital information collected from its nearby neighbors and consequently, acts upon a potential actuation event. for example, the process of opening/closing a valve in a factory needs to be first educated by the temperature and pressure reading of nearby sensors. this is often essential to avoid hazards of extreme pressure or temperature gradients. 3 illustrates the actuation process for the back-end centric paradigm (i.e., fig. 3(a) ) as well as the front-end empowered paradigm (i.e., fig. 3(b) ). note that with the back-end centric paradigm, i.e., the classical iot setup, the information collected from nearby neighbors are forwarded to the back-end core to make the right decision which is, in turn, sent to actuator to perform the required action. however, with the front-end empowered paradigm, the device is supported with intelligence to take in-situ the decision based on the information collected from nearby sensors that directly communicate with the smart device."
"an example of collaborative iot devices is provided in fig. 10 where iot devices within a neighborhood belonging to different ads are serving applications with different performance objectives. graph theory is a suitable tool to represent social relationships between devices. in this example, the edges in the graph are given weight pairs which represent trustworthiness and collaboration benefit, respectively."
"in this paper we proposed a skill-aware crowdsourcing platform model which allows to provide crowdsourcing services with slas and to control the task performance quality. in contrast to existing crowdsourcing platforms such as amt, which follow a task market-oriented approach, our platform model is based on services computing concepts. such a model is typically applied in enterprise workflow systems using, for example, the ws-humantask specification to design human interactions in service-oriented systems. however, ws-humantask and related specifications lack the notion of human slas and task quality. in our approach, negotiated slas and monitoring help to assign task requests to suitable workers. thus, our platform ensures quality guarantees by selecting skilled workers. we introduced the proof-of-concept implementation with particular algorithms for task scheduling and worker profile management. the applicability of the platform design was proved in a simulated environment. the experimental results shows the clear advantage of skill-based scheduling in crowdsourcing, as the average quality is 50% better in the large comparing to the case when the workers choose tasks by themselves. the skill monitoring and updating mechanism improves the overall quality by 10-15%."
"after the contract parties' details, schedulingplatform and consumer listed in lines 4 to 11, listing 1.1 states the contract items from line 12 to 20. these are a collection of serviceobjecttype items including scheduling, operation description, and configuration, and also, the slaparameter s. here, as an example the parameter (taskskills) uses a crowd particular metric compareskills to compare skill profiles of the workers to the skill required in the document. important to note, we extend the wsla definition with our own namespace (task scheduling platform tsp) defining xpath methods for expressions and type definitions to comply with all requirements of the platform. listing 1.2 shows the agreement's terms as obligations of the contract including some slos. an slo consists of an obliged party, a validity period, and an expressions that can be combined with a logic expression (e.g., or ). the value tag in the predicates of wsla is restricted to double. hence, another extension with xpath methods allows us to provide more complex expressions for the values (e.g., see line 8) . generally, in an slo an evaluation event defines the trigger for the evaluation of the metric function. the content of the expressions connects the pool of slaparameters of the items to a predicate (e.g, greaterequal) and threshold value (value). in the example we define three objectives. the first, sloskills defines that the match between the task skill requirement skills required and the selected potential worker's skills must be greater or equal. the second, sloquality is a composed objective by an or expression. the agreement states, that either a defined number of tasks (100) is delivered at the end of an agreed interval (e.g., wsdl:months) or the focus of processing is on the task's quality, and hence, a result quality of at least 80% is expected. the final objective slofee obliges the consumer to confirm the quality of the result and pay the related fee. in the example, similar to line 8, the xpath method getinput parses a document taskdesc and returns the task's fee. as the fee depends also on the result, the result report taskres contains the reported quality. multiplied they give the final fee due."
"device heterogeneity coins the wide spectrum of capabilities, form factors, bandwidth, process power, and radio access technologies that iot devices may support. it is encouraging at one hand to see a number of standardization bodies and industrial forums currently engaged in the development of standardized access protocols and messaging technologies for the iot [cit] . however, historical evidence and prior experience in similar situations indicate that it is highly unlikely for the industry to eventually converge into a single homogenized solution. the rivalry that the wireless community has witnessed between the wireless interoperability for microwave access (wimax) and long-term evolution (lte) radio technologies is one striking example [cit] . such an expected divergence is specifically true for iot devices at the edge in light of the multitude of wireless access technologies and spectrum bands currently on the iot radar screen [cit] . consequently, it is inevitable and even mandatory for iot systems to entertain what seems to be inherent technological diversity, in other terms: heterogeneity."
"while the benefits and gains of collaboration seem to be rather obvious, a key challenge that has to be addressed is how to resolve conflicts of interest that may arise. iot devices within proximity would typically belong to multiple ads having different operational objectives and qos mandates. hence, one of the main problems is how to create a virtual marketplace whereby iot devices belonging to different ads bargain resources to offer versus those to ask for. the idea is based on the concept of iot devices ''socialization'' [cit], where the devices explore their neighborhood and establish relationship with other devices belonging to other ads in order to improve the qos/qoe from the perspective of the application. accordingly, mechanisms for relationship building and optimized decision-making will be developed given privacy and trustworthiness levels. this indeed shall drive towards building more intelligence and decision-making capability into iot devices. therefore, novel models and techniques should be developed and investigated to design the give-and-take social relationships to be established amongst smart iot devices. graph theory and game-theoretical models are the best tools for these kinds of problem design. as a final note, a natural consequence of collaboration and socialization across ads is to further scale up iot deployment scenarios in terms of device count and density per unit area. therefore, this sheds light again on the large-scale nature inherent to iot systems."
"axiom 2: for a given physical process, the probability of false alarm, denoted by p fa, and the probability of miss, denoted by p m, are both negative monotones in n . this is based on the following rationales:"
"without a delicate treatment, any attempt to address the key trends and sought-after features outlined above may rapidly lead to the compromise of end user aspirations. as such, there is a strong motivation to construct an iot system design framework that addresses the iot application needs in light of operational, deployment, and cost constraints. to that end, this paper introduces a rather holistic approach whereby the iot dilemma is treated from three distinct perspectives: the application, connectivity, and collaboration."
"on a different iot wave front, live video streaming seems to be a crucial ingredient for future cyber-physical systems (cps) [cit] . with the rocket-speed development in unmanned autonomous vehicles (uav), whether aerial or terrestrial, new horizons are currently being explored. we can now imagine public safety personnel performing critical field missions while being aided by clusters of uav agents [cit] . research shows that the exchange of live video amongst cluster members and with the centralized control center tremendously increases the success and efficiency of the mission. this is true since it enhances the situational awareness of field personnel [cit] ."
"when dealing with perceptual objects (especially, components in images), it is sometimes necessary to relax the equivalence condition of defn. 1 to facilitate observation of associations in a perceptual system. this variation is called a tolerance relation and is given in defn. 3."
"while there is strong evidence of superior performance to be attained by connectionless routing [cit], there are still some open research fronts. since the rach area is a cornerstone in the operation of connectionless routing, it is indeed a rich area for improvement. the overhead associated with the rach area is one important aspect. the tradeoff between the probability of collision on the rach and the overhead needs to be preferably relaxed. one way to achieve that is through the implementation of online learning mechanisms. an iot device can sequentially inspect the collision statistics of forwarded frames. hop after the other it can simply note the selected rach blocks by other nodes. with time, and without any cross-coordination, devices should eventually be able to converge to the selection of unique blocks. this has the effect of reducing the probability of collision. subsequently, the radio resources allocated for the rach can also reduce thereby lowering the rach phy overhead."
"assignment control enables the platform to estimate the crowd occupancy and to give certain guarantees to consumers. such guarantees can be given in form of slas, and allow to integrate crowdsourcing activities in service-oriented environments. as slas are crucial for business process management, such a model can substantially sustain the use of crowdsourced services in business processes."
"of course, the platform is intended for usage by multiple consumers, this fact is not depicted for simplicity. a deterministic time model is used in the platform, so the time is discreet and is represented by sequential equally long time periods. a time period can represent any time duration like a minute, an hour, or a day."
"1) demonstrates the advantage of front-end intelligence for large-scale application-oriented iot systems. 2) provides rather a novel model for optimizing the performance of application-oriented iot in light of connectivity constraints and collaboration potential. 3) identifies non-debatable shortcomings of some of the existing techniques and technologies in the context of making iot devices ''smarter''. 4) proposes potential viable solutions while outlining open research directions. in summary, the output of this study is a conceptual framework for a new generation of iot devices enabling multiple new features for the iot platform administrator as well as the end user. these smart iot devices will have significant positive impacts on different domains allowing fast, reliable, and intelligent management of diverse iot-based applications."
"in light of fig. 3, the latency and energy associated with an actuation event for the back-end centric paradigm and the front-end empowered paradigm are given in table 1 : conclusion: based on axiom 3 and lemma 2, it is evident that iot paradigms with front-end intelligence are by far more energy-efficient and offer lower latencies."
"require: qf quality feedback of the provider, qe quality expected by the provider require: worker processing worker and taskskills required task skills [cit] . if the quality is above a certain threshold ϑ q and is better than a previous then we consider the required skills close to the workers own (line 3-14) . hence, the difference between the required and the worker's own skills (weighed by the factor α w ) influence the worker's skill update."
"this article presents a practical application of near sets in discovering similar images and in measuring the degree of similarity between images. near sets themselves reflect human perception, i.e., emulating how humans go about perceiving and, possibly, recognizing objects in the environment. although a consideration of human perception itself is outside the scope of this article, it should be noted that a rather common sense view of perception underlies the basic understanding of near sets (in effect, perceiving means identifying objects with common descriptions). and perception itself can be understood in maurice merleau-ponty's sense [cit], where perceptual objects are those objects captured by the senses. in presenting this application, this article has presented details on how to apply near set theory to the problem of classification of images by way of calculating the nearness of images. the results presented here demonstrate that the nm measure can be used effectively to create pattern classification systems. moreover, it is the case that the choice of probe functions is very important. the results obtained so far in comparing nearness measures and svm are promising. future work in this research includes further comparisons between svms and nms relative to selections of features and corresponding probe functions. for example, it may be that probe functions that are invariant with respect to scale, rotation, and translations would produce closer results. what is certain is that the results presented in this article demonstrate that near set theory can be a useful tool in image recognition systems and that perception based classification is possible."
"collaboration between iot devices may actually occur within three distinct planes. the first is the data forwarding plane and encompasses collaboration in relaying and routing information towards the back-end server platform. therefore, it entails the sharing of radio and bandwidth resources. the second plane involves sharing of intelligence and information, i.e., exchanging sensor measurements and knowledge of the surroundings. collectively, this is expected to relax the sensory circuitry requirements and should improve the ability of iot devices to perform in-situ decisions. the third plane of collaboration is based on offering computational resources to peers within the neighborhood. in other words, iot devices in proximity help each other in performing possibly complex computations (relative to their computational capabilities). this can prove to be quite beneficial in terms of moving decision-making capabilities towards the edge and subsequently freeing valuable network and bandwidth resources."
"other researchers have developed wearable video systems with sensors to record gps, ecg, video clips, and/or acceleration [cit] . further, some researchers used cell phone platforms to recognize multi-activities by accelerometer only [cit] . the research in this paper used the blackberry storm2 as a wmms. previous preliminary work confirmed that the blackberry wmms, using acceleration and pictures, could identify walking movements, standing, sitting, and lying down [cit] ."
"even if there was enough data an accurate calculation would not be feasible in all cases. thus, we decided to stick to a simpler quicker update algorithm that provides almost constant quality improvement and, after all, supports quality negotiation with a considerable and steady lower bound to make agreements. the performance of scheduling and skill updating in a high workload test (10000 workers and 1000 tasks) was good enough for a period size of one minute. thus, the performance is not a concern, since the real period size is likely to be bigger (e.g., 10 -60 minutes)."
"system architecture raw data including accelerations, gps location, and video were collected at the maximum sampling frequency (8 hz), for multimedia recording and 20 hz without video capture (fig. 1) ."
"articulated slightly differently, this paper seeks to devise an optimal iot design framework. optimality here is perceived as the ability to cater for applications' objectives in light of connectivity constraints while exploiting potential collaboration amongst nearby devices. in practice, such a design approach yields a set of operational and management routines/algorithms which an iot system needs to execute. currently, iot systems are highly centralized in terms of architecture. the current iot system model implies a back-end core (often referred to as a ''service platform'') where most of the management and operational decisions are executed [cit] . as will be shown in this paper, a highlycentralized paradigm does not lend itself to networking efficiency as well as computational efficiency. this is mainly due to the inherent inefficiencies associated with the decisionmaking process in centralized paradigms. this has been essentially the main driver for the iot community to emphasize the need for more computational power at the front edge, often referred to as ''edge computing''. we have rather reverted to the use of the term ''front-end intelligence'' to coin the fact that front-end iot devices will be essentially exploiting edge-computing capabilities to perform intelligent operational decisions."
"as a matter of fact, the iot is expected to introduce another form of disparity. this is related to the fact that multiple ads need to coexist next to each other [cit] . within this context, an ad encompasses all iot assets, including front-end devices and back-end platform resources, which fall under the jurisdiction of a single entity. an iot ad may indeed serve one or more applications but eventually all assets are considered to belong to a single ownership. indeed, an ad may concurrently serve multiple applications. while an end user can own and manage more than one ad, an ad by definition falls under the authority of a single end user."
"accelerations, gps location, and video were collected at the phone's maximum sampling frequency. the accelerations sampling rate was 8 hz, since multimedia capture was active [cit] . the gps location listener was updated each second to get longitude, latitude, altitude, heading, and speed, when in an outdoor environment. system output included time, raw sensor data, activity features, and digital video."
"defn. 1 [cit] . using the indiscernibility relation, objects with matching descriptions can be grouped together forming granules of highest object resolution determined by the probe functions in b. this gives rise to an elementary set"
"it is an established and undeniable fact that the iot will be large in scale. to further escalate the complexity associated with iot scalability, it is noteworthy that there is a growing trend of bandwidth-hungry or delay-intolerant iot applications (or rather often a mix of both) [cit] . this primarily appears in applications demanding real-time transmission of video streams. following are a couple of examples."
"in order to address such tradeoffs and potential compromises, it is recommended to design application-oriented optimization problems and provide low complexity solutions to solve them. this section is mainly concerned with devising a logical flow for addressing the challenge of optimizing the iot infrastructure in light of the application objective and underlying constraints. from the perspective of end users, the iot is only a means to create value-added applications. in mathematical terms, the task mainly aims to solve a set of joint optimization problems over the domain of five kpms. the underlying networking paradigm (multihop or cellular) may be regarded as the 6th dimension of the problem. the fact that the iot must simultaneously support different ads and application scenarios inherently entails pareto optimality."
"two interesting trends associated with the massive roll-out of iot are next outlined. in specific, this section sheds some light on the design and operational challenges brought forward by such trends. the intention is to pave the way for the following section which discusses in more depth the motivation for front-end intelligence."
"this key perspective has to be designated to boost the capacity on the lte ul interface, in terms of both total throughput and number of connections. undoubtedly, this has been a rich area of research over the past five years [cit] . however, our first recommendation calls for the capitalization of ubiquitously available technologies such as wi-fi for the creation of an underlay coordination plane. said plane shall serve as a medium for nearby smart iot devices to collaborate and/or negotiate their ul access schedules. in contrast to the prevailing mentality of barring machine-tomachine (m2m) access to protect human-made traffic [cit], the goal is to alleviate the priority of time-critical m2m traffic while minimizing penalty to other network users. we believe this is a novel twist to the problem since it joins ul scheduling and cognitive radio access techniques and brings them closer to a practical setup. in fact, a very recent ieee spectrum feature article highlighted d2d as a key enabler in six out of eight key objectives of fifthgeneration (5g) networks' development pillars [cit] ."
"auction-based marketplaces have been already proposed but for mobile broadband data offloading [cit] . however, the centralized nature of such approaches almost eliminates any possibility from reusing them at the edge, i.e., deviceend. there do exist however some early attempts for understanding social aspects in iot device collaboration [cit] . however, without a strong grasp of iot application objective functions and device capabilities/resources, the bargaining game becomes quite loose and hazy."
"as a final statement in this section, it is essential to point out that illustrating a clear vision of the future iot demands and challenges is quite crucial as a tool rather than a goal. the goal is eventually to deliver the best end user quality of experience (qoe) and build iot systems capable of meeting qos objectives of the underlying applications [cit] . optimization of the underling iot systems to serve the needs of applications will be effort in vain without such a clear vision. this paper proposes to take bandwidth and delay constraints into consideration while concurrently catering for the large-scale and device heterogeneity properties of the iot [cit] ."
the paper is organized as follows. in sect. 2 we list work related to crowdsourcing and scheduling. section 3 outlines our platform model that provides agreementbased task assignments to a crowd and also gives insights in the application of current crowdsourcing platforms. section 4 details the proposed crowdsourcing model. a prototype of the platform is evaluated in sect. 5. section 6 concludes the work.
"1) decisions related to the management of the underlying iot application are dominantly processed and taken at the back-end core. 2) management of front-end resources, i.e., devices, is typically carried out at the back-end core. in addition to that, interaction with other ads cannot be performed in the first place if front-end devices are not entitled to execute basic data gathering operations about their neighborhood of devices."
"in addition to that, smart iot devices should include an abstraction layer enabling administrators to program (i.e., customize/ personalize) the operation of their iot devices. fig. 11 further illustrates these concepts. end users will be provided with intuitive high-level interface or graphical user interface (gui) wizard to provide their ''wish list'' for the underlying iot system. this is translated into qos requirements and is fed into a parameterization engine. the engine shall have the intelligence to map the qos requirements into the best operational profile. in other words, it will output a set of optimal values for the device to observe. this includes connectivity-related parameters such as transmit power, bandwidth, modulation scheme, routing rules, etc. it also includes socialization-related parameters such as trustworthiness of neighbors, degree of altruism/greediness, etc. based on the prior performance and after identifying the capabilities, limitations and restrictions, software-defined architecture allows iot devices to dynamically redefine their input parameters to optimize their outputs. this is known as the do-it-yourself (diy) concept [cit] . the diy represents a major asset yielded by software-define architecture to bring intelligence to iot devices enabling self-organized devices and in-situ decision based on information and resource share with neighbor devices."
"application development an application was developed for blackberry os 5 using eclipse, blackberry sdk, and blackberry desktop manager. testing was performed with a blackberry 9550 smartphone."
"in light of the above, the primary focus of this paper is to discuss the issues and challenges associated with the empowerment of front-end iot devices. front-end intelligence is explored in light of the three design perspectives already highlighted above, i.e., applications, connectivity, and collaboration. the paper first identifies some of the obvious drawbacks in existing off-the-shelf iot design techniques. it then shifts gears to proposing potential solutions as well as outlining open research directions. the main contributions of the paper are summarized as follows:"
"iot communications is mainly envisioned by standardization bodies to operate in licensed spectrum. however, we believe that large-scale iot shall eventually drive the consideration of unlicensed-spectrum. as such, it is paramount to fortify the corresponding routing and relaying techniques with effective power control mechanisms. this is instrumental to maintain the overall interference level within operationally acceptable ranges. although large-scale iot setups are quite likely to be noise-limited, there are other cases where interferencelimited scenarios become dominant, e.g., outdoor scenarios. power control strategies have to accordingly cater for that as they inevitably are a necessary tool to control and manage interference."
"in particular, two main conclusions could be drawn from this exercise: 1) as the number of connections/frame arrival rate increases, resource allocation becomes less efficient (and consequently achievable enb capacity region is eroded). in the network entry procedure, the congestion on the physical random access channel (prach) becomes a real bottleneck as the number of connections and frame arrival frequency increases. 2) drop rate and channel access waiting time increase as connections increase. thus, rendering same qoe for htc and qos for mtc is unacceptable."
"in this section a model of a crowdsourcing platform (see fig. 1 ) which supports agreement-based task assignment is outlined. to begin with, there is a negotiation that states the objectives between the consumer and the platform regarding the coming task assignments. the life-cycle of a task begins at the consumer. s/he submits a collection of tasks to the crowd. we assume that the tasks have distinct skill requirements, thus, need to be assigned accordingly. after processing, the result is returned to the consumer which is invited to provide a quality feedback on the result. once a consumer submits a task, s/he provides skill requirements along with the task. in the assignment phase the platform estimates the matching between tasks and workers. the more a worker's skills match the requirements the more this worker is suitable for the task. also, in service-oriented environments a service level agreement (sla) usually is negotiated which has influence on the assignment strategy. an sla includes temporal and quality requirements, and preferences. the challenge for the platform is to negotiate the service level objectives (slos) of an sla related to possible assignments. these slos must base on the observed and predicted behavior of the crowd and the current situation. in particular, already distributed tasks and the resulting task load must be considered."
"however, the extension of multihop networking protocols for use in large-scale iot has been also a debatable issue. some researchers have voiced concerns that the iot setup pushes the envelope of scalability to an extent that discourages the use of multihop networking [cit] . this may have been motivated by the fact that the routing and medium access protocols overhead builds up quite rapidly as the network grows in scale [cit] . this is particularly true for dynamic networks whose topologies undergo frequent changes due to mobility and/or uncoordinated sleep schedules [cit] . indeed, the utilization of multihop networking for large-scale iot must first address the concerns raised above. in specific, for the network to scale we need to tackle the challenge of protocol overhead [cit] ."
"therefore, problem formulation must be delicately handled. it is highly advised to conduct an information gathering exercise prior to the start of the design process. in such an exercise, data about the iot framework from an applicationoriented perspective will be collected. there are indeed some interesting efforts within the community to compile an architectural framework for iot application domains (e.g., the volume 4, 2016 ieee p2413 standard group). however, different geographical and demographical factors may eventually entail divergent set of objectives by the underlying iot stakeholders. for example, a highly industrialized locale is expected to involve a large number of closed-loop control applications. such application naturally entail an abundance of actuation instructions. as already indicated before, while such applications are light in terms of throughput demand, they are quite often delay-intolerant. to add to its stringency in terms of constraints, closed-loop applications (illustrated in fig. 9 ) require a fast round-trip time therefore placing lots of pressure on the overall delay budget. as such, it is highly advised to run through the survey exercise mentioned above to really understand the overall iot application landscape."
"the traditional line of thought within this context is to utilize d2d communications for the sake of bandwidth sharing. some of the low-hanging fruits to be captured using this strategy include the reduction of the number of ul connections on the lte interface. this subsequently allows once again for using the more efficient channel-aware ul scheduling. the bandwidth-sharing scheme allows user equipments (ues) to route their ul traffic through a ue which enjoys the best ul sinr. for the sake of simplicity, it is assumed that a ue is allowed to associate itself only with a ue that is a single volume 4, 2016 hop away. if there is no single-hop neighbor with a better sinr, a ue reverts to a stand-alone state."
"therefore, active tasks are assigned not only according to suitability of workers, but also, in line with the slos of the agreement. the agreement aims to avoid or minimize the losses and to maximize the overall result's quality. in other words, the objective is to maximize the quality while enforcing the sla. also, the availability of workers is taken into consideration by requesting short-term information regarding their ability to perform tasks."
"near set theory can be used to determine the nearness between two images. the nearness measure can be considered a feature value as defined in pattern classification literature (see, e.g., [cit] ). the following sections describe an approach for applying near set theory to images."
"simulation of a real crowdsourcing environment is challenging due to the lack of comprehensive statistical data in this area. although we don't rely on any real data in our simulations, we tried our best to prognosticate the meaningful simulation parameters based on our knowledge and experience."
"fortunately, recent advances in low-cost computationally efficient processing units (e.g., microcontrollers) provide substantial incentives to revisit such a legacy paradigm. the large-scale iot era is hard to materialize and realize without empowering front-end devices. in other words, it should be the era of ''smart iot devices''."
"once the qos requirements for each class of applications are identified, one or more objective functions must be constructed. the construction of such functions should take into consideration the five kpms, such that tradeoffs and weighted significance are captured. following that, a study must be conducted on the implications associated with the case of iot devices belonging to different ads and therefore refraining from availing resources. in the light of the previous steps, optimization problems can be formulated and solved in order to optimize the developed objective functions in terms of the key metrics."
"the proposed architecture represents an improvement of the traditional centralized iot system. it advocates a new model that moves part of the intelligence near the front-end without neglecting the existence of the back-end. indeed, depending on the application and/or the complexity of the executed task, the ads' owners or the system in general can decide at which level the processing could be made based on workload requirements and device capability. hence, the proposed model complements the already existing architecture and provides more degrees of freedom for decision making allowing additional efficiency in terms of applications' outputs."
"also within the context of asset integrity monitoring, there are machinery health applications. industry has recently started to pay better attention to preventive maintenance as a best practice. this in return entails the roll-out of various asset integrity monitoring and inspection systems. for example, factories are increasingly resorting to the installation of accelerometers on rotating parts [cit] . although the transmission of readings back to the control room is quite delaytolerant, the throughput requirement is not trivial. it typically entails sending a vector containing the power spectral density as measured by the accelerometer. similarly, ultrasonic corrosion/erosion measurements are periodically carried out on hydrocarbon transmission pipelines [cit] . again, these measurements are delay-tolerant but generate an appreciable amount of upstream throughput."
"in our experiments we use a workers unavailability parameter which indicates the mean ratio of unavailable workers for each period of time (values used: 0.2 − 0.6, step 0.1). the busy periods are generated randomly, but have a continuous form which reproduces human behavior. the amount of time that takes a worker to finish the job is the job duration with injected variations. in our experiments we used a value of 30%, which means that a job can be executed for 0.7 − 1.3 of job duration. this reflects the random nature of the real world."
"optimizing the exchange of typical resources including communication bandwidth, computational power, buffers, physical sensors, battery lifetime, etc. could be also captured and investigated such that the cost-benefit tradeoffs of collaboration for various application scenarios can be determined. furthermore, marketplace game theory could play a key role to build an optimization framework for making the right decision and answering the essential questions in such scenarios: when to collaborate, what resources to release, and how much to ask for (the price). different protocols could be designed in such marketplace. centralized and distributed socialization volume 4, 2016 protocols can be developed with respect of the feasibility for collaboration marketplaces, particularly in terms of latency and network reliability. the impact of mobility and dynamic network topology on the coherence time of collaboration decisions (i.e., time after which decisions become obsolete) while considering the associated overhead for extreme cases of mobility, represents a challenging problem in this context."
"however, this information must not be considered complete and reliable. some workers might not know about their real skill-levels or overestimate their capabilities. hence, the platform management must be allowed to monitor the activities in the crowd and update the created profiles according to the observations. the resulting real quality is reported by the consumer's feedback. the difference between expected quality and required skills for the tasks hint the real skills of the workers. also, if an assignment is refused by a worker, despite his claim for availability, various penalty sanctions can be imposed to this worker."
"furthermore, the iot is expected to generate an upstream data tsunami. this is partly due to the large-scale nature of iot. it is also driven by a growing number of bandwidth-hungry (and often delay-intolerant) applications. the upcoming iot is also characterized by a high degree of heterogeneity not only in terms of device types but also with respect to applications and quality of service (qos) requirements. finally, it is not a secret that cost effectiveness is a non-negotiable feature desired in every iot setup."
"it is actually demonstrated here that decision-making in centralized paradigms are associated with higher latencies in addition to higher energy consumption. on the other hand, the inferiority of centralized back-end systems in terms of computational efficiency can be illustrated using arguments similar to those in favor for cloud or distributed computing. for instance, moving towards a distributed paradigm has the effect of shifting capital expenditure to operational expenditure; a sufficiently attractive incentive for many iot operators. there is also substantial evidence that a centralized paradigm supports lower computational power (e.g., number of instructions per unit time) [cit] . fortunately, recent advances in low-cost computationally efficient processing units (e.g., micro-controllers) [cit] provide substantial incentives to revisit such a legacy paradigm. the large-scale iot era is hard to materialize and realize without empowering frontend devices. in other words, it should be the era of ''smart iot devices''."
customers. the customers submit tasks to the platform and provide the feedback on completed jobs. tasks are submitted randomly while ensuring the average crowd workload and avoiding overloads.
"proper parameterizations of the lte ul interface can address the ul capacity deficit driven by large-scale iot, but only partially. as such, there is an undeniable momentum in the research and development community to incorporate native support for a wide-range of iot applications into the lte standardization efforts. nevertheless, from a business case viewpoint, an essential question is not ''what'' but rather ''when'' and ''how long before''. it is an established fact that the technology uptake, particularly in emerging markets, has a slow pace. for example, the time gap between having an lte release ratified and making it to the market in the levant and arabian gulf area was between 3 and 5 years. accordingly, this may jeopardize and subsequently delay the business case for rolling out massive iot services. it may be argued that densification of base stations (roll-out of small cells, also known as femto cells) is a ready and immediate recipe to address ul bottlenecks. our intention is definitely not to refute such a very accurate argument, since small cells have already proven to positively impact capacity and qos. nonetheless, densification in the currently advocated form is not necessarily the answer for many operators and markets. for small cells to offer the expected capacity boost, interference must be managed by means of coordination occurring over the backhauling interface (referred to as the x2 interface). in order to be able to really capture the benefits of small cells, an ultra-low latency and robust x2 interface is necessary [cit] . this is true since the available lte spectrum bandwidth is typically limited to 10-20 mhz thus robust interference management becomes crucial. however, an ultra-low latency x2 interface may prove to be very costly. wireless x2 backhaul is also neither expected to be cost-effective nor practical. subsequently, the concept of densification quickly loses its appeal. this further entrenches the seriousness of the time-to-market and technology-tomarket dilemmas."
"to evaluate our platform, we set up a simulated environment that comprises a crowd which perform tasks and consumers who submit tasks and provide the feedback. we assigned a real skill set for each simulated worker to calculate the real quality outcome (which consumers report) using the suitability formula (see sect. 4.1). the platform management had no access to the real skills, but was only able to estimate workers' skills based on the feedback provided by consumers. we evaluated the average job quality in different setups, varying the workload of the crowd, the availability of workers, the scheduling algorithms, and the skill awareness."
"similarly, the output of alg. 5 is notice that the order of the elements is the order they are placed in the class by the algorithm. in addition, the output of alg. 5 will intentionally always produce duplicate classes in order to identify every tolerance class. this problem can be reformulated in terms of lagrange multipliers written as"
"the rest of this paper is organized as follows. section ii presents the recent iot applications' requirements and trends. section iii emphasizes the need of pushing intelligence to front-end devices based on practical and analytical arguments. section iv investigates the challenges and potential solutions for making iot devices smarter from a connectivity perspective. in specific, this addresses shortcomings of cellular networks, while also advocating the use of deviceto-device (d2d) communications. section v looks at the key performance metrics (kpms) which need to be considered when optimizing the network design while also discussing means to handle the inherent tradeoffs amongst them. to close the loop, collaboration across iot ads is tackled in section vi. the section aims to propose means for the iot front-end devices to share resources with each other even if they belong to different jurisdictions. next, the paper discusses in section vii software-driven device architectures which are perceived to enable the next generation of smart iot devices. finally, the paper is concluded in section viii."
"hence, the study of uncoordinated power control in the context of large-scale iot setups merits a considerable attention from the iot research community. a primary question herewith is whether the overhead associated with any coordinated power control mechanism justifies the move to an uncoordinated mechanism. accordingly, the application scenarios whereby the overhead associated with coordinated power control makes it suboptimal to uncoordinated power control should be investigated. for such scenarios, novel strategies for uncoordinated power control in noise-and interferencelimited scenarios can be devised. for example, the optimization of power control techniques by means of introducing a slow base-station-triggered feedback message. said message may contain location information of devices, form factors, historical traffic patterns, and rf measurement reports."
"the complexity of application-oriented iot system optimization is further deepened by the fact that applications have different key performance objectives. it is quite possible to be in a situation where devices belonging to two ads have utterly different objectives. for example, there could be a bandwidth-hungry delay-tolerant vibration sensor whose neighbor is a low-bandwidth delay-intolerant actuator."
"shannon introduced entropy as a measure of the amount of information gained by receiving a message from a finite codebook of messages [cit] . the idea was that the gain of information from a single message is proportional to the probability of receiving the message. thus, receiving a message that is highly unlikely gives more information about the system than a message with a high probability of transmission. formally, let the probability of receiving a message i of n messages be p i, then the information gain of a message can be written as"
"in the simulation, the devices apply a random access channel (rach) procedure to determine when they can start their transmission with the enb. then, they transfer their data transmission depending on the channel quality indicator (cqi). we notice, from fig. 4, important gains are reached compared to the traditional scenario in terms of transmission time and power consumption mainly for dense cell. indeed, for low density of devices, the probability of collaboration and socialization is small. then, as the number of devices increases, more collaboration either within the same ads or between different ads is performed. therefore, more devices exploit those with better channel quality to complete their transmissions and hence, the risk of collision in rach procedure is reduced and data is transferred in a faster way as the selected cqis, on average, will be higher. for instance, the transmission time and the power consumption are reduced by around 15% for dense cells. the socialization scenario offers more gain compared to the collaboration within same ads scenario. this appears mainly in the time transmission which is slightly reduced as there is more flexibility for devices' collaboration."
"as each worker is individual, it is natural that the skills of the workers are manifold. the tasks submitted to the platform are also diverse in their requirements. hence, efficient crowdsourcing must consider the suitability of a worker for a task. one can assume that the more the worker is suitable for a task, the better the expected outcome quality is. therefore, given the suitability information and a control mechanism for a worker assignment, it is possible to improve the overall result's quality by assigning tasks to best suitable workers for the current situation. moreover, from a bpm perspective, this mechanism also provides control over task completion times which can be used to maintain objectives, such as deadline fulfillment. to sum up, the task assignment control would enable a crowdsourcing platform provider to develop qos policies for offered crowdsourcing services, so these services can be integrated into qossensitive business processes. the assignment control demands for a scheduling problem to be solved. the problem is to maximize overall quality while satisfying agreed objectives. such a scheduling problem is hindered by a number of crowd-specific features, such as lack of full control of the workers and their membership, and their limited predictable availability."
"the main objective of this paper is to outline a framework for the design of iot systems which are optimized from an applications point of view. such a framework is primarily driven by front-end devices that are simply intelligent and empowered to make decisions. to achieve such an objective, the problem can be viewed from three distinct yet interconnected perspectives as shown in fig. 5 . one of the shortcomings which seems to plague current iot research and development (r&d) efforts concerns is the lack of holistic yet sufficiently deep approaches in treating the problem. indeed, there is often considerable inertia towards embracing the heterogeneity and diversity of iot devices as an undeniable fact. attempts to unify physical layer (phy) techniques and networking protocols are quite unlikely to succeed. in contrast, this paper proposes to embrace diversity and difference. it also strongly advocates for the development of ''socialization'' schemes enabling devices to share resources even if they belong to different ads. we believe that such a holistic and neutral approach is novel in itself."
"this study evaluated the sensitivity and specificity of the new blackberry wmms, which only used internal sensors and cell phone video, for identifying mobility activities for able-bodied individuals."
"based on the above, we believe that high-bandwidth and/or delay-intolerant iot applications are rapidly forming shape. nonetheless, they are highly overlooked in the current research and standardization work on iot. for example, the 3rd generation partnership project (3gpp) recently launched a narrow-band initiative for handling massive iot roll-outs [cit] . while this indeed tackles certain classes of iot applications, it is certainly not all-inclusive."
"the main goal is to drive towards a comprehensive framework for designing and optimizing iot systems for a wide range of applications. there are indeed massive efforts to create a common service layer specification such that interoperability between iot systems becomes possible [cit] . the most noteworthy of such efforts is onem2m which has recently release its first common service layer specification [cit] . without any doubt, the ongoing efforts within the context of onem2m offer a great value for application developers by means of creation of a common service layer. however, the service layer on top of which applications are developed reside in the back-end of iot systems. as such, there is still a dire need to bring intelligence to the front-end of the iot by investigating means for smart iot devices to communicate, collaborate, and socialize at the edge."
"thus, the more worker's skills are proportionally closer to the required skills of a task, the more the worker is suitable to the task. if the worker's skill is equal or greater than the corresponding required skill, then this skill suits perfectly."
"obviously, the underlying iot application is the starting point. iot design and optimization tools need to cater for qos criteria mandated by the applications as set forth by end users. on the other hand, there is no doubt that without a reliable and fast connectivity pipe, the iot will fail to deliver its promise. the third perspective, i.e., collaboration, stems from the fact that iot devices within proximity are naturally expected to belong to different ads. rather than working in silos, collaboration across ads is poised to offer substantial gains. collaboration is essentially a catalyst factor which leads to the abundance of connectivity or computational resources available to a given iot locale."
"indeed, the recommendations and outlines drawn herewith are driven by the need for practicality and shorter timeto-market cycles. consequently, techniques advocated and proposed are non-invasive. in other words, they can be conveniently implemented by industry on off-the-shelf (ots) gear without necessitating major changes to the underlying hardware. this is an essential property characterizing this work in contrast to much of the work carried out in the research community under this realm."
"the current iot system model implies a back-end core where most of the management and operational decisions are executed. this is not limited to critical decisions, but also often includes operational and routine management actions [cit] . this is manifested in the following dimensions:"
"the internet-of-things (iot) continues to evolve as an indispensable component for economic and social development. the iot in fact is increasingly becoming a vital enabler of innovation in various application verticals touching every aspect of our lives. for instance, it is considered as an instrumental tool for the transformation of factories to implement further automation, in what is often referred to as the factory of things (fot) [cit] . moreover, iot systems are expected to step up the challenge of carrying the large amounts of data generated by wearable devices used for healthcare services [cit] . it is becoming very essential for different other applications such as municipal services, crowd and traffic management, environmental services, and personalized services [cit] ."
"scheduling is performed based on deadlines and skill requirements also derived from an sla. the objective of the scheduling module is to maximize the overall quality, while fulfilling deadlines. the assumption is that missing a deadline can not be justified by any quality gain, thus, meeting a deadline is the first-priority objective, and the quality maximization is the second-priority objective. algorithm 1 describes a scheduling algorithm which is used in our platform. the idea behind the algorithm is that the best quality is achieved when a task is assigned to most suitable workers. the quality is higher when a task is performed by a smaller number of best workers, but this number should not be too small, so the task can be finished until the deadline. this number is calculated in tot ake for each active task. the tasks with earlier deadlines are assigned in the first place. in an attempt to improve the algorithm's efficiency, we tried a number of heuristic extensions, such as:"
"1. a consumer submits a task that consists of manifold similar jobs. the consumer also specifies required skills and the deadline for the task, so all the jobs should be processed until this deadline. the task is added to active task pool. 2. the scheduling module periodically assigns active jobs to workers according to deadlines and suitability of workers. 3. when a job is done, the result is sent to the consumer. 4. the consumer can provide a feedback about the result quality. it is reported to the skill profile updating module, which matches it with the expected quality and corrects the worker skill profile if necessary."
"this section addresses the recommendations and challenges of the implementation of all the techniques presented above. modularity, reconfigurability, flexibility, and agility are all key design features expected in future iot setups [cit] . consequently, software-defined architectures become a natural candidate to enable smart iot devices' features. indeed, the benefits of implementing such software-driven architecture in iot systems is becoming more and more recognized in various iot applications [cit] thanks to their ability in effectively integrating robust control and communication platforms that simultaneously provision different classes of iot traffic. moreover, software-defined architectures enable the management of open, geographically distributed, and heterogeneous infrastructures. the objective would be to develop software tools and utilities enabling iot devices' intelligence while exploiting the benefits offered by softwaredefined architectures allowing socialization, collaboration, and dynamic retune of the operational parameters according to the end users performance targets."
"as an example of the degree of nearness between two sets, consider fig. 1 in which each image consists of two sets of objects, x and y, that are subsets of the universe of objects o. each colour in the figures corresponds to an elementary set where all the objects in the class share the same description. the idea behind eq. 1 is that the nearness of sets in a perceptual system is based on the cardinality of equivalence classes that they share. thus, the sets in fig. 1(a) are closer (more near) to each other in terms of their descriptions than the sets in fig. 1(b) ."
"another key design objective that is strictly needed to the application of front-end intelligence relates to the emphasis on producing applicable solutions. this is better illustrated by means of examples. when it comes to cellular network optimization, the developed algorithms shall be streamlined to the ongoing or roadmap standards platforms. otherwise, deviation from that shall render any outcome almost useless from a practical point of view since it cannot be applied in the field. another example is to focus on software-driven solutions [cit] . while hardware is notoriously known to be quite intolerant to changes, software-or firmware-based platforms offer sufficient elasticity [cit] for the implementation of new techniques."
"in this work we position crowdsourcing in a service-oriented business setting by providing automation. in crowdsourcing environments, people offer their skills and capabilities in a service-oriented manner. major industry players have been working towards standardized protocols and languages for interfacing with people in soa. specifications such as ws-humantask [cit] and bpel4people [cit] have been defined to address the lack of human interactions in service-oriented businesses [cit] . these standards, however, have been designed to model interactions in closed enterprise environments where people have predefined, mostly static, roles and responsibilities. here we address the service-oriented integration of human capabilities situated in a much more dynamic environment where the availability of people is under constant flux and change [cit] . the recent trend towards collective intelligence and crowdsourcing can be observed by looking at the success of various web-based platforms that have attracted a huge number of users. well known representatives of crowdsourcing platforms are the aforementioned form yahoo!, liveops, and amazon. the difference between these platforms lies in how the labor of the crowd is used."
"in our future work we will focus on workload and workforce availability predictions, sla negotiation, and pricing in such scheduled crowdsourcing platforms. also, we plan to extend the boundaries of our previous work [cit] to consider the scenarios where the slas between the crowdsoucing platform and a business process engine can be negotiated beforehand in an autonomic and adaptive fashion."
"the adverse implications of a highly-centralized paradigm can be mainly assessed from two distinct perspectives: 1) networking efficiency, and 2) computational efficiency. five kpms are considered herewith, namely: energy, latency, throughput, scalability, and reliability. a centralized networking paradigm is notoriously known to lag in terms of these kpms. it is straightforward to argue that the centralized paradigm incurs higher latencies. this is manifested in longer trip times (in case of single-hop cellular systems) or larger number of hops (in case of mutlihop d2d enable systems). as a result, centralized systems are indeed associated with higher aggregate energy consumption per bit and lower throughput per iot device. similarly, it is intuitive to argue that centralized systems do not lend themselves to scalability and high reliability. 1 on the other hand, centralized back-end systems raise the bar in terms of capital expenditure while featuring lower computational capability [cit] ."
"the suitability is calculated as a match between required skills for the task and the skills of a worker. the skills of crowd workers are maintained in their profiles by the platform management. initially, skill information is provided by the workers themselves."
the aim of the experiments is to show the advantages of the platform's architecture. the first experiment type demonstrates the convenience of skill-based scheduling to the ordinary (random) task assignment. the second experiment type gives evidence of the skill update mechanism efficiency.
"as the user interface is not the focus of our work, the communication with consumers and workers is performed by means of web services. the full implementation of the platform can employ, e.g., a web interface developed on top of these web services."
"1) extensive interference in the d2d layer: excessive use of the unlicensed spectrum for d2d communications eventually leads to raising the interference levels and adversely impacts performance. 2) unfair battery depletion rates: maximization of ul throughput inevitably leads to the rapid depletion of the battery of the virtual access point (ap), and consequently resilience from end users due to degraded qoe. as such, ues need to take turns in assuming the virtual ap role for the sake of fairness. 3) accounting and billing challenges: the bandwidthsharing scheme entails intrusive changes to the backend accounting system in order for fair billing of data usage. moreover, there is an additional intricate challenge related to the qoe of end users, and specifically battery depletion. while taking turns in assuming the virtual ap role is expected to diminish the effect of battery depletion, it is also expected to diminish the attainable gain in terms of ul throughput (ues with low sinr will eventually assume the ap role). an alternative strategy would be to incentivize / compensate the ue with the best sinr to maintain its role as the virtual ap. therefore, the recommendation is to develop a parallel plane approach wherein devices within proximity can rather coordinate their access schedules and hence, relax the ul random access channel constraints. the goal is to introduce the concept of underlay/overlay coordination, borrowed from cognitive radio communication, and apply them at the control plane level whereby iot devices may collaborate together, as illustrated in fig. 7 . the goal herewith shall be to reduce collision probability and improve the radio access procedure in the context of large-scale system. this can be applied among devices belonging to the same ad. in the case of devices belonging to different ads, game-theoretic approaches might be involved to assess tradeoffs between altruistic and greedy paradigms. hence, optimization schemes can be developed to attain the benefits of the underlay coordinate plane in terms of the key metrics, namely ul enb capacity, mean channel access waiting time, and packet time-out rate."
"the first consideration focuses on determining radio access network settings and parameters for optimal ul performance. in other words, the objective is to optimize the ul resource allocation in the context of highly dense cellular networks where machine-type communications (mtc) and humantype communications (htc) co-exist. indeed, in spite of their low complexity, traditional frequency-hopping-based approaches do not guarantee enhanced system performance mainly for large-scale iot setup as their methods are channelindependent. therefore, there is a need to develop channelaware resource allocation approaches aiming to perform the allocation according to the achieved signal-to-interferenceplus-noise ratio (sinr) of different users. hence, resource allocation is performed depending on the requirement of the users' applications while considering the key evaluation metrics, namely ul enb capacity, mean channel access waiting time, and packet time-out rate."
"martphone platforms provide an ideal interface for mobility assessment in the community (home, school, shopping, etc.). a previous project showed that synchronized sensors in a \"smart-holster\", combined with blackberry gps and pictures, were useful as a wmms [cit] . new blackberry devices that integrate an accelerometer and video camera provide an opportunity for mobility analysis using only integrated sensors."
"in this example, we have incorporated three advantageous options thanks to the front-end intelligence: in-situ decisionmaking, socialization among different ads, and optimized connectivity. we propose to quantitatively investigate the gain in terms of latency and energy as selected kpms. it is quite intuitive to conclude that the number of transactions involved in the back-end paradigm is substantially higher. thus, backend paradigms are inferior in terms of energy and latency performance, and may eventually jeopardize performance mandates or targets set forth by the underlying application. such an intuition is further detailed in the sequel."
"a final example of a bandwidth-hungry and delay-intolerant industrial iot application is wireless seismic surveys [cit] . thousands of seismic sensors (typically called geophones) are spread over large swaths of area for the sake of creating an image of the hydrocarbon reservoirs underneath earth's surface [cit] . generally, each sensor generates at least 12kbps of sustained upstream throughput in a continuous fashion. the upstream throughput demand is magnitude of order higher in case of seismic imaging during fracturing (fracking) processes. on the other hand, there exist industrial applications which are low-throughput but highly delay intolerant. this mainly includes closed-loop control and automation applications [cit] . upon the occurrence of a certain event, other sensors and actuators need to be notified very quickly in order to drive the process to a safe shutdown or emergency outage state."
"an essential task from the perspective of connectivity entails rigorous evaluation of cutting edge cellular, d2d, and multihop networking in light of large-scale heterogeneous iot deployment setups. the evaluation is mainly approached from a capacity-oriented point of view. the emphasis on capacity is in line with the recently observed rise of bandwidth-hungry ul-centric applications mainly involving video transmission, as highlighted in section ii. with that in mind, it is illustrated in this section that the ul interface of lte networks is inevitably going to suffer from a ''capacity deficit''. accordingly, the aim is to devise techniques and methods which will help alleviate such a shortcoming."
"-based on reported short-time worker availability, assign less jobs at a given time to wait for more suitable workers to become available (while avoiding possible crowd \"overloads\") -assign more jobs at a given time if the suitability of additional workers is almost as good as the suitability of best workers. -having tot ake numbers calculated, optimize the worker-task assignments for each time period using an optimization framework."
"this application is a striking example of bandwidth-hungry delay-intolerant applications. in order to classify a crowd (e.g., gender, age, ethnicity) or estimate its parameters (e.g., density and flow intensity), high-definition (hd) video needs to be streamed and analyzed [cit], mostly in realtime. such a task mandates the installation of a large number of cameras covering the crowds from various angles. the higher the spatial resolution of any computational task, the more cameras will be obviously required. fig. 1 depicts a few examples of applications associated with management of massive crowds."
"to close the loop, collaboration across iot ads is tackled in this paper. there is growing evidence that such collaboration is expected to help meet end user objectives while concurrently relaxing the aforementioned operational-application tradeoff. consequently, the aim is to discuss the collaboration between devices across ads. in particular, the goal is to propose means for the iot front-end devices to share resources with each other even if they belong to different jurisdictions."
"the resulting figures show how an important economic sector as the agriculture is significantly behind the country's average when it comes to broadband availability. the presented conclusions may contribute to enhance or speed up the development of broadband in rural areas, now that some concrete numbers about the situation are available. the improvement of broadband availability in farms, and by extension for the surrounding rural population, may require efforts from all the parties involved (farmers, carriers, public institutions...), as the geographical location may not allow to apply a traditional market driven plan."
"in previous work, [cit] it has been shown that lesion detection accuracy can be improved by taking symmetry information of the contralateral breast into account. in refs. 30 and 31, this symmetry information was incorporated in a patch-based scheme. we followed a similar approach to exploit the symmetry information in a patch-based fashion. for each candidate location in a breast, we computed the corresponding location in the contralateral breast, using the breast masks obtained in the breast segmentation stage. as shown in fig. 4, we initially computed the center of gravity (cog) for each breast. we considered the cog of a breast as the origin of a 3-d coordinate system for the given breast. the coordinate systems for right and left breasts were mirrored with respect to the median plane. the location of the candidate in this coordinate system was applied to the coordinate system of the contralateral breast to find the corresponding location of the candidate."
"secondly, fig. 3 presents the quantification of the broadband divide for the danish farms. the four mentioned groups are presented, accessibility of connections of at least 30 mbs and of at least 10 mbs. again, the results are illustrated for each of the five regions and also for the whole country, and the obtained values are presented in terms of percentage of covered farms/households. as an illustration, looking at the right bar in fig. 3(a), above 90% of the danish households have access to a connection of at least 10 mbs downstream, but only 40% of the farms have it available. it is important to remark that the values for the country's households are taken from an external broadband report, and the values for the farms are calculated specifically for this study."
"we reported the performance of the proposed system for different lesion types (mass-like and nonmass-like lesions) and for different lesion subsets (screening-detected, prior-visible, and prior-minimal sign lesions)."
"the results of this study show that even though den-mark is one of the top countries in terms of broadband accessibility, danish farms are clearly behind in this aspect compared to the rest of the country. these results might open possibilities for new initiatives towards improving the broadband access for farms, and by extension for rural areas in general. in addition, the methodology used can be extrapolated to perform similar studies in other geographical regions or different professional sectors. the rest of the paper is organized as follows: section ii summarizes important concepts and definitions in relation to the studied topic together with a brief literature review. section iii presents the broadband divide analysis, covering the scenario, methodology, and results. section iv highlights the most relevant conclusions of this work."
"to reflect a real screening situation, we evaluated the performance of the developed cade system on a test set including cancer and normal cases from woman participating in a highrisk screening program with mri. however, the training set did not include screen-detected cancers because we did not have them available at the time of the study. occurrence of malignant lesions in screening breast mri scans is much less frequent compared with the diagnostic mri scans. the test set included lesions that were visible or had minimal signs in prior scans, which allowed us to test the performance of the system for lesions that were overlooked or misinterpreted by the radiologists. the most remarkable increase in detection performance was observed in lesions visible in prior mri scans (see fig. 7 and table 1 ). the proposed cade system was able to detect 60% more lesions compared with the conventional cade system at the threshold equivalent to 1 false positive per scan. this suggests that the proposed cade system might be more useful to prevent reading errors."
"the results of the analysis are summarized in the following paragraphs. firstly, fig. 2 presents the coverage of farms in relation to their loop length for the five regions of the country. it can be clearly identified how the coverage is more or less uniform across denmark. the maximum difference between the curves is around 10%."
"in relation to the obtained results, only 40 % of the farms have access to a connection of at least 10 mbs downstream while for the rest of the country this value goes above 90%. for the rest of the studied groups (at least 30 mbs upstream and at least 10 mbs up/downstream) the percentage of covered farms is always below 10 %."
"a fully automated breast segmentation method based on deep learning 22 was used in this study to segment the breasts in mri volumes. this method is based on a two-dimensional (2-d) u-net architecture. 26 u-net is a fully convolutional network, which produces \"dense prediction.\" in the other words, for each pixel or voxel in the input image, u-net generates a likelihood value, which is a useful property for segmentation or detection problems. in a u-net, this is achieved using the de-convolutional part of the network that comes after the convolutional part, where the output of the convolutional part is up-sampled. each axial slice is provided individually to this algorithm to generate the corresponding likelihood map, where a voxel value in this map indicates the likelihood of the voxel to belong to the breast. the final segmentation for an axial slice is obtained by thresholding the likelihood values at 0.5. the 3-d segmentation of the breasts is obtained by combining these 2-d segmentation slices into a 3-d volume."
"this paper analyzes the broadband divide for the danish agricultural sector. usually, digital divide type of studies are carried at a regional or zip code level, however this approach does not reflect the real situations of farms as these do not contribute significantly to the regional statistics. instead, a more detail framework is proposed and each farm in the country is treated as an individual entity. then, the broadband possibilities for each of them are calculated. the obtained results are compared to the general broadband statistics for the whole country, in order to quantify the broadband divide suffered by the agricultural sector."
"our study has some limitations. we did not evaluate how the addition of contrast dynamic information may improve the performance of the presented cade system. although contrast dynamic information from the late-phase may not be available in a screening setting, fast mri acquisition protocols make it possible to obtain dynamic contrast uptake information in the early-phase of the acquisition, which does not cause an increase in the duration of the breast mri. it should be investigated in a further study whether the addition of such early-phase dynamic information can increase the detection performance of a cade system. additionally, we assumed that the two breasts have the same (mirrored) shapes to identify corresponding locations in the contralateral breasts. although this assumption holds for most of the cases, some patients may have asymmetric breast shapes. in the future, we will investigate more advanced registration methods that take into account the differences in shapes between the right and left breasts of a given woman."
"the analysis consists of a study of the farms in denmark distributed in five regions, the information about the farms has been taken from a publicly available industrial database [cit] . table i shows the numerical details about the scenario and the geographical information used, and fig. 1 illustrates the five different regions."
"the following paragraphs describe the methodology followed to study the broadband divide affecting the farms. for the rest of the document the farms are referred to as nt (network termination), and the access points as ap ."
"in conclusion, we developed a cade system for breast mri that uses only the early-phase of the acquisition without using dynamic information from the late-phase t1w acquisitions. to fully exploit the spatial information obtained in the early-phase, we used 3-d morphology of the candidate regions and symmetry between the enhancements of two breasts of the patient, using a deep-learning approach. the proposed cade system significantly outperformed a conventional cade system which uses the full dynamic breast mri protocol. the developed cade system can be used in abbreviated mri protocols that have been suggested for mri screening programs."
"these concrete results cannot be extrapolated to other countries or regions, as the broadband coverage information and geographical parameters may differ significantly from the ones used for this danish study. however, the same methodology can be used together with the relevant input data (gis databases and reliable broadband statistics), in order to perform a similar analysis in other regions or for different sectors."
"we compared the proposed cade system to an existing cade system. 11 this system uses the full dynamic breast mri protocol: the precontrast and all registered postcontract images. it consists of the following steps: breast segmentation, candidate detection, and false positive reduction. for breast segmentation, an atlas-based method was used. for the first stage detection, a likelihood map of the mri was produced using a voxel classifier, which was trained using several re and blob feature maps. using this likelihood map, candidates were extracted using the local maxima algorithm, which was also used in the presented study. finally, false positives were reduced in a second-stage classifier, which used both morphological and contrast-dynamics information. note that this system was trained and tested on the same datasets as the ones used to evaluate the presented pipeline."
"briefly commenting the results, the divide in relation to downstream data rates is significant, as mentioned above less than half of the farms in the country have access to 10 mbs. only 7% could reach 30 mbs while 80% of the rest of the country has access to it. in connection upload data rates, the divide is even broader, only 7% of the farms have availability to 10 mbs, being 50% the availability for the whole country. also, less than 2% of the farms can have access to 30 mbs while this percentage goes up to 35% for the rest of the households."
"broadband divide: in this study, broadband divide is specifically referred to as the difference in between the broadband possibilities between two defined groups. in this case, the farms are compared to the rest of the country's average broadband accessibility."
"to study the effect of symmetry information on the performance of the proposed cade system, we trained another 3-d cnn that only uses the candidate patch coming from the suspicious breast as an input. the input for the contralateral patch and its corresponding stream was removed from the cnn and training was performed using the same hyperparameters. we refer to this system as cad-wos, where wos stands for \"without symmetry information.\""
"to detect candidate locations, we followed a similar approach as the one used to segment the breasts. we trained a two-level version of the same u-net that was used in the breast segmentation algorithm, using axial slices to identify the voxels that belong to a lesion. for this purpose, the lesion segmentations manually generated by the radiologists were used as the groundtruth. about 20% of the training dataset was separated at the patient level to be used as a validation set. the u-net was trained in batches of 40 randomly selected axial slices: 20 slices containing a segmented lesion and 20 slices without a segmented lesion. the slices for each batch were randomly selected from the entire dataset, where the total number of slices containing a segmented lesion was 4106. the loss function used during training was the log of the absolute value of the differences between the target and output likelihood values, averaged over the breast region of the given slice. we used glorot-uniform initialization 27 for weights of the u-net and rmsprop 28 for gradient-descent optimization. the initial learning rate was set as 0.001 and it was divided by 10 at each 1000 iteration. we continued training until the loss in the validation set was stable. the resulting trained u-net, given an mri axial slice, outputs a likelihood map for each voxel of the given mri slice to belong to a lesion. an example result for an mri slice is given in fig. 3 . subsequently, we obtained a likelihood volume for each mri, combining these 2-d likelihood maps. the final candidates were obtained by applying a local maxima algorithm 11 on the likelihood volumes."
cpm values increased in all lesion subsets compared with the cpm values obtained by the previous cade system. we observed the most remarkable performance improvement compared with the previous cade system in the prior-visible
"for each mri examination, postcontrast t1w volumes were registered to the precontrast t1w volume to correct for motion using the elastix toolbox. 25 subsequently, the subtraction volume was obtained by subtracting the precontrast image from the motion-corrected first postcontrast image. the relative enhancement (re) volume was also obtained, which is computed by normalizing the subtraction intensities relative to the precontrast intensities using the following equation:"
"we also tested the performance of the proposed cade system individually for mass-like and nonmass-like lesions (see fig. 7 and table 1 ) within the screening-detected subset of the test set. we observed a performance increase in the detection of both lesion types. however, the detection performance for nonmass lesions was still lower than that for the mass lesions. more training data of nonmass lesions and the development of dedicated algorithms may be needed to increase detection performance for such lesions."
"other factors may influence the performance of the copper based connections such as electromagnetic interferences (emi) [cit], but since these are dynamic in time, are not considered in this study."
"all lesions were annotated by radiologists in an in-house developed dedicated breast mri workstation, which includes a semiautomated tool \"smart opening\" 23, 24 for lesion segmentation. with this tool, a 3-d lesion segmentation is obtained after the annotator places a seed-point at the center of a lesion. annotators were able to add more seed-points when the result was not satisfactory especially for the large lesions. each lesion was classified by the annotators as mass or nonmass. motioncorrected subtraction volumes were used in this process. 11"
"the results in relation to broadband accessibility in farms for the different regions are consistent, illustrating that the global picture of the situation is similar all across the country. in addition, the broadband gap between the farms and the average broadband conditions in the country can be clearly identified and quantified from the obtained figures. if this gap was quantified between farms and urban areas, the difference could probably even be more significant."
"the mri scans included in this study were obtained from 1.5 or 3 tesla siemens scanners using a dedicated bilateral breast coil. a gradient-echo dynamic sequence was performed to obtain a t1w mri before the administration of a contrast-agent [gd-dota (dotarem, guerbet, france), at a dose ranging from 0.1 to 0.2 mmol∕kg]. within 2 min after the administration of the contrast agent, the first postcontrast t1w scan was obtained, which was followed by three to five additional t1w scans. acquisitions were either in transversal or coronal planes with pixel spacing in 0.664-to 1.5-mm range and slice thickness in 1-to 1.5-mm range. other mri acquisition parameters were 1.71 to 4.76 ms for echo time, 4.56 to 8.41 ms for repetition time, and 10 deg to 25 deg for flip angle."
"gis data: the work is carried out using real geographical information system data for denmark. the information consists of databases containing the location and other relevant information of farms, roads, and access points."
"in the first candidate detection stage, a set of candidates was obtained based on 2-d shapes and patterns since the u-net was applied in a slice-by-slice basis. in the second and final step, the likelihood for each candidate is further refined to reduce false positives. for this purpose, we employed a 3-d cnn that uses two types of information available in the re volumes: 3-d spatial (morphological) information in the local region around the candidate, and the information arising from the asymmetry between the enhancements of the two breasts."
"studies at zip code or municipality level are not detailed enough for the purpose of this study, as they reflect overall average results. moreover, focusing on farms, these do not significantly contribute to the average statistics due to their much lower number in relation to the total number of household in a studied area, and their very specific location (lowly populated areas). therefore, a feasible approach to identify the broadband situation for a specific sector with very special locations within a zip code or municipality region, is to treat each household/farm as an individual entity."
"the automated lesion detection system developed in this study, as shown in fig. 2, uses two images as inputs: the precontrast t1w volume and the re volume. given these inputs, the lesions are detected in three steps. initially, the breast is segmented based on the precontrast t1w image, and subsequently, this segmentation mask is applied to the re image. at the second step, a candidate detection algorithm uses the re image to search for possible candidate locations within the segmented breast. here, we define the term \"candidate\" as a voxel or location, which is expected to represent a lesion in its local neighborhood in the breast mri volume. at the final step, the candidates are classified to reduce the false positives of the previous stage."
"we used a validation set to determine the hyperparameters of the network and the training process and to monitor the performance during training. for this purpose, 20% of the training dataset was randomly selected at the case level. area under the roc curve (auc) was used to measure the performance in this set. we used glorot-uniform initialization 27 for weights of the network and rmsprop 28 for gradient-descent optimization. drop-out was applied to the last dense layer with a rate of 0.85 and a starting learning rate of 0.001 was selected. at each batch we used 32 positive and 32 negative candidates, which were randomly selected from the entire dataset. as the training continued, the learning rate was dropped by half when the performance in the validation set did not improve further for 50 epochs, where each epoch consisted of eight batches. the training was stopped when the performance in the validation set was not improved in the last 200 epochs. the final model was selected as the model resulting in the highest area under the roc curve (auc) on the validation set. for data augmentation, we applied random translations (maximum five voxels in each direction), rotations and mirroring to the input patches."
"however, it is difficult to find examples of studies carried out at a household level due to the magnitude of the problems. even to analyze small communities requires an enormous amount of time and computational resources, due to the size of the databases involved (households, roads and other relevant gis information). one example found in literature is [cit] where authors estimate the cost of three types of access technologies to provide high speed connections to most of the households in the state of victoria, australia."
