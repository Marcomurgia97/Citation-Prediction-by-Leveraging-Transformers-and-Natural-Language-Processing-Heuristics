text
"and the natural frequencies of the rst six orders before and after optimization are calculated and shown in table 3, in which we can get that the corresponding frequency has been increased 5% after optimization."
"thus, in this letter we present an overview of the design of an underwater acoustic modem starting with the most critical component from a cost perspective-the transducer. the design substitutes a commercial underwater transducer with a homemade underwater transducer using cheap piezoceramic material and builds the rest of the modem's components around the properties of the transducer to extract as much performance as possible. the goal of the design is to provide a low-cost, underwater acoustic modem that will be suitable for short-range sensor network applications."
"the power consumption of the analog-to-digital converter (adc) and digital-to-analog converter (dac) are specified to be determined (tbd) as the devices on our evaluation board are over specified and too power consuming for our low-power design. in the future, to further reduce power consumption, we plan to explore the possibilities to provide signal detection at even lower power levels. this is paramount to building a modem that has low listening power, which is also a key requirement to ensure long lifetime on a limited battery supply. we plan to eventually utilize a design that has a programmable gain, which is dynamically controlled by the digital hardware platform. in addition, further changes to the circuit design of the transceiver will be made to further increase its efficiency."
"(2) e in uence factors a ecting the performance of the aerostatic spindle were evaluated, whose in uence degree were obtained by cross-correlation analysis. and the correlation coe cients of more than 0.7 were determined, which are imported into the optimization process as the independent variables. (3) e dynamic characteristics after optimization were analyzed, which show that the optimization method proposed in the paper provided e ective parameters for the aerostatic spindle. e corresponding natural frequency and the response speed have been increased when the response amplitude is in the same order magnitude with the value before optimization. e optimization of aerostatic bearing is proceeded combining the theory with microscale characteristics."
"scanning was conducted at odense university hospital, denmark, on a philips achieva 3.0 tesla mri scanner. whole-brain functional images were acquired using a gradient-echo planar imaging sequence [45 axial slices with 3 mm thickness and 2.625*2.625 mm in-plane resolution, repetition time ( fmri data pre-processing and analysis data pre-processing was performed using fmri expert analysis tool (feat) version 6.00, from fmrib's software library [cit] . head motion was corrected using mcflirt [cit], before linear trends and low-frequency drifts were removed (high-pass filter of 0.011 hz)."
"cost more than $8000 dollars [cit], and require a minimum of 4 w transmit power. in order to make more short-range underwater acoustic communication networks a reality, the cost and power consumption of underwater acoustic modems must come down."
"underwater transducers are typically made from piezoelectric materials-materials (notably crystals such as lead zirconate titanate and certain ceramics) that generate an electric potential in response to applied mechanic stress and produce a stress or strain when an electric field is applied. the piezoelectric transducer needs to be encapsulated in a potting compound to prevent contact with any conductive fluids (fig. 2) . commercial transducers are expensive, on the order of thousands of dollars, due to timeconsuming characterization and the subsequent optimization of the analog circuitry, which is further exacerbated by low volume production. however, the raw piezoelectric materials themselves are not very expensive and alternative cheaper, less accurate ways of characterizing and manufacturing the transducers exist. the key to our low-cost design is to forego a commercial underwater transducer, and use a homemade one built from low-cost piezoelectric material. the rest of the system is built around the properties of this inexpensive transducer to extract as much performance as possible."
"in this study we compared the participants' answers when they interact with virtual agents in both ivr and semi-ivr systems and found significant outcomes. firstly, in most questions, participants gave similar answers for both systems: they found it less comfortable and more difficult to navigate in the vr with collision avoidance than without. secondly, extending the relationship between the user and the virtual agents with some basic level of interaction between them made the user's experience even more positive. the evaluation of all examined factors by the user was considerably better when there was a basic level of interaction with the virtual crowd. the behavior of the crowd was perceived as more realistic and the user reported a stronger sense of presence. this finding is even more important in the semi-ivr system since in some questions (awareness of the others, awareness of himself, realism of the environment, sense of presence) the differences were greater. this can be explained if we consider that in a semi-ivr system the user is not fully immersed and the factors that contribute to the plausibility of the system and the sense of presence are more \"weak\" than in an ivr system. thus, the introduction of collision avoidance accompanied with basic interaction between the user and the virtual agents is perceived by the users in some aspects as more important, in terms of realism and sense of presence in our semi-ivr system."
"further work is necessary to study more factors that affect the user experience when they participate in populated ive with virtual agents in both ivr and semi-ivr systems. for example, it would be useful to examine the plausibility of different agents' group formations and sizes and if these affect differently the users in an ivr from the users in a semi-ivr."
"the division of labour between the hemispheres in visual processing has become a central point of dispute in recent years, in particular with regards to development of potential category-sensitive areas and cerebral competition/neuronal recycling of areas involved in face and word processing [cit] . while theories diverge on the assumed degree of category sensitivity in such areas, and the learning mechanisms involved, a core hypothesis is that learning to read may capitalize on cerebral areas involved in face recognition. in that light, it is interesting that we find clear differences between dps and controls in left lateralized areas involved in face processing, but no difference in activation for words or in behavioural reading responses. this stands in contrast to patterns revealed in developmental dyslexia (a disorder affecting reading acquisition), where recent studies have revealed behavioural impairments in recognition of faces as well as words [cit] . [cit] found lower activation in the vwfa and right ffa, respectively, in dyslexic children. in comparison, then, dp appears as a more selective developmental deficit than dyslexia both in behavioural and anatomical terms. whether the left hemisphere abnormalities in dp observed here are related to the suggested cerebral competition or neuronal recycling that may occur when learning to read will be a central question in future research. further characterization of the unique contributions of left hemisphere areas in normal face recognition will also be helpful in understanding better how abnormalities in this part of the network may contribute to the behavioural deficits in face recognition seen in dp."
"the design was implemented and successfully tested on a dini group dnmeg ad-da evaluation board [cit] . hardware simulation results with packet lengths of 10 000 symbols achieved a bit error rate of at 10 db snr. transmitting at 35 khz, given a transmit source level of 170 db re 1 upa and a noise level of 50 db re 1 upa, using the passive sonar equation with no directivity index, the signal may obtain 10 db snr at 1140 m. packets of 100 symbols were sent underwater in a 12-inch plastic bucket and achieved 0% bit error rate. table ii shows the fpga hardware resources required for the components of the acoustic modem design with standard optimization. these resources are reported for a xilinx spartan 3 xc3s4000 fpga. the power consumption, determined by xpower estimator 9.1.03, is 0.379 w. the design offers a"
"e bearing capacity and sti ness of the aerostatic spindle are a ected by many factors, including structural and working parameters. next, we will evaluate the e ect of the main factors on the spindle performance."
"e bearing capacity and sti ness of aerostatic spindle are optimized by using genetic algorithm toolbox in simulation system software that combines the optimization theory and genetic algorithm. first, the performance of aerostatic bearing is optimized by the single objective function of the capacity and the sti ness. e optimization results are shown in table 1 ."
"in this paper, the performance of aerostatic spindle has been improved by a novel optimization approach, and the results of before and after optimization are compared in static and dynamic aspect. in the optimization process, the main factors a ecting the bearing capacity and sti ness of aerostatic bearings are determined by cross-correlation analysis, and the corresponding actual experimental platform of the aerostatic spindle system is established for analysis. e following major conclusions are obtained:"
"e aerostatic spindle is widely used in precision and ultraprecision machining by the advantages of low friction and high accuracy [cit] . when the aerostatic bearing is working, the air supply is provided by an external air compressor."
"all participants in control group 1 performed within 2 [cit], and also within the normal range of the danish reference sample on both the cfmt and the fir scale. for participation in the scanning part of the study, the controls received gift certificates of 400 dkk ($60 usd)."
"where ω n is the natural frequency, m is the mass of the spindle, which is 3.7 kg, and c is the damping coe cient. e solution of above equation is as shown."
". e industrial applications require high bearing capacity and sti ness, but their values vary with design variables. erefore, the multiobjective optimization containing the bearing capacity and sti ness is proposed, and the multiobjective optimization functions f(x) including the maximum bearing capacity function f 1 (x) and the maximum sti ness function f 2 (x) are established."
"a power management circuit is provided to adjust the output power in real-time to match it to the actual distance between transmitter and receiver. the ability to provide a low-power output has several important benefits: less interference for nearby ongoing communications; reduced noise pollution; and considerable power savings. the current configuration of the transmitter is equipped with a power management system that can switch between output levels of 2, 12, 24, and 40 w, which is efficiently coupled into the transducer in over its operating frequencies. the power management system has been designed so that the transmitter will maintain maximum efficiency over this wide range of power output levels. the system is controlled by a low-current 5 v signal from the digital hardware platform so that the power may be dynamically controlled for different operating conditions."
"the receiver's architecture consists of a set of narrow (high-q) filters with high gain. these filters are based on biquad band-pass filters, and essentially combine the tasks of filtering and amplification. the receiver is configured so that it only amplifies signals around 35 khz (to match the resonant frequency of the transducer), while attenuating low frequencies at a rate of 120 db per decade and high frequencies at rate of 80 db per decade. the receiver must be able to amplify only the frequencies of interest because of the large amount of noise associated with underwater acoustic signals. the current receiver configuration consumes about 375 mw when in standby mode and less than 750 mw when fully engaged. the relatively high power consumption [in comparison to that of the woods hole oceanographic institute (whoi) micromodem (200 mw)] is a result of the receiver's high gain (65 db) which is capable of sufficiently amplifying an input signal as small as 10 mcv allowing the receiver to pick up signals at longer distances. an ultralow-power wake up circuit will be added to the receiver to considerably reduce power consumption. a few receiver component values can be changed to widen its bandwidth (but decrease its gain) to allow for transmission of modulation schemes that require more bandwidth."
the dps received gift certificates of 400 dkk ($60 usd) for participating in the scanning part of the present study but did not receive remuneration for participating in the behavioural tests.
"immersive vr system: a cave-like projection based system where participants' heads were tracked with an intersense is 900 tracker, and the navigation was facilitated through a wand."
"to examine whether the dp group performed differently than control group 1 on the face measures (cfmt, feq-fir and cfpt), their scores were subjected to independent t-tests. the same procedure was applied to examine whether the dp group differed from control group 2 on the object recognition (rt and a) and reading measures (rt and wle). the 95% ci's and the independent t-tests were based on bias-corrected and accelerated bootstrapping (1000 samples). bootstrapping is based on taking n random samples with replacement from the sample data and estimating properties of the sampling distribution from these samples. as an example, the 95% ci of the mean can be estimated by looking at the variance of the mean found across n (bootstrap) samples of equal size to the sample data. compared with traditional parametric estimates, bootstrap estimates are less affected by bias (e.g. outliers and violations of homoscedasticity and normality)."
"e bearing capacity of the aerostatic spindle mainly comes from the gas film between the aerostatic bearing and the spindle. for ultraprecision machining, the stability of aerostatic spindle should be guaranteed. in fact, the performance of the aerostatic spindle is completely different due to some factors, such as structural parameters of air cavity, orifice diameter, gas film thickness, and air supply pressure. and the performance of the aerostatic spindle is a key factor affecting the quality of the machine [cit] . besides, it will be also directly reflected to the surface of the workpiece and affect the quality of the workpiece. erefore, it is important to study the performance of the aerostatic spindle to improve the precision of ultraprecision machining."
"e dynamic characteristics of the aerostatic spindle are important in actual work, and the performance in axial direction is the key factor a ecting the machining accuracy shock and vibrationof the machine tool. in order to better verify the optimization e ect of aerostatic bearing, the axial dynamic response is analyzed. e motion equation which has natural frequency and damping coe cient is proposed."
"eoretical calculations show that the factors a ecting performance of aerostatic spindle include the bearing clearance h, ori ce diameter d 3 and the diameter of ori ce distribution circle d 2, the depth of the air cavity d and the pressure supply p s . however, the in uence extent of these factors is di erent. for example, some factors improve the performance of the aerostatic bearing, and others reduce the performance."
"it can be seen from table 1 that the contradiction is present in the optimal result of the sti ness and load capacity function, respectively. us multiobjective optimization function is used to optimize the performance of the aerostatic spindle in order to get ideal consistent results."
"the second aim of our study was to directly address potential differences in activation in the vwfa between dps and controls. activation patterns for words, and analyses looking directly at the vwfa, have not previously been reported in dp. this comparison becomes even more interesting as the key abnormalities we find in dp also involve the left hemisphere. during presentation of orthographic stimuli, however, we found no evidence of abnormal activation in the vwfa in dps compared with controls. this pattern corresponds well with behavioural reports of preserved word processing in dp [cit], and also with the present behavioural findings showing normal word reading rts and wles in the dps. this suggests that dp is not associated with reduced activation in category-sensitive areas in general; not even in areas like the vwfa which is located quite near/partially overlaps with the left pffa where there was a clear difference in activation between groups. [cit], we also found no significant reductions in activation for the dps in left or right lo during object processing."
"1943-0663/$26.00 © 2010 ieee fig. 2 . from left to right: the raw piezoelectric ring ceramic, the potted ceramic, and the transducer in the potting compound mounted to a prototype plate to be attached to a modem housing."
"the analog transceiver (fig. 4) consists of a high power transmitter and a highly sensitive receiver both of which are optimized to operate in the transducer's resonant frequency range. when produced in large quantities, the estimated cost of the transceiver is $125 dollars. the transmitter and receiver portions of the analog transceiver are described in more detail in the following subsections."
"our digital hardware platform makes use of reconfigurable hardware, particularly a field-programmable gate array (fpga), for all the digital signal processing and control required for the modem. reconfigurable devices provide a suitable digital hardware platform for the low-cost, low-power underwater acoustic modem as they strike a balance between solely hardware and solely software solutions. they have the programmability and nonrecurring engineering costs of software with performance capacity and energy efficiency approaching that of a custom hardware implementation [cit] . reconfigurable systems are known to provide the performance needed to process complex digital signal processing applications and especially provide increased performance benefits for highly parallel algorithms [cit] . furthermore, they are programmable allowing the same device to be used to implement a variety of different communication protocols. unfortunately, they require substantially longer design time than a dsp."
"e dynamic performance is solved by finite difference method. first, the perturbation reynolds equation is discretized and then meshed. finally, the boundary conditions of steady state and perturbation are introduced into the equation to solve the problem. by substituting the steady pressure p a and the steady film thickness h 0 obtained from the steady-state solution into the perturbation equation, the dynamic pressure and the dynamic film gap can be obtained. p b and p c can be obtained by solving equation (5)."
the dp participants and controls provided written informed consent according to the helsinki declaration. the regional committee for health research ethics of southern denmark has approved the project (project-id: [cit] 0134).
"parameters. by optimizing the parameters, the static performance of the aerostatic spindle is improved; however, the dynamic performance of the spindle can be a ected. is is because the corresponding frequency varies with the change of bearing capacity and sti ness. here, the combin14 spring-damper unit is used in modal analysis of the spindle model, in which the thrust bearings can be seen as the longitudinal spring-damper. e node of each ori ce in the gas cavity is only stretched or compressed and not a ected by bending and torsion. e radial bearings can be seen as the longitudinal and torsional spring-damper, and the node of each ori ce in the gas cavity has the rotary element with three degrees of freedom in addition to the tensioncompression in the normal direction. e model of aerostatic spindle is shown in figure 14 ."
"the rest of this letter describes the high level design, cost and power characteristics of each of the major modem components: the transducer (section ii), the analog transceiver (section iii), and the digital signal processor (dsp) (section iv). we conclude with a summary of the cost and power characteristics of the full design and a discussion of future work in section v."
"we examined group differences in six rois corresponding to the following ventral areas bilaterally in the face network: ofa, posterior ffa (pffa) and anterior ffa [affa; by some also termed atl [cit] ]. [cit] . in addition, we examined group differences in object sensitive areas lo bilaterally based on the mean coordinates (averaged over the right and left hemispheres) [cit] . for the mean coordinates associated with each roi, see table 1 ."
"where n 1 and n 2 represent weighting factors which re ect the speci c gravity of bearing capacity and sti ness in the total objective function, respectively. and, they are calculated by the equation:"
"e optimization process of the solution is shown as figure 8 . e result of optimization is shown in figure 9 . when the design variable x [10.521 3.501 82.214], that is, bearing clearance is 10.521 μm, ori ce diameter is 3.501 mm, and the diameter of stomatic distribution circle is 82.214 mm, the optimal result of multiobjective function f(x) is max f(x) 1.8682."
"two key steps for solving reynolds equation by finite difference method are (1) dividing the solution domain into nodal grids and (2) transforming partial differential reynolds equation into difference equation. finally, the pressure distribution of p a can be obtained by substituting boundary conditions, as shown in figure 2 ."
"in this paper, an optimization method is proposed to optimize the factors affecting the performance of the aerostatic spindle. ese factors include gas film thickness, diameter of orifice, and the diameter of stomatic distribution circle. e relationship between load capacity, stiffness, and the main influence factors determined by correlation analysis is deduced. e results are verified from static and dynamic characteristics. and the corresponding simulation and experiment of the bearing capacity and stiffness of the aerostatic spindle are carried out to achieve enhanced performance in widely application."
"en, the gas enters the air cavity through the orifice to form a layer of gas film which can support the workpiece to machine and achieve frictionless lubrication."
"to ensure that the dps were able to perform the task at the same level as the control group [cit] who kindly provided the stimuli. we chose this paradigm because it has already proven sensitive to the kind of category-sensitive activations we wanted to examine. while in the scanner participants were presented with stimuli from four main categories (faces, houses, tools and words) as well as a baseline stimulus (revolving checkerboards; see fig. 1 ) and were instructed to press a button when a red asterisk appeared above or below the stimulus (i.e. in the periphery). the stimuli were displayed on a screen in front of the scanner and viewed through a tilted mirror on the head coil. all stimuli were black on white background. the face and object stimuli (houses, tools) were high contrast grey-scale photographs that had been matched for size and luminance. the face images were of unknown people (12 females and 12 males) with neutral or happy expression, shown in frontal or slightly lateral view and with hair and occasionally accessories included (e.g. eyeglasses). the tools were 24 common handheld household tools (e.g. hammer and scissors). the houses included 24 frontal exterior views of unknown houses and buildings. the words were 24 common sixletter words in danish (e.g. smykke and kontor), with every second word presented in uppercase. the control stimuli consisted of two circular images of a checkerboard that were presented in iteration to create the impression of a moving checkerboard."
"e flow factor q embodied microfactor whose equation is q � c 1 + c 2 /ph is introduced into the reynolds equation, where c 1 and c 2 can be obtained from the literature [cit] . so, the reynolds equation after introducing the flow factor q is as follows:"
"in order to assess the validity of the experimental paradigm, we estimated group-level effects of task conditions in the healthy control group using general linear models, and obtained non-parametric p-values corrected for multiple comparisons across space using randomize [cit] with threshold-free cluster enhancement [cit] ) and 5000 permutations for each contrast."
"brain extraction tool [cit] was used to remove non-brain tissue from the fmri data. spatial smoothing was performed using a gaussian kernel filter with a full width at half maximum of 6 mm (susan; [cit] ) . fmrib's nonlinear image registration tool was used to register the participant's fmri volumes to montreal neurological institute (mni) 152 standard space using the t1-weighted scan in an intermediate step to improve alignment. the t1-weighted volume had the skull and other non-brain tissue removed using freesurfer 5.3 (http://surfer.nmr.mgh.harvard.edu (27 [cit], date last accessed); [cit] ."
"e circular flat pad aerostatic bearing with eight orifice-type restrictor is shown in figure 1 . according to the structure characteristics and actual working situation of the aerostatic bearing, the main design parameters contain bearing clearance h, orifice diameter d 3, the diameter of orifice distribution circle d 2, the rotate speed of the spindle, and so on. e bearing clearance h, orifice diameter d 3, and the diameter of orifice distribution circle d 2 are considered as design variables because they have more influence on the aerostatic bearing performance, and the ranges are 5-20 μm for h, 2 mm-5 mm for d 3, and 75-85 mm for d 2, respectively. e air supply pressure p s is given between 0.3 mpa and 0.6 mpa."
"e comparison of optimization parameters obtained by multiobjective function f(x) and original parameters are shown in table 2 . using optimization parameters and original parameters, the bearing capacity and sti ness of the di erent bearing clearance are simulated. e bearing performance before and after optimization is shown in figures 10 and 11 . compared with original parameters, the performance of bearing with optimized parameters is improved."
equation (2) is dimensionless for the convenience of calculation. e characteristic pressure of the film is p 0 and the characteristic thickness of film is bearing radius clear-
"e optimal values of bearing capacity and sti ness calculated above are brought into equation (10) to obtain the weighting factors n 1 and n 2 . substituting formulas (15) and (19) in (9), the multiobjective function including bearing capacity, sti ness, and in uence parameters of aerostatic bearing is determined."
"virtual agents are an important component for many vr applications in both ivr and semi-ivr systems. several studies have shown that participants react and behave towards virtual agents in a highly realistic manner [cit] . even if participants are aware that virtual agents are computer characters, they automatically attribute mental states to them [cit] ."
"semi -immersive vr system: a custom-built semi-ivr system, using a large screen front-projected wall, driven by a workstation. using a kinect for motion sensing and human body tracking, the participants were able to navigate."
"the participants were given the task to locate a virtual child and follow her wherever she went. this was their primary goal and was clearly stated to them. the virtual child was programmed to follow a trajectory, where she came across other virtual characters, and avoiding collisions with them."
"where p is the film pressure, h is the bearing film thickness, μ is the dynamic viscosity of the lubricating gas, u is the circumferential speed of the bearing surface, and u � ωr 0 . ω is the angular velocity of spindle rotation, r 0 is the bearing radius, and t is the time. equation (1) is converted into the polar coordinate for the convenience of calculation."
"(grant no. dff -4001-00115), to c.g. and r.s. l.t.w. was supported by the european research council (erc) [cit] research and innovation programme (erc starting grant, grant agreement no. 802998) and the research council of norway (249795). d.a. was supported by the south-eastern norway regional health authority (2019107)."
"the present finding of within normal range neural and behavioural responses to orthographic material in dp is not necessarily incompatible with the notion that other and associated deficits observed in dp may reflect individual differences in abnormal neural migration patterns [cit] or white matter integrity [cit] . it does, however, highlight a limitation of such broad-spectrum explanations of developmental disorders [cit] . they offer no principled account of which disorders should co-occur more frequently than others and why. it seems clear that developmental deficits in face recognition may occur without preventing the acquisition of normal reading skills and development of a cerebral area supporting word recognition (the vwfa), and this suggests that a general deficit in neural migration across ventral occipitotemporal cortex is not a viable explanation for dp."
"the transmitter was designed to operate for signal inputs in a range from 0 to 100 khz. the architecture is unique as it consists of two different amplifiers working in tandem. the primary amplifier is a highly linear class ab amplifier that provides a voltage gain of 23 while achieving a power efficiency of about 50%. the output of the class ab amplifier is connected to current sense circuitry that in turn controls the secondary amplifier, which is a class d switching amplifier. the class d amplifier is inherently nonlinear, but possesses an efficiency of approximately 95%. with both of the amplifiers driving the load and working together in parallel, the transmitter achieves a highly linear output signal while maintaining a power efficiency greater than 75%. because of its high linearity, the transmitter may be used with any modulation technique that can be programmed into the digital hardware platform."
"the distance that participants tend to maintain with a virtual agent appears to be an important issue, since it was found that a greater distance is maintained with more realistic agents [cit] and negative reactions can be caused due to violations of interpersonal space [cit] . these motivated us to study further how collisions between virtual agents and the participants affect the participants' behavior. interaction is another important issue of how participants are influenced by virtual agents in vr systems. [cit] found that if there is no interaction between the participant and the virtual agents, then any sense of presence is eliminated."
"by integrating equation (11) with z twice and combining the equivalent mass ow equation and the gas state equation, we can get the expression of pressure p as follows:"
"the dependent variable in the cfmt and the cfpt is accuracy rather than rt as neither task is based on speeded responses. in comparison, all other experimental tasks involve speeded responses in that the participants are encouraged to respond as fast and as accurately as possible. hence, for these tasks rt to correct trials is often the most sensitive dependent variable. this is in particular true of the reading task where (non-dyslexic) participants often make very few errors [cit] ."
"we have recently conducted an experimental study that showed that facilitating collision avoidance between the user and the virtual agents does not guarantee that the vr system will be more plausible or easy to use [cit] . we found that collision avoidance should be accompanied with basic interaction between the user and the virtual agents in order to increase the plausibility, the feeling of comfort and the sense of presence of the participant. using those results, we are studying in this research to find any differences between the participants' behavior, perception of realism, and their sense of presence, in the ivr and in the semi-ivr system."
"we have described the major design components of our prototype low-cost, underwater acoustic modem intended for shortrange, low data rate sensor network applications. currently, the digital hardware platform has been tested and supports data rates of up to 200 bps with bit error rate at a low signal to noise ratio (snr) of (10 db). initial in-water tests of the transducer and analog transceiver in mission bay, ca, indicate our design is capable of detecting a signal at distances greater than 350 m. we are currently developing a power supply board, battery pack, and watertight housing (that can withstand pressures at depth of up to 100 m) to be able to test the modem in the open ocean in order to assess its performance. table iii provides cost estimates for a fully packaged underwater modem and table iv summarizes its current power consumption."
"scenario 2 (s2): the agents avoid collisions with the participant but have no other interaction. scenario 3 (s3): the agents interact with the participant using some basic socialization (talking to him, looking at him, waving etc.) as well as applying collision avoidance with the participant."
"where x 1 represents the gas lm thickness h, x 2 is the ori ce diameter d 3, and x 3 is the diameter of distribution circle d 2 . figure 4 . e result of optimization is shown in figure 5 . e constrains are also transformed to the program of m les. e m les re ecting the sti ness and constrains are imported to the genetic algorithm toolbox. e process of solution is shown in figure 6 . e result of optimization is shown in figure 7 . e optimal value of the sti ness is max f 2 (x) 135.4."
"two aspects of the present results are noteworthy: first, developmental prosopagnosics (dps) show reduced brain activation in response to facial stimuli in posterior parts of the face network. interestingly, this reduction was seen bilaterally and tented to be greater in the left than in the right hemisphere. secondly, our analyses reveal no evidence of group differences in activation to orthographic material in the vwfa in the left hemisphere or to objects in lo bilaterally. these imaging results correspond with the behavioural pattern observed in the included dps, who show preserved reading and recognition of line drawings, but severely impaired face recognition. a recent study suggested that dp is characterized by widespread selectivity reductions across category selective visual cortex [cit] . our results show that this does not apply for word selectivity in the vwfa, thus constraining hypotheses of a general deficit affecting category selective visual processing in dp."
"underwater transducers are typically omnidirectional in the horizontal plane and do not transmit in the vertical plane to reduce reflections off the surface and bottom [cit] . this is especially important for shallow water communications. our transducer uses a radially expanding ring that provides 2-d omnidirectionality in the plane perpendicular to the axis. our transducer makes use of a $10 dollar steminc model smc26d22h13 smqa lead zirconate titanate (pzt) ceramic ring and a two-part urethane potting compound manufactured by cytec industries (fig. 2 ). this particular potting compound was chosen because it has a density identical to that of water, which provides for efficient mechanical to acoustical energy coupling the water. experimental tests have shown the ceramic maintains a linear output response up to 1000 v and 50 w. frequency response tests of two of our transducers (fig. 3) indicate that the resonant frequency of the transducer is 35 khz with a khz bandwidth. calibration tests giving the transducer's transmitting voltage response (tvr) and receiving voltage response (rvr) indicate it produces 140 db re upa/v/m at 35 khz and receives db re v/1 upa. when produced in large quantities, the estimated cost of the entire transducer including ceramic, potting, and labor is less than $50 dollars."
"where p a is the film pressure in the steady state, h 0 is steadystate film thickness, ω 1 is the perturbation frequency, p b is the pressure perturbation term in phase with the input perturbation, and p c is the pressure perturbation term with phase lag. substituting equation (4)"
"e bearing capacity and the sti ness are increased 7.98% and 9.17%, respectively. e aerostatic spindle system and the static sti ness measurement system of aerostatic bearing are, respectively, established by the optimization parameters of the bearing, as figure 12 . an inductive micrometer is used to measure the variation of the displacement of the spindle table, and it is necessary to perform zero adjustment rst. e speci c operation is to determine the height of the induction microprobe by adjusting the height of the measuring gantry. e contact between the probe and the table is within ±2 μm to ensure the accuracy of the test."
"microfactors will lead to decrease of the gas film density and increase of the gas compressibility. erefore, the traditional reynolds equation is no longer applicable to describe the microscale flow."
"as can be seen from table 2, the dp group was clearly impaired on all face measures but did not differ significantly from the controls with respect to object recognition and reading performance (for the individual scores, see supplementary table 1 )."
"many experimental studies have been conducted in the area of understanding the interaction between virtual agents and human participants. still, for ivr and semi-ivr systems there is not sufficient background work on how different levels of interaction between the participant and the virtual crowd affect the participant's behavior, perception of realism and sense of presence and if there are any differences in these two types of systems. in this study, we concentrate on the comparison of the behaviors of the participants between these two systems focusing on two main virtual agents' behavior characteristics, collision avoidance and basic interaction between the virtual crowd and the participant."
"in order to nd the relationship between the performance of aerostatic spindle and the in uence factors. an array x with these design variables which include the bearing clearance h, ori ce diameter d 3, and the diameter of stomatic distribution circle d 2 is determined,"
"the authors confirm that the behavioural data supporting the findings of this study are available within the article and its supplementary material. the imaging data are available from the corresponding author, upon reasonable request."
"three different scenarios were created with different level of interaction between the virtual agents and the user: scenario 1 (s1): the virtual agents ignore the participant (the virtual characters do not avoid any collision with the participant, and have no other interaction with him/her)."
"air compressor provides gas to the aerostatic spindle system, and the supply pressure is 0.5 mpa. when the mass block as load is applied to worktable of aerostatic spindle, the workbench will have vertical displacement change. e change is sensed by the probe of the inductive micrometer and transmitted to the inductive micrometer which shows the change in display. e weight of mass block is 50 n, and the method of stepwise loading is used to perform multiple measurements and average. according to loaded load of the aerostatic bearing and the displacement change detected by the inductance micrometer, the static sti ness value is calculated by the sti ness calculation formula. e values of sti ness test are shown in figure 13 which shows that the sti ness after optimization is more close to the values of sti ness test. so, the correctness and the feasibility of optimization method in this paper are veri ed by sti ness test."
"and the response curve is shown in figure 15 which indicates that the response speed after optimization has been improved, but it still in the same order of magnitude with the values before optimization."
taking the load capacity and sti ness of the spindle as the optimize function in the multiobjective function. e relationship between the optimize function and the main in uence factors is proposed.
"en, use the disturbance method to establish unsteady reynolds equation. e basic idea is to assume that when the input disturbance is a sinusoidal or cosine function, the output is the superposition of sinusoidal and cosine functions when the object of study is a first-order linear system. so, the integral of the pressure disturbance term in phase with the input disturbance can be defined as dynamic stiffness, and the integral of the pressure disturbance term with phase lag can be defined as dynamic damping coefficient."
"localizing and tracking localized current dipoles using eeg/meg measurements can provide very useful information during brain surgery of patients with medically uncontrolled partial seizures [cit] . for instance, it can distinguish different kinds of seizures based on the location and orientation of the seizure foci. for parkinsons disease, neural dipole tracking can identify and locate the exact source of the electrical nerve signals and as a result, improve the performance of deep brain stimulation based treatment [cit] ."
"the known number of dipoles tracking results are first demonstrated with the mpf approach using synthetic data. the data was created by inserting current dipoles into the sphere head model and calculating the resulting magnetic field using (1) with gaussian noise. for the multi-dipole case, the tracking results are shown in figs. 6(a)-6(b) . the mpf used two sub-pfs with 5,000 particles each, and the conventional sir pf (spf) used 20,000 particles. the average root mean-squared error (rmse) comparison for 100 monte carlo simulations is summarized in table iii . although both trackers provide reasonable estimates of the meg source locations, the mpf tracker needs significantly table iii comparison of rmse and pc processing time for two-dipole model fewer number of particles. when the mpf tracker uses the same number of particles as the spf, then the mpf results in improved rmse performance. in addition, we compared the timing performance of matlab implementation of the proposed mpf and spf on a multi-core pc (intel core quad cpu q6600 @ 2.4ghz). for comparable performance, the processing time of spf for 100 samples was 583 seconds compared to 375 seconds required for mpf. thus, for two dipole sources tracking problem, mpf takes significantly shorter time than spf."
"for the proposed tracking system, the number of particles used for each dipole is a critical parameter as it impacts the estimation accuracy and processing time. fig. 16 shows the tracking performance in terms of rmse for dipole location and the processing time for pf-phdf with respect to . here we choose the number of pe . from fig. 16, we can see that as increases, the rmse decreases and the processing time increases. however, when is greater than 1000, there is no significant improvement in the rmse but the processing time increases rapidly. a good tradeoff between rmse and processing time is obtained at . for the real data case corresponding to the visual experiment of a person tracking green squares, the maximum number of dipoles was small (less than 5). from the fpga timing results, we project that if the maximum number of dipoles is 3, the proposed system can perform real-time processing at sampling rates of up to 10 khz for window length samples. however, for epilepsy patients, the number of dipoles during seizures can be greater than 10 [cit] . fig. 17 shows the timing performance of the proposed system with respect to the maximum number of dipoles for 1,000 particles per dipole and . from fig. 17, we can see that as the maximum number of dipoles increases, the processing time for a window grows, as expected. even when the number of dipoles is as large as 15, our system can still support real-time tracking with sampling rate of about 1.5 khz."
"in section iv-d, we discussed that eigenvalue decomposition can be used to reduce the dimension of eeg data by choosing several leading eigenvalues instead of all the eigenvalues. eigenvalue selection method is crucial since it determines the number of independent components and the reconstruction error of ica. a threshold based method is used to select the leading eigenvalues. fig. 8 shows the amplitudes of all the eigenvalues of the covariance matrix."
"the pf-phdf algorithm requires each measurement vector to be generated by a single object. however, for the eeg/meg dipole model, the eeg/meg measurement in (1) is generated by multiple neural sources. thus, we first need to decompose the eeg/meg measurements into individual components, with each component corresponding to an individual neural source. the decomposition can be simplified by first performing a preliminary pre-whitening of the eeg/meg data . specifically, the data is linearly transformed to whose elements are mutually uncorrelated with unit variance, [cit], where is the identity matrix and is a linear transformation matrix. an example of such a linear transformation can be obtained using eigenvalue decomposition of the measurement covariance matrix, where is a diagonal matrix whose elements are the eigenvalues of and is a matrix whose columns consist of the corresponding eigenvectors. in particular, assuming eeg/meg measurement sensors, the eigenvalues of the covariance matrix need to be obtained. although there are many algorithms to calculate eigenvectors from a covariance matrix, when the size of, most of them can hardly be mapped into an efficient vlsi architecture. in most eeg/meg systems, the number of sensors is between and, and so the challenge is to find an efficient approach to calculate the eigenvalues of a large matrix. in our previous work [cit], we used channel decomposition to solve this problem. the channels were divided into several groups and ica was applied to each group. however, this approach resulted in estimation performance loss, and, despite the fact that the computational intensity was reduced, this step was still the bottleneck of our implementation."
"where (9) substituting (8) into (6), we obtain the particle approximation of the posterior intensity function as (10) where and based on (5) and (6), a particle approximation of the posterior intensity can be obtained at time step from its particle approximation at the previous time step . using this particle approximation, both the number of sources in a given region and the sources' parameters can be estimated. the pf-phdf method can solve the eeg/meg inverse problem even when the number of dipoles is unknown. it assumes that the measurement is generated from a single-target or false alarm. however, as the eeg/meg measurements in (1) are due to contributions from multiple dipoles, we next consider an approach that first decomposes the eeg/meg data before estimating the unknown number of dipoles using the pf-phdf."
"the rest of the paper is organized as follows. in section ii, we present the neural activity dipole model for the eeg/meg inverse problem. we develop an algorithm based on the multiple pf to track the parameters of a known number of dipole sources in section iii. in section iv, we first review the phdf and its pf implementation. we then present the new algorithm for jointly estimating the unknown number of dipole sources and their parameters using the pf-phdf. hardware implementations of both algorithms are provided in section v. we present numerical simulation results, together with algorithmic and hardware performance results, in section vi."
"the high level block diagram of the proposed pf-phdf architecture is shown in fig. 4 . it consists of processing elements (pe) and one central unit (cu) connected by a global bus. local processing steps, such as initialization (step 1), prediction ( step 2) and part of updating (step 3) are conducted in each pe. global processing steps, such as computing normalization factors, estimating the number of dipoles (step 3), resampling (step 4) and clustering (step 5), are executed in the cu. each pe communicates with the cu, but there is no communication among pes."
"in order to find the optimal threshold for leading eigenvalue selection, we use the root mean square error (rmse) of the reconstructed eeg data as the cost function. the rmse is defined as the difference of estimated individual eeg data obtained using (11) and the true eeg data where is the number of time steps and is the number of sensors. table iv shows the rmse of the reconstructed eeg data for different threshold values. we can see that as the threshold value decreases, the rmse increases and that there is a significant rmse degradation when the threshold is smaller than 150. in addition, with thresholds smaller than 100, we can hardly distinguish dipole signals with noise which causes the fastica algorithm to fail. based on these results, we choose a threshold value of 500 for the rest of the simulation."
"3) window length selection: the choice of window length is crucial as it greatly impacts the estimation results. table v shows the reconstruction rmse with respect to different . we can see from the table that if the window length is too long (much longer than the duration of dipole), the independent component analysis cannot capture the changing information of the dipole. as a result, the reconstruction error is large. if the window length is too small, the samples in the window cannot statistically represent the whole data, which also leads to larger reconstruction error. based on these results, we choose a window length samples with sampling rate 1 khz."
step 5-estimating dipole state: the resampled particles are clustered and the state parameters are estimated. the clustering is performed in 3-d using the k-means clustering algorithm.
"2) component analysis of pre-whitened measurements: after pre-processing, the new data vector has reduced dimensionality and reduced noise power. since distinct neural sources are mutually independent [cit], the independent component analysis (ica) technique can be used to decompose the pre-processed eeg/meg data. here, we assume that in a short time window the location and orientation of dipole sources are fixed. as a result, the lead-field matrix doesn't change with time in this period. the window length is a critical parameter which will be analyzed in section iv-e. for the eeg/meg data in one window, we use fastica, a free matlab software package that implements a fast fixed-point ica algorithm [cit], to obtain a de-mixing matrix,"
"we consider a different approach, where we assume that only a small set of patches of the human brain are activated at a time [cit] . under this assumption, the number of dipoles is much smaller than the number of sensors, . thus instead of calculating all the eigenvalues and eigenvectors, we only need to find the leading eigenvalues and eigenvectors corresponding to the active dipoles. we propose an efficient leading eigenvector and eigenvalue calculation approach based on the eigenvector distilling algorithm [cit] . the steps of the proposed method are described in algorithm 1 . in order to obtain the largest eigenvector, we first initialize randomly with norm 1 and then update it using iterations to obtain . its corresponding eigenvalue is obtained and compared to the threshold . if, this eigenvector is retained and the procedure continues to calculate the next one. note that the leading eigenvectors are calculated one by one, in reducing order of dominance. as the number of dipoles is unknown, the number of eigenvalues and eigenvectors to be distilled is unknown as well. so we use a threshold to pick the eigenvalues to be computed. we will analyze the influence of this threshold on algorithm performance in section vi."
"the corresponding dipole source model is given in fig. 1(b) . here is the radius of the head model, is the distance between the dipole source and the sensor, is the angle between the vector pointing to the sensor and the vector pointing to the dipole location, is the angle between the dipole orientation and the vector pointing to the dipole location, is the angle between the plane formed by the dipole and the origin, and is the head tissue conductivity constant. also, in (2) ."
"the pf-phdf is robust and computationally inexpensive compared to existing multiple target tracking techniques, and it has been successfully used in radar and sonar tracking [cit] . however, there are significant challenges in applying the pf-phdf to solve the eeg/meg inverse problem. whereas each measurement in radar is generated from a single target, eeg/meg sensor measurements are due to contributions from all dipole sources; thus, the observed data must be decomposed before it can be used by the pf-phdf. also challenging is the large dimensionality of the data due to the large number of sensors used to collect it."
"to address these challenges, the proposed dipole estimation pf-phdf algorithm: 1) reduces the size of the measurement matrix and simplifies computation using efficient pre-whitening of the eeg/meg measurement; 2) reduces the number of prewhitened components using independent component analysis (ica); and 3) applies the pf-phdf to estimate the number of dipoles and their parameters. these three stages are described next in more detail."
"that have a resting state characterized by a cross-membrane voltage difference. when an electromagnetic signal is transferred from one neuron to another, a chemical postsynaptic potential is created that can be modeled as a localized current dipole [cit] . when thousands of neighboring neurons are simultaneously in this postsynaptic excitation state, localized current is generated that creates an electromagnetic field outside the skull. the magnetic field can be recorded as an meg signal using a superconducting quantum interference device; the corresponding electric potential can be recorded as an eeg signal using multiple electrodes placed at different locations on the scalp."
"in this paper, we described the application of the pf-phdf for tracking an unknown number of neural dipoles, and we demonstrated its performance using numerical simulations for both synthetic and real eeg data. the proposed method achieves good performance in terms of rmse using significantly fewer number of particles compared to existing approaches. we also presented a window based processing method and a threshold based eigenvalue distilling algorithm to enable real-time processing. the proposed method was implemented on the xilinx virtex-5 fpga platform. the processing time for a window with 100 samples using 3,200 particles for a system with 3 dipoles was shown to be only 5.1 ms. thus, this implementation is capable of real-time processing of systems with larger number of dipoles and/or larger number of samples per second."
". using this matrix, we can obtain estimated independent sources as . the estimated mixing matrix, where is the column of, can be obtained as . using, the eeg/meg signal that contributed from the individual source is given by (11) after this stage, the decomposed unmixed eeg/meg measurement vectors are given by . thus, each new measurement is assumed to have originated either from a single dipole source or an artifact (non-brain) activity (false alarm)."
"resource utilization: table vii summarizes the architecture resource utilization for the 4 pe parallel architecture for the pf-phdf. since the total number of particles is 3,200, each pe processes 800 particles. for the likelihood calculation in the updating step, the exponential functions are implemented using cordic units, and the rest of the units are implemented using dsp cores."
the decomposed pre-whitened eeg/meg measurement rfs can now be directly used as the input to the pf-phdf algorithm to estimate the number of dipole sources and their unknown parameters. the corresponding state-space rfs model for the eeg/meg source estimation problem is given by (
"the particle filter (pf) technique [cit] has been used to track dipole source parameters [cit] . the state equation for tracking the dipole source is given by (3) where is a modeling error random process that is independent of the measurement noise in the measurement equation in (1) . using the state-space model in (3) and (1), the pf provides an iterative approach to sequentially compute the posterior probability density function of the state at every time step, conditioned on the eeg/meg measurements . since there are only 7 unknown parameters for the dipole, the pf can achieve good tracking performance with a fairly small number of particles. for dipole sources, the state-space model can be written as in (3) and (1) with the state vector having dimensions. as the dimension of the dipole source state increases when multiple dipole sources are tracked, the number of particles required for accurate tracking increases substantially. as a result, it becomes computationally impractical to directly use a pf for multiple dipole neural tracking."
"even with the computational complexity reduction due to the use of eigenvalue distilling, the proposed estimation system can still be computationally intensive. table i lists the number of operations when the system uses sensors, dipole sources, and 1,000 particles per-dipole. the pre-whitening step includes the computation of the measurement covariance matrix, which is computationally intensive. however, the pf-phdf computational complexity is the largest, creating a system bottleneck. table i also lists the computation time when running matlab implementation of the different algorithms on a multi-core pc (intel core quad cpu q6600 @ 2.4ghz). we see that the processing time for pf-phdf is more than 25 seconds which is too large to support the real-time processing of an eeg/meg system. in the next section, we describe a hardware implementation of the pf-phdf which makes use of pipelining and parallel processing to reduce the computation time."
"we have also evaluated our approach after a real-world application, pornographic detection. the pornography dataset contains nearly 80 hours of 400 pornographic and 400 non-pornographic videos. for the pornographic class, we have browsed websites which only host this kind of material. for the non-pornographic class, we have browsed general-public purpose video network and selected two samples: 200 videos chosen at random (which we called \"easy\") and 200 videos selected from textual search queries like \"beach\", \"wrestling\", \"swimming\", which we knew would be particularly challenging for the detector (\"difficult\"). figure 7 shows selected frames from the dataset. fig. 7 . illustration of the diversity of the pornographic videos (top row) and the challenges of the \"difficult\" non-pornographic ones (middle row). the easy cases are shown at bottom row. the huge diversity of cases in both pornographic and non pornographic videos makes this task very challenging."
"the amino acid sequence represents the primary structure of a protein, which is the simplest type of information either obtained through direct sequencing or translated from dna sequences. many research efforts address the ppi problem based on predefined features extracted from protein sequences, such as ontological features of animo acids [cit], autocovariance (ac) [cit], conjoint triads (ct) [cit] and composition-transition-distribution (ctd) descriptors [cit] . these features generally summarize specific aspects of protein sequences such as physicochemical properties, frequencies of local patterns, and the positional distribution of amino acids. on top of these features, several statistical learning algorithms [cit] are applied to predict ppis in the form of binary classification. these approaches provide feasible solutions to the problem. however, the extracted feature sets used in these approaches only have limited coverage on interaction information, as they are dedicated to specific facets of the protein profiles."
"by nature, the ppi prediction task is comparable to the neural sentence pair modeling tasks in natural language processing (nlp) research, as they both seek to characterize the mutual influence of two sequences based on their latent features. in nlp, neural sentence pair models typically focus on capturing the discourse relations of lexicon sequences, such as textual entailment [cit], paraphrases [cit] and sub-topic relations [cit] . many recent efforts adopt a siamese encoding architecture, where encoders based on convolutional neural networks (cnn) [cit] and recurrent neural networks (rnn) [cit] are widely used. a binary downstream classifier is then stacked to the sequence pair encoder for the detection of a targeted discourse relation. in contrast to sentences, proteins are profiled in sequences with more intractable patterns, as well as in a drastically larger range of lengths. precisely capturing the ppi requires much more comprehensive learning architectures to refine the robust information from the entire sequences, and to preserve the long-term ordering information. one recent work [cit], dppi, uses a deep cnn-based architecture which focuses on capturing local features from protein profiles. dppi represents the first work to deploy deep-learning to ppi prediction, and has achieved the state-of-the-art performance on the binary prediction task. however, it requires excessive efforts for data pre-processing such as constructing protein profiles by psi-blast [cit], and does not incorporate a neural learning architecture that captures the important contextualized and sequential features."
where n b is n bw less than the number of samples between two beats (rounded down to the nearest integer). k bw is a user defined static positive gain.
". the residual mechanism passes on an identity mapping of the gru inputs to its output side through a residual shortcut [cit] . by adding the forwarded input values to the outputs, the corresponding neural layer is only required to capture the difference between the input and output values. this mechanism aims at improving the learning process of non-linear neural layers by increasing the sensitivity of the optimization gradients [cit], as well as preventing the model from the vanishing gradient problem. it has been widely deployed in deep learning architectures for various tasks of image recognition [cit], document classification [cit] and speech recognition [cit] . in our deep rcnn, the bidirectional gru is incorporated with the residual mechanism, and will pass on the following outputs to its subsequent neural network layer:"
"for each experiment, we calculate the average training time over either 5-fold (yeast dataset) or 10-fold (others) cross-validation. in both binary and multi-class predictions, the training time increases along with the increased number of training cases. the regression estimation generally requires more iterations per training case to converge than classification tasks. thus, with much fewer cases, the training time on skempi for affinity estimation is more than that on the yeast dataset for binary prediction."
"we proposed in this paper a new representation of images for classification tasks. analyzing the popular bag-of-words scheme, we pointed out weakness in the standard pooling operation used in the bow signature generation. the bossa scheme presented here offers a more information-preserving pooling operation based on a 2 http://www.stoik.com/products/svc/ distance-to-codeword distribution. with this improvement to the basic pooling scheme, we carried out our final super-vector image signature used in svm framework for classification."
"jaemi hubo now has one of the three senses (hearing, sight, and touch) required to achieve our overarching goal of creating a robot capable of being an interactive participant in a live ensemble. jaemi's sense of sight gives it key information about a musical environment: tempo and ictus. the next step is to combine the audio beat tracker from ellenberg's adh system [cit] with the vbt. as a result jaemi will be able to follow the beat through visual and auditory cues. the addition of tactile and sensory multiplexing systems will close jaemi hubo's interactive participant gap and allow us to achieve our ultimate goal: having our adult size humanoid"
"running in discrete (frame by frame) time, our annotation and calculations must be modified. let e j,k,n be the brightness of the pixel located on the j th row, k th column on the n th image. j spans from 0 to the width of the image frame. k spans from 0 to the height of the image frame. the top left corner is the origin (0, 0). the bottom right corner is the max width and height (j max, k max ). [cit] determined that e x, e y, and e t can be estimated by:"
"our contributions are three-fold. first, we construct an end-to-end framework for ppi prediction that relieves the data pre-processing efforts for users. lasagna requires only the primary protein sequences as the input, and is trained to automatically preserve the critical features from the sequences. second, we emphasize and demonstrate the needs of considering the contextualized and sequential information when modeling the ppis. third, the architecture of lasagna can be flexibly used to address different ppi tasks. besides the binary prediction that is widely attempted in previous works, our framework extends its use to two additional challenging problems: multiclass interaction type prediction and binding affinity estimation. we use five datasets to evaluate the performance of our framework on these tasks. lasagna outperforms various state-of-the-art approaches on the binary prediction task, which confirms the effectiveness in terms of integrating both local features and sequential information. the promising performance of the other two tasks demonstrates a wide usability of our approach. especially on the binding affinity estimation of mutated proteins, lasagna is able to respond to the subtle changes of point mutations and provides the best estimation with the smallest errors."
"we address three challenging ppi prediction tasks based only on the primary sequence information: (i) binary prediction seeks to provide a binary classifier to indicate whether the corresponding protein pair interacts, which is the simplest and widely considered problem setting in previous works [cit] . (ii) interaction type prediction is a multi-class classification problem, which seeks to identify the interaction type of two proteins. (iii) binding affinity estimation aims at producing a regression model to estimate the strength of the binding interaction."
"as illustrated in figure 1, if we represent a matrix h with columns x and rows c, the coding function f for a given descriptor xj corresponds to the j st column. the second step is a pooling step, that can be modeled by the following function g:"
"the results in table a1 show that, once we remove either of the two parts of the proposed embedding, the performance of the model slightly drops. meanwhile, the proposed pre-trained embeddings lead to noticeably better performance of the model than adopting the simple one-hot encodings of the canonical amino acids."
"the residual gated recurrent units. the gated recurrent unit model (gru) represents an alternative to the long-short-term memory network (lstm) [cit], which consecutively characterizes the sequential information without using separated memory cells [cit] . each unit consists of two types of gates to track the state of the sequence, i.e. the reset gate r t and the update gate z t . given the vector representation v t of an incoming item (either a pre-trained amino acid embedding, or an output of the previous layer), gru updates the hidden state h"
"music that is best tracked from audio in real-time requires large magnitude changes and a constant beat present in the music [cit] . consequently pop and other similar musical styles track well, however, non-percussive melodic music, such as most orchestral pieces, does not. in orchestral performances a conductor can keep the beat during periods of absent auditory cues. one reason humans are able to accurately track the beat and tempo present in a multitude of different musical styles is because we can utilize our senses of hearing, sight and touch to do so. our overarching goal is to make our adult size (130cm) humanoid robot jaemi hubo (see fig. 1 ) musically aware and have it become an interactive participant in a live human ensemble. bringing our goal to fruition means that all senses must be utilized."
s vn can contain non-tempo related frequencies due to harmonics or other visual stimuli. applying a piecewise tempo weight t w to s vn reduces the effects of the nontempo related frequencies. s w vn is the weighted s vn vector and is described in section iii-c. s w vn can be used as a direct replacement for s vn when the tempo weight is needed.
"the experiments are conducted on the following datasets. [cit] ). based on the assumption that two proteins are less likely to interact if they reside far away from each other, the negative samples are generated by pairing those found in different subcellular locations. besides this pairing process, additional negative samples of human ppis are included from the negatome database [cit], resulting in a total of 36,480 non-interaction pairs. we also use this dataset to evaluate the binary prediction task for ppis. string datasets. the string database [cit] annotates ppis with their types. there are seven types of interactions: activation, binding, catalysis, expression, inhibition, post-translational modification (ptmod), and reaction. we download all interaction pairs for homo sapiens from database version 10.5 [cit] . among the corresponding proteins, we randomly select 3,000 proteins and 8,000 proteins to generate two subsets. in this process, we randomly sample instances of different interaction types to ensure a balanced class distribution. eventually, the two generated datasets, denoted by shs27k and shs148k, contain 26,945 cases and 148,051 cases of interactions respectively. we use these two datasets for the evaluation of the ppi type prediction task."
"residual rcnn the rcnn seeks to leverage both the global sequential information and local features that are significant to the characterization of ppi from the protein sequences. this deep neural encoder stacks multiple instances of two computational modules, i.e. convolution layers with pooling and bidirectional residual gated recurrent units."
"the basic bow representation has important limitations, and several improvements have been suggested. to overcome the loss of spatial information, separate bows can be computed in different sub-regions of the image, as in the spatial pyramid matching (spm) scheme [cit] . to attenuate the effect of coding errors induced by the descriptor space quantization, one can rely on soft assignment [cit] or explicitly minimize reconstruction errors, e.g. local linear coding [cit] . finally, averaging local descriptor contributions (sum pooling) can be reconsidered by studying alternative (more biologically plausible) pooling schemes, e.g. max pooling [cit] ."
"protein sequence encoding. the rcnn encoder e rcn n (s) alternately stacks multiple occurrences of the above two intermediary neural network components. a convolution layer serves as the first encoding layer to extract local features from the input sequence. on top of that, a residual gru layer takes in the preserved local features, whose outputs are passed to another convolution layer."
", where each latent vector combines the local features from each h-gram of the input sequence. the nmax-pooling mechanism is applied to every consecutive n-length subsequence (i.e., non-overlapped n-stride) of the convolution outputs by h"
for which h is the kernel size and b c is a bias vector. the convolution layer applies the kernel as a sliding window to produce a sequence of latent vectors
"i:n+i−1 ). the purpose of this mechanism is to discretize the convolution results, and preserve the most significant features within each n-stride [cit] . by definition, this mechanism divides the size of processed features by n."
using the sense of sight to perceive the ictus 4 (instant at which the beat occurs) has been shown to be effective. [cit] demonstrated vision system that is able to estimate the period of an unstructured beat gesture expressed in any part of the viewable scene. this system used the lucas-kanade algorithm [cit] to calculate the optical flow. this locates the regions in the scene (between two consecutive frames) that contain relative movement. the trajectory of the center of gravity of the optical flow was used to influence the sound of a piano by actuating robotic fingers. [cit] created a system that allowed the humanoid robot nico to play a drum in concert with human drummers. color target tracking was used to track the conductor's hand. nico used both sight and sound to enable precise synchronization in performing its task.
the plot of t w for multiple tempos x i is shown in fig. 5 . the weighted spectrum s w vn is calculated by:
"we preprocess this dataset by segmenting videos into shots. an industry-standard segmentation software 2 has been used. as it is often done in video analysis, a key-frame is selected to summarize the content of the shot into a static image. in our case, we have just selected the middle frame of each shot."
"in our development, we have found that the residual mechanism is able to drastically simplify the training process, and largely decreases the epochs of parameter updates for the model to converge."
where i is the index corresponding to x i defined in eq. 8. i is all integer values from 0 to (n fft − 1).
"in this paper, we introduce lasagna, a deep learning framework for ppi prediction using only the primary sequences of a protein pair. lasagna employs a siamese architecture to capture the mutual influence of a protein sequence pair. the learning architecture is based on a recurrent convolutional neural network (rcnn), which integrates multiple occurrences of convolution layers and residual gated recurrent units. to represent each amino acid in this architecture, lasagna applies an efficient property-aware lexicon embedding approach to better capture the contextual and physicochemical relatedness of amino acids. this comprehensive encoding architecture provides a multi-granular feature aggregation process to effectively leverage both sequential and robust local information of the protein sequences. it is important to note that the scope of this work focuses only on the primary sequence as it is the fundamental information to describe a protein."
"for classification, we apply the popular maximum-margin svm classifier, specifically a non-linear χ kernel and the one-versus-all approach for multi-class approach. kernel matrices are computed as exp(−γd(x, x )) with d being the distance and γ being fixed to the inverse of the pairwise distances mean."
"binary ppi prediction is the primary task targeted by a handful of previous works [cit] . the objective of these works is to identify whether a given pair of proteins interacts or not based on their primary sequences. we evaluate lasagna based on the yeast and the human datasets. our model is compared against multiple baseline approaches, including: svm-ac [cit], knn-ctd [cit], eelm-pca [cit], svm-mcd [cit], mlp [cit], random forest lpq (rf-lpq) [cit] and dppi [cit] . meanwhile, we also report the results of a siamese residual gru (srgru) architecture, which is a simplification of lasagna, where we discard all intermediary convolution layers and keep only the bidirectional residual gru. the purpose of srgru is to show the significance of the contextualized and sequential information of protein profiles in characterizing ppis. we also report the results of siamese cnn (scnn) by removing the residual gru in lasagna. this degenerates our framework to a similar architecture to dppi, but differs in directly conducting an end-to-end training on primary sequences instead of requiring the protein profiles constructed by psi-blast."
"the pooling function g for a given visual word cm corresponds to the m st row of the h matrix, as shown in figure 1 . for example, in the basic bow representation:"
"detecting protein-protein interactions (ppis) and characterizing the interaction types are essential toward understanding cellular biological processes in normal and disease states. knowledge from these studies potentially facilitates therapeutic target identification [cit] and novel drug design [cit] . high-throughput experimental technologies have been rapidly developed to discover and validate ppis in a large scale. these technologies include yeast two-hybrid screens [cit], tandem affinity purification [cit], and mass spectrometric protein complex identification [cit] . however, experimentbased methods remain expensive, labor-intensive, and time-consuming. most importantly, they often suffer from high levels of false-positive predictions [cit] . evidently, there is an immense need for reliable computational approaches to identify and characterize ppis."
"our objective is to calculate the tempo and beat timing (ictus) directly from jaemi hubo's single camera live video feed. from musical conducting or rhythmic action the tempo and beat timing can be calculated by examining the spectral content of the mean magnitude and angle of motion in a given scene m n (across two or more frames). to calculate m n a real-time video feed (gray scale -intensity) must be captured. each frame in the video is equalized to compensate for different lighting conditions. the size of the image is reduced to allow for faster computation time. the computer vision (cv) horn-schunck optical flow method is used to calculate the relative motion between the current frame and the previous frame [cit] . the mean magnitude and angle moved between the two frames (m n ) is then calculated. the vector v n containing the past n fft values of m n is created, where n fft is the length of the forthcoming fast fourier transform (fft). the spectral content of v n is calculated via the use of the fft creating s vn . to reduce the effects of harmonics a triangle piecewise weight is applied to s vn with the mean tempo for popular music as calculated by moelants [cit] receiving the greatest weight. the tempo corresponding to the location of the highest peak in the weighted s vn is the tempo contained in the captured video. the tempo is normally measured in beats per minute (bpm)."
"we report training time for each experiment in table a2, as well as the training time of the amino acid embeddings. all these processes are conducted on one nvidia geforce gtx 1080 ti gpu. the amino acid embeddings are trained on the 8,000 sequences of the shs148k dataset. the training setup has been described in section 4.2. the process is very efficient, which only takes 8 seconds to complete. this is also a one-time effort, as the same embeddings can be applied to different tasks and datasets."
"corresponds to the confidence score for the i-th class. the learning objective is to minimize the following cross-entropy loss, where c p is a one-hot indicator for the class label of protein pair p."
"the horn-schunck optical flow algorithm is used to find the direction and magnitude of motion in each frame set i n and i n −1 [cit] . the horn-schunck algorithm calculates the flow velocity (u, v) of each point in the frame set. fig. 3 . video taken from a real-time feed. each frame is equalized to compensate for different lighting conditions. current image i n is resized (reduced in size) to allow for faster computation time. the optical flow is taken between the current frame i n and the previous frame i n −1 . the resulting data consists of the magnitude and angle the each pixel moved from i n −1 to i n . the magnitude and angle is averaged over the entire image resulting in a mean magnitude and angle moved for the image set i n and i n −1 . this is referred to as the movement data. the fft of the past m values of the movement data is calculated. a weighted filter is placed over the fft spectrum to give higher weight to the most common tempos [cit] . the maxima of the fft spectrum correlates to the overall tempo present in the video."
"we experimentally compared the performances of our bossa algorithm with the classic bow on a standard image flower dataset as well as on a realistic application. in both cases, bossa performed better than bow."
the beat timing (location of each beat or ictus) is calculated by examining the first derivative of the mean angle between the current and the previous frame. the time instance where the resulting value becomes less than the dynamic beat marking threshold (dbmt or b c n ) marks the beginning of the beat. b c n is a function of the current calculated tempo that increases the probability that the beat will be detected at the next ictus by decreasing its magnitude as the likelihood of the next ictus increases.
the tempo can be extracted by finding the maximum peak in the fft spectra vector s vn for the mean motion data vector v n where:
"with the use of the visual beat tracker (vbt), jaemi hubo has the ability to calculate the timing for a musical beat in 0.075 seconds and estimate the tempo in 4.0 seconds. the use of the horn-schunck optical flow algorithm allows the vbt to function in unstructured real-world environments. the dynamic beat marking threshold (dbmt) allowed the beat/ictus tracker to be unaffected by irregular conducting directions."
"we evaluate the effectiveness of different strategies in representing the 20 canonical amino acids in our framework using the yeast dataset. as describe in section 3.2, we use a pre-trained 12-dimensional embedding to represent each amino acid. the first part of the embedding vector contains a 5-dimensional vector, a c, which measures the co-occurrence similarity of the amino acids in protein sequences. the second part contains a 7-dimensional vector, a ph, which describes the categorization of electrostaticity and hydrophobicity for the amino acid. we examine the performance of using each part individually, as well as the performance of combining them as used in our framework. in addition, we include a simple one-hot vector representation, which does not consider the relatedness of amino acids and treats each of them independently."
"repeating of these two components in the network structure conducts an automatic multi-granular feature aggregation process on the protein sequence, while preserving the sequential and contextualized information on each granularity of the selected features. the last residual gru layer is followed by another convolution layer for a final round of local feature selection to produce the last hidden states"
"the best baseline using random forest thereof, achieves satisfactory results by more than doubling the accuracy of zero rule on the smaller shs27k dataset. however, on the larger shs148k dataset, the accuracy of these explicit-feature-based models is notably impaired. we hypothesize that, such predefined explicit features are not representative enough to distinguish the ppi types. on the other hand, the deep-learning-based approaches do not need to explicitly utilize these features, and perform consistently well on both settings. the raw sequence information is sufficient for these approaches to drastically outperform the random forest by at least 5.30% in accuracy on shs27k and 17.40% in accuracy on shs148k. scnn thereof outperforms srgru by 4.48% and 1.24% in accuracy on shs27k and shs148k, respectively. this implies that the local interacting features are relatively more deterministic than contextualized and sequential features on this task. the results by the residual rcnn-based framework are very promising, as it outperforms scnn by 4.02% and 6.62% in accuracy on shs27k and shs148k respectively. it also remarkably outperforms the best explicit-feature-based baselines on the two datasets by 13.80% and 25.26% in accuracy, and more than 3.5 of fold changes over the zero rule on both datasets."
"we apply our proposal on two challenging datasets: oxford flowers [cit] (image classification) and pornography (video classification). as a low-level local descriptor, we have employed hue-sift [cit], a sift variant including color information, which is particularly relevant for our datasets. the 165-dimensional hue-sift descriptors are extracted densely every 6 pixels."
"our scheme has the advantage of being conceptually simple, non-parametric and easily adaptable. compared to other schemes existing in the literature to add information to the bow model, it leads to much more compact representations."
"e(x, y, t) denotes the image brightness at point (x, y) in the image plane at time t. e x, e y and e t are the partial derivatives for the image brightness in respect to x, y, and t respectively. the constraints on the flow velocity can now be defined as:"
"we use amsgrad [cit] to optimize the cross-entropy loss, for which we set the learning rate α to 0.001, the exponential decay rates β 1 and β 2 to 0.9 and 0.999, and batch size to 256 on both datasets. the number of occurrences for the rcnn units (i.e., one convolution-pooling layer followed by one bidirectional residual gru layer) is set to 5, where we adopt 3-max-pooling and the convolution kernel of size 3. we set the hidden state size to be 50, and the rcnn output size to be 100. we set this configuration to ensure the rcnn to compress the selected features in a reasonably small vector sequence, before the features are aggregated by the last global averagepooling. note that the model configuration study is provided in appendix i. all model variants are trained until converge at each fold of the cross-validation. for amino acid embeddings, we pre-train the skip-gram on all sequences on our largest string dataset, shs148k, using a context window size of 6 and a negative sampling size of 5. this process obtains an 5-dimensional vector for the first sub-embedding part of each amino acid, which is concatenated to the dipoles and side chain volume based one-hot encoding (a ph ) to obtain the 12-dimensional embedding representation of amino acids. we also evaluate different representations in appendix ii. as reported in table a2 of appendix iii, the embedding pre-training process finishes within 1 minute on a commodity workstation, which is a one-time effort that can be reused on different tasks and datasets. the obtained amino acid embeddings are used to transform the protein sequences to vector sequences, for which we zero-pad short sequences to the highest sequence length in the dataset. this is a widely adopted technique for sequence modeling in nlp [cit] as well as in bioinformatics [cit] for efficient training. evaluation protocol. following the settings in previous works [cit], we conduct 5-fold cross-validation on the yeast dataset and 10-fold cross-validation on the human dataset. we aggregate three metrics on the test cases of each fold, i.e. the overall accuracy, precision and f1 on positive cases. all these metrics are preferred to be higher to indicate better performance."
"in this paper, we propose a novel end-to-end framework for ppi prediction based on the primary sequences. our proposed framework, lasagna, employs a residual rcnn, which provides an automatic multi-granular feature selection mechanism to capture both local significant features and sequential features from the primary protein sequences. by incorporating the rcnn in a siamesebased learning architecture, the framework captures effectively the mutual influence of protein pairs, and generalizes well to address different ppi prediction tasks without the need of predefined features. extensive experimental evaluations on five datasets show promising performance of our framework on three challenging ppi prediction tasks. this also leads to significant amelioration over various baselines. different sizes of the datasets also show satisfying scalability of the framework. for future work, one important direction is to apply the lasagna framework to other sequencebased inference tasks in bioinformatics, such as modeling rna and protein interactions. we also seek to extend multi-task learning [cit] into our framework, and jointly learn different objectives based on corpora with multifaceted interaction information."
"t . regarding image categorization, the aim is to find out which operators f and g provide the best classification performance using z as input."
"(i) cross-entropy loss is optimized for two classification problems, i.e. binary prediction and multiclass interaction type prediction. in this case, the mlp outputŝ p is a vector, whose dimensionality equals the number of classes m.ŝ p is normalized by a softmax function, where the i-th dimension s"
the peak will only be calculated for index values from 0 to where n fft is the fft length. the tempo of the peak x p is calculated using the index value of the peak i p and eq. 8.
"results. as shown in fig. 3, the cnn-based architecture, dppi, demonstrates state-of-the-art performance over other baselines that employ statistical learning algorithms or densely connected mlp. this shows the superiority of deep-learning-based techniques in encapsulating various types of information of a protein pair, such as amino acid composition and their co-occurrences, and automatically extracting the robust ones for the learning objectives. that said, dppi requires an extensive effort in data pre-processing, specifically in constructing the protein profile for each sequence. on average, each psi-blast search of a protein requires around one hour of computation on our server, and we are unable to obtain the profile for 8,958 protein sequences in the human dataset to evaluate dppi. for this reason, we implement scnn to evaluate the performance of a simplified cnn architecture on different datasets. our scnn implementation produces comparable results as dppi while evaluating the yeast dataset. compared to the use of sae [cit], fig. 4 shows that the simplified version of our framework with only cnn can already leverage the significant features from primary protein sequences. in addition, the srgru architecture has offered comparable performance to scnn on both datasets. this indicates that preserving the sequential and contextualized features of the protein sequences is as crucial as incorporating the local features. by integrating both significant local features and sequential information, lasagna outperforms dppi by 2.68% in accuracy, 0.61% in precision, and 2.83% in f1 on the yeast dataset, and outperforms scnn by 0.90% in accuracy, 1.78% in precision, and 1.01% in f1 on the human dataset. thus, the residual rcnn is very promising for modeling binary ppis."
the non-thresholding methods used by the horn-schunck optical flow algorithm allows the system to be more tolerant of low contrast and poor lighting conditions when compared to other optical flow algorithms.
the block diagram of the process to calculate the beat timing and the tempo contained in the captured video can be found in fig. 3 .
"this work focuses on jaemi's sense of sight as it pertains to a musical environment. a novel approach to tempo and beat tracking in the absence of auditory cues through the use of computer vision and digital signal processing methods is used to give jaemi hubo an additional component (sight) to musical awareness. the approach is called visual beat tracking (vbt). experiments showed that when equipped with the vbt jaemi hubo is able to accurately track the beat and tempo when watching a trained musician conduct to a steady tempo with its on board camera, see fig. 1 . experiments were performed at multiple measure timings/meters. fig. 2 shows the conducting pattern for the different timings/meters."
"where b denotes the number of bins of each histogram zm, and α max m is a parameter defined in section 3.2.1. thus, the g function represents the discrete (over b bins) density distribution of the distances αm,j among the center cm and the local descriptors of an image."
to reduce the effects of harmonics and to ensure the location of the highest peak in s vn depicts the correct tempo a weight is applied. the each value of the weight vector t w is different for each discrete tempo value. t w has the same length as s vn . t w is a triangular piecewise weight that peaks at the mean tempo t m of popular music. d. moelants calculated t m ≈ 128bp m [cit] . t w is calculated by:
"in this paper, we propose a new representation of images extending the bow approach, called bossa (bag of statistical sampling analysis). basically, the idea is to keep more information than the bow during the pooling step. indeed, in bow, the pooling step summarizes the vectorial information contained in αm,j into a single scalar value (equation 2): e.g. sum or max pooling. instead, we propose here to estimate the distribution, i.e. the probability density function (pdf), of these αm,j."
"our representation is defined by the three followings parameters: the number of visual words m, the number of bins b in each histogram zm, and the maximum distance α m has a similar meaning than in standard bow approaches. b defines the granularity to which pdf (αm) is estimated. the choices of m and b are co-dependent, and m · b determines the compromise between accuracy and robustness. the smaller m · b, the less the representation is accurate, the larger m · b, the less the statistical estimate of the underlying distribution is confident (too large m · b values may lead to sparse vectorial representations). in our experiments, we use m ∼ 500 and b in the range [2; 10] ."
"for image retrieval and classification tasks, some methods use complex structured models [cit] to represent specific types of object, e.g. humans. nevertheless, other approaches represent images by orderless local descriptors, such as the bag-of-words (bow) model [cit] . bow becomes popular due to its simplicity and good performance. inspired by the bag-of-words model from text retrieval [cit], where a document is represented by a set of words, the bow representation describes an image as a histogram of the occurrence rate of \"words\" in a vocabulary induced by quantizing the space of a low-level local descriptor (e.g., sift [cit] )."
"we present the experimental evaluation of the proposed framework on three ppi prediction tasks, i.e. binary prediction, multi-class interaction type prediction, and binding affinity estimation."
"in this work, we present a new framework for action recognition in rgb-d video based on spatiotemporal features and segmentation technique. we use stip detector to select interest points for both rgb and depth channels. spatiotemporal descriptors consist of hog3d, 3ds-honv and hof2.5d are extracted. these descriptors capture shape, appearance and motion information which are vital properties for action representation. we use gmm instead of k-means in bow model to create more distinctive for action representation. also, we apply segmentation and max pooling technique to capture the temporal structure of action. our approach systematically is evaluated on several benchmark datasets such as utkinect-action, 3d action pairs, and msrdaily activity 3d dataset with final recognition accuracies of 93.5%, 99.16% and 89.38% for fusion of descriptors, respectively. the experimental results have shown outcome performance compare to the-state-of-art methods in overall in most cases. for the spatiotemporal descriptors, 3ds-honv has shown robust descriptor in most cases. however, hog3d is better than 3ds-honv in case that needs to distinguish these objects that have the similar shape as 3d action pairs dataset. and, hof2.5d is better than hog3d and 3ds-honv in case that needs to differentiate these actions that have the similar motion. thus, to improve the action recognition system, fusion of the descriptors is the best way. for the part-based model, from experimental results also show that gmm is more powerful than k-means when using to create visual words in bow model. for segmentation method, in addition, we indicate that overlapping method performs the best in most cases. and, the length of segment also impacts to the performance of the system. however, the length is not fixed for all the dataset that it depends on the descriptors are used and the nature of dataset. in this work, the experimental results indicate that the length of segment is 15 and 20 frames are the best performances. www.ijacsa.thesai.org in summary, the key problems of this research are summed up as follows: firstly, we have explored the utility of spatiotemporal features derived from rgb and depth information. these features are extracted to capture both shape and motion in action. secondly, gmm used to instead of kmeans in bow model to have more distinctive and descriptive for action representation. finally, we have modeled temporal structure of action based on video segmentation and max pooling technique."
"in all case, furthermore, it is commonly believed that in order to obtain high recognition rate, it is important to select an appropriate set of visual features that usually have to capture the particular properties of a specific domain and the distinctive characteristics of each action class. the most important aspect of any action recognition system is to seek an efficient action representation. the target of the feature extraction is to find an efficient and effective representation of the action which would provide robustness during recognition process. besides, in case action representation from multiple feature vectors will need a robust method to combine feature vectors in the right way so that the system achieves good performance. in this work, we use the average-pooling technique to aggregating visual words in bow model and the max pooling technique to aggregating the segment feature vectors into the final feature vector for action representation."
"the msr-daily activity 3d dataset contains 16 different human activities: drink, eat, read book, call cell phone, write on a paper, use laptop, vacuum cleaner use, cheer up, sit still, toss paper, play game, lay down on sofa, walk, play guitar, standup, sit-down, and each subject performs an activity in two different poses: a standing pose and a sitting on sofa pose. each pose has 160 total samples, with each subject is one sample per activity in each pose. this dataset is created to cover daily activities and human-object interactions in the living room. these tests are more challenging than the other datasets because of frequent human-object interactions. [cit] 42.50 depth motion maps [cit] 43.13 local hon4d [cit] 80.00 actionlet ensemble [cit] 85.75 snv [cit] 86.25 bhim [cit] 86.88 our approach 89. 38 in this dataset, table v shows the experimental results of our different methods. from the results one can see that 3ds-honv is the best descriptor in case only one descriptor is used and the fusion of hog3d, 3ds-honv and hof2.5d outperforms the single descriptor in using bow with k-means and gmm. fig. 12 presents a comparison of the accuracy of overlapping and non-overlapping segmentation with the difference on the length of segment. the overlapping method is better than the non-overlapping method in all cases. and, the length of 20 frames for each segment achieve the best performance. table vi compares our approach results with state-of-the-art results on msr-daily activity dataset. we can see that our result of 89.38% in accuracy is better than all previous results using the same settings. our recognition rate is higher than the current best rate is 86.88% by 2.5%."
"in the future, we will investigate new method to improve appearance, motion properties as well as consider the impact of context and evolution of human when performing the action."
"video segmentation is the method that divides video into fixed length segments. these approaches can be divided into two types: non-overlapping and overlapping segments. for non-overlapping segments, a video is divided into continuous and equal length segments. the method does not take account information about the semantic boundary of a segment. however, this information is important because it keeps semantic meaning of each segment. this method also has the advantage that the subsequent ranking algorithm does not have to deal with problems arising from length differences. a variant of this fixed length method uses overlapping segments. in this method, a video is divided into overlapping and equal length segments. this approaches can be used that try to identify lexically and semantically coherent segments."
"following the previous researches [cit], human action could be defined by structured patterns of the human's movements and poses. with the perspective, a robust feature extraction and description must capture shape and motion properties in action representation. as such, human action can be modeled by spatiotemporal features, where encode shapes and movements of the whole body or body parts, for instance temporal progression, e.g., one human action as the whole can decompose into local shapes and movements of parts. in the past two decades, a significant amount of research has been done in the area of human action recognition using a sequence of 2d images [cit] . a single spatiotemporal structure, however, is unlikely to be sufficient to represent a class of action in all but the simplest scenarios. firstly, the execution of the action may differ from subject to subject, involving different body parts or different space-time progressions of body part movements. secondly, the video capture process introduces intra-class variations due to occlusions or variations in camera viewpoint. thus, the resulting space-time and appearance variations necessitate using a collection of spatiotemporal structures that can best represent the action at large. in addition, another property be also considered in action representation is evolution of action by time. it indicates that action also contains temporal structure for each action class. in this work, we apply video segmentation and max-poling technique which help to model temporal structure of action."
"in part-based methods, a video is modeled by the bag of words (bow) model which is the way of constructing a feature vector based on the number of occurrences of word. each visual word is just a feature vector of patch. the major issue of bow is vector quantization algorithms to create effective clusters. the original bow used k-means algorithm to quantize feature vectors. although k-means is used widely in clustering, its accuracy is not good in some cases. in addition, binary weighting for histogram of word occurrences which indicates the presence and absence of a visual word with values 1 and 0 respectively, was used. generally speaking, all the weighting schemes perform the nearest neighbor search in the vocabulary in the sense that each interest point is mapped to the most similar visual word. many researches argue that, for visual words, directly assigning an interest point to its nearest neighbor is not an optimal choice, given the fact that two similar points may be clustered into different clusters when increasing the size of visual vocabulary. on the other hand, simply counting the votes is not optimal as well. for instance, two interest points assigned to the same visual word are not necessarily equally similar to that visual word, meaning that their distances to the cluster centroid are different. ignoring their similarity with the visual word during weight assignment causes the contribution of two interest points equal, and thus more difficult to assess the importance of a visual word in video."
where and b are found by using an svc learning algorithm. and is a kernel function for the training sample and the test sample .
"with the recent advent of the cost-effective kinect, depth cameras have received a great deal of attention from researchers. it is excited to promote interest within the vision and robotics community for its broad applications [cit] . the depth sensor has several advantages over the visible light camera. firstly, the range sensor provides 3d structural information of the scene, which offers more discerning information to recover postures and recognize actions. the common low-level difficulties in rgb imagery are significantly alleviated. secondly, the depth camera can work in total darkness. there is a benefit for applications such as patient/animal monitoring systems which run 24/7. with these benefits, the kinect has been opened a new opportunity to improve the performance of human action recognition significantly. recently, researchers have paid more attention to using 3d spatiotemporal features for describing and recognizing human actions [cit] based on depth information from kinect. compared with conventional color data, depth maps provide several advantages, such as the ability to reflect pure geometry and shape cues, or insensitive to changes in lighting conditions. moreover, the range sensor provides 3d structural information of the scene, which offers more discerning information to recover postures and recognize actions. these properties help depth data provide more natural and discriminative vision cues than color or texture. furthermore, the depth images provide natural surfaces which can be extracted to capture the geometrical structure of the observed scene in a rich descriptor. however, depth sensors cannot differentiate between objects of the same depth but different color, which is trivial for color cameras. clearly the color and depth information are correlated but also complementary to a large extent, so it would be expected to have considerable benefits by fusing them appropriately together aiming at more robust pervasive action recognition systems."
"where is the vector of soft-counts associated with feature . the soft-weights of each visual word, contributed by all features in the video, are then pooled into a histogram: which is the final video representation ( is the number of descriptors). the standard average pooling operator aggregates word counts into bins of and normalizes as follows:"
"automatic human action recognition is attractive research topic in the fields of computer vision and machine learning since it plays an important role in the applications such as human-computer interaction, intelligent surveillance, human action retrieval system, health care, smart home, and robotics. due to its wide range of applications, automatic human action recognition has attracted much attention in recent years [cit] . the goal of human action recognition is to automatically analyze ongoing action from an unknown video (i.e. a sequence of image frames). generally speaking, action recognition framework contains three main steps namely feature extraction, action representation, and pattern classification. though much progress has been made [cit], the problem of classifying action is currently of the most difficult challenges, especially in the presence of within-class variation, occlusion, background clutter, pose and lighting condition. these challenges address that the combination of different kinds of features action because action representation based on single feature is not enough to capture the imaging variations (view-point, illumination, etc…) and attributes of individuals (appearance, shape, motion, etc…)."
"in this dataset, table i shows the experimental results of our different methods. from the results one can see that 3d-honv is the best descriptor in case only one descriptor is used and the fusion of hog3d, 3ds-honv and hof2.5d outperforms the single descriptor in using bow with k-means and gmm. fig. 8 presents a comparison of the accuracy of overlapping and non-overlapping segmentation with the difference on the length of segment. the overlapping method is better than the non-overlapping method in all cases. and, the length of 15 frames for each segment achieve the best performance. table ii compares our approach results with state-of-the-art results on utkinect-action dataset. we can see that our result of 93.5% in accuracy is better than all previous results using the same settings. our recognition rate is more than the current best rate by 1.6%. www.ijacsa.thesai.org"
"we firstly evaluate the performance of the proposed approach on the three challenging 3d action datasets such as utkinect-action, 3d action pairs, and msr-daily activity dataset. then we compare our results to the state-of-the-art methods to demonstrate the superiority of the proposed approach."
"secondly, we evaluate the performance of separation of descriptors and combination of descriptors that are used bow with k-means and gmm to yield the histogram represents for actions. furthermore, in order study the effect of the size of the video segmentation on the final classification performance, we choose segment lengths of 10, 15, 20, and 25 frames on nonoverlapping and overlapping segmentation. and we use uniform segment sampling with 50% of overlapping. therefore, the number of segments will be doubled for each overlapping experiment."
"the 3d action-pairs dataset contains activities which are selected in pairs such that the two activities of each pair are similar in motion and shape. for example, \"pick up\" and \"put down\" actions have similar motion and shape. this dataset has six pairs of activities: \"pick up a box/put down a box\", \"lift a box/place a box\", \"push a chair/pull a chair\", \"wear a hat/take off a hat\", \"put on a backpack/take off a backpack\", and \"stick a poster/remove a poster\". the dataset includes 12 activities performed by 10 different subjects. each action was performed three times by each subject. we used this dataset in order to emphasize two points: 1) to evaluate the performance of our proposed method in the case of actions that have similar trajectories and objects; 2) to show the advantage of using the feature fusion to enhance the recognition rate. [cit] 63.33 depth motion maps [cit] 66.11 skeleton + lop + pyramid [cit] 82.22 hon4d [cit] 96.67 snv [cit] 98.89 bhim [cit] 100 our approach 99.16 in this dataset, table iii shows the experimental results of our different methods. from the results one can see that hof2.5d is the best descriptor in case only one descriptor is used and the fusion of hog3d, 3ds-honv and hof2.5d outperforms the single descriptor in using bow with k-means and gmm. fig. 10 presents a comparison of the accuracy of overlapping and non-overlapping segmentation with the difference on the length of segment. the overlapping method is better than the non-overlapping method in most cases. and, the length of 15 frames for each segment obtain the best performance. table iv compares our approach results with state-of-the-art results on 3d action pairs dataset. we can see that our result of 99.16% in accuracy is better than most previous results using the same settings. our recognition rate is less than the current best rate is 100% by 0.84%. www.ijacsa.thesai.org"
"the key to the success of part-based methods is that the interest points are distinctive and descriptive. following the approach commonly used for local interest points in images and video, the detection and description of spatiotemporal interest points are separated in two different steps. this section describes local feature detector and descriptor used in our approach. for spatiotemporal interest points detector, we apply stip detector [cit] as a space-time extension of the harris detector [cit] . for spatiotemporal interest points descriptors, we use three descriptors such as hog3d [cit], 3ds-honv [cit], and hof2.5d [cit] ."
"in this work, we propose a new framework for human action recognition that combines both rgb images and depth maps. this approach falls in the part-based method category. more details, we use spatiotemporal features based on the interest points that are detected by stip in both rgb and depth channels. these interest points are represented by hog3d, 3ds-honv and hof2.5d that capture shape, appearance and motion of action. moreover, we also apply video segmentation and max pooling techniques to capture the temporal structure for action representation."
"for all used methods we have to determine the length of the segments or the number of segments for a video. for the action recognition task as described above long segments clearly have two disadvantages: longer segments have a higher risk of covering several subtopics and thus give a lower score on each of the included subtopics. in the second place, long segments run the risk that they include the relevant fragment but that the beginning of the segment is nevertheless too far away from the jump-in point that should be found. short segments, on the other hand, might get high rankings based on just a view words. furthermore, short segments make the recognizing process more costly. in our approach, we choose different length segments to select the optimal one."
"where is the filtered image, is the original input image, are the coordinates of the current pixel to be filtered, is the window centered in, is the range kernel for smoothing differences in intensities and is the spatial kernel for smoothing differences in coordinates. in this research, and are supposed as gaussian functions."
"in video segmentation stage, we divide the video into the set of segments. each segment is represented by three feature vectors (hog3d, 3ds-honv, and hof2.5d) that are computed by bow. we use the following temporal aggregation pools feature values for each feature dimension over time as fig. 7 . pooling features over time means that the temporal structure of action will be modeled. with three descriptors, we have three feature vectors for action representation. finally, we concatenate them into a final feature vector that presents for action. the vector feature will be provided to classifier to identify the label of action class which performed in video. in this research, the max pooling technique are proposed for aggregating feature vectors."
the model calculates the mos from these values. the calculation depends on the class of the video that is classified into three content types (cts) as follows.
"in order to solve the problem mentioned in the previous section, this section proposes a new handover algorithm that allows sharing of communication quality information between terminals to evaluate the mos of connectable access points around the current position as handover candidate destinations before connecting [cit] . this algorithm assumes that each terminal has a radio interface to facilitate direct communication between terminals in addition to another radio interface for connecting to an access point. since typical terminals often have multiple wireless interfaces such as wireless lan and bluetooth, this assumption should not cause any problems."
"in this work, we proposed a time interval aware self-attention model for sequential recommendation (tisasrec). tisasrec models the relative time intervals and absolute positions among items to predict future interactions. extensive experiments on both sparse and dense datasets show that our model outperforms state-of-the-art baselines. we also explore various features of this model. we demonstrated the influence of relative time intervals on next item prediction tasks."
"though our time interval aware attention layer is able to incorporate all previous items, absolute position, and relative time information with adaptive weights, it does so via a linear combination. after each time-aware attention layer, we apply two linear transformations with a relu activation in between, which could endow non-linearity to the model:"
"since many mobile terminals are moving while communicating, each terminal can share its estimated mos with other terminals so that it can select a destination candidate with a better mos than that of the currently connected network."
"in future work, the proposed algorithm will be evaluated in more complicated and practical network environments such as those where many mns move randomly meaning that the predicted mos will change over time. in the simulation results, even when two or four rns exist in the proposed method, their average moses do not so increase from that when no rns exist. thus, the qualitative evaluation must be required to show what kind of complicated and practical network environments requires the latest information of each ap from rns. since the proposed algorithm requires two network interfaces to introduce rns, it also needs to suppress the power consumption. for example, the beacon interval for the ad hoc communication may become longer to suppress the frequency for sharing the network status list. however, longer beacon interval may also decrease mos, since the network status list is out of date when the handover decision occurs in each terminal. in table 4, the coverage area of aps is 25 m and the mn's speed is always 1 m/s. then, each mn may stay 25 s in the same coverage area in the average. in this situation, it may be considered as the interval should be at most 12.5 s, a half of the average duration time. in this manner, the interval should be determined as considering the trade-off relationship between the power consumption and the sensitiveness to the fluctuation in network conditions."
"in this section, we first formulate our next item recommendation problem and then describe our approaches to obtain time intervals and components of tisasrec, which includes personalized time interval processing, an embedding layer, time-aware self-attention blocks, and a prediction layer. we embed items, their absolute positions, and relative time intervals. the attention weights are calculated by these embeddings. as shown in figure 1, our model will have different predicted items given different time intervals even if several previous items are the same. to learn the parameters, we employ a binary cross entropy loss as our objective function. the goal of tisasrec is to capture sequence patterns and explore influence of time intervals on the sequential recommendation which is an unexplored area."
"this section describes three evaluations of the proposed algorithm, which use network simulator 3 (ns3) [cit], and the first scenario compares them with the two handover algorithms described in section 3 and the conventional received signal strength reference algorithm that is the closed handover algorithm used by most mobile terminals and maintains a connection as long as the currently connected access point is connectable. the second and third scenarios also compare with the qos-driven bathich's method [cit] . in the previous and proposed algorithms, the expiration time of a record in the block list is 100 seconds. in the proposed algorithm and previous algorithm, each mobile node (mn) measures per of the currently connected ap every 100ms to estimate mos. the average mos calculation period is 3 minutes."
"in the first scenario, the mn moves along the predetermined route and executes handover according to each algorithm. figure 2 shows the network topology in the first scenario where the server transmits packets to the mn, and the router forwards packets between the server and one of the access points (aps) denoted as ap1-ap4. the mn connects to one of the four aps (ap1-ap4) selected by the adopted handover algorithm. figure 3 shows the simulation scenario. in the figure, the unit of coordinates is meters (m). the server sends udp packets containing movie data over udp toward the mn, the router forwards these packets to the ap to which the mn is connected, and the ap sends them to the mn. when the simulation starts, the mn stays at (0, 0) and is connected to ap1. during the simulation, the mn always moves along the dotted line shown in figure 3 . therefore, the mn goes straight from the starting point (0, 0), turns right at ap2, ap3, ap4 and returns to the point (0, 0) at ap1 again. since the mn's speed is 1 m/s, the simulation time is 150 seconds."
"both algorithms described in this section execute handover based on the mos of the currently connected network, while it cannot predict the mos of any destination candidates. therefore, if the mos of the destination network is lower than that of the previously connected network, the qoe becomes worse as a result of the handover."
"we evaluate our methods on six datasets from three real world platforms. these datasets have different domains, size and sparsity, and all are publicly available:"
"we compare tisasrec with the following methods. these methods include classic general recommendation (e.g. pop, bpr) without considering sequential patterns, first-order markov chain-based methods (e.g. fpmc, transrec) and neural network (nn) based methods (e.g. gru4rec+, caser, marank)."
"this paper proposed a new handover algorithm for video communications by sharing information on network communication quality with mobile terminals by ad hoc communication. in order to estimate the performance of the proposed algorithm, a simulation was carried out. the results of the simulation showed that the proposed method can execute handover appropriately and maintain higher qoe than other handover algorithms."
"when the average mos of the currently connected access point is lower than 3.5 and the beacon from the new access point is received, the terminal searches for the network id in the beacon from the list. if it is found, the terminal predicts the mos from per in the list. then, if the predicted mos is lower than that of the currently connected access point, the terminal remains connected to the current access point. otherwise, the terminal executes handover to the new access point that sent the beacon as in the previous algorithm."
"the difference in the average moses between the rss-based, bathich's method, and the proposed one shown in figures 9 and 10 is not so large. however, the difference in figure 9 is larger than that in figure 10 . as mentioned that the second scenario is considered for an urban area, the former figure is more important than the latter figure, since the result can be applied for many users."
"(1) directly use timestamps as the features. we will subtract the minimum timestamps in the dataset to let all timestamps start from 0. (2) using unscaled time intervals. for every user, we subtract the minimum timestamps in the sequence to let users' timestamps start from 0. (3) using personalized time intervals. for every time interval of a user, these intervals are divided by the smallest intervals as stated in section 3.4. table 6 presents the results of different timetamps processing methods. please note that we don't clip timestamps in the first two methods. we highlight the highest ndcg with boldface. method (3) (i.e. personalized time intervals) achieves the best performance in the first three datasets."
"where q, k, v represent queries, keys and values respectively. in self-attention, the three inputs usually use the same object. the self-attention modules of transformer have also been used in recommender systems [cit] and achieved state-of-the-art results on sequential recommendation. since the self-attention model doesn't include any recurrent or convolutional module, it is not aware of the positions of previous items. one solution is to add positional encodings to the inputs, which can be a deterministic function or learnable positional embedding [cit] . another solution uses relative position representations [cit] . they model the relative positions between two input elements as pairwise relationships. inspired by self-attention with relative positions, we combine the absolute positions and relative positions to design time-aware selfattention which models items' positions and time intervals."
"we remove the absolute position term in the two equations to obtain a new model. we denote this model as tisasrec-r. we view sasrec [cit] as a self-attention method which only considers the absolute position (rq2). we compare sasrec, tisasrec-r and tisasrec by applying them on the six datasets discussed above."
"in recent years, the user's experience referred to as quality of experience (qoe), rather than qos, needs to be considered during handover. qoe is a subjective measure of qos and is based on the user's perception of the overall quality of the service provided. for example, video viewers have a low qoe when the video is frequently affected by blocking noises and/or lack of synchronization between sounds and images."
"attention mechanisms have been shown to be effective in various tasks such as image captioning [cit] and machine translation [cit] . essentially the idea behind attention mechanisms is that outputs depend on specific parts of an input that are relevant. such mechanisms can calculate weights of inputs and make models more interpretable. recently, attention mechanisms have been incorporated into recommender systems [cit] . a purely attention-based sequence-to-sequence method, transformer [cit], achieved stateof-the-art performance on machine translation tasks. transformer uses the scaled dot-product attention which is defined as:"
"when the simulation starts, the mn stays at (50, 50) and is connected to ap9. during the simulations, the mn always moves at 1.5 m/s according to the random waypoint mobility model of ns3, i.e., the mn first randomly determines the moving direction, randomly selects the duration time between 2.0 seconds and 10.0 seconds, and then moves to the determined direction. when it elapses the duration time or the mn reaches the boundary of the topology range, the mn determines the next direction and the duration time."
"however, figure 10 evaluated only the average mos. to evaluate how long the mn connected to aps classified by mos values, table 6 shows the total and average connecting durations to the aps and the average number of handover in the third scenario, where the proposed-rn2 and the proposed-rn4 are omitted, since their average moses were almost same as the proposed-rn0. all results of connecting duration to each ap are shown in table 7 in appendix a. in the rss-based algorithm, the average number of handover was 2.8 less than the proposed one's. however, the total duration at aps whose mos is the lowest value of 3.0 was 113.5 s longer, and then the duration at ap9 whose mos is the highest value of 4.0 was reduced by 50.7 s. in bathich's method, the average number of handover was 0.9 greater than the proposed one. the total duration at aps with the lowest mos was 68.7 s longer, but the duration at ap9 was reduced by 104.0 s. from these results, the rss-based algorithm and bathich's method connected aps with lowest mos for longer time. since these methods had no disconnection time, their average moses were only a little smaller than the proposed one's. in the qoe-driven vho algorithm and the previous method, most aps are registered in the block list. since the mn could not connect to any ap for longer time, the disconnection time also became 334.7 s and 96.6 s, respectively, and then their average moses were also reduced. in contrast, in the proposed method, even if the mn cannot find any rns, the mn recorded aps with lower mos and avoided connecting to them again. as a result, the mn maintained to connect with aps with higher mos. from the above comparisons, the simulation results show that the proposed algorithm can select appropriate aps to maintain higher predicted mos than other handover algorithms including a qos-driven one, even if each terminal under the qos-driven bathich's method can obtain sinr of connectable aps by ieee 802.21. this is because per on each terminal, which is one of basic criteria for qoe-driven handover decision algorithms, can be obtained only after the terminal exchanged several packets with the currently connected ap. thus, the result shows that per is sometimes different from qos information such as rss and sinr."
influence of latent dimensionality d. figure 4 shows ndcg for dimensionality d from 10 to 50 while keeping the other optimal table 6 : comparison of three timestamp processing methods (ndcg@10).
"in this paper, we argue that user interaction sequences should be modeled as a sequence with different time intervals. figure 2 indicates that the interaction sequence has different time intervals, some of which might be large. previous work omitted these intervals and their influence on the predicted item. to address these above limitations, inspired by self-attention with relative position representations [cit], we propose a time-aware self-attention mechanism. this model not only considers the absolute positions of items like sasrec [cit], but the relative time intervals between any two items. from our experiments, we observe that our model outperforms state-of-the-art algorithms on both dense and sparse datasets. finally, our contributions are summarized as follows:"
"for fair comparison, we use the same maximum sequence length for sasrec as shown in table 3 . results are shown in table 5 . tisasrec-r has better performance than sasrec on the movie-lens, cds&vinyl and beauty datasets, but not for the other three. overall, using only relative time intervals will have similar performance. the main reasons that restrict the performance of tisasrec-r is that there are many identical timestamps in a user sequence (some have only a single timestamp). under this situation, tisasrec-r will degrade to self-attention without any positional information. to alleviate this problem, we combine relative time intervals with absolute positions. this way, tisasrec incorporates richer item relations to compute attention weights."
"in the proposed algorithm, mobile terminals rns with the proposed method also move randomly according to the same random waypoint mobility model. when the simulation starts, the rns are randomly placed within the circle whose center coordinate is (50, 50) and radius is 50 m, and connect to the nearest ap. if there are two or more such aps, then the one with the smallest number is selected. when the mn enters the rns' ad hoc communication range, it immediately shares each network status list with the rns. in the simulation of the proposed method, the number of the rns is selected from 0, 2, or 4. table 4 shows the simulation parameters. all the aps support ieee802.11g standard. their communication radius is 25 m and bandwidth is 11 mbps. since the per of ap1-ap4 is 3.0% and the one of ap5-ap13 is 17.5%, their predicted moses are 3.0 and 1.0, respectively. the content type of the transmitted video is rm with a sender bit rate of 4 mbps and a frame rate of 60 fps. the mn starts moving from the starting point (50, 50) for 600 seconds and executes handover according figure 9 shows the average mos and its 95% confidential interval of 10 simulations for each handover algorithms. in the received signal reference based algorithm (rss-based), since the mn continued connecting to the same ap until it moved out from the communication area, it connected to the ap whose predicted mos is 1.0 for a long time and the average mos became 1.84. in the bathich's method, since the mn sometimes succeeded to select a better ap from its qos-driven handover decision, the average mos of 1.93 is a little larger than that of the rss-based, but its confidential interval is covered by that of the rss-based. in the qoe-driven vho algorithm (qoedriven), when the mn received a beacon from other aps, it executed handover immediately and switched the connection to an ap with the lower mos, because all ap's moses were lower than the threshold. furthermore, since the mn recorded every connected ap into the block list, it could not connect to any ap for a long time. as the result, the average mos is 0.39, which is the worst result of all the algorithms. on the other hand, in the previous method (previous), which improved the qoe-driven vho algorithm, the mn removed networks from the block list if a predefined time passed after they were registered. since the mn was able to use again the aps to which it had already connected, the average mos is 0.66 that is slightly higher than that of the qoe-driven vho algorithm. however, since the mn could not use such aps until they were removed from the block list, there were periods when the mos became 0.0, the average mos is second-worst result of all the algorithms."
"we use tisasrec to predict the next item a user might interact with. we then set all time intervals to be the same to get another prediction. as shown in figure 7, time intervals influence on prediction results. figure 8 shows four heatmaps of average attention weights on the first 256 time intervals. note that when we calculate the average weight, the denominator is the number of valid weights, so as to avoid the influence of padding items in short sequences. from the four heatmaps, we make the following conclusion:"
an sm video has little movement of either the object or the background so that the changes in the entire screen are small. an example of this type would be a video showing a newscaster reading the news.
"in the second scenario, the mn randomly moves within the topology range using the random waypoint mobility model randomwalk2dmobilitymodel of ns3. to compared with a qos-driven method, this scenario added the bathich's method [cit] to the methods in the first scenario, since the method can obtain sinr (signal to interference and noise ratio) from the connectable aps by ieee 802.21, and sinr is related to per (packet error rate) that is used to predict mos in the proposed method. furthermore, in the proposed method, in addition to the mn, other mobile terminals are also implemented the proposed method and randomly move in the topology range. the number of the mobile terminals is changed for each simulation. figure 7 shows the network topology in the second scenario and figure 8 shows the position of all aps. the unit of coordinates is meters (m). this network consists of the server, the router, the aps (ap1-ap13) and the mn. the packet error rate of each wired link is 0%. the mos of ap1-ap4 is 3.0 and the one of ap5-ap13 is 1.0. in the figure 8, the squares and triangles represent ap1-ap4 and ap5-ap13, respectively. the dotted square shows the topology range. the dotted circles are the communication range of each ap. in the topology range, there is at least one ap that the mn can connect to."
method (1) influence of maximum sequence length n. figure 5 shows the ndcg for maximum length n from 10 to 50 while keeping other optimal hyperparameters unchanged. performance improves when considering longer sequences and eventually begins to converge. sasrec converges earlier than tisasrec.
"rq3: how do the parameters affect model performance, such as the number of dimensions, the maximum length of sequences and the maximum time intervals we consider?"
"inspired by relative position self-attention mechanisms [cit], we propose an extension to self-attention to consider the different time intervals between two items in a sequence. however, only considering the time intervals is not enough, because the user interaction sequence might have many instances with the same timestamp. under this condition, the model would become selfattention without any position or relation information. so, we also consider the position of items in a sequence."
"a handover is an operation that involves switching a base station or an access point as a wireless connection end point, when a mobile terminal is moving while communicating. 3) the terminal disconnects the connection with base station a, and then connects with base station b. if this process takes a long time, the communication is temporarily interrupted. in particular, when the terminal performs real-time communication such as voice and video, the quality the user experiences will become worse due to the interruption. thus, the handover algorithm is very important for switching the connection smoothly."
"although the qoe-driven vho algorithm can detect the degradation of qoe and start handover based on the prediction model, it also has some problems. when a certain mos prediction temporarily falls below the threshold for some reason, such as where many terminals are connected to the network within a short time, the terminal executes the handover to another network and registers the previously connected network in the block list. in order to solve this problem, the authors proposed an improved handover algorithm. in the algorithm, each terminal estimates the mos periodically and calculates the average mos from a fixed number of the estimated moses to take into account a temporary change in qoe [cit] . when the average mos is 3.5 or more, the terminal continues predicting the mos and updating its average value. otherwise, the terminal executes handover if it has received a beacon packet from another network. in addition, the terminal registers the previous connected network in the block list. however, after a certain period, the network is removed from the list. from this procedure, the terminal can connect again to the network, if the mos of the network recovers to an acceptable level."
"for the proposed algorithm, the terminals fn1 and fn2 are fixed at (37.5, 27.5) and (10.0, 37.5) and connected to ap3 and ap4, respectively. these terminals have a list containing information such as the per of the ap to which each is connected. when the mn enters the communication range of each ap, which is indicated by a large circle in figure 3, the ap's per is observed immediately. when the mn enters the ad hoc communication range of the fixed node, which is indicated by a small circle with a radius of 10 m (figure 3 ), it immediately starts ad hoc communication to share the list of the ap's information with other mns. table 3 shows the simulation parameters. ap1-ap4 support the ieee802.11g standard. their communication radius is 25 m and bandwidth is 11 mbps. although the maximum bandwidth of the ieee 802.11g standard is 54mbps, this paper selected 11 mbps as the effective bandwidth on outdoor fields. since their per is 10 %, 0.0 %, 3.0 %, and 5.5 %, their predicted mos is 2.0, 4.0, 3.0, and 2.5, respectively. the content type of the transmitted video is rm with a sender bit rate of 4 mbps and a frame rate of 60 fps. the server sends these packets at a constant bit rate over udp to the mn. figure 4, figure 5, and figure 6 show the changes in the predicted mos. the horizontal axis of the figures shows the elapsed time from the beginning of the simulation and the vertical axis shows the predicted mos values in the simulation scenarios for each algorithm. since these values overlap to a significant degree to facilitate comparison, figure 4 shows the predicted mos values of only the received signal reference and qoe-driven vho algorithms. for the same reasons, figure 5 international journal of networking and computing figure 6 compares the previous algorithm with the proposed algorithm. from figure 4, in the received signal reference algorithm, since the mn continued connecting to the same ap until it moved out from the communication area, the mn could not execute handover from ap1 to ap2 between 12.5 and 25.0 seconds. in the qoe-driven vho algorithm, since the mn detected that the predicted mos of ap1 was lower than the threshold value of 3.5, it executed handover to ap2 at 12.5 seconds so that it could provide a higher mos more quickly than the received signal reference algorithm. however, in the communication area of ap3, whose predicted mos is lower than the threshold, the mn executed handover to ap4 at 87.5 seconds even though it could continue to connect to ap3 even after 100.0 seconds have elapsed, and then the predicted mos dropped to 2.5. furthermore, since ap1 was still registered in the block list at the time the first handover was executed, there was no connectable ap after the mn moved out from the communication range of ap4, and then the mos was dropped to 0.0. on the other hand, as shown in figure 5, in the previous method, since ap1 was removed from the block list at 100.0 seconds, the mn executed handover from ap4 to ap1. but the previous algorithm also dropped the predicted mos to 2.5 between 87.5 and 100.0 seconds just as the qoe-driven vho algorithm did."
"in contrast, as shown figure 6, the proposed algorithm postponed executing handover until 100.0 seconds when the mn moved out from the communication area of ap3. this is because, when the mn moved inside the communication area of ap3, the mn performed ad hoc communication with fn2 to obtain the information list of ap4. as a result, the mn found that the predicted mos of ap4 was lower than that of the currently connected ap3, and then it could delay the execution of handover from ap3 to ap4."
"given our motivation for comparing the performance of models which only have absolute positions or relative time intervals, we modify eqs. 15 and 16 to let tisasrec only consider the relative time intervals and not consider absolute positions. we modify eq. 15 as:"
"generally, previous sequential recommenders discard timestamps and preserve only the order of items, that is, these methods (implictly) assume that all adjacent items in a sequence have the same time intervals. the factors that influence the next items are only the position and identity of the previous items. however, intuitively, items with more recent timestamps will have more influence on the next item. for instance, two users have the same interaction sequence but one of the users produced these interactions within one day, while another user completed these interactions in one month, therefore, the interactions of the two users should have different impact on the next items even if they have the same sequential position. however, previous sequential recommendation techniques view the two situations as the same because they only consider the sequential position."
"(1) small time intervals usually have larger weights than big intervals which means more recent items have more impact on the next items predictions. (2) dense datasets (e.g. movielens) need larger scope of items than sparse datasets (e.g. cds&vinyl), as there's a larger green region on the left of movielens heatmap, and a yellow region on the right. doesn't have obvious sequential patterns. this explains that why several sequential methods perform poorly on this dataset."
"this section describes the qoe prediction model [cit] for video streams on wireless networks. based on the prediction model, the qoe-driven vho algorithm [cit] and its improved version, referred to in this paper as the previous algorithm [cit], are also explained."
"in the model, the mos is predicted for a video from equation (1) whose coefficients from a 1 to a 5 depend on the ct of the video as shown in table 2 . (1). when the predicted mos is lower than the threshold value of 3.5, the terminal executes handover to another connectable network. note that when a video is evaluated by the acr (absolute category rating) method recommended by itu-t p.910, if the mos of the video is 3.5 then about 90% of the evaluators feel that the video's score is 3 (fair) or more in general [cit] . after completing the handover, the previously connected network is recorded in the block list as an ineligible candidate as it cannot provide the user with an acceptable qoe. next, the registered candidates are excluded from the destination candidates for the handover to maintain the qoe."
"this sharing operation is periodically performed by ad hoc communication in which the terminals directly communicate with each other. then each terminal exchanges its network information comprising network id, per, and record time of the connected networks, and adds the received network information into its own list if the network id is not already listed or updates the information if the record time of the network id is newer. since the proposed algorithm is dedicated for video communications, the proposed sharing operation may be installed on video applications. in this case, these applications typically monitor statistical information such as per, and then there is less cost to share a network status list if the applications agree with some sharing protocol standard such as wi-fi direct and ftp (file transfer profile) for bluetooth. when the video applications are restricted on a specific service, the service may also provide the sharing mechanism like dynamic dns for terminals with a single wireless interface."
"where n represents the maximum length that we consider. if the sequence length is greater than n, we only consider the most recent n actions. if the sequence length is less than n, we add padding items to the left until the length is n. similarly for the time sequence"
"we adopt two common top-n metrics, hit rate@10 and ndcg@10, to evaluate recommendation performance [cit] . hit@10 counts the rates of the ground-truth items among the top 10 items. ndcg@10 considers the position and assigns higher weights to higher positions. following [cit], for each user u, we randomly sample 100 negative items, and rank these items with the ground-truth item. we calculate hit@10 and ndcg@10 based on the rankings of these 101 items."
"two important lines of work that seek to mine users' interaction histories: temporal recommendation and sequential recommendation. temporal recommendation [cit] focuses on modeling absolute timestamps to capture the temporal dynamics of users and items. for example, the popularity of an item might change during different time slots, or users' average ratings might increase or decrease over time. these models are useful when exploring the temporal changes in datasets. instead of sequential patterns, they consider temporal patterns which are dependent on the time."
"in contrast, in the proposed method with no rns (proposed-rn0), the mn recorded the networks to which had been already connected and their parameters into the network status list. thus, when such networks in the status list became the candidates for handover, the mn compared the predicted mos of the candidates with that of the current network, and then it could execute handover to the network with higher mos. it connected to aps whose mos was 3.0 for a long time. as the result, the average mos is 2.30, which is the highest among the four methods. when the number of the rns are two or four (proposed-rn2/rn4), the average moses are 2.33 and 2.32, which are almost same as that of the proposed-rn0. this is because that as the simulation progressed, the unknown aps for the mn decreased and the mn had gathered all the information of all aps except for a short period after the scenarios started. since mos is always changing in a real environment, the mn needs to obtain the latest information of each ap from rns. in addition, when the topology range of the mn is wider and the shape is complex like an urban area, sharing the network status list with rns should be effective to maintain higher qoe. in the second scenario, all moses are lower than the threshold value of 3.5. this situation is typically considered as an urban area where there are many aps in some narrow region and so many people access to these aps. in the third scenario, in contrast, some moses are equal to or greater than 3.5 as shown in table 5 . this condition is considered as a suburban area where there are some aps and not so many people access to these aps. the other parameters such as the network topology, the start point of the mn, and the simulation time are the same as in the second scenario and table 4 . note that the third scenario increased the mn's speed from 1 m/s to 1.5 m/s to occur more handover. in the scenario, since there are five aps whose mos is equal to or greater than the threshold, the mn may maintain connecting to the same ap for long time, if the mn's speed is too slow. figure 10 shows the average mos and its 95 % confidential interval for each handover algorithms. in the received signal reference based algorithm, since the mn continued connecting to the same ap whose predicted mos is 3 or more, the average mos became 3.46. in the bathich's method, the mn selected a better ap from its qos-driven handover decision like in the second scenario and the average mos became 3.57. since the bathich's method often attempted to connect to the best ap, the mn switched the connection frequently. in the qoe-driven vho algorithm and the previous method, if the currently connected ap's mos is equal to or higher than the threshold value, then the mn continued connecting to the same ap and ignored the beacons from other aps to prevent from switching the connection frequently. otherwise, however, since the mn delayed to find better aps, the average moses of these algorithms stayed at 2.25 and 2.56, respectively. in the proposed method, since the mn properly used the network status list, it could compare the currently connected ap with the predicted ones to connect to a better ap. when the number of the rns are zero, two, and four, the average moses became 3.67, 3.67 and 3.7, respectively. since the topology range and the number of aps are the same as in the second scenario, sharing list function could not work well and the average moses had slight difference each other."
"after stacked self-attention blocks, we get the combined representation of items, positions and time intervals. to predict the next item, we employ a latent factor model to compute users' preference score of item i as follows:"
"in the proposed algorithm, each terminal basically works as in the previous algorithm described in section 3.3. moreover, each terminal creates a network status list comprising network id, per, and record time of the connected networks in order to share the list among connectable terminals around the current position. such per may depend on the situation of each terminal, because each terminal sends in different bit rate or different communication environment. each terminal such as a smartphone adopts the same communication standards such as ieee 802.11 and wimax, and has only one or two antennas within its compact body. under similar communication environment, the bit rate should also be similar, since each terminal typically selects the highest bit rate. thus, if it does not take for a long time from the record time, then the communication environment is expected to be same at the record time, and the received per should be adoptable."
"we implement tisasrec with tensorflow and fine-tune hyperparameters on our validation set. we use two time interval aware selfattention layers, and learned positional and interval embeddings and shared item embeddings in the embedding layer and prediction layer. the learning rate is 0.001, batch size is 128, dropout rate is 0.2 for all datasets. the rest of our parameter settings are shown in table 3 . all experiments are conducted with a single gtx-1080 ti gpu. table 4 shows the recommendation performance of all methods on the six datasets (rq1). among the baseline methods, marank [cit] has state-of-the-art performance compared with other baselines. marank can capture individual-level, union-level user interactions and tackle sparse datasets well. for dense datasets, neural network based methods (i.e., gru4rec+, caser) have obviously better performance than markov chain-based methods because these methods have stronger ability to capture long-term sequential patterns which is important for dense datasets. since markov chain-based methods (i.e., fpmc, transrec) focus on dynamic transition of items, they perform better on sparse datasets."
"qoe is often represented by a numerical measure called the mean opinion score (mos). the mos is the average score assigned by multiple users when evaluating the quality of a service and is expressed as an integer between 1 and 5 as shown in table 1 . for example, the previous handover algorithm described in section 3.3 specified that the mos over the current access point should be 3.5 or more. otherwise, mobile terminals connecting to the access point would consider performing handover to another access point if one could be found. 5 excellent 4 good 3 fair 2 poor 1 bad"
"however, miniaturising multiple stereo setups to the level as required by microscopes poses a problem to hardware fabrication since lens diameters restrict baseline gaps between cameras. as an alternative, a micro lens array (mla) may be placed in front of an image sensor of an otherwise conventional microscope [cit], which is generally known as a light field camera. an obvious attempt to regard the micro lens pitch as the baseline proves to be impractical as optical parameters of the objective lens affect a light field's geometry [cit], b) ."
"for example, a baseline b 1 ranging from a 0 to a 1 is identical to that from a −1 to a 0 . this relies on the principle that virtual cameras are separated by a consistent width. to apply the triangulation concept, rays are virtually extended towards the image space by"
"as it depends on the intersections at refractive main lens plane u, focal plane f u and the chief ray's travelling distance, which is f u in this particular case. with reference to preliminary remarks, an object ray's path may be provided as a linear function f i, j of the depth z, which is written as"
") by means of index variables s j, t h and u c+i, v c+g for spatial and directional domains, respectively. as can be seen from colour-highlighted pixels, samples at a specific micro image position correspond to the respective viewpoint location in a camera array. since raw spc captures do not naturally feature the e f s s j, u c+i index notation, it is convenient to define an index translation formula, considering the light field photograph to be of two regular sensor dimensions [x k, y l ] as taken with a conventional sensor. in the horizontal dimension indices are converted by"
where u and c have been omitted in the subscript of e i since i is a sufficient index for sub-aperture images in the 1-d row. equation (6) implies that the effective viewpoint resolution equals the number of micro lenses. figure 6 depicts the reordering process producing 2-d sub-aperture images
after substituting for z 0 . this approximation suffices to estimate the depth z for small rotation angles φ in stereoscopic systems without the need of an image rectification.
"over the years, several studies have provided different methods to acquire disparity maps from an spc [cit] . to the best of our knowledge, researchers have not dealt with the estimation of an object's distance using triangulation on the basis of disparity maps obtained from a light field camera. one reason might have been that baselines are required, which are not obvious in the case of plenoptic cameras as the optics involved is more complex than with conventional stereoscopy. attempts to estimate a plenoptic camera's baseline were initially addressed in publications by our research group [cit], b), which provided validation through simulation only. besides, main lens pupil positions have been ignored in this work, yielding large deviations when estimating the distance to refocused image planes obtained from an spc . it is thus expected that our previous triangulation scheme [cit], b) entails errors in the experimentation which is subject to investigation. [cit] has also proposed a baseline estimation method without giving details on the optical groundwork and lacking validation activities."
"where b n is an arbitrary scalar which can be thought of as a virtual image distance and n i, j as a spatial position at the virtual image plane of a corresponding sub-aperture. the scalable variable b n linearly affects a virtual pixel pitch p n, which is found by"
"let b u be the objective's image distance, then a chief ray's intersection at the refractive main lens plane u i, j is given by"
"the presented work has provided the first experimental baseline and distance results based on disparity maps table 7 disparity simulation and distance with g obtained by a plenoptic camera. predictions of our geometrical model match measures of the experimentation without indicating a significant deviation. an additional benchmark test of the proposed model with an optical simulation software has revealed errors of up to ±0.33% for baseline and distance estimates under different lens settings, which supports the model's accuracy. deviations are due to the imperfections of objective lenses. more specifically, prediction inaccuracies may be caused by all sorts of aberrations that result in a non-geometrical behaviour of a lens. by compensating for this through enhanced image calibration, we believe it is possible to lower the measured deviation. the major contribution of the proposed ray model is that it allows any spc to be used as an object distance estimator. a broad range of applications for which stereoscopy has been traditionally occupied can benefit from this solution. this includes endoscopes or microscopes that require very close depth ranges, the automotive industry where tracking objects in road traffic is a key task and the robotics industry with robots in space or automatic vacuum cleaners at home. besides this, plenoptic triangulation may be used for quality assurance purposes in the large field of machine vision. the model further assists in the prototyping stage of plenoptic photo and video cameras as it allows the baseline to be adjusted as desired."
"to verify claims made about spc triangulation, experiments are conducted as follows. baselines and tilt angles are estimated based on eqs. (22) and (26) using parameters given in tables 1 and 2 . thereof, we compute object distances from eq. (27) for each disparity and place real objects at the calculated distances. experimental validation is achieved by comparing predicted baselines with those obtained from disparity measurements. the extraction of a disparity map from an spc requires at least two sub-aperture images that are obtained using eq. (6). disparity maps are calculated by block matching with the sum of absolute differences (sad) method using an available implementation [cit] (abbeloos, 2012 . to measure baselines, eq. (27) has to be rearranged such that"
"to conceptualise a light field ray model for an spc, we start tracing rays from the sensor side to the object space. for simplification, we consider chief rays only and follow their path from each sensor's pixel centre at micro image domain u to the optical centre of its corresponding micro lens s j with lens index j. in an spc, the spacing between mla and image sensor plane amounts to the micro lens focal length f s . fig in earlier publications [cit], b), it was assumed that mics lie on the optical axes of corresponding micro lenses. however, it has been argued that this assumption would only be true if the distance between objective lens and mla were infinitely large [cit] . due to the finite separation, mics are displaced from their micro lens optical axes. a more accurate approach in estimating mic positions is to model chief rays in a way that they connect optical centres of micro and main lenses [cit] . in fig. 4b we further refine this hypothesis by regarding the centre of an exit pupil a to be the origin from which mic chief rays arise. detecting mics correctly is essential for our geometrical light ray model because mics serve as reference points in the viewpoint image synthesis. figure 5 depicts our more advanced model that combines statements made about light rays' paths in an spc. for clarity, the main lens u is depicted as a thin lens meaning that the exit pupil centre coincides with the optical centre. however, the distinction is maintained in the following."
"further research may investigate how triangulation applies to other types of plenoptic cameras, such as the focused plenoptic camera or coded-aperture camera. more broadly, research is also required to benchmark a typical plenoptic camera's depth resolution against that of competitive depth sensing techniques like stereoscopy, time of flight and light sectioning."
"an object point's depth distance z can be directly fetched from parameters in fig. 2 . as highlighted with a dark tone of grey, x may represent the base of any acute scalene triangle with b as its height. another triangle spanned by the base b and height z is a scaled version of it and shown in light grey. this relationship relies on the method of similar triangles and can be written as an equality of ratios"
"and therefore, where k and l start to count from index 0. to apply the proposed ray model and image process, the captured light field has to be cali-brated and rectified such that the centroid of each micro image coincides with the centre of a central pixel. [cit] ."
"as the name suggests, sub-aperture images are created at the main lens' aperture. to investigate ray positions at the aperture, it is worth introducing the aperture's geometrical equivalents to the proposed model, which have not been considered in our previous publications [cit] . an obvious attempt would be to locate a baseline b a at the exit pupil, which is found by"
"to obtain quantitative measures, this section investigates the positioning of a virtual camera array by modelling a plenoptic camera in an optics simulation software [cit] . table 6 reveals a comparison of predicted and simulated virtual camera positions just as their baseline b g and relative tilt angle φ g . thereby, the distance from an objective's front vertex v 1u to entrance pupil a is given by"
"where f s is the micro lens focal length and d a is the distance from mla to exit pupil of the main lens, which is illustrated in fig. 4b . micro image sampling positions that lie next to mics can be acquired by a corresponding multiple i of the pixel pitch p p as given by"
"the object-side-related position of a i can be acquired by with this, a baseline b g that spans from one a i to another by gap g can be obtained as follows"
"fig. 5 illustration of the spc ray model, where mics can be found by connecting the optical centre of the main lens with that of each micro lens and extending these rays (highlighted in yellow) until they reach the sensor. here, the main lens is modelled as a thin lens such that entrance and exit pupils are in line with principal planes (color figure online) which means that [x k ] is formed by"
"where c has been left out in the subscript of u i, j as it is a constant and will be omitted in following ray functions for simplicity. the spacing between principal planes of an objective lens will be taken into account at a later stage."
"in essence, this paper presented the first systematic study on how to successfully apply the triangulation concept to a standard plenoptic camera (spc). it has been shown that an spc projects an array of virtual cameras along its entrance pupil, which can be seen as an equivalent to a multi-view camera system. thereby, the proposed geometry of the spc's light field suggests that the entrance pupil diameter constrains the maximum baseline. [cit], who considered the aperture size to be the baseline limit. our customised spc merely offers baselines in the millimetre range, which results in relatively small stereo vision setups. due to this, depth sampling planes move towards the camera, which will prove to be useful for close range applications such as microscopy. it is also expected that multiple viewpoints taken with small baselines evade the occlusion problem."
"taking the left camera as the orientation reference, the right lens o r is seen to be tilted as shown in fig. 3 . in this case, perspective image rectification is commonly employed to correct for non-coplanar stereo vision setups [cit] concludes that optical axes intersect in a point z 0 as both axes lie on the x, z plane if angle rotation occurs around the y-axis, whereas image planes of both cameras are still seen to be parallel. [cit] method serves as a first-order approximation for small angle rotations in the absence of image processing. as demonstrated in sect. 3.2, this approach, however, is suitable for our plenoptic triangulation model where imaginary sensor planes of virtual cameras are coplanar, whilst their optical axes may be non-parallel. let φ be the rotation angle, then laws of trigonometry allow to put"
"establishing the triangulation in an spc allows object distances to be retrieved just as in a stereoscopic camera system. on the basis of eq. (5), a depth distance z g, x of an object with certain disparity x is obtained by"
"reasonable scenarios exist in which a camera's optical axis is tilted with respect to the other. in such a case, the principle of similar triangles does not apply in the same manner as in eq."
"in this paper, we propose a refined optics-geometrical model for light field triangulation and estimate object distances captured by an spc. our plenoptic model is the first to pinpoint virtual cameras along the entrance pupil of the objective lens. verification is accomplished through real images from a custom-built spc and a ray tracing simulator [cit] ) for a quantitative deviation assessment. a top-level overview of the processing pipeline for experimental validation is given in fig. 1 . by doing so, we obtain much more accurate baseline and object distance results than by our previous method [cit] . the proposed concept will prove to be valuable in fields where stereo vision is traditionally used. this paper has been organised in the following way. section 2 briefly reviews the binocular vision concept by means of the geometry in order to recall stereo triangulation. this is followed by a step-wise development of an spc ray model in sect. 3 where the extraction of viewpoints images from a raw spc capture is also demonstrated. experimental work is presented in sect. 4, which aims to assess claims made in sect. 3 by measuring baseline and tilt angle from a disparity map analysis and a ray tracing simulation [cit] . results are summarised and discussed in sect. 5."
"by shifting the sensor away from the mla focal plane, research has shown that the spatial and directional resolution can be traded off, which involves different image synthesis approaches [cit] . to distinguish between these optical setups, lytro's camera was later named standard plenoptic camera (spc) [cit], who devised a more complex mla that features different micro lens types. the spatio-angular trade-off in a plenoptic camera is determined by diameter, focal length, image position and packing of the micro lenses, just as the sensor pixel pitch, which thus makes it part of the optical hardware design."
"in the previous section, it was shown how to render multiviews from spc photographs by means of the proposed ray model. because a 4-d plenoptic camera image can be reorganised to a set of multi-view images as if taken with an array of cameras, it is supposed that each of these images has an optical centre of a so-called virtual camera with a distinct location. the localisation of such is, however, not obvious. this problem was first recognised and addressed in publications by our research group [cit], b), but, however, lacked of experimental verification. as a starting point, we deploy ray functions that proved to be viable to pinpoint refocused spc image planes and further refine the model by finding intersections along the entrance pupil. once theoretical positions of virtual cameras are derived, we examine in which way the well established concept of stereo triangulation (see sect. 2) applies to the proposed spc ray model. in order to geometrically describe rays in the light field, we first define the height of optical centres s j in the mla by"
"with a chief ray passing through a point f i, j along the main lens focal plane f by means of its image side slope m c+i, j and the main lens focal length f u . consequentially, a chief ray slope q i, j of that beam in object space is given by"
"computer vision has been striving to recreate our human visual perception. wheatstone's fundamental observations (wheatstone 1838) state that a set of solely two adjacent cameras facilitates imitating a human's binocular vision. using these two images in conjunction with a stereo display technique, e.g. stereoscopic glasses [cit], allows for the reproduction of depth as perceived by human eyes. with regard to the location in object space, however, this stereo vision system concedes much more freedom than the human's perception as the distance between cameras, called baseline, may vary. hence, the flexibility in camera stereoscopy makes it possible to adapt to particular depth scenarios. for example, triangulation is used in stellar parallax to measure the distance to stars [cit] . what applies to a macroscopic universe, may also be useful for a microscope."
"ensuring society's security and welfare is one of the main concerns of every government, however, this is not an easy task. the main purpose of video surveillance is to monitor activity occurring within an area of interest. so far, video surveillance systems have proved to be very good for access control, preventing crime, or even when criminal activity has occurred, video surveillance turns to be very helpful on providing valuable evidence for further criminal investigations."
"the acu manages the tss admission while maintaining the qos of the already admitted ones. when a new ts demands an admission, the acu first obtains a new si as shown in the previous step and computes number of msdus arrived at the new si using equation (3). next, it calculates the t xop i for the particular ts using equation (4) . finally, acu admits only the ts if the following inequality is satisfied."
"different visualization techniques have been used to improve data understanding of a specific phenomena. the most common, easy and understandable way to visualize data is through classical charts (pie-charts, bar-charts, time-series, etc.) [cit] and heatmaps [cit] . on the other hand, more complex information, such as multiparametric data, requires more complex techniques (multiparametric data visualization techniques) [cit] . circle segments, recursive patterns, parallel coordinates and chernoff face are examples of these."
"although video surveillance systems have been widely used for several years, there are some issues closely related to this type of application. problems start to arise when the amount of video cameras connected to the system becomes considerably large, this decreases the system's overall monitoring performance. the greatest issue on assigning a security guard for the task of watching over a set of video streams, is that people's attention, in general, is limited; therefore, there is always a good chance that they miss some relevant information, that in the context of security, is considered inadmissible because of the consequences this might have. another important problem with classic surveillance systems is that the data stored by the system (video footage) has such enormous dimensions, that it becomes intractable for off-line analysis; for instance, looking for evidence on some reported criminal activity. even when the problem of intractability in analysis of information for video surveillance systems has been well defined, approaches on solving this issue are still in an early stage."
"once the vp module extracts relevant information from the scene, we send this information to the cim sub-module, which resend it to the visualization module where the graphics to be visualized are generated. in addition to this real time information, it is also possible to visualize the historical data stored at the storage sub-module."
"mpeg-4 is an efficient video encoding covering a wide domain of bit rate coding ranging from lowbit-rate for wireless transmission up to higher quality beyond high definition tv (hdtv) [cit] . for this reason, mpeg-4 video coding has become from among the prominent videos in the internet nowadays."
"there are only a few existing tools [cit] to analyze such read mapping profiles of rna-seq data and detect footprints of the post-transcriptional processes or degradations of rnas. flaimapper [cit] predicts and annotates non-coding rna-derived fragments by determining the start and stop positions from read mapping profiles. flaimapper can predict mirna boundaries with high accuracy. deepblockalign [cit], which our present study takes quite a similar approach to, is an alignment-based method for alignments of read mapping profiles to find non-coding derived rnas that share similar post-transcriptional processing. alps [cit] ) is also alignment-based, but is not designed for the purpose of identification of short derived rna fragments. blockclust [cit] computes a similarity score between read mapping profiles to detect similar processing patterns and cluster read mapping profiles. blockclust includes a high-dimensional feature representation to encode read mapping profiles and calculates the similarity scores based on a graph kernel. therefore, blockclust does not rely on alignment-based techniques and is not designed for the purpose of calculating the alignment of read profiles. however, all of these methods are computed based only on the information of read mapping profiles and do not take the rna sequence and secondary structure information into account. further, those methods did not address the reliable distinguishability of processing patterns from the random degradation."
"in order to incorporate the dafs framework of the simultaneous aligning and folding of rna sequences into the alignment of read mapping profiles on the basis of the mea principle, we need to define the expectation of the gain function for alignments of read mapping profiles. first, we define the partition function [cit] for alignments of read mapping profiles. the partition function is equal to the alignment kernel k(a, b) [cit] where two variables rg and lg are used for controlling the number of inserted gaps. the 'backward' algorithm to compute bði; j; rg; lgþ can also be defined in a similar manner. now, we can define a posterior probability of positions i and j to be aligned in the alignments of read mapping profiles:"
"with the aim of offering computational tools for comprehensively analyzing the post-transcriptional processing patterns of non-coding rnas and detecting their common processing patterns, based on rna-seq, we developed a new algorithm called sharaku to align two read mapping profiles of next-generation sequencing data for non-coding rnas. sharaku incorporates the primary and secondary structures of rna sequences into an alignment of read mapping profiles by combining with dafs, which constructs reliable structural alignments that maximize the expected accuracy of a predicted common secondary structure and its sequence alignment. sharaku could simultaneously align the read mapping profiles and rna sequences with information of the folded rna secondary structures. in an experiment using a simulated dataset, sharaku achieved an almost perfect clustering result, and exhibited higher accuracy than deepblockalign. in an experiment with real data of small rna sequencing for the common marmoset brain, sharaku succeeded in identifying the five major clusters plus four scattered clusters representing typical processing patterns. this method also revealed some interesting clusters consisting of mixtures of several rna families that predicted common processing patterns among different rna families. these results demonstrate that sharaku can be an indispensable tool for analyses of the processing patterns and functions of regulatory non-coding rnas with deep-sequencing data."
"(i) manager sub-module: this sub-module acts like a bridge for communicating the outside of the module with internal sub-modules, retrieves data from the storage module and communicates with the selected visualization technique for generating the requested visualization."
"in the following section, we propose a video surveillance system that integrates geovisualization techniques to provide a representation of data in a way that the exploration, analysis and understanding of all of the monitored areas' behavior, as a whole, is possible."
"during the last two decades, geovisualization has been used for different applications such as exploring traffic data (air, ground), transportation routs, diseases, weather, etc. thus, by combining geovisualization with video surveillance gives us some advantages. for instance, we have the possibility to develop, implement and embed icts (information and communication technologies) into a smart city and consequently provide the user access to different layers of information that might be combined and integrated to facilitate the analysis of a situation, and improve the decision making performance [cit] ."
"where m is the maximum msdu and o denotes the overhead, including mac and physical layer mode (phy) headers, interframe spaces (ifss), and the acknowledgment and poll frames overheads."
"we have implemented three visualization techniques. two with iconic representation (chart-based) and one geometrical (multiparametric). the two chart-based (pie-chart and bar-chart) techniques implemented in the system can be used for both, visualize the volatile information which is arriving to the context from the vp and for visualizing small time windows of stored data. the multiparametric technique implemented (parallel coordinates) is only used to visualize large amounts of historical data."
the general aim of the visualization techniques is to produce a representation in which it is possible to observe the correlations between the variables [cit] without previous knowledge about the data. for the smart security scenario we are using several visualization techniques to handle data and its relation with the geospatial component. by using these techniques it is possible to provide to the users relevant information of the monitored areas.
"after the traffic setup phase, the qap transmits the first poll frame granting the qsta a txop duration. the station will accordingly transmit the first packet of its traffic to the qap. note that the interarrival time between encoded video traffic frames is a 7 figure 4 : flow chart of the proposed scheduling algorithm multiple of a fixed interval (typically 40 ms) depends on the encoding parameters. that is to say, it is expected to receive only one packet at a multiple of a designated interval. details about the operation of our approach at qap is reported in fig. 4 . at the beginning of each cap, hc goes through the stations i list and computes t xop i for the station i according to one the these cases: first case is when a data packet is received from the station i in the previous cap/si period, the msdu size (size i ) of the next frame is obtained from the qs field of ieee 802.11e mac header. then, a t xop i of qst a i is calculated using equation (6) . in other words, the txop in si i+1 is scheduled based on the information received by qap/hc during si i, as depicted in figure 3 ."
"in this algorithm, the exact msdu size of the next frame of the uplink stream is obtained from the application layer through cross layering. this information is transmitted with each packet to the qap carrying the next frame size. upon a data frame reception, the hc recalculates the txop duration to be granted for a particular station in the next si so as to adapt to the fast varying in vbr video traffic. consequently, it minimizes the packet end-to-end delay and conserve the channel bandwidth. in this section, we present the description of txop operation at both qstas and the qap."
"in order to show how our visualization module works lets see the following example. suppose the user wants to see the amount of people, vehicles and other objects that transited in front of each of the four cameras from \"may 8 13:00:00 [cit] \" to \"may 8 15:00:00 [cit] \". to do that, we perform the communication between modules through json messages. in listing 1 we are showing the type of message to make a query and then generate a visualization such as the one depicted in figure 5 (a). sometimes, local visualization does not allow the global understanding of a phenomenon, or the amount of data is such that it becomes impossible to visualize with the common techniques (chart-based). to solve this problem we provide another type of visualization technique, the multiparametric data (parallel coordinates) visualization. for the sake of simplicity, we will first explain how to interpret graphics generated with the parallel coordinate technique for only one video stream. in figure 6 we are showing an instance of this visualization technique. the first axis represents the hour of the day (in hours). to identify the hour of the day we are using lines with different colors (from green to red). the second axis is used to represent the camera id. the remaining axes are used to represent the attributes (persons, vehicles, others) that we want to observe. this graph is interpreted as follows, the intersections between each horizontal line with each of the vertical axes denotes the value of the attribute for a specific hour of the day. figure 6 shows that the camera (a) was working for the entire day, during this period of time the number of persons never exceeds 40. furthermore, we can observe that cars and other objects are not detected in this monitored area. for this figure we are visualizing the content of only one camera, however, all the cameras might be visualized at the same time (see figure 7) ."
"however, when it comes to video surveillance systems, it is almost impossible for a user to keep track on what is occurring within a large set of video streams simultaneously."
(iii) chart sub-module: this module allows to visualize volatile information managed by the context in real time with high performance and without losing descriptive information.
"second, when we calculate a structural alignment for a pair of rna sequences, their correct structural alignment is unknown. in such case, we compute the expectation of a gain function under the distribution over all possible structural alignments of the pair of rna sequences. the expectation e hja;b ½gðh; b hþ of the gain function gðh; b hþ under a given probability distribution over the space a s ða; bþ of structural alignments is maximized to find a structural alignment b h:"
"a representative read mapping profile in each cluster is shown below the clustering tree. interestingly, 60% of the trnas were processed for deriving shorter fragments mostly at the 5 0 -ends, whereas 80% of the snornas were non-processed. a few other interesting small clusters were obtained and will be discussed below. in addition to confirming the utility of the new algorithm, this experiment revealed the post-transcriptional processing and the expression patterns of small derived rnas in the marmoset brain. to the best of our knowledge, this represents the first identification of the processing patterns of derived rnas expressed in the brain. the result of hierarchical clustering based on the deepblockalign alignments is shown in supplemental figure s4 ."
"since high-throughput sequencing allows for deep sequencing with high sensitivity, rna sequencing with a next-generation sequencer (rna-seq) can detect not only the complete expression patterns of transcribed rnas but also fragments derived due to the splicing, maturation processing, or non-functional degradation of the rnas. rna-seq studies targeting micrornas (mirnas) have revealed the existence of many different rna fragments derived from small rna species other than microrna, providing further proof that derived rna fragments are not simply random degradation products but are rather stable entities, which might have functional activity [cit] . the evidence accumulating about shorter sequences or fragments derived from non-coding rnas indicates that post-transcriptional processes are relatively common mechanisms to derive functional smaller molecules from various rna families such as trnas and snornas. for example, the so-called trna-derived rna fragments (trfs) are derived from processing at the 5 0 or 3 0 -end of mature or precursor trnas (lee"
"the dendrogram of clustering tree of hierarchical clustering based on the sharaku alignments of read mapping profiles of the 619 non-coding rnas is shown in figure 4 . most of the read mapping profiles were clustered and well separated into five major clusters, representing (1) 3 0 -end processing of mirnas (containing 80 mirnas); (2) 5 0 -end processing of snornas and trnas (containing 25 snornas, 62 trnas and 5 mirnas); (3) non-processing and non-degradation of snornas and trnas (containing 210 snornas, 10 trnas and 1 mirna); (4) degradation of snornas and trnas (containing 42 snornas, 20 trnas and 1 mirna); and (5) 5 0 -end processing of mirnas (containing 81 mirnas and 2 trnas). in addition, the read mapping profiles representing 3 0 -end processing of snornas and trnas (containing 26 snornas, 3 trnas and 3mirnas) were scattered into four different clusters (6-1), (6-2), (6-3) and (6-4). these 4 clusters could be defined as the complement of the clusters (3) and (4), which might provide a clear interpretation."
"a star topology has been used for constructing the simulation scenario to form an infrastructure network with one qap surrounded by varying number of the qstas ranging from 1 to 12. all qstas were distributed uniformly around the qap with a radius of 10 meters as shown in figure 5 . the stations were placed within the qap coverage area, in the same basic service set the basic service set (bss), and the wireless channel is assumed to be ideal. since we focus on hcca performance measurement, all the stations operate only on the contention-free mode by setting t cp in equation (5) to zero. qap is the sink receiver, while all stations are the video sources. each send only an uplink video traffic as only one flow per station is supported in ns2hcca patch. therefore, for simulating concurrent video streams multiple stations are added each with one flow. in order to leave an ample time for initialization, stations start their transmission after 20 (sec) from the start of the simulation time and last until the simulation end. wireless channel assumed to be an error-free, and no admission control used for the sake of investigating the maximum scheduling capability of each examined algorithm under heavy traffic conditions. simulation parameters are summarized in table 2 . for evaluating the performance of our scheduling algorithm against the reference scheduler of hcca, jurassic park 1 video sequence trace encoded using mpeg-4 was chosen from a publicly available library for video traces [cit] . we tested the proposed scheduler on jurassic park 1 and formula 1 trace files which can be classified into movie and sport, respectively, which show different variability level. table 3 demonstrates some statistics of the examined traces. the table 4 with regards to video qos requirements."
"where pðxjaþ and pðyjbþ are the probability distributions of rna secondary structures over sðaþ and sðbþ, respectively, and pðtja; bþ is a probability distribution of alignments over aða; bþ. the expected gain function (2) can then be approximated as follows:"
the rest of the paper is organized as follows. section 2 illustrates the reference hcca mechanism and its deficiency in supporting vbr video streams and demonstrates some of the hcca related works. section 3 illustrates the proposed dynamic assignment algorithm. the performance evaluation and results discussion is shown in section 4. section 5 concludes the work presented in this paper.
"where p ðaþ ij denotes a base-pairing posterior probability, p ða::bþ ik denotes an alignment-matching posterior probability, r and s ð0 r; s 1þ are balancing parameters between true positives and true negatives, and c is a constant independent of b h [cit] and its supplementary material for the derivation)."
"as previously stated, the main problem in video surveillance systems is the intractability in analysis, caused by the large amounts of data generated by the system and people's limited capability to perform several tasks simultaneously. nevertheless, there has been a lot of work made on the development of computer vision techniques to detect events from a real-time video stream. the automatic detection of certain situations, by its own, does not solve the intractability issue. a method able to represent huge amounts of information into compact and understandable graphics is still required. for this reason, scientific visualization becomes a very useful tool to address the intractability problem by generating images that show, with great quality, as many aspects of the original information as possible. moreover, by integrating a geographical information system along with scientific visualization to a video surveillance system, besides having a synthesized version of the data, it helps the user to know where the data is coming from, by relating every data plot (scientific isprs annals of the photogrammetry, remote sensing and spatial information sciences, volume iv-4/w3, 2017 2nd international conference on smart data and smart cities, 4-6 [cit], puebla, mexico visualization) to a geographical location."
"the behavior of the examined schedulers in terms of allocating txop in each si is illustrated in figure 6 for the formula 1 video sequence. the allocated txop for one flow is shown against number of sis for a duration of 10 seconds. the results reveal the fact of assigning fixed txop in hcca for all sis of the flow with accordance to equation (4) . in this case, the hcca computes txop duration based on the maximum msdu size of the flow, namely 7032 bytes and 14431 bytes for low and high quality video respectively. nevertheless, the proposed scheduler adaptively allocates a txops for each si based on the actual frame size obtained from the feedback information which show that in some sis the allocated txop duration in hcca is much higher than the actual need of the flow which considered as over-allocation cases. it is obvious that the txop duration given in 6(b) is higher than that in 6(a) as the mean bit rate of high quality encoded formula 1 is considerably higher than that in low quality video sequence, refer to table 3 ."
"in figure 2 we can identify the five sub-modules of the smart security module: (i) video processing (vp), which is a server for media content that eases the implementation of applications that involve video streaming. in this sub-module is where all the algorithms related to the video stream processing are implemented, (ii) context information manager (cim), used to create temporal entities capable of handling information useful for other applications, this information is available to every user subscribed to such entities. this sub-module can be seen as the moderator that carries out the communication process between the other sub-modules. (iii) security sub-module which is required considering the context of our system (surveillance scenario), thus, the access control becomes an important element that needs to be implemented. (iv) storage, a sub-module for data persistence capable to store relevant information in order to retrieve it later, and finally (v) visualization, where we deal with all the information obtained from processing the video stream (video processing sub-module) which is shown to the users. for more de- it is worth mentioning that all these sub-modules are based on the fiware 1 middleware platform, which provides a set of apis that facilitate the design and implementation of smart applications. see [cit] in this same volume for more information about the architecture."
to test the smart security system we use as test scenario the inaoe's campus. to visualize the map we use openstreetmap 2 and for data visualization we used d3.js (parallel coordinates visualization) and extjs (charts visualization and gui) javascript libraries. all the data is coming from a video surveillance system prototype that we already have implemented. this prototype consists of four geolocalized ip cameras. in order to get latitude and longitude of each camera we used the gps of a smartphone.
"using equation (3), the reference scheduler calculates the expected number of the received packets every si based on the mean tspec parameters at the traffic setup phase. in our algorithm, this parameter is set to 1 as only one packet is expected to be received every si."
"to validate the behavior of the examined schedulers, the measurements are done for an increasing number of tss. the system throughput was also investigated to verify that the improvement in the delay is achieved without jeopardizing the channel bandwidth."
"hcca scheduler computes the txop duration by estimating the amount of data expected to be transmitted by the qsta during service interval (si). this es- timation is based on the tspec negotiated with hc which considers the mean characteristics of the traffic, equation (4). the proposed scheduling algorithm described in this section is referred to as dynamic transmission opportunity because it adapts txop duration based on the feedback of the next frame packet size reported by qstas. the proposed algorithm gives an actual txop needed by stations and ensures that the delay is minimized without jeopardizing the channel bandwidth. the delay experienced by the proceeding unused(wasted) txops is minimized using this algorithm as illustrated in figure 3 . the scheduling parameters along with the scheduling operation are described below."
"in this work we propose a video surveillance system that integrates gis technologies and scientific visualization techniques. with this system we are able to acquire a video stream from a set of cameras, process that information and display it in such way that we give to the user access to different layers of information in order to provide a means that makes easy to analyze and understand what is happening in a monitored area."
"to investigate the efficiency attained by the proposed scheduler in supporting prerecorded videos against hcca, the aggregate txop duration is measured and can be defined as the total of txop duration granted to all qstas for the simulation time in units of seconds. in figure 9, the aggregate txop is shown in the examined videos with increasing the network load. for low-quality videos figures 9(a) and 9(c) demonstrate that allocating fixed txop for all ts frames in hcca might exceed the need of the traffic. in that case only a small portion of the granted txop is exploited resulting in what's called wasted txops. on the contrary, the proposed scheduler operates according to the actual information about frame size, the granted txop is considerably smaller than that in hcca without jeopardizing the throughput. this fact is more obvious when transmitting high-quality videos (figures 9(b) and 9(d)) where the wasted txop is much higher. it is worth mentioning that jurassic park 1 shows higher wasted txop than that in formula 1 as the cov is higher."
"the reads for such short derived rnas are relatively abundant, i.e. greater than background levels, in small rna-seq datasets. [cit] pointed out that 'these reads are precisely mapped to specific regions of primary or secondary structures, and might contain special motifs or base biases, reflecting the involvement of special enzymes involved in their generation'. hence, mapping a large amount of sequence reads onto a reference sequence can reveal specific forms of mapping patterns for the maturation process or random patterns for non-functional degradations (see fig. 1(a) ). these mapping patterns of rna-seq short reads constitute the socalled read mapping profiles (see fig. 1 (b)) . [cit] also pointed out that the non-specific degradation products include rna fragments that are rapidly digested by the surveillance machinery from rna molecules, which are defective in processing, folding and functions. thus, it is crucial to reliably distinguish the true shorter rnas from their non-functional degradation products to clearly identify derived and functional small rnas and fragments. therefore, the aim of the present study was to develop a computational tool for the comprehensive analysis and rapid identification of the post-transcriptional processing patterns of non-coding rnas based on high-throughput rna-seq data. the algorithm developed in this study was designed to capture specific forms of read mapping patterns mapped to specific regions of primary or secondary structures reflecting the functional activities of enzymes, and to distinguish them from non-functional degradation products."
"where g i is the generation time of packet i at the source qsta, r i is the receiving time of the particular packet at the mac layer of the qap, and n is the total number of packets for all flows in the system. the end-to-end delay has been measured for the three video types to study the efficiency of both hcca and our schedulers with different traffic variability. figure 7 (a), 7(b) and 7(c), 7(d) depict the delay experienced by data packets for the low, medium and high quality video, respectively. one can notice that the end-to-end delay boosts with the increase of the packet size, higher quality exhibit higher endto-end delay and vice versa. the increase of the delay in higher quality videos can be justified by the large amount of the allocated to each ts, as in equation (4), which leads to maximize the wasted txops that keep the subsequent tss awaiting in their transmission queue longer time. it is worth noting that the proposed scheduler achieved about 52% and 46% delay improvement over hcca for jurassic park 1 and formula 1 respectively. the reason of achieving better improvement in jurassic park 1 is the higher packet size comparable to that in formula 1 thereby the granted txop obtained in hcca is far from the needed txop which in turn causes higher packet delay. furthermore, the delay improvement in our scheduler is justified by the accurate calculation of the txop. unlike the hcca scheduler that only relies on the mean traffic characteristic which is not reflecting the actual traffic behavior."
"geovisualization is a multidisciplinary field which involves mapping, scivis, scientific analysis, image analysis, information visualization, exploratory analysis and gis aiming to provide methods and tools for visual exploration and analysis of georeferenced data (vázquez r., 2016) ."
"the other case is when no data packet is received due to loss, the qap will use the equation (4) of hcca scheduler to compute the t xop i . it is worth noting that at the first cap of any ts, the txop is calculated based on equation (4) because no information about the next packet size has been reported yet."
"icon-based techniques have two distinguishable parameters: the type of geometry for representing the instance and the position defined for every icon in the image. these techniques are recommended when the number of dimensions/variables is between ten and fifteen, and the number of instances is large. examples of these are chernoff face, start fields, shape coding, etc. [cit] . on the other hand, geometry-based use geometrical entities as visual properties to produce visualizations (lines, curves, points). among the set of existing techniques, there are three which are the most used: parallel coordinates, dispersion diagram, and andrews plot [cit] . pixel-based techniques reduce the visualization space using a pixel codification for each instance's variable. the codification is always through a color value and every instance has as many pixels as attributes/variables has. the more common used techniques are circle segments [cit] and recursive patterns [cit] ."
"this is an open access article distributed under the terms of the creative commons attribution non-commercial license (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. for commercial re-use, please contact journals.permissions@oup.com bioinformatics, 32, 2016, i369-i377 doi: 10.1093 /bioinformatics/btw273 [cit] . these sequences constitute a class of short rnas that are the second most abundant type of rna after mirnas. since the discovery of small rnas derived from trnas, a variety of names have been used to refer to similar entities, such as trna-derived rna fragments (trfs) [cit], stress-induced small rnas (tirnas) [cit] ) and trna-derived small rnas (tsrnas) [cit] . their uniform start and stop sites of cleavage in trna, together with their nonrandom size property, strongly suggest that trfs are derived from trna cleavage in a specific and regulated manner [cit] ). in addition, several studies have identified a class of small fragments derived from snornas (called sdrnas), some of which may regulate splicing or translation [cit] . the biological roles of such sno-derived rna and trna-derived small rnas have primarily been investigated in cancer cells [cit] . trna-derived fragments participate in several types of biological processes, including as signal molecules in a stress-induced response and as regulators of gene expression. sdrnas can function as mirnas, regulators of alternative splicing, and tumor suppressors and oncogenes."
"where pðhja; bþ is a probability distribution of the rna structural alignments. to reduce the computational complexity of equation (2), the probability distribution pðhja; bþ of the structural alignments is factorized as follows by assuming the independence of the structures and alignment:"
"the 'optimal' alignment b h can be obtained by maximizing the expected gain function e hja;b;c a ;c b ½gðh; b hþ on the basis of the mea principle. further, the maximization of the expected gain function can be efficiently implemented using the techniques such as 'dual decomposition', 'subgradient optimization' and 'dynamic programming' [cit] for more details)."
"there are two common tendencies in gis development: desktop-based and web-based. desktop-based gis are characterized by its high computing power and visualization capabilities. arcgis [cit], grass gis [cit], quantum gis [cit], geovista studio [cit] and gvsig [cit] ) are examples of desktop-based gis. on the other hand, a web-based gis is mainly focused in the collaborative analysis of geographical information through map servers. for instance, mapserver [cit] and geoser-ver [cit] are web-based map servers, while google maps [cit] and openstreetmaps [cit] are considered gis."
"constructing an alignment for a pair of biological sequences such as dna, rna and protein sequences is a fundamental and robust method of sequence analysis [cit] . the pairwise alignment of biological sequences is achieved according to insertions, deletions and match operations so that the two sequences are aligned with the same column length. the similarity score of an alignment is calculated according to the predefined scores for insertions, deletions and matches. similarly, one can define the alignment between a pair of read mapping profiles (see fig. 1(c) ). in previous work with the deepblockalign algorithm [cit], the alignment program was designed to compare and align any read mapping profiles regardless of the rna family to which they belong. the alignment program was also applied to the comparison of read mapping profiles of the same rna gene across different samples that might be expected to induce different read mapping profiles [cit] . this method is effective for the differential processing analysis of read mapping profiles of each non-coding rna gene. in contrast with previous work, our primary goal was to detect specific forms among similar read mapping patterns mapped to specific regions of primary or secondary structures reflecting the functional activities of enzymes by alignments of read mapping profiles, and to distinguish these from non-functional degradation products. we developed a new read mapping profile alignment program named sharaku, which incorporates the primary and secondary structures of rna sequences into an alignment of read mapping profiles. this is accomplished in combination with dafs, a program for the simultaneous aligning and folding of rna sequences [cit] . as a result, the most advanced feature of sharaku is that it simultaneously aligns the read mapping profiles and rna sequences with the folding rna secondary structures. since each type of derived rna fragments is cleaved from its precursor with a specific context of primary sequence and secondary structure, we can expect that the simultaneous alignment of read mapping profiles with the primary and secondary structures contributes to precise identifications of the type of derived rnas. thus, application of sharaku to the mixture of different rna families would enable the accurate clustering of read mapping profiles with respect to 5 0 -end processing or 3 0 -end processing of each rna family, and facilitate the detection of common processing patterns shared among different rna families (see fig. 1(d) ). note that sharaku was not designed for classification of rna sequences into different rna families. further, sharaku produces an alignment of read mapping profiles at the nucleotidelevel resolution, as figure 1 (c) displays. in contrast, deepblockalign uses blockbuster [cit] to generate block groups and aligns any read mapping profiles at the level of individual blocks and block groups, including those in unannotated regions or unknown rna genes. on the other hand, the current version of sharaku can only be applied to the annotated non-coding rna regions. [cit] ) have explored the similarities and evolutionary relationships between snornas and mirna precursors. these similarity features represent molecules involved in the same processing pathways with a similar set of processing enzymes and the same rnai targets. these similarity features are often confirmed based on the conservation of their primary and secondary structures, such as structural characteristics of typical h/aca or c/d boxes. we hypothesized that in addition to sequence structure conservation, determining the similarity among read mapping profiles might help to identify the functional or processing similarities between snornas and mirna precursors."
"in order to evaluate the performance of our method, we generated simulated read data in the form of typical next-generation sequencing output (based on an illumina-type of short sequence reads). to evaluate the clustering ability to detect similar processing patterns, the labeled data of various classes of different processing patterns are required, such as 5 0 -end processing, 3 0 -end processing and nonprocessing. however, comprehensive annotations of such labels for all small non-coding rnas are not available in existing real data such as the sequence read archive. the only exception is the mirna family, for which the mature mirna annotations are accessible in the ncbi reference sequence database. first, we obtained the small rna annotation and sequence data for mirnas (24 genes), snornas (37 genes) and trnas (41 genes) with a length less than 120 nt from the ensembl genome browser for the mouse genome (grcm38). second, according to the results of previous studies that analyzed and identified the existence of various types of small rna-derived fragments [cit], we determined the start or end sites of derived fragments that simulated 5 0 -end processing or 3 0 -end processing in snornas and trnas to be within a few bases from the 5 0 -end or 3 0 -end, respectively. further, we set the length of the derived fragments to 20 nt for snornas and to 30 nt for trnas. for mirnas, we used the annotations for mature mirnas in the ncbi reference sequence database. third, we generated 88 324 simulated sequence reads that contain the fulllength transcripts of snornas and trnas (that is, the complete sequence reads exactly identical to the annotated snorna and trna sequences), mature mirnas and their derived fragments. in addition, the set of 'degradation reads' was artificially constructed from the simulated complete reads of snornas and trnas. in general, there are three major classes of rna-degrading enzymes [cit] : endonucleases that cut rna internally, 5 0 exonucleases that hydrolyze rna from the 5 0 -end and 3 0 exonucleases that degrade rna from the 3 0 -end. in our study, we only considered the degradation of rna from the 3 0 -end, which is more reflective of non-functional degradation for unstable rnas. thus, the set of degradation reads was obtained by eliminating 40-60% of the nucleotides from the 3 0 -end in order to simulate the degradation process. the degradation reads constituted 20% of the set of simulated complete reads. in total, the simulated read dataset consisted of the complete reads of snornas and trnas, the derived fragment reads for 5 0 -end processing of mirnas, snornas and trnas, the figure s1 that displays all read mapping profiles obtained by mapping the simulated read data to the 102 annotated small rna sequences.) to demonstrate the practical usefulness of sharaku, we used a real dataset obtained from an rna-seq experiment of the hippocampus of the left brain of a 2-year-old male common marmoset that was being bred at the central institute for experimental animals (ciea). the total rna was extracted and was subject to removal of rrna and 5 0 cap, and then the cdna library of small rna was prepared using the truseq small rna sample prep kit (illumina). the small rna transcripts were sequenced with the next-generation sequencer miseq (illumina) for a sequence read length of 270 bp, which enabled generating the complete sequences of most small rna families. the short reads were subject to cutting adapters and quality filtering. the qualified reads were then mapped to the common marmoset (callithrix jacchus) draft genome caljac-3.2 [cit] by bwa [cit] ). the trna annotation was predicted by trnascan-se [cit]"
"one of the merits of sharaku is to calculate the optimal alignment based on not only the pattern of read mapping profiles, but also primary and secondary structures of rna sequences. since each type of derived rna fragments is cleaved from its precursor with a specific context of primary sequence and secondary structure, we can expect that the simultaneous alignment of read mapping profiles with the primary and secondary structures contributes to precise identifications of the type of derived rnas. to combine the pattern of read mapping profiles and the primary and secondary structures of rnas, a sankoff-type algorithm is required to be implemented. we implemented sharaku with dafs framework, which enabled to efficiently combine the pattern of read mapping profiles and the primary and secondary structures of rnas by the dual decomposition technique."
(ii) multiparametric sub-module: this module is able to visualize multiparametric data and must be employed to to visualize the global behavior of the entire smart security system at any given time.
"where l i is the nominal mac service data unit (msdu) length for the i th qsta. then the txop duration of the particular station, t xop i, is calculated as the maximum of the time required to transmit n i msdu or the time to transmit one maximum msdu at the physical rate r i, as stated in equation (4)."
"in order to verify that a derived rna fragment predicted by the clustering method based on sharaku is truly derived and expressed and not an experimental artifact, we performed northern blotting analysis with a sample of the marmoset spleen. a small rna-seq analysis for the total rnas extracted from the marmoset spleen was executed and northern blotting for leu-caa-trna was performed. the read mapping profile of this leu-caa-trna belonged to a cluster representing 5 0 -end processing of trnas in the hierarchical clustering tree generated by sharaku. figure 3 shows the read mapping profile for leu-caa-trna obtained from the rna-seq reads (left), as well as the northern blots for leu-caatrna (lenght: 105 nt) and the derived fragment (expected length: 35 nt) from the trna (right). the results clearly proved the actual presence of the derived rna fragment."
where n is the number of admitted tss and m si i is the maximum si of the i th stream. the si is computed so that it satisfies the condition in equation (2) .
"the proposed scheduler has been implemented in the well-known network simulator (ns-2 ) [cit] version 2.27. the hcca implementation framework ns2hcca [cit] has been patched to provide the controlled access mode of ieee 802.11e functions, hcca. the ns-2 traffic trace [cit] agent is used for video stream generation."
"as we previously mentioned, all the blocks of our proposed smart security module are based on fiware. this middleware provides a set of functional components, known as as generic enablers, to develop applications for future internet. specifically, we are using four of these components: the orion context broker (ocb) 3, kurento 4, cygnus 5 and cosmos 6 ."
"when read mapping profiles for a pair of non-coding rnas are obtained, sharaku fundamentally aligns two read mapping profiles by inserting gaps so that the sum of the differences of coverages at all positions between the two profiles is minimized. simultaneously, sharaku takes information on the sequence and secondary structures of rnas into account when aligning read mapping profiles by integration with dafs, which calculates reliable structural alignments that maximize the expected accuracy of a predicted common secondary structure and its sequence alignment. the algorithm can be efficiently implemented by using dynamic programming. second, sharaku calculates a similarity score (correlation coefficient) between two read mapping profiles based on the alignment. third, sharaku produces a similarity score matrix for all pairs of read mapping profiles of non-coding rnas in the target reference genome. subsequently, the agglomerative hierarchical clustering is constructed based on the similarity score matrix."
"this study proposed a novel scheduling scheduler to support vbr video streams in ieee 802.11e wlans. this scheduler dynamically assigns txop to a qsta based on piggybacked information about the next frame size with each packet sent of uplink traffic instead of assigning fixed txop of hcca. accordingly, hc is able to poll qstas with regard to their fast changing in the traffic profile so as to prevent qstas from receiving unnecessary large txop which produces a remarkable increase in the packet delay. the proposed scheduler has been evaluated over two video streams with varying quality level to verify the performance of supporting videos with low and high variability traffics. simulation results reveal"
"cameras were placed in a real scenario where it is difficult to have control of the environment conditions, this represents a challenge for the computer vision algorithms (e.g. background subtraction, tracking and person recognition). in figure 4 we depict the map of our campus as well as the location of the cameras and its respective fields of view."
"in video surveillance it is common to have a set of cameras that generate a lot of information which becomes difficult to analyze by the human eye. to address this problem we have developed video processing algorithms in order to extract relevant information. this information is processed in such a way that the system is capable of alerting the user about risk situations. furthermore, in order to make more comfortable and understandable for the user this information; is geovisualized. it is important to add that the system was designed to detect the presence of moving objects and then classify them into three classes: people, vehicles and others. in order to be able to classify any of these objects, we developed three sub-modules: background subtraction, classification and tracking. figure 3 shows how this modules are connected. first, with the background subtraction sub-module and based on the subsense algorithm [cit], we extract the objects that do not belong to the static part of the scene. after that, these objects are fed into the classification sub-module where a nearest neighbor algorithm [cit] ) is used to classify into one of the three predefined categories. as final step, the tracking sub-module [cit] ensures that once an object is classified and its presence is reported to the system, this object keeps its label and is only reported once as long as it stays in the scene."
"simulations have been carried out to exhibit the performance of the examined schedulers using a different variability level of the same videos. since the main objective is to achieve superior qos support by accurately granting txop to the station to fit its need, packet end-to-end delay of the uplink traffics has been measured which considered as one of the significant metrics to evaluate a qos support of video streams."
"at the qsta, information about the next msdu frame size is obtained from the application layer via cross-layering. this information is carried in the qs field introduced by ieee 802.11 standard [cit] which is a part of the qos control field of the qos data frame. the qs field is exploited in this approach for sending information about the next msdu frame size to the qap."
"the resulting visualization can be of two different types: iconic or geometric. for iconic representations (pie-chart and bar-chart) a new map layer is generated and each visualization is drawn at its corresponding geographic location. on the other hand, the geometric representation is visualized in an independent window to avoid overlapping. in figure 5 is shown the way the system displays the information to the user through iconic representations and all the plotted data is placed over each camera's location, while figures 6 and 7 illustrate the geometric visualization technique over independent windows."
the aggregate throughput of the examined schedulers has been investigated against the number of stations. this is to verify that our scheduler is efficient in supporting qos for vbr traffics which maintaining the utilization of the channel bandwidth. the aggregate throughput is calculated using equation (8) .
"the hc updates l i in equation (4) with regards to the information piggybacked with each packet received from a qsta. in fact, this is the major part of the proposed algorithm in which the txop duration given to a qsta is calculated dynamically to accommodate the actual packet size to be received at the qap from an uplink ts."
this section describes ieee 802.11e hcca scheduler and some characteristics of mpeg-4 vbr video traffic. the deficiency of hcca in supporting vbr is illustrated and some related works in enhancing its performance are also discussed.
"on the other hand, scientific visualization (scivis) aims to transform data, through coding techniques, in a per- * corresponding author ceptible representation that improves the understanding and communication of models, at the same time that increases the effectiveness in its analysis and evaluation. visualization techniques, in general, are used for exploratory analysis, confirmatory analysis and presentations. in the last two decades there has been an increase in the use of information systems and scientific visualization that has facilitated the analysis of large volumes of data [cit] . also, another technology that has been applied in different research areas along with scientific visualization are the geographical information systems (gis), which are based on the integration of high performance computers, geospatial databases and visualization software for editing, managing and deploying geographical data in different ways."
"cloud computing [cit] aims at the design of software modules that use accessible internet ondemand services. these services are available from anywhere, with the cloud as a point of access for the consumers to all the computing resources. in figure 1 we show the general scheme of the architecture of our smart security approach. this architecture considers three main modules. the first one is the smart security module, in which all the data from the sensors, e.g. video cameras, is processed. in a second module we have the gis, in this module we manage all the information concerning to the map and geolocalization of every object within the map, and finally in a third module, we have the web-browser. this block can be seen as the user interface where it is possible to interact with the map elements, select the type of information that will be displayed and make several interactions over the resulting visualization. figure 1 . general architecture for a smart security in smart cities assisted by geovisualization."
"as the proposed scheduler operates based on the feedback, the hcca scheduler will change some scheduling parameters in equation (4) upon receiving a feedback from qsta. herein a description of these parameters:"
the end-to-end delay is defined as the time elapsed from the generation of the packet at the source qsta application layer until it has been received at the qap and is expressed in equation (7) .
"where size i is the received packet size at the qap, time is the simulation time and n is the total number of the received packets at qap during the simulation time. figure 8 aggregate throughput with increasing the network load for the low-, medium-and high-quality jurassic park 1 videos, respectively. the results show that the throughput is the same as that achieved by the hcca scheduling scheduler. this implies that our approach enhanced the end-to-delay without jeopardizing the channel bandwidth."
"due to the wide spread of ubiquitous applications in the internet and the rapid growth of multimedia streams, providing differentiated qos for such applications in wireless local area networks (wlans) has become a very challenging task. the ieee802.11 [cit] has become the most deployed technology in wlans due to some of its key features like deployment flexibility, infrastructure simplicity and cost effectiveness. ieee802.11 introduces two channel access modes, namely distributed coordination function (dcf) and point coordination function (pcf). the former is the mandatory medium access method which is appropriate to serve best effort applications such as hypertext transfer protocol (http), file transfer protocol (ftp) and simple mail transfer protocol (smtp). multimedia streams that require a certain qos level are served during the controlled mode (i.e. pcf) since it provides a contention-free polling-based access to the channel to provide the demanded qos. however, it is not efficient enough to support high qos requirement applications due to the fact that pcf only operates on the free-contention period, which may noticeably cause an increase in the transmission delay especially with high bursty traffics. consequently, ieee 802.11 tge has presented 1 ieee 802.11e protocol [cit] and revised version [cit] with new technical enhancements on medium access control (mac) and physical layer."
"where n is the number of currently admitted tss, so that (n + 1) represents the incoming ts, t is the time interval between two consecutive tbtts periods, beacon interval and t cp is the duration reserved for edca. the hc sends an acceptance message (addts-response) to the requested qsta if the condition in equation (5) is true or send a rejection message otherwise. the accepted ts will be added to the polling list of the hc."
"advances in integrating gis and scivis techniques result in the creation of systems like snap-together visualization [cit], geovista studio, vis-stamp [cit] and gav flash tools [cit] among others. all these tools enable the visualization of temporal data, gathered by experts or sensing devices (e.g. temperature, rain, etc.), in the exact location where they were measured."
"the hc which usually resides in the qos-enabled acces point (qap) maintains the tspecs of all tss in the so-called polling list. accordingly, hc computes the duration of the time to be granted to each qsta for the transmission of their traffics (txop). the admission of the tss is governed by hc, using the admission control unit (acu). hc reserves the right to accept or reject any ts so as to preserve the qos of the previously admitted tss. if hc accepts the traffic it will respond by an addts-response or a rejection message otherwise."
"previously, we developed dafs [cit], an algorithm that simultaneously aligns and folds rna sequences on the basis of maximizing the expected accuracy (mea) of a predicted common secondary structure and its alignment. that is, dafs calculates the maximization of the expected accuracy over all possible structural alignments of a pair of rna sequences. then, dafs finds one optimal structural alignment, i.e. the most accurate structural alignment with the most plausible secondary structure of each input rna sequence. here, we combine the alignment algorithm for read mapping profiles with dafs according to the same mea principle in order to incorporate the primary and secondary sequence structures into the alignment of read mapping profiles."
"in the present study, we have only dealt with annotated noncoding rnas. however, sharaku can also be applied to the alignment and clustering of novel and unannotated regions by employing tools such as blockbuster [cit] in order to determine the expressed block regions on the reference genome that are obtained from rna-seq reads. as non-functional degradation products, we only considered the degradation of rna from the 3 0 -end in the simulated dataset. therefore, performance evaluations of sharaku to determine the tolerance for the other two classes of rna-degradations, endonucleases and 5 0 exonucleases, are required. these issues will be addressed in our future work."
"the rest of the paper is organized as follows, section 2 summarizes the main advances in geovisulization, smart cities and smart surveillance. section 3 describes the proposed surveillance system aided by geovisualization. in section 4 is presented an application of the proposed system, and finally in section 5 is presented the conclusions and future work."
"as mentioned earlier, in order to initiate an uplink traffic, the qsta issues a qos reservation through transmitting an addts-request frame. this frame carries information about the tspec which is required by hc for scheduling purpose. the mandatory fields of the tspec are described as follows:"
"in order to evaluate the performance of the proposed scheduler, we have used a network simulation tool. the simulation environment setup, and video traffic used as uplink traffics is described in details in this section. the performance of our scheduler is compared against the hcca. the results of end-to-end delay and throughput are also discussed."
"with the increase of internet web applications in the wireless mobile devices, the user-generated content (ugc) such as pre-recorded video streams has become more prominent nowadays. to the best of our knowledge, the scheduling of uplink pre-recorded continuous media in hcca has not been addressed efficiently despite the fast growth of uplink streams of the ugc in the internet such as pre-recorded video streams. in this paper, we present an enhancement on the hcca scheduling algorithm aiming to adapt to the fast fluctuation of vbr video traffics profile. basically, the proposed scheduling algorithm makes use of the fact that in the video applications that use prerecorded streams, the video traffic can be analyzed prior to the call setup. this fact has been highlighted in feed forward bandwidth indication .it computes the txop for a traffic based on knowledge about the actual frame size instead of assigning txop according to mean characteristics of the traffic which is unable to reflect the actual traffic. this algorithm uses the queue size (qs) of ieee 802.11e mac header to carry this information to the hc."
"although hcca guarantees a qos for video traffic based on the required tspec parameters, there is a probability to have frames smaller than the mean negotiated msdu size. consequently, a larger txop than needed will be assigned to a qsta causing wasting in wireless channel time and remarkable increase in the end-to-end delay. figure 2 illustrates the effect of assigning txops for vbr traffics based on the mean tspec parameters on increasing the packet delay and on the poor wireless channel utilization. suppose there are three stations sending uplink video traffics to qap. hc will accordingly assign t xop 1, t xop 2 and t xop 3 to qst a 1, qst a 2 and qst a 3 respectively. assume that in any si some or all the frames sent is considerably smaller than the negotiated msdu for the ts, in this case the qsta will only utilize a portion of the scheduled txop for sending its data as explained in the si i and si i+1 . the next scheduled txop will initiate according to its scheduled time regardless the actual exploited time in the previous txop causing increases in the delay and wasting the channel time as well. this issue may be noticeably severe when the number of qsta with vbr traffics increase. moreover, in the transmission of the prerecorded video, the traffic behavior is known prior to the traffic setup. these observations motivate us to present an enhancement to hcca scheduler in which hc exploits information sent by qsta about the changes in the traffic profile so as to accurately assign txop to qsta and advance the consecutive txops to minimize the delay and add the residual wireless channel time to edca period. the proposed scheduling algorithm is presented in details in the next section."
"to develop a geovisualization-based system is important to take into account the dimensions and the shape of every resulting visualization in order to make a useful and understandable integration. pixel-based, icon-based, and chart-based techniques generate iconic representations, this representation can be drawn over specific locations in a map (see section 3.3). conversely, the dimensionality of geometric-based techniques impede its directly visualization over the map, but can be used for statistical exploration or for projection pursuit [cit] . the use of the correct visualization method might be the difference between extracting valuable information or keep digging out in a sea of data for an undefined amount of time. although there is no rule on how to analyze a bundle of information, certainly there are some parameters to take into account before generating a visualization for some given data."
"where the denominator, x, is an integer number that divides the beacon interval into the largest number that is equal or less than the m si min ."
the scheduler calculates si as the minimum of all maximum service intervals (m si) of all admitted traffic streams which is a submultiple of the beacon interval. the minimum m si for each qsta is obtained from equation (1).
hc allocates a txop to each admitted qsta so as to enable it to transmit its data with regards to the negotiated qos parameters of the tspec.
"ieee 802.11e introduces hybrid coordination function (hcf) which extends the mac of ieee 802.11 standard. enhanced distributed channel access (edca) function is an extension to dcf, which operates in a distributed manner to provide prioritized qos. hcca is an extension of pcf that introduces a polling mechanism to provide parameterized qos for applications that require rigorous qos requirements. edca introduces a random access to the wireless medium by means of access categories (acs). the traffics are mapped to acs according to their priority. every access categorie (ac) will be associated with a backoff timer so that the highest priority acs will go through a shorter backoff process. despite edca provides qos support, it is still not efficient for application with rigid qos requirements. delay-sensitive multimedia streams are more adequate to be transmitted throughout hcca since it was designated to minimize the overhead of messaging caused by the distributed approach of edca and thus guarantee the required better qos. in hcca, the hybrid coordinator (hc) polls wireless stations periodically and allocates txop to them. and yet, hcca schedules traffics upon their qos requirements negotiated in the first place, it is only suitable for cbr applications such as g.711 [cit], audio streams, and (h.261/mpeg-1) video [cit] . the allocation of the txop based on the mean characteristics negotiated at the traffic setup is not accurate, because of the deviation of vbr traffics from its mean characteristics. [cit], about 91% of web traffic will be video streams [cit] . for this reason several researches have been carried out to improve the performance of wlans in terms of provisioning qos for such streams. motion picture experts group type 4 (mpeg-4/h.264) has become a prominent video the internet due to its scalability, error robustness and network-friendly features. hcca is not convenient to deal with the fluctuation of the vbr traffic such as mpeg-4 streams, where the packet size shows high variability during the time. this consequently leads to a remarkable increase in the end-to-end delay of the delivered traffics and degradation in the channel utilization."
"in this paper we have proposed a novel visualization approach for smart video surveillance that integrates webbased geographic information systems and scientific visualization techniques. our approach gives the user the possibility of visualizing large amounts of data through different layers of information, which in turn allows him to have a better idea of the global behavior of the monitored areas. besides, this system gives the user the option of watching the video streaming of a specific area, so that the user can take decisions related to an alert. in addition, the proposed approach allows to incorporate another kind of sensors and visualize the data generated by these sensors. we have implemented our system in a real scenario using the fiware middleware. as a future work we want to extend the set of algorithms in the vp sub-module and to evaluate the visualization techniques with experts in the video surveillance area."
"in ieee 802.11e, a parameterized qos is supported during hcca using polling access method. a bea- figure 1 demonstrates an example of hcca transmission during cfp and cp periods."
"a total of 33.8 million (m) sequence reads with a length of 270 bp for small non-coding rnas was generated using an illumina miseq sequencer. after quality filtering, 30.5 m reads were mapped to the common marmoset (callithrix jacchus) draft genome caljac-3.2 using bwa. from the output in bam format, the normalized read mapping profiles for 619 non-coding rnas were obtained and fed to the alignment programs."
"psnr is badly correlated in target iv. because psnr does not consider inter-frame relations, the subjective improvement of increasing temporal layers is not being reflected on the objective results. table 4 values of the exponential correlation between objective metrics and subjective results"
"this section discusses how to dynamically identify which and when applications should be allowed to place blocks in remote caches (section 4.1), our oracular placement policy (section 4.2), and a hardware-implementable placement policy (section 4.3)."
"the potential gain in the qoe resulting from streaming adaptive services is not stated. for that reason, subjective experiments are needed to directly compare user's choice between an adaptive or traditional non-adaptive streaming service. in our subjective experiments [cit], different situations are evaluated to seek the user's limits of tolerance towards any of the systems in chosen scenarios."
"the remainder of this article explores the various aspects of app's design and performance, and is organized as follows. section 2 describes related work in cache management of uni-and multiprocessor systems. section 3 motivates our approach to solving the problem of private l2 management. section 4 describes the design and working of app, and oprp in details. section 5 describes our evaluation methodology. section 6 provides the results from our evaluations and analyzes the findings. section 7 concludes this work."
"to map objective results with cmos scores, we obtain the difference between the objective metric calculated for the adapted and the reference sequence in the same pair. then, psnr and ssim positive differences indicate that objectively speaking, the adapted sequence is better than the reference sequence. for vqm, since its scale is reversed with respect to the psnr or ssim, positive vqm differences indicate objective preference towards the reference sequence."
"at first glance, the average improvement of weighted speedups achieved by app over dsr may seem modest. however, there are three important points to consider. first, improving the performance over the state-of-the-art technique is not a minor feat. while app's improvement over dsr may seem modest, dsr's improvement upon cc, which was the state-of-the-art of the time, may similarly seem modest (25% vs. 19%)."
"we found the behavior of blocks not being accessed during their residence in the l2 cache to be quite common among acceptors. one situation that leads to such behavior is when the block's temporal and spatial reuse can be captured completely in the l1 cache. thus, the block is brought into the l2 cache, but is only accessed at the l1 cache, until the block is evicted from the l2 cache, and a new miss prefetches the block."
"obviously, oprp and opp are not implementable as they rely on future trace information. in order to design a placement policy that has a practical hardware implementation, we can approximate future information with past information. we refer to this scheme as adaptive placement policy (app). such a scheme would require a predictor table and logic that determines whether an incoming or remotely hit block should be in the local or remote cache."
"the aim of this paper is to corroborate if objective metrics can be correlated with subjective scores. we carried out a subjective study with 75 participants and 48 pairs of video sequences. the participants had to assess their preference towards an adaptive streaming system or a traditional, i.e. non-adaptive, system. the subjective results of the experiment, expressed in comparison mean opinion scale (cmos) values, are compared with the objective evaluation of each pair of sequences."
"management of private/distributed caches in cmps. victim replication (vr) [cit] and adaptive selective replication (asr) [cit] ] studied techniques to allow remotely-homed blocks to be replicated in the local cache. these schemes primarily apply to multi-threaded workloads, where blocks identified to be more important are replicated locally (or victimized remotely). with multiprogrammed workloads, which is the focus of this study, there is no significant data sharing and therefore no reason to replicate data."
"however, pc-based predictor's performance superiority must be evaluated against its cost and complexity. figure 9(d) shows the total hardware overheads of the prediction table and the required additional tag information for cached blocks. pc.12 requires 11.5kb prediction table, 33-bit additional tag information per cache block, for a total of 77.5 kb, representing a 7.5% area overhead for a 1 mb cache. tag.8 is much simpler, only requiring 32-byte prediction table, and 1-bit additional tag information for the \"accessed\" bit per cache block, for a total of 2kb, representing a 0.2% area overhead for a 1mb cache. considering the tiny hardware overhead for address-based predictor, and that it still performs within 3 percentage points in weighted speedup compared to pcbased predictor, we view address-based predictors as more cost effective. furthermore, a pc-based predictor requires a relatively major change to the entire memory hierarchy, as pc is not normally available at the last level cache (section 4.3.1)."
"if the app predictor determines that the incoming block should be placed in a remote cache, a random remote receiver cache is selected, and the lru block there is replaced by the incoming block (step 2b). the incoming block is marked as the mru block at the remote cache (step 3), while the remote victim block is discarded or written back if dirty (step 4). the incoming block is always supplied to the requesting processor and its l1 cache (step 5). figure 5 illustrates how app handles a remote cache hit (upon a local l2 cache miss). the app predictor is looked up to determine whether the remotely hit block should be left where it was or be brought into the local l2 cache (step 1). if the predictor determines the block should be brought into the local l2, the remotely hit block is swapped with the lru block in the local l2 cache set ( step 2) and marked as the mru block. otherwise, the remotely hit block remains in the remote cache, but becomes the mru block there. the remotely hit block is supplied to the processor and its l1 cache (step 3)."
"we use the following metrics to evaluate performance and fairness of the 50 multiprogrammed workloads: weighted speedup, harmonic mean, and throughput. these three metrics and their significance have been described in detail in prior work [cit] . table iv defines these metrics."
"in order to study the upper-bound performance improvements from remote placement policies, we developed a hypothetical oracular placement and replacement policy (oprp). when a spiller suffers a cache miss, oprp determines if the incoming block should be placed in the local l2 or in a remote l2 by looking at a trace of future accesses of the spiller application. this trace is generated prior to simulation run time. similar to belady's optimal replacement algorithm [cit] ], all blocks in the local cache set of the missed block, and the miss block itself, are compared against the trace. if the missed block is accessed farther in the future compared to currently cached local blocks, the block is placed on a remote cache. otherwise, a victim block is selected from the local cache to be spilled into a remote cache. in any case, the missed block is supplied to the processor and its l1 cache."
"both vqm and ssim present better fitting results, according to the quantile plot (top right). however, both models point out an outlier in observation number 5. this observation corresponds to experiment pair number 5. this pair (marathon t4q3_t4q2) is a quality adaptation of the sequence \"marathon\" compared to a sequence affected by high loss rate (40%). according to the objective metrics, the quality adaptation performed by the adaptive system improves the quality of the adapted sequence notably. the same content, with the same loss pattern but a temporal adaptation instead of a quality one (6 th pair) improves objective quality in a smaller quantity, according to objective metrics used. other content with similar spatial complexity (factory sequence, 3 rd pair) does not stand out either with a high objective difference between adapted and non-adapted sequences. we argue that the scene change present only in the marathon sequence is affecting the objective measure of the adapted sequence in the case of quality adaptation. the study of the effects of scene changes on quality and temporal adaptation from both a subjective and objective perspective should be addressed more deeply."
"recognizing the capacity fragmentation problem in cmps with private caches, prior work, such as cooperative caching [cit] (cc) and dynamic spill receive [cit] ] (dsr), has proposed schemes that allow cores to share their local caches with other cores. such capacity sharing mechanisms are enabled by allowing a core to spill a block evicted from the local cache to a remote cache. cc allows any cache to spill into any other cache regardless of cache usage behavior of the applications running on the corresponding cores. while this provides fluid capacity sharing among cores, sometimes it produces an unwanted effect. an application, which cannot benefit from additional cache capacity, may pollute the cache of a core running an application that really needs all the cache space it has. to avoid this drawback, dsr identifies applications that can benefit from additional cache capacity (called acceptors), and applications that have excess cache capacity that can be donated without impacting performance (called donors). further, it proposes a dynamic mechanism to identify when an application is allowed to spill, and when an application is allowed to receive a cache block."
"it is already assumed that the best fit model for qos and qoe relationship is an exponential approach (iqx method) [cit] . however, this quest is unfinished and there are many flaw points still to be addressed. for example, not all factors influencing qoe are totally understood yet. parameters rarely investigated include privacy concerns, interaction of audio/video, user interface, user's awareness of quality, cost, last mile equipment/environment or quantization of video coding."
"note that oprp consults the future trace to guide both placement and replacement. the decision of whether to place an incoming cache block in the local or remote cache is a placement decision, but what block is victimized from the local cache is a replacement decision. in order to bound the performance improvement attainable from the placement decision alone, we design a new oracular placement policy (opp). opp uses the same policy as oprp in determining whether a block should be placed in the local or remote cache. however, when it is decided that a block should be placed locally, the lru block is selected as a victim, instead of the block used the farthest in the future. thus, future trace information is only used for making placement decisions."
"to address the prediction table, a hashed index is generated by folding down the cache block address tag into an n-bit index, which addresses a table with 2 n entries. by folding we mean dividing up the address tag into n-bit entities and xoring them (an entity is zero-padded before xoring, if it is not a multiple of n-bits). such hashing does not guarantee that addresses with similar behavior would share a common predictor entry, but because of global access behavior common to many blocks, the choice of hashing function only acts to ensure that entire table is utilized. the global reuse behavior across blocks is corroborated by our experiments, where even a small table with only 256 entries gives the most cost-effective design (section 6.3)."
"workloads that enjoy significant performance improvement in app (e.g., m11, m31, and m44) are also ones that are dominated by acceptor applications with anti-lru temporal reuse patterns. the average local hit rate of the acceptors in each workload for app versus dsr are 15% vs. 7% (m11), 20.3% vs. 9.3% (m31), and 22% vs. 5% (m44), respectively. since remote placement of blocks is the only difference between app and dsr, the results clearly point to the significant role placement decisions play in improving local cache hit rates of acceptor applications. the increase in local hit rates also demonstrates that by allowing anti-lru incoming blocks to be placed in remote caches, existing blocks in the local l2 cache enjoy better temporal locality from less cache perturbation."
"the figure shows two rows of applications, where the top row contains ones that show significant miss rate reduction when the cache capacity is increased from 1mb to 4mb (acceptors), as evident in the large area under the curve between way 8-31. the second row shows applications that do not benefit from increasing the cache capacity from 1mb to 4mb (donors). some of the applications in the second row have very small miss rates (namd, povray, and sjeng), indicating small working sets, while others have very large miss rates (libquantum and milc), indicating either very large working sets or streaming memory access patterns with little temporal reuse. these donor applications not only cannot benefit from larger cache capacity, but also have substantial cache capacity that can be donated without affecting their miss rates."
"in order to cover all distinct workload scenarios, we choose fifty 4-benchmark workloads that cover different ratios of acceptors and donors, different types of acceptors (anti-and perfect-lru), and different types of donors (small-working-set and streaming). the workloads are shown in table iii . a workload denoted as a x d y represents a workload with x acceptors and y donors. the first ten workload mixes have one acceptor and three donors. the next fifteen workload mixes have two acceptors and two donors. the next fifteen workload mixes have three acceptors and one donor. the last ten workload mixes have four acceptors and no donors. acceptor applications are distributed nearly uniformly across all workloads (i.e., each acceptor appears in almost the same number of workloads). donors are selected in a similar fashion. however, since all donors behave the same from the point of view of this study, they might as well be selected randomly."
"handling a miss and remote cache hit. figure 4 illustrates how app handles a global cache miss, i.e., miss in the local and remote caches. while the spiller's l2 miss is outstanding, the app predictor is looked up to determine whether the incoming block should be placed in the local l2 or a remote cache (step 1 in the figure) ."
"second, across different workload groups, the performance improvement of app compared to dsr varies. the performance improvement of app over dsr for a1d3 is the smallest (27.3% versus 26.5%) because only one acceptor application benefits from app's remote block placement, whereas the weighted speedup reflects the average improvement for all applications running together. thus, the more acceptor applications in a workload, the more the weighted speedup is improved with app compared to dsr. app's average weighted speedup improvement for a2d2 is 33.6% versus dsr's 29%. for a3d1, app achieves an average weighted speedup of 22% compared to dsr's 18.2%. finally, for a4d0, app achieves 33.7% improvement in weighted speedup while dsr achieves 28.2%."
"a small number of workloads show almost negligible performance improvement, or a slight degradation over dsr (e.g., m25, and m04). acceptors in these workloads (example bzip2 and hmmer) show an almost perfect-lru behavior and very little anti-lru behavior (figure 2 ). these workloads already experience high local hit rates (66% for m25, and 82% for m04), hence most of the performance improvement can only come from additional cache capacity rather than converting remote cache hits into local cache hits."
"in section 1, we showed results that show a gap between the performance of dsr, a state-of-the-art capacity sharing technique, and the performance of an oracle scheme (oprp). our hypothesis is that this gap primarily exists because of the oracle scheme's ability to selectively place newly fetched/accessed blocks in remote caches. in this section, we investigate this hypothesis in detail as a first contribution of this article."
"of these metrics, weighted speedup is the preferred metric [cit] . it corresponds to a physical, system-level measure of performance: the number of instructions executed across all applications in a multiprogram mix per unit of time. this metric equalizes the contribution of each program in the mix by normalizing its performance in the mix to its performance when run in isolation. weighted speedup does not bias the measured performance by favoring high-ipc applications. figure 6 shows the weighted speedup results for cooperative caching (cc), dynamic spill receive (dsr), our scheme app, and our two oracular schemes: oracular placement (opp) and oracular placement-replacement (oprp). recall that cc provides capacity sharing where all applications can spill their victim blocks to other caches, while dsr improves upon it by detecting acceptor applications and only allowing them to spill to other caches. the dsr implementation is based on the code provided by the authors of dsr [cit] ], integrated into our simulation infrastructure. the results are arranged into four charts (a,b,c,d) based on the acceptor to donor ratio in each group of workloads. the weighted speedups are normalized to the base case of a cmp with private caches without capacity sharing. the average weighted speedups over all 50 workloads are shown in the last set of bars."
"results show that the evaluated objective metrics have an exponential relationship with cmos values. moreover, we have found that, among the evaluated objective metrics, psnr has the worst behavior while vqm has the best correlation with subjective assessment."
"target ii is weakly correlated for ssim and vqm metrics. the scatter plot seen in fig. 7 shows the relation of subjective cmos and the logarithm of objective difference (delta ssim and delta vqm). pair numbers are included in the plot. assessing the loss tolerance threshold (target ii) through objective measures is even more difficult than through subjective measures. subjective evaluations showed that threshold definition relies on many unaccounted factors, such as content type or starting conditions (initial layers). modeling that behavior through objective measures is complex."
"multiple r-squared statistics for each model are shown in table 3 . values are low since the predictions of human behavior normally produce lower fitting values. however, observing the residual and quantile plots, the model fitting is reasonably good. ssim correlation in target iii is weak, mainly due to the influence in ssim metric of frame copy concealing effect needed in several layer temporal adaptation processes. the subjective preference is strong towards adapted sequence but ssim gives smaller to adapted sequences, especially for the \"tractor\" sequence (with lower temporal and complexity indexes)."
"the most used objective metrics, normally full reference (fr), have been peak signal to noise ratio (psnr) and structural similarity (ssim). more recently, the video quality metric (vqm) has been adopted by ansi as the objective metric standard."
"the figure shows that although dsr improves the speedup over the base case, there is a significant performance gap between it and oprp. considering one of the main differences between these two schemes, in addition to having an oracle replacement policy, it also adds placement decisions to capacity sharing (by allowing newly fetched blocks to be placed at both the local and remote caches), we are motivated to investigate the significance of placement decisions in capacity sharing strategies."
"finally, in all the workloads, app is almost never outperformed by dsr, suggesting that app rarely makes mistakes that result in the decrease of local hit rates versus dsr."
"predictor consult and update mechanism. figure 3 (a) illustrates how the app predictor table makes placement decisions, and figure 3 (b) shows how each cache block is tagged with an \"accessed\" bit. as mentioned earlier, the table is indexed by folding the cache block address tag into an 8-bit index. the table is tagless."
"to ease the evaluation of objective proposals, subjectively evaluated databases are being released. the work [cit] provides a study and a resulting database, known as the live video quality database, where 150 distorted video sequences obtained from 10 different video content sources were subjectively evaluated by 38 human observers. the subjective evaluation was performed using a single stimulus paradigm. they also provide the performance of several freely available objective, fr algorithms on the live video quality database. psnr is shown to perform very poorly in correlating with human subjective judgments and, according to the authors, is clearly an unreliable predictor of quality in any application where the end user of the video is a human observer. movie index had the best performance of the algorithms tested."
"cs and cc allow each core to spill an evicted block to a remote private cache. such capacity sharing allows any core to spill into other caches regardless of the temporal locality of the application running on that core. dsr improves upon cc, and identifies the existence of acceptors and donors. further, it presents a dynamic mechanism to classify which application can spill, and which application can receive at any given time. cooperative cache partitioning (ccp) [cit] ] also builds upon cc by prioritizing cooperative caching opportunities, across applications in a coschedule that are competing for cache space, in a time-sliced manner. cs, cc, ccp, and dsr explore remote block placement on eviction of a local block, in contrast, in this article we study remote block placement when a block is first brought on chip."
"the applications and the category they fall into are shown in table ii . [cit] . we chose only a subset of all donors because we observed that donors behave similarly from the point of view of the insights we hope to gain from this study. all donors with a small working set share the same trend (l2 cache miss rate of less than 2%); therefore, we use a small representative set for such donors. similarly, all donors with a streaming data access pattern behave similarly (less than 2% reduction in miss rate upon increasing the cache from 1m to 4mb)."
"not all applications should be allowed to place their blocks in remote caches, because they may not benefit from increased cache capacity. evaluating which and when applications should be allowed to place blocks in remote caches should be performed dynamically and continually, because temporal reuse behavior is not only application specific, but may also be phase-specific. we refer to a cache (or the application using it) that is allowed to place blocks in remote caches as a spiller, and a cache that can only receive blocks from others as a receiver."
"instruction-and-address-based predictor. the final predictor structure we try combines pc and block address behavior history recording. to index the prediction table, both the pc and block address are folded into n-bit entities, which are then xored together. the structure of this predictor is similar to the address-based one in terms of table size. the difference is only the input used for indexing the table."
"each entry in the prediction table contains a saturating counter, which is decremented when an evicted block records that it was accessed during its recent residence in the local l2 cache (its \"accessed\" bit is 1), and incremented when the evicted block was never accessed during its recent residence (its \"accessed\" bit is 0). when later the local l2 cache suffers a miss to the same block, the prediction table is looked up. if the value in the saturating counter is 1 or higher, the block is placed in a remote l2 cache. otherwise, the block is placed in the local l2 cache. the initial values of the saturating counters in the predictor table are 0, hence blocks are initially always placed locally."
"in this section, we evaluate app performance when the l2 cache size is varied from 512kb, 1mb (default in other sections), and 2mb. latencies for various cache sizes are shown in table i . figure 10 shows the weighted speedup results across various cache sizes, normalized to the base case of no capacity sharing."
"address-based predictor. another possibility for app predictor table organization is to keep per-address information. in contrast to pcs, where keeping a few pcs that produce most misses is sufficient, we can expect that the number of unique memory block addresses within a program to be very large. attempting to learn individual behavior of each block is impractical, as we need a huge prediction table structure to keep all of them, even when entries can spill to main memory. allowing new entries to discard older entries still will not allow effective recording when the number of active blocks is much higher than number of table entries. this leaves us with the only one choice left: allowing multiple block addresses to share a common entry. such a sharing policy works if many blocks share the same behavior, but not when they produce different behavior. fortunately, in most cases, when an application has a high reuse distance, it affects most of the blocks it touches. hence, allowing multiple block addresses to share a common entry helps accelerate the learning, rather than hurts performance."
"several authors postulated an exponential relationship between qoe and qos parameters [cit] . in our evaluation, we follow a different approach and we find the same relationship between the subjective assessment and the objective metric. we also find that subjective decisions between adaptive and non-adaptive systems have an exponential correlation with the difference between the objective metric results of both systems. we have also found that vqm has the best correlation with subjective preferences. although the model fits well, it is not possible to obtain a high correlation with subjective data because there are many variables that cannot be taken into account, such as, among others, the characteristics of the video contents, illumination or the intention of the users to perform the test."
"while remote hits are cheaper than misses, local cache hits are much preferred over remote hits. remote l2 cache hits are costly from both a performance and a power point of view. a remote hit has a significantly longer latency compared to a local hit due to the coherence action needed for the remote cache to source the block into the local cache. in addition, it also consumes more power due to the coherence request posted on the bus, snooping and tag checking activities in all remote caches, and data transfer across the bus. furthermore, future technology scaling allows more cores to be implemented on a chip, which increases the relative distance of remote caches relative to the local cache, for instance, due to additional on-chip network hops and routing delay. in contrast, the clock frequency growth of the processor may be slowing: hence, the off-chip memory access latency is only growing slowly relative to the access latency of on-chip cache. thus, remote cache hit latency will become a larger problem in the future. it is very important, therefore, to maximize the local hits by converting remote hits into local hits."
"ideally, we want acceptor applications to be identified as spillers and donor applications as receivers. however, the identification is not that simple for several reasons. first, an application may switch behavior between acceptor and donor dynamically based on its execution phase. second, an application's classification as a spiller or a receiver cannot be determined in isolation. for example, in a workload with many acceptors and few or no donors, it may be beneficial for system performance to classify some acceptors as receivers so that some of their cache capacity can be donated to other acceptors which respond much more to additional cache capacity. finally, applications or threads may migrate from core to core, changing the behavior cores exhibit. therefore, each application must be classified dynamically and continually as spiller or receiver."
"4.3.1. app predictor design. to be practical, app predictor must be low cost and efficient. each of private l2 cache is augmented with the predictor, but only when a core runs an application that is determined to be spiller, the predictor is used."
"cc manages to bridge most of the gap by improving the chip-wide hit rates significantly over the base: from 33% (base) to 66% (cc). dsr improves upon cc and achieves an average of 73%. not only is app able to maintain this chip-wide hit rate, but it also manages to increase it slightly. it is interesting to note that app continues to outperform dsr even in this metric. the reason for this is that by matching the temporal locality of each cache block with the its placement position (local vs. remote), app is able to reduce perturbation to both the local and the remote cache space, resulting in a slight improvement in chip-wide hit rates."
"note that if a capacity sharing technique only places blocks evicted from the local cache in remote caches, then necessarily the local cache stores more recently used blocks than remote caches. since all existing capacity sharing techniques (cc and dsr) use remote caches as the victim cache for the local cache, they guarantee that remote caches store less recently used blocks. however, our earlier observation concludes that applications that benefit from additional cache capacity (i.e., acceptors) are also the ones with anti-lru behavior. therefore, by treating remote caches as a victim cache for the local cache, cc and dsr guarantee the high occurrence of remote cache hits relative to local cache hits."
a software tool is used to present the content to the users following the guidelines of itu bt.500 and to store electronic scores given at the end of each test.
"scalable video technology and the concern about its subjective effects have been present for several years [cit] . however, scalable video coding (svc) technology has finally enabled a wide range of applications that exploit the benefits of layered encoding and, in particular, applications that leverage layer encoding to perform adaptive streaming. adaptive streaming is based on the premise of delivering at each moment the right combination of layers that maximize quality of experience (qoe) while coping with network technology constraints."
"finally, applications in a given workload mix are continually adapting to act as either spillers or receivers. therefore, it is critically important that the performance improvement for spiller caches does not come at the expense of degrading any receiver cache's performance. we find that the threshold of app's miss-rate monitoring system (section 4.5) is rarely triggered for the qos level we chose (5% hit-rate decline), confirming the resilience of most receiver caches to losing some of their cache space. further, only 4% of all applications (8 out of all the 200 applications in the 50 workload mixes) suffered an ipc degradation of more than 4% compared to the base case of no capacity sharing. even for these applications, the worst-case slowdown for a receiver is only 14%, which is more than completely offset by the speedup improvement in spiller applications. in comparison, the worst-case slowdown for a receiver in dsr is 13%."
"in section 4, we described three possible implementations for app predictor. figure 9 shows the performance of various designs with other parameters fixed according to table i . each predictor is denoted as x y where x is the predictor type (pc for pc-based predictor, tag for address-based predictor, and pc.tag for a hybrid pc and address-based predictor); while y is the number of index bits and the size of the table (2 y entries). figure 9 (a) shows the average weighted speedup across all 50 workloads normalized to the base case of private caches. it shows that pc-based predictors outperform address-based and hybrid predictors. to analyze the source of performance improvements, figure 9 (b) breaks down the contributions to speedups from various factors. the lowest component of bars represents a dummy predictor that always makes the opposite placement decision (local vs. remote) vs. opp. the second component represents the additional speedup from dsr (always placing blocks locally). the third component represents the additional speedup from app with various predictor designs, whereas the last component represents the additional speedup from opp. the figure shows that the headroom for improving speedup from the default app predictor (tag.8) is relatively small. the pc.12 predictor almost completely bridges the gap of app and opp, while pc.8 predictor shows a close performance as well."
"there are two main observations that can be made from the figure. first, all schemes improve the weighted speedup for all workloads compared to the base case of private caches without capacity sharing. there is, however, a varying gap between the weighted speedup improvements across the various schemes. on average, over fifty workloads, cc and dsr improve the weighted speedup by 19% and 25% respectively, while the hypothetical opp and oprp improve the weighted speedup by 32% and 36% respectively. the weighted speedup improvement from the hardware-implementable app falls right between dsr and opp, averaging 29%."
"placement decision must be made not only when there is a global miss to a new block. it must also be made when there is a remote cache hit, where we must decide whether to bring the block into the local cache. for oprp, when there is a remote l2 cache hit, the future access trace is consulted again. if the remotely hit block is accessed farther in the future compared to currently cached local blocks, the block is left in the remote cache. otherwise, a victim block is selected from the local cache to be swapped with the remotely hit block. in any case, the original or swapped remote block is marked as the new most recently used block in the remote cache. the block is, as always, supplied to the processor and its l1 cache."
"in our design, we choose 2-bit saturating counters. we tried 1-bit, 2-bit, and 3-bit saturating counters, and found that there is a slight performance advantage in using 2-bit over 1-bit counters, but there is no performance advantage in using 3-bit over 2-bit counters."
"we have investigated strategies that consider placing not only locally evicted blocks in remote caches, but also newly fetched blocks in remote caches. we investigate the upperbound performance that can be gained from combined placement and replacement decisions in capacity sharing, by using future trace information to make the decisions. based on our findings, we propose a scheme that is implementable in hardware with little hardware overhead. we show that this scheme, app, improves performance by 29% on average compared to a baseline with no capacity sharing, across 50 [cit] applications. app outperforms dsr, the state-of-the-art capacity sharing mechanism that only places local victim blocks in remote caches, by up to 18.2%, with an average improvement of 3%. app dynamically identifies which applications should be allowed to place their blocks in remote caches, and which applications should not. in addition to improving aggregate performance significantly, app also has safeguards to ensure that applications whose caches are accepting blocks from other cores are not slowed down by much."
"the key to converting remote hits into local hits is to selectively decide to place blocks locally versus remotely when they are brought into the l2 cache from the lower level memory hierarchy. remote placement allows blocks that have the highest chance to be accessed by the processor, regardless of their stack positions, to be placed in the local cache."
"simulation model. to evaluate our capacity sharing schemes with oracular placement/replacement, oracular placement, and realizable placement, we build a cycleaccurate multicore machine model on top of simics [cit], a full system simulation platform. we model a 4-core cmp system, where each core has private l1 caches and a private l2 last level cache. each of the l2 cache is has 1mb size, 8-way associativity, and access latencies derived from cacti 6.0 [cit] ]. we also assume that the l2 caches are interconnected with a shared bus, and are kept coherent with a mesi broadcast/snoopy coherence protocol. mesi protocol already has a support for facilitating cache to cache transfer. table i lists all relevant configuration parameters used in our experiments. figure 2 shows applications with each of these two temporal locality behaviors. donors, on the other hand, cannot benefit from an increase in the cache space from 1mb to 4mb. this is either because they have small working sets that fit in a 1mb l2 cache, or they have a streaming data access pattern which suffers a high miss rates regardless of any additional cache capacity. consequently, we divide donors further into two groups: small-working-set and streaming."
"predictor analysis. let us compare the complexity of pc-based, address-based, and hybrid predictor structures. first, in most memory architectures, the pc of a load/store instruction is not propagated down to the last-level cache (llc). requiring a predictor associated with a llc to have pc information requires relatively major changes to the memory hierarchy architecture. second, in order to update the predictor table when a block gets evicted from the cache, the pc of the instruction that brought the block on chip should be stored along with each cache line. this enlarges the tag array considerably and increases its access latency. third, if sequential/stride hardware prefetcher is used at the llc, the prefetcher issues only addresses. if our predictor requires pc information to work, it will not be able to decide where to place prefetched blocks."
"figs. 4, 5 and 6 show the results of the regression applying a logarithmic transformation, previously mentioned, on the objective values of psnr, ssim and vqm. observing the quantile plot (figures on the right) for the three metrics, the psnr model is the least accurate of the three because psnr samples are deviated from the linear distribution. nonetheless, psnr has been criticized throughout its history for not serving well as a qoe metric [cit] . yet, the exponential model is not entirely inaccurate, as the observation of residuals reveals. the residuals of the three models (figures on the left) show no discernable pattern. residual medians of the three models are close to zero."
"with any limited-size predictor structures, we have to deal with the possibility that multiple pcs map to the same entry in the table. conflicts between pcs that map to the same entry can be handled by discarding older entries from the table, allowing entries to spill to the main memory, or by allowing them to share a common entry. after experimentation with various designs, we find that a small (4096) and simple (direct-mapped) table works well enough to deliver accurate predictions. the reason for why a small predictor table works well is that many cache misses are caused by a small number of load/store instructions, hence keeping a small number of pcs in the table gives good performance."
"app can also be adapted (with some modifications) for tile-based cmps where caches are interconnected with point-to-point links and kept coherent using a directory-based protocol. such a system has a nonuniform cache architecture (nuca), and is beyond the scope of this paper. however, we will comment on what changes are needed to adapt app to such a system. one modification is that since cache latencies depend on the physical distance from a given core, app must be modified to take remote cache distance into account, for instance, by restricting remote placement to near or nearest neighbor caches. in addition, the outcome of app predictor (local or remote placement) must be sent along the miss request to the directory at the home node. local placement is handled in a traditional way. remote placement requires the directory to identify which cache the block should be sent to, considering the physical distance. finally, the determination for a remote cache hit is more difficult, because the directory information may contain stale information, for example when a clean block in a remote cache is evicted silently. to avoid delaying global miss resolution, a possible solution is for the directory to simultaneously inquire the remote cache and fetch the block from main memory. this avoids delay in fetching from main memory in the case that the remote cache no longer has the block, at the expense of additional power consumption. an alternative solution is to distinguish remotely placed blocks from local blocks, and require the directory to be notified when a remotely placed block is evicted (even when it is clean). finally, co-ordination with the directory is needed when blocks in the local and remote cache are swapped."
"second, dsr and the base case experience similar local l2 hit rates (32%), with slightly lower hit rates in dsr for all groups of workloads. the similarity in local hit rates is due to the fact that in both policies, a block requested by the spiller application is always brought into the local cache, and the only difference between them is in how to handle blocks evicted from the local cache. hence hit rates between them should be identical if everything else is equal. however, since dsr dedicates 32 sets in each cache to always receive (this is necessary to dynamically identify spillers/receivers), dsr's local hit rates are always slightly lower than the base case."
"in the different network scenarios selected, the adaptive system reacts to network availability. the svc based system adapts the content being delivered, increasing or reducing layers, according to network conditions. meanwhile, the non-adaptive system tries to maintain the same delivered layers (i.e. quality) as initially chosen."
"in order to provide the block usage history during the block's residence in the local l2 cache, each block in the l2 cache is tagged with a single \"accessed\" bit that is reset when the block is initially installed in the l2 cache, and is set when the block is accessed. when the block is evicted, the block's information, along with its access bit, is given to the app predictor to be recorded. the local app predictor is looked up when placement decision needs to be made (essentially at every local l2 cache miss), and is updated on each local block eviction. therefore, the lookup and update are not on the critical path and should not affect cache access latency."
"second, recall that our workloads are designed to cover a wide range of workload mixes and behavior. for five workloads, app outperforms dsr by more than 5%, and sometimes by significantly more (18%). such a difference is significant considering that over dsr, app relies on reducing cache hit latencies by only 30 cycles (converting a modest remote cache hit latency of 40 cycles to local cache hits latency of 10 cycles). as the number of cores on a chip grows, we expect that the ratio of remote cache hit latency to local cache hit latency will grow substantially, causing app's benefit to increase as well. we quantify this in a later section when we vary the remote cache hit latencies (section 6.5)."
"if it is determined that the incoming block should be placed in the local l2, the lru block is victimized for the incoming block (step 2a). the victim block is moved to a randomly-selected remote receiver l2 cache. the random selection of a receiver cache is appropriate in a bus-based cmp we assume, because the cache latency is uniform across all remote caches. for a nuca cmp, however, a distance-aware selection is warranted. we leave this as future work."
"comparing the average local l2 cache hit rates achieved by opp versus dsr (46.5% vs. 32%) and versus oprp (46.5% vs. 56%), we can infer that the majority (roughly 60%) of the performance gap between dsr and oprp is due to the intelligent placement of blocks, while the remaining is due to perfect versus lru replacement policies. this corroborates our initial hypothesis that selective placement of cache blocks plays a major role in boosting the performance of capacity sharing schemes, and that our priority should be in providing intelligent placement of blocks, ahead of improving the cache replacement policies."
"our future work includes the subjective evaluation of more different contents and the study of the correlation of the subjective results with other objective metrics, including reducing reference and no-reference metrics, as well as full-reference metrics. also, the use of objective metrics in adaptation streaming algorithms has to be analyzed."
"if the block exists in any remote cache, and app predictor determined local placement, then a swap transaction is initiated. a swap transaction places the local victim block in a special buffer to create a placeholder for the remote block. then, the remote block is placed on the bus using a regular cache-to-cache transfer, and is picked up by the requesting cache. finally, the local victim is pushed to the remote cache. the first two steps are already supported in current mesi protocols. the last step, pushing a block to a remote cache, is already supported in some systems, such as the ibm wirespeed cmp [cit] . these steps do not involve a load or store instruction, so there are no critical path or memory consistency concerns. the only modifications to the cache coherence protocol are: (1) a new swap transaction to distinguish it from regular read miss, (2) a transient state in the coherence protocol to indicate the swap is pending completion, and (3) potentially additional buffers to hold swapped blocks temporarily."
"in this section we evaluate the performance of app against a base case of private caches without capacity sharing (base), cooperative caching (cc) [cit], table iv . performance metrics used and their definitions. ipc i,base represents the ipc of an application running alone on core i without capacity sharing enabled, while ipc i represents the ipc of an application running on core i with capacity sharing enabled"
"the scalable technology and svc in particular is an important subject in this field. the survey [cit] gives a good review of the state of the art of subjective and objective studies, with scalable streaming in general, including svc technology. most of the studies involving svc are evolutions of codec evaluation studies, applied to svc to evaluate different layering strategies and their qoe [cit] . the direct outcome that svc technology has enabled is the upsurge of adaptive streaming services. the importance of a qoe driven adaptive system has been experienced in the context of a p2p application [cit] . the authors directly use objective qoe metrics of each layer in the scaling decision taking algorithm."
"we can make the following observations from figure 7 . first, we can see that cc experiences the lowest local l2 hit rates (30.5%). recall that in cc any core is allowed to spill victim blocks to any core regardless of their cache demand. hence, the hit rates of acceptors decrease due to the cache pollution from donors, compared to the base case of private caches."
"finally, while app improves the hit rate significantly over the base case, there remains a gap between base and opp hit rates (37% vs. 46.5%). the gap is caused by opp making placement decisions based on future accesses, whereas app makes decisions based on past access patterns. in addition, app uses a relatively simple predictor design. more sophisticated predictor designs can narrow the gap with opp significantly, at the expense of increased complexities (section 6.3)."
"overall, the experimental results stress that app outperforms local block placement schemes because of the improvement in local miss rate (more blocks are found in the local l2 cache), as well as the slight improvement in the chip-wide miss rate (more blocks are found in the l2 caches on chip). further, the performance gain in anti-lru applications does not come at the expense of degrading perfect-lru applications. finally, app safe-guards receiver applications from being adversely affected as they donate excess cache capacity to spiller applications."
"target i: in severe congestion situations, do users value significant quality and temporal target iii: in severe congestion situations, which is better, a drastic temporal adaptation or keeping up with high loss rate?"
"to understand why app outperforms cc and dsr but is outperformed by opp and oprp, figure 7 shows the average local cache hit rate for each group of workloads. the hit rate for acceptors in each group of workloads is calculated by averaging the hit rates of all acceptor applications."
"chip-wide hit rate. earlier, we have shown that app increases the local l2 cache hit rate for acceptor applications by about 5% compared to dsr. such an increase in the local cache hit rate is sufficient to make app's performance better than dsr's. however, this assumption holds true if and only if the increase in the local cache hit rate does not come at the expense of an inordinate drop in remote cache hits. figure 8 shows the average chip-wide l2 hit rates (the total number of l2 hits that are satisfied by the local or remote caches, divided by the total number of l2 cache accesses) for all acceptors in a given group of workloads. this metric considers both local and remote hits."
"the rest of the paper is organized as follows: section 2 reviews the most important related work. section 3 summarizes the subjective experiments performed. section 4 discusses results and finally, section 5 contains the conclusions and outlines future work."
"we assume that l2 caches are interconnected with a shared bus, kept coherent with a mesi broadcast/snoopy coherence protocol. upon a local cache miss, the app predictor is consulted, and the miss is broadcast on the bus."
"as a result to pc-based predictor's serious drawbacks, we adopt the address-based predictor as a primary design, but we evaluate all three predictor designs in detail: we quantify the hardware costs, predictor accuracy and performance of the three predictor designs in section 6.3."
"the reason why pc-based predictors perform better than address-based and hybrid predictors can be seen in the predictor accuracies in figure 9 (c). pc.12's average accuracy is 82% (93% maximum and 62% minimum), while tag.8's average accuracy is 73% (80% maximum and 60% minimum). the superiority of pc-based design can be attributed to the small number of pcs in a program phase contributing to most of cache misses, and the uniform temporal reuse behavior for all blocks accessed by the pcs. the address-based predictors, on the other hand, map many more block addresses to a small number of prediction table entries, hence accuracy relies on the blocks showing identical reuse distance behavior."
"while capacity sharing is appealing due to significant performance improvement spiller applications may achieve, it may reduce the performance of receiver applications sufficiently to violate some quality-of-service (qos) requirements. such a situation must be avoided."
"the figure shows that as the cache size increases, the average performance improvement from app (and dsr) increases. the reason is that with larger caches, there is more excess cache capacity that can be donated by receivers, and there are more benchmarks which change from spillers to receivers as their working sets now fit in the cache. the increase in excess cache capacity in the system increase the number of spillers and improves the performance of each spiller."
"carrying out subjective tests is rather costly. for that reason, objective metrics are continuously being investigated to be able to fully substitute subjective measures, with sufficient conviction. using objective qoe metrics is the path for qoe driven automatic adaptation methods. they can be incorporated into adaptive systems to fine tune adaptation decisions on the server or proxy side. however, for that method to serve its purpose, objective metrics have to be strongly correlated with subjective scores."
"through stimulus comparison adjectival categorical judgement (scacj) methodology, 75 users are presented with a pair of sequences corresponding to the result of video streaming in a given network situation with the two systems, adaptive and non-adaptive, fig.1 . after the visualization, the users gave grades in a categorical scale, fig.2 . the proposed methodology assumes seven assessment categories to indicate the preferred stimuli, from -3 to +3. fig.3 shows the proposed scale. results are given in cmos. positive values of cmos indicated the preference for the adaptive system, while the preference for the non-adaptive system is translated into negative values."
"next, we will discuss the app predictor structures. instruction-based predictor. the role of app predictor is to record the usage behavior of evicted blocks when they were last resident in the local l2 cache. an important question is whether the information should be recorded per instruction, per address, or a combination of them. the first option is to record the usage information per program counter (pc) of the memory instruction that accesses the block. with this option, we keep a per-pc access history in a prediction table. any blocks that are brought into the cache by a pc will be tagged by this pc, and when any of them is accessed during its residence and is evicted, we set the accessed bit for this pc to record the reuse."
"following the structure of the subjective tests performed, we have computed the objective fr metrics for all sequences grouped in pairs used in the subjective experiment. in our subjective experiment, participants were given the choice of which sequence in the pair they liked the most (according to the double stimulus subjective methodology). as a result, we obtain cmos which indicate the direction and amplitude of the users' preferences. as explained in the previous section, a positive cmos value for a pair is indicative of the users' preference towards the adaptive system. correspondingly, a negative cmos, is a sign of users opting for the reference (non-adaptive) sequence."
"to test each workload, each of the four applications in the workload are fastforwarded for 10 billion instructions in order to skip their initialization phase. after that but prior to statistics collection, the cache models are warmed for 1 billion cycles. finally, timing simulation is started and each workload is run until the slowest application runs for 250m instructions."
"capacity sharing is a technique for reducing cache fragmentation in a cmp system with private last-level caches; it allows applications that need additional cache space to place their blocks in remote caches. current capacity sharing mechanisms treat remote caches as the victim cache for the local cache. we have shown that such a strategy guarantees a high number of remote cache hits relative to local cache hits for applications that exhibit anti-lru behavior, which are usually the same applications that benefit from additional cache capacity."
"to achieve that, we rely on set dueling [cit] ], where each cache dedicates a small subset of sets (e.g., 32 randomly chosen sets) across all caches to always spill, and another subset to always receive. the miss rates of spiller subset and receiver subset are continuously compared to determine which subset achieves a lower all-core miss rate. the policy applied to the rest of the cache sets follows one from the subset that achieves the lowest miss rate. in order to choose a winning policy (spill vs. receive), each cache is augmented with a 10-bit saturating psel counter [cit] ]."
"measured as the comparative mos between adaptive and non-adaptive system, mean cmos values processed for the total amount of 75 participants were mainly positive, which implies a preference towards the adaptive system. according to age, dividing the population in half (under or over 35 years of age), we observed that the younger group is more critical with their subjective evaluations while the older subset provided cmos values more centered towards zero (indifference)."
"in this paper, we have evaluated the relation between human decision and objective fr metrics. this work allows evaluating the readiness of the objective fr metrics to be applied to decision taking algorithms in the adaptive streaming systems. the large subjective studies have allowed performing an expansive study correlating subjective metrics with objective video quality algorithms. the particular nature of the subjective methodology makes it possible to corroborate if objective measures are able to mimic human decisions when choosing between different streaming systems."
"if an application has a perfect lru temporal reuse behavior, that is, blocks that were accessed more recently have a higher likelihood of being accessed again, the stack distance profile will show monotonically decreasing bars. the figure shows that all donor applications exhibit perfect lru behavior. however, acceptors show more interesting behavior. four out of five acceptor applications (xalancbmk, omnetpp, soplex, and hmmer) show imperfect (or anti-) lru temporal reuse behavior as evidenced by significant bumps at way8-32 in the stack distance profile. only one acceptor application (bzip2) has a nearly perfect lru profile. the extent of anti-lru temporal reuse behavior varies across applications. for example, omnetpp has less than 3.60% hits in the first 8 stack positions (1mb of cache) but 95.90% hits in the next 24 ways (1 to 4mb of cache). the high correlation between acceptors and anti-lru behavior makes sense, because with lru's monotonic decrease, once one bar has a small fraction of access, accesses to higher stack positions must be lower than it. therefore, overall we can conclude that acceptors tend to exhibit some anti-lru temporal locality behavior, while donors tend to exhibit perfect lru temporal locality behavior."
"the question is how to analyze which combinations of layers and adaptations give the best qoe results at the end user location. the answer is clear, ask the user. we previously performed a subjective evaluation of an adaptive system, comparing in different scenarios the adaptive streaming system with a regular non-adaptive streaming service [cit] . in that work we answered a set of key questions regarding adaptation decisions and provided a subjective database for others to continue reinforcing our results."
"chip multiprocessor (cmp) design presents interesting design choices in how to organize the on-chip caches. while typically l1 caches are private per core due to their tight timing requirement, whether the l2 (or l3) cache should be private to each core or shared by all cores is an open question. a physically shared l2 cache allows applications to more fluidly divide up the cache space, and maximizes the aggregate cache capacity because no block is replicated in the l2 cache. however, a large cache has a high access latency. on the other hand, private per-core l2 caches provide low this research is supported in part by national science foundation (nsf) grant ccf-0347425. contact author's address: a. samih, department of electrical and computer engineering, north carolina state university, raleigh, nc 27695-7256; email: aasamih@ncsu.edu. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. permissions may be requested fromlatency accesses to the corresponding core, provide performance isolation between cores, and allow a more scalable multicore configuration. however, private l2 caches have the major disadvantage of capacity fragmentation, that is, when a diverse mix of sequential programs runs on different cores, some programs may overutilize the local l2 cache due to their large working set, while others may underutilize it due to their small working set."
"our investigation confirms that applications that benefit from additional cache capacity (acceptors) are also ones that tend to experience a high number of remote hits in dsr. the high number of remote cache hits is a significant reason for the performance gap between dsr and oprp. to narrow the gap, we design a simple, predictor-based scheme called adaptive placement policy (app) that learns from an application's past cache behavior to make a remote placement decision. app's predictor structure is small (32 bytes in size) and has a simple organization. app tracks whether a local cache block is accessed by the processor while it resides in the local cache and marks blocks that are never accessed as potential candidates for remote placement. further, app uses set-dueling [cit] ] to dynamically identify applications that should be allowed to place their blocks in remote caches. despite its simplicity, app performs robustly across a wide range of workloads and scenarios. we evaluate app on a quad-core cmp system with 1 mb, 8-way, private last level l2 caches. we use 50 multiprogrammed workload mixes each consisting of 4 [cit] applications. the workload mixes contain varying ratios of acceptor and donor applications. our evaluation shows that app improves the performance of a cmp without capacity sharing by 29% on average. app also outperforms the state-of-the-art capacity sharing technique, dsr, by up to 18.2%, with a maximum degradation of only 0.5%, and an average improvement of 3%. further, we study the sensitivity of app and dsr to increasing remote cache hit latency, and a range of l2 cache sizes, as would be expected in larger-scale cmps. we find that app continues to perform robustly under these scenarios."
"the figure also shows that across all cache sizes, app consistently outperforms dsr. however, the relative gap between them narrows with larger cache sizes. the reason is that larger caches also convert many remote cache hits in dsr into local cache hits as the local cache can hold more of the working set of the acceptor applications. thus, in a way, larger caches compete with app in tackling the same problem, which is, improving the local hit rates. however, averages in this case can be misleading, because with larger caches, some acceptor applications become donor applications as their working sets now fit in the local cache. since app only benefits acceptor applications, the average performance improvement reflects fewer benchmarks that enjoy significant performance improvement."
"there is a rich body of work that has studied the problem of effectively managing shared on-chip cache resources in cmps. solutions include software-controlled schemes [cit], and hardware-only techniques. hardware techniques differ in their approach: partitioning cache, ways [cit], partitioning cache sets [cit], partitioning groups of lines [cit], thread-aware replacement policy [cit] ], thread-aware insertion policy [cit], and thread-aware insertion and promotion policies [cit] . dynamic insertion policy (dip) [cit] and thread-aware dynamic insertion policy (tadip) [cit] motivates the need for careful insertion of a block into a cache set. another recent proposal, pseudo-lifo [cit] ] effectively impacts the insertion position of a block by utilizing both the recency stack and the fill stack, which ranks lines based on the order in which they were inserted into a cache set. this body of work explores the impact insertion (and replacement) decisions can make in shared caches."
"the optimality of the placement or replacement decision can only be guaranteed for a given application and not globally across a mix of applications. this is because there is no way to know a priori how traces of accesses from different applications will be interleaved in the actual multiprogrammed execution. therefore, oprp only makes oracular decisions from a spiller's perspective, but not a receiver's. for receivers, we simply incorporate performance guard bands so that their performance is not impacted by much (discussed in section 4.5)."
"the paradigm of qoe is an important subject of study which has received a great deal of focus in recent years. seeking an improved performance in terms of users' perception, many works have focused on understanding the implications of all the factors influencing qoe. the analysis of mean opinion score (mos) has evolved from codec evaluation to be used to assess qoe studies in general. for practical reasons (related to time consumption), subjective studies need to be complemented by objective measures. their close relationship is an ongoing work."
"without future trace information, app cannot determine exactly when an incoming block will be accessed again in the future. however, it can record the past history of the block and extrapolate its behavior into the future. app records whether a block was accessed while it was last resident in the cache. a block that was not accessed during its residence in the local l2 cache either indicates that it will never be accessed again, or more likely its stack/reuse distance is larger than the l2 cache associativity. thus, if the behavior repeats, such a block should still be fetched on chip, but placed in a remote l2 cache so that it does not pollute the local l2 cache while it is waiting to be accessed again. app predictor attempts to identify such blocks."
"both cc and dsr have a common characteristic: remote caches are treated as the victim cache for the local cache. this article investigates if such a strategy is the best way for providing capacity sharing. in particular, we investigate a previously unexplored strategy that considers placing newly fetched blocks directly in remote caches, in addition to considering placing locally evicted blocks in remote caches. a remote placement strategy is motivated by our observation that the stack/reuse distance of applications that benefit from capacity sharing tends to be high, which causes a lot of remote cache hits compared to local cache hits. by placing incoming blocks in remote caches, we less disturb the local cache population, and many remote hits can be converted into potential local hits. to test the potential performance improvement from remote placement, we developed a hypothetical oracular scheme that, based on a future-access trace, determines whether a newly fetched block should be placed in the local cache or remote cache. in addition, in case it decides to place a newly fetched block in the local cache, it then consults the future-trace again to identify the ideal replacement candidate in the local cache. we call this hypothetical capacity sharing technique oracular placement and replacement policy (oprp). figure 1 shows the speedup of the state-of-the-art dsr [cit] ] and the oracle scheme (oprp) compared to the base case where the cmp has private caches without capacity sharing. a x d y represents workloads that have x number of acceptor applications and y number of donor applications. the figure shows the average behavior within subsets of workloads grouped by a unique acceptor-to-donor ratios, as well as the average across all workloads."
"private caches present a new set of challenges and opportunities. placement across caches, not insertion in the local cache at a particular stack position, should be considered. placement decisions, even on a hit, can significantly impact the block's access latency. in addition, there is typically no global recency or fill stack information across corresponding sets in the private caches."
"next, we provide the graphic relationship between individual objective metric differences and subjective cmos values. to solve the problem of negative values, a linear transformation is applied on the objective metric series. then, the exponential relationship can be calculated as a linear relationship between the subjective scores and logarithm of the objective metric series."
"worst case performance of app. we discussed in section 6.1 that applications with an almost perfect-lru behavior are not expected to benefit from app since all incoming lines in such an application should be placed locally. however, if app cannot improve the performance of some applications compared to dsr, it should not degrade them either. our results confirm that app's maximum performance degradation compared to dsr, across all workloads, is a negligible 0.5% (m20). this is in contrast to app outperforming dsr by 18.2% in the best case (m44), and by 3% on average."
"third, compared to dsr, app's local l2 hit rates for acceptors are about 5% higher for all workload groups (37% vs. 32%). this result demonstrates that by selectively placing incoming blocks in remote caches, app is successful in retaining more useful blocks in the local l2 cache, thus improving the local l2 hit rates. the increase in local l2 cache hit rates is responsible for the performance improvement app achieves over dsr."
"the predictor table is is indexed by 12 least significant bits of the pc, whereas the remaining pc bits are stored in each entry as tag bits. in addition to tag bits, each entry includes a valid bit, which is used to validate a given pc entry, and a 2-bit saturating counter, which is used to approve a remote versus local prediction. the use of these bits will be discussed in greater detail later."
"as an applied example, we used the same cage hela data set as above and identified bcs outside of exonic regions and more than 1 kbp upstream of annotated promoters (based on ucsc transcript models) as enhancer candidates. this resulted in a total of 6384 enhancer candidates (3780 intronic and 2604 intergenic)."
"one of the reasons for this is that, traditionally, system performance has only ever been evaluated against the gold standard annotations of a single native speaker (rarely, two native speakers). as such, system output is not actually scored on the basis of grammatical acceptability alone, but rather is also constrained by the idiosyncrasies of the particular annotators."
"in a nutshell, i believe that people should be obligated to tell their relatives about the genetic testing result for the good of their health."
"to illustrate these features in cagefightr, we used the ulcerative colitis set, and assigned tcs to genes using ucsc gene models and determined how much each such tcs contributed to overall gene expression. without any composition filtering, 40% of all genes used more than one tc, falling to 23% when only considering tcs contributing more than 10% of total gene expression in more than 6 samples (fig. 4a ). the majority of discarded tcs were found in protein-coding, intronic and 3′-utr regions ( fig. 4b ), but several interesting cases remained, for example in the slc16a5 gene, where we identified a highly expressed novel intronic tc (fig. 4c) ."
"now that we had empirical evidence showing how f 0.5 scores varied with the number of annotators, an additional question to ask was whether the same trends for 50 essays were also present in a smaller subset of essays. we therefore repeated the main experiment with all error types, but this time used just 10 essays (specifically, essays 1-10) in both the hypothesis and gold standard. the results are shown in figure 3 ."
"transcript and gene models were obtained from ucsc via the r/bioconductor packages txdb.hsapiens.ucsc.hg19. knowngene and txdb.mmusculus.ucsc.mm9.knowngene. chromatin data was obtained from the roadmap epigenomics project [cit] via the annotationhub r/bioconductor package, for hela cells ah32877, ah32879, ah32881 and ah32884 and for lymphoblastoid cell lines ah32865, ah33899, ah33901 and ah33904."
"to put it in a nutshell, i believe that people should have the obligation to tell their relatives about the genetic testing result for the good of their health."
"in all, while the specific time taken to complete annotation of all 50 essays was not calculated, all annotators completed the task over a period of about 3 weeks, at a rate of about 45 minutes per essay."
"one important study that made use of κ [cit] ), who asked two native english speakers to insert a missing preposition into 200 randomly chosen, well-formed sentences from which a single preposition had been removed."
"to put it in a nutshell, i believe that people should be obliged to tell their relatives about their genetic test results for the good of their health."
"equal to the total number of annotators. we only compute all combinations here in order to quantify, for the first time, how much each additional annotator affects performance."
"pooled ctsss can be used to identify clusters of closely spaced ctsss on the same strand, referred to as unidirectional clusters, or conventionally tag clusters (tcs) in most cage papers. cagefightr identifies tcs using a slice-reduce approach: first, ctsss with pooled ctss signal below a chosen threshold are discarded (slice) and surviving ctsss on the same strand are then merged into clusters (reduce) ( fig. 1a-b ). cagefightr includes a host of functions for analyzing such tcs, including filtering on expression, hierarchical annotation of tcs with transcript models and analysis of tc shapes (see below)."
"as can be seen, there is quite a difference between the annotator who made the most edits (a1) and the annotator who made the fewest edits (a7), with a1 making more than twice the number of edits as a7. this just goes to show how varied judgments on grammaticality can be. incidentally, annotators a3 and a7, [cit] . there is also a large difference between edits in category a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 total artordet 879 639 443 503 665 620 331 358 390 624 5452 cit 0 0 0 0 0 1 0 2 0 0 3 mec 227 376 493 325 411 336 228 733 598 780 4507 nn 404 290 228 264 360 300 215 254 277 365 2957 npos 21 21 15 21 31 28 19 25 29 23 233 others 42 186 49 116 95 43 44 34 125 105 839 pform 431 52 18 57 30 83 47 53 19 18 808 pref 4 79 153 18 223 53 96 92 250 180 1148 prep 755 488 390 421 502 556 211 276 362 459 4420 rloc-488 308 199 331 187 244 94 174 296 240 2561 sfrag 1 5 1 3 1 5 13 2 12 2 45 smod 1 4 5 0 1 0 0 3 1 1 16 spar 0 18 24 0 2 11 3 2 8 0 68 srun 157 38 21 16 17 18 7 15 17 terms of category use, with almost half of all edits falling into the categories for article or determiner (artordet), spelling or punctuation (mec), preposition (prep), or word choice (wci) errors."
"to summarize, we first showed that 10 individual annotators can all correct the same sentence in 10 different ways, yet also all produce valid alternatives. this implies that inter-annotator agreement statistics, which rely on exact matching, are not well-suited to grammatical error correction, because it may not be the case that annotators truly disagree, but rather that they have a bias towards a particular type of alternative answer."
"below, we overview the core functionality of cage-fightr with examples using previously published 5′-end data. names of the main cagefightr functions for each analysis are indicated in fig. 1, 2"
"analysis of all 5′-end data was based on supplied ctsss from the geo repositories of the respective papers. the hela set [cit] was obtained from gse62047, using only egfp samples. the ulcerative colitis set [cit] was obtained from gse95437, using only active ulcerative colitis (uca) samples from the largest batch. the pro-cap set [cit] was obtained from gse110638, using only nonreplicated samples."
"this time, despite operating at the more general sentence level, the authors reported κ scores of just 0.16, 0.4 and 0.23, surmising that \"the low numbers reflect the difficulty of the task and the variability of the native speakers' judgments about acceptable usage.\" if that is the case, then true disagreement may be indistinguishable from native variability, and we should be wary of using iaa statistics as a measure of agreement or evaluation in gec."
"additional file 1: figure s1 . details on finding bidirectional clusters (bcs). a: calculating balance score using the bhattacharyya coefficient. for a potential bc midpoint, pooled ctss signal is summed within a certain distance (200 bp by default) on both strands, yielding four values (left). the \"ideal\" bidirectional cluster would have only perfect divergent signal (50% pd and 50% md). the bhattacharyya coefficient quantifies the difference between the observed signal to this ideal enhancer (right), with a balance score of 1 indicating perfect agreement. the balance score is calculated for every bp in the genome (fig. 1c)"
"while conceived as a tool for analyzing cage data, cagefightr can analyze any 5′-end data similar to cage, including nascent rna 5′-end methods ( table 1) ."
"in particular, noun number (nn) and subjectverb agreement (sva) errors achieve the highest scores, at just under 90% f 0.5, which is also not far from the 100% f 0.5 that would be achieved if we had gold standard answers for all possible alternative corrections of this type. the most likely reason for this is that, as the correction of these error types typically only involves the addition or removal of an -s suffix, i.e., a minor change in number morphology, there is very little room for annotators to disagree."
"the last three categories, conjunctions (alltypes) (trans), word order (woinc) and word choice (wci), are all notable because they perform significantly worse than the hitherto mentioned categories. the main reason for this is that these error types all typically have a scope much larger than most other categories in that they often involve changes at the structural or semantic level; e.g., changing an active to a passive or choosing a synonym. for this reason, there are often many more alternative ways to correct them, meaning they are also much more likely to be affected by annotator bias."
"in light of the results, we also explore how human annotators themselves compare against the combined annotations of the remaining annotators and thus calculate an upper bound f 0.5 score for the given dataset and number of annotators; e.g., if one human versus nine other humans is only able to score a maximum of 70% f 0.5, then it is unreasonable to expect a machine to do better. for this reason, we propose a more informative method of evaluating a system based on the ratio of that system's f 0.5 score against the equivalent human f 0.5 score."
"using this method, we plotted the number of tcs falling into the different annotation categories and their expression distributions (fig. 2b ). most tcs candidates were found at annotated promoters and were generally highly expressed. however, a substantial number of novel (not overlapping annotated promoters) tcs were identified, in particular in the promoter-proximal region and 5′-utr, which on average had lower expression than those overlapping annotated promoters. in vertebrates, the distribution of ctsss within tcs is related to cell specificity and dna sequence properties: sharp ctss distributions have an overrepresentation of tata-boxes and are more cell-or tissuespecific, while broad ctss distributions are gc-rich and more ubiquitously expressed [cit] . classification is often based on the width of the ctss distribution, expressed as the interquartile/interquantile range (iqr), as this measures the width of the bulk of ctsss within a tc without being affected by a few straggler ctsss potentially greatly extending the width of the tc. we used cagefightr to calculate the genomic region covering the 5-95% iqr of total ctss expression for each tc in the hela set (similar results were obtained with tighter iqr intervals). this showed a clear bimodal distribution corresponding to sharp and broad ctss distributions ( fig. 2c-d ). investigation of promoter regions using sequence logos also confirmed that sharp, but not broad distributions had a stronger tata box ( fig. 2e )."
"in other words, while the overall relationship between the system and human scores on 10 and 50 essays remains more or less the same, researchers must be aware that smaller datasets may have more skewed error distributions, which in turn may affect system performance, dependent upon correction strategy. with a balanced test set though, it would seem feasible to carry out future evaluation research on as few as 10 essays (about 6000 words)."
"we notice from these scores that, as expected, both system and human performance increases as more annotators are used in a gold standard. we do now, however, have data that quantifies exactly how much each additional annotator affects the score. this effect can be more clearly seen in figure 1 ."
"as an initial validation step, we investigated whether enhancer candidates had the expected chromatin patterns compared to tcs, by overlapping with dnase i hypersensitive sites sequencing (dnase-seq), h3k27ac, h3k4me3 and h3k4me1 chip-seq signals from the same cell type. as expected, we observed high dnase sensitivity at enhancer midpoints and tc peaks, and higher levels of h3k27ac at tcs, compared to enhancer candidates. the ratio of h3k4me3 to h3k4me1 is often used to predict enhancers from chip-seq signals, and consistent with this we observed low average h3k4me3 and high h3k4me1 signals around predicted enhancer candidates, and the opposite patterns around tcs (fig. 3a) [cit] ."
"compared to figure 1, the most significant difference between these two graphs is that the ranking for amu and cuui has changed, although not by much in terms of f 0.5 . the most likely reason for this is that the distribution of error types in the smaller subset of essays is better suited to amu's more general smt approach than to cuui's more targeted classifier based approach. for instance, see table 9 [cit] shared task."
"although tcs can be identified de novo, it is useful to be able to analyze their expression across known gene models. examples include the ability to compare 5′-end expression with rna-seq expression on gene level [cit], or to link 5′-end expression estimates with gene-centric databases, such as gene ontology (go) terms [cit] or pathway/interaction annotation (kyoto encyclopedia of genes and genomes (kegg) [cit], string [cit], etc.). cagefightr includes functions for annotating tcs to known genes and summarizing their expression within genes to obtain a gene-level expression matrix (fig. 1a, bottom) . this gene-level expression matrix can readily be used with other bioconductor packages for gene-level analysis (e.g. limma [cit], edger [cit], deseq2 [cit] )."
"tcs reflect the fact that when rna-polymerase associates with the dna, it rarely initiates from a single bp, but rather from an array of nearby bps. such arrays are expected to produce nearly identical rnas that are subject to the same regulatory cues, as they will share the same promoter sequence and genomic neighborhood. while genes are transcribed from multiple different ctsss, these ctsss are grouped in a single or multiple tcs corresponding to the major rnas (or transcripts/isoforms in rna-seq terminology) produced from the gene."
"interest in grammatical error correction (gec) systems has grown considerably in the past few years, thanks mainly to the success of the recent helping our own (hoo) [cit] and conference on natural language learning (conll) [cit] ) shared tasks. despite this increasing attention, however, one of the most significant challenges facing gec today is the lack of a robust evaluation practice. [cit] even go as far to say that it is sometimes \"hard to draw meaningful comparisons between different approaches, even when they are evaluated on the same corpus.\""
"several genome-wide, high-throughput sequencing assays have been developed for identifying tss activity, all based on the idea of capturing and sequencing only the 5′-end of rnas (called tags), leading to the collective name of 5′-end methods [cit] . in terms of tss identification, such methods have distinct advantages over other assays, e.g. rna-sequencing (rna-seq) and chromatin immunoprecipitation sequencing (chip-seq). while rna-seq is widely used for studying gene expression and splicing, it is ineffective for accurate detection of tsss. this is due to the random fragmentation of rna molecules, leading to a trail-off of sequencing reads near the end of transcripts. in contrast, 5′-end methods effectively pile up reads at tsss, providing high local coverage for accurate prediction of tsss. similarly, chip-seq targeting rna polymerase ii or pre-initiation complex proteins has low positional resolution due to the length of chip-seq fragments, and does not explicitly measure tss usage."
"to put it in the nutshell, i believe that people should have the obligation to tell their relatives about the genetic testing result for the good of their health."
"all annotations were made using an online annotation platform, wamp, especially designed for annotating esl errors [cit] highlight a minimal error string in the source text, provide an appropriate correction, and then categorize their selection according to the same 28 [cit] . before commencing annotation, however, each annotator was given detailed instructions on how to use the tool, along with an explanation of each of the error categories. in cases of uncertainty, annotators were also encouraged to ask questions."
"to put it in a nutshell, i believe that people should be obligated to tell their relatives about the genetic testing result for the good of their health."
"the 10 annotators who annotated all 50 essays include: the 2 [cit], the first author of this paper, and 7 freelancers who were recruited via online recruitment website, elance. 3 all annotators are native british english speakers, many of whom also have backgrounds in english language teaching, proofreading, and/or linguistics."
"because of this, many studies use a simplification in which such tcs are referred to as tsss for genes, even though technically tcs group several nearby bp-accurate tss / ctsss. to avoid confusion on terminology, and remain consistent with previous cage literature, we will primarily use the term`tc`to describe unidirectional clusters."
"finally, not only are we making the corrections by 10 annotators of all 50 essays available with this paper, we also showed that the trends found in the data are also consistent with the annotations of just 10 essays, allowing future research to be conducted on much less text."
"in summary, i believe that people should have the obligation to tell their relatives about the genetic testing result for the good of their health."
"in a nutshell, i believe that people should have an obligation to tell their relatives about the genetic testing result for the good of their health."
"verb \"have the obligation\" from active to passive, although a1 still disagreed with the others on the form of the participle. similarly, there is also a great difference of opinion on whether \"testing result\" should be corrected or not, and if so, how. while half of the annotators left the phrase unchanged, a1, a6, and a10 all changed both words to \"test results\". meanwhile, somewhere in between, a5 decided to change \"result\" to \"results\", but not \"testing\" to \"test\", while, conversely, a9 decided to do the opposite. this would suggest that error correction of even minor phrases falls along a continuum governed by each annotator's natural bias."
"cagefightr is a user-friendly r-bioconductor package for analyzing cage and 5′-end data. it includes a wide range of functions for analyzing tsss and enhancers and generating publication-ready visualizations. cagefightr was designed from the ground up to adhere closely to bioconductor standards, making it easy to learn, use and combine cagefightr with other bioconductor packages for transcriptomic analyses. cagefightr is extensively documented, including both a vignette describing the core functionality (http://bioconductor.org/packages/cagefightr/) and a case-study based workflow discussing common analysis tasks on a full dataset [cit] (http://bioconductor.org/packages/cageworkflow/)."
"nevertheless, while several papers have successfully discussed ways to minimize annotator bias effects in smt [cit], iaa metrics such as κ still unhelpfully play a role in the field and have, for example, been reported almost every year in the workshop on machine translation (wmt) conference."
"cap analysis of gene expression (cage [cit] ), based on reverse transcription of total rna followed by captrapping, is arguably the most used 5′-end method and has the widest range of developed protocols (table 1) . cage has been applied in a multitude of different settings, including consortiums (fantom [cit] and en-code [cit] ), multiple species (mammals [cit], insects [cit], fungi [cit], plants [cit], etc.) and in clinical settings (inflammatory bowel disease [cit], diabetes [cit], cancer [cit], retts syndrome [cit] ). despite its wide usage, the current toolbase available for analyzing 5′end data is not as developed as that for rna-seq or chip-seq (table 2 ). most tools are either stand-alone pipelines (moirai [cit], reclu [cit], etc.) or focused on a single analysis problem, e.g. tag clustering (paraclu [cit], capfilter [cit], etc., further discussed below), making it hard to combine different tools. an alternative to stand-alone tools is using r-packages from the bioconductor [cit] project, which allows easier interoperability between tools due to shared data representations. bioconductor currently contains three packages (cager [cit], icetea [cit], tsrchitect [cit] ) for analyzing 5′-end data in general and cage in particular. while these packages offer functionality for tss identification, quantification and annotation, they lack any functions for predicting, quantifying and analyzing enhancer candidates, and are not efficiently scalable for large datasets."
"like figure 1, we can see from figure 2 that the f 0.5 performance of individual error types increases diminishingly as the number of annotators in the gold standard also increases. more importantly, however, we notice that some error types achieve much higher scores than others, which suggests some annotators agree on certain categories more than others."
"cagefightr is implemented purely in r making use of several r-packages from the bioconductor project. it is based on standard bioconductor s4-classes, primarily granges (genomicranges), rangedsummarizedexperiment (summarizedexperiment) [cit] and ginteractions (interactionset) [cit] and visualization via gviz [cit] and genomicinteractions [cit] . this makes it easy to use cagefightr in conjunction with other bioconductor packages. 5′-end data is conventionally stored, shared and analyzed by first mapping tags to the genome, followed by counting the number of 5′-ends of tags mapping to each individual base pair (bp), on each strand. in cage terminology, such data are referred to as cage-defined tsss (ctsss) [cit], but we use the term generally for all 5′-end methods here. the processing of tags differs between 5′-methods due to distinctive protocols (5′-end isolation technique, single-end vs. paired-end sequencing, etc.), and for cage in particular specialized tools have been developed, e.g. rrnadust for removing contaminant ribosomal rna (http://fantom.gsc.riken.jp/5/ suppl/rrnadust/) and/or removing g's added by reverse transcriptase at cdna 5′-ends [cit] . while filtering, mapping, and counting of tags can be done efficiently by dedicated tools a single library at a time, cagefightr is focused on analysis from the point when multiple libraries are jointly analyzed. to be as general as possible, cagefightr was designed to import and analyze 5′-end data after mapping and processing by starting from ctsss from each library stored as bigwig files."
"all analyses of 5′-end data were done using cagefightr as indicated in the main text. average meta profiles were [cit] (https://github.com/anderssonrobin/enhancers). tcs used as input were defined by cagefightr with default settings. a balance cutoff of 0.6 was used, as this corresponds to the 0.95 balance cutoff used in cagefightr in the case of a bc with only divergent signal (pd and md in additional file 1: figure s1a )."
"as described below, cagefightr can analyze 5′-end data on three different levels: bp-accurate ctsss (fig. 1a, top), clusters of nearby ctsss (fig. 1a, middle) or expression summed over known gene models (fig. 1a, bottom), where each analysis level is associated with a specific expression matrix ( fig. 1a, right) . these expression matrices and other data structures used in cagefightr are designed to be readily usable by other bioconductor packages, in particular popular differential expression packages such as limma [cit], edger [cit], deseq2 [cit], dexseq [cit], drimseq [cit], etc."
"to put it in a nutshell, i believe that people should be obligated to tell their relatives about the genetic testing results for the good of their health."
"in short, while annotators already vary as to what they consider an error, these observations show that even when they do apparently agree, there is no guarantee that every annotator will define the error in exactly the same terms. this poses a problem for iaa statistics, which rely on an exact match to measure agreement."
"as it was slightly harder to control the quality of the 7 independently recruited annotators via elance, they were each preliminarily asked to annotate only the first two essays before being given detailed feedback on their work. the main purpose of this feedback was to make sure that they a) understood the error category framework, and b) knew how to deal with more complicated cases such as word insertions, punctuation, etc. unless it was felt that they had overlooked an obvious error in these first two essays, the feedback did not go so far as to tell annotators what they should and should not highlight in an effort to preserve individual annotator bias."
"in fact, the issues regarding the reliability of iaa metrics are not unique to gec and we can also draw a parallel with the field of statistical machine translation (smt). in the same way that there is often more than one way to correct a sentence in gec, it is also well known that there is often more than one way to translate a sentence in smt."
"we next showed that, as has long been suspected, increasing the number of annotators in the gold standard also leads to an increase in f 0.5, although at a diminishing rate. this data can be used to help researchers decide how many gold standard annotations should be used in gec evaluation."
"whereas previously we could only calculate f 0.5 scores on a system vs human basis, when there are two or more annotators, we can also calculate scores on a human vs human basis. in fact, as the number of annotators increases, we can also start to calculate scores against different combinations of gold standard annotations. 6 to give an example, since we have 10 annotators, a subset of these annotators, say annotators a2-a8, could be chosen as the gold standard annotations. we could then evaluate how each of the remaining annotators (i.e., annotator a1, a9, and a10) performs against this gold standard, by computing the m2 score for annotator a1 against annotators a2-a8, annotator a9 against annotators a2-a8, and annotator a10 against annotators a2-a8. we then average these 3 m2 scores, to determine how, on average, an annotator performs when measured against gold standard annotators a2-a8."
"in addition to identifying such alternative tcs within gene models, cagefightr offers the option of filtering tcs within genes based on their contribution to overall gene expression: as 5′-end methods can detect very lowly expressed tcs, cagefightr can remove alternative tcs making up less than e.g. 10% of total gene expression in a given number of samples to focus only on the most highly expressed rnas from a gene. this filtering approach is also useful when combining cagefightr with popular tools for differential transcript usage such as limma [cit], edger [cit], dexseq [cit] and drimseq [cit], to investigate whether a given tc within a gene is preferentially used under certain conditions."
"the obvious solution to this problem would be to compare systems against the gold standard annotations of multiple annotators, in an effort to dilute the effect of individual annotator bias, however creating manual annotations is often considered too time consuming and expensive. in spite of this, while other studies have instead elected to use crowdsourcing to produce multiply-corrected annotations, often concerning only a limited number of error types [cit], one of the main contributions of this paper is the provision of a dataset of 10 [cit], that is moreover annotated for all error types. 1 with this new dataset, we have, for the first time, been able to compare system output against the gold standard annotations of a larger group of human annotators, in a realistic grammar checking scenario, and consequently been able to quantify the extent to which additional annotators affect system performance. additionally, we also noticed that some annotators tend to agree on certain error categories more than others and so attempt to explain this."
"the resulting h i values are hence the average f 0.5 scores achieved by any human against any combination of i other humans, and so, in some ways, also represent the upper bound of human performance on the current dataset. the specific values for h i are shown in the second column of table 5 ."
"to put it in the nutshell, i believe that people should have an obligation to tell their relatives about their genetic test results for the good of their health."
"to investigate the extent to which different annotators have different biases, we first counted the total number of edits made by each annotator and sorted them by error category (table 3) ."
"as well as carrying out experiments at the system level, we also carried out similar experiments at the error category level. more specifically, we re- single specific error type. [cit] were not asked to classify the type of errors their systems corrected, we were only able to calculate these new values using the 10 sets of human annotations."
"it is important to note, however, that even with 9 annotators, human output itself does not reach close to 100% f 0.5 and instead, the difference between the systems and the humans is about 20% f 0.5 . furthermore, the curves for humans and systems also remain roughly parallel, suggesting human corrections gain as much benefit as system corrections from larger sets of gold standard annotations."
"cagefightr can import ctsss from bigwig files and quantify their expression levels across all samples. the ctsss can then be normalized to tags-per-million (tpm) and summed across samples to yield a global or pooled ctss signal (fig. 1a, top) . in case of a large number and/or low quality samples, cage-fightr offers various strategies for calculating more robust pooled ctsss signals, chiefly by filtering ctsss only observed in a single or few samples. the pooled ctss signal can be visualized in genome-browser style along the genome ( fig. 1b -c, 2d, 3c, 4c, 5d)."
"the raw text data in our dataset was originally produced by 25 students at the national university of singapore (nus) who were non-native speakers of english. they were asked to write two essays on the topics of genetic testing and social media respectively. all essays were of similar length and quality. this was important because varying the skill level of the essays is likely to further affect the natural bias of the annotators, who may then consistently over-or under-correct essays. [cit] test data [cit] . see table 2 for some basic statistics on the resulting 50 essays."
"in light of the above observation that even humans vs humans are unable to score 100% f 0.5, it thus seems unreasonable to expect machines to do the same. as such, we propose that it is much more informative to score system output against the average performance of humans instead of against the theoretical maximum score. [cit] teams against the human gold standards of various sizes are hence also reported in table 5 . the most important thing to note is that these figures are not only much higher than the low f 0.5 values currently reported in the literature, they are also more representative of the state of the art. [cit], camb, is actually able to perform 73% as reliably as a human, which suggests gec may actually be a more viable technology than was previously thought."
"despite the simplicity of this correction task, the authors reported κ-agreement of just 0.7, noting that in cases where the raters disagreed, their disagreements were often \"licensed by context\" and thus actually \"acceptable alternatives\". this led them to conclude that they would \"expect even more disagreement when the task is preposition error detection in 'noisy' learner texts\" and, by extension, imply that detection of all error types in 'noisy' texts would show more disagreement still."
"to put it in a nutshell, i believe that people should have the obligation to tell their relatives about the genetic test result for the good of their health."
"another key use of gene models in relation to 5′end methods is the analysis of alternative tss or alternative promoter usage, which is a key contributor in generating transcript diversity (multiple different transcripts/isoforms from genes). this can be done by identifying genes harbouring several tcs on the same strand, with each tc giving rise to distinct rnas. in this way, tcs can be seen as tss candidates for the different transcripts/isoforms produced by a gene, phrasing the analysis in a similar way to alternative splicing or transcript usage for rna-seq. to be clear, this is different from analyzing changes in the distribution of ctsss within a tc (see above), as different tcs in a gene will be widely spaced, have different promoter sequences and genomic neighbourhoods and produce different truncations of rna, with potentially different regulation and function."
"most genome bps are not ctsss (have no tags mapped to them), and only a small fraction of ctsss have a high number of tags. cagefightr takes advantage of this sparsity by using sparse representations to efficiently store and analyze large ctss datasets using little memory. this allows tens of samples to be analyzed on a typical laptop computer and hundreds of samples on a typical server. most computationally heavy tasks can be parallelized, providing further speed increases when multiple cores or clusters are available."
"in order to quantify how much the f-score can vary in a realistic grammar checking scenario when there is only one gold standard annotator, we first computed the scores for a participating system vs each annotator in a pairwise fashion. table 4 [cit], camb [cit], performed against each of the 10 human annotators individually."
"section 2 contains an overview of some of the latest research in both gec and smt that makes use of iaa statistics. section 3 shows an example sentence from our dataset and qualitatively analyses how individual annotator bias affects their choice of corrections. section 4 describes the data collection process and presents some preliminary results. section 5 discusses the main quantitative results of the paper, formalizing the formulas used and introducing the more informative method of ratio scoring for gec, while section 6 summarizes the results from our additional experiments on category agreement and essay subsets. section 7 concludes the paper."
"transcription start sites (tsss) are central entities of transcriptional regulation, where a wide range of cues from surrounding factors such as core promoter elements, transcription factor binding sites, chromatin modifications, and distal elements such as enhancers and silencers are integrated to decide whether transcription initiation takes place, and with what rate [cit] . hence, accurate identification of tsss and their activity is a prerequisite for understanding gene regulation."
"whenever we discuss multiple annotators, researchers invariably raise the issue of interannotator agreement (iaa), or rather the extent to which annotators agree with each other. this is because data which shows a higher level of agreement is often believed to be in some way more reliable than data which has a lower agreement score. within gec, agreement has often been reported in terms of cohen's-κ [cit], although other agreement statistics could also be used. 2 in the rest of this section, however, we wish to challenge the use of iaa statistics in gec and question their value in this field. specifically, while iaa statistics may be informative in areas where items can be classified into single, welldefined categories, such as in part-of-speech tagging, we argue that they are less well-suited to gec and smt, where there is often more than one correct answer. for example, two annotators may correct or translate a given sentence in two completely different yet valid ways, but iaa statistics are only able to interpret the alternative answers as disagreements."
"the main result of this paper however, is that by computing scores for human against human, we determined that it is not true that any human correction is able to score 100% f 0.5 . instead, we found that the human upper bound is roughly 73% f 0.5 and that the top 3 [cit] actually perform, on average, between 67-73% as reliably as this human upper bound. this result is highly significant, because it suggests gec systems may actually be more viable than their previously low f 0.5 scores would suggest."
"finally, one of the most important results of this qualitative evaluation is that even though all 10 annotators edited the same sentence to a level they deemed grammatical, not one single annotator agreed with another exactly. this fact alone suggests iaa statistics are not a good way to evaluate gec data and that a more robust agreement metric must take into account the possibility of alternative correct answers."
"the most important question to ask then, as a result of this study, is whether low κ-scores in 'noisy' texts are truly indicative of real disagreement, or whether, as in this preposition test, the disagreement is actually the result of multiple correct answers, and therefore not disagreement at all."
"in addition to the above, we also found that humans tend to agree on some error categories more than others, and suggest that one of the main reasons for this concerns the size of the confusion set of the particular error type."
"as an example, table 1 illustrates sample results of the experiment for one specific frequency, for a specific participant. the last column of table 1 shows the responses of users for vibration only stimuli."
"comparing the results of so and sv tests, it is seen that vibration has no significant effect on enhancement of sound detection at 20-170 hz, since there is no significant difference in number of positive responses. at higher frequencies starting from 200 hz, sound detection performance of participants in sv test becomes significantly better compared to their performance in the so test. it can be noticed in figure 2 and figure 3 that the percentage of positive responses is significantly higher in sv test at 200 -390 hz. also, according to the results of statistical comparison between so and sv in table 2, the hypothesis is valid at test frequencies of 200hz, 230hz, 300hz and 390hz in contrast to the rest of the frequencies. this shows that vibration can elicit tactile sound perception or enhance inaudible sound detection at this particular range of frequencies."
"for k-mc, 100 iterations were performed to determine the cluster centres. however, clustering is a random process, which starts with random cluster centres. during these iterations, if the clusters did not change their positions, new clusters centres were determined to avoid the local minima problem. this process replicated itself 5 times."
"further, by using whole-scalp magnetoencephalography (meg) and analysing results, authors concluded that human auditory cortex can be activated by feeling fixed intensity vibration of 200-hz at the fingertips. also, [cit] extended this study and demonstrated auditory cortex activation by vibrotactile stimulation alone. both research experiments were conducted at fixedfrequency of 200-hz vibrations, without providing level of frequency or location effects on this phenomenon. in another work researchers studied the perceptual integration of 50, 250, and 500-hz vibrotactile and auditory tones in a detection experiment as a function of the relative phases of sound and vibration pulses [cit] . the results did not establish significance regarding the effect of phase difference in sound detection performance. however, combination of 250-hz and phase difference resulted significantly high scores in sound detection in contrast to other fixed-frequencies (e.g. 50-hz and 500-hz). the work suggests that auditory and vibrotactile signals can be effectively integrated without regard to phase difference and fine structure regulation. also, it can be speculated that audio-tactile integration is more notable in some frequencies than in others. for effective design of vibrotactile interfaces it is important to establish further understanding of the range of frequencies in which audio-tactile integration is stronger. the main hypothesis of this work is that there is a specific range of vibration frequencies in which audio-tactile integration is most intense. when it comes to sensitivity to vibrotactile stimuli, it is known that the fingertips and hand have greater density and more sensitive regions compared to the rest of the body and are more appropriate for receiving tactile information than other regions (bensmaïa, 2005; [cit] ) . tactile sensation can be caused by mechanical vibration of the skin at frequency ranges between 10 and 500-hz (johansson and löfvenberg, 1984) . [cit] have shown that at the fingertips the discriminative increment or just noticeable difference (jnd) for frequencies of 20, 50, 100 and 200-hz are 0.32 ± 0.07%, 0.19 ± 0.07%, 0.21 ± 0.03% and 0.14 ± 0.04%, respectively. however, another work suggests that jnd is constant across frequencies with a discriminate increment of 22 % (johansson and löfvenberg, 1984) . this information is employed in experimental design in this work, namely for choosing the set of test frequencies shown in table 2 . more specifically, for lower frequencies, jnd of 50, 100 and 200 hz were used to choose the frequencies (johansson and löfvenberg, 1984; [cit] ), while higher frequencies were incremented by 22 %. [cit] ."
"the purpose of this stage is to eliminate the redundancy in the eeg signals by selecting the discriminative features from the raw data. the process also helps in reducing the size of the input feature vector. in the case of eeg recordings, we have 19 channels from the 19 eeg electrodes (sensors) as shown in fig. 2 . the research in absence epilepsy has demonstrated that not all 19 channels are of the same importance. successful experiments in ref. [cit] has also established that only 10 electrodes (fp1, fp2, f3, f4, c3, c4, f7, f8, t3 and t4) out of total 19 have the properties that can help in classifying the three seizure phases. a feature vector was then formed based on these 10 selected channels."
"the organisation of this paper is as follows. section 2 explains the mechanism of digital data acquisition from epilepsy patients and also discusses the process of feature extraction method for designing the epilepsy classification system. it then explains how the three computational intelligence techniques; svm, ann and k-mc work. section 3 presents classification results that include design of classifiers using svm, ann and k-mc and evaluation of these classifiers against two traditional classification methods, nbc and k-nn. the paper ends with concluding remarks in section 4. fig. 1 shows the design cycle of epilepsy classification system used in this research. this is an iterative process in which the system is updated based on the output produced by the learning algorithms. the details of this design scheme are discussed in the following sections."
"where kð,;,þ represents a kernel function, d is the dimension of the feature vector, q is the degree of the polynomial and g sets the influence boundary for the radial basis kernel function. svm is a binary classifier that can only handle two-class classification problems. to overcome this shortcoming different strategies are used to combine the binary svm classifiers to tackle multiclass classification problems. as in our case, we are required to classify three phases of absence seizure. we have compared three different methods to combine binary svm classifiers, which are binary decision tree (bdt), one vs one (ovo), and one vs all (ova)."
"to capture the brain functioning over a period of time, local and global values of the same features were calculated from the eeg signals in the time domain. global values were calculated based on the entire signal length and local features were extracted using a windowing function. the importance of the features in the time domain is evident from fig. 2, in which there are no sudden or abrupt changes in the brain recordings during the seizure-free and pre-seizure phases. such activities are very well captured by the time domain analysis. energy (e), range (r), standard deviation (sd), sum of absolute values (sav), mean absolute values (mav) and variance (var) of each selected channel were extracted in the time domain to form the part of the feature vector."
which means a1 and d1 are the first split of signal xðtþ. then from using a1 the next split is a2 and d2 and the process continues until the desired level is reached. fig. 3 depicts the dwt decomposition scheme of a signal of length n and y 2 shows that the signal is downsized by a factor of 2 at each level.
"using 10-20 international standard electrode placement system, with all 19 electrodes (fp1, fp2, f3, f4, c3, c4 p3, p4, o1, o2, f7, f8, t3, t4, t5, t6, fz, cz, and pz) the data has been collected from the epilepsy patients suffering from the absence seizure. these recordings were taken in the peking university people's hospital, china, and the patients (6 males and 4 females), aged 8e21 years old, have agreed to use this data for research and publishing results [cit] . to record the neural activity, eeg data was samples at 256 hz, filtered from 0.5 to 35 hz bands using neurofile nt digital video eeg system [cit] . extracted data has been divided into three phases of absence seizure; seizure-free, pre-seizure and seizure, by an epilepsy neurologist. these phases are separated based on the criteria of 1) the interval between the seizure-free phase and beginning point of seizures is greater than 15 s, 2) the interval is 0 and 2 s prior to seizure onset, and 3) the interval is the first 2 s of the absence seizure [cit] . each dataset has 110 samples and each sample size is 19 â 512. fig. 2 shows the example of 19-channel eeg recordings of these three seizure phases. from the figure, seizure-free and seizure phases are quite obvious and easier to classify. however, the brain shifts from seizure-free to an absence seizure (pre-seizure phase) looks almost the same and is hard to distinguish. the generalized spike-wave discharges with a repetition rate of 3 hz are typically associated with clinical absence seizures. more details about the data collection can be located in refs. [19e21]."
"from the above discussion, we can conclude that the advantage of using machine learning approaches over the traditional classification methods is that the classification system does not need to know much about the input data in the beginning but the characteristics of the input data are learnt by the learning algorithm. once the classification system has been built, replacing a learning algorithm with any other would not require any changes in the system. considering this advantageous factor, classifiers such as ann and svm have been widely used in designing the epilepsy detection systems [cit] . also, features that are highly discriminative not only yields better recognition accuracy but also speeds up the process. recently [cit], has shown a very good recognition accuracy of seizure phases by extracting features in the time domain as well as in the frequency domain using ft."
"the recognition accuracy of all the methods are presented in tables 2e7, where svm-ova represents svm classifier with one vs all strategy, svm-ovo is svm classifier with one vs one method and svm-bdt shows the results obtained using svm classifier with binary decision tree method. linear, polynomial and radial basis kernel functions show the results against each individual kernel method discussed in section 2.3.1. nn is the neural network classifier consisting of four hidden layer architecture, nbc is naive bayes classification method, which uses two different density estimation methods, normal (nbn) and kernel smoothing function (nbk). k-nn is shown with different values of k (i.e., 1, 3, 5 and 7). and finally, k-mc shows the k-means clustering results."
"c-binary svm classifiers are needed for c-class problem when using ova method. therefore, 3 binary classifiers for the problem in hand. three-class problem is again divided into two-class classification problem and the training procedure is shown in fig. 7, where class 1 is seizure-free, class 2 is preseizure and class 3 is seizure phase. a new sample x ðãþ is passed through all three svms and the svm having highest value will decide the final class label."
"where j is the analysing wavelet, a is the scalar parameter, and b is the position parameter. the cwt is converted into discrete wavelet by using binary scale factors. hence, from (1) [cit] :"
"to achieve the maximum recognition accuracy of the three seizure phases, we have tried different number of hidden layers, hidden nodes and various combination of activation functions in this study. the best recognition accuracy was achieved by using 4 hidden-layer ffnn. the first hidden layer has 22 neurons and uses radial basis activation function. the second hidden layer has an array of 8 neurons and each node uses tangent sigmoid activation function. the third and fourth hidden layers contain 10 and 4 neurons respectively, and both the class labelling scheme has been shown in table 1 . the training parameters and procedure for this 4 hidden-layer ffnn architecture have been provided in the results section."
"intelligent techniques take feature vector as an input and use a learning strategy these algorithms learn the patterns in the features vector. after the learning (training) process has finished any new data sample is classified based on the trained classifier. this phase is called classification phase. according to the learning procedure, these techniques can be divided into supervised and unsupervised learning. in supervised learning, the algorithm takes example inputs (training dataset) and their corresponding output (class labels). it then learns a general rule of mapping inputs to outputs. svm and ann follow this strategy. in an unsupervised learning mechanism, no output is attached with the input examples. the data is grouped and clustered based on their natural or similar characteristics. k-mc is an unsupervised learning algorithm. in this research, we have explored svm, ann and k-mc."
"since, it is hard to completely isolate the sound coming from the vibration generator, the results of sv and svnt tests need to be compared to ensure that the leaked sound is significantly low. from the statistical comparison between sv and so, valid frequencies are determined to be 200hz, 230 hz, 300hz and 390 hz. thus, hypothesis testing of sv versus svnt was performed for the valid frequencies and for the remaining frequencies separately. for valid frequencies, as is seen from figure 5, the percentage of positive responses is relatively higher in the sv test as compared to svnt test. therefore, it can be safely concluded that the trend observed in figure 3 and figure 4 is potentially a result of audiotactile excitation. hypothesis testing results also show the validity of tactile sound perception with pvalue of 0.0286. for the remaining 4 frequencies, hypothesis testing result indicates that sv and svnt results are not significantly different with p -value of 0.6571. figure 6 further establishes that the trend shown in figure 3 and figure 4 is not a result of unwanted sound detection (as a response to sound coming from the vibration generator and reaching the user through the headphones). further it is noted that the sample acceleration measurements did not detect transfer of the generated vibration through conduction since throughout the experiments the generated frequencies were not present in the measured signals."
"at the beginning of the experiment, the participant seated in a relaxed position with the headphones on and the noise and ambient sound cancellation activated. the participant had the headphones on, throughout the duration of the experiment. the experiment consisted out of three stages: 1. vibration intensity calibration. 2. audio-tactile sensitivity test. 3. control measurements. the third stage was performed only by two participants mainly for testing the sound shielding performance provided by the headphones. all three stages were performed 13 times, one for each of the frequencies shown in table 2 . furthermore, during each experiment, sample sound and vibration frequency measurements were performed to ensure that the correct signals are delivered to the vibration generator. also, sample acceleration measurements were taken on the user's forearm to ensure that vibrations did not transfer to the ears by conduction through the body."
"classification systems based on bayesian statistical theory have been widely and successfully used commercially [cit] . this method gives a way to represent sensory evidence; features extracted from the raw data, and prior information about the problem in hand that is collected from domain knowledge. considering the equal prior probabilities for all classes and hence, ignoring the hassle of obtaining the domain knowledge, the analysis becomes very straight forward. although, it is a powerful and simple rule to handle and implement, yet estimating posterior probabilities from the data is a non-trivial task [cit] and the distribution of data may not be uniform. k-nn rule as the name implies, classify an observation by giving it a label after probing the labels on all the k-nns and making decision based on majority voting. usually, euclidean distance is used to measure the distances between the neighbouring instances. this algorithm mostly provides an acceptable performance in many applications [cit] such as visual category recognition [cit] and is also very easy to implement. however, k-nn algorithm suffers due to large memory requirements and also there is no logical way to choose the best k value, which would affect the classification problem and may not yield very good results."
"the fact that the real-life data will not be smooth and it will contain noise due to certain factors mentioned above. tables 3e7 summarizes the results of testing data classification when original testing dataset is contaminated with different levels of noise. addition of noise is a random process. thus to collect the statistical information, each experiment was repeated 10-times on the test dataset only. these experiments were performed without training the classifier on the noisy dataset. \"worst\", \"average\", \"best\" and \"std\" (standard deviation) show overall values for all the three seizure phases. the worst individual is the lowest individual accuracy obtained after these experiments."
"eight young adults participated in the experiments. their age ranged between 19 and 21 years (mean 19.9, standard deviation 0.60). one of the persons participated in a similar experiment before, but he had no information regarding the primary aim of the investigation or details of the study. all other participants did not have any knowledge about the topic of the study and were not involved in vibrotactile experiments before. all of them signed an informed consent and were compensated for participation."
"the recognition accuracy (classification performance) of all the methods are evaluated. firstly, the experiments were performed using the original dataset, without any noise. in the eeg process the noise however is inevitable due to certain uncertainties in the measurement process, such as, signal conversion, filtering, amplifying and environment conditions. to reflect these factors present in the real-life patient's clinical data, how the underlying seizure recognition system would behave in the presence of noise and also to investigate the robustness property of each algorithm, the data has been contaminated with different levels (0.05, 0.1, 0.2, 0.5 and 1.0) of random noise."
"the first stage of the procedure for each frequency involves calibration of the vibration intensity. the purpose is to achieve minimization of the audible sound generated by the vibration generator, so that only controlled sounds through the headphones are delivered to the user. at this stage the user is not touching the probe. vibration signals are generated and the user is asked to tap whenever he listens to a tone. the vibration intensity is reduced after each signal until the user does not respond to the tone. the resulting sound intensity is used for the subsequent stage of the study."
"in this paper, we first present a different strategy for extracting features from the eeg signals. in the time domain local features, based on windowing filtered mechanism, and global features, based on the entire signal length, are calculated. the signal is then decomposed into different levels using discrete wavelet transform (dwt) and features at each level are calculated. in the next phase, we compare different machine learning algorithms in terms of their recognition accuracy based on the extracted features. traditional classification methods provide an acceptable performance in many applications and can be used as a benchmark for other techniques [cit] . comparisons have also been performed between both types of classification methods, traditional and intelligent algorithms. the robustness property has also been tested using noise-contaminated data."
"the ffnn architecture discussed in section 2.3.2 was trained with training samples along with their corresponding class labels for 30 epochs and with initial learning rate of 10 à4 using levenberg-marquardt backpropagation algorithm. mean squared error criteria was used to calculate the error between the outputs produced by the network and the actual class labels. for each successful step, the learning rate was decreased by 10 à1 and for every failure, it was incremented by 5. the network takes 40 dimensional feature vector as an input, and produces the output of size 3. ð1; 0; 0þ represents an observation from class seizure-free, ð0; 1; 0þ corresponds to a sample from pre-seizure phase and ð0; 0; 1þ classifies a seizure phase."
"the svm method maps the non-linear and inseparable data from an input space into a higher dimensional feature space where the data would then be linearly separable [cit] . this task is accomplished by utilising the concept of separating hyper planes [cit] . instead of computing a mapping function, the use of kernel function saves the computational demand especially for feature mapping function of higher dimensional space. the svm algorithm aims to maximize the margin (the region separating the support vectors on either side of the hyperplane) and tries to find an optimal hyperplane. hence, also called the maximal margin classifier. although, the training parameters for svm technique are very few, it can still be a computationally time consuming and highly complex. nonetheless, svm has a good generalization ability, solution to the overfitting problem and also performs well in a high dimensional feature space."
"from section 2.1, each dataset (seizure-free, pre-seizure and seizure) has 110 samples. to design classifiers using extracted features from the eeg signals as discussed in after the training process, the classifier is tested using the testing dataset to evaluate its recognition accuracy for any new pattern (sample). the percentage of recognition accuracy for both training and testing datasets are calculated separately based on correctly classified samples within each individual dataset."
"we have used daubechies d4 wavelets, which is most widely used dwt for eeg signal analysis. d4 uses four scaling and wavelet coefficients [cit] . the scaling coefficients (lpf) are:"
"bdt method requires c à 1 classifiers for c-classes. converting a three-class classification problem into a twoclass classification problem, svm1 is trained on samples from seizure phase (class 3) with labels, à1, and samples from seizure-free and pre-seizure phases (classes 1&2) combined together as one class with labels, þ1. svm2 is then trained on samples from seizure-free phase (class 1) with labels, þ1, and pre-seizure phase (class 2) samples with labels, à1. a test sample x ðãþ is processed through both the svms and to obtain the category of this new sample the results of both svms are then combined as shown in fig. 5 . the classification accuracy depends on the svms in the upper levels."
"in table 2, the recognition accuracy (%) of both training and testing datasets is shown where \"average\" represents the average accuracy for all three classes (seizure phases). it can be seen that the highest average testing accuracy is 93.9394% which is achieved by svm-ovo using polynomial kernel function and the corresponding average training accuracy is 99.6970%. whereas, 1-nn has obtained the best average training recognition accuracy of 100%. on the other hand, the lowest average testing accuracy is 66.6667% which is achieved by k-mc. talking about successfully predicting the preseizure phase, svm-ovo with polynomial kernel function is again leading with 90.9091% testing recognition accuracy. also, traditional classification methods are not able to predict the pre-seizure phase with good recognition accuracy. nbc with kernel function has improved the average testing accuracy from 69.1358% to 72.8395%. however, nbc provides a poor performance in predicting pre-seizure phase."
"k-mc is an unsupervized learning technique that clusters samples based on similar properties. whereas, k represents the number of clusters or classes. within a cluster the samples are similar but different from the samples grouped in other clusters. this is an iterative process in which samples are grouped together according to their closet mean and cluster positions are adjusted until these positions do not change for some iterations. the advantage of this technique is that it does not require labels with the input examples. the convergence is also faster if k is small. however, if clusters are of different sizes, densities and not globular then the results are poor. the results may also vary depending on the initial location of the cluster centres."
"in figure 2, it can be seen that the median percentage of positive responses is roughly constant in so test among all frequencies. this is reasonable, since, sound intensities were specifically chosen to have half audible and half inaudible sounds. figure 3 demonstrates that the median percentage of positive responses is relatively low in sv test at frequencies of 20-170 hz. with further increase of frequency, the percentage of sound detection increases, reaching its peak at 300 hz. there is a sharp decrease in sound detection performance of users for frequencies higher than 300 hz. as it was reported by one of the participants, the vibration was less sensible at 500 hz, as it was naturally expected. the calibration stage might have contributed to this fact since the gradual reduction of the vibration intensity (for sound isolation purposes) might have led to undetectable amplitudes in certain frequencies in which hearing is more sensitive. this is believed to be the case in some of the instances of 500 hz generation."
"this paper has presented different supervised (svm and nn) and unsupervised (k-mc) learning algorithms for classifying epilepsy seizure phases. computationally intelligent techniques nn and svm have proved to be very good in recognising and classifying the complex and complicated patterns in the input data (eeg signals). the performance of these algorithms has been compared with two traditional classification methods, nbc and k-nn. it can be concluded that nn and svm have demonstrated the best recognition accuracy compared with traditional classification methods and an unsupervised learning algorithm, k-mc. furthermore, nn and svm both showed robustness property by maintaining best results even when the input data is contaminated with different noise levels. in addition, the feature extraction method is also introduced in the paper, which is able to gain rich information of the signal at a moderate dimension of the feature vector. also, the nn method outperforms the svm methods for some of the noisy dataset cases. in future, more classification methods based on neural networks such as selforganising maps and deep learning architecture will be explored and try to find the method, which is best at both noisy case and noise free case."
"overall, the work is organized as follows: first a methods section describes the group of participants, equipment and experimental procedure. this is followed by the results sections and finally a discussion and conclusion sections elaborate on the results and investigation in general."
"it is worth mentioning that after the training process, all the parameters and weights of the classifiers are fixed values and can be used in the real-life applications. the computational cost for the classifiers to give out the results are all very low and can be ignored in most of the cases."
"approximation (a1), consists of low frequency components, and detail (d1), consists of high frequency components, are the coefficients produced by lpf and hpf respectively. for the next level of decomposition, a1 is further decomposed and the whole process is repeated until the desired level of decomposition is achieved. for one dimensional signal xðtþ the continuous wavelet transform (cwt) is defined by morlet-grossman [cit] :"
"in total, 25 tones were delivered to the user for each of the test frequencies. 10 sound tones, 10 sound and vibration tones (sound through the headphones and vibration at the fingertip) and 5 vibration only tones. all 25 tones were generated in a random order. the amplitude of vibratory stimulation remained the same in all 15 stimuli (5 vibration and 10 sound and vibration). auditory stimuli had 10 different intensities and they contained both normally audible and nonaudible tones which were calibrated based on experiments with two young adults for each frequency. as in the calibration stage, whenever the user heard the tone, he tapped on the workbench. the number of positive responses (taps) for each user in each frequency are counted, then the median as well as boxplots for all users in each frequency are calculated. this is done for positive responses in sound only (so) and sound plus vibration (sv) signals. also, a further criterion is considered for testing the audio-tactile integration: if the user cannot hear a specific sound intensity played on its own (so), but can hear it when it is combined with a vibration (sv), then this is a valid case where it is shown that vibration enhances hearing. all such cases are counted and statistically analysed. this group of results is termed svs as it is a comparison between sound and vibration versus sound only. vibration only (vo) signals were generated for randomization purposes of sv and so signals. providing a third option (vo) alongside the signals that are under investigation (sv and so) reduces the possibility that the user will become biased towards either so or sv signals. only five vo signals are provided since firstly this option does not presently involve any investigation and secondly due to duration limitations. they are not used in the analysis for the test group. they are only considered in the analysis when it comes to the control test."
"where the subscript j indexes units in the input layer and p indexes units in the hidden layer. w pj denotes the input-tohidden layer weights at the hidden unit p. the subscript k indexes in the output layer and n h denotes the number of hidden units. d represents the dimension of the ith input sample. the advantage of using ffnn compared with svm is that for c-classes (outputs), the network can learn c discriminant functions z k and a training input x ðiþ is assigned to the function z ðiþ k that has the maximum value from all other functions. usually, these functions are learnt through a wellknown backpropagation algorithm. the goal of this algorithm is to find a set of weight values for all the connections, such that it can minimize the error between the actual and desired output. a new sample x ðãþ is classified using the trained ffnn."
"eeg signal has been decomposed into 5-levels and at each level energy, range and standard deviation has been calculated. in addition to these features, waveform length (wl), average amplitude change (aac) and difference absolute standard deviation (dasd) [cit] have also been computed at each level of decomposition to form the rest of the feature vector. combining all the features from time domain as well as from the decomposed signal resulted in a large feature vector, which is considered to be computationally expensive in the classification process and can also generate poor recognition accuracy. fig. 4 depicts the whole process of feature extraction discussed above. as from section 2.1, the individual electrode provides 512 brain readings (values) and only 10 channels are selected to build the feature vector. hence, form each electrode, 192 local values using a window size of 16 and 6 global feature values are extracted. the values of individual electrode is then decomposed to 5-levels. d j /d l represent the detail values and a l represents approximate values as shown in"
"to evaluate the success of our feature extraction scheme and to equate the recognition accuracy between learning algorithms and traditional classification methods, we have also selected to perform experiments with naive bayes and k-nn classification methods. to estimate the data distribution for naive bayes classifier, we have employed two ways to estimate the input data distribution. the one uses the normal gaussian function (naive bayes with gaussian function nbn) and the other uses the kernel smoothing function (naive bayes with kernel smoothing function nbk). k-nn has been tested for four different values of k (i.e., 1, 3, 5 and 7). class labels 1, 2, and 3 have been used for seizure-free, pre-seizure and seizure samples respectively, for both the traditional classification methods."
"the vibrotactile testing apparatus consists of the following equipment: 1. pc. 2. external sound card. 3. a pair of headphones with active ambient noise and sound cancellation (sony wh-1000xm2). these include automatic performance optimization given current environmental conditions. 4. a vibration generator with a vibrating probe (frederiksen 2185.00). 5. amplifier (l-frank audio paa30usb). 6. custom-made sound insulation box. the vibration generator was placed inside the insulation box with only the vibrating probe protruding, so that sound generated due to mechanical parts movement is isolated to the maximum possible extent. a cylindrical 4mm wooden interface with flat end is inserted in the centertap as the probe endpoint (which the user touches), [cit] . the complete experimental setup is presented in figure 1 . mpu6050 accelerometer and arduino software are used to take sample acceleration measurements on the forearm of the participant to make sure vibration is not transferred through the body by conduction."
"the results further support existing research regarding perception of vibrotactile stimuli as sounds. the hypothesis that there is a frequency range in which the phenomenon under study is most intense, is validated, with the optimal audio-tactile integration frequency range being at 200-390 hz. given the results presented in this work, further tests that accurately address hardware issues, including response curve of hardware to frequency, sound isolation, absolute values of vibration and sound intensities and other issues should be designed so that a more precise understanding of the audio-tactile integration is achieved."
"in the second part of the procedure, the participant touched the probe with the index fingertip. the participant was asked not to exert intense pressure on the probe, rather just rest the centre of the fingertip on the probe end. a pillow was placed under the participant's forearm to keep the wrist and arm relaxed. three types of sinusoidal signals were generated at this stage. 1. sound only (so). 2. sound and vibration (sv). 3. vibration only (vo). frequency steps were chosen by considering jnd suggested by literature as described in the introduction."
"absence seizure is one from many forms of generalized epileptic seizures in which larger part of the brain is disturbed. these seizures are very short and sometime may go un-notice. the patient seems confused and may not remember the seizure afterwards. the complex spike-and-wave patterns generated by the brain during these seizures can be recorded on the electroencephalogram (eeg) and a neurologist can identify the three absence seizure phases namely seizure-free, preseizure and seizure [cit] . to automate this process, eeg data is converted into a digital format and fed into a computerized seizure detection (classification) system, which can automatically recognize the input pattern. the two core modules of this classification system are: feature extraction and design of a classifier using these features. a feature extraction method extracts the most discriminative information from the eeg recordings, which means an ideal feature can have the property of differentiating among three phases of absence seizure."
"cðcà1þ 2 binary svm classifiers are required for c-classes using ovo method. thus, for three-class problem, it requires 3 binary svms. svm1 is trained on samples from class 1 (seizure-free) and class 2 (pre-seizure), svm2 on samples from class 2 and class 3 (seizure), and svm3 on samples from class 3 and class 1. for a new observation (sample) x ðãþ, it chooses final class label with majority votes. this whole process is summarized in fig. 6 ."
"besides that, table 2 shows that the biggest effect on sound detection performance is at 300 hz with negligible p-value of 0.0001, and additionally, sound detection is also high at 200 hz, 230 hz, 390 hz with p-values of 0.0144, 0.0095 and 0.0047, respectively. figure 4 shows a similar trend to sv results in figure 3 . this graph confirms previous claims and shows that there is almost no audio-tactile exc~\\\\itation at 20-170 hz. starting from 200 hz, the percentage of tactile sound perception increases. as it was already mentioned above, audio-tactile feedback is highest at 200-390 hz having a peak at 300 hz. this roughly agrees with [cit], where the respective frequency was 250 hertz. furthermore, 300 hz coincides to the frequency at which maximum tactile sensitivity with respect to amplitude of excitation is located [cit] ) ."
"in literature, there is a long list of methods that can be used to extract features from the eeg signals. these methods include fourier transforms (ft); good for analysing stationary signals, and time frequency distribution (tfd); provides a tool for examining continuous segments of eeg signals. however, eeg signals are non-stationary in nature and conventional frequency analysis methods may not capture the full details of the brain signals. lyapunov exponents; discussed the detection and prediction of epileptic seizure in refs. [cit], analysis of correlation structure [cit], and high order spectral analysis (hos) [cit] are the examples of non-linear methods for eeg signal analysis in the domain of epilepsy. advances in wavelet theory has also made it a very suitable for bio-medical signal processing. it has a built-in advantage of capturing repeated and irregular patterns. it can also deal with the analysis of transient and sudden signal changes [cit] . this is possible because this technique provides variable window size, narrow at high and wide at low frequency levels. furthermore [cit], has discussed several different methods for eeg signal analysis and concluded that wavelet transform (wt) has more advantages over other methods. the next step in designing the classification system is to combine these extracted features with an appropriate learning method to design a classifier. these methods can be divided into traditional classification methods and modern learning algorithms also known as computationally intelligent algorithms or machine learning algorithms. bayesian methods based on statistical theory, k-nearest neighbour (k-nn) and decision trees based on logical branching, are considered to be in the category of traditional classifiers. on the other hand, support vector machine (svm), artificial neural network (ann), k-means clustering (k-mc) and self-organising maps (soms) exhibit intelligent behaviour by learning the complex and nonlinear patterns hidden inside the input feature vector, are considered to be computationally intelligent approaches. below is the brief introduction about all the methods used in this research."
"epilepsy is a neurological condition such that it affects brain and the nervous system. it is a very commonly known neurological disorder and approximately 1% of general population is affected [cit] . only in the uk, around 1 in 100, more than half a million people suffer from epilepsy. there can be many causes of epilepsy and sometimes it is not possible to identify them. in the domain of epilepsy, seizure is referred to as an epileptic seizure and brain is the source. during an epileptic seizure normal functioning of the brain is disturbed for that certain time period, causing disruption on signalling mechanism between brain and other parts of the body. these seizures can put epilepsy patients at higher risk for injuries including fractures, falls, burns and submersion injuries, which are very common in children [cit] . these injuries happen because seizure can happen anytime and anywhere without prior warning and the sufferer would continue his or her activity with an unconscious mind. if a system can effectively predict the pre-seizure phase (the transition time of the brain towards developing seizure), it could then generate an early warning alarm so that precautions can be taken by the sufferer."
"dwt decomposes the signal into several levels and each level represents a particular coarseness of the signal. at each level, the signal is passed through the high pass filter (hpf), which acts as the mother wavelet, and the low pass filter (lpf) that acts as a mirror version of the corresponding hpf. the output of each level is the downsized signal by a factor of 2."
"generally, the recognition accuracy is decreasing when the noise level is increasing. the recognition accuracy of svm with polynomial kernel function has badly degraded in all the multiclass svm methods. however, svm with linear kernel function has shown good robustness property in the presence of different noise levels. among all the classification methods, the nn classifier has shown the maximum recognition accuracy of 85.8943% at highest noise level of 1. from the discussion and the results presented, it is fair to conclude that nn has a good generalization ability which is not very much degraded under the increasing levels of noise."
"this criterion defines clusters as their mean vectors m k in the sense that it minimizes the sum of the squared lengths of the error, x ðiþ à m k . where c represents the total number of clusters (classes), x ðiþ is the ith sample, and d k is the kth cluster. hence, a partition is optimal if it minimizes j e and also called minimum variance partition. in our case, the data was clustered into three groups (seizure-free, pre-seizure and seizure phases) and a new sample was assigned to the cluster that has minimum distance from these three clusters."
"in contrast to sound tests, in vibrotactile tests it is nearly impossible to completely isolate the user acoustically from the vibration source. it is expected that despite isolating the vibration generator in a box and using specialized sound-cancelling headphones, still some sounds coming from the vibration generator will reach the participant. to get an idea for this unwanted sound detection it was requested from two of the participants to perform the whole experiment again, but in this case, they were not touching the vibration probe. they assumed the same posture and had the headphones on as before. they were also asked to tap whenever they heard a tone. the results of these controlled tests were compared to the results of the tests that included touch and are shown in the results section. in this case the results are described with the letters vont (vibration only, no touch) and svnt (sound and vibration, no touch). figure 2 represents the percentage of positive responses in sound only (so) test. figure 3 demonstrates similar data for sound and vibration (sv) test. table 3 shows the results of hypothesis testing between sound only and sound and vibration tests. figure 4 illustrates boxplots for all 13 frequencies, comparing sound and vibration versus sound only positive responses. the graph indicates the percentage of valid cases. the blue boxes contain 50% of the cases and the red lines the medians. the red crosses represent the outliers. black dotted lines include the rest of the results."
"the current project takes a first step toward developing new objective measures for capturing and visualizing the extent to which a child seeks or avoids social interaction. we take as our starting point interactions between two individuals. after consulting with behavior analysts and therapists at a local autism treatment center, we built a video visualization system that supports the analysis of social approach and social avoidance through interactive graphs of mutual distance and orientation between the two individuals."
"from the subject's individual orientations, we define and compute a normalized measure of mutual orientation. we define mutual orientation to range between 0 and 1, where 0 is facing each other and 1 is facing away from each other. everything in between is a linear mapping across the two extremes. note that this definition is a many-to-one mapping. for example, two people facing north will add to 0.5, facing south will add to 0.5, and one facing north and one facing south will add to 0.5. again, our goal is to determine if a simple and approximate metric of social orientation will suffice for effective behavior analysis. figure 3 provides some examples of our simplified definition of mutual orientation."
"it is important to note here that we define an adult-centric coordinate system because we are interested in the child's behavior, the dependent variable that we can't control. if we place the child as the center of the reference system, the visualization becomes unstable and hard to read. also, it is common for behavioral interventions to control the behavior of the therapists, which in our case would be the adult in the room. by filtering on controlled and discrete behaviors, we expect to be able to compare the differing results in the child's behavior. in the adult-centric polar coordinate system, we placed the child at radial distance d from the center. the angular position  of the child is where the child is with respect to the adult. in other words, we simply map  1 to , keeping 0° pointing north (90° in polar coordinates). recall that  1 is the angle between the orientation of the adult and the line connecting the adult and the child. next, we discretized the polar coordinate space into bins. each time the child's location falls on a particular bin, the system increases the bin's counter by one. thus, the bin count over a specific period reflects the frequency with which the child was in that particular location."
"one adult and one child participated in a one-hour recording session at csl. we provided the participants with a set of play materials (painting set, train set, and blocks) and told them to play and engage as they wished. we classified a large number of captured activities, including table-top interaction, floor-play, and larger movements around the room. to manually pinpoint location and orientation, we selected a representative segment of video lasting 15 minutes and we manually coded 450 frames at a frequency of one frame every two seconds."
"because we wish to visualize distance and orientation on the same graph, we normalize the two measures to be on the same unit-less scale. to normalize distance, we linearly map the diagonal of the image (an approximation to the room's diagonal) to 1.0 and two adjacent pixels to 0.0. thus, the furthest two people can be apart is 1.0 and the closest is 0.0. again, this measure is a simple approximation where we do not consider the complexities of wide-angle perspective and optical distortion."
"the goal of vizkid is to facilitate the observation and analysis of the flow of the interaction between two individuals. specifically, the system's success will depend on the extent to which it helps behavior analysts understand reciprocal interactions between the child under observation and the person interacting with the child. we implemented the backend of vizkid in matlab and the frontend in processing, a javabased open source programming language geared towards interactive visualizations. the next sections describe the three phases of the system: data collection, data annotation and aggregation, and data visualization."
"we developed vizkid in processing, a high-end interactive graphics java library. vizkid is an information visualization system that supports the analysis of social orienting (distance and mutual orientation) between two people interacting in the observation space. figure 5 shows the three components of vizkid: the video panel in the upper left corner, the timeline panel on the bottom, and the aggregate panel in the upper right corner."
"the development of vizkid was motivated by the increased prevalence of autism spectrum disorders (asds) in the united states, and the concomitant need for objective measures of social behavior to help diagnose the condition and to track children's development [cit] . in particular, measures of the extent to which children with autism seek or avoid social interactions figure prominently in evaluating treatment outcomes for this population [cit] . current methods for measuring such behaviors typically involve parent or teacher-report questionnaires [cit], or timesampled direct observations of specific behaviors [cit] . the behaviors of particular interest are typically the number of approaches the child makes to their interactive partner, the child's responsiveness to the interactive partner's social bids, and the amount of time the child spends in proximity to the partner versus alone in solitary play [cit] . whereas parent and teacher reports of such behavior are inherently subjective and may be unduly influenced by external factors, direct observations are often too labor and time intensive to scale up."
"we built a simple matlab application to click on the center of the shoulders and on a vector heading denoting the orientation of the each individual. this resulted in four clicks per frame or 1800 clicks for the 450-frame sequence at one frame every two seconds. this wizard of oz solution replaces a computer vision system that would track blob location and orientation. in the future, we will automate this extraction process by placing colored shoulder pads or similar fiducial markers on the individuals' shoulders and by using robust computer vision techniques to accurately compute location and orientation. figure 2 shows the world coordinate system of two individuals, the adult and the child. the distance d of the adult and the child is measured from the center of the adult's shoulders to the center of the child's shoulders in pixels. orientation values  1 and  2 are obtained by calculating the angles between the line connecting the two individuals and their individual orientations, as defined above. note that we are not marking the orientation of the head, which would require a fiducial maker on it. in our figure, the orientation of the head is denoted by the small black triangle. rather, we are marking the orientation of the vector perpendicular to the line connecting the shoulders, where we will place the markers. we considered it would be more robust and less invasive to compute the orientation of the chest as an approximation to social orientation. in future work, we may place fiducial markers on the head as well, especially if our preliminary experiments determine the necessity for them. from the subjects' locations, we compute the euclidian distance between them in image space using the pythagorean theorem and the angle of the line connecting the two points. we do not calibrate the cameras or reconstruct physical world coordinates. thus, distance is not in meters or feet, but in pixels. because of wide-angle perspective projection from a 3d world to a 2d image space and because of wideangle lens optical distortions, the mapping between pixel distances and physical distance in a one-camera system is a computationally under-constrained problem. furthermore, a heuristic approximation to physical distance is complex and requires some understanding of the scene, such as people's height. again, this metric simply approximates the common idea of social distance. part of the purpose of the current work is to determine the level of accuracy necessary to provide useful support to behavior analysis. if we determine that pixel distance is not enough, we will reconstruct physical distance with more complex vision algorithms."
"on the technical end, we will incorporate computer vision techniques to automatically extract the spatiotemporal data reflecting the relative orientations and positions of the individuals being observed. one proposal for doing so is to attach different colored patches on both of the adult's and the child's shoulders and to use color detection techniques to automatically detect the position of each shoulder. by doing so, we will be able to calibrate the positions of the shoulders and, consequently, the positions and orientations of the adult and the child. based on the psychological and behavioral literature on measuring social behavior in autism, the future functionality of the system includes: 1) additional capabilities that quantify the aggregated data; 2) specific measures of who initiates social contact; and 3) the ability to track the child's social approach and avoidance behavior to multiple individuals at the same time. we expect this functionality to approach the affordances necessary for vizkid to collect and analyze data in real environments, such as in a daycare or in a school setting."
"note that the adult-centric polar coordinate system does not account for the orientation of the child. in our current implementation, we are ignoring that information. through a user study, we plan to determine whether that information is necessary. if it is, we plan to compute a vector sum of all the orientations at a particular location and visualize a vector field of the sums in the adult-centric polar coordinate system at an interactive request from the user."
"the video panel (figure 5a ) shows the raw video frames and the vector of the child's and the adult's location and orientations. this panel allows the user to view the actual footage corresponding to the distance and mutual orientation data at a specific point in time. it provides a reification tool to understand the concrete details abstracted by our proxy visualizations of distance and orientation. users can see specific objects, places, gestures, and actions. the timeline panel (see figure 5c ) contains playback control buttons that allow the user to play, pause, rewind, and fastforward the video while brushing both the timeline view and the aggregate view at the correct points in time. users can observe the interaction flow between the child and the adult in the video and relate it to the visualizations."
"we collected the data for designing vizkid at georgia tech's child study laboratory (csl), an experimental environment designed to mirror a typical playroom while facilitating the collection of high-quality video and audio data for behavioral experiments. csl consists of two components. the first is an assessment room measuring 14 by 21 feet where data collection takes place. the assessment room is equipped with child-friendly furniture and toys (see figure 1a) . the second component of csl is an observation and control room from which we can monitor the activity in the assessment room and manage the data collection. a human operator controls the cameras to optimize the data collection based on position, orientation, and observed behaviors (see figure 1b) . the assessment room is equipped with 11 cameras, eight around the perimeter of the room and three overhead cameras that fully cover the floor plan (see figure 1c) . for developing vizkid, we collected video from the overhead camera positioned directly in the middle of the ceiling. the overhead cameras are axis 209 mfd recording motion jpeg at a resolution of 640 by 480 pixels (vga) and at 30 frames per second. we replaced the standard lens with a shorter 2.8 mm lens with aperture f2.6 and an angle of view of 106°."
"the aggregate panel displays the polar coordinate information for the child's distance and relative orientation from the adult, described above in section 3.2, using a heat map (see figure 5b) . the heat map represents the child's spatiotemporal location relative to the adult over some pre-specified period of interest to the analyst. this version of the heat map is in gray-scale, with white indicating that the child rarely appeared in that particular bin position, and darker shades of gray, indicating increased frequency at a particular position. because the graph is adult-centric, the location of the heat map clearly conveys where in respect to the adult the child spent their time. in other words, if the graph shows a dark region to the left of the center of the circle and close to its edge, the child spent most of the time far away from the adult and tended to stay to the left of the adult. the blue dot denotes the position being brushed in the time line (approximately frame 170 in the x axis). a double-sided arrow slide bar at the bottom of the timeline allows users to specify the window of time over which they wish to aggregate position and orientation data. it is a tool for dynamic queries. this aspect of the visualization goes beyond a single moment in time to allow the user to define and observe at a glance how the child interacted with the adult over some specific period, such as a particular condition within an experiment or even over the course of the entire experiment. figure 5c shows the timeline panel that graphs normalized position and orientation on the vertical axis and time on the horizontal axis. the yellow line shows the normalized distance and the green area is formed by adding and subtracting the normalized mutual orientation from the normalized distance. this common information visualization technique is called theme rivers and it is meant to make visible the patterns in a multivariate time series. moment by moment, the instantaneous mutual orientation is both added and subtracted from the instantaneous distance. thus, the possible range of values goes from -1 to 2. in other words, the smallest possible value for distance is 0 and the largest possible value for mutual orientation is 1. if you subtract this value of orientation from distance, you get -1. on the other hand, if you add the largest possible value for orientation, 1, to the largest possible value for distance, 1, you get 2. so, the combined normalized scale is [-1:2] . to interpret the visualization the user needs to keep track of the center and the width of the green area: the wider the area, the less oriented towards each other the individuals; the higher the center, the more distant the individuals. it is important to note that a single (x, y) coordinate in this graph is an ambiguous representation due to the fact that multiple distances and orientations may add up to the same value. we disambiguate the graph by including both metrics in yellow and green."
"we developed vizkid, a capture and visualization system with the aim of facilitating more fine-grained examination of children's social approach and avoidance behaviors over the course of an extended interaction. the main contribution of vizkid is the user interface, particularly the integration of the visualization of the interactions between a child and an adult with original video frames, and a means for aggregating and visualizing the distance and orientation data over various time scales. our next step is to deploy this system with our collaborators at a local treatment center for children with autism, and via a series of case studies, examine how they apply the system to analyze practical problems, and refine the system based on their feedback."
"the distance and mutual orientation data obtained via the process detailed above results in two time series. to gain a historical overview we aggregate the data. to visualize the aggregate, we map distance and mutual orientation to polar coordinates (see figure 4) . we placed the position of the adult at the center of the polar coordinate system, and we fixed the orientation of the adult to always point north."
"we worked towards solving some of the practical clustering concerns outlined above. for this we tried a new metaclassification approach. we first split the variables into four classes: actions, skills, exploration and social. then we applied different classification algorithms to each class. the underlying idea is to first classify players according to their skills. for example, player a could be a highly skilled shooter while player b is a new player who has widely explored the game but has not yet developed an efficient shooting strategy. then we classify the players based on their exploration variables. we use these variables as an example to extract players' exploration preferences, e.g. player a prefers to stay hidden in a small area of the game world waiting for enemies while player b prefers to explore the game world as much as possible. lastly, the social classification should give us an indication of how socially active a player is and consequently his/her predisposition to participate in social interactions."
initial observations showed that it is possible to identify clusters of players using unsupervised classification. the analysis from applying k-means clustering revealed that the previously identified large cluster corresponds to new players. this large cluster is the sum of a very large number of beginners combined with a large number of people who created an account but never accessed the game. we can also identify other clusters containing players with very clear identifiable skills. the identified clusters are: the combined analysis of these two techniques also allowed the identification of variables containing minimum variance and which consequently did not contribute to the differentiation between players.
"this type of clustering is characterised by adding information concerning social interactions among players to the previous clustering stages. figure 7 shows the social network of the 25 previously selected players. this network is formed by friendship relations between players. based on these friendship relations, we extract a matrix of geodesic distances to calculate how far is one player from another in terms of mutual friends. we insert this information as weights into the mds algorithm such that players who are already friends are brought closer together. figure 8 shows the end result of inserting the friendship-based weights into the emd/mds clustering algorithm. from figure 7, it can be observed that player 15 is a cutpoint that joins two subgraphs. figure 8 shows how points belonging to the same subgraph are pulled closer to each other. it is particularly clear for points belonging to the left hand side subgraph (players 7, 9, 12, 14, 16 and 18). points which in figure 6 where already close to each other and which belong to the same subgraph, are brought even closer (e.g. players 8, 13, 19 and 20) . points representing players in the beginners cluster remain in the periphery and do not form part of any of the newly formed clusters. this should be expected as one characteristic of new players is their lack of social connections."
"player classification consists of taking players' actions as an input and delivering these actions grouped into different playing styles. simple classification methods are normally designed for one of the many types of variables, e.g. continuous, discrete, mixed, sequential, sociometric, etc. instead, we propose a combination of clustering algorithms (a meta-classification approach) to take advantage of the different characteristics of different dataset variables."
1. initialise the partition by choosing k as the number of clusters and arbitrarily choose one cluster centre (mean) for each cluster 2. partition the data into k clusters by iteratively assigning points to the cluster with the nearest mean 3. recalculate the mean for each cluster 4. repeat 2 and 3 until there is no change
"as mentioned in section 2, hidden markov models have been extensively used as a tool for clustering action sequences. here we propose a new approach based on clustering of histograms extracted from a players' trails. figure 1 -right shows the histogram generated from the player trail illustrated by figure 1-left. clustering is performed by defining a metric which provides a similarity distance between histograms. the different distances between histograms can then be used to build a dissimilarity matrix which, together with multidimensional scaling provide a n-dimensional representation of the similarities between histograms."
"the accuracy of our clustering algorithm heavily relies on the selection of a suitable metric for measuring the distance between histograms. for this, we have employed the earth mover's distance (emd) as metric. emd is a well-known metric which is used to measure distance between two distributions. it is widely used in image processing algorithms as a way to measure similarity between two images. emd is based on the idea of finding the minimum amount of work that it takes to transform one distribution into another. examples of commonly used distributions are histograms, colour distributions and image intensities. work is measured by the cost c ij of moving one unit of distribution mass from one distribution i to another distribution j."
"in this paper, we focus on the problem of player classification where there is no previously defined classes. thus, we adopt an unsupervised player classification approach, i.e. player clustering. many clustering algorithms exist in the literature but many present practical challenges when applied to gameplay datasets. some of these challenges are:"
"before integrating our histogram based algorithm into our metaclassification approach, we analyse the results of applying the algorithm to a random sample of 20 players from our dataset. figure 2 shows the result of using mds on the distances obtained by applying the emd metric to the 20 random trails."
3. socially based clustering: this level applies a combination of social network analysis metrics such as shortest path length to social data to find clusters in the players' social network.
"we start from the assumption that two players could be motivated to create a social link between them if they perceive that they share some common ground. traditionally, in social networking websites, this common ground can be extracted from information explicitly expressed in the players' profiles. some examples of this common ground are mutual friends, common expressed interests and common affiliations. we use this information to weight the distances obtained from the emd analysis. more specifically, we use the geodesic distance between pairs of points built from the length of the shortest path between players in order to build a weight matrix. this weight matrix is incorporated into the mds analysis to redistribute the distances between points according to their social interactions."
"for dimensionality reduction, we use the widely known principal component analysis (pca) algorithm as the main dimensionality reduction technique. pca provides a simple way of extracting the principal component from a dataset. we also consider other more complex and non-linear reduction techniques. however, as will be seen in our experimental results, more complex algorithms did not considerably outperform the result from pca. thus, the result from pca was used in the succeeding clustering stages."
"we test our approach by analysing logged game data from the freely available game 'the hunter'. this is a free roaming, animal hunting game where the objective is to track, spot and harvest animals following the rules of ethical hunting."
this end result of applying the three layers of meta-clustering gives us enough information to design mechanisms which take advantage of the different levels of similarity between players. some examples of these mechanisms aimed at boosting social interaction among players are:
"we applied the six different dimensionality reduction techniques described in section 3.1.1 to the set of actions and skills variables. the results obtained from each technique are shown in figure 3 . it can be observed that in most techniques points are either concentrated in one particular area of the graph or spread around the figure. an in-in-depth analysis of the graphs revealed that most points concentrated around that particular area forming one very large and dense cluster. the rest of the players that spread around the graphs exhibit different characteristics. however, as explained and due to the practical reasons outlined in the introduction, it is not possible yet, in any of the six graphs, to identify clusters only through visual inspection."
"in this clustering stage, the classification of players' exploration variables was done by finding similarities between preferred hunting areas. preferred hunting areas are identified by calculating 2d-histograms of walking paths and finding the similarities between them. we implemented the algorithm described in section 3.2 which requires the definition of a cost function. this function measures the cost from moving one distribution mass unit from one histogram to another. we assume histograms of equal size. we define a cost function where moving mass units vertically or horizontally to adjacent bins has a unitary cost per bin travelled. the cost of moving one unit from equivalent bins is zero. a vertical plus a horizontal move compose diagonal moves between adjacent bins. thus, diagonal moves have a cost of two. from the clusters obtained from our previous pca/k-means clustering stage, we randomly selected 5 players from each cluster. for each of these 25 players, we extracted their set of trails, calculated their histograms and applied the emd/mds clustering algorithm. the result of this algorithm is shown in figure 6 . from this figure it can be observed that points within the same pca/k-means cluster also seem to fall within the same concentric circle around the zero point of origin. this is particularly clear for clusters containing all rounders, photographers/explorers and shooters. it is not as clear for the beginners class, but this can be easily explained by the fact that some beginners might walk the same areas as those preferred by the shooters. however, they not necessarily share the same shooting skills. an exception to this double clustering seems to be players belonging to the hikers class."
"it can be observed from figure 2 that points which are close together (1-5, 4-26, 6-9, 14-17) present histograms with similar distributions. moreover, even though the interpretation of the axes is not obvious from the found distances, it can be observed that points with larger values in the vertical axis show a concentration of mass in the right hand side of the map while points with larger values in the horizontal axis show a concentration of mass in the lower part of the map."
"the field of machine learning offers a wide range of well-tested algorithms which have been used in player classification. some of these algorithms include decision trees, artificial neural networks and reinforcement learning. a characteristic of these algorithms is that they rely on the prior definition of classes and a reliable training set in order to classify players in a supervised approach. however, it is not always clear which player classes are most appropriate for a game. moreover, classes might be previously inexistent and exclusive to one game and classes might change and evolve over time."
the above practical challenges make it very difficult to find a single clustering algorithm which is able to find a complete and accurate player classification. we propose a partitioned specification and classification of the game features where it is possible to abstract the description of player classes according to different levels of granularity by using actions and states as the main descriptive blocks. we propose a three layer metaclassification approach consisting of the following stages:
pca is a linear dimensionality reduction technique based on finding a mapping between a high and a low dimensional data representation which maximises the amount of variance in the data. other dimensionality reduction techniques we analysed are:
a second avenue of future research will look at using the output from this meta-clustering to define classes and training sets which can be used as input to supervised machine learning algorithms such as decision tree learning or a genetic algorithms approach.
"in the second step of our meta-clustering approach, we are interested in the clustering of action sequences as a way to classify player's actions. more specifically, we look at the data generated from the movement of players across the game world, i.e. the sequence of location coordinates. for simplicity, we call this sequence of points the player's trail. figure 1 (left) illustrates a trail left by a player in the game 'the hunter'."
"we proposed a meta-clustering approach for player classification which divides the players' game variables into different classes and then sequentially applies different clustering algorithms to subsets of these classes. it has been shown how this metaclustering approach leverages the particular characteristics of the individual clustering algorithms to solve some of the problems faced when analyzing real game datasets such as overlapping classes and mixed-variables datasets. also, we have given some examples of how the end result of the meta-clustering allows for the designing of mechanisms which take advantage of a combined clustering analysis."
"in section 2 we explore some existing work on player clustering and classification. section 3 describes our meta-clustering approach giving details of the individual clustering algorithms used in each step. in this section we propose a new approach to the clustering of exploration sequences using rubner's earth mover's distance (emd) [cit] as a similarity metric and multidimensional scaling (mds) techniques as a visualization tool for exploring the data. section 4 describes the results of applying our meta-clustering approach to a large, highdimensional dataset from the freely available game 'the hunter'. in this section, we explore a set of linear and non-linear dimensionality reduction algorithms. these algorithms allowed us to visualise and analyse the game data in a low-dimensional space while maintaining the local and global properties of the high dimensional dataset. we then used the k-means unsupervised classification technique to identify clusters of players with similar profiles. finally, we extracted the adjacency matrix from the game's social network and using the geodesic distance between pairs of nodes we weighted the results from the two previous steps to identify clusters of agents which are similar in skills, preferences and social compatibility. we conclude in section 5 by discussing future work."
"we extracted a set of gameplay features from data collected from a series of real players' gaming sessions. this feature extraction and selection was based on choosing variables which can be used to describe a player's profile. 30 different features were selected; 11 from the actions class, 11 from and skills class, 1 from the exploration class and 7 from the social class. the complete dataset included multiple hunting sessions from approximately 50,000 players."
"where the numerator is the overall cost and the denominator is a normalization factor to compensate for distributions of different sizes. in order to find the optimal set of flows which minimizes the cost, we adopt rubner's optimization implementation which computes the optimum emd distance and corresponding flows [cit] . the representation of these distances in a n-dimensional space is done by applying multidimensional scaling with kruskal's normalized stress criterion [cit] ."
we explore k-means [cit] as our method for partitional clustering. k-means is a method for unsupervised classification and one of the most popular clustering algorithms so we will only briefly describe the most basic algorithm here:
"transformations are represented as a set of flows f ij which show the number of units of distribution mass which are moved from i to j. the problem then becomes finding the set of flows which minimises the overall cost. thus, the earth mover's distance is defined by the following equation:"
future work will focus on replacing the k-means clustering algorithm in the action/skill-based clustering stage in order to tackle the problem of identifying clusters within highly dense areas. one example of such highly dense areas is the cluster formed by beginners. the dbscan (density based spatial clustering of applications with noise) algorithm provides a good starting point.
"player classification is quickly becoming a key area in the development of games. it allows learning from player's actions in order to deliver personalized content back to the player. a traditional example, not originated in computer games, is targeted marketing where purchase suggestions are delivered to players based on a customer profile built from the player's previous purchases. a recent example of targeted marketing is apple's 'genius recommendations for apps' which recommends new applications based on those already installed on an iphone/ipod touch [cit] . a more recent and developing area which relies on player classification is adaptive games where the game adapts to the player's actions and decisions in order to provide a more enjoyable and challenging experience [cit] ."
"the first step of our meta-clustering approach consists of applying dimensionality reduction and partitional clustering algorithms to the action and skills variable classes. dimensionality reduction techniques allow us to identify and remove redundant information which might be contained in more than one feature due to a high correlation between the chosen variables. also, they provide a low-dimensional visualization which allows the visual exploration of the dataset. partitional clustering algorithms aim at assigning points in the dataset to k clusters by optimizing a given criterion function and an associated distance matrix. since partitional clustering algorithms do not provide a clear definition of the k extracted clusters, this level of clustering heavily relies on the visual inspection of the low-dimensional space to give meaning to the low-dimensional components in terms of the original variables. below we describe the dimensionality reduction and partitional clustering algorithms used in our experiments."
our meta-clustering approach consists of the sequential implementation of different clustering techniques and is summarized as follows: we propose a framework of variable classification consisting of the following four classes:
"apart from nns, for the last decade, various alternative nonparametric methods in the field of machine learning regression algorithms (mlras) have been introduced, many of them with interesting properties. in particular, kernel-based mlras, such as kernel ridge regression (krr) [cit], have proven to be simple for training and yield competitive accuracy [cit] . gaussian process regression (gpr) can even provide associated uncertainty intervals for the estimations [cit] . they both could become powerful alternatives to replace nns in hybrid retrieval schemes [cit] . however, these advanced mlras go at a computational cost, typically scaling poorly with the number of samples. this is currently the main shortcoming for their adoption in hybrid schemes for retrieval."
"v. results fig. 1 shows the average r 2 performances of the al methods over ten runs against validation data, each time until a training set of 1000 is reached. the dashed line represents the r 2 accuracy of the full reference training data set containing 2500 samples. the gray line represents the performance of the random sampling sequence, which is added to appreciate the benefit of al. the colored lines represent the six al methods considered. for both lcc and lai, the random sampling technique reduces the uncertainty in the prediction model as more samples are added, but it is unstable as it leads to irregular convergence curves and is outperformed by the al methods in terms of convergence rate. all al methods surpass the reference training data set before reaching 1000 training samples."
"at the end, rmse seg (see table 5 ) confirms results for the best choice of kernel size parameters. however, to improve the behavior of the testing algorithm, additional algorithm for the evaluation between line distance is prerequisite. incorporation of this algorithm will reduce the undersegmentation phenomena leading to better text line segmentation results evident by higher f-measure and smaller rmse seg values."
"all test results from algorithm #1 and #2 are reorganized according to segmentation binary classification. such results are given in table 6 . according to rmse seg, algorithm #2 shows slightly better performances than algorithm #1 in the domain of text line segmentation."
"to understand clearly the purpose of the rmse seg measure, two different segmentation results are evaluated by it. after the procedure of text line segmentation by the algorithms #1 and #2, obtained results are shown in fig.7 ."
"although krr and gpr proved to deliver high accuracies and were fast in processing, they incurred a computational cost, thus limiting the number of input training samples. initial tests revealed that the largest set that could be processed under a reasonable amount of time for both regression algorithms was the set containing 5000 samples of simulated rtm data; thus, this was used to analyze the al methods."
"this brings us to the following main objective: to introduce al techniques that deal efficiently with rtm-simulated data to 1545-598x © 2016 ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"this work led to an updated artmo mlra toolbox with the ability to apply al methods into hybrid retrieval models in a semiautomatic manner. combining both al methods with mlras makes the toolbox particularly powerful in processing optical remote sensing images into vegetation products. it opens possibilities toward the development of powerful (kernelbased) and generic hybrid retrieval schemes, e.g., based on rtm simulations. however, to pursue with developing hybrid strategies for an operational setting would imply introducing a larger training data set, which requires more samples to be added per al iteration. a generic validation data set would then be required to ascertain the minimum amount of samples that stabilizes the retrieval performance. another recommendation is to feed the al methods not only with simulated data but also with data collected in the field or coming from earlier products. for instance, esa's current lai product comes from an nn that is trained by earlier lai products [cit] . also, while the initial training set was chosen in a random way, a more sophisticated initialization strategy may further improve the al sequence. eventually, the freely available mlra toolbox will allow users to develop their own robust, compact, accurate, and generically applicable retrieval algorithms with little user interaction."
"the mlra retrieval toolbox [cit] offers a suite of regression algorithms that enable the estimation of biophysical variables based on either experimental or simulated data with little user interaction. the toolbox is built around the simpler package [cit] with over 15 mlras and additionally includes feature reduction techniques such as principal component analysis (pca) and nonlinear alternatives (e.g., kernel pca) along with crossvalidation subsampling techniques. the option of applying al methods was implemented for the first time in the latest mlra toolbox version (v1.17). a gui has been developed which allows selecting one or multiple al methods, as well as the selection of a starting data set, the number of samples to add per iteration, and the stopping criterion. a goodness-offit validation overview table allows the user to select the best performing hybrid method, which then can be used to process remote sensing images efficiently. the artmo package runs in matlab and can be freely downloaded from http://ipl.uv. es/artmo/."
"prosail-simulated spectral and input variable data were used to test and compare the al techniques on different kernel regression models. sensor settings corresponding to s3-olci were chosen over 18 bands from 443 to 940 nm. different selections of variables such as solar angles and leaf characteristics were selected in order to create a generic data set by combining the ranges of all variables, as shown in table i . the resulting data set contained 218 750 000 prosail-simulated reflectance-parameter pairs and was stored in a lut in artmo. this data set, which is overly big and largely redundant for regression, was randomly sampled into subsets of different sizes ranging between 1000 and 100 000 samples. the various subsets were used to investigate when the computational processing of the kernel regressions failed, using a random training/validation splitting of 50%."
"the variance gives an indication of the spread of the estimations. the samples with the highest variance, i.e., greater disagreements between the different regressors, are added to the training set and removed from the candidate set. 2) eqb: in this approach [cit], the predictions of k different regressors are ranked according to their entropy"
"regarding lcc retrieval with krr, the diversity al methods ebd, abd, and cbd, along with the uncertainty method pal, led to similarly good results. these methods reached the accuracy of the full training with about 500 samples, a fifth of the original data set size. moreover, these methods led to better accuracies than with the training of the whole 2500 learning samples. the uncertainty methods rsal and eqb resulted in poorer results."
"krr minimizes the squared residuals in a higher dimensional feature space and can be considered as the kernel version of the regularized ordinary least squares linear regression [cit] . the linear regression model is defined in a hilbert space h of very high dimensionality, where samples have been mapped through a mapping φ(x i ). we have used the radial-basisfunction kernel function."
"this study was conducted within an in-house developed graphical user interface (gui) software package named automated radiative transfer models operator (artmo) [cit] . artmo embodies a suite of leaf and canopy rtms, including prosail (i.e., the leaf model prospect coupled with the canopy model sail [cit] ) and several retrieval toolboxes, i.e., a spectral index toolbox, a lut-based inversion toolbox, and an mlra toolbox."
"many proposed algorithms for text line segmentation used a custom text database as a sample for the test procedure [cit] . however, they are mostly based on english document handwritten text [cit] or extended by additional scripts [cit] . additionally, most of the evaluation procedures are based on a pixel-based method [cit] . their performance evaluation technique is based on comparison of the detected segmentation result with an already annotated ground truth [cit] . in addition, this is an automated approach. however, the performance evaluation is a goal-oriented task. this is particularly correct for text line segmentation."
"where x u is a sample from the candidate set and x l is a sample from the training set. all distances between samples are computed, and then, the farthest are selected. 2) abd: this strategy [cit] measures the diversity between samples using the cosine angle distance, defined as"
"the selected criterion algorithms can rank the samples according to the uncertainty of a sample or its diversity [cit] . these criteria are sometimes used together within classification problems [cit] and are here applied separately to regression. the separate use of these criteria in this letter already provides benefits, although it is possible to combine them as in classification problems. selecting samples by uncertainty picks the most uncertain samples, i.e., those with the least confidence. uncertainty criteria include variance-based pool of regressors (pal) [cit], entropy query by bagging (eqb) [cit], and residual regression al (rsal) [cit] . selecting samples by diversity ensures that added samples are dissimilar from those already accounted for. diversity criteria include euclidean distancebased diversity (ebd) [cit], angle-based diversity (abd) [cit], and cluster-based diversity (cbd) [cit] . the algorithms are briefly described hereinafter."
"regarding lai retrieval with gpr, similarly to the lcc case, the use of al methods led to retrieval accuracies superior to that resulting from the full training data set. the results converged faster here, with 400 samples needed to reach the full training accuracy. random sampling led to similar results to the al methods until 400 samples, but it then became unstable. contrary to the aforementioned lcc case, rsal performed better, leading to r 2 values similar to that of the other al methods. in fact, all al techniques showed similar accuracy results. the fastest converging method was pal until 400 samples, but past this, cbd and ebd led to slightly superior accuracy results, although the overall difference between them is negligible."
optimize kernel-based mlra hybrid retrieval strategies. the subobjectives are as follows: 1) to implement al methods into a software framework that facilitates the use of these methods into hybrid retrieval strategies and 2) to evaluate the efficiency of al methods in optimizing hybrid prosail-mlra models with a manageable training data set. this analysis is illustrated for the retrieval of lai and leaf chlorophyll content (lcc) from simulated sentinel-3 olci data.
"multi-line fractured text segmentation test consists of the text samples that are based on fractured baseline. fractured baseline is defined by the parameter called fractured skew angle γ. typical values of γ that correspond to the handwritten text are those up to 20°. hence, it is assumed that it takes value from 5° to 20° with consecutive step of 5° [cit] . furthermore, inter-line spacing is set to 20% of the standard character height [cit] . the resolution of the text samples is 300 dpi. the set of multi-line straight text samples consists of 96 lines of: latin, cyrillic, glagolitic and bengali text. its definition is illustrated in fig.4 ."
"during the test procedure for text line segmentation, the algorithm is applied to the text samples. to evaluate the testing algorithm from the obtained results, following terms should be defined [cit] :"
"consequently, text line segmentation goal is object-line oriented scanning. few methodologies are established on this premise [cit] . hence, similar methodology for the evaluation of algorithms for text segmentation is proposed. it is based on the experiments linked with synthetic text samples [cit] extended by real handwritten ones [cit] . they are highly adaptable and can be extended with different types of scripts and languages. obtained measurement results are classified according to the signal detection theory. furthermore, they are extended with additional measure."
"the 5000 samples were separated into an initial learning set of 2500 samples and a validation set of 2500 samples. the gpr and krr models were first trained and validated on those data sets, further referred to as full training models, and were used as reference models. subsequently, an initial random training set of 50 samples of spectral data was selected from the learning set, followed by mlra training and validation. samples from the learning set were then added to the training set using the al methods in batches of 50 samples per iteration until no more samples were left in the learning set. the labels at each iteration were taken from the lut containing the simulated reflectance-parameter pairs. this was repeated ten times with a different initial training set each time to reach statistically reliable results. the number of pools used in the pal and eqb methods (k) was determined by comparing their predictive capability and processing time using 5 and 10 pools in the initial testing phase. since they both gave similar results, the number of pools was fixed to 5 in order to be faster. finally, the averaged coefficients of determination (r 2 ) values of the ten runs over the first 19 iterations, i.e., to reach 1000 training samples, were plotted. this procedure was completed using krr and gpr to retrieve lai and lcc values, although because of leading to similar results, only lcc by krr and lai by gpr results are addressed here."
"binary classification is based on the signal detection theory postulate [cit] . its task is to classify the members of a given set of connected components into two groups. the classifying is based on whether they have some property or not. it is represented by confusion matrix which is shown in table 1 . hence, if some connected components have a property and the test confirms it, then those connected components will represent true positives (tp). if some connected components do not have a property, but the test confirms it, then they will represent false negatives (fn). however, if some connected components have property, but the test mistakenly does not confirm it, then they will represent false positives (fp). finally, if some connected components do not have a property, and the test confirms it, then they will represent true negatives (tn). in the context of classification tasks, all previous statements are used to compare the given classification of an item. correlation of the previous statements with the testing results of the algorithm is as follows [cit] :"
"ext line segmentation is an important step in document image processing [cit] . it represents mainly a labeling process, which consists in assigning the same label to spatially aligned units [cit] . text line detection techniques are successful mainly in printed documents. however, processing of the handwritten documents has remained a key problem [cit] . text line segmentation of handwritten documents is a complex and diverse problem, complicated by the nature of handwriting [cit] . hence, it represents a leading challenge in handwritten document image processing [cit] ."
"for two kernel-based mlra methods, i.e., gpr and krr, we demonstrated the utility of al methods as an intelligent sampling step that selects the best possible samples from a large (simulated) training data set. by using a data pool of prosail-simulated lai, lcc, and s3-olci spectra, each al method converged faster to the lower error bound than a random sampling strategy. diversity criteria (ebd, abd, and cbd) performed generally fastest, both in terms of reaching high accuracies as well in processing time. random sampling converged in the long run to similar accuracies but did not offer a stable error minimization over few samples. the reason that each of the al methods smoothly converged to the full training error bound is that the methods only allow the addition of samples that improve the overall accuracy of the model. these observations using simulated training data confirm earlier results that were obtained in experimental regression problem studies [cit] . this demands for a follow-up study to combine uncertainty and diversity criteria. another attractive path to pursue is to select samples according to a density criterion [cit] ."
"although test experiments are diverse, theirs results are inter-related. hence, decision-making is required to combine results and make conclusions. as a result of the decisionmaking procedure, the set of the algorithm's parameter values is revealed. this set is the starting point for the procedure of the algorithm's optimal parameter selection."
"one of the major challenges in this respect is to cope with the large amount of data that has to be processed. over the last few decades, a wide diversity of biophysical variable retrieval methods have been developed, but only a few of them made it into operational use, and many of them are still in their infancy [cit] . generally, there are four main approaches to the problem of estimating biophysical variables from reflectance spectra, i.e., 1) parametric regression; 2) nonparametric regression; 3) physically based, i.e., inverting radiative transfer models (rtms); and 4) hybrid methods. hybrid methods combine the generic properties of physically based models combined with the flexibility and computational efficiency of nonparametric nonlinear regression models [cit] . hybrid retrieval schemes proved to be successful in operational processing chains. current hybrid schemes for the generation of land products such as leaf area index (lai) rely on neural networks (nns), typically trained using a very large amount of rtm-simulated data [cit] . for lai retrievals for instance, the most widely used rtm is prosail [cit] ."
"the problem when using rtm-simulated data is that these data can easily scale up to several millions of samples corresponding to different variable selections, particularly when lookup tables (luts) are used and all input variables are systematically combined. simulated data created in this way might be redundant, as slight variable variations might produce very similar spectra. therefore, in order to have these kernelbased mlras trained by large data sets, we need intelligent sampling approaches that select the most informative samples from a large pool data set. in the spectral domain, we can use feature extraction techniques [cit] to reduce these large data sets. in the sampling domain, we propose to use active learning (al) techniques [cit] to obtain an optimized training set while keeping its size small enough. this letter analyzes the utility of those al techniques for the benefit of developing efficient hybrid retrieval schemes."
"finally, the number of samples needed to reach maximal retrieval accuracy and their corresponding processing time were recorded (see fig. 2 ). the diversity methods abd and cbd processed fastest to reach maximized accuracies. since these two diversity methods also led to high accuracies with relatively small training sets, they seem to be the best methods for operational applications. ebd and rsal were also fast, but ebd performed unstably when comparing lcc against lai and rsal led to the poorest accuracies. pal and eqb, although yielding high accuracies, took the longest time to compute since these methods created five regression models to be compared."
"where x u, x l is the inner product between x u and x l . the learning samples showing the largest cosine angles with the training data are added to the training set. [cit] first groups the data using a clustering algorithm, for instance, k-means. the number of clusters k is set to the number of samples to add in each iteration of the al algorithm. for each cluster, the nearest sample to the cluster centroid is selected."
"furthermore, as an extension to algorithm evaluation based on binary classification, the fourth measure is introduced. it is called rmse seg . it represents comparison of the number of detected and referent connected components per each text line. hence, the number of referent connected components per line is equal to one. the variance evaluation is given rmse seg [cit] : ( )"
"kernel-based mlras allow for fast and accurate variable predictions, and they possess attractive properties to become the core of next-generation operational hybrid retrieval schemes [cit] . however, most of them cannot process large training databases. for instance, here, it was found that standard implementations of gpr cannot typically cope with more than 5000 samples, i.e., a biophysical variable associated with superspectral or hyperspectral data, under reasonable time, which hampers its direct application toward generic retrieval models. to mitigate this problem, the use of six al methods was explored in this letter. al methods are promising to crystallize an optimal training sampling set that is manageable by kernelbased mlra methods."
"let us assume that we have a large (simulated) training data set composed of many samples, so large that it is unfeasible to use all the samples to obtain a regression model. in order to reduce its size, the typical naive approach is to select some samples from it randomly. this strategy does not necessarily optimize the selection since all samples, including redundant and noisy samples, are treated equally. al [cit] can be used to solve this shortcoming using smart sample selection criteria to improve data compactness, diversity and richness, and, hence, the model's estimation accuracy. al techniques aim to select the most informative samples in a training data set to reach high accuracy using fewer labeled samples. while widely used in data classification [cit], there are few experimental studies carried out for regression problems, and these are focused on the estimation of terrestrial [cit] and ocean biophysical [cit] variables from remote sensing data. none of these aims to be generic at larger scales via the training of rtm simulations. accordingly, the application of al techniques to smartly sample large simulated training data sets in hybrid schemes creates the opportunity to improve the prediction accuracy of the resulting regression model. al methods start with a small training set of labeled data-in this case, reflectance-variable pairs-and use query strategies, or selection criteria, to select samples from a larger unlabeled data pool, i.e., one in which samples have no assigned variable value. the selected samples are labeled and are added to the training set until it becomes optimal [cit] . in classification, the samples are typically labeled by a human expert. the use of simulated training data removes the need for a human expert as the labels are taken from the lut, thus creating an automated optimized sampling solution. the optimal final training set should be small enough to substantially increase the computational efficiency of the final prediction model and large enough to accurately represent the original training set and give highly accurate predictions. the selection criteria select the most significant training samples from the pool, i.e., those which, if labeled, would improve the regression model the most."
"this strategy [cit] first generates k subsets by randomly choosing samples from the original training set. each subset is then used to train a regressor and to obtain a prediction for each sample in the candidate set. this ends up with k different predictions for each candidate sample. then, the variance of each prediction is estimated as"
"multi-line handwritten text segmentation test consist of the unconstrained handwritten text samples. they are written in serbian latin, cyrillic as well as in english script [cit] . this is a document text database which consists of 220 text lines of handwritten text. all handwritten text samples contain variable skew lines, i.e., multi-oriented text. resolution of the text samples is 300 dpi. a few fragments of the handwritten dataset are shown in fig.5 ."
"multi-line straight text segmentation test consists of the text samples that are based on straight baseline. however, straight baseline is defined by the parameter called skew angle β. typical values of β that correspond to the real handwritten text are those up to 20°. for the test purposes, it is assumed that it takes value from 5° to 20° with consecutive step of 5° [cit] . for the subsequent parameter, inter-line spacing is chosen. it is set to 20% of the standard character height [cit], which corresponds to the single line spacing [cit] . accordingly, resolution of 300 dpi for text samples is used. the set of multi-line straight text samples consists of 96 lines of: latin, cyrillic, glagolitic and bengali text. its definition is illustrated in fig.2 ."
"gpr has recently been introduced as a powerful regression tool [cit] . the model provides a probabilistic approach for learning generic regression problems with kernels. the gpr model establishes a relation between the input and the output variable in the same way as krr. however, two additional advantages of gpr must be noted. first, not only a predictive mean but also a predictive variance can be obtained, which can serve as an uncertainty interval. the second advantage is that one can use very sophisticated kernel functions because hyperparameters can be learned efficiently by maximizing the marginal likelihood in the training set [cit] . in this letter, a standard rgb kernel function was used."
"the paper is organized as follows: in section 2 test framework is presented. section 3 contains the test evaluation procedure. section 4 gives a brief introduction to the testing algorithm. in section 5 results are analyzed and discussed as well. conclusions are given in section 6. furthermore, appendix incorporates the examples of text sample scripts as well as the explanation of rmse seg measure."
"the local information-based distributed (lid) algorithm (cf. algorithm 1 for pseudocode) uses, at each node i, four sets (u i, p i, a i, k i ) and a function (topranked(·)) and sends two kinds of messages (prop and rej):"
"besides the reported and structured analysis based on table 1, we also observed further promising approaches for assessing pd patients using wearable imus, that could be relevant for further investigation. first, besides the reported parameters, which are the most common in studies performing gait analysis, other potentially meaningful parameters were also collected in some of the reported studies. as an example, besides the already reported arm swing asymmetry, these were: stride regularity, harmonic ratio, and turning characteristics (for the tests that included turning). with motor asymmetry being a characteristic of pd-related abnormalities in locomotion [cit], bilateral measurements and asymmetry parameters seem promising disease-specific sensor metrics. thus, obviously our review is not exhaustive and further work may be done for other sets of parameters."
"the locally unique edge weights are assumed as a means to make sure that there are no cyclic dependencies. there may be other ways to break symmetry and perhaps achieve higher parallelism; we expect that it can be possible to build further on this paper's results, e.g., also via working with other, more elaborate algorithms for standard weighted matching as a starting point."
"proof: for a static network, lemma 4 is applicable. for a dynamic network, by lemma 7, after a finite amount of time, every node i cancels all proposals to neighbors that are no longer locally heaviest after a change and proposes to the locally heaviest ones. some of these proposals will result in a match if the receiving node also considers the originating node locally heaviest. the same will happen to all proposed nodes, as long as the originating and receiving nodes have available quotas. if some node lacks in available quota, we know that its matched incident edges are heavier than its unmatched locally heaviest, since by lemma 7, it would have to cancel the appropriate proposals and issue new ones towards locally heaviest neighbors."
"by using these weights to construct and solve a many-to-many weighted matching problem, we also get a solution for the truncateds b-matching problem, as the following lemma suggests."
"in the beginning, each node i reports to each neighboring node, j, only j's relative rank in i's preference list, in the form of ∆s j i, ∆s i j exchange between i and j, and they both compute the weight w(i, j) of their connecting edge. this ensures the flexibility of the ranking metric each node uses (it does not need to reveal the metric) at the cost of only local communication between nodes. every node keeps these newly formed weights of its adjacent edges in a weight list, which is then used during the algorithm's execution to determine the desirability of a neighboring node. note that these weight lists do not replace the individual preference lists of the nodes: they are auxiliary lists that are used in a similar way (to determine desirability), but only during the algorithm's execution, i.e., not for measuring the final satisfaction."
"proof: if there are less than b i locally heaviest edges, algorithm lic will choose all of them, since, by construction, it will continue to select them until there are no quota slots available at either endpoint. otherwise, again by construction, it will choose the b i heaviest of them for node i."
"review process: search strategy and selection criteria we excluded conference proceedings, articles reporting results from less than 10 pd patients, that did not assess gait, or that assessed gait over a walking distance shorter than five meters. as a note, \"3000\" as year of end is the default pubmed option when performing a search until the current day. note, \"3000\" as year of end is the default pubmed option when performing a search until the current day. figure 2 presents a flowchart of the review process."
"as the first step to approximate the maximizing satisfaction b-matching problem, we define a modified problem based on the same basic b-matching problem, but computing satisfaction using a modified version of equation (4) (and subsequently equation (1)):"
"and, specifically, its minimum value, r min . knowing this minimum value, we can conclude that the total satisfaction of the modified problem, s mod, is a r min -approximation of the total satisfaction of original b-matching problem s orig, since:"
"the walking of patients with pd is characterized by alterations (with respect to healthy subjects) of the most important spatio-temporal gait parameters such as gait velocity, cadence, stride time, and length, parameters also known to degrade with disease progression. the assessment of walking is part of the most common pd rating scales such as the unified parkinson's disease rating scale (updrs), hoehn and yahr (h&y) staging, and the schwab and england rating of activities of daily living. recent technological developments provided clinicians with new tools to evaluate treatments in a more objective way. in the last few years there has been a growing interest for developing technologies and methods for enabling human motion analysis. specialists are focusing more than ever on products that perform a clinical and biomechanical evaluation in a way that is as much as possible cheap, fast, and effective. gait analysis using wearable sensors is expected to play an increasingly important role in neurology and other clinical fields. by using inertial sensors, it is possible to detect and characterize specific movements and register variations in the clinic (supervised by a clinician) and monitor activities of daily living of people with parkinson's disease in their own home (unsupervised). in this review we focus on using inertial sensors for gait analysis in the clinic environment (i.e., we do not cover the use of inertial sensors to monitor walking or general activities at home, in unsupervised environments where the registration of freezing or tremor event frequency, duration, and intensity may also be a useful wearable sensor application). we want to provide an overview of the recent (last 10 years) state of the art in this field, characterizing current usage and possible limitations."
"proof: every change triggers weight updates at a distance of one (nodes that the joining/leaving node connect to/disconnect from or direct neighbors of a node that changes preferences) and possibly at a distance of two (neighbor's neighbors of a joining/leaving node). by lemma 6, the weight updates complete in a finite amount of time. when procedure 3 executes next in any node i at a distance of one or two from the change, it will repeatedly examine nodes that are either available or have simultaneously rejected and been rejected by node i, as long as they are heavier than the proposed node of the lowest weight. every time it encounters a node of the latter category, it sends a wake message, prompting the node to revoke its rejection, whereas every time it encounters a node of the former category, it sends a prop message, preceded by a rej to the proposed node of the lowest weight (note that whether a node is considered locally heaviest or not may change during a single execution of procedure 3.). in any case, at the end of procedure 3's execution, all proposals from node i to its neighbors that are no longer locally heaviest are canceled, and complementary proposals are sent to available neighbors that are locally heaviest; the same is true for every node in the network."
"the most common way to describe stability in stable roommate problems is through the notion of blocking pairs [cit] . for example, when indifference is not considered in the preference lists, two nodes form a blocking pair when they both strictly prefer each other over some of their current matches, and a matching configuration is called stable when there are no blocking pairs in the network. the satisfaction definition of equation (1), together with the interpretation in section 3.1, can assist in moving beyond the classic stable matchings and towards the overlay network context. although work in this area has been fairly limited, the notion of satisfaction does seem to stand out as a natural extension to the blocking pairs notion: instead of nodes forming a strict, stable matching, satisfaction gives them the flexibility to make choices of measurable \"goodness\", by assigning real values to their choices and applying algorithms that, e.g., maximize the total satisfaction on a network level. this maximizing total satisfaction criterion introduces a social element to the traditional notion of stability and can be considered stricter than the above definition of blocking pairs, as the following observation suggests."
"the truncateds maximizing satisfaction b-matching problem is a 1 2 ( proof: since only s s i is used in the modified maximizing satisfaction b-matching problem, we are interested in studying the ratio:"
"the walking of patients with pd is characterized by alterations (with respect to healthy subjects) of the most important spatio-temporal gait parameters such as gait velocity, cadence, stride time, and length, parameters also known to degrade with disease progression. the assessment of walking is part of the most common pd rating scales such as the unified parkinson's disease rating scale (updrs), hoehn and yahr (h&y) staging, and the schwab and england rating of activities of daily living. recent technological developments provided clinicians with new tools to evaluate treatments in a more objective way. in the last few years there has been a growing interest for developing technologies and methods for enabling human motion analysis. specialists are focusing more than ever on products that perform a clinical and biomechanical evaluation in a way that is as much as possible cheap, fast, and effective. gait analysis using wearable sensors is expected to play an increasingly important role in neurology and other clinical fields. by using inertial sensors, it is possible to detect and characterize specific movements and register variations in the clinic (supervised by a clinician) and monitor activities of daily living of people with parkinson's disease in their own home (unsupervised) . in this review we focus on using inertial sensors for gait analysis in the clinic environment (i.e., we do not cover the use of inertial sensors to monitor walking or general activities at home, in unsupervised environments where the registration of freezing or tremor event frequency, duration, and intensity may also be a useful wearable sensor application). we want to provide an overview of the recent (last 10 years) state of the art in this field, characterizing current usage and possible limitations. we excluded conference proceedings, articles reporting results from less than 10 pd patients, that did not assess gait, or that assessed gait over a walking distance shorter than five meters. as a figure 1 . electronic board of a wearable sensor with an inertial measurement unit (imu)."
"the definition above coincides with the one of equation (2). the following two lemmas address the dynamics arising from the aforementioned recursive definition by showing two important properties: the algorithm's execution at node i: (i) chooses only locally heaviest edges (although not in any particular order); and (ii) chooses edges in a way, such that any unselected locally heaviest edge adjacent to node i at the end of the algorithm's execution has lower absolute edge weight than the selected ones adjacent to the same node."
"in this paper, we presented klognlp, a natural language processing module for klog. based on an entity-relationship representation of the domain, it transforms a dataset into the graph-based relational format of klog. the basic klognlp model can be easily extended with additional background knowledge by adding relations using the declarative programming language prolog. this offers a more flexible way of experimentation, as new features can be constructed on top of existing ones without the need to reprocess the dataset. in future work, interfaces will be added to other (domain-specific) nlp frameworks (e.g., the bllip parser with the self-trained biomedical parsing model [cit] ) and additional dataset formats will be supported."
"finally, some papers report reference values for their parameters, which could aid the exchange and comparison of results, while others do not. some also present values from healthy controls, which can help in evaluating the usefulness of parameters in characterizing pd impairments. furthermore, some papers perform comparison with gold standards (or use methods that were previously compared with gold standard technologies), while others do not. this may also impact on reliability and comparison across results. it is clear that every imu system used to quantify gait in pd should have been previously validated in pd patients with respect to a gold standard such as an optical motion capturing system."
"the two network types show their differences more prominently under churn ( figure 5 ). for the er networks, a joining node under churn can be seen as a \"reincarnation\" of a leaving node with all its previous connections dropped and its preferences changed, since both of them have comparable node degrees. however, the churn operation is detrimental for the ba network, since the joining nodes are of a low degree and the departing ones of a potentially much higher degree. as a result, the degree distribution itself is changing, leading to higher reconvergence times (cf. join operation). this phenomenon can be seen more clearly when looking at networks of different sizes ( figure 6 ): high churn for small networks is not particularly problematic (small slope on the graph for 100 nodes), since degree variation is limited, but for networks of a size of 1,000 it is quite detrimental, leading to higher reconvergence times (high slope on the graph for 1,000 nodes). in both cases, though, by comparing the churn and preference change graphs in figure 5, it is easy to see that, somewhat counterintuitively, it takes progressively more time for the adaptivelid algorithm to reconverge when more nodes change preferences, but the reconvergence time stays more or less the same even for high values of churn, or it is consistently lower than preference change, as is the case in ba networks. figures 6 and 7 show this phenomenon in more detail for all network sizes. the reason behind this behavior is that in churn situations, there are more parallel events taking place: a new, joining node that replaces a leaving one starts as an empty slate and sends an amount of prop messages equal to the desired number of connections. on the other hand, a node that changes preferences might need to repair only some of its connections (which are now suboptimal) by sending appropriate prop messages. however, in both cases, some responding nodes might decline, which will lead to additional prop messages to be sent, and so on, until the issuing nodes are satisfied or no available nodes are left."
"since in nlp, most tasks are situated at either the document, sentence, or token level, we propose the e/r model in figure 2 as a general domain model suitable for most settings. it is able to represent interpretations of documents as a sequence (nexts) of sentence entities, which are composed of a sequence (nextw) of word entities. next to the sequence relations, also the dependency relations between words (deprel) are taken into account, where each relation has its type (deptype) as a property. furthermore, also the coreference relationship between words or phrases (coref) and possibly synonymy relations (synonymous) are taken into account."
"the following theorem follows directly from lemmas 1 and 2. (24), is a having approximated the original b-matching problem by a many-to-many maximum weighted matching, we need only a distributed algorithm for that problem. the rest of this paper presents a simple, distributed algorithm in two variations, an adaptive and a non-adaptive, that solve the many-to-many maximum weighted matching problem using a 1 2 -approximation guarantee. we also present and utilize a centralized helper algorithm that we employ in order to prove the approximation ratio of the distributed algorithm."
"with the research focused on preference systems that are known to have a stable configuration, such as acyclic ones, there is a relative scarcity of results concerning arbitrary preference systems and practical algorithms for achieving stable configurations (if existing) or good approximations thereof. can we look at this problem from a different perspective? how can a solution be measured? what are the characteristics of a practical algorithm in a fully distributed scenario? can we adapt to and tolerate changes in the overlay, such as preference change or peer joining/leaving?"
"nltk the python natural language toolkit (nltk) [cit] ) offers a suite of text processing libraries for tokenization, stemming, tagging and parsing, and offers an interface to wordnet."
"to summarize, approaches to gait analysis in pd patients are various regarding the type of sensor, sensor location, measurement duration/length, gait parameters, and clinical measures of patients. no consensus is present yet. currently in the literature there is not enough evidence to support an informed clinical decision about the best system characteristics (number and positions of sensors) to choose to obtain effective clinical outcomes."
"regarding the gait spatiotemporal parameters, significant differences were found between the selected articles, as indicated in table 1 . this is clearly related to the different set-up (number and positions of sensors that were used). as a clarifying example, we found that foot clearance was only calculated in two studies which used sensors on the foot. as expected, and as shown in figure 4, gait speed is the most common parameter reported in these studies (although not in all studies), appearing in almost 90% of them. other studies reported stride velocity, which should reflect the same underlying parameter. we grouped together the two in the table. cadence, which is also reported as step frequency, is present in 61% of the studies. stride length, which can be particularly useful to evaluate pd patients (who often may exhibit short steps), is present in 52.8% of the studies. stride time, which is also referred as gait cycle duration, has been calculated in 41.7% of the studies."
"the simple greedy algorithm we are proposing is fully distributed and operates by choosing the locally heaviest edges in every node's neighborhood, generalizing the one-to-one matching algorithm by hoepman [cit] ."
"the following lemmas prove that the algorithm always converges after a finite amount of steps or, in the case of changes in the network, in a finite amount of steps after the changes stop. although implied by the distributed nature of the algorithm, it is useful to note that the algorithm continues to run at all nodes regardless of any changes that are happening in the network. in fact, as we show in the experimental section, it manages to maintain a reduced but steady level of service while under extremely heavy stress or possibly a network attack. however, convergence can be guaranteed after all changes complete, since any changes that might occur require appropriate readjustment by the distributed algorithm."
"in short, the interpretation of satisfaction (equation (1)) we consider here can be described simply as follows: a node i is totally satisfied when it connects to its b i \"most wanted\" preferences with its b i degrees of freedom; otherwise, each \"opening\" in the list and/or each degree of freedom that remains unused (not matched) is reflected in the node's lower satisfaction."
"in a dynamic setting, where nodes join/leave the network or change preferences about their neighbors at any time, there is a partial or full solution that is disturbed by a specific operation. in this case, it is desirable to \"repair\" the solution locally instead of recomputing it globally. it would also be advantageous to limit the repairs to the neighborhood of the operation, so that far enough nodes would remain unaffected. note that the locally-heaviest-edge property that we are using here seems ideal for this purpose: it only makes sense to preserve and use it further to support dynamicity."
"studies included an average of 51.3 ± 48.8 patients, with a minimum of 10 and maximum of 190. we excluded papers based on less than ten patients. most studies (72.2%) reported hoehn and yahr staging of disease, so it is recommended that new studies report this value. some of the studies additionally (or alternatively) report the motor score of the unified parkinson's disease rating scale (updrs), which can indeed be useful to further characterize the cohort of patients. unfortunately, some studies did not report these values at all, making it difficult to draw conclusions on effectiveness or to compare with other studies."
"studies included an average of 51.3 ± 48.8 patients, with a minimum of 10 and maximum of 190. we excluded papers based on less than ten patients. most studies (72.2%) reported hoehn and yahr staging of disease, so it is recommended that new studies report this value. some of the studies additionally (or alternatively) report the motor score of the unified parkinson's disease rating scale (updrs), which can indeed be useful to further characterize the cohort of patients. unfortunately, some studies did not report these values at all, making it difficult to draw conclusions on effectiveness or to compare with other studies."
"our search selected 36 articles. they are summarized in table 1 . we reported the most commonly derived parameters for spatio-temporal gait analysis. regarding sensor locations, we grouped together imus on the lower part of the leg but listed sensors on foot or feet separately. the hoehn and yahr scale (h&y) is provided as a clinical rating scale evaluating progression in parkinson's disease stages. a better consistency and standardization in placing and in reporting should be done in the future. for example, \"shin\" and \"shank\" refer to the same part of the leg: the tibia. also, placement on the shins, ankles and on the feet can be ambiguous and mean different relative positions (lateral, front, different height or relative positions). furthermore, the asymmetry column is checked if in the article there is at least one parameter related to the right-left difference of one of the parameters in the table (thus, asymmetry of right-left arm, which is present in some papers, is not considered in table 1 )."
"the mean value and standard deviation of convergence speed for a variety of network sizes can be found in figure 3 . it is easy to see that the convergence speed depends on the type of the network. for example, ba networks of a size of 1,000 take almost twice the amount of time to converge than networks of a size of 100, while er networks of a size of 1,000 need less than twice the amount of time needed by networks of a size of 100 and only a slightly higher amount of time than the networks of a size of 500 and 750. likewise, the time needed for reconvergence can be seen in figures 4 and 5, for networks of a size of 1,000 of both types and for the four types of operations under consideration. by focusing on low percentages of affected nodes (i.e., up to 20% of the network size, which is a high volume of change), it is easy to see that reconvergence is obtained in most cases for a fraction of the rounds needed for initial convergence. for join and leave operations, reconvergence is expressed not in rounds, but in relation to the convergence time, since network sizes change significantly. note here that this extreme change in network sizes leads in some cases to percentages greater than 100%, i.e., more rounds are needed for the reconvergence than for the initial convergence. for the preference change and churn operations, this is not the case: the network size remains the same, either because no node joins or leaves (preference change case) or the amount of nodes joining and leaving is the same (churn case), and the reconvergence time is expressed in rounds. in the case of join operations, nodes arrive at the network and want to join the already established equilibrium of connections by being more attractive choices for some of the nodes with whom they are neighbors. this creates cascade effects of nodes rejecting old connections in favor of the newcomers, the rejected nodes trying to repair their lost connections, and so on. naturally, the more nodes wanting to join the network, the bigger upheaval is created. a similar effect is generated during leave operations, where previously rejected nodes suddenly become attractive choices for nodes that were left behind by departing nodes. note here that the ba networks reconverge much faster than the corresponding er networks in the case of join operations. this happens because new nodes (being of a low degree) connect preferentially to relatively few high degree nodes, limiting the extension of the upheaval in the network. in the case of leave operations, the same behavior poses a challenge, since departing nodes may happen to be of a high degree themselves, leaving behind a lot of low degree nodes to repair their connections. notice though that in both cases (er or ba networks), when a substantial percentage of the network departs (i.e., above 35%), the remaining nodes repair their connections much more easily, since they have more unformed connections than established ones."
"s orig with q and q, respectively, as well as q min for the minimum value of the latter. using the above notation and equations (13) and (18), we get:"
motor symptoms and gait impairments are among the issues that most affect the quality of life of patients with parkinson's disease. pd patients can exhibit slow gait with less foot clearance and smaller step lengths with respect to healthy individuals [cit] ). pd patients consider the progressive loss of motor autonomy the first and most worrying symptomatology [cit] . this is one of the reasons why having effective and reliable tools for gait analysis is paramount. wearable sensor systems can
"the preprocessing toolkit to be used can be set using the klognlp flags mechanism, as illustrated by line 3 of listing 2. subsequently, the dataset predicate (illustrated in line 4 of listing 2) calls klognlp to preprocess a given dataset 3 . this is done according to the specified klognlp model, i.e., the necessary preprocessing modules to be called in the preprocessing toolkit are determined based on the presence of the entities, relationships, and their attributes in the klognlp script. for example, the presence of namedentity as a property of word results in the addition of a named entity recognizer in the preprocessing toolkit. the resulting set of interpretations is output to a given file. in case several instantiations of a preprocessing module are available in the toolkit, the preferred one can be chosen by setting the name of the property accordingly. the names as given in listing 1 outline the standard settings for each module. for instance, in case the snowball stemmer is preferred above the standard (wordnet) lemmatizer in nltk, it can be selected by changing lemma into snowball as name for the word lemma property (line 8). listing 2: full predicate for 10-fold classification experiment each interpretation can be regarded as a small relational database. [cit] dataset on hedge cue detection, a binary classification task where the goal is to detect uncertainty in sentences. this task is situated at the sentence level, so we left out the sentence and nexts signatures, as no context from other sentences was taken into account. a part of a resulting interpretation is shown in listing 3."
"it is easy to see that ∆s i,j, which we call the a priori part and the a posteriori part, respectively, since the former is computable for each j regardless of whether it is currently matched with i or not, while the latter is computable only for those j that are currently matched with i by the solution. alternatively, we will refer to these parts as static and dynamic, respectively. the static part depends only on the rank of node j in preference list l i, but the dynamic part depends on the rank of node j among node i's chosen connections at any given time. this rank among the nodes connected to i may depend on various factors in an execution, possibly also choices of nodes that are relatively remote to node i. knowing that even simple matching is not a locally solvable problem [cit], this is no surprise. note that an algorithm that makes connection decisions based on the static part of ∆s j i has all the necessary information available from the beginning of its execution (i.e., does not use any runtime information), and therefore, a node's valuation of other nodes (that depends only on that static part) does not change over time, due to its runtime connection decisions. as a result, oscillations, due to cyclic preferences are avoided."
"note that the lid algorithm proposes to nodes of heavier edges first and proceeds only if it receives an explicit decline (meaning it was not a locally heaviest edge or the other node filled its quota), so the chosen edges are always heavier than any unchosen one."
"proof: by lemma 8, we get that the adaptivelid algorithm terminates for every node, and by lemma 9, we know that at termination, it has chosen only the locally heaviest edges of maximum weight. since by lemma 4 the static algorithm also selects the locally heaviest edges of maximum weight at termination, it follows that the two algorithms make the same choices for the same networks."
"the remainder of this paper is organized according to the general klog workflow, preceded with the klognlp module, as outlined in figure 1 . in section 2, we discuss the modeling of the data, and present a general relational data model for nlp tasks. also the option to declaratively construct new features using logic programming is outlined. in the subsequent parts, we will illustrate the remaining steps in the klog pipeline, namely graphicalization and feature generation (section 3), and learning (section 4) in an nlp setting. the last section draws conclusions and presents ideas for future work."
"the rest of the paper is organized as follows. in section 2, we introduce the necessary notation and define our problem, while in section 3, we present, separately, the notion of node satisfaction, along with an extensive commentary, due to its importance in our modeling. in section 4, we show how our original b-matching problem can be reduced into a many-to-many maximum weighted matching problem that is guaranteed to have a stable solution, and we prove that this can be done with constant factor approximation. we present our distributed algorithm for the many-to-many maximum weighted matching problem in two variations, adaptive and non-adaptive, in section 5, along with analytical proofs of their approximation ratio for our original problem. our extensive experimental study can be found in section 6, while discussion on the presented, as well as related work can be found in section 7. finally, section 8 concludes the paper."
"observation 2 following the main argument of the classic chandy and misra [cit] drinking philosophers algorithm, the convergence complexity is bounded by the longest increasing edge weight path in the network."
"a node j is the locally heaviest node in the neighborhood of node i at some point in time if there are no available nodes adjacent to node i that are endpoints of heavier edges. note that edges with such a node as an endpoint are candidates for being locally heaviest edges (hence, the name \"locally heaviest node\"). in fact, when the endpoints of an edge consider simultaneously each other locally heaviest, the edge between them is the locally heaviest edge."
"in this review we did not find one particular multi-sensor set-up that is used by most of the studies but a wide variety of combinations. a consensus in this regard in the clinical research community is still to be gained, in terms of sensor numbers and positions, probably in a trade-off between the number and accuracy of gait parameters and clinical usability. thus, related themes that would deserve further investigation are accuracy, redundancy, and comfort of different positions. in general, research work on what is the minimum set-up to guarantee a certain level of accuracy on a specific set of parameters would be very useful for clinical research. in this review we did not find one particular multi-sensor set-up that is used by most of the studies but a wide variety of combinations. a consensus in this regard in the clinical research community is still to be gained, in terms of sensor numbers and positions, probably in a trade-off between the number and accuracy of gait parameters and clinical usability. thus, related themes that would deserve further investigation are accuracy, redundancy, and comfort of different positions. in general, research work on what is the minimum set-up to guarantee a certain level of accuracy on a specific set of parameters would be very useful for clinical research."
"despite the fact that we considered only relatively recent articles, we observed a very high variability in the number and positions of these wearable sensor. a single common set-up was not prominent. set-ups with between one and eight sensors, in different positions, were used. however, we found that the most used set-up was with a single sensor on the lower back, which obviously poses a trade-off in the number of spatio-temporal parameters that can be effectively recorded (e.g., it would not be possible to report foot clearance). further research is needed to recommend the best set-up (or the best set-ups if two or more set-ups are equivalent) for assessing a set of spatio-temporal parameters. regarding this, we also found a very high variability concerning the different spatio-temporal parameters reported. also, further research should be performed linking the position of sensors and the parameters that can be obtained from those positions. parameters should be evaluated with respect to their usefulness in clinical practice. thus, the choice of number of sensors and sensor location(s) should be driven by the clinical relevance and required accuracy of the specific gait parameters and the usability of the set-up. gait speed, cadence, and stride length were the most reported parameters; further work should be done to choose the most effective parameters for different aims (e.g., to evaluate fall risk, to evaluate efficacy of a treatment, to evaluate difference between health and pd etc.). in a patient-centric approach the development of wearable sensor assessment protocols and derived outcome parameters should consider the usability aspects of the test as well as the impact of the derived outcome metrics for the patients in order to establish the method in clinical routine. in such an approach, the wearable sensor technology's capacity to quantify upper extremity locomotion (which was not the focus of the present paper) could also be evaluated, independent by itself or simultaneous and in relation to gait."
"the lic algorithm produces a many-to-many maximum weighted matching m c that is a 1 2 -approximation of the matching m op t produced by the optimal algorithm, opt."
"these properties prove especially advantageous in the case of nlp. the graphical approach of klog is able to exploit the full relational representation that is often a natural way to express language structures, and in this way allows to fully exploit contextual features. on top of this relational learning approach, the declarative feature specification allows to include additional background knowledge, which is often essential for solving nlp problems."
"proof: as edge (i, j) is eventually locked, we assume without loss of generality that node i sent to node j a prop message, inserted it in p i \\k i, later on received a confirmation from node j and eventually locked the edge. it is easy to see that sets p i \\k i and p j \\k j contain the heaviest available edges in the neighborhoods of node i and j, respectively; heaviest because algorithm lid sends prop messages to neighbors in decreasing edge weight order and available, since prop messages were sent, but no reply came back yet, either positive or negative (a positive answer will move the answering node to k, and a negative answer will remove it from p ). since we only need to search for locally heaviest edges in sets p i \\k i and p j \\k j, we can replace sets u i, u j with p i, p j respectively in equation (28) and prove that the new condition holds at some point during the algorithm's execution:"
"the lid algorithm is a 1 4 ( proof: by lemmas 11 and 4, we know that algorithms lic and lid choose the same edges for each node and therefore produce the same solution. this means that the lid algorithm is also a 1 2 -approximation algorithm for the many-to-many maximum weighted matching (as theorem 2 suggests for the lic algorithm). furthermore, the many-to-many maximum weighted matching we solve, by theorem 1, is a 1 2"
"cohorts of patients present in the studies were up to 190 patients, with an average of more than 50 patients. recently there has been a number of studies with a relatively high number of pd patients using wearable sensors, showing increasing usage and interest in clinics and research. the presence of healthy matched subjects (not present in all studies) would be advisable. the characterization of patients with at least the h&y stage is advisable, preferably adding further information such as the updrs motor score."
"regarding the type of gait tests, we excluded the ones with a walking distance of less than 5 meters in order to obtain only tests where continuous walking was less affected by the start and stop phases of gait. we observed a variability in the distance walked and in the type of gait test, which may be an important aspect to investigate further which could also benefit from standardization."
"in this section, we present a distributed algorithm, in adaptive and non-adaptive forms, that solves the many-to-many maximum weighted matching problem with an approximation ratio of 1 2 . the nonadaptive version of the algorithm is presented in section 5.1, followed by the analysis of its correctness and convergence properties in section 5.2. sections 5.3 and 5.4 present the adaptive version and its analytical properties, respectively. finally, section 5.5 shows that the solutions of both variations are a 1 2 -approximation for the many-to-many maximum weighted matching problem."
"the networks used during the experiments were power-law and random networks, created with the barabási-albert (ba) [cit] and erdős-rényi (er) [cit] procedures, respectively, having mean degree and initial (joining) degree of 0.05n and 0.05n, respectively (where n is the network size). these networks were selected for their different node degree distributions: in power-law networks, the vast majority of nodes has a very low degree, and few nodes have a very high degree (power-law distribution), while in er random networks, all nodes have comparable degrees (binomial distribution). in fact, high degree nodes in ba networks are connected mostly with many low degree ones, which leads to the creation of very different neighborhoods around individual nodes for these two network types. this difference, coupled with the adaptivelid algorithm's ability to perform local repairing operations, leads to the varying behaviors that can be seen in the experiments below."
"this review aimed at showing the state of the art in performing gait analysis with wearable sensors and the related variability across studies regarding characteristics such as the number and positions of the sensors, the reported spatio-temporal parameters, and so on. consensus on the assessment protocols and selected parameters is still missing with wearable technology, which is a new technology still maturing. still, it offers the possibility to be used in many applications and it provides the assessment of known and novel digital gait parameters. this review can be seen as a"
"klog assumes a closed-world, which means that atoms that are not known to be true, are assumed to be false. for extensional signatures, this entails that all ground atoms need to be listed explicitly in the relational database of interpretations. these atoms are generated automatically by the klognlp module based on the klog script and the input dataset. considering the defined attributes and relations in the model presented in listing 1, the module interfaces with nlp toolkits to preprocess the data to the relational format. the user can remove unnecessary extensional signatures or modify the number of attributes given in the standard klognlp script as given in listing 1 according to the needs of the task under consideration. an important choice is the inclusion of the sentence signature. by inclusion, the granularity of the interpretation is set to the document level, which implies that more context can be taken into account. by excluding this signature, the granularity of the interpretation is set to the sentence level."
"the mean satisfaction in the network achieved by both algorithms for a variety of network sizes can be found in figure 8, along with the values of minimum and maximum satisfaction in the network. note that satisfaction is slightly lower in the case of ba networks, due to differences in topology (i.e., minimum satisfaction is lower, due to the large amount of low degree nodes), but it follows the same behavior as in the er case. it is easy to see that the algorithms achieve consistently high satisfaction values, which are also increasing as network sizes increase. of particular interest is that: (a) the minimum satisfaction in the network is being increased, also, meaning that individual nodes enjoy high levels of satisfaction, too, implying asymptotically improved fairness properties, as well; and (b) the minimum satisfaction does not significantly affect the mean satisfaction, which implies that the number of nodes having low satisfaction is consistently very low compared to the size of the network. even though the reconvergence results showed that the adaptivelid algorithm can efficiently repair its solution once churn stops, it is interesting to see the levels of achieved satisfaction while churn is in progress. the relative satisfaction for er networks under churn (to the one achieved before churn starts) can be found in figure 9 : the different graphs from top to bottom correspond to the relative satisfaction when churn affects 5% to 50% of the network's nodes (in steps of 5%), for a network of 100 nodes. it is obvious that the amount of satisfaction achieved remains fairly constant during churn and depends greatly on the amount of churn. however, even though churn is an intense operation, it is possible to retain a significant percentage of the original satisfaction, even for churn as high as 50% (i.e., when half of the network is changing at every round). figure 9 . satisfaction while churn is in progress, affecting 5% to 50% of the network's nodes (in steps of 5%, top to bottom). on the other hand, the satisfaction drop is significant compared to the one caused by preference change. figure 10 shows the relative satisfaction for both preference change and churn on er networks, right after the change happens, for various amounts of affected nodes, supporting our argument in favor of improving connections instead of rebuilding them from the beginning."
parkinson's disease (pd) is a chronic neurological disease characterized by deterioration of the dopaminergic neurons in the brain. patients suffering from this disease manifest disorders that affect motor behavior and compromise the quality of life.
"to the best of our knowledge, the distributed algorithm presented here in both its adaptive and non-adaptive variations is the first method able to solve the b-matching with preferences problem with satisfaction and convergence guarantees. we expect that this contribution will be helpful for future work in the area, since the method can facilitate overlay construction with guarantees in a wide range of applications, from peer-to-peer resource sharing, to overlays in intelligent transportation systems and adaptive power grid environments. finally, interesting paths of future research would be to develop variations of the proposed algorithm that can give minimum satisfaction guarantees individually to each collaborating peer or that can take into account scenarios in which malicious nodes actively try to disrupt the algorithm's execution."
"besides the reported and structured analysis based on table 1, we also observed further promising approaches for assessing pd patients using wearable imus, that could be relevant for further investigation. first, besides the reported parameters, which are the most common in studies performing gait analysis, other potentially meaningful parameters were also collected in some of the reported studies. as an example, besides the already reported arm swing asymmetry, these were: stride regularity, harmonic ratio, and turning characteristics (for the tests that included turning). with motor asymmetry being a characteristic of pd-related abnormalities in locomotion [cit], bilateral measurements and asymmetry parameters seem promising disease-specific sensor metrics. thus, obviously our review is not exhaustive and further work may be done for other sets of parameters."
"this review aimed at showing the state of the art in performing gait analysis with wearable sensors and the related variability across studies regarding characteristics such as the number and positions of the sensors, the reported spatio-temporal parameters, and so on. consensus on the assessment protocols and selected parameters is still missing with wearable technology, which is a new technology still maturing. still, it offers the possibility to be used in many applications and it provides the assessment of known and novel digital gait parameters. this review can be seen as a first step toward defining a consensus set-up (or a number of set-ups that can be suggested for different parameters), and a minimum subset of parameters that should be present in all studies. furthermore, several points to increase standardization, replicability, and comparison across studies were raised."
"besides, an extensive experimental study of the proposed algorithms encompasses a variety of scenarios, including ones that put the adaptive algorithm under heavy stress and that have been previously used in the literature to simulate network attacks. in these scenarios, the adaptive algorithm succeeds in maintaining a reduced but steady level of network service while under attack and resumes to normal service levels after the attack stops. furthermore, both algorithms show attractive properties with respect to the satisfaction they can achieve and the convergence time (and hence overhead) needed. regarding changes in particular, the experiments clearly strengthen the argument that it is preferable to improve connections and adapt to changes instead of rebuilding all the connections from the ground up."
"the entities in our model also have a primary key, namely wordid and sentid for words and sentences respectively. additional properties can be attached to words such as the wordstring itself, its lemma and pos-tag, and an indication whether the word is a namedentity. this e/r model of figure 2 is coded declaratively in klog as shown in listing 1. the klog syntax is an extension of the logical programming language prolog. in the next step this script will be used for feature extraction and generation. ev-ery entity or relationship is declared with the keyword signature. each signature is of a certain type; either extensional or intensional. klognlp only acts at the extensional level. each signature is characterized by a name and a list of typed arguments. there are three possible argument types. first of all, the type can be the name of an entity set which has been declared in another signature (e.g., line 4 in listing 1; the nexts signature represents the sequence relation between two entities of type sentence, namely sent 1 and sent 2). the type self is used to denote the primary key of an entity. an example is word id (line 6), which denotes the unique identifier of a certain word in the interpretation. the last possible type is property, in case the argument is neither a reference to another entity nor a primary key (e.g., postag, line 9)."
"preference change affects both network types in the same way: a node that changes preferences destroys some connections, creating waves of changes in its neighborhood. for the case of ba networks, it may happen that a node changing preferences is a high degree one, causing a lot of nodes to repair their connections. however, this effect dies off quickly, since most of its neighbors are of a low degree, leading to an overall performance similar to the er case."
"it is easy to see from equation (1) minus a penalty if the connected nodes are not the top choices in the preference list. more specifically, using the previously defined connection list c i of node i, observe that for each connected node j the penalty is proportional to the difference between its rank q i (j) in the connection list of node i and its rank r i (j) in the preference list of node i:"
"is the ratio of the maximum and minimum neighbor list sizes in the graph, l max and l min respectively, which proves the desired approximation ratio."
"implying that the collection c achieves a greater value than c ′ for the maximizing expression of the truncateds b-matching problem. however, this contradicts the definition of c ′ ."
"in this paper, we present klognlp 1, an nlp module for klog. starting from a dataset and a declaratively specified model of the domain (based on entity-relationship modeling from database theory), it transforms the dataset into a graph-based relational format. we propose a general model that fits most tasks in nlp, which can be extended by specifying additional relational features in a declarative way. the resulting relational representation then serves as input for klog, and thus results in a full relational learning pipeline for nlp."
"the klog script presented in listing 1 can now be extended using declarative feature construction with intensional signatures. in contrast to extensional signatures, intensional signatures introduce novel relations using a mechanism resembling deductive databases. this type of signatures is mostly used to add domain knowledge about the task at hand. the ground atoms are defined implicitly using prolog definite clauses."
"due to the relational approach, the span can be very large. furthermore, since these features are defined declaratively, there is no need to reprocess the dataset each time a new feature is introduced, which renders experimentation very flexible 4 ."
"step time and step length are reported by 27.8% and 25% of the studies, respectively. variability of both stride and step characteristics is reported less than the normal value. among variabilities, stride time variability is the most reported one (22.2% of the articles), followed by the (related) variability of step time (11.1%), and the variabilities of step length (8.3%) and stride length (5.6%). right-left asymmetry is also reported for at least one of the parameters in the table (16.7%). very specific parameters related to the spatial position of the foot (foot clearance, heel strike and toe off angles) are reported only in two studies (5.6%). instead, time parameters related to the characteristics of the gait cycle ( figure 5 ) like double support, stance, and swing time are reported between 22% and 25% of the times. these parameters are reported as percentages or as durations (seconds). in the majority of studies, the spatiotemporal gait parameters derived were known or derivates of metrics already established in gait laboratory assessments using e.g., video-capture. thus, the parameters' value for outcome assessment or diagnostics was largely assumed as given. novel parameters enabled by the new sensor modalities such as for example gait complexity measures [cit] or body segment coordination variability [cit] have not yet been explored much and would require a more systematic validation approach for its construct, discriminative power, and minimal clinically important difference (mcid), or as a clinical study endpoint."
"which is a contradiction. for the second case, by lemma 7 and since we assumed that the changes are completed, we have that any potential canceling and reissuing of proposals finishes in a finite amount of time and therefore no oscillations occur."
"in klog, the feature set is generated in a combinatorial fashion by explicitly enumerating all pairs of neighborhood subgraphs; this yields a highdimensional feature space that is much richer than most of the direct propositionalization approaches. the result is an extended high-dimensional feature space on which a statistical learning algorithm can be applied. the feature generator is initialized using the new feature generator predicate and hyperparameters (e.g., maximum distance and radius, and match type) can be set using the klog flags mechanism (listing 2, lines 6-10)."
"step time and step length are reported by 27.8% and 25% of the studies, respectively. variability of both stride and step characteristics is reported less than the normal value. among variabilities, stride time variability is the most reported one (22.2% of the articles), followed by the (related) variability of step time (11.1%), and the variabilities of step length (8.3%) and stride length (5.6%). right-left asymmetry is also reported for at least one of the parameters in the table (16.7%). very specific parameters related to the spatial position of the foot (foot clearance, heel strike and toe off angles) are reported only in two studies (5.6%). instead, time parameters related to the characteristics of the gait cycle ( figure 5 ) like double support, stance, and swing time are reported between 22% and 25% of the times. these parameters are reported as percentages or as durations (seconds). in the majority of studies, the spatiotemporal gait parameters derived were known or derivates of metrics already established in gait laboratory assessments using e.g. video-capture. thus, the parameters' value for outcome assessment or diagnostics was largely assumed as given. novel parameters enabled by the new sensor modalities such as for example gait complexity measures [cit] or body segment coordination variability [cit] have not yet been explored much and would require a more systematic validation approach for its construct, discriminative power, and minimal clinically important difference (mcid), or as a clinical study endpoint."
"using the above definition, we effectively disregard the dynamic part of equation (4), making the prospective satisfaction increase ∆s j i of node i independent of the number of connections. we refer to the node satisfaction defined by equation (7) as truncated satisfaction and the corresponding maximizing satisfaction b-matching problem as a truncateds maximizing satisfaction b-matching problem. in the following lemma, we prove that by solving the truncateds maximizing satisfaction b-matching problem, we get an approximation of the original problem."
"klog employs a learning from interpretations setting [cit] . in learning from interpretations, each interpretation is a set of tuples that are true in the example, and can be seen as a small relational database. listing 3, to be discussed later, shows a concise example. in the nlp setting, an interpretation most commonly corresponds to a document or a sentence. the scope of an interpretation is either determined by the task (e.g., for document classification, the interpretations will at least need to comprise a single document), or by the amount of context that is taken into account (e.g., in case the task is sentence classification, the interpretation can either be a single sentence, or a full document, depending on the scope of the context that you want to take into account)."
"proof: for the static case, lemma 5 applies, and the algorithm terminates. for the dynamic case, a change may cause some proposals to be canceled and reissued on nodes at a distance of one or two (lemma 7). the only cases in which the algorithm may not terminate on these nodes is when they wait indefinitely for a neighbor's answer or their preference list \"oscillates\", with proposals being canceled and reissued on the same neighbors in an alternating way over time. by lemma 6, we can ignore weight updates, since they complete in a finite amount of time."
"the adaptivelid algorithm uses at each node i five sets (p i, k i, a i, r i, b i ) and an incoming message queue, queue i, and sends three kinds of messages (prop, rej and wake):"
"at the center of the algorithm above is the notion of the locally heaviest edge [cit], since the nodes send prop and rej messages in order to compare their heaviest edges and find the locally heaviest ones. note here that we assume unique edge weights, since it is important for the greedy algorithms described here to be able to recognize the locally heaviest edges in an unambiguous way (ties can be broken using node identities). however, globally unique edge weights are not necessary: it is sufficient for edge weights to be unique in the local neighborhoods of their endpoint nodes."
"in this section, we show how a simple modification connects the maximizing satisfaction b-matching problem with well known optimization problems, such as the maximum weighted matching."
"optionally, additional extensional signatures can easily be added to the knowledge base by the user, as deemed suitable for the task under consideration. at each level of granularity (document, sentence, or word level), the user is given the corresponding interpretation and entity ids, with which additional extensional facts can be added using the dedicated python classes. we will now turn to declarative feature construction. the following steps are inherently part of the klog framework. we will briefly illustrate their use in the context of nlp."
"regarding the type of gait tests, we excluded the ones with a walking distance of less than 5 meters in order to obtain only tests where continuous walking was less affected by the start and stop phases of gait. we observed a variability in the distance walked and in the type of gait test, which may be an important aspect to investigate further which could also benefit from standardization."
"on the other hand, nodes of both network types had full preference lists (consisting of all their neighbors), but ranked uniformly at random and with a desired number of connections equal to half their own degree. focus was given on random preference lists, since previous research [cit] showed that: (a) a strict matching solution cannot always be found when they are used; and (b) the measured satisfaction of unconverged instances can be relatively low. these characteristics make random preferences a challenging test case to evaluate the performance of the algorithms."
"is an ordered list of node i's connections in decreasing preference. satisfaction, due to its significance in the context of this paper, is discussed and analyzed separately in section 3."
"during the analysis of the distributed algorithm, we will use the notion of a locally heaviest edge [cit] : if we define the set e ij as the set of edges that have either node i or node j as an endpoint (but not both):"
"since klog is rooted in database theory, the modeling of the problem domain is done using an entity-relationship (e/r) model [cit] . it gives an abstract representation of the interpretations. e/r models can be seen as a tool that is tai- indicates, e/r models consist of entities, which we will represent as purple rectangles, and relations, represented as orange diamonds. both entities and relations can have several attributes (yellow ovals). key attributes (green ovals) uniquely identify an instance of an entity. we will now discuss the e/r model we propose as a starting point in the klognlp pipeline."
"an example network that illustrates this observation can be found in figure 2 . in this toy example, we assume that every node has only one free connection slot, and the preference lists are shown above the respective nodes. here, a maximizing total satisfaction configuration is depicted by solid lines connecting nodes a and b with c and d, respectively. however, this configuration is not stable, since nodes a and b constitute a blocking pair (each one prefers the other over their current partners). by resolving this blocking pair, we get the stable configuration depicted by a dotted line between nodes a and b (see figure 2) . however, using equation (1) to compute the total satisfaction of the network before (maximizing total satisfaction configuration s) and after (stable configuration s ′ ) the blocking pair resolution, we observe that the loss of satisfaction from nodes c and d offsets the gain from the resolved blocking pair, a,b:"
"it follows that the selfish choices of nodes a and b resulted in a network with decreased total satisfaction. this behavior also suggests that total satisfaction maximization may be preferable to traditional stability in overlay collaborative applications, where nodes pool together resources towards a common goal,i.e., construct an overlay network, and social characteristics are inherent. furthermore, in the above example, we showed that it is possible to achieve equal or greater satisfaction to the one achieved by the stable configuration, whose existence cannot be always guaranteed. this is in contrast to the maximizing total satisfaction criterion, which can always be applied, becoming a natural and sensible target goal to strive for in an overlay collaborative environment. note here that the maximizing total satisfaction criterion need not be applied necessarily to the whole network: any amount of nodes, working closely within the network, can apply it and benefit from it as a group."
"proof: when a node joins (leaves) the network, it gets inserted to (deleted from) neighboring nodes' preference lists, causing changes that need to be communicated to their own neighbors. the same happens to the node itself when it changes its own preference list. therefore, every change causes a weight update that propagates at a maximum distance of two and to a bounded amount of nodes (bounded by the size of a distance of two neighborhood from the originating node). note that the neighbor's neighbors (or the immediate neighbors in the case of simple preference change) accept the weight update passively and do not propagate it further. since we assumed that nodes do not fail and messages do not get lost, it is evident that all nodes are fully updated in a finite amount of time after the change."
"regarding the gait spatiotemporal parameters, significant differences were found between the selected articles, as indicated in table 1 . this is clearly related to the different set-up (number and positions of sensors that were used). as a clarifying example, we found that foot clearance was only calculated in two studies which used sensors on the foot. as expected, and as shown in figure 4, gait speed is the most common parameter reported in these studies (although not in all studies), appearing in almost 90% of them. other studies reported stride velocity, which should reflect the same underlying parameter. we grouped together the two in the table. cadence, which is also reported as step frequency, is present in 61% of the studies. stride length, which can be particularly useful to evaluate pd patients (who often may exhibit short steps), is present in 52.8% of the studies. stride time, which is also referred as gait cycle duration, has been calculated in 41.7% of the studies."
"one may even wish to compare the two situations from the point of view of what is a desirable action by a node who changes preferences: to improve existing connections or to perform a leave and come back (thus contributing to churn). this is especially meaningful in the case of er networks, since for ba networks churn is consistently cheaper in any case, due to their special structure and the parallelism mentioned above. in the case of er networks, comparing the reconvergence times of the two situations, churn has the advantage over preference change in high values. this is only natural, since in that case more nodes start with no connections and all possibilities are explored in parallel. in contrast, having high values of preference change means that more nodes want to repair their connections, but other nodes have already connections that they want to maintain, leading to longer times of reconvergence. it could be useful in practical terms if there were a mechanism able to detect a high volume of preference changes in the network and to enforce a policy of pseudo-churn, with nodes dropping all connections when changing preferences. however, as is shown below, the amount of satisfaction under churn is far less than the satisfaction under preference change before reconvergence, which is a significant argument in favor of improving connections instead of dropping them and starting again."
"we will first discuss extensional signatures, and the automated extensional feature extraction provided by klognlp, before illustrating how the user can further enrich the model with intensional predicates. listing 1: declarative representation of the klognlp model"
"our search selected 36 articles. they are summarized in table 1 . we reported the most commonly derived parameters for spatio-temporal gait analysis. regarding sensor locations, we grouped together imus on the lower part of the leg but listed sensors on foot or feet separately. the hoehn and yahr scale (h&y) is provided as a clinical rating scale evaluating progression in parkinson's disease stages. a better consistency and standardization in placing and in reporting should be done in the future. for example, \"shin\" and \"shank\" refer to the same part of the leg: the tibia. also, placement on the shins, ankles and on the feet can be ambiguous and mean different relative positions (lateral, front, different height or relative positions). furthermore, the asymmetry column is checked if in the article there is at least one parameter related to the right-left difference of one of the parameters in the table (thus, asymmetry of right-left arm, which is present in some papers, is not considered in table 1 ). step length"
"with other sensors) is used in total in 55.6% of the studies. other common positions are the position on both ankles (or tibias) and on both feet, found respectively in 41.7% and 30.6% of the articles. in many studies the position on the ankles/shins/shanks or feet are mutually exclusive, showing that they are mostly considered as alternative for gait analysis. the chest position is also present (25%). the chest and lower back are used both together and as alternatives. they both reflect trunk movement. sensors are also placed on wrists in some papers, mostly to detect arm swing and its right-left asymmetry (figure 3) ."
"using properties of the modulo operator to change the sum limits and since edge weights are symmetric to their respective endpoints (see equation (24)), we get the following:"
the following extensive experimental study complements the preceding analytical part with useful observations and conclusions about the behavior of the lid and adaptivelid algorithms in a variety of scenarios. focus was given on the performance of the algorithms in regard to the following points:
"proposal-refusal schemes in the literature, many of the algorithms for matching with preferences are inspired by the proposal-refusal algorithm of gale and shapley [cit] . here, we also utilize the proposal-refusal scheme, but in order to address a different problem with unique characteristics: while the gale-shapley algorithm is focused on absolute stability, we aim at solving an optimization problem with the maximum possible satisfaction. for example, in the case of the gale-shapley algorithm, it is important to guarantee that no cycles exist, since, given the distributed nature of the algorithm, a reply may not be possible to be given immediately by a node to another node's proposal. this is not necessary here, since any cycles in preference lists are broken by reducing the original problem to an acyclic weighted many-to-many matching, upon which the algorithm operates (cf. also lemma 8) . on the other hand, by focusing on optimization, we are able to develop both non-adaptive and adaptive variations of our algorithm in order to tolerate and work under changes in the underlying network (cf. lemma 10) ."
"we define an optimization variation of the b-matching problem, which we call the maximizing satisfaction b-matching problem, where the objective is to find a b-matching that maximizes the total sum of the nodes' satisfaction. later on, we also define a truncatedsmaximizing satisfaction b-matching problem, which is based on the same basic b-matching problem, but tries to maximize a different (truncated) satisfaction function (see section 4) (although we may refer to them simply as the b-matching problem and the truncateds b-matching problem, respectively, it will be clear from the context that we are referring to the optimization versions)."
the following lemma mirrors lemma 4 and is the key lemma in proving the equivalence of both centralized and distributed algorithms in the subsequent theorem.
"-approximation for the original maximizing satisfaction b-matching, where b max is the maximum connection quota in the graph. from the above theorem and lemma 10, we also get the following theorem about the approximation ratio of adaptivelid:"
"in the following sections, we will assume that the nodes following the algorithms proposed here, whether a group or the whole network, cooperate willfully, and we will provide guarantees about the maximization of the total satisfaction in this group or network."
"other related work in general, matching is a quite well-studied problem in various contexts (e.g., [cit] ). in the simplest forms of matching, the maximum cardinality matching and the maximum weighted matching problems, the aim is to find a subset of edges in which no pair of them shares a common endpoint and which includes as many edges as possible or exhibits the maximum sum of edge weights (if they exist), respectively. other popular variations, such as the stable marriages/roommates problems, assume that nodes have preference lists regarding their potential partner nodes and prefer to be matched to the ones of higher ranking in their preference list. in all of the above cases, the literature usually assumes a one-to-one matching of nodes, but even less studied variations of many-to-many connections appear to be solvable in polynomial time by centralized algorithms [cit], with the exception of particularly difficult subcategories of the stable marriages and roommates problems [cit] ."
"calculating edge weights in the way previously described enables the conversion of the original maximizing satisfaction b-matching problem of section 2 to a many-to-many maximum weighted matching problem, but as an added benefit it enables us to use the weight lists to make preference-based decisions, in a way similar to a b-matching problem. this new b-matching problem does not replace the original maximizing satisfaction b-matching: the nodes keep their original preference lists, but a new b-matching problem arises when they try to cooperate in order to collectively achieve a guaranteed level of connection quality. however, this new b-matching problem always converges regardless of the original problem, due to the symmetric nature of the edge weights [cit] . the following lemma expresses exactly this property by showing that the algorithm terminates for all nodes."
"in the adaptive algorithm, adaptivelid presented here, all three cases of dynamicity mentioned above (join/leave/change) are supported. in the case of a joining (respectively departing) node, neighboring nodes add (respectively delete) it to (respectively from) their preference lists. on the other hand, when a node changes preferences, no change occurs to the neighboring nodes' preference lists, but edge weights may change radically. a common thread between these cases is that the nodes directly involved in the operations must recalculate their marginal satisfactions for their neighbors and exchange them so that their adjacent edges have the correct weights. afterwards, they must re-evaluate their connections: if they are not locally heaviest any more, the nodes abandon the least weighted ones and try to get matched with the locally heaviest ones. note here that this method avoids the recalculation of the solution over the whole network, instead limiting it to a neighborhood around the network area where the dynamic operation took place. an additional benefit is that the involved nodes maintain their current connections unless proven to be non-optimal, i.e., they change them only if necessary."
"in the last step, different learning tasks can be performed on the resulting extended feature space. to this end, klog interfaces with several solvers, including libsvm [cit] and svm sgd [cit] . lines 11-15 (listing 2) illustrate the initialization of libsvm and its use for 10-fold cross-validation."
"in this section, we discuss the previous definition of satisfaction in depth, providing examples that motivate its usage and clarify the defining formula. furthermore, by digging into satisfaction, we get a derivative form of equation (1) to use directly in the algorithms of the following sections."
"to propose answers to these questions, we focus on the distributed b-matching problem: we suggest a novel modeling, present algorithms for both its adaptive and non-adaptive forms, show convergence and guaranteed approximation bounds and complement our results with an extensive experimental study."
"algorithm lic is a simple greedy algorithm with the distinctive feature of using only locally available information, by selecting locally heaviest edges in a centralized way. note that the comment of section 5 about the recursive nature of the locally heaviest edges is still valid here: by systematically removing from the edge pool p the edges we select (line 6, algorithm 3), along with any unselected edges of nodes with filled quotas (lines 8 and 9, algorithm 3), we get the same dynamics as in the distributed case (cf. lemma 11). in the following theorem, using a similar proof strategy to the one used by preis [cit] for his centralized one-to-one weighted matching algorithm, we prove that it achieves a -approximation compared to the optimum algorithm (opt) that selects edges with maximum weights over the whole graph."
"the average number of sensors used in these studies is 3.2 ± 2.4 (± one standard deviation) with a minimum of 1 and a maximum of 8. the most used set-up is with a single sensor (13/36) . when a single sensor is used, this is most frequently worn on the lower back (8/36). the lower back position (alone or with other sensors) is used in total in 55.6% of the studies. other common positions are the position on both ankles (or tibias) and on both feet, found respectively in 41.7% and 30.6% of the articles. in many studies the position on the ankles/shins/shanks or feet are mutually exclusive, showing that they are mostly considered as alternative for gait analysis. the chest position is also present (25%). the chest and lower back are used both together and as alternatives. they both reflect trunk movement. sensors are also placed on wrists in some papers, mostly to detect arm swing and its right-left asymmetry (figure 3) ."
"in the next section, we prove that it is possible to define a variation of the maximizing satisfaction b-matching problem (based on a modified definition of ∆s j i ) that approximates the original problem and can lead to a simple greedy algorithm."
"finally, some papers report reference values for their parameters, which could aid the exchange and comparison of results, while others do not. some also present values from healthy controls, which can help in evaluating the usefulness of parameters in characterizing pd impairments. furthermore, some papers perform comparison with gold standards (or use methods that were previously compared with gold standard technologies), while others do not. this may also impact on reliability and comparison across results. it is clear that every imu system used to quantify gait in pd should have been previously validated in pd patients with respect to a gold standard such as an optical motion capturing system."
"by the definition in section 2, at most, one locally heaviest edge can be attached to a node i at any specific point during the execution of the algorithm. however, once an edge is selected by node i, another one can possibly become locally heaviest in node i's neighborhood, and so on, until the algorithm selects enough of them. on the other hand, when nearby nodes fill in their quotas of possible connections, any unselected edges they might have with node i become unavailable. in order to express this recursive property of locally heaviest edges, we continue to use condition equation (3):"
"we define as available, with respect to node i, a node j in the neighborhood of node i that has neither been proposed by node i nor rejected node i."
"klognlp is most related to learning-based java (lbj) [cit] in that it offers a declarative pipeline for modeling and learning tasks in nlp. the aims are similar, namely abstracting away the technical details from the programmer, and leaving him to reason about the modeling. however, whereas lbj focuses more on the learning side (by the specification of constraints on features which are reconciled at inference time, using the constrained conditional figure 1 : general klog workflow extended with the klognlp module model framework), due to its embedding in klog, klognlp focuses on the relational modeling, in addition to declarative feature construction and feature generation using graph kernels. klog in itself is related to several frameworks for relational learning, for which we refer the reader to ."
"overlays represent a significant puzzle piece in contemporary and future networking infrastructure, whether they are intended to support resource sharing or other collaborative applications, such as searching, ad hoc connectivity and persistent services. the common scenario is that peers are able to know part of the overlay network (in terms of potential neighbors), but want to connect only to a small number of other peers in order to conserve resources. connection decisions are not to be taken blindly, but should rather be based on some suitability metric related to, e.g., the peer's distance, interests, recommendations, transaction history or available resources. in the fully distributed scenario, every peer may follow an individually chosen metric (that it may even not want to disclose to other peers), but still wants to be able to coordinate with others in order to improve the quality of its connections. we present algorithms that enable peers that follow them to achieve a guaranteed level of collective quality in their connections. they achieve that by disclosing a limited amount of metric information to their immediate neighbors, but not the metric itself and are, in fact, independent of any individual metric choices. at the same time, the algorithms proposed here are able to adapt and tolerate the high dynamicity commonly found in these resource sharing overlays, with peers leaving, joining or changing the metrics about their neighbors at any time."
"hereafter, sgi keeps running by monitoring the traffic condition on the network. upon a significant change of traffic distribution which leads to an overload on the controller, sgi carries out a greedy refinement function called incupdate to incrementally update the grouping in order to reduce the inter-group traffic. the refinement process runs iteratively and in each iteration, two groups (cgroups) between which traffic volume increases the most are merged (as sgroup) and split again to ensure minimized communication between the two new groups (ngroups). this is identical to finding a minimum bisection cut of a given graph, which can be accomplished efficiently in polynomial time [cit] . the refinement process will terminate when the workload of the controller meets some predefined threshold. the high threshold in the algorithm is chosen at runtime according to the controller hardware specifications, while the low threshold is usually defined as a portion (e.g., 90 percent) of the high threshold. the reason to have two thresholds is to reduce oscillation in the system. the pseudocode of the sgi algorithm is given in fig. 3 . the expected running time of the mlkp algorithm is oðjejþ, which is oðn 2 þ in our case. the running time of the sgi algorithm is dominated by mlkp as incremental updates will be applied only to single groups with much smaller sizes than the original graph."
"the traffic appears to be concentrated within some groups of hosts. for example, when partitioning the 6,509 hosts evenly into five groups using k-way partitioning, we observe that only less than 9.8 percent of the traffic traversed different groups. we define the centrality of a group as the ratio (in ½0; 1) of the intra-group traffic and the total traffic related to the hosts in this group. for the collected trace, the average centrality of the 5 groups is 0.853, indicating a very high concentration of the data center traffic."
"the architecture design of lazyctrl is depicted in fig. 2 . in our design, the network is separated into two parts: the core and the edge. we employ a hybrid control model where control tasks are handled by the distributed control mechanisms in lcgs at the network edge, complemented by a central controller."
"control link. a control link refers to a logical control channel (an ip tunnel or a tcp/ssh connection on top of the underlay network) via which the controller receives forwarding requests, and/or sends commands or rules to individual edge switches. the control link is extended from the secure channel between an openflow controller and an openflow switch by allowing the exchange of switch grouping and other related messages. when a control task cannot be handled by local control groups, packets will be forwarded to the controller and the controller will react to the edge switches by sending them flow rules or other commands, all through the control link."
"both distributed and centralized control approaches have some instinct limitations in scaling to large-scale data center networks. in general, distributed control such as link-state protocols depends on broadcasting to synchronize network states, which will be a disaster when the network becomes inconceivably large. moreover, the edge switches need to learn the locations of all hosts, leading to explosive forwarding table sizes. while eliminating the need for state synchronization in principle, centralized control suffers from the stress of handling frequency and resource-exhaustive events such as flow arrivals and network-wide statistics collection events at the controller, which consequently limits the scalability of network control in data centers."
"controller workload. we validate the effectiveness of lazyctrl by measuring the controller workload under traffic dynamics. we first conduct a comparison to standard openflow control (with the original floodlight implementation) using the real traffic trace. for lazyctrl, the initial grouping is done based on the first-hour traffic pattern and we test in both static and dynamic cases with and without incremental updates for the grouping, respectively. to further verify the consistency of the results, we expand the real trace by introducing 30% extra flows among the hosts that did not communicate with each other in the real trace during the time interval from 8 to 24. using the expanded trace, we test again in both static and dynamic cases. we compare the workload of the controller in the above cases and the experimental results are illustrated in fig. 7 . it can be observed that iþ lazyctrl can help achieve a significant level of workload reduction (about 61-82 percent) for the controller; iiþ the controller workload in lazyctrl is relatively stable during the day on the real trace, which is due to the fact that majority of the traffic growth happens among those \"strongly connected\" hosts inside local control groups, being transparent to the controller. iiiþ the controller workload can be significantly reduced when the incupdate function is applied due to the fact that the additionally introduced flows keep breaking the skewness of the traffic over time and thus grouping updates have to be applied continuously to adapt to the changes in order to prevent the controller from being overloaded. grouping update. in addition, we examine the update frequency of switch grouping on both the real and expanded traces. the update frequency results are shown in fig. 8 . it can be noticed that the incremental update function has very limited influence on the controller workload on the real trace. at the same time, the update frequency keeps at a very low level (10 updates per hour), indicating that maintaining a relatively effective grouping is feasible in practice. on the expanded trace, the cost for keeping the controller lazy is a reasonable increase in update frequency (with a maximum of 34 updates per hour)."
"a local control group is a group of edges switches whose clients are observed to have frequent mutual communication. these switches are grouped together by the controller and share the network state with each other consistently. each local control group employs a distributed control mechanism to take over the control workload of intra-group traffic from the controller. the distributed control mechanism inside each group is carried out by equipping each edge switch with some local forwarding tables that are maintained by the switches themselves. these local forwarding tables keep track of network states such as host-to-switch mapping inside the corresponding group. for each local control group, a designated switch (with some backups) is selected randomly by the controller, which is responsible for aggregating group-wide network states from the edge switches in this group and reporting them to the controller in an asynchronous manner."
"we also carry out measurements to examine the computation time in switch grouping generation on the three synthetic traffic traces. the results of applying the inigroup function in sgi with various group size limits are shown in fig. 6b . it can be seen that switch grouping can be accomplished as fast as less than 5 seconds and the grouping time is inversely proportional to group size limit. note that switch grouping is only carried out when there is a very significant change in traffic pattern and incremental updates are not possible to retain good grouping quality in reasonable time. during most of the time, applying the incupdate function is sufficient, which is more than one order of magnitude faster than the inigroup function."
"state dissemination consists of the mechanisms to spread and synchronize network states (e.g., the host-to-switch mapping) and updates in the control plane. in general, there are two types of state dissemination in lazyctrl:"
"ordering and informing edge switches. the controller orders all switches in a group according to the physical (mac) address of switch's management interface. this is for building a logical ring for failure auto-detection (detailed in section 3.5). the controller then delivers to each switch its neighbors on the logical ring. besides that, the controller will also inform the switches in a group with the designated switch id and some global timing and performance parameters such as the group size, the frequency to apply group synchronization or keep-alive heartbeats."
"we propose lazyctrl, a hybrid network control plane design for large-scale data centers, which seeks to bring laziness to the global controller. in the lazyctrl design, edge switches are grouped dynamically according to their communication affinity. the central controller devolves the coarse-grained control for frequent intra-group events to each switch group while handling infrequent inter-group and other specified (fine-grained) control tasks by itself. each switch group autonomously carries out distributed control within the group, keeping the intra-group packets in the data plane. the controller groups the switches in such a way that the size of each group is as large as possible to exhaust switches' memory (such as tcams) capacity while inter-group traffic is minimized to support the laziness of the controller."
"it has been demonstrated that full control and visibility over all flows are not always necessary and devolving some control authority to the data plane by proactively suppressing frequent events can result in better scalability in software defined data center networks [cit] . however, the right granularity of flows to be handled by the controller is still not clear (or hard to define). in this paper, we advocate a new solution for control devolvement in data center networks based on traffic locality. our idea stems from the observation that traffic distribution in data centers (especially those with multi-tenancy support) could be highly skewed, i.e., frequent communications are more likely to take place inside certain small groups of hosts. as a result, it is possible to shield the global controller from many frequent events inside these groups if distributed control mechanism is applied independently in each of the groups."
"similar to that of typical openflow control, in lazyctrl, the edge switches are configured to point to the central controller at the setup phase. besides generating the local control groups by invoking the sgi algorithm, the controller is also in charge of the following configurations for every group before the whole lazyctrl system comes into function."
"live/synchronized state dissemination. live state dissemination refers to the host discovery process driven by the end hosts via arp broadcasting in the bootstrapping stage and at virtual machine migration or removal. in the lazyctrl design, live state dissemination can be cascaded in three different levels: iþ upon receiving an arp request, the edge switch learns the source address by inserting or updating an item in its l-fib and then floods the request to all relevant local ports. iiþ if no local hosts (that are attached to this edge switch) answer this request and the requested destination cannot be recognized by the g-fib of the switch either, this request will be forwarded to the designated switch in this group for an intra-group \"broadcasting\". iiiþ further, if there is still no response from the hosts in this group, the request will be forwarded to the central controller, which relays the request to the designated switches in all other groups that contain hosts belonging to the relevant tenant (e.g., according to tenants' vlan settings)."
"nevertheless, our solution is also orthogonal to distributed designs in the sense that it employs a hybrid control model, aiming at trying best to offload frequent coarsegrained control tasks from the central controller and handle them using distributed control mechanisms near datapaths. therefore, the aforementioned research efforts for scaling flow-based fine-grained control is still applicable on top of lazyctrl to further mitigate the performance bottleneck at the controller and consequently improve control plane scalability in data center networks."
"the design of lazyctrl is based on the concept of grouping switches to form multiple local control groups. thus the quality of efficiency of the grouping is essential to the whole design. given a limit for the group size (determined according to empirical or historical data), a good grouping scheme is defined as one in which the inter-group traffic is small (in order to facilitate the laziness of the controller) and the computational complexity of the grouping algorithm is sufficiently low such that it can fast adapt to traffic dynamics. our grouping algorithm aims at satisfying the above principles and we base our design on solving the classical graph partition problem, with improvements on time complexity and support for incremental updates."
"given an intensity matrix w, the goal of the switch grouping problem is to find out a grouping scheme g such that the inter-group traffic intensity w inter is minimized. this problem is similar to the graph partition problem where the goal is to partition a given graph into k roughly equal components such that the total weigh of the edges connecting the vertices in different components is minimized (called k-way partitioning). the graph partition problem has been shown to be np-hard [cit] . the switch grouping problem differs slightly from the graph partition problem in terms of that the largest size of a group is strictly contained by a constant while the number of groups is variable."
"link failures indicate routing-related issues, e.g., packet loss due to link congestion or temporary routing loops on the underlay. we adopt detour routing based approaches to handle link failures in lazyctrl. when a data path failure occurs, for instance, between s n and s nà1, the controller will be notified and an alternative path will be chosen for delivering packets following s n ! s nà1 . for a failure on the control link between the controller and a switch such as s n, the controller will send a request to the upstream switch of s n on the failure-detection wheel, i.e., s nà1, to pass on the control message from s n to the controller. when a peer link failure occurs"
"a switch failure usually turns to be a reboot or a reset of the switch, especially in the case where edge switches are implemented with virtual switches in hypervisors. the controller is responsible for detecting the malfunction of the switch and then carries out the following actions: iþ informing the designated switch in the same group this switch failure and asking the designated switch to spread the temporary outage of the failed switch in the group in order to avoid unexpected detour routing requests; iiþ rebooting the failed switch remotely and checking its comeback periodically; iiiþ removing the outage signal and proactively triggering a state synchronization in the group when the switch is back to function. if the failed switch is the designated switch in the group, in addition to the above actions, the controller will select a new designated switch for the group. if backups are set for the designated switch, no single point of failure exists since those backups work simultaneously and will be fixed upon a failure independently."
"ctrl-if module is an interface for the switch to interact with the controller, which also implements the control link. unknown packets (from inter-group traffic) will be forwarded to the controller using openflow packet_in messages."
"we have completed a full implementation of lazyctrl based on open vswitch and the floodlight openflow controller. experiments on our prototype with both real and synthetic traffic traces show that an effective switch grouping is easy to maintain in multi-tenant clouds and the hybrid control design highly reduces the workload of the controller and provides lower delay in packet forwarding. as expected, the laziness we introduced to the controller decouples centralized control and complete visibility and consequently scale the system much better compared with totally centralized designs."
"g-fib: the g-fib of each edge switch is a replica of the l-fibs of all switches in the same group. to save storage space, we implement g-fib using bloom filter (bf), as the storage space required by a bf is independent from the number of elements it contains. the g-fib of each edge switch is comprised of multiple bfs generated from the l-fibs of all switches in this group. given an address of a virtual machine, each bf decides whether this address is under the corresponding edge switch. all the bfs together will return a vector of boolean values indicating the possible location of this address. note that it might happen that there are multiple possible locations for one address, which is resulted from the false positive of bfs. however, the false positive rate is predictable and controllable by spacetime trade-offs [cit] ."
we implement lazyctrl by extending the openflow protocol and developing edge switches and the controller based on open vswitch [cit] and floodlight [cit] . the source code of our implementation can be found on [cit] .
"peer link. a peer link refers to a logical control channels used for disseminating network states for address learning and updating among the switches in the same local control group. in principle, peer links would rely on multicasting. however, assuming native multicast support for the underlay may not be practical. therefore, our design adopts an alternative approach: the designated switch (or its backup, if any) gathers network states from every peer edge switch and then disseminates them to all other switches in the same group with multiple unicast messages."
"in contrast, the network edge is in charge of network intelligence, i.e., host-to-switch mapping. the layer two virtual networks (overlays) for providing connectivity for the edge switches are conducted by the network edge via encapsulation or tunneling on top of the underlying physical network core. as a result, one-hop distance can be assumed for each pair of edge switches. we introduce a hybrid control model for the control plane to handle network control tasks."
"section 2 reveals some observations that motivate our design. section 3 presents the lazyctrl architecture with design details. section 4 presents our implementation, followed by the performance evaluation in section 5. section 7 concludes the paper."
"lazyctrl also targets the scalability issue of centralized control in large-scale data center networks. the most salient feature of lazyctrl is that it carries out network control in the right granularity by exploring traffic locality in data center networks. we summarize the advantages of lazyctrl as follows. first, it solves the super-linear complexity problem by devolving control tasks to local control mechanisms. second, it prevents path-stretching by taking advantage of direct distributed control for local traffic-intensive communication identities. last, lazyctrl requires no modification on physical switches and it is very easy to implement."
"in order to handle the above circumstances, an asynchronized switch-driven state dissemination mechanism must be introduced. in lazyctrl this mechanism contains two aspects: iþ when an update event occurs at an edge switch, this switch sends its updated l-fib to the designated switch in the group via the peer link; the designated switch then relays this update to all the other switches in the same group to synchronize the group-wide network state. the designated switch then sends the update to the controller via the state link to synchronize the network state between the controller and the local control group. iiþ when the grouping of the switches has been changed, the controller sends the l-fibs of the switches in a new group to the designated switches in this group via the state links. the designated switch then \"broadcast\" the l-fibs to all the edge switches in this group for updating their g-fibs."
switch grouping management module handles the management of the local control groups. we base our implementation of switch grouping on the proposed sgi algorithm. a daemon module is introduced to handle the state reports from the designated switches in all groups and keep analyzing the changes in traffic pattern. regrouping will be triggered when iþ the workload of the controller suffers from an accumulated growth of up to 30 percent from last update or iiþ it has been two minutes since last update. setting up a minimum update interval (2 minutes here) is to prevent the oscillation caused by short-term traffic fluctuation.
"we evaluate the quality of the proposed switch grouping scheme by calculating the normalized inter-group traffic intensity (w inter as defined in section 3.3). fig. 6a depicts the results of applying the size-constrained mlkp algorithm (the inigroup function in sgi) to the traffic derived from each of the three synthetic traces with various numbers of groups. we observe that the grouping quality varies across different traces. in general for traces with higher average centralities, it tends to have smaller values for w inter, indicating better performance in reducing the workload of the controller. we also observe that w inter increases almost linearly with the increase of the number of groups, confirming that maximizing the sizes of single groups (thus consequently reducing the number of groups) will best facilitate the laziness of the controller."
"in the lazyctrl design, each edge switch is associated with a local forwarding information base (l-fib), which tracks the hosts or virtual machines that are attached to this switch. to handle intra-group traffic, each edge switch also maintains a replica of the l-fibs of all other switches in the same group, which we call group forwarding information base (g-fib). the central controller retains global visibility of the network by maintaining a central location information base (c-lib), which contains the l-fibs of all edge switches in the network. using this c-lib, the controller can handle inter-group traffic and any other specific flows whose control requires global visibility. a general overview of the table organization in the lazyctrl design is depicted in fig. 4 ."
"the traffic distribution is uneven among hosts. among a total of 6,509 hosts, only 11,602 of more than 20 million distinct hsrc; dsti host pairs exchanged traffic in the trace. and over 90 percent of the flows are contributed by about 10 percent of the host pairs that exchanged traffic."
"data center networks [cit], the excessive coupling of central control and complete visibility has brought many scalability challenges to both the network control and data planes in large-scale data centers. on the one hand, having the controller to set up all flows would bring too much workload to the controller and such centralized bottlenecks are difficult to scale. on the other hand, maintaining visibility of all flows in a large-scale network can require hundreds of thousands of flow table entries at each switch, which is far from practical for commodity switches."
"we conducted experiments based on a real traffic trace collected from an enterprise production data center in europe, which consists of 272 gige edge switches and 6,509 hosts. accordingly, we built a prototype system using 6 pronto 3,290 switches and 24 ibm x3550 8-core (two quad cores) servers. the switches were interconnected with a full mesh via 10 gige links, severing as the network core (ip-based underlay). each switch was connected with 4 servers via gige links. to emulate the 272 edge switches in the real data center, we deployed 272 linux virtual hosts running our modified open vswitch implementation on the 24 servers. a custom-made trace re-player was developed and deployed on each of the 272 linux virtual host to replay the inter-switch traffic generated by the 6,509 hosts in the trace. the floodlight-based central controller was hosted on a standalone linux pc (with intel core 2 duo cpu 2.2 ghz) and could be configured to run in either lazy or normal mode."
"to extend the scalability of the control plane, we introduce a hybrid control model in the lazyctrl design. this hybrid control model involves a central controller and a set of local control groups."
"encap action realizes packet encapsulation in edge switches by extending the existing openflow v1.0 protocol. in the lazyctrl architecture, packet forwarding in the data plane overlay replies on a gre-like encapsulation. when a rule with this action is applied to a flow, the switch will encapsulate the packets with a new header targeting a given remote ip address."
"2. with a bit abuse of notation, we will use host to refer to virtual machine that is running in a physical server. seen in the right figure, the controller clusters sa, sc, and se into the first group while sb and sd together form the second group. (we assume that the group size limit is three in our example.) this way, the traffic within the first group (e.g., sa$sc), as well as the traffic within the second group (e.g., sb$sd), can be handled by carrying out local control mechanism that is dedicated for each group. the controller then is only needed to take charge of the inter-group traffic, i.e., sa$sd. the switches will be dynamically regrouped in response to traffic variation."
"the switch grouping scheme ensures that switches in the same group are \"strongly connected\" due to their frequent traffic exchange. as a result, failures in the data plane can be passively detected quickly. in contrast, handling failures in the control plane is more laborious."
"our design for the switch grouping algorithm is based on the multi-level k-way partition (mlkp) algorithm proposed by karypis and kumar for fast k-way partitioning for a given graph [cit] . mlkp first reduces the size of the graph by collapsing vertices and edges. when a k-way partitioning of the smaller collapsed graph is found, the algorithm uncoarsens and refines this partitioning to construct a k-way partitioning for the original graph. the running time of mlkp is linear in the number of edges in the graph. however, direct application of mlkp to the switch grouping problem may lead to infeasible solutions, i.e., the sizes of the resulted partitions may exceed the given group size limit."
"ethernet stands as one of the most widely used networking technologies today due to its plug-and-play semantics such as automatic host location learning and flat addressing, which can highly simplify many aspects of network configuration and ensure service continuity. however, replying on network-wide dissemination of per-host information makes ethernet-based solutions difficult to scale and forcing paths to comprise a spanning tree introduces substantial inefficiencies. in contrast, ip networks can easily scale to large networks but require massive effort to configure and manage."
"as a promising solution for building large-scale data center networks, network overlay can exploit the advantages of both ethernet and ip networks. an overlay network in a data center consists in creating a dynamic mapping between the overlay (virtual) network and the underlying (physical) infrastructure. this mapping ensures that packets can be transmitted by the routing substrate between any pair of overlay nodes. however, in order to handle location resolution at network edge, a global location information base has to be maintained, which can be challenging in large networks."
"tenant information management module is used to manage tenant information such as vlan ids in switches. being aware of this information, the controller can determine where to spread the arp messages and when intergroup traffic control is necessary. failover module is in charge of failure detection and recovery as we have discussed in section 3.5."
"steady-state latency. steady-state latency refers to the average forwarding latency of all processed packets over a relatively long period of time (2 hours here). the experimental results on the real trace with a 24-hour span are illustrated in fig. 9 . it can be observed that on average a 10 percent reduction on latency can be achieved by lazyctrl compared with standard openflow. moreover, this improvement is a byproduct of reducing the workload of the controller as less load on the controller leads to higher processing speed. moreover, the synchronized state dissemination speeds up topology learning, which implicitly help reduce the response time of the controller."
"lazyctrl aims at finding out the right balance between distributed and centralized control and integrates the advantages from both sides to fundamentally solve the scalability issue of network control in data centers. the scalability of lazyctrl is interpreted in three aspects: table organization and state dissemination. due to the group size limit, the l-fib and g-fib on each switch will be constrained to limited sizes and thus do not have any scalability issue. switch-wide state dissemination is also constrained in specific group and is decoupled from the size growth of the data center. the controller is designed to passively receive network states from local control groups for state dissemination and address leaching, which scales as well."
"the core-edge separation enables one-hop \"logical\" distance between any pair of edge switches, leaving basic packet routing to the ip underlay. what remains unsolved is the host-to-switch mapping."
"host exclusion in switch grouping. when a edge switch is connected to hosts belonging to many tenants, it may be difficult for a greedy method to generate a grouping with superb quality. in this case, the controller can choose some hosts and exclude them from the grouping process. the control tasks for these hosts will be accordingly handled by the controller."
"lazyctrl realizes a hybrid control plane for data center networks. in this section, we discuss four aspects of its design: the architecture, the switch grouping scheme, the packet forwarding routine, and the failover mechanisms. we first provide a high-level overview to state the intuition of our design."
"the central controller has holistic visibility over the entire data center network and is responsible for iþ maintaining a central location information base (c-lib) which preserves host location information, iiþ adapting the grouping of the edge switches, and iiiþ managing the flow tables on the edge switches to handle inter-group traffic and any specific traffic that needs flexible centralized control. the goal of the central controller is to stay lazy by devolving as many control tasks as possible to the local control groups. the central controller can be a stand-alone physical server or a logical controller comprised of a cluster of servers with strong reliability and coherency of network state."
the floodlight openflow controller provides a rich set of components. the central controller in lazyctrl is implemented based on the existing floodlight controller by introducing the following extensions.
"lazyctrl is highly relied on traffic locality and thus, it fits very well in cloud scenarios with multi-tenant support as we have already discussed in section 1.2. on the other hand, having a good understanding on traffic pattern is also crucial to the effectiveness of the system. in practice, this can be well handled by traffic prediction and also the incremental updates we proposed can alleviate repetitive computation to a large extent. determining the right sizes for groups also plays an important role in keeping lazyctrl effective. intuitively, the larger the group size, the lower the expected workload for the controller due to less inter-group traffic. on the other hand, the larger the group size, the higher the control overhead on the switch side, as a larger group means more network states to spread among the switches in the group and more l-fibs and g-fibs to maintain. compared with empirically driven or static group sizes, we believe that a dynamic group size negotiation between the controller and the switches can be helpful, as networks can be heterogeneous and the switches might differ significantly in terms of performance and capacity. furthermore, the flexibility of ondemand group size makes it possible for the controller to customize its workload (e.g., during peak hours). as an alternative, we also implement a game-based (modified rubinstein bargain model) dynamic group size limit negotiation approach in lazyctrl. before the controller calculates the grouping, the switches are allowed to dynamically bargain the group size limit with the controller according to their real-time monitored and self-evaluated data."
"cold-cache forwarding latency. we evaluate the forwarding latency under \"cold-cache\" scenarios upon the first packet of a fresh flow is injected into the network. we emulate coldcache scenarios by launching 45 new flows among five newly deployed hosts and compare the average forwarding latency of the first packets of these flows in lazyctrl to that in the standard openflow control. for intra-group traffic, the coldcache forwarding latency in lazyctrl (0.83 ms) is more than an order of magnitude smaller than that in openflow (15.06 ms). this is due to the fact that packets from intra-group traffic will be forwarded locally without involving the controller. the data plane operations such as l-fib lookup and packet encapsulation are very fast and thus packets can be processed at line speed. for inter-group traffic, lazyctrl also outperforms standard openflow by achieving a cold-cache latency of 5.38 ms. this is because lazyctrl requires no passive learning of the network topology through all arp flooding as is the case of standard openflow (the learningswitch module in floodlight), which is another benefit brought by the lazy principle in lazyctrl."
"in the lazyctrl design, we propose a self-detection mechanism to handle failures in the control plane based on a group-wide failure-detection wheel with the controller at the center and the switches at the edge. as we have mentioned previously, at the setup phase the controller orders the switches to form a wheel and informing the switches in the same group their neighbors on the wheel. to detect failures, keep-alive messages will be initiated from upstream switches to downstream switches and from the controller to each switch. all possible cases of failures depending on the place of packet loss are listed in table 1 ."
"the real traffic trace we collected consists of the traffic among 272 gige edge switches and 6509 hosts over a whole day. to check the consistency of the performance results under different traffic scenarios, we generated three synthetic traffic traces based on the real trace. the main characteristics of all traces are summarized in table 2 . in the synthetic traces, traffic is assumed to be exchanged through 2,713 edge switches among 65,090 hosts, with a scaling-up factor of 10 compared with the real trace. the traffic flows in the synthetic traces were generated in the following manner so that the key characteristics such as the temporal patterns could be retained: p% of the flows are generated by selecting from a given set of host pairs (q% of all host pairs) uniformly at random in the synthetic topology, and assigning each selected host pair a payload randomly chosen from the real trace. we vary the values for p and q and three synthetic traces are generated with significant differences in traffic locality represented by average centrality. the rest flows are generated by selecting host pairs uniformly at random from all host paris in the synthetic topology. each selected host pair is assigned with a payload randomly chosen from the real trace."
"we describe now how traffic control is carried out in lazyctrl. the detailed forwarding routine of a packet is shown in fig. 5 . when a packet arrives at an edge switch, depending on packet type, the following two actions will be applied: iþ if the packet is plain (which originates from a local host), the switch first carries out a lookup in its flow table to check whether there are matched rules for this packet. if so, the action corresponding to the rule is then applied to the packet; otherwise, the switch continues looking up in its l-fib to check whether the destination of this packet is a local host. a packet with an address of a local host will be forwarded directly to that host. if no entry matched the l-fib, the switch carries out a query in its g-fib. note that there might be multiple targets for this packet returned from this query due to the false positive of bfs. the switches then send to all the targets a copy of the packet. if all the elements in the boolean vector are false, it means that the target of this packet is not in the current group and thus the packet will be forwarded to the controller to request inter-group control rules. iiþ if the packet is encapsulated, the switch first decapsulates it and then carries out a lookup in its l-fib to determine its destination host. if no matched entries are found, the switch simply drops the packet as it knows that this packet is mis-forwarded to the switch due to bf's false positive. optionally, this mis-forwarded packet could also be directed to the controller for installing flow entries on related switches to avoid further false positive for the same destination."
"failure detection and failover. clustering switches into \"strongly connected\" groups with limited sizes simplifies the process of failure detection and recovery of a large network system as failover tasks can be carried out independently in each of the groups. in addition, control authority in lazyctrl is shared by local control groups and the controller, avoiding a single performance bottleneck."
"preload for seamless grouping update. during grouping updates, the l-fibs on the related switches will be modified, leading to forwarding interruptions. to relieve this, the controller can preload some rules to the related switches to temporarily handle the control tasks for them. these rules will be removed when the grouping becomes stable."
"the main forwarding component of open vswitch consists of the ovs-vswitchd and datapath modules. the ovsvswitchd module works in the user space, handling slow path processing such as learning, remote configuration, full flow-table lookup; the datapath module in the kernel space handles fast path processing including packet forwarding, quick-table lookup, modification, and tunneling. the implementation of the lazyctrl edge switch follows a similar design principle. fast path processing, such as l-fib lookup (including bf matching), packet encapsulation, and forwarding, are integrated into the kernel space (datapath) module while a few slow path modules are integrated into ovs-vswitchd which are listed as follows."
state reporting module will only be activated when the switch is selected as the designated switch for the group. this module implements all functions associated with the state link.
"acceleration by parallelism. the incupdate function in the sgi algorithm can be easily parallelized by carrying out merge and split operations simultaneously for multiple group pairs. consequently, the computation overhead brought by the regrouping process can be further reduced."
"we propose sgi, a size-constrained grouping algorithm with incremental update support. in the initial stage (function inigroup), sgi first determines the right number k of groups to be generated. this value can be estimated by the number of switches divided by the group size limit. next, sgi constructs an intensity graph where the vertices in the graph represent all the switches while each edge represents the communication between the two end switches of this edge. the weight on each edge indicates the normalized traffic intensity between any pair of switches, which is estimated based on history traffic statistics. then, an initial feasible grouping of the switches is produced by using the mlkp algorithm with the constructed graph as input."
"state link. a state link is a logical communication channel between the controller and a designated switch. the designated switch in each group aggregates the network states it collects from other edge switches in the group and reports them to the controller periodically via the state link. thus, global and coherent visibility can be achieved at the controller."
fib maintenance module maintains the l-fib and the bloom filter based g-fib structures according to the network states collected by the state advertisement module and then updates the kernel space module for fast path processing.
"in conventional flow-based centralized control environments such as those based on openflow [cit], the controller maintains the network-wide state (the host-to-switch mapping here) and handles all the flows between switch pairs that exchange data, bringing extremely high burden to the controller. lazyctrl mitigates this problem by clustering the switches into multiple switch groups according to their communication affinity and devolving intra-group control to these switch groups (termed local control group, lcg). 1 to support its laziness, the controller prefers clustering the switches into a few big groups in order to reduce inter-group communication. however, larger group size would result in larger distributed forwarding tables and more control tasks inside each local control group. due to the limited size of high-speed memory in switches, the largest size of a group will be constrained by some constant. the controller clusters the switches in such a way that the size of each group is maximized under a given limit while the inter-group traffic volume is minimized. example 1. consider a multi-tenant cloud data center containing a central controller and five edge switches (namely sa, sb, sc, sd, and se) with hosts 2 directly attached. we focus on the scenario shown in fig. 1 . there are three tenants, a, b, and c, each of which has some virtual machines. the left figure illustra tes the case when centralized controlling is applied directly and thus the central controller has to handle all the flows among all edge switches. lazyctrl changes this situation by clustering edge switches into independently groups. as can be 1. we will use group and local control group (lcg) interchangeably in the rest of this paper."
"in this paper we present lazyctrl, a novel hybrid control plane design for data center networks. lazyctrl is based on a core-edge separated architecture and the control functionality is implemented in a hybrid fashion: frequent coarsegrained control tasks are largely devolved to network edge by clustering edge switches into local control groups according to traffic locality and carrying out distributed control independently inside each group; the central controller is only in charge of very limited number of intergroup or other fine-grained control events. the central controller keeps adapting the grouping of edge switches to maintain its laziness. our evaluation on the lazyctrl prototype with both real and synthetic traffic traces show that lazyctrl can help reduce the workload of the central controller by up to 82 percent, improving the scalability of standard openflow to a large extend. moreover, lazyctrl is fully compatible with existing solutions for scaling flowbased centralized control to large networks."
"location resolution. location resolution responsibility in lazyctrl is shared among switches and the controller. switches handle the majority of tasks locally in local control groups while the controller is only involved when intragroup control is not sufficient. as a result, the workload of the controller can be kept at a very low level, mitigating the scalability issue."
"selecting designated switches. for each local control group, the controller selects a designated switch among all edge switches in this group by applying some given principle such as shortest physical distance, shortest response time to the controller. if necessary, the selection process also includes choosing some backups for the designated switches."
"(between s n and s nà1 ), the control functionality is affected only when one of the two end switches is the designated switch. in this case, the controller will ask s n or s nà1 to quit as the designated switch and reselects one from the backups for the designated switch to fulfill the role of designated switch."
"in cloud data centers, the traffic among the hosts is usually unevenly distributed and is strongly localized within some groups of hosts. to verify the correctness of this notion, we collected a day-long traffic trace from a production data center in europe running multi-tenant applications and made the following quantitative findings:"
"for multi-tenant cloud data centers, we observe that the number of virtual machines for a single tenant is changing slightly, while the number of tenant users, as well as the total number of hosts in a multi-tenant data center, is experiencing a significant increase. for amazon, a popular cloud service provider, the number of tenants, as well as total virtual machine instances of amazon's ec2, grew about 2.5 [cit] . the total number of objects held by amazon s3 has grown 150 [cit] . in contrast, the size of a specific tenant in terms of number of rented virtual machines is constantly around 20-100 [cit] . these facts consequently lead to the property that traffic is aggregated within some size-limited groups of hosts in multi-tenant data centers as the traffic exchanged among different tenant slices is very limited. by taking full advantage of this property, we show that the explosive increase in the number of tenants does not necessarily result in scalability issues for centralized control in data center networks."
"our design splits the core from the edge. the network core can be any simple and scalable network (e.g., an ip unicast network), which serves as the underlay providing connectivity for edge switches. the core-edge separation releases the network core from handling complicated and dynamic network control tasks (e.g., network virtualization, virtual machine migration) and thus allows the network core to be constrained only by performance and reliability. since our focus is the control plane, we omit the detailed design of the network core."
"asynchronous state dissemination. when the traffic pattern changes, the grouping of the switches may not be effective for shielding the central controller thus needs to be adjusted. the condition of inter-and intra-group traffic is also changed. therefore, the host-to-switch mapping must be re-disseminated across the control plane in order for local control groups to handle all intra-group traffic. this is different from the case of virtual machine migration in the sense that there is no change to the host-to-switch mapping. as a result, the end hosts cannot sense this change and thus cannot accordingly drive any updates. moreover, in an extreme case where all hosts from a certain tenant appear in the same local control group, the controller may want to block all arp request from that tenant to avoid unnecessary workload of itself. however, this could lead to incomplete visibility for the controller as the traffic from that tenant will be transparent to the controller."
"with the rapid evolvement of sdn, flow-based centralized control has been recently adopted as a mainstream control plane design for data center networks. as one of the first sdn solutions for enterprise networks, ethane [cit] enables the direct application of fine-grained flow-based policies to the network by coupling flow switches with a centralized controller. however, exposing all flows to the controller could bring too much workload to the controller, leading to poor scalability. even after applying multithreading optimizations that help achieve graceful linear core scaling factors [cit], the gap between actual and desired performance of the centralized controller is still very significant. it was shown that the popular openflow controller can only be able to handle approximately 30 thousand flow initiation requests per second on commodity x86 platforms [cit] . unfortunately, a small network consisting of only 100 switches could have a spike of more than 10 million flow arrivals per second [cit] . even after applying multithreading optimizations that help achieve graceful linear core scaling factors [cit], the gap between actual and desired performance of the centralized controller is still very significant."
"the data of an actual three-dimensional urban space framework have increasingly become an important foundation of urban construction and development [cit] . the virtual urban traffic scene in this study was built based on the geographical data of qingdao, as shown in figure 2 . we visualized the terrain data and the surface model data to build a virtual traffic geographic environment, especially detailed with respect to traffic crossings and traffic police. in this system, an interface was reserved for traffic crossing and was used to communicate between the user action recognition system and the vge. the users interacted with vehicles in the virtual scene by making traffic gestures and hence were able to learn about the traffic in the vge."
"case 1: previously we have provided a method of marking the neighbor nodes of previously clustered nodes. it is stated that the un-clustered neighborhood nodes of the respective last hops of the clusters, that are formed already, are marked. now one of these nodes, that are marked for not to be selected as cluster head, is randomly chosen. the chosen node checks its immediate neighborhood for nodes available and unmarked (i.e. unclustered and eligible of being selected as a cluster head) for selection of next cluster head. if there is a number of nodes are available, then any one of them can be selected as cluster head. so we have adopted a randomized approach toward selection of cluster head i.e. one of the sorted out nodes is selected randomly. this is shown in figure 3 . case 2: beside the first method of selection, another approach towards head section is given equal priority. this approach mainly focuses upon reduction of the message count in course of selection of cluster heads. we have a set of nodes that cannot be selected as cluster head. so we can easily select any one node among the rests (i.e. nodes not included in that set) as cluster head. this approach is also randomized. any one of the nodes, available for selection as cluster head, is selected randomly as the next cluster head. this is shown in figure 4 ."
"after the tpcgs dataset with attributes of time and spatial domains was obtained, using the st-cnn model proposed here, the spatiotemporal characteristics could be fully analyzed, and the gestures of traffic police could be recognized."
"people now have a strong dependence on traffic, and requirements with respect to such traffic have recently been put forward. the concept of smart traffic aids in governmental decision-making and management and reduces traffic accidents [cit] . in traffic systems, traffic command gestures help to alleviate traffic jams. the virtual traffic command gestures experience system proposed in this paper is helpful. the intelligent recognition of traffic command gestures can promote traffic safety awareness. users experience the traffic police command process in the virtual environment. when users actually walk or drive on roads, they are able to identify traffic police's actions accurately, so as to prevent traffic accidents."
"the rest of this paper is organized as follows. section ii presents the model of the proposed c-fbmc-im system. in section iii, we give the advantages of c-fbmc-im by comparing the interference power. besides, the ml detector and the llr detector are upgraded for the c-fbmc-im system. simulation results of the c-fbmc-im system are 978-1-5090-5935-5/17/$31.00 ©2017 ieee presented in section iv. finally, section v concludes the paper."
"an intelligent traffic command gesture recognition system cannot work without a virtual geographic environment (vge) . vges provide open virtual environments that correspond to the real world so as to assist in computer-aided geographic experiments. four subenvironments include (1) the data environment, (2) the modeling and simulation environment, (3) the interactive environment, and (4) the collaborative environment [cit] . at present, people pay more attention to traffic scene. section 5 mainly describes the experimental process. the virtual police gesture command system is shown in figure 1 . figure 1 . the virtual city traffic scene is constructed, and the intersection modeling is emphasized. (a) is a part of the virtual urban scene. the traffic police command gesture recognition system is set up to facilitate real-time human-computer interaction (hci). communication between the traffic scene and the identification system is achieved. a volunteer is making traffic police command gestures in the real environment in (b), which is mapped to the virtual scene in (c)."
"we created a tpcgs dataset. the dataset uses depth trajectory data based on skeleton points. compared with the video stream, the depth trajectory data features are more precise. the dataset provides a new means of identifying traffic police command gestures."
"as one of the multicarrier modulation (mcm) systems, orthogonal frequency division multiplexing with index modulation (ofdm-im), which can be regarded as an extension of the spatial modulation (sm) concept [cit] into frequency domain, has attracted considerable interest recently [cit] . at the transmitter of the ofdm-im system, the subcarriers are divided into groups and the active subcarrier indices are used as a source of information. at the receiver, the subcarrier indices are detected by a maximum likelihood (ml) detector or a low-complexity log-likelihood ratio (llr) detector [cit] . by the utilization of subcarrier indices as a source of information, the ofdm-im system exhibits an improvement in bit error rate (ber) performance compared to the conventional ofdm system, especially for low-to-mid rate cases [cit] ."
"after a tpcgs dataset with time and spatial domains was obtained, the st-cnn model proposed in this paper could be used to fully analyze spatiotemporal characteristics and recognize the gestures of traffic police."
"the gesture recognition algorithm was performed with python on a pc with a 3.40 ghz cpu and an 8 gb memory. the skeletal point data were collected by a kinect 2.0 sensor. the 10 volunteers simulated the traffic police command gestures for 6 key skeletal points. based on the 3d skeleton point data provided by kinect 2.0, the input signals of the st-cnn model were obtained by normalization. the input signals of the eight gestures were visualized as shown in figure 6 . the tpcgs dataset includes the sequential position information in continuous space. the features of the spatial dimension and the temporal dimension were extracted continuously by the movement of the convolution kernel. the st-cnn model was thus established. the model could be migrated to other pc devices or mobile devices for real-time recognition of traffic police command gestures, servicing intelligent transportation and a smart city. we set up the virtual urban scene, as shown in figure 1, and the traffic police command scene was the area of focus. the actions of volunteers were mapped to traffic police actions in the scene that controlled vehicle operations. for urban scene construction, we focused on the implementation of traffic police gesture interaction."
"recent advancement in technology made the manufacturing of small and low cost sensors technically and economically feasible, which are composed of a single chip with embedded memory, processor, and transceiver. in general such sensors are also equipped with data processing and communication capabilities. the technological advances in micro-electro-mechanical systems and low power and highly integrated digital electronics have led to the development of micro-sensors [cit] . the sensors measure ambient condition in environment surrounding it and then transform them into an electric signal. processing such a signal reveals important properties about objects located and/or events happening in the vicinity of that particular sensor. a large number of these disposable sensors can be grouped or networked in many applications that require unattended operations."
"these two types of selection method are given probability of 0.5. the probability can be set to any value within 0.3 to 0.7. we have chosen 0.5 to provide both operations equal occurring probability since these methods don't perform well alone. if both of these methods fail, then we check for nos. of un-clustered and unmarked nodes. if the number is high enough our algorithm will randomly select one of these left-out nodes as cluster head in the high density region."
"in this paper, we build a c-fbmc with im (c-fbmc-im) system model in matrix form. the spectral efficiency and error performance of the c-fbmc-im system are investigated. as some subcarriers carry nothing but zeros in the c-fbmc-im system, the minimum mean square error (mmse) equalization bias power will be smaller comparing to the conventional c-fbmc systems. as a result, our c-fbmc-im system outperforms the c-fbmc system. the simulation results demonstrate that 1db performance gain can be achieved for the proposed c-fbmc-im system when the spectral efficiency of the c-fbmc-im system is 6.25% higher than that of the c-fbmc system."
"the tpcgs dataset was constructed in order to efficiently and accurately recognize gestures. the tpcgs dataset is comprised of skeleton point positional information, which was obtained via a kinect 2.0 sensor. it covers all eight kinds of chinese traffic police command gestures."
"we have proved that our proposed smcp is very efficient in terms of energy awareness, message count, nos. of formed cluster etc. there are also scopes of modification in smcp. we have implemented sequential clustering in this paper. our further work will be focused on implementing a simultaneous clustering protocol i.e. all the cluster should form simultaneously for time saving."
"the tpcgs dataset was built to compensate for the lack of a gesture dataset for traffic police command gestures. a new deep learning method for extracting features is presented here for the recognition of traffic police command gestures. ultimately, a virtual urban traffic intersection environment was built to test the model, and the model was found to be stable and robust."
"where y, x and n are the frequency response of y, x and n, respectively. then, the equalization is processed in frequency domain and the channel equalizer diagonal matrix can be expressed as"
"in the traffic police gesture recognition module, we obtained a scientific deep learning model based on tpcgs dataset after continuous experiments and parameter adjustment. the size of the model was 29.79 m, which is portable and can be connected to the vge. users of different heights (160 cm-180 cm), ages (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30), and genders made traffic command gestures and stood 1.5 m from the sensor. the st-cnn model recognized gesture semantics in real time, and its accuracy and robustness were high."
"as another mcm system, filter bank multicarrier (fbmc) can provide the best out of band emission [cit] among the new waveforms proposed for 5g and future networks, such as generalized frequency division multiplexing (gfdm) [cit], universal filtered multi-carrier (ufmc) [cit], filtered orthogonal frequency division multiplexing (f-ofdm) [cit] and their variants. this advantage enable the fbmc system to utilize the fragment frequency bandwidth in a spectrum efficient way. apart from its advantage of significantly reduced out-of-band (oob) emission, fbmc can achieve higher spectral efficiency as it does not use any cyclic prefix (cp). however, due to the fact that there exists the ramp-up and ramp-down problem at the beginning and end of the frame in fbmc system [cit], the spectral efficiency of the fbmc system decreases in the applications with short messages, such as machine-to-machine (m2m) communications and internet of things (iot) [cit] ."
"in this section, the proposed sequential multi-clustering protocol is described. since this is a sequential clustering method only one cluster can be generated at a moment. that means until the previous cluster formation is complete next cluster cannot be generated. the most important thing of clustering is proper selection of cluster heads which is the primary focus of any clustering method. we must also keep in mind the available energy, message complexity and most of the important parameters regarding any wsn. our proposed method can be divided into few subparts. in the entire method we assumed that the base station has a complete knowledge of all the nodes of the network and also it can communicate any node when necessary (this may be direct communication or may be indirect communication by routing protocol). also we assume that the bs station determines the cluster head according to the algorithm and then send necessary information to the cluster heads. also the bs updates its information after formation of each cluster by communicating through the cluster head."
"for checking the applicability of our smcp method, at first we have created the wsn topology over which all the clustering algorithms expanded ring, rapid, persistent and mmec [cit] run in ns 2.33. the average outcomes of the simulation are given here for two different topologies."
"the key feature of a typical sensor network is that their nodes are un-tethered and unattended. these sensors possess the ability to communicate either among each other or directly to an external base-station (bs) . no doubt a greater number of sensors allows for sensing over a large geographical regions with greater accuracy than small number of sensors. wsn finds its applications in diverse fields. in the field of healthcare, sensor nodes have been deployed to monitor the condition of patients [cit] . sensor nodes have also found its application in cattle ranch monitoring [cit], cattle control [cit], zebra herd monitoring [cit], etc. efficient flood forecasting systems utilizing wsn, are also being developed [cit] nowadays. with the flourishing growth in the use of wsn, the size and complexity of the networks will increase. the sensor nodes are essentially low-cost, low-power devices capable of communicating over short distances and processing some data. these nodes are generally densely deployed in the concerned region either in a regular or irregular manner. as wsn could get exposed to adverse environments [cit] e.g. -disaster monitoring, they must have a self organizing capability and immunity to node failures. so it is expected that a large number of cheap, simple sensor devices will be randomly scattered over the region of interest. a network, deployed with large number of sensor nodes remains functional [cit] when all nodes or a predefined percentage of nodes are alive in a network. network lifetime [cit] thus becomes an important parameter for efficient design of sensor networks. in case of wsns, the lifetime can be taken as the time from inception of the nodes to the time when the network becomes non-functional. a network may become non-functional when a single node dies or when a particular percentage of nodes die depending on requirement. each node is provided with transmit power control and omni directional antenna and therefore can vary the areas of its coverage [cit] . each node is battery powered and has limited processing and memory capabilities. therefore it is critical that these resources are to be used in a judicious manner in order to maximize the benefit from the network before it dies. although there is a cooperative effort from the device research community towards designing low power hardware and efficient energy sources, the network research community has also realized that inefficient algorithms at the various networking layers can result in nodes dying prematurely. there are several proposals at the mac ( [cit] ) and network layers ( [cit] ). thus it becomes necessary to prevent single points of failure which in turn calls for distributed algorithms. the self-organizing property of wsn requires network decomposition into clusters of specific sizes. the main aim should be to minimize the message complexity of the clustering algorithms and to ensure the cluster formed attain the specific bound. lack of sufficient power in the nodes and limited bandwidth of the wireless medium [cit] makes the task of clustering more challenging."
"after the selection of initial cluster, the first cluster formation is initiated. after the cluster formation is complete, the last hop of the formed cluster marks their available (i.e. un-clustered) neighbor nodes so that they cannot be selected as cluster head. it can be inferred that all the nodes that are already clustered cannot be selected as cluster heads either. so they are marked as well. after the first cluster is formed, we have to select next cluster head."
the st-cnn model performs convolution operations on 3d position data well and has strong portability. a convolution kernel extracts temporal features of the skeleton point positional information from consecutive frames and extracts spatial features from the relationship between multiple skeleton points.
"where θ i and x are column vectors, and the θ t i x may be replaced by function f i (x). the softmax function sets the range of p(i) between [cit] . in the output layer, the model divides the pictures into eight classes that represent eight traffic command gestures."
"feature extraction of motion trajectory is a key step in traffic police command gesture recognition. feature extraction methods of action recognition include human geometric characteristics [cit] and motion features [cit] . after feature extraction, researchers usually use common pattern recognition algorithms, such as the hidden markov model (hmm) and support vector machine (svm), to classify them. jie [cit] developed a method to model actions using a hidden markov model (hmm) representation. however, hmm is based on probability statistics. the hmm model's computational requirements for training the transition matrix and confusion matrix are too large to simulate complex actions. [cit] constructed video representations in terms of local space-time features and integrated such representations with svm classification schemes for recognition. mathematical models of human behavior recognition based on probability statistics are unable to practically simulate complex behavior. deep learning provides new ideas for human behavior recognition, including convolution neural networks (cnns) and recurrent neural networks."
"immediately after selection of each cluster head, the clustering is performed. the clustering is performed using the available clustering algorithms like expanding ring, rapid etc. the message exchange schedule of such clustering is same as the clustering algorithms used. after the formation of next cluster, previously describes steps are repeated again and again until most of the nodes are clustered. again, we assume that the base station has a count of the no of clustered nodes."
"the softmax function was applied as a fully connected layer before the output layer. the softmax function maps the output of multiple neurons into (0,1) intervals, which expresses the probability of each activity. the activity with the highest probability is then set as the predicted activity and the activity label is outputted to the final node (in red), as shown in the fully connected layer in figure 4 ."
"we built a virtual traffic interaction environment with virtual reality technology. users can have interactions between their actions and the objects in the virtual traffic environment through a communication interface, experiencing and interacting with \"real traffic crossroads.\" 2."
"a new traffic command gesture recognition method is thus presented. we built the st-cnn model based on the depth data provided by a depth camera. real-time traffic gesture signals were applied to a virtual urban scene. the recognition module result was connected to the reserved interface of the virtual traffic environment module, and the signals controlled vehicles at traffic crossroads. traffic police models were changed according to differences in the signals, so that the traffic police in the virtual scene were more realistic."
our model in algorithm form is given below with assumption that every node has a unique node id and every node knows it neighbor: include the nodes to their nearest cluster;
"where γ is a mmse equalization bias diagonal matrix with the -th diagonal element (1 − 2 2 + 2 ). transforming the equalized signal back to time domain, we can obtain"
"in any sequential clustering algorithm, the selection of the first cluster head is the most crucial part. only a proper selection of initial cluster head leads towards splendid success of the entire clustering process. for example in figure 1 if any corner element is selected as cluster head, then the initial cluster will contain many hops. also this cluster will have less number of neighboring sensor nodes; this may hamper the overall process. so in our proposed method, the first cluster head is not selected randomly from the entire set of available nodes. let us assume that bs knows the entire topology of the network and it can command any sensor node to start acting like a cluster head. the entire paper is based on that assumption. this is also valid for a network without any assumption but the results will be slightly different. in our algorithm the first cluster head is selected as the center of the densest region of the entire topology. the density is determined by considering a circle of certain area and calculating the number of nodes within it. again, as time complexity of this process is much large we need some procedure to reduce it. we can reduce the total area of this process by taking the average and the standard deviation of the co-ordinates of the nodes. ). now the centers of the circles are chosen randomly within the determined area. we observed that taking the m.p as centre gives the highest density in some cases. we can also use some optimization algorithm to determine the starting cluster head. in figure 2, the darkened node (or dot) is the m.p and the dashed area is the area selected for initial cluster head searching."
"future works employing the real-time traffic command gesture recognition method will take the pose (position, size, and orientation), deformation, motion speed, sensor frame rate, texture [cit], and other factors into account. different types of people, such as children, will be added to the tpcgs dataset so that more people will be involved in the 3d interaction of this virtual traffic environment."
it is observed that by using our method 1-3 nodes are left out after the clustering is complete. so we have included another mechanism to include them also. they are included to their nearest cluster at the expenses of one or two extra overhead.
"however, one major problem of learning method based on svm is the insufficiency of labeled examples especially the small number of irrelevant examples, which might bring great degradation to the performance of the trained classifier [cit] . in addition, svm is not suitable when a feature has highdimensionality as a result of computational complexity [cit] ."
abstract: the neural dependency parser submitted by stanford to the conll shared task on parsing universal dependencies. our system uses relatively simple lstm networks to produce part of speech tags and labeled dependency parses from segmented and tokenized sequences of words. we include a character-based lstm word representation in addition to pretrained and token-based representations. our system was ranked first according to all relevant metrics for the system.
"in this paper, we have chosen to classify images using rf as proposed by breiman [cit] . this classifier has been shown to be effective in a large variety of high-dimensional problems with high computational performance and accuracy."
"in this paper, we use local wavelet based cs-lbp (wcs-lbp) [cit] for feature extraction. local wcs-lbp extracts a cs-lbp from all multiscale sub-images, including low-passfiltered sub-images, after two-level wavelet decomposition. in general, since the x-ray image has strong edge distribution in the horizontal, vertical, and diagonal directions, the three high-pass-filtered sub-images (lh, hl, hh) have important properties when classifying image categories."
"the reference hospital(s) (rh), located outside of the disaster area, acts as an expert center by providing telemedical services to the mfh using the high-bandwidth satellite link (vsat, 2 mbps). these services consist of off-line and on-line telediagnosis, access to external medical databases, as well as real-time interactive telemedical services such as live teleconsultations, live telesonography, intraoperative virtual reality simulation and interactive telemicrobiology (see fig. 3 ). the microscope in the mfh can be completely controlled by an expert in the rh. the video data stream of the microscope camera is transmitted live to the rh using wotesa / winvicos (insert left). in this way the expert in the rh can perform the investigation of the microbiological sample in the mfh (insert right) completely and interactively."
-the technical code (t) describes the image modality -the directional code (d) models body orientations -the anatomical code (a) refers to the body regions examined -the biological code (b) describes the biological system examined.
"mobile teams are deployed on the disaster site for search, identification, triage and evacuation of victims. they communicate with the coordination-and medical-teams located in permanent center or mobile field hospital via low-rate (globalstar, 9.6 kbps) and medium-rate (inmarsat, 64 kbps) satellite telecommunication systems. their positions are tracked via the established global positioning system (gps satellite system)."
"as the digitalized medical images are considerably increased, medical image retrieval is an important issue to assist effective and accurate patient diagnoses. for example, the medical doctors search database images using keywords or visual features to find interesting or important cases and to compare visually similar images with their diagnoses. moreover for fields such as case-based reasoning or evidence-based medicine, there is a need for finding similar medical cases [cit] ."
"confidence score assigning for image retrieval after keyword annotation, an image has the at least eight to ten keywords. however, since all keywords have the same priority, if the user input a query \"left elbow\", the retrieval system gives the all images which have the same keyword according to their sequential order regardless of their similarity. therefore, we introduce the confidence score assigning method by giving the different priority to keywords by combining probabilities of rf and distance of brg."
"the costs for emergency interventions for removing a passenger from the ship and hospitalisation abroad are not to be undervalued. the removal of a passenger in the carribean can cost up to $ 11.000 and the cost for hospitalisation can range from 500-1000 € per day. consequently, market trends force passenger shipping lines to offer services that help to improve the response to on-board clinical emergencies, improve the customer satisfaction and the companies' image. platform includes a bi-directional satellite network (up to 2 mbps) between 10 centers of excellence in the euro-mediterranean region (morocco, algeria, tunisia, egypt, cyprus, turkey, greece, italy, france and germany; see fig. 6 ). for dissemination of the achieved results and for maximising its impact, emispher has organised international conferences at each of the mediterranean partner sites."
"the remainder of this paper is organized as follows. the \"image classification using local wcs-lbp and random forests classifier\" section describes image classification method using local wcs-lbp and random forest. the \"automatic keyword annotation and confidence score assigning\" section introduces keyword annotation and confidence score assigning method. in the \"keyword-based image retrieval and relevance feedback based on rf\" section, keyword-based image retrieval and relevance feedback using random forests is introduced. the \"experimental results\" section evaluates the accuracy and applicability of the proposed annotation and relevance feedback method based on experiments, and in the \"conclusion\" section, we present some final conclusions and areas for future work."
"on the basis of the distributed environment of a cooperative medical workbench (fröhlich, 1995) a high immersive virtual environment called surgical table has been developed, specially designed for the simulation and training of surgical interventions [cit] b) . briefly: a surgeon supervises a surgical training of one of his medical students. therefore a three-dimensional reconstruction of radiological patient data is being projected onto the workbench. the student manipulates the model, rotates and moves it on the workbench. he may touch bones or cut through some skin or tissue with a virtual tool. while doing so he watches his actions in 3d and feels matching haptic sensations. at the same time the surgeon is able to observe his student's actions and he can give guidance as he is able to point at structures (e.g. a tumour), to talk to the student or to demonstrate an intervention (telementoring). during such a training session it is also possible to have virtual windows showing additional information, movies, or video conferences with other experts or participants. the surgical table consists of two high-resolution hdtv-projectors (1600x1200 pixels) integrated in a mobile unit, where virtual objects and control tools are projected on a real workbench (fig. 8) . collaborative simulations for two users positioned on opposite sides of the surgical table are possible, as is the case during real surgery. the projective display system frees the user from the heavy load and inconvenience related to head mounted displays and enables virtual reality for routine applications. due to the combined application of two hdtv-projectors, polarisation technique and shutter glasses the surgical table allows for several working modes: -double-tracked mode: simultaneous projection (in broadcast quality) of two different stereoscopic views of a virtual scenario by combination of shutter and linear polarisation techniques. both users are individually tracked with the electromagnetic multi-channel polhemus fastrak sytem (sensors on the glasses and the stylus) and wear polarised glasses that only allow visibility of their corresponding projector. for stereoscopic imaging active shutter glasses are used. both users can work on the common data set, each using their own toolbar. in this mode, collaborative simulations for two users positioned on opposite sides of the surgical table are possible, as is the situation during real surgery (see fig. 9 ). -double mode: this mode enables two users to work simultaneously on the surgical table, each on their distinct data set. each user has an additional monitor for medical second opinioning. -stereoscopic hdtv-mode: projection of stereoscopic, full resolution hdtv. sources for the displayed scenarios are computer-generated, three-dimensional models, computer-based movies, as well live pictures from stereoscopic hdtv cameras. [cit] these are various 3d hdtv camera systems available: 3d hdtv camera for open surgery, a 3d hdtv surgical microscope and a hdtv pathological microscope. table: simultaneous display of two opposite tracked views of the same data set (double-tracked mode)"
"in this work, we propose three frameworks that assigning multiple keywords into images, keyword-based image retrieval, and supporting relevance feedback to retrieval system for reducing semantic gap between the user and a retrieval system in real time."
"haptic feedback for touching and navigation of virtual objects with simultaneous stereoscopic visualisation in a distributed network environment has been implemented [cit] b) . this allows multiple users to feel the shape and surface structure of an organ with simultaneous stereoscopic visualisation and tracking of the user for surgical training. client-server architecture allows for distributed usage of the simulation. the server manages the 3-d scene graph by vrml loading and java-3d scene graph building. changing of the scene graph by a client is enabled by the synchronisation of the 3-d data. this architecture allows for navigation through the environment by each client. the task of making the objects touchable is achieved by the integration of the phantom haptic device, a high precision 3-d force-feedback system for touching and manipulating the virtual objects (see fig. 10 ). the schematic setup for a client with haptic integration is depicted in fig. 11 . the 3-d data are transferred and mapped to the haptic device. the simulation utilises the dynamic data from the phantom haptic device. a 3-d graphic card and a monitor with an active lcd-shutter in combination with polarising glasses allow the stereoscopic display of selected 3-d objects (e.g. spinal cord, brain, heart, etc.). the head movements of the user are tracked by an ir-tracking system enabling a visualisation of the object with the correct perspective according to the actual www.intechopen.com with the phantom haptic device it is possible to rotate and translate the object. also with the phantom it is possible to navigate a small pointer on the screen around the object and feel the surface structure at the tip of the pointer. through natural access pathways it is also possible to navigate inside the object."
"the database images are subset of the imageclef benchmark for image annotation and retrieval as mentioned in the \"introduction\" section. to perform the training, 900 images were randomly selected from 30 image categories and each class has equal 30 images. for the test, 1,500 images which did not use for training were used and each class has equal 50 images as shown in table 1 ."
"rf is an ensemble classifier of a number of decision trees, with each tree grown using some types of randomization. rf has a capacity for processing huge amounts of data with high training speeds, based on a decision tree. the structure of each tree in the rf is binary and is created in a top-down manner, as shown in fig. 2 ."
"after keywords are annotated, the user input a query keyword and the system finds images including the same keyword in their annotation. in this system, we use a two-pass approach. first, our system retrieves a set of images having the same keyword with query q using linear search method and second, a more sophisticated matching technique is used: once the system finds a sufficient number of candidate images, the final distance is estimated by sorting the confidence values of keyword q included in candidate images and even though keyword-based image retrieval provides accurate retrieval results, query results based on annotated keywords still is far from user's satisfaction. therefore, the combination of relevance feedback mechanism with keyword and visual feature is more desirable to enhance image retrieval performance. in our relevance feedback mechanism, our objective is to retrain the decision trees of rf that have only two classes, relevant and irrelevant for next retrieval."
"a mobile field hospital (mfh), which will be deployed at or close to the disaster site, provides all activities related to the co-ordination of the mobile teams on the disaster site, the victims medical triage, reception, first aid treatment, conditioning for transportation, further medical expertise for some patients by teleconsultations between mfh and reference hospital(s)."
"to improve keyword annotation performance, this study first proposes a novel medical image classification method combining local wavelet-based center symmetric-local binary patterns (wcs-lbp) with random forests. second, for semantic keyword-based image retrieval, we propose confidence score assigning method to each annotated keyword by combining probabilities of random forests with predefined body relation graph. after confidence score assigning, we prove that our keywords having different confidence scores produce more efficient retrieval results when our method is applied image retrieval system. in addition, to overcome the limitation of keyword-based image retrieval, we combine keyword-based image retrieval with relevance feedback mechanism based on visual feature and pattern classifier. in relevance feedback mechanism, we also use the local wcs-lbp feature and random forest for confidence score updating."
"cooperating partners: d'appolonia (i), avienda (uk), eutelsat (f), ncsr demokritos (gr); co-funded by the european union (eu) under the eten programme. the main objective of the service developed by the medaship project is to supply integrated solutions for medical consultations on-board of ships [cit] b) . the satellite-based telemedicine services address both passenger ships and merchant vessels and are intended to provide passengers and crew members with an effective medical assistance in cases of emergency and in all those cases where the on board medical staff requires second opinion. during the validation phase the service was tested on board of three ships with the possibility to have it connected to three land medical centers (fig. 4) . in addition to the standard medical equipment aboard the ships, two video cameras, an electrocardiograph (ecg) and an ultrasound (us) equipment are used. with this equipment the following telemedical services have been realised using satellite transmission at a bandwidth of 512 kbps up to 1 mbps offering the required high quality of images and video transmission:"
"in the training procedure, the random forest starts by choosing a random subset i′ from the local wcs-lbp training data, i. at the node n, the training data i n is iteratively split into left and right subsets i l and i r by using the threshold, t, and split function, f (v i ), for the feature vector, v, using eq. 1."
the ecg system is connected to wotesa on board the ship and can be controlled by the physician from this workstation. via application sharing software also the expert can control the ecg system from the land-based workstation. the main menu that includes all the functions of the ecg as well as the patient's ecg is transmitted to the expert. thus the expert and the physician on board can jointly acquire and analyse the ecg report.
"recently, another key issue in relevance feedback is the learning strategy. especially, svm is one of the most effective learning techniques used in relevance feedback as image classification. the aim of svm is to create a classifier that separates the relevant and relevant images and generalizes well on unseen examples [cit] ."
"in this paper, we use flat and axis-wise method concurrently: basically, we use the flat strategy for image classification using rf because it is simple and produce the best results out of the three different classification strategies as well as it requires n classifiers for n classes [cit] . after rf is trained with training data collected from each axis, each test image is classified one of total classes. then, an image is given the final irma code using the proposed body relation graph (brg) that is the modification of axis-wise strategy. the brg is designed for hierarchical representation of irma code. in brg structure, brg is composed of four fig. 1 representation of the final local wcs-lbps histogram generation. local wcs-lbps histograms are generated from one low-passfiltered sub-image and the other two wavelet energies. all histograms are then concatenated to create the final local wcs-lbps histogram layers as it was defined in axis-wise strategy as described in fig. 3 : one top technical layer (t), three anatomical layers (a), and one biological layer (b). especially, a layer is designed to consider human body parts and their relations. however axis-wise strategy should have four different multiclass classifiers for each individual axis, e.g., n classifiers for each layer, but, in brg code, we need only major 30 classifiers and other 20 classes are classified according to the hierarchical connection with the 30 major classes. for example, if one image is classified class \"finger\", it is also a member of higher classes \"hand\", \"arm\", and \"radiography\" according to the brg. therefore, we can classify 50 classes using only 30 classifiers. as shown in brg, the numbers marked in individual class is used for irma \"d\" and \"a\" code and other codes (t, b) are assigned automatically based on brg."
"keyword annotation using image classification and body relation graph there are three different classification strategies for annotating the radiographs as follows [cit] : -flat strategy: an image is classified into one of the n base classes regardless of irma code. this strategy is usually performed using multiclass classifiers (e.g., msvm). -axis-wise strategy: according to the irma code, four different multiclass classifiers are trained for individual technical, directional, anatomical and biological axes. -binary tree strategy: the classification tree is generated using agglomerative clustering and the class distances. classification with binary classifier (e.g., svm) starts from top, each classification chooses one of the two possible child nodes until a leaf node is reached."
"third, to validate the effectiveness of our feedback approach, we compare the retrieval performance of our algorithm with feature reweighting [cit], svm-based feedback [cit] combining confidence score, and our proposed feedback method based on rf using 12 test keywords by increasing the numbers of iteration and top k. figure 8 shows the fig. 6 block diagram of the proposed relevance feedback framework performance comparison for each feedback algorithm evaluated by precision and recall. as we can see from fig. 8, our approach consistently increases the performance as the amount of iteration time increases. the average precision and recall of the proposed method on four different top ks is about 86.2% and 82.6% without relevance feedback. however, after three iterations, the average precision and recall of the proposed method is increased to 96.3% and 90.1%. in contrast, svm-based feedback showed average precision and recall on four different top ks is about 83.5% and 78.1% without relevance feedback. after three iterations, the average precision and recall is increased to 93.2% and 85.7%. feature reweighting method showed lowest average precision and recall on four different top ks is about 62.3% and 61.7% without relevance feedback. after three iterations, the average precision and recall is increased to 68.8% and 68.3%, respectively. even though svm-based feedback method shows higher precision and recall rate as 24.4% and 17.4% than reweighting method after three iterations, it has 3.1% and 4.4% lower precision and recall rate than proposed method. the main reasons for the higher precision and recall rate of the proposed and svm-based feedback method are that two methods not only retrieve similar images using probability and membership values based on image classification results, but also combine the confidence scores with probability and membership values. moreover, the proposed algorithm is able to retrieve more similar images than svm-based method regardless of the number of iterations. the main reason for the higher performance of the proposed is that proposed method used rf for image classification and learning feedback classifiers. in particular, rf can construct robust decision trees (classifiers) using only small subsets of the feedback samples and this structure helps to reduce overfitting than svm. figure 9 shows a retrieval result of \"cranium\" on top 10 retrieval results without iteration and after first iteration. as shown in fig. 9, third, sixth, and tenth images including sagittal cranium are removed and coronal cranium images are ranked in higher order after iteration."
"abstract: builds off recent work in dependency parsing using neural attention in a simple graph-based dependency parser with biaffine classifiers. our parser is the highest-performing graph-based parser on standard treebanks for six different languages, gettin sota or near sota performance on all of them. we also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over similar approaches."
"one of the conventional relevance feedback approaches is feature reweighting [cit] . in approach, if the feature of relevant images has a low variance, it indicates that these relevant images are consistent in this feature and that the feature should be assigned a relatively high weight. conversely, a high variance gives a relatively small weight."
"hospital outside of the disaster area. in medaship a system for telemedical support onboard of cruise ships and ferries has been set-up and evaluated. the emispher project provides an equal access for most of the euro-mediterranean countries to online services for healthcare in expedient quality. most of these services use winvicos and combine high quality live video transmission with remote control of medical equipment. the use of specifically designed networks for telemedicine contributes to the continuous improvement of patient care. combined with the implementation of various enabling ict tools to support distributed collaborative medical scenarios, such as high immersive visualisation, haptic feedback and stereoscopic and high-resolution visualisation, it can really contribute to the realisation of ubiquitous healthcare. at the same time, however, these innovative developments in ict bear the risk of creating and amplifying a digital divide in the world, creating a disparity in the quality of life, as this new ict-based era leads to an increasingly dominant role of access to ict resources in securing the quality of performance in many aspects of society [cit] . in recent years different projects have demonstrated how the digital divide is only one part of a more complex problem: the need for integration. [cit] b; [cit] ) . in order to progress from e-health and telemedicine towards u-health (i.e. ubiquitous access to high-level healthcare for everyone, anytime, anywhere) a real integration of both the various technology platforms (quality of service) and the various medical services (class of services) is needed. a virtual combination of interactive telemedical services to support medical telepresence serves as a basic concept for the development of virtual hospitals (vh). one key element within vh will be the medical workplace of the future, which is to provide each of the various user groups with tailored access to all relevant information at the right place and time and in an optimised form."
"then, the final distance is re-estimated by sorting the new confidence values and the top k nearest images are displayed as the same method. this process is repeated until all of the desired images are found. the order of the proposed relevance feedback is the following four steps."
"because keyword can represent semantics of images, keyword-based image retrieval is the typical query method. however, as a large number of medical images are annotated manually by doctors and medical experts, manual annotation is a time consuming and tedious work. to overcome the difficulty of manual annotations, content-based image retrieval (cbir) was proposed. however, because cbir systems in medical images typically relied on visual properties of an image, such as color, texture, shape, etc., simple query-by-image may fail in situations where the visual features cannot adequately capture semantic concept of images. therefore, keyword-based searches have been the dominating approach for managing medical image database in contrast to natural image [cit] . however, automatic keyword annotation considering semantic concept of medical images is a difficult work. to overcome the limitations of manual annotation and provide semantic keywords to medical images, some recent researches [cit] have proposed automatic keyword annotation based on image classification."
"stereoscopic visualisation has been realised to achieve a better spatial coordination for the surgeon where he has to rely on video images instead of on his direct sight. as medical stereoscopic video sources a 3d camera integrated into the operating light is used to transmit images from the site of operation in open surgery. a 3d surgical microscope can visualise structures as small as 50 micrometers and gives the surgeon a magnified view e. g. in vascular surgery. for minimally invasive surgery a 3d laparoscope gives the surgeon a stereoscopic view inside the body of the patient. high quality and high definition cameras have been adapted to different medical imaging devices and tested for medical purposes. for example a single definition 3-chip-ccd camera and an hdtv camera have been compared in a microscope for telepathology revealing greater details in the hd image of a pathological slide. also a stereoscopic surgical microscope has been equipped with a pair of high definition 3-chip-ccd cameras with a special hdtv optical adapter yielding a higher contrast image. the use of hd-video systems in endoscopic and laparoscopic surgery leads to improved surgical dexterity however, despite substantial improvements that have been realised, these developments bear the risk of creating and amplifying digital divides in the world. to avoid and counteract this risk and to fulfil the promise of telemedicine, namely ubiquitous access to high-level healthcare for everyone, anytime, anywhere (so-called ubiquitous healthcare or u-health) a real integration of both the various platforms (providing the \" quality-ofservice\", qos) and the various services (providing the \" class-of-service\", cos) is required [cit] b; [cit] . a virtual combination of applications serves as the basic concept for the virtualisation of hospitals. virtualisation of hospitals supports the creation of ubiquitous organisations for healthcare, which amplify the attributes of physical organisations by extending its power and reach. instead of people having to come to the physical hospital for information and services the virtual hospital comes to them whenever they need it. the creation of virtual hospitals (vh) can bring us closer to the ultimate target of u-health [cit] b) . the methodologies of vh should be medical-needs-driven, rather than technology-driven. moreover, they should also supply new management tools for virtual medical communities (e.g. to support trust-building in virtual communities). vh provide a modular architecture for integration of different telemedical solutions in one platform (see fig. 12 ). the technologies of vh (providing the \" quality-of-service\", qos) like satellite-terrestrial links, grid technologies, etc. will be implemented as a transparent layer, so that the various user groups can access a variety of services (providing the \" class-of-service\", cos) such as expert advice, e-learning, etc. on top of it, not bothering with the technological details and constraints."
"second, to validate the effectiveness of image-retrieval approach, we compare the retrieval precision using our confidence score assigning method with equal confidence score for all keywords by changing the top k retrieved images. first, three different experts were asked to select 40 representative ground-truth images from each category, and only those images where at least two experts were in agreement were then used for the precision comparison. the test is performed on 30 categories and five queries from each category. in all experiments, performance is measured using average retrieval precision. as shown in table 2, the overall performance of our approach outperforms the first method regardless of the number of top k as by average percentages of 77% and 71%."
"despite keyword-based image retrieval providing easier query interface and more accurate retrieval results than cbir, query results of annotated keywords still is far from user's satisfaction because image annotation is performed based on image visual features. therefore, the combination of relevance feedback mechanism with keyword and visual feature is more desirable to enhance image retrieval performance. in relevance feedback mechanism, the user only needs to mark which images he or she thinks are relevant to execute the query. by the user's feedback action, weights for similarity [cit] or parameters for learning methods [cit] are readjusted."
"after that, the relation weight (τ) of a classified class i and its linked parent classes are estimated using following scaled fermi function [cit] ;"
"this system is developed in visual c++ 2008 language as offline system for training and the test system was developed for on-line based on asp.net 3.5 using c# language. the proposed method is applied to our retrieval system, medical image searching system and it can be demonstrated at the following website (http://cvpr.kmu.ac.kr/miss2)."
"some of the multimedia teaching material needs to be presented in real-time. live transmission of surgical operations from operating theatres, lectures, etc. from one site to one or several sites simultaneously (point-to-point or multipoint) are possible in the network between the 10 partners."
"the meaning of each class and the number of test images are shown in table 1 . as shown in fig. 3 and table 1, all classes for flat strategy are leaf nodes of brg. other classes can be predicted using brg."
"ubiquitous access to high-level healthcare (u-health) requires increasing use of information and communication technology (ict) solutions. telemedicine describes the use of ict for the delivery of medical services. it aims at equal access to medical expertise irrespective of the geographical location of the person in need. new developments in ict have enabled the transmission of medical images in sufficiently high quality that allows for a reliable diagnosis to be determined by the expert at the receiving site [cit] . through telemedicine patients can get access to medical expertise that may not be available at the patients' site. networks for telemedicine enable the integration of distributed medical competence and contribute to the improvement of the quality of medical care, to the costeffective use of medical resources and to quick and reliable decisions. for optimal performance of telemedical applications, the networks and communication tools used must be optimised for medical applications, both with respect to the quality-of-service (qos, a set of parameters characterising the performance of the communication channel per se, such as transmission bandwidth, delay, jitter, data loss, etc.) as well as to the class-ofservice (cos; a set of terms specifying the medical services offered in the network, like telesurgery, telepathology, telesonography, tele-teaching, -training & -education, etc.) . using the specially-developed high-end interactive video communication system winvicos for real-time interactive telemedical applications at a moderate transmission bandwidth of 0.5-1 [cit] has designed and implemented various satellite-based networks for telemedicine. to serve the specific requirements for management of disaster emergencies, the system developed in the framework of the deltass project provides logistic and telemedical services for disaster emergencies. [cit] has designed and validated various satellite-based interactive telemedical services that support the medical staff of a mobile field hospital within the disaster area by medical experts from a designated reference"
the s-video output of the us equipment is directly connected to the osprey video capture board. satellite transmission tests have shown that not only still images can be transferred but also live ultrasound investigations can be transmitted at 500-700 kbps (see fig. 5 ). with a document camera analogous patient data can be captured and digitised by winvicos as a document. for example x-ray or ct-images can be captured from an illumination board and displayed locally and transmitted using this document camera function.
"the third field of service operated in emispher is medical assistance. as tourism constitutes a substantial economical factor in the mediterranean region and because of the increasing mobility of the population, continuity of care through improved medical www.intechopen.com assistance is of major importance for improved healthcare in the euro-mediterranean region. introduction of standardised procedures, integration of the platform with the various local communication systems and training of the medical and non-medical staff involved in the medical assistance chain allow for shared management of files related to medical assistance (medical images, diagnosis, workflow, financial management, etc.) and thus for improved care for travellers and expatriates."
"for example, if a sample image is classified into \"nose area\", it has a code \"213\" for a layer as top down manner because \"nose area (3)\" is connected \"facial cranium (1)\", \"cranium (2)\". it also has \"400\" code for \"d\" because it is a member of \"other orientation\". moreover, it has \"1211\" code for t and \"700\" code for b, automatically. therefore, the final code for this image is \"1211-400-213-700\" and it has the following keywords according to the irma codes (fig. 4) ."
"as can be seen from fig. 7, the annotation performance of the msvm with local wcs-lbp shows 25.5% for error rate, 57.31(a) and 36.96(d) for error count. in contrast, rf with the wcs-lbp method showed 20.3% for error rate, 38.8(a) and 23.1(d) for error count, respectively. the main reason of the good performance of proposed methods in error rate and count is that rf was able to to be effective in a large variety of proposed high-dimensional local wcs-lbp, with high accuracy. in addition, rf uses an ensemble of distributions of trees that are trained on only small random subsets of the data and it helps to speed up training and reduce overfitting [cit] . in contrast, because msvms are not suitable when the feature has high-dimensionality and the database contains over 1,000 images, it gave higher error than rf."
"in this paper, we proposed three frameworks: multiple keywords annotation for medical images, image retrieval using the keywords and their confidence scores, and relevance feedback for reducing semantic gap between the user and a retrieval system. to improve keyword annotation performance, this study first proposed a novel medical image classification method combining local wcs-lbp with random forests. second, for semantic keyword based image retrieval, confidence score was assigned to each annotated keyword by combining probabilities of random forests with predefined body relation graph. to overcome the limitation of keyword-based image retrieval, we combine image retrieval based on keyword and relevance feedback mechanism based on visual feature and pattern classifier."
"emispher has set up a satellite-based network using the combined wotesa and winvicos modules for real-time telemedicine. in the field of real-time telemedicine the following categories of applications are offered: second opinion (fig. 7), teleteaching & teletraining (demonstration and spread of new techniques), telementoring (enhancement of staff qualification), and undergraduate teaching courses and optimisation of the learning curve. the leading medical centers in the project provide expertise in the following medical fields: open and minimally-invasive surgery, multi-organ transplantation, endoscopy, pathology, radiology, interventional imaging, neurology, infectious diseases, oncology, gynaecology and obstetrics, reproductive medicine, etc. these real-time telemedical applications contribute to improved quality of patient care and to accelerated qualification of medical doctors in their respective specialty. the main target audience are specialist doctors."
"the formation and operation of the emispher virtual medical university (evmu) for elearning (teleteaching) is one of the main efforts in the project. the evmu uses real-time broadcast of lectures, live surgical operations and pre-recorded video sequences etc., as well as web-based e-learning applications. the target population of the evmu is comprised of medical students (both undergraduate and postgraduate) hospital staff, general practitioners and specialists, health officers and citizens. each of the leading medical centers provides didactical material and modules for synchronous and asynchronous e-learning in their medical specialties. the central gateway to evmu is the project's website: www.emispher.org."
"the permanent center is located outside the disaster area. the permanent center constitutes a new element in the architecture of support systems for disaster emergencies and is unique to the deltass system. in conventional set-ups the mobile teams at the disaster site are coordinated and supported by the staff of a mobile field hospital deployed at or close to the disaster site. however, complete deployment of such a mobile field hospital takes at least ~6 hours, usually ~12 hours, and consequently the activities of mobile teams in these first, highly critical hours, are ill-coordinated and far from optimal. to improve this bottleneck, deltass has a designated permanent center that is in control of coordination and medical support to the mobile teams from time zero on. the permanent center is equipped with terrestrial gateways to the globalstar and inmarsat satellite systems through which it receives all data from the mobile teams. it coordinates all actions of the mobile teams and manages all medical and logistic data, thus assuring efficient operation during the first critical phase. all data received at the permanent center are processed, appropriate reference hospitals (rh; see below) are identified and the logistic and medical data are transferred to these rh via terrestrial telecommunication links."
the live camera on-board of the ship can be used to transmit the image of the doctor who is leading the examination on-board of the ship or the image of the patient when being questioned by the land-based expert. it can also be used to show the land-based expert an injured part of the patient's body which he needs to see for his consultation. thus a very realistic and effective live communication is possible.
let q be the current query keyword. find all candidate images including the same keyword. 2. compute k nearest images from candidate images using confidence scores and their descending sorting. figure 6 shows the block diagram of proposed relevance feedback framework.
"first, to measure the annotation performance, we compare the proposed classification method with the widely used msvm [cit] with the same local wcs-lbp by estimating the error rate and error count. the error rate means the percentage of codes that have at least one error in one position within one axis (t, d, a, b). the error count gives a greater penalty for misclassification in higher hierarchical instances than for less precise classification in lower hierarchical positions [cit] .thus, an image where all positions in all axes are wrong has an error count of 1, and an image where all positions in all axes are correct has an error count of 0. in this paper, because t and b codes are assigned automatically according to the classification result, we only evaluated the error rate and error count on d and a codes."
"coarse filtration: embedded software traces tend to present very distinct cycles corresponding to very distinct behaviors. indeed, cyclic programming tends to distribute different tasks over different cycles. it is our belief that such disparity may hinder the search of the fault using suspiciousness ranking. consequently, the selection of cycles to analyze is important to mitigate this phenomenon, our coarse filtration method first fig. 5 . suspiciousness ranking using a single trace determines, using the hamming distance estimation discussed earlier, the cycles that most resemble the erroneous one."
"3) phase 1. in this phase, the algorithm b is required to create a private key for each access structure (ma, ρ) queried by the adversary a. the restriction is that the access structure is not met by either ats 0 or ats 1 ."
"moreover, since the abnormal behavior necessarily appears at the end of the trace, we assume that the last cycle in the trace corresponds to a failing run, the other cycles being considered as passing runs. therefore, the cause of the failure and the failure occur in the last cycle."
"for each cycle in trace, a binary vector is constructed. the hamming distance is computed on the basic element of the trace, that is, in our context, the program counters (pcs)."
"in this paper, we propose a new generic construction of epeks from anonymous kp-abe and formally prove its security. an efficient concrete epeks scheme over the prime-order groups is given and its performance is analyzed. yet, the epeks proposed in this paper only supports the logical expression of ''and'' and ''or'', excluding ''not''. and existing schemes that support the logical expression of ''and'', ''or'' and ''not'' are all based on composite-order volume 8, 2020 groups, hence not quite efficient. therefore, to propose an efficient epeks scheme over the prime-order groups that supports the ''and'', ''or'' and ''not'' operations of search keywords deserves further research efforts. his research interests include cryptography and information security, cloud computing, wireless security, and trusted computing. he has published more than 150 research articles in refereed international conferences and journals. his work has been cited more than 3000 times at google scholar. he has served as a program committee member in more than 20 international conferences and served as a reviewer in more than 90 international journals and conferences. volume 8, 2020"
"paper organization: section ii gives an overview of related work in fault localization. in section iii, the automatic fault-localization methods on which our approach is based are detailed. section iv describes our fault-localization approach using a single microcontroller execution trace. section v (resp. vi) presents an experimental evaluation of loop-header detection (resp. fault localization). section vii discusses the threats to validity of this evaluation. finally, section viii proposes some conclusions and perspectives."
"a watchdog is a mechanism that triggers a system reset if the main program neglects to check in regularly with the watchdog, because of some error. using p13, if the user presses b3 button the regular check-ins with the watchdog are disabled and a system reset is triggered, which generates a hardware interrupt. this failure was reported to us as being known to be hard to debug due to the delay between the execution of the faulty statement and the hardware interrupt."
it is important to note that the original versions of the mentioned afl methods does not work in our context where only one faulty trace is available.
"an interesting feature of this scenario is that there may be abrupt changes in the achievable rate of user 2 when we shift from one region to the other, and which are due to a drop in the transmit power of user 2."
"proposed by renieris and reiss [cit], the nearest neighbor approach first consists in finding the passing run that corresponds the most to the failing run, by comparing their spectra. the similarity between spectra, represented as bit vectors, is measured using the hamming distance 1, which was originally conceived for detecting and correcting errors in digital communication [cit] . then, the nearest neighbor approach applies the union and intersection models on the selected passing runs and the failing run. this step produces"
"then, globally, we observe that the nearest neighbor and the o p suspiciousness ranking require significantly more manual inspection to localize faults than the other suspiciousness rankings. in particular, on p1, p2, p5, p6, p8 and p11, while the fault localization expense with nearest neighbor rises up to 35%, the expenses with tarantula, jaccard, ample and ochiai stay under 5%. we also observe that in 75% of cases, at most, we analyze strictly less than 20% of the executed statements. note that compared to the other techniques, the ochiai method allows a fault localization with a less expensive analysis. figure 8 shows the result of the filtration evaluation. the graph represents the expense of locating the fault in program p 6, using the ochiai coefficient. when our fault-localization approach uses 10% of cycles of the trace, the engineer needs to analyze 5% of the source-code to localize the fault. the execution trace generated by the program p 6 contains 323 different cycles. thus, by using 32 cycles, the engineer analyzes 5% of the source code before he localizes the fault. however, by using 30% to 100% of cycles, the expense of locating the fault remains stable (less than 4% of the source code to analyze). it is important to note that the experimental evaluation concerns the 13 programs presented in this section. the fault localization in the other 12 programs requires the same expense to locate the fault since at least 10% of cycles are used."
"the passing run (cycle) that most corresponds to the failing run is found by measuring the similarity between each passing run and the failing run. the similarity between runs is measured using the hamming distance. recall that the hamming distance consists in the difference between two binary vectors. therefore, the smaller the distance is, the higher the similarity is."
"with the prevalence of the internet and the widespread application of cloud computing technology, personal privacy information often undergoes massive transmission via channels such as computer networks and public communication devices. these information transmission media are unsafe yet hardly replaceable. asymmetric cryptosystem was developed to allow people to share secret information without transmitting decryption keys. but in some cases, people need to process the encrypted information. imagining such a situation, a user uploads a large quantity of encrypted data files to an untrusted server. later, the user wants to fetch back some certain files from the server. how could the server pick out the target documents from a large amount of ciphertexts?"
"the indicators na(k), da(k), and first(k) are combined to form a score that allows us to rank program counters according to their likelihood to correspond to the loop-header. it is important to note that the first ranked pc is selected as the loop-header and used to divide the trace into cycles. the loopheader score is noted lhscore(k) and is defined as:"
"in this section, we independently evaluate our approach to automatically localize the loop-header. this evaluation consists in comparing our automatic detection of the loop-header on several programs with an oracle."
"a cycle can be of two forms: it can be either a sequence of symbols starting on an lh-symbol and ending on a symbol preceding an lh-symbol without any lh-symbol in between, or, it starts on an lh-symbol and terminates on the last symbol of the trace without any lh-symbol in between."
"section ii briefly lists some background notions and definitions. in section iii, we give the generic construction from an anonymous kp-abe scheme to an epeks scheme and then demonstrate its security. in the ensuing section iv, we propose an anonymous kp-abe scheme over the prime-order groups and formally prove its security. then we convert the proposed kp-abe scheme into a concrete epeks scheme."
"this means that any point of the region boundary is achieved by either both users employing proper signaling or improper signaling, but no mixed strategies."
"debugging embedded software remains a difficult task. in fact, embedded software consists of lowlevel code with a tight integration in a unique environment in terms of sensors, outputs, etc. this makes both static and dynamic analyses very hard. recent microcontrollers are able to record execution traces. however, the size of the collected traces makes the manual analysis tedious. in this paper, we propose a complete approach to help locating a fault in an embedded program based on a single failing execution trace."
"the rest of the paper is organized as follows. section ii provides some preliminaries of improper random variables and describes the system model. the characterization of the rate region boundary is derived in section iii, and a discussion on the results is presented in section iv along with several numerical examples illustrating our findings. finally, section v concludes the paper."
"the bug in p4 consists in using a memory address as argument instead of an integer variable, i.e., let be value_check (int x) a function, and v an integer variable, the fault is the use of value_check(&v) instead of value_check(v)."
"as is known to all, attribute-based encryption (abe) has a very strong access control capability [cit] . in abe, attributes are usually administered by a single central trusted authority that awards private keys to users. each user's private key contains information on user attributes. there are two types of abe schemes: one is the key-policy abe (kp-abe), and the other is ciphertext-policy abe (cp-abe). in a kp-abe, an access structure (as) is implanted in the private key and the ciphertext has a bearing on a set of attributes. opposite to that in kp-abe, an access structure in a cp-abe is implanted in the ciphertext and the private key has a bearing on a set of attributes. figure 2 shows the framework of kp-abe. in a kp-abe scheme, the trusted center uses a logical expression of attributes (which, in figure 2, is shown as a logic tree) to generate an access structure. one sound way to construct an access structure is using a linear secret-sharing scheme (lsss). the ciphertext gets decrypted only when the access structure is met by the attribute set. an access structure built via lsss could enable the kp-abe scheme to realize access control in cases that the logical expressions of attributes contain ''and'' and ''or''. this paper proposes a generic construction of epeks from kp-abe and gives an efficient epeks scheme over the prime-order groups."
"notice that, by expressing p 2 as a function of κ 2, r 2 (κ 2 ) is now also a function of κ 2 only. that is, the key task now is to determine the optimal circularity coefficient of the second user, or, in other words, the degree of impropriety of its transmit signal such that its achievable rate, given by (25), is maximized."
"if the conditions for strong and moderate interference are not met, proper signaling is the optimal strategy, which yields the weak interference regime and (33)."
"one at a time, each program is downloaded on a stm32f107 eval-c microcontroller board and executed. then, the execution trace is retrieved using a keil ulinkpro probe [cit], and saved in csv format. the trace file contains, for each instruction, its index, which is an id, the time when it was executed, its corresponding assembly instruction and the program counter (pc)."
"a preliminary step to our (adapted) fault-localization approaches is the loop-header localization. the experiments with the 13 faulty execution traces agree with the independent evaluation in section v. indeed, on each of the 13 traces, the proposed loop-header localization effectively detects the correct loop-header in the trace."
"another kind of fault-localization techniques, sometimes called delta debugging [cit], proposes to change the program state during its execution, to detect the origin of the fault. as they rely on experiments with the programs, those techniques are not usable in the embedded context."
"discussion: a last point we would like to address is the translation of program counters into actual program statements. as noted earlier, embedded machine code is heavily optimized. consequently, the actual association between machine code, on which the failure occurs, and the source code, which should be debugged, is very limited. the task of finding the statement of source code corresponding to some machine instruction is consequently in itself very hard. automated or manual fault localization needs this task to be done precisely. however, given a trace in terms of program counters, manual approaches and automatic approaches differ greatly in the number of machine instruction to translate back into source statements. on the one hand, standard manual fault localization approaches would likely have to translate a lot of machine instructions to source statements, browsing the trace from the failure back to the error. on the other hand, our approach will limit this translations to a few instructions by finding first the most likely faulty machine instructions before actually translating them to source statements. those elements lead us to believe that automatic fault localization will greatly speedup debugging in the embedded context and in any other context where the association between the collected trace and the source code is not a given."
"the complementary variances of the interference-plus-noise signals. notice that the rate of user 2 does not directly depend on p 1 andp 1, but implicitly through their impact on the optimal value of p 2 andp 2 . because of that and in order to keep an homogeneous notation, we express r 2 as a function of the four parameters in (5) . assuming that the power budget of the ith user is p i, the achievable rate region with improper gaussian signaling is then the union of all achievable rate tuples, i.e.,"
"before applying any fault-localization technique on the faulty execution, an important pretreatment takes place in our approach. indeed, fault localization relies on the division of the trace into cycles. this can be done through a preliminary treatment of each execution trace that consists in two steps: first identifying the particular program counter (pc) that corresponds to the loop-header, and second slicing the trace before each of the occurrence of that particular pc."
"this section provides a discussion on the derived characterization along with some numerical examples illustrating the most remarkable features of improper signaling in the z-ic. afterwards, the connection to related works in the literature is presented."
"moreover, the afl approaches were implemented and integrated in comet. first comet uses our compression approach to divide the trace into cycles. then, it applies an afl method chosen by the user, either the adapted nearest neighbor or the adapted suspiciousness ranking in which case the user also needs to specify the similarity coefficient to use (among tarantula's, jaccard, ample, ochiai and o p ) and the coarse filtration parameter as a percentage (cf. section iv-d). by default, the ochiai coefficient is used (as experiments show that it provides better results), and the percentage of the coarse filtration is set to 30%. finally, comet outputs the results. for the nearest neighbor approach, the results consist of two sets: one containing potential missing statements and one containing potentially superfluous statements. for the suspiciousness ranking method, the statements are given in decreasing order of suspiciousness."
"in order to characterize the boundary of the region defined in (20), we notice that the achievable rate of user 1 is bounded as"
"the presented approaches are implemented in a tool named comet and evaluated on several faulty programs. the evaluation shows promising results, in particular for the ochiai suspiciousness ranking. indeed, by using this particular method, the user is able to find the faulty statement by inspecting in most cases less than 5% of the program."
"the traces used to evaluate our approach come from 13 embedded programs, which are provided by stmicroelectronics and easii ic (see also table v) . each of these 13 programs contains a fault that is commonly found in embedded software development. also each program allows the user to interact with the microcontroller, by using the lcd-screen or the 4 microcontroller buttons. when a button is pressed, the microcontroller executes a specific processing. if the processing is finished without error, the led corresponding to the pressed button is turned-on, and a message is displayed on the lcd-screen."
"b. security of the proposed kp-abe scheme theorem 2: if the q-2 decisional assumption holds, then the proposed kp-abe scheme conforms to the ano-ind-cpa security in the standard model."
"1) keygen(f ). this algorithm is performed by the receiver and requires a security parameter f as input. it outputs user's public key pk and private key sk. 2) trapdoor(pk, sk, p). this algorithm is executed by the receiver and requires pk,sk and a search predicate p as input. it generates t p as the trapdoor of the predicate p. 3) encrypt(pk, ws). this algorithm is executed by the sender and requires pk and a keyword set ws as input."
"that is, the moderate interference regime shrinks as the rate of the first user increases, thus becoming almost non-existent for high values of r 1 . furthermore, the thresholds approach 1 as γ increases, which means that the conditions to operate under strong and moderate interference become more stringent as the snr of the first users grows."
"some techniques combine statistics and program slicing, e.g., [cit] . this kind of techniques requires a fine-grain association between the program source code, on which the slicing is usually done, and program execution traces. in our context, because of the various codeoptimizations performed during compilation, this association does not exist, and as a result, such techniques are not directly usable."
"similarity coefficients: there exists a whole variety of similarity coefficients between bit vectors used in the automatic fault localization. among the best known and most used coefficients, we choose to experiment with the following ones:"
"adapting suspiciousness ranking should be straightforward when considering cycles as independent runs. however, the adaptation must consider the important size of the execution trace. one way to reduce the problem is to limit the number of cycles to analyze. our suspiciousness-based method for a single faulty trace consists of two steps: a coarse filtration of cycles and the suspiciousness scoring."
"in this section, we present the evaluation of the proposed automatic fault-localization approaches for a single trace, namely the adapted nearest neighbor (section iv-c) and the adapted suspiciousness ranking (section iv-d). our evaluation essentially consists in applying our approach to known erroneous programs and to measure the quality of the diagnosis emitted by our fault-localization approaches. the evaluation is performed using our tool named comet."
"it must be noted that our technical approach relies on the analogy between program runs and cycles. programs runs are usually independent, especially if they are obtained by executing a test suite (usually) consisting of independent test cases. on the contrary, multiple cycles of same run could interact in many ways. in general independence between cycles cannot be guaranteed. however, the cyclic nature of embedded software can make this assumption valid for a lot of errors. in fact, many embedded programs interact with the microcontroller environment via hardware (e.g., sensors), and execute tasks without taking into account the results of previous tasks. all programs provided by our industrial partners are with independent cycles and fits into the model depicted in fig. 2 ."
"in this work, we adopt the augmented complex model to provide a complete and insightful characterization of the optimal rate region boundary, called the pareto boundary, of the z-ic, when users may transmit improper gaussian signals, assuming that interference is treated as noise. our main contributions are summarized next."
"this paper focuses on the efficient construction of epeks from kp-abe. kp-abe has strong access control capacity and efficient operation performance. in a kp-abe scheme, every user is marked by an attribute set and only users with specific attributes are authorized to decrypt a specific ciphertext. clearly, kp-abe makes user screening possible. implementing such a screening process on a cloud storage sever, users can only retrieve specific files, which is exactly what epeks could do. this inspires us to devise a generic transformation from kp-abe to epeks."
"2) perspectives: in the future, we will apply our methods to bigger programs and different faults. in particular, it would be interesting to study programs where an erroneous state propagates through multiple cycles before a failure occurs. this kind of faults raises an interesting challenge in the context of fault localization based on a single erroneous trace. indeed, we need to identify the cycle that we can trust to be correct in order to detect the fault. we believe that identifying cycles and comparing them will allow us to help find such faults rapidly. moreover, it must be noted that embedded software is not the only type of systems with a cyclic and long running behavior. for instance, most server-side applications satisfy these same characteristics. that is why we are also interested in applying and adapting our approach to other cyclic systems."
", which is strictly higher that the one obtained for the general z-ic (see the strong interference regime in theorem 1). this is in agreement with the fact that, if we let the first user optimize its circularity coefficient, the rate achieved by the second user can only increase."
"the goal of automatic fault localization (afl) is to ease the debugging step, this by pointing out to the engineer the lines of code that are the most likely responsible for the observed failure. a lot of techniques have been proposed to locate fault(s) in programs. this section discusses existing techniques. however, note that our very specific context has not been investigated earlier, mostly because our work is based on a single trace."
"the contours of their distribution are ellipses whose major axes are rotated by π/2 with respect to each other [cit], so that the signal and interference power are concentrated along orthogonal dimensions."
"programs and traces: we use execution traces that come from 14 cyclic programs, denoted p i, where i is the number of the program. table iv indicates for each program the number of lines in the source code, and the number of lines in the collected execution trace. each program contains a main while (true)-loop, which is repeatedly executed. in order to ensure that the loop-header localization is not biased regarding fault localization, we chose programs that differ from the faulty programs used in section vi. note however that our faultlocalization approach relies on the loop-header localization, and that the fault localization fails if the loop-header is not correctly identified."
"in this approach, we take advantage of the cyclic nature of most embedded programs to adapt well known faultlocalization techniques. the method first detects cycles in the execution trace automatically. then, this work adapts known fault localization techniques to the context of a single failing execution trace. those techniques take multiple passing and failing executions of the program as input. our adapted techniques consider cycles as program runs. finally, the paper presents an adapted nearest neighbor method and an adapted suspicious ranking method compatible with any similarity coefficient (e.g., ochiai or tarantula's)."
"a. optimal strategies 1) optimality of proper signaling: as pointed out at the beginning of section iii, if proper signaling is the optimal strategy for one of the users, then it is also optimal for the other one."
"in this context, our industrial partners would like to localize a fault in a program given a single trace that ends at the failure. they are interested into failure that stops the execution of an application (such as the so-called hard fault). the only available piece of information is one trace because of the due to difficulty of reproducing failures in general. to summarize, our context contains several interesting challenges: a single execution trace, a huge volume of data, and a fragile association between source code and execution trace. however, any improvement of the manual process of locating faults may considerably speed up the development process."
"definition 1: define q as an integer and let there be a bilinear group environment (p, g, g t, e). the decisional (q-2) assumption is: given elements"
"the circularity coefficient of a complex random variable x is defined as the absolute value of the quotient of its complementary variance and its variance, i.e.,"
"a particular case of the 2-user ic is the z-ic, also known as one-sided ic [cit] . the difference with respect to the 2-user ic is the fact that only one of the receivers is affected by interference."
"many embedded programs can be categorized as cyclic programs, as they rely on a main loop that iterates indefinitely. in the following, the instruction that defines this main loop is called the loop header. usually, at each iteration of the main loop, the sensors are monitored and actions are taken in response to changes. for instance, at each cycle, an anti-lock braking system (abs) reads speed sensors attached to the driving wheels and adjusts the braking power accordingly. due to the cyclic nature of embedded programs, collected execution traces consist in long sequences of multiple repetitions of instructions."
"another way to help locating fault is program slicing, see for instance [cit] . program slicing points to a set of statements that may affect the values of variables at a given point in the program (e.g., at the failure). however, for the purpose of fault localization, the set contains statements that are associated with the failure and that may not cause the failure. also, the set does not come with an order for suspect examination. our approach however does rank statements given their suspiciousness."
this approach is called the union model. the intersection model expresses another idea that is complementary to the union model. it tries to find features that are absent in the failing run but present in passing runs:
"we consider the z-ic with single-antenna users and no symbols extensions. denoting by h ij the channel response between transmitter j and receiver i, the signal at each receiver can be modeled by"
"the expense of locating a fault with the nearest neighbor method depends on the number of statements in the output and the order in which they are inspected. we choose to inspect the statement in the reverse order of the trace, which is common in manual debugging. for locating a fault with suspiciousness ranking, the number of statements to inspect is simply the number of correct statements with a higher rank than the faulty statement. in case of a tie in the ranking, the strategy used for the nearest neighbor method is applied; statements are inspected in the reverse order of the trace."
"finally, the weak interference regime defined in theorem 1 explains the cases where managing the interference by means of improper signaling strictly decreases the performance independently of the power budget. hence, it contains its information-theoretic counterpart, for which treating interference as noise is optimal."
"2) analysis: figure 7 shows the results of the experimental evaluation, where the axis of abscissa represents the programs from p1 to p13, and the axis of ordinates represents the expense for the fault localization. for each program we calculate the expense using the nearest neighbor, tarantula, jaccard, ample, ochiai and o p methods. first, consider the spikes where the expense is very high. to localize the fault, it was required to analyze the traces of p3 and p4 in details (expense above 75%), except with the nearest neighbor method and the o p suspiciousness ranking where the expense is less than 5%. the main reason behind this bad fault localization is that p3 and p4 propagate a state between cycles and that most suspiciousness rankings seems ill-adapted in this case. for p2 and p12, o p ranks very badly the fault, and as a result the engineer needs to inspect the whole trace and the source code (100%). in those execution traces, almost all statements obtain similar suspiciousness scores with o p . a possible reason is that the programs are too different from the model ite2 for which o p is optimal. for p13, the nearest neighbor and ample also require a detailed analysis of the trace and the source code, with expenses above 80%."
"contributions: in this paper, we propose a complete and novel approach to help locating faults in cyclic programs based on a single faulty execution trace. this approach is based on automatic fault-localization (afl) methods [cit] and takes advantage of the cyclic nature of traces. our approach first automatically detects the cycles in the trace before using adapted afl method to find the most suspicious statements. the effectiveness of our method is demonstrated by an experimental evaluation, made possible thanks to the development of a tool named comet [cit] . this evaluation shows encouraging results. in fact, the proposed method allows to find the bug in several faulty programs by inspecting in most cases less than 5% of the program code, with the best suspiciousness ranking, namely ochiai."
"if the condition for strong interference is not met, lemma 2 states that there may be a rate improvement if 2, thus we operate in the moderate interference regime. by lemma 2, we know that improper signaling is beneficial in this regime only when κ 2 exceeds a given value, and, in that case, the achievable rate increases monotonically with κ 2 ."
"suspiciousness scoring: computing the suspiciousness of a spectrum element relies on comparing the error detection vector with the vector of presence of the given spectrum element over the cycles. in case of an execution trace, the spectrum element is a program counter (see fig. 4 ). figure 5 summarizes the steps needed to rank program counters in a faulty execution trace: from the cycle detection to the actual scoring. one final step is needed to point out the fault in the program, that is, to associate the program counters with the source code statement from which it originates."
"in this section, we present an approach to analyze a single failure execution trace (without data) to provide engineers with program counters that are most likely the suspects for a fault. this approach leverages the cyclic nature of embedded programs and is based on the fault-localization techniques presented in the previous section. this section first presents our hypothesis, then our pretreatment of traces in order to detect cycles in the considered execution trace, and finally the two adapted fault-localization methods."
"the nearest neighbor method relies on the comparison of spectra to identify the closest passing run to the failing one. in this section, we propose an adaptation of the classical method in the context of a single trace. as noted above, a run becomes a cycle. in other words, we consider the failing cycle f and a set of passing cycles s . therefore, we apply the formulas of the union and intersection models on the cycles of a given trace where we consider each cycle as a spectrum. the hamming distance is used to identify the passing run closest to the failing one."
"hence, for each achievable rate of user 1, the corresponding pareto optimal point is given by the one maximizing the rate of user 2, r 2 (p 2, κ 2 ). it is worth pointing out that the first user can only approach the lower bound in the left-hand side of (21) by reducing its transmit power."
"oracle: the oracle analyzes the execution trace and the source code for each program used in the experimental evaluation to identify the loop-header. the oracle in our experimental evaluation is an engineer. experiments: for each program used in the experimental evaluation we observe the program counter selected as loopheader by our approach and we compare it with the one defined by the oracle. if they are the same, then the localization of the loop-header is a success. otherwise, if they are different, we analyze the ranking proposed by our approach to determinate the rank of the loop-header in the list of candidates. our approach provides a margin of error in the loop-header localization. therefore, if the rank of the loop-header is lesser than or equal to 3 2, the localization of the loop-header is considered a success. however, if the rank of the loop-header is greater than three, then the localization of the loop-header is a failure. according to our experimental results shown in table iv, we observe that our automatic loop-header localization approach succeeds in 100% of cases. note that the loop-header is always ranked first."
"in a kp-abe scheme, a trusted center authority generates users' private keys according to the user attributes. if the user attributes are regarded as the search keywords, then the private key generation algorithm in the kp-abe scheme could be used to generate the trapdoors of search keywords in the epeks scheme. correspondingly, the keyword ciphertexts in epeks could be generated by using the kp-abe encryption algorithm to encrypt a random message. the test algorithm in the epeks scheme could be executed by decrypting the random-message ciphertext and checking whether the decrypted message is the same as that in the original ciphertext. in so doing, the strong access control ability of kp-abe on user screening could be inherited by the derived epeks scheme to screen files. however, such transformation is unsuitable to most existing kp-abe schemes, because these schemes should attach an attribute set behind the generated ciphertext and thus don't provide any protection to the user attributes. privacy protection of the keywords is a very important issue in the construction of epeks. therefore, these kp-abe schemes cannot be directly exploited to construct the epeks schemes."
"we have analyzed the benefits of improper signaling in the z-ic. we have derived a complete and insightful characterization of the optimal rate region boundary, and the corresponding transmit powers and circularity coefficients in closed-form. the rate region has thereby been described by three interference regimes, under which the degree of impropriety affects the achievable rates differently. we have shown that the rate region can be substantially enlarged by using improper signaling, especially when the relative level of interference is high."
the expense of locating a fault e depends on the number of inspected statements i and the total number of statements t in the program. the expense is defined as follows:
"with all these ingredients, we can derive a complete characterization of the optimality of improper signaling for this scenario, and, consequently, of the pareto optimal region. our main result is formally presented in the following theorem."
"in this section, we first propose an efficient kp-abe scheme and demonstrate it to be ano-ind-cpa secure. then, we transform the proposed kp-abe scheme into an epeks scheme by using the generic construction presented above."
"a microcontroller is an integrated circuit that incorporates onto the same microchip the essential computer elements such as the processor, memory, peripherals and input/output interfaces [cit] . microcontrollers are embedded in various kinds of equipment such as cars, washing machines or toys. surprisingly, even though microcontrollers are now quite affordable, the development of embedded software still weights heavily both on the final cost of the product and the time to market."
"we show that the rate region boundary can be described by three interference regimes, depending on the ratio between the gain of the interfering link and that of the direct link of the interfering user. we denote these regimes as strong, moderate and weak interference, which slightly differ from their information-theoretic definition [cit], and account for the behavior of the improper signaling scheme depending on the relative level of interference."
"spectrum matrix error detection vector those two sets contain elements that, by their presence or absence, are likely to have caused the observed failure. finally the engineer analyzes the two sets to localize the fault."
"last generation of microcontrollers include parts dedicated to trace collections. for example, arm cortex-m includes a section dedicated to trace collection, called embedded trace macrocell (etm) [cit] . using specialized probes, such as keil ulinkpro [cit] and stmicroelectronics st-link probes [cit], it is possible to collect basic execution traces without input/output data. an execution trace consists in a sequence of program counters (pcs) that reflects the execution. however, even in a recording of a few seconds, the execution trace contains huge volume of low-level data. in addition, the analysis of a trace is particularly difficult because the association between pcs and the actual source code's statements to debug is often complicated. in fact, because of memory constraints or performance reasons, software code is heavily optimized before being loaded in microcontrollers."
"remark: we would like to point out that the definition of strong, moderate and weak interference in theorem 1 is made for the sake of exposition and does not adhere to the informationtheoretic definition of such interference regimes (see [cit] . hence, the definitions that we provide in theorem 1 account for the system behavior when we are restricted to gaussian signaling and treating interference as noise. notice also that the thresholds depend on α, hence the interference regimes are relative to the rate of the first user. the intuition behind this is that the slope of the logarithm is lower the higher its argument is. as a result, a fixed interference power yields a higher decrease in achievable rate when the signal-to-noise ratio (snr) is low. in our scenario, this means that the interference is more significant when r 1 is low (more details will be given in the next section), which explains the dependence of the interference regimes on r 1 ."
"assume a given failing run f and a set of passing runs s. to find events that are present in the failing run but absent from passing runs, a simple approach consists in computing the differences of spectrum between the failing run f and the union of all the passing runs:"
"1) setup(f ). this algorithm is executed by a trusted central authority (tca) and requires a security parameter f as input. it generates the public parameters pp and a master key mk. mk is maintained secret by the tca and the pp are made public. 2) keygen(pp, mk, as). this algorithm is executed by the tca and requires pp, mk, and an access structure as as input. it generates a private key sk as according to the access structure as. 3) encrypt (pp, m, ats) . this algorithm is executed by the sender and requires pp, a message m and an attribute set ats as input. it generates a ciphertext ct ats and outputs it. only users with access structure as that is met by ats can decrypt ct ats . 4) decrypt(pp, sk as, ct ats ). this algorithm is executed by the receiver and demands pp, sk as and ct ats as input. it outputs a message m if the attribute set ats corresponding to the ciphertext ct ats meets the access structure as embedded in sk as . otherwise, the algorithm will fail. the following adversarial game defines the security of an anonymous kp-abe scheme [cit] . this game is carried out between an adversary a and a challenger ch:"
"the set of constraints of problem p, which defines the feasibility set of our design parameters, is comprised of two constraints affecting the design parameters independently, namely, the power budget constraint and the bounds on the circularity coefficient, and an additional one that jointly constrains p 2 and κ 2 . the latter expresses a rate constraint on user 1, so that a specific point of the region boundary, determined by α, is computed. for a given κ 2, this constraint essentially limits the transmit power of user 2, p 2 . consequently, we can rewrite it in a more convenient form as"
"according to our industrial partners, the main costs are due to the validation step, and especially the fault diagnosis. in fact, in spite of the existence of several development environments for embedded applications, there are few tools dedicated to validation. classical approaches used for software validation are difficult to use. for instance, specificationbased verification tools (provers, model checkers) can not be used since applications are not formally specified. moreover, because the actual code is very low-level and specific to the environment, static analysis is almost impossible. note that dynamic validation is still difficult due to the limited observability of the execution of the embedded software. indeed, engineers still use oscilloscopes to analyze embedded applications by interpreting electric signals. consequently, validation and fault diagnosis are usually carried out manually, and are thus tedious and time-consuming tasks [cit] ."
"the third threat to validity comes from the loop-header localization, which is the basis of fault localization. an incorrect loop-header could lead to irrelevant and misleading results. however, in our experiments, such a case did not occur and a manual loop-header detection is always possible."
"with the level of information in modern society continues to improve, the rapid development of multimedia technology and network technology, community education industry put forward new demands. how the times, so that learners acquire more knowledge in a limited time, to train to adapt to social development, students with good knowledge and innovation ability, for particularly prominent in terms of vocational colleges [cit] . the emergence of multimedia courseware to some extent, to solve this problem, it has a wealth of expression, good interaction and sharing. teaching content through multimedia courseware can be difficult to show the vivid show to the students, a powerful tool to promote the modernization of teaching."
"parallel to the growing of electronic markets, practical e-commerce systems need to adapt to end users' preferences. to facilitate their construction, it is rst necessary to develop a framework for the automated simulation, evaluation and comparison of strategies for the purchase of goods in markets with varying protocols."
"there has been extensive research in recent years into agent negotiation in the context of marketplace simulation, usually with a focus on negotiation strategies. the e-game (electronic generic auction marketplace) platform [cit] ) is a java based, fipa compliant platform that shares many similarities with pirasa. built for simulations into market infrastructure, negotiation protocols, and strategic behavior, it supports complex and dynamic auctions, facilitated by a scheduler agent, which is roughly analogous to the broker agent in pirasa. how- ever, e-game does not just provide facilities for scheduling, running and conducting experiments, but enables modular implementation of auction-based market simulations. the development of new simulations in e-game is tedious, requiring several thousand lines of code for a simple market scenario."
"technology use. insert the appropriate graphics, audio or video and other media files in flash multimedia courseware, courseware can become vivid. for example, in flash software, you can add mp3, wav and other audio files, in addition, to create a sound object in the action-script can also be more flexible control of the sound, so flash multimedia courseware can be enriched by a lot. the use of layers is also a good way to increase the flash multimedia courseware. layer can reflect the spatial characteristics of objects, object, background, motion, etc. placed on different layers, seemingly independent of each other but in reality closely linked to the future of editing maintenance is extremely beneficial. courseware when using full screen better, users can opt out at any time classroom, so that teachers provide a convenience. it is noteworthy that in the use of multimedia courseware, if a large number of graphic images into the courseware, it is very likely due to improper use and dispersed the students' attention to cause side effects."
"services. therefore, we anticipate that the agent would gain higher utility in either direct sales or negotiation protocols. since there is no competition from other buyers, the simulations should result in a low mean spend for the agent."
"1. the composite good, strong interaction flash can put text, graphics, images, audio, video, animation and other information integrated in the flash animation clips and jumps action scenes can be used to achieve control, flash offers multiple interaction types, you can create a courseware with powerful interactive features, users can also enter tools and interactive courseware via mouse, keyboard and other powerful features to meet the needs of producers to provide."
"these attributes enable a variety of protocols to be formulated, including direct sales, auctions, and negotiations. when looking for a particular item, buyers look for a protocol for that specic item. they can either activate a dormant service, or join a running one (assuming that the maxclients variable is greater than than the number of agents the have already placed oers). a protocol is initially created dormant, with no buyers subscribed to it. as soon as the rst buyer subscribes, the protocol is activated and it exists only for as long as its max length is not surpassed. a protocol can also be automatically started when the broker starts to host it."
"4. a strong compatibility, scripting languages flash animation format courseware may be other types of references, as can be inserted in powerpoint courseware, flash courseware inserted in dreamweaver web production using action script language capable of flash create colorful animation, and access to the database through the xml language to achieve contact with the web database flash for courseware to provide a broader application of space .flash courseware but also through the \"publish settings\", select \"html \"option, the system automatically generates a namesake of\" html \"page file. thus, flash has good prospects in the course of the production network. figure 1 . flash working interface flash animation is divided into three types: tween animation, frame by frame animation, scripted animation. tween animation means to draw only the start key frame and end key frame, the middle of the transition frames are designed by flash software, tween animation includes two types: motion tween animation and shape tween animation. frame by frame animation: frame by frame animation is a traditional production method. in traditional animation, each frame is hand-painted. script animation: flash owns built-in script language action script. action script can easily create animation which is difficult to accomplish or simply unable to complete by above two animated types. 540 m. su 69.3.2 the working interface of flash the working interface of flash includes menu bar, toolbar, control panel, edit area. to create flash multimedia courseware, we must first create a new flash animation file, and then set the file attributes and save the file courseware. when we finish flash courseware, we should create running file which is freed from the flash environment and used in teaching. flash can work output for a variety of file formats, such as the swf, html, and the exe. we can select a format to output as needed. in order to create a beautiful interface, combining static and dynamic, sound and shape, convenient interact of multimedia courseware, we should proficient grasp the basic operation of flash cs3, such as layers, frames and key frame operation, which is shown in figure 1 ."
"further experimentation in this direction would improve the prediction of bidding behavior in online auctions using human-based historical data, and enable the development of more sophisticated agents."
sales allows sellers to sell goods at a xed price; auction lets the market of buyers determine the price of goods; and negotiation supports sellers to negotiate with buyers the price of goods.
"hypotheses 47: we expect the bidding dynamics of the simulated agents to closely correspond to the dynamics of human bidders in the historical auctions, where the bids being placed reect the historical trend."
"in this section, we review the technical background that our simulation framework is built upon; electronic contracts and their formalization as regulatory norms, agent reasoning based in the event calculus, and the agent development platform jade."
"we evaluate the performance of pirasa through a series of experiments with numbers of agents ranging from 50 to 500, incrementing by 50. we run these experiments on an i7 4770k computer with 16gb of memory running windows 7 64-bit os. for each experiment we run 3 sets of 10 simulation runs with an equal number of buyers and sellers, nding the average time taken for 10 runs to elapse."
"it is crucial for market designers to customize the protocols available in their market place to attract a variety of buyers and sellers. in addition to the predened protocols, pirasa supports the creation of new protocols via customization of protocol attributes. the attributes of a service encapsulating a protocol are the following:"
performing statistical tests and understanding the connection between agent traits and protocol properties more closely would be helpful to determine whether a protocol is signicantly more benecial to an agent over the others.
"2. powerful graphics, animation capabilities fiash with strong graphics rendering capabilities, flash support frame by frame animation, shape gradient animation, animation and motion gradient mask animation .flash common formats can import images without distortion scaling, graphical vector format, zoom in or out will not affect the picture quality courseware, a big-screen playback."
"pirasa can be extended to enable the participation of human agents as part of the simulations to capture more realistic e-commerce settings. such humans agents would not be autonomous, and would simply follow orders from a human operator. crowdsourcing studies can be conducted to evaluate the interactions among intelligent agents and human users."
"we use the notion of electronic contracts to regulate the interactions between the agents in a protocol, which are formalized via social norms represented as commitments, authorizations, and prohibitions [cit] . agent reasoning about norms is provided via the event calculus [cit] . following example 1, bob can check the delivery status of the watch via his contract with the seller. if something goes wrong and delivery is missed for father's day, the seller should commit to issuing a refund for bob. a normative approach provides a high level of organizational exibility, where the involved parties can create or cancel a norm, release it, or delegate it to others. we propose to perform strategic protocol selection via simulations. once presented with a set of alternative purchase protocols and market options, the buyer agent simulates its potential interactions in each available protocol and records its utility, which results in a ranking of the protocols. the multiagent simulation infrastructure of our framework relies upon the jade agent platform [cit], on top of which we have provided a user interface to congure various properties of the simulation. pirasa incorporates dynamic market elements and agent autonomy into the simulation environment. design-time solutions such as model checking [cit] fail to incorporate such autonomy, and can only verify whether an agent's goals comply with the protocol specication. to the best of our knowledge, this paper is the rst attempt to tackle this problem from a run-time perspective."
"no competition: in simulation 1, where there are no competing buyers, we observe via figure 4 that the eagersaver agent maintains a mean utility of one, as in each run it has three possible services to choose from with no competition. as compatible with our hypothesis, it chooses direct sales and negotiation protocols more often than auctions."
"(iii) negotiation: a negotiation protocol is somewhat similar to the auction protocol outlined above, in that it is better for the buyer when it is not competing with others for an item, otherwise it is advantageous for the seller. the negotiation architecture is demonstrated in figure 3 . since the seller has control over whether or not it will accept an oer, it can make counter-oers to buyers which will result in the item being sold below its market value, or alternatively the seller can wait for a long time to receive a higher oer by rejecting all lower oers. certainly, this is a riskier strategy, because if a seller overvalues their item, it could result in no sale being made at all. in pirasa, the propensity for sellers to adopt such strategies is based on their personalities, which are determined by attributes such as greed and eagerness. the following norms are relevant for the negotiation protocol."
simulations 47: we extract traits from four historical ebay auctions whose dynamics are illustrated in figure 7 . we use these traits to construct four auction-only settings composed of three to ve buyer agents.
"hypothesis 3: we expect the saver agent to only bid when the service price is low and therefore to remain dormant unless a service exists in the market with an item price low enough to entice the agent. we anticipate that the saver agent would be out-competed in cases where it shares a protocol with other buyer agents (as competitors), and therefore have the lowest utility amongst other buyers."
"in this paper, we mainly adopt the commitment norm as the basis for representing electronic contracts. commitments have previously been used in e-commerce [cit] . consider the following commitment: c(store, customer, payment, delivery)"
"flash-based multimedia courseware in the development, we can treat it as a project, the project consists of multiple process components. an excellent courseware content requires teachers teaching science planning and design, so as to improve the usefulness courseware, play an active role in teaching [cit] . the flash multimedia courseware development process is show in figure 2 . release courseware figure 2 .the flash multimedia courseware development process selected topic. multimedia courseware aided teaching purpose is to break the existing mode of teaching, the teacher dictating and writing on the blackboard into a courseware, a more vivid way to show them, to improve students' interest in learning, thus improving the quality of teaching. but it is worth noting is that courseware needs most is scientific and practical, the only way to play the advantages of multimedia technology, better teaching results. issue is particularly important in the selection of multimedia courseware in flash. courseware design issues should be preceded by a clear content, and the content of the required subjects for analysis. sometimes the subject content does not require flash multimedia courseware, once the use of courseware will bother to make the design courseware became futile work. in flash multimedia courseware design but also on the subject of feasibility analysis, see the topic needs to avoid time-consuming and difficult to achieve long phenomenon, the only way to make the courseware to play their own value, improve classroom efficiency."
"we present pirasa: an agent-based simulation framework that is designed to test dierent strategies in dynamic electronic marketplaces, and evaluate which protocol is the most preferable given the user's goals, preferences, and constraints. for example, an agent can be designed to help bob choose the best protocol for purchasing a watch for his father. such an agent should understand that bob's time constraint is more important (since father's day is approaching) than the price of the watch. therefore, the agent should decide to purchase the watch from the seller oering a direct sales protocol. however, if the watch oered by the direct sales protocol is signicantly more expensive than purchasing the watch through an auction, then bob's agent might reconsider its decision."
"a protocol in pirasa is dened as a seven-tuple containing two integers and ve boolean variables (described in detail in section 4.3), which represent fundamental parameters of real world transaction protocols. the protocol description acts as a blueprint for the creation of a service, which in turn is the construct that actually handles oers from the buyer agents, and establishes norms among the buyer and seller agents. pirasa supports three predened protocols. c(seller, buyer, payment, delivery)"
"listing 1 demonstrates a sample narrative in ec. according to the recorded events, the customer has paid for the item at time 4, the store has processed the order at time 5, and the courier has delivered the item at time 7. like protocol descriptions, event traces are agent-dependent. that is, each agent is aware of only the events that are relevant, but does not see the events that might have happened for other agents. happens ( pay ( customer, store, item ), 4). happens ( process_order ( store, item ), 5). happens ( deliver ( courier, customer, item ), 7)."
"flash multimedia courseware development requires a combination of knowledge and technology, when a beautiful interface of flash multimedia courseware can not only during the instructional design, prompting the teacher to grasp the overall teaching plan, assist classroom teachers, but also can promote students' self-initiative and can potentially enhance students' aesthetic taste. therefore, flash multimedia courseware application is bound to be more widely applied to teaching. with the continuous development of science and technology, more modern teaching model applied to teaching, flash virtue of its interactive, small-capacity, the advantages of vivid colors, having a rich appeal. in this paper, combined with flash multimedia courseware features, a simple analysis of its development process."
"once the rec engine is run with above input, it produces an outcome that demonstrates the uents the agent is aware of through time (e.g., states of commitments). rec can be extended with additional functionality besides commitment tracking such as exception handling behavior [cit] 4 pirasa framework e-commerce transactions in the real world such as listing and purchasing of items are simplied for buyers and sellers on sites such as amazon and ebay by acting as a hub between the agents. we"
"propose to mimic such transactions on the web by adding broker agents in addition to the sheer number of potential buyers and sellers. the inclusion of brokers simplies the process of a potential buyer nding a seller, and therefore both increases the throughput of the system, while minimizing the amount of time an agent is actively looking for a new transaction. this architecture is reected in figure 2 ."
"whereas, pirasa supports many protocol types depending on how many sellers and buyers are in the market, and whether the goals of the seller and buyer are to maximize prot or to prefer a quick transaction. auctionbot supports many auction types including the dutch auction. the same limitation for fisher market applies to auctionbot, it is not suitable when the sellers want to negotiate quickly or if there is one agent in the market. on the other hand, tete-a-tete and kasbah oers one kind of negotiation protocol, whereas pirasa oers three kind of protocols, which can further be customised and extended. one novelty that pirasa provides is that we are not limited to one type of protocol. a variety of protocols, including dutch auctions, can be integrated into pirasa using the customizable protocol attributes, as well as adding new attributes."
"multimedia refers to human interaction and a variety of media, information about your computer interact, giving users a more efficient means of control and use of information. multimedia show at the same time teaching content by entering text, click the button, buttons, etc. to achieve dialogue and computers for students to integrate into the learning environment provided, which were involved in learning, rather than just passively receive. learners through a variety of mouse and keyboard interaction, increasing the uncertainty of unknown content and inspire learners to explore enthusiasm. therefore, interactive multimedia courseware design will directly affect the merits of learning learners. only scientific, rational and effective interaction design can fully mobilize the enthusiasm of the learners to dedicate themselves to the study, the initiative to explore, to mobilize enthusiasm for learning, improving the quality and efficiency of learning. the interactive multimedia courseware application based on flash is show in figure 3 . learners and interactive courseware interface. learners for interactive media interface level directly affects the content of the course of study, a good man-machine interface can successfully guide the learner to start teaching content, teaching quality has a great role in promoting. how will teaching content on the computer to show flexibility and to highlight teaching points, breaking the difficulty of teaching, students and other abilities and qualities."
"learners and interactive learning content. unlike interactive learning content interactive human-machine interface, and can be expressed in the mouse, keyboard and other intuitive way. content interaction is implicit, are invisible interactive content and instructional design strategies reflected in flash courseware, teaching objectives and the like."
"having a depreciation or appreciation modier for items would add a new layer into pirasa, as it would allow buyers to be more strategic when choosing services to bid on, i.e., a saver agent could choose to fulll a goal with a lower quality yet cheaper item, as opposed to spending more for a better item. this would also tie into seller strategy, as the quality of an item could partially dictate the protocol they use with which to sell it. for example, when trying to sell a slightly depreciated item, such as a car which has had a previous owner, there is a greater scope for negotiation. similarly, low quality items, such as an old sofa, could either be negotiated or auctioned."
"max length: the maximum number of time steps that the service lasts. in the case of an auction, it would be a nite number, whereas for a negotiation it would be innite, as a negotiation lasts as long as the two parties want it to before the transaction is nalized."
"we run a similar performance evaluation for recon [cit] . figure 9(b) shows that the average cycle time for buyer agents in recon grow linearly over 100 runs with increasing numbers of agents. although the simulation settings in pirasa and recon vary, we can see by comparison of two plots that pirasa can support a fairly large number of concurrent agents to simulate e-commerce protocols. moreover, note that pirasa can support multiple protocols whereas recon is specically built for negotiation protocols."
"the above results support our hypotheses that the agents with high spend and eager modiers have the highest utility, and generally out-compete agents with save modiers in markets with a xed number of protocols and agents. moreover, we observe that competition (the inclusion of other buyers in the market) impacts the utility of buyers in closed market environments, resulting in a change of utility from 1.0 to 0.76 from simulation 1 to simulation 3."
"spend vs save: in simulation 2, we have two competing buyers (eagersaver and eagerspender), which are both more likely to choose direct sales and negotiation protocols than auctions. however, figure 4 : the stacked mean utility for eagersaver in simulation 1. since eagerspender can spend more and is highly eager in nature, we observe via figure 5 that it out-competes eagersaver in almost all direct sale protocols, forcing the eagersaver agent to fulll figure 6 : the stacked mean utilities for eagersaver, eagerspender and saver in simulation 3."
norm 6 states that the buyer is authorized to make an oer if its previous oer is rejected by the seller. norm 7 states that the buyer is committed to sending the payment to the seller if the oer is accepted. norm 8 is the same commitment from the direct sales protocol (norm 2) that handles delivery of the item once it is paid for.
"hypothesis 2: since the eagerspender agent has a higher propensity to spend, and is more eager than the eagersaver agent, we expect the eagerspender agent to out-compete the eagersaver agent in instances where both agents attempt to bid in the same service. moreover, since the eagerspender agent does not care about the price of an item, we anticipate that it would gain higher utility in direct sales protocols."
"there might be other protocol types that can be supported by modifying the protocol parameters. a dutch auction (also known as a clock auction or an open-outcry descending-price auction) is the idea that a seller tries to sell an item at a very high price, and slowly over time lowers that price until some buyer is willing to pay and obtain the item. this is referred to as an auction because it tends to provide the market ceiling price for a seller."
"saving' attributes of agents and using the`max cost' constraints from bid history are sucient to replicate the dynamics of four historical ebay auctions [cit] . a comparison of figures 7 (real auction data) and 8 (agent-based simulations) shows that our simulated buyer agents make bids that closely mimic those placed by their human counterparts. moreover, due to the autonomy associated with agent behaviours, some agent bids show deviations from mined data, which would enable e-commerce researchers to simulate settings with varying market dynamics."
"adobe flash, macromedia american company designed a two-dimensional animation software [cit] . usually includes macromedia flash, flash for designing and editing documents, and adobe flash player, for playing flash document. it is in addition to the development of web animation production and web applications are widely used outside, in terms of interactive multimedia courseware are also many advantages .flash supports multiple file formats leaders can be text, images, audio, video leaders and other documents, multimedia courseware provides a great convenience."
"when the seller is notied of payment, it is committed to ensuring the delivery of the item. the progression of commitment states is handled by each agent's rec engine. note that the commitment theory is not the focus of this paper. however, we still record the number of commitment violations per agent as part of our simulation data."
"for all experiments, we simulate the selected buyers in all three protocols and report the average utilities. moreover, we state our hypotheses informally, and investigate whether they hold."
"we then derive from this the average length of a single run. one can see from figure 9 (a) that there is a linear increase in the simulation time, up until 350 agents. this number of agents is quite signicant compared to other e-commerce platforms with considerable computational requirements for the running agents."
"we have evaluated protocols from the buyer's point of view. in a more realistic setting, a trader agent might act both as a buyer and a seller in multiple competitive markets. investigation of such markets, as well as taking into account the violations of agents' norms, can help understand how metrics such as social welfare evolve in those markets."
"there are many commercial negotiation simulation environments in the market [cit] ): tete-a-tete, kasbah, auctionbot, and the fisher market. fisher market is based on the dutch auction protocol. the limitation with the dutch auction is when there is one seller then the auctioneer will sell the product for the seller if the buyer reservation price has not yet reached."
"motivated by the current limitations in genius, recon [cit] was developed as a robust multiagent environment for simulating concurrent negotiations. recon is build on top of the golem agent platform [cit], and supports the development of software agents (both buyers and sellers) negotiating concurrently with other agents over multiple issues. in contrast to most agent development platforms such as genius, which only support imperative agents built in java, recon supports agents developed with declarative program. declarative agents enable developers to specify strategies that can be transparent to a human user, in that explanations can be provided for describing why the agent has taken certain actions during a negotiation."
"a contract describes how the involved parties should act in a business dealing. we adopt social norms [cit] to formally represent electronic contracts. norms (commitments, authorizations, and prohibitions) take their basis from deontic logic concepts [cit], and have been widely used in elds of articial intelligence that deal with legal concepts [cit], compliance checking [cit], and requirements engineering [cit] ."
"it is becoming increasingly common for virtual assistant systems such as`amazon echo' and google home' to contain basic integration into e-commerce platforms in addition to their information retrieval capabilities. although these systems have the capability to purchase items for a xed price, they are unable to enact strategies for the acquisition of goods in e-commerce markets with varying protocols (e.g., auctions). with recent advances in agent technology and machine learning, such personal agents should be able to make decisions on behalf of their users' goals, preferences and"
"max clients: the maximum number of buyers which can subscribe to the service. in the case of an an auction, for example, there would be no limit, but for a sale, the maximum number would be one."
"to highlight the use of pirasa, we construct a set of experiments to test the individual utility of several buyer agents. each experiment is repeated 100 times with the average utility being measured and compared amongst buyers. the utility metric identies the eectiveness of buyer agents, which corresponds to the goal completion rate of buyers. naturally, this metric is most useful for simulations in which there are fewer number of sellers."
"3. the file size is small, fast running online flash on the network using the popular current streaming media technology, small files, transmission speed, suitable for network transmission and file sharing .fl gas sh achieve while downloading while playing, not because of the network cause inconsistent play, affecting playback. students teaching resources through the network, web-based learning can be easily set up in the school lan."
"we create multiple market settings, which reect dierent levels of competitiveness and purchase options for the buyers, and simulate the buyer agents' strategies in those settings. we propose hypotheses on how buyer agents should behave in the presence of other buyers, and verify the hypotheses using buyer utilities gathered from the simulation data."
"(iii) brokers and market transactions: buyers initiate market interactions by requesting all broker agents for services which match their goals (see figure 2) . upon perceiving the services available, a buyer strategically determines the best service to subscribe to, in accordance with its personality traits. depending on the nature of the service, upon the receipt of a new oer from a buyer, the broker enforces the rules of the protocol in terms of a set of norms. if accepted, the"
"we provide a simulation framework for agent-based electronic commerce, where buyer agents interact with seller agents through broker agents. the broker agents regulate the interactions between the agents (formalized via regulatory norms) in the market."
"(i) sellers and product submission: at the beginning of a simulation, broker agents solicit the seller agents to publish all items they want to sell through the broker in the form of services. in order for a seller to submit their item for sale to a broker, the seller rst locates all brokers and then sends each one a message asking them to host the item. this seller protocol is shown in figure 3 . each broker replies either rejecting the proposal, or returning a potential protocol with which to host the item. the seller can then select the protocol which best suits its goal, e.g., whether they want to maximize prot or prefer a quick transaction. at this point, a message of conrmation containing details about the item is sent to the preferred broker. the broker then replies with a conrmation of receipt, and instantiates an unactivated service. potential buyers can now attempt to obtain the item, which triggers the activation of the service, starting the count down to its termination."
"an auction is dened as an interaction between any number of buyers and a single seller that lasts for a predetermined time, mediated by a broker. technically, the auction is regarded as a single-item, rst-price, open-cry, ascending auction [cit] ). an auction is started as soon as the seller accepts the proposal from the broker to host it, and during its lifecycle the broker receives bids from any buyer agent. the broker does not interact with the seller during this time, and therefore can accept or reject an oer based on whether or not the oering agent has violated any norms. once a buyer has its oer accepted by the broker, the following norms are created amongst the buyer, seller, and broker."
"in this paper, we have presented pirasa: an agent-based platform for simulating e-commerce protocols. it allows agents to determine which protocol is more benecial via experimentation in dierent settings. it supports customization of agent attributes, which govern the agent's behavior. this enables to simulate realistic e-markets where buyers compete with other to buy items from seller. our attempt is a rst to do this in real-time."
"pirasa supports basic goals and constraints for the agents, e.g., a deadline to purchase a specic item. future work could result in hierarchical trees of predicates to allow agents to retain highly complex goals. goals could also be extended to be domain specic, conforming to a predened ontology. in the current framework, we have focused on buyers and have not implemented complicated seller strategies. having goals for the sellers as well would lead to more realistic e-markets."
"agents in pirasa additionally have personality traits. for example, some buyers are eager to buy an item, whereas others prefer to save money when purchasing items online. similarly, some sellers oer fast delivery times, whereas others oer good deals via exible pricing options. each seller also has a preferred sales type chosen among three general purchase protocols: direct sales, negotiation, and auction. pirasa supports endless protocol congurations based on the protocol attributes, e.g., duration which determines the amount of time before the protocol terminates."
"in table 3 we show how the protocols compare with one another. in addition to the creation of the above scenarios, we capture data from human users in historical ebay auctions [cit], specically the auctions of cartier watches that lasted 7 days."
"flash is an authoring tool that designers and developers can use it to create presentations, applications, and allows the user to interact with other content .flash can contain simple animations, video content, complex presentations and applications, and between them nothing in between. typically, each flash creative content are called applications, even though they may be just a very simple animation. you can also add pictures, sound, video and special effects, build rich media applications include flash program [cit] ."
"the interaction between teachers and learners. in the learning process, the learner through interactive human-machine interface interaction and learning content can basically complete the teaching content. but for some complex, uncertain and difficult to understand the knowledge, self-learning alone, to think for themselves is difficult to solve."
"(ii) buyers and user preferences: the formulation of buyer agents' goals enables constraints to be placed on how a buyer goes about obtaining items. a goal can currently be constrained by a time limit, a price limit, or both. for example, if an agent has a time constraint (i.e., deadline) for an item and there is an auction which will only end after the time constraint has been violated, then the buyer will ignore it. however, a buyer will not make any assumptions with regards to the total duration of a negotiation, as it is possible at any point for the seller to accept an oer, and end the negotiation. pirasa currently supports the following four attributes for buyers:"
norm 3 states that all buyers are authorized to make bids on the auctioned item. norm 4 states that the buyer with the highest bid is committed to sending the payment to the seller. this commitment ensures that there is no way for a buyer to retract a bid (that has not been outbid) without violating their commitment. norm 5 is the same commitment from the direct sales protocol (norm 2) that handles delivery of the item once it is paid for.
"event calculus (ec) [cit] ) is an extension of rst-order logic to interpret and reason about events in time. table 2 summarizes the domain-independent axioms of ec. predicate happens records events with the time points of their occurrence. predicate initially species uents that hold initially. predicate holds_at queries the happened events to check whether a uent holds at a specied time point. predicate initiates marks that an event initiates a uent at a specied time point. predicate broken checks whether a uent is terminated during a time period. predicate terminates marks that an event terminates a uent at a specied time point. [cit] as a logic programming tool that extends ec for run-time monitoring. the rec engine takes as input (i) a normative theory shared amongst all agents that describes how norms change state; (ii) a protocol description specic to each individual agent that describes the domain, e.g., consequences of the agents' actions as well as any known facts; and (iii) a narrative specic to each individual agent that contains the events performed through the evolution of time."
"constraints. an agent should be able to search through available marketplaces for potential sellers, and determine which protocol would result in a purchase that meets the user's needs, possibly based on further interactions with other agents, whether human or articial. the issue then becomes, if we had to build such an agent, how to determine in an electronic market which protocol is most suitable to best satisfy the specic user's needs."
"to determine the necessary attributes of a system to solve this problem, we aimed to develop a system that could simulate the common protocols found in modern e-commerce markets with sucient agents to simulate protocols in even the most crowded of markets (akula & menascé, 2004) . in order to motivate the agent's strategy, it is also necessary to imbue the agent with two key constraints that humans experience in traditional markets, i.e. time and cost. section 3 reviews the technical background for pirasa, and section 4 describes the various components of its implementation. section 5 demonstrates our practical ndings through simulation experiments, and discusses the potential integration of real-world e-commerce datasets to infer simulation parameters. the capacity of the system to operate with large numbers of agents; (ii) the number of e-commerce protocols that can be simulated; (iii) the exibility of agents to mirror the myriad constraints and preferences that inuence human behaviour when operating in a market; and (iv) the ease with which new agent strategies can be constructed. pirasa is designed with these criteria in mind, and is compared in section 6 with the relevant literature on simulation environments and e-commerce platforms."
"courseware test. in the design of flash multimedia courseware for too long because of time-consuming, so inevitably there will be omissions, some errors in the production process is difficult to find, but if you run immediately after production is complete, will be in the classroom problems, and the expected effect of the opposite class, then you need to make at the end of the flash multimedia courseware for testing. people often have such a misunderstanding, do their own thing is hard to find the problem, then it is best to ask other teachers to flash multimedia courseware for testing, but can also listen to others to evaluate the courseware, so early detection and timely corrections."
"we transform this data to agent traits and constraints, enabling the simulation and permutation of historical auctions, for the purpose of evaluating whether historical outcomes could be improved. from historical bids, we normalise the bid price and bid time, deriving the proportion of an auction that has elapsed, and the proportion of the end price that has been achieved at every bid. we then regress bid time against bid price for each bidder, and capture the resultant parameters as`eager' and`spender/saver' traits for each participant in the auction, applying`max price' constraints derived from the maximum bid that each user made. the above development is reected in the following subsections, where we describe our simulations and results."
eagerness: the propensity of an agent to place a high urgency on a transaction. a high eagerness modier results in agents aiming for the quickest transactions possible.
"seller involvement: whether the broker is authorized to make decisions on the seller's behalf. for example, if during a negotiation the broker can accept an oer made by the buyer, or whether they have to forward the message to the seller for approval."
"formally, a norm n(x, y, antecedent, consequent) represents a social relationship between its subject (x) and ob ject (y) regarding its consequent when its antecedent holds. here, n is the norm type (c for commitment, a for authorization, or p for prohibition), x and y are agents, and the antecedent and the consequent are rst-order logic predicates (either atomic propositions, or conjunctions or disjunctions of them). we model conditional, detached, satised, and violated norm states. figure 1 describes the lifecycle of norms [cit] . a conditional norm is detached when its antecedent holds. satisfaction and violation conditions are described according to the norm type."
"its goal mainly through negotiation and auction protocols (compatible with our hypothesis). this competition results in both agents ending the simulation with high, albeit suboptimal utility. eect of competition: in simulation 3, we have all three buyer personalities in the same environment. the saver agent is far less likely to spend money in pursuit of its goals, and is therefore more timorous than the other two agents. we observe via figure 6 that there is signicant competition between the agents, driving down the overall utilities of all agents. surprisingly and in contrary to our initial hypothesis, the saver agent maintains a higher mean utility than the eagersaver agent. this is possibly due to the agent being less likely to compete in direct sales protocols against the eagerspender agent, and instead going for negotiation protocols. this is a dierent strategy than what the eagersaver agent adopts. the eagersaver agent (unsuccessfully) attempts to compete with eagerspender on direct sale protocols."
"the above commitment is a conditional commitment; if the antecedent (payment) is satised, then the subject (store) becomes committed to the object (customer) for satisfying the consequent (delivery), and the commitment becomes detached. a base-level commitment is simply a commitment with its antecedent condition being true. if the consequent is satised, the commitment is satised. after the commitment is detached, if the consequent is not satised, the commitment is violated."
"the problem domain discussed in this work is e-commerce agent-based market simulation, whose solutions are instances of practice-inspired research [cit] . the practice that motivates our work is online shopping. in current state of the art, a human user visits a number of websites to choose the best buying option based on her needs. for example, if she wants to buy a watch, she can order it from amazon prime with next day delivery option, or buy the same watch from an ebay auction for a lower price but with longer order processing time (e.g. wait for the auction to nish). reasoning about the tradeos among such choices results in a loss of time for the user. our proposal acts as a recommender system for automating this tedious process by using intelligent agents that know the preferences of their users. specically, we propose a framework, pirasa, to support negotiation, auction, and purchasing protocols, and the capacity of agents to conduct strategies for optimising their users' market utilities."
"for each of the three experiments, we reran the annealing to estimate the optimal parameters for that scenario. as we are not attempting to learn a generalisable function of pirasa, but rather learn the best parameters for a given auction, over-tting did not present an issue."
"here i δ is the size of the largest current problem, i.e. the difference between the actual current and the maximum cable capacity. it is essential that this current problem is situated on the cable between the battery and the distribution transformer or else the battery power will have negligible effect on this particular current. for example, if in fig. 9 i δ is located at customer 3, then the community battery cannot solve this problem."
"assume both s i and s j have multiple data packets for transmission. during the x-th time slot in terms of s i 's time scale, s i 's backoff timer is set to t i 2 ð0; c . during the y-th time slot in terms of s j 's time scale, s j 's backoff timer is set to t j 2 ð0; c . evidently, if the x-th time slot has no overlap with the y-th time slot as shown in fig. 4a, then s i and s j are not competing for the spectrum at this moment. therefore, we only have to prove p when the xth time slot of s i has some overlap with the y-th time slot of s j ."
"the basic structure of the optical packet switch contains an input interface, a switching fabric, an output interface, and a control unit. the input interface receives packets, aligned those packets, extract header information and remove header. the switch fabric is performs optical switching."
"recirculation buffer -this switch [cit] was developed from a previous atm switch, starlite. all input and output fibers carry single wavelength, as shown in fig. 3 . the output wavelength varies with packet. the switch is implemented with a coupler that couples incoming wavelengths through tunable wavelength converter (twc) and distributes through tunable optical filters (tof) and fixed optical filters (fof). a control unit controls this slotted switch. during a time slot, the control unit sends one incoming packet to the designated output along the single time slot delay. those not going to proper destinations are recirculated through the odl (fdl) and fed back the coupler in the following time slot."
"many studies try to find interesting new business cases for batteries. using electric vehicles for electricity storage purposes is not yet feasible [cit] . most work regarding battery energy storage systems (bess) focuses on residential applications [cit] . however, parra [cit] calculates that a community battery is 56% cheaper than separate residential batteries for a 100-home community."
"the battery control theory was formulated as a linear optimization problem. a receding horizon controller was developed to be used in a continuous way. the controller is suited very well to be integrated with other battery control goals, while still securing the voltages and currents within the network. it has been shown that a community battery is able to stabilize and control the loads in a real world low voltage network to a large extend."
"the definition of a concurrent set implies that all the sus in c have spectrum opportunities, and the activities of all the sus and pus in c are interference-free."
"the main reasons for linearizing the load flow model are the improvement in speed and stability regarding a non linear model. because since the load flow equations can be solved without iterative methods, it can be solved for large networks in a very short time span [cit] . this makes it viable for control purposes, as it can be used to evaluate many different control strategies. computational power is often an expensive resource in a control environment. for example, the local controller of the community battery has a clock speed of 500 mhz and 64 mb ram, which is very slow compared to a modern pc."
"while in most cases the lv network is radial, this is not necessarily always the case. there are low voltage networks in liander dno area which are operated in a non-radial manner and sometimes span over a thousand kilometer of cable and supply tens of thousands of households. it happens that the load flow equations as formulated in this paper are directly applicable to these large lv grids, while maintaining good performance [cit] ."
"we formally define a data collection task as follows. at a particular time slot t, every su in the secondary network produces a data packet of size b, which is smaller than the size of a primary data packet. the set of all the n data packets produced by sus at time t is called a snapshot. the task of gathering all the n data packets of a snapshot to the base station without any data aggregation is called a data collection task. the data collection delay is the time consumption to finish a data collection task. the data collection capacity is defined as the average data receiving rate at the base station during a data collection process."
"this switch was developed as part of the keops project [cit], fig 5. the output wavelength varies with packets. it also has contention resolution and switching parts. at the beginning demultiplexers pass packets to the designated odl which is predetermined in the twc upon the packets' entry and subsequent switching is implemented and then sent to the precise output. except for the distribution and input buffer at the input -which distributes the incoming packets to buffers -the rest of the switch is similar to the switches described earlier. the distribution part solves head-of-line blocking."
"these tasks can be used in both learning and assessment environments. these tasks consist of three dimensions: (a) mathematical cognitive dimension [cit] ), (b) algebraic content dimension, and (c) algebraic approach dimension [cit] ."
"furthermore, (18) can also be used to determine the potential destabilization of the grid. for example: given the fact that the fig. 9 . a battery placement example. if a battery is placed at between customer 2 and 3, it can control the currents and voltage drops in cable 1, 2 and 3. community battery of rijsenhout can control the voltage 12 v and that the maximal measured voltage in the lv network is already 250 v, the community battery can easily cause voltages of over the legal limit of 253 v. to be able to estimate this problem is of great use for dnos, as it proves that the battery power levels cannot be left unmonitored."
"cognitive task analysis is a method for specifying the covert cognitive structures and processes that are associated with overt task performance. cognitive task analysis focuses on covert cognitive processes [cit] . [cit] cognitive task analysis has three steps: (1) develop visual representations of knowledge structures, (2) describe cognitive processes underling performance, and (3) determine implications of results. [cit] argue that cognitive representations are critical for conceptual and performance features of cognitive task analysis. multivariate techniques such as multidimensional scaling can characterize quantitative features of cognitive representations [cit] . researchers have suggested various frameworks, such as representing problem solving skills and strategies [cit], knowledge structures and measurement modeling [cit], and knowledge modeling classification [cit] ."
"to make the experiment broader than just the dno perspective, an additional control objective was formulated. most of the customers have their own pv installation, and by aggregating their consumption and defining it as e ref, the customers can 'live on their own solar energy' as much as possible. this is also of interest to the dno as it mitigates peak loads from other network areas."
"using batteries in addition to a regular connection to the power grid is a relatively new phenomenon in western europe, because grid connections are very stable and relatively cheap. with the rise of decentralized renewable power generation in the power grid however, grid investment costs are expected to rise sharply [cit] . this is a strong motivation for the dnos to explore innovative solutions, such as battery storage."
"liander, the largest dno of the netherlands serving over three million customers, placed a community battery in rijsenhout, a suburban village close to amsterdam, the netherlands. the battery is connected to the low voltage power grid as can be seen in fig. 4 . the community battery has a usable energy rating of 126 kw h and a 55 kw peak power rating. the battery itself is capable of a higher power output, but safety regulations required the 55 kw limit."
". extensive simulations are conducted to examine the performance of addc. the simulation results show that addc can effectively gather all the data packets to the base station and significantly reduce data collection delay. particularly, compared with coolest [cit], the improvements on the induced delay of addc varies from 171 percent to 314 percent with respect to different system parameters for snapshot data collection, and the improvement on the delay of addc is 301.24 percent for continuous data collection."
"numerous efforts have been spent for different issues in crns, including spectrum sensing [cit], spectrum access, scheduling, and management [cit], capacity/throughput/ delay scaling laws [cit], network connectivity [cit], routing protocols [cit], multicast communication [cit], and etc. data collection is a common and important operation in wireless networks, as well as in crns, which can be used to gather data from an entire network. many practical applications in biomass monitoring, agriculture management and production, public safety, disaster rescue, ecology and mobile health care involve the data collection demand to grasp global information. interestingly, few efforts have been devoted to data collection for crns, especially practical distributed data collection in asynchronous crns."
"at the distribution transformer and the community battery both power and voltage are measured. the modeled part of the network consists of 34 customers. at 12 households, the power was measured. for privacy reasons, their exact location could not be displayed, but they are almost uniformly distributed along the cable. the data which is displayed in this section is averaged on the time scale of one minute. fig. 5 shows the result of the attempt to make the lv network selfsufficient. the community battery did most work in august, nearly doubling the self-consumption of the generated solar energy within the fig. 4 . gis view of liander's low voltage network of rijsenhout [cit] . the outlined modeled network is the feeder that is considered for the lv model. the unmodeled cables are not physically connected to the modeled network, except for a connection in de dt transformer. lv grid. still 77 kw h per household could not be stored in the battery because of capacity limits in this months. august and september are the two last months of the summer in the netherlands. in october and november, there was much more power consumption on average and less solar power generation. it can be observed from fig. 5 that in october and november almost no power was delivered to the grid for this reason. battery losses (and other transportation losses in general) are not part of fig. 5, because they have to be compensated by the dno. fig. 6 shows the maximum measured voltage from all available sensors. the battery controller keeps all voltage within the set bounds of 245 v and 215 v. in september and november, the community battery did not need to act to keep the voltages within the required bounds. during the months the community battery and its controller were active, the maximum network voltage was lowered from previously observed voltage peaks of 250 v to voltage peaks of 245 v, mitigating the voltage problems."
"the first step in designing a community battery is determining its location. the size of a large battery and its control installation is significant, which strongly limits the number of available placement locations. for example, in the case of the community battery of rijsenhout the battery size is half a standard shipping container and only a single placement location was available."
"to determine the minimum power required to mitigate current problems and given the network model's assumptions, the following formula is proposed, assuming that the power will mainly flow between the dt transformer and the battery:"
"the future work of this paper will be conducted along the following directions. first, in this work, we implement and analyze addc under a specific network distribution and the physical interference model. although this is reasonable to investigate and resolve the data collection issue for crns, we would like to study the performance of addc under different network distributions and interference models in the future. second, the delay and capacity bounds derived in this work are from the view of expectation. this is because the probability that an su has an opportunity to access the spectrum during a time slot is obtained from the view of expectation. therefore, another future research direction is to investigate the delay and capacity performance of addc in the worst case. third, in this work, we focus on snapshot and continuous data collection. for other important communication modes, e.g. multicast, unicast, data aggregation, etc., we will design distributed and asynchronous algorithms in the future work of this paper. fourth, we assume perfect spectrum sensing in this work which might induce a high cost. in the future, we would like to remove this assumption and study the data collection problem in crns where sus may fail to sense the actual pus' activities (named, false alarm and missed detection). finally, in this paper, it is also assumed that each pu determines whether to initiate a data transmission only at the very beginning of each time slot. moreover, if an su senses that there is a primary data transmission (which is started at the very beginning of a time slot) within its pcr, it will freeze its backoff timer and keep silent during that time slot. this implies a pu will not return to transmit data on the licensed spectrum during the time when an su is transmitting a data packet. therefore, in the future, we also want to remove this assumption and investigate the data collection issue for crns where the network is completely asynchronous and pus can initiate data transmissions anytime."
"the following section contains design principles to quickly determine the key properties of a community battery for network congestion reduction purposes, in new or existing grids. using the observation of the previous sections, generalized rules have been established. these rules have been designed to be used by network planners and have been kept as simple as possible. it has been assumed that a standard load flow simulation is unavailable to maximize the simplicity of the network analysis. a drawing of the low voltage network is sufficient to apply the proposed rules, once the problem and its size are known."
"basic switching node architectures were discussed in the preceding texts. it is clear that all-optical switching will not be economically viable in the near future due to technical constraints, the optical packet switching (ops) approach will likely to remain the only choice of faster transmission backbones."
"given is the network model of fig. 9 and the linear relation between battery power and voltage drop as seen in fig. 8 . if one assumes that the customer load is not significantly influenced by the voltage drop, simple approximate formulas can be constructed for battery placement."
"keops switch with a broadcast-and-select -originally developed as part of the european acts keys to optical switching (keops) project [cit] . all input and output fibers carry single wavelength, as shown in fig. 2 . however, the wavelength of an output varies with packet. there are three blocks in the switch fabric: encoder, buffer, and selector. each of the encoder block consists of n fixed wavelength converters (fwc) followed by a multiplexer. the buffer block consists of a splitter, k odl, and a space-switching stage implemented by means of splitters, optical gates, and combiners. finally, the wavelength selector block consists of n wavelength channel selectors implemented by means of demultiplexers, optical gates, and multiplexers. these three blocks make up the broadcast-andselect switch fabric. the switch consists of header, payload and guard time. header holds information about the packet and data is packet in the payload. the guard time is a special function that determines the setup time in the switch usually inserted either between the data and header or between two consecutive packets of the same wavelength. also mixed rate coding is used where header is processed at a lower speed (in gb scale) and the payload is processed at a higher speed (in mb scale). a fixed time length in micro seconds is kept for the packets where the packet sizes are variable. thus the advantages are of two fold: the processing speed depends on the header rate not on the payload; and, buffering space does not depend on the payload rate. in the case of large scale deployment, the switch itself is considered a large one and the incoming signals are demultiplexed into a group of wavelengths which are in turn put to a similar number of group of wavelengths to be combined for transmission on a single output channel."
"to address the aforementioned challenges, we propose a distributed data collection algorithm for crns without the time synchronization requirement. first, we study the proper carrier-sensing range (pcr) for an su. working with the pcr and the re-start (rs) mode 1, an su can successfully conduct data transmissions as long as there are no active pus or sus within its pcr, and will not cause unacceptable interference to any activity of pus. subsequently, we propose a connected dominating set (cds)-based data collection algorithm, named asynchronous distributed data collection (addc), for crns. through theoretical analysis, we show that addc is order-optimal when an su has a positive probability to access the spectrum. thirdly, we extend addc to deal with the continuous data collection problem, and analyze the delay and capacity performances of addc for continuous data collection, which are also proven to be order-optimal. we also conduct extensive simulations to validate the performance of addc. specifically, the contributions of this paper are summarized as follows."
"proof. for convenience, we first assume the secondary network is a stand-alone network. 3 for an arbitrary su s i, assume s j is another arbitrary su within s i 's pcr. we try to prove that ''p: if s i and s j are competing for the spectrum, then, before s i obtains the spectrum to transmit one data packet, s j can transmit at most two data packets''."
being able to control such a large battery to freely test control algorithms provides many opportunities for future research. next steps will include the application of state estimation algorithms to optimize the estimates of the voltages and currents and also properly account for uncertainties in network measurements and properties.
"an example data set is presented below for the purposes of illustration and simulation. this data simulated 3 students' performance in a given algebra i learning task. the variables were students' successful performances (or errors) in each cognition and content dimension. in cognitive assessment analysis, students' competences to complete algebra i tasks can be represented in theoretical framework which consists of explanatory variables and evidence variables [cit] . these variables can be further developed into a bayesian network, a quantitative diagnostic assessment model. student individual progress information can be provided by the quantitative bayesian network model [cit] . the following is one example of the bayesian network model of algebra i learning task completion (see figure 1) . [cit] b, [cit] ."
"intuitively, crns are distributed asynchronous systems and thus prefer distributed algorithms. first, crns tend to be large-scale distributed wireless systems, for which it is difficult and expensive (sometimes, even impossible) to obtain real-time network information for network management and configuration. second, the deploy environment of crns may be ever-changing, which may induce significant degradation of centralized and synchronized algorithms. third, in crns, some existing sus might leave the network and some new sus might join the network at any time. in this case, centralized and synchronized algorithms cannot adapt to these network changes in real time. finally and most importantly, sus should not cause any unacceptable interference to pus, which makes the status of a crn change more frequently and unpredictably. hence, centralized and synchronized algorithms are not preferable for crns. therefore, we investigate the distributed data collection issue in asynchronous crns in this paper."
"on a side note, it can be argued that modeling the customers as a constant impedance load model is not necessarily less accurate as a constant power load model. in reality, customers will have a mix of devices which require a constant power load, such as home computers and tvs, and devices which are in reality a constant impedance load, such as boilers and heaters."
"cognitive tasks, also called learning tasks, are usually defined as a series of objects which are implicitly contained in a learning environment [cit] . these cognitive tasks are \"wrapped\" into content knowledge of a given domain. in a pedagogical context, the instructor and learners rarely place the focus on appropriate cognitive tasks. the instructor can basically target some contents, but they do not precisely tailor the content into knowledge categorization and different problem solving features. in other words, the instructor cannot elaborately design cognitive tasks for both [cit] . at the end of a learning period, the instructor collects a piece of content knowledge arbitrarily or randomly to develop them into test items. learners' response to these items can be represented at a score level. however, these scores do not report what knowledge these learners acquired, what problem solving skills the learner developed, what kind of issues and errors they made, and what reasons led to these problems. any progressive details in the learning process cannot be recorded and recognized. thus, an effective representation of a cognitive task in learning is a crucial step towards establishing an assessment that reports diagnostic information of knowledge acquisition and skill development. learning involves learning tasks and researchers in the learning sciences seek to examine them from diverse angles. [cit] describe learning tasks in terms of a covert-overt continuum. the covert learning tasks, explored in this study, involve unobservable knowledge corresponding to psychological activities, thought processes and goal structures as opposed to overt learning tasks which involve observable performance."
"there are two parts of the research design: learning task design, and data for simulation. the purpose of the learning task design is to explore characteristics of learning tasks used in both learning and assessment. the data collection design further clarifies factors that influence students' algebra learning when students expose to these learning tasks and solve these algebraic problems."
"since the battery controller is designed with a horizon of several days, the energy lost by the self-discharging of the battery is neglected. furthermore, the load cycle efficiency of the battery is also neglected, as it is known to be over 90% in normal operating ranges. to mitigate the inaccuracies caused by these assumptions, e start has to updated at every optimization step using the measurement of the state of charge of the battery provided by the battery management system."
"fig. 2 contains a schematic of the battery controller. the rest of this paper is structured as follows: to calculate the characteristics of the lv network, a linear low voltage network model is constructed in section 3.1 and 3.2. after the linearization is motivated in section 3.3, the battery control problem is formulated in section 3.4. the models are applied to the community battery of rijsenhout and the results and accuracy of these models is investigated in section 4. the results of the experiments are used to formulate battery design principles in section 5 and are again applied in section 6."
"because the constructed load flow model is linear, network states can be evaluated independently using the principle of superposition. this is useful, as the impact of the network load on the voltages and currents can be calculated separately from the impact of the battery power. the maximum and minimum battery power can therefore be obtained by dividing the available voltage drop by the voltage drop caused by the battery at 1 w. since the network is only as strong as its weakest connection; the weakest cable or bus determines the boundaries of the battery. these boundaries can be calculated with the following formulas:"
"outputs of twc are fed to splitters which distributes the same signal to each of the n different output fibers. again these signals are separated into d + 1 through another set of splitter. output fibres from these splitters are connected to the odls, located at the destination output buffer. then the packets are forwarded to appropriate optical gate, while rests are closed. the control unit possesses knowledge of entire switch. it may order a particular wave to be processed at a particular twc and destine that to a particular fdl of the output buffer."
"most of the literatures were dedicated to the qos provisioning in the internet. however, they could not come out of the classical concepts of electrical packet switching and buffers. though the fdl was mentioned in many literatures, however, fdls are not viable for providing with delays for the optical data. three different approaches were proposed for service differentiation. firstly, offset-time-based differentiation is easy to implement. sufficiently large offset time is allotted to high priority bursts. this also introduces additional delay at the edge. the performance of the differentiation depends on the burst length and interarrival time distributions [cit] . active dropping can avoid above mentioned shortcomings. secondly, selected dropping of bursts is initiated according to loss rate measurement or traffic profile to guarantee the high-priority class to make a successful reservation. both of these two schemes make differentiation at the burst level. third one, which works at the packet level, can be achieved with burst segmentation. this mixes up low-priority and high-priority packets in special manner: putting low-priority bursts at the head and tail and high-priority bursts in the middle. if packets at the head or tail are dropped while in contention, differentiation on packet loss can be achieved [cit] ."
"cognitive characteristics will be examined in conceptual understanding, basic skills and symbol sense. the basic skills consist of procedure work, logical focus and algebraic calculation; the symbol sense includes strategic work, global focus and algebraic reasoning."
"using the newly determined properties of the community battery, a cost-benefit analysis can be easily obtained because location, power rating and capacity are also the most important factors for battery costs. the exact break down of the operational and capital costs of the battery and its comparison to conventional network strengthening methods are beyond the scope of this paper."
"wavelength switch optical packet network (waspnet) is shown in fig 6. the switch [cit] consists of twc on both sides of the awg. the first set of n twc for the incoming packets on the left of the awg selects packets to be recirculated; the rest of the n twc converts packets to wavelengths for the output as required by the switch, where more packets remain than the incoming and outgoing packets. one advantage is that it can support priority for the optical packets through delay to allow any other high priority packet. [cit] . now the challenge is to operate the switch at a switching speed of the order of nanoseconds."
"some researchers claim that the core goal of school algebra education is to develop problem solving strategies and reasoning skills rather than procedural fluency [cit] . however, symbol sense and flexibility are important in solving algebraic problems. [cit] as objects to \"denote any situation in which a concrete entity such as a mark on paper, an icon on a computer screen, or an arrangement of physical materials is interpreted as standing for or signifying something else\" (p.17)."
"the main purpose of the optical switches is to connect two ends of an optical transmission line. the switches are as such that the user data remains as an optical signal during its entire travel duration. the aim of such a switch is to make signals all optical from the beginning to the end, that is, a signal is never converted from optical to electrical or vise versa, thus setting up an end-to-end connection and reserve resources to transfer of traffic through an optical transport network for the burst of the signal in question. optical switching is suitable for atm and ip traffics. however, some technical difficulties have barred optical packet switching from becoming more successful, and much works were not done with the ip packets transfer techniques. on the other hand, more research were done in the optical burst switching for which it is becoming a promising solution for the wdm techniques. in order to transfer data from one access station to another, a connection needs to be set up at the optical layer. optical packet network consists of optical packet switches interconnected with fibers running wdm. this operation is performed by determining a path (route) in the network connecting the source station to the destination station and by allocating a common free (or idle) wavelength on all of the fiber links in the path. such an alloptical path is referred to as a lightpath or a clear channel. the entire bandwidth available on this lightpath is allocated to the connection during its holding time during which the corresponding wavelength cannot be allocated to any other connection."
"based on the understanding of cognitive task analysis, development of algebra learning tasks becomes an important theme in algebraic education. [cit] reported that they developed a sequence of tasks for the learning and teaching of algebra at ages 11-13. in the study the researcher focused on three activities: a) general activities, b) transformational activities and c) global, meta-level activities. these researchers attempted to establish a relation between the algebraic tasks and activities. however, they did not do cognitive explorations of these tasks and activities."
"the learning tasks in assessment design pursue a framework consisting of content criteria and cognitive characteristics. the content criteria include algebra i assessment, eligible essential knowledge and skills. these content topics include functional relationship, properties and attributes of functions, linear functions and inequalities, quadratic and other nonlinear functions. algebra i contents are classified into several topics which all reflect the functions: (a) general formulas, (b) forms of linear equations, (c) circumference, (d) area, (e) surface area, and (f) volume. these content domains will be integrated into the algebraic learning tasks in assessment."
"to create the constant impedance model as in fig. 3, it is necessary to convert the power use of a customer into an equivalent resistance. this can be done by the following formula:"
here i n is the current entering a network bus and y is the so-called admittance matrix. the admittance matrix can be directly obtained from the network lay-out using the following formula [cit] :
"unlike the classical switches, the wdm optical packet switch receives packets at the input interface. the header is separated from the payload. the header is then converted into an electric signal and processed electronically. however, the payload remains an optical signal and passes through the switching fabric. after the switching is done, both the header and payload are put together and the signal is converted into optical one at the output interface. there are many packet coding techniques, of which three are of basic types: bit serial, bit parallel, and out-of-band signaling [cit] . bit serial coding can be achieved through any of the mentioned techniques: optical codedivision multiplexing (ocdm), optical pulse interval, or mixed-rate techniques. each bit has its own routing information in ocdm. for the other two techniques, multiple bits are arranged in such a manner that they form a payload and header separately. in th e optical pulse interval, both the packet header and the payload pass the switch at the same rate. however, the header is processed electronically which has a lower transmission rate than the payload. bit parallel coding is implemented with multiple bits which are transmitted in parallel with each other, e.g., at the same time, using different wavelengths. out-of-band signaling coding includes subcarrier multiplexing (scm) and dualwavelength coding."
"now, we derive the conditions on pcr for sus. to this end, we have two constraints. the first one is to guarantee the sus will not cause unacceptable interference to the primary network, which is stated in lemma 2. the second one is all the sus that transmit data simultaneously are interference-free, which can be satisfied as shown in lemma 3."
"under the crn communication paradigm, a secondary network, which consists of secondary users (sus) (unlicensed users) equipped with cognitive radios, is coexisted with a primary network, which consists of primary users (pus) (licensed users). the sus sense and exploit spectrum opportunistically and share the same space, time, and spectrum with pus. for an su, when it has some data for transmission, it begins to sense the communication environment. if there is a spectrum opportunity, i.e., the data transmission of the su will not cause unacceptable interference to any pu, meanwhile, the receiver of this data transmission is out of the interference range of any primary transmitter, then the su can initiate a data transmission. during the data transmission of an su, if a pu comes back to transmit/receive data, the su has to immediately handoff the spectrum being occupied to guarantee that its data transmission does not interfere with the communications of pus."
"the rest of the paper is organized as follows. in section 2, we describe the network model and interference model. the derived pcr and addc are presented in section 3, followed by theoretical analysis of the delay and capacity performances of addc."
"in this section, we first construct a cds-based data collection tree as the routing structure for data collection. subsequently, we derive the proper carrier-sensing range (pcr) for sus. by working with the pcr, an asynchronous distributed data collection (addc) algorithm is proposed. addc can guarantee that the activities of pus will not be interfered. meanwhile, the sus that transmit data simultaneously are also guaranteed to be interference-free. finally, we theoretically analyze the performance of addc. it shows that addc can achieve order-optimal data collection capacity as centralized data collection algorithms [cit] for traditional wireless networks."
"unlike the electronic switches or routers, obs employs no or limited buffer. the burst scheduling and connection are done in a different manner. [cit] . a single scheduling horizon is maintained for each wavelength. only the channels with latest scheduling horizon is chosen, and scheduling horizons preceding the new burst are considered available. the horizon is then updated and makes reservation for the next burst. this algorithm eliminates minimum bandwidth gaps (or voids) due to the creations of new reservations. such voids are generated when offset-time-based qos or fdl sets are used. luac with void filling (luac-vf) is proposed [cit] ."
"without overall network information and time synchronization, it is very complicated to study distributed data collection algorithms in asynchronous crns. there are three main challenges. first, data transmissions of sus should not cause any unacceptable interference to pus. therefore, how to guarantee that a secondary network does not interrupt the primary network in a distributed manner is a challenge. second, centralized algorithms can make an overall optimized decision, however, it is difficult for distributed algorithms to guarantee an overall optimized solution by using only local network information. hence, how to design an effective distributed data collection algorithm for crns is another challenge. third, in an asynchronous crn, many data collisions, interference, and retransmissions occur due to lack of time synchronization, followed by capacity degradation and unfairness among data flows. therefore, how to overcome the problems induced by lack of time synchronization and meanwhile taking fairness into consideration is also a challenge."
"the nodes in c initiate data transmissions simultaneously. then, to guarantee sus will not cause unacceptable interference to pus, it is sufficient to have the pcr"
"based on the constructed data collection tree and the pcr r, we propose an asynchronous distributed data collection (addc) algorithm (algorithm 1). in algorithm 1, c is the time duration of the contention window and c g . since we assume that the size of a secondary data packet is smaller than the size of a primary data packet, we further assume that a secondary data packet can be transmitted during time à c when the spectrum opportunity is available. moreover, assume that no two sus located within each other's pcr have their backoff timers expired at the exactly same time. 2 the basic idea of addc is as follows. on the view of macroscopic, for each su, e.g. s i, with data for transmission, after setting the backoff timer, s i will sense the local communication environment within its pcr, which is determined according to the system parameters, e.g., path 2. collisions due to simultaneous countdown-to-zero can be tackled by an exponential backoff mechanism in which the transmission probability of each su is adjusted in a dynamic way based on the network business [cit] . loss exponent, sir threshold, working power, etc. on one hand, if s i senses that some node within its pcr is transmitting some data, i.e. the spectrum is occupied by some node, s i stops the countdown process (freezes the backoff timer). this is mainly because if there is some transmission activity within the pcr of s i, then the spectrum is not available for s i and thus the spectrumnot-available time could not be counted in s i 's timer countdown process. as the results of this freeze, 1) s i will not perform any harmful impacts on the ongoing data transmission; and 2) it can also be guaranteed that no two sus located within each other's pcr have their backoff timers expired at the exactly same time. on the other hand, if the sensed spectrum is free, s i continues the timer countdown process. when the backoff timer expires, s i transmits the data packet when a spectrum opportunity appears and then waits for extra time of duration c à t i, where t i is the selected backoff time of s i . the transmission of s i can be guaranteed when the spectrum opportunity appears mainly because of the assumptions that 1) no two sus located within each other's pcr have their backoff timers expired at the exactly same time, and thus s i will not be interfered by other sus within its pcr; and 2) each su decides whether to transmit data at the very beginning of each time slot which implies an su will not start a data transmission in a time slot after the beginning time point. furthermore, we assume that c g and a secondary data packet can be transmitted during time à c . consequently, s i will not be interrupted by pus during its transmission. on the view of microscopic, each su will compete the spectrum opportunities with other sus located within its pcr area. this can be controlled by the backoff timer setting together with fairness control (which is shown in theorem 1). the detailed competition process is demonstrated by the example cases shown in fig. 4 and analyzed in the proof of theorem 1."
"it is interesting to note that the optimal location for stabilizing the battery is the inverse of the optimal location for day trading. (18) shows that a battery has a larger influence on the voltage if it is further away from the transformer. while this is a desirable trait if the battery is used for network stabilization, a battery used primarily for day trading should be placed as close to the transformer as possible to minimize the impact on the grid. however, regarding voltage problems the negative impact of day trading can be partly mitigated by using the reactive power control capabilities of a battery inverter. these capabilities were not available in this field test and are therefore beyond the scope of this paper."
"student models represent student knowledge, skills and expertise. these cognitive aspects cannot usually be observed, but they can be elicited through what students say or performance in their learning processes. after cognitive analysis, an assessment construct can be established."
"the network time is slotted and the duration of a time slot is . we use a generalized probabilistic model to describe the data transmission activities of pus. during a particular time slot, each pu transmits data (performing as a transmitter) with probability p t . generally speaking, given a specific probabilistic distribution, such as the poisson distribution and the uniform distribution, of the activities of the primary network, p t can be determined accordingly. furthermore, at the very beginning of each time slot, if a pu is not scheduled to transmit a data packet, it will keep silent in that time slot."
"in fact, evidence models consist of two sub-models: (a) the evaluative sub-model, and (b) the statistical model. the evaluative sub-model is composed of a set of evidence rules by means of which feature of student's performance are extracted. the statistical model is applied to make inferences about assessment constructs (student models) based on the evidence variables."
"in the previous section it was shown that there is a linear relation between battery power and voltage level, which motivates the next formula. once the location is determined, one can determine the minimal required power rating of the battery with the following formula:"
"the lv networks are generally very well conditions for linear simulations. compared to the mv network, they consist of relatively short cables with a low x/r ratio. as can be derived from (1), the difference between the non linear constant power model and the constant impedance model is caused by the voltage drop i.e. the difference of the estimated voltage u n,ref and the actual voltage u n . according to dutch law and alliander dno policies, the voltage drop in the lv network is not allowed to be more than 4.5%, resulting in a worst case difference in absolute voltage of less than 1 v in a network which operates on 230 v and has a 4.5% voltage drop. however, the linearization quickly loses its accuracy as the voltage drop gets higher and is only to be applied on networks with a relative small voltage drop."
"there are three different architectures for wavelength routing technique, which have two phases in their operations: odl are used to solve contention; then packets are routed to output using wavelength routing. those switching architectures are: input buffer, input buffer with a distribution network, and waspnet."
"the diagnostic assessment model for algebra i learning was simulated with 3 learners' data. with a larger sample size, this assessment model will become more robust."
"in this subsection, we analyze the delay and capacity performances of addc. in our analysis, since c is very small compared with the waiting time for a spectrum opportunity and the data transmission time, we ignore the delay induced by the backoff time. actually, considering the backoff time only introduces a constant factor to the delay and capacity of addc."
"for the atm networks, there are two burst level admission control mechanisms [cit] : telland-go (tag) and tell-and -wait (taw). when tell-and-go (tag) is used, the source transmits bursts without making any bandwidth reservation in advance. sometimes bursts need to wait at the intermediate nodes until the forward path is cleared off. if such a path is not created, the signal is dropped off, or arrives at a later time than the anticipated time of arrival. this forms the basis for tera-bit burst switching [cit] . however, tell-and-wait (taw), the source reserves a path up to the destination with a short request message prior to transmission. the intermediate nodes reserve a specific output link. this is informed to the source when the entire path is reserved. only then the source starts sending the burst. if the path is not reserved, the source is notified and it dropped sending the burst. if compared for propagation delay and burst length, tell-and-wait (taw) outperforms telland-go (tag). on the contrary, tag is better when propagation delay is significant to the burst length. a variant of tell-and-wait (taw) is just-intime (jit) [cit] . just-in-time (jit) means that by the time a burst arrives an intermediate node, the switching fabric has already been configured, as shown in the fig 8. in this scheme each burst transmission request is sent to a central scheduler. the scheduler then informs each requesting node the exact time to transmit the data burst. another version of the jit protocol is just enough time (jet) [cit], based on hop-byhop reservation. this is one of the prevailing distributed protocols for obs network. this does not require any kind of optical buffering or delay at each intermediate node. each control packet carries the offset time information and makes a delayed reservation for the corresponding burst at the expected arrival time of the burst. the bandwidth is reserved for the first burst starting from the burst arrival time. at each intermediate node, the offset time is reduced to compensate for the actual control packet processing switch configuration time. fig. 9 . jet protocol [cit] the effect of offset time varies with the deflection routing in the obs network. if the burst takes a longer path, the minimal offset time for the primary path might not be enough. the burst length information is carried by the control packet. this enables to reserve a closeended reservation for the burst duration and an automatic release at the end. the intermediate nodes can make intelligent decisions on the possibility of such a reservation for a new burst and increase bandwidth utilizations."
"from lemma 4, we can derive the following lemma, which states the number of dominators and connectors within the pcr of an su in algorithm 1. ì an su obtains a spectrum opportunity only when no ongoing data transmission is initiated by any pu within its pcr during a time slot. consequently, we show the expected waiting time and probability of a spectrum opportunity appearance for an su in lemma 7."
"the standard approach for modeling dno power grids is formulating a load flow problem and solving it using a newton-raphson methodology [cit] . usually the load is modeled as a combination of a constant power, constant impedance and constant current [cit] . this paper however proposes a simple linear load flow approach by only using a constant impedance load model and investigates its feasibility in a real world situation."
"to determine if the constant-impedance model is indeed accurate on the voltage range of the lv network, a short experiment was performed. as can be observed in fig. 7, the battery was given a significant 'saw tooth' shaped load profile as a reference. the charging experiment was performed in a few hours around noon, which is the time with the least power consumption during the day because of the presence of solar panels. the customer power consumption is significantly less than the battery power."
"however, dnos do generally not have the knowledge to design and employ a community battery, which results in both newly planned and currently installed storage capacity not being used for congestion control. this paper provides the necessary theory to solve this problem, backed up by experimental results. with the principles developed in this paper, a dno can quickly estimate the potential (de) stabilization of a community battery on the steady-state voltages and currents in the grid. the control framework provided can be used to safeguard network constraints and is compatible with other battery control goals, such as energy trading or energy independence."
"regarding model stability, a linear network model it is not prone to finding unfeasible solutions or numerical difficulties, which can occur in normal load flows [cit] . given that stability of a controller is essential, this property makes the linear method more suitable for control."
"in this section, the previously proposed formulas are applied on the community battery of rijsenhout as an example. while studying the network of rijsenhout, it was discovered that the largest voltage problem is 5 v. the location of this problem is at the customer closest to the community battery. applying (18) results in a required battery power of 15 kw. this is much less than the battery's rated power of 50 kw. no current problems were measured in the network of rijsenhout, so (19) does not need to be applied."
"to this end, a barrier function has been implemented. the barrier function gives a large penalty for violating the voltage and current boundaries, barely influencing the regular optimization. in case of unattainable requirements, the solver will still find a solution which violates the constraints as little as possible. the variable corresponding to the barrier function is p over . this function is given a large weight c, where ≫ c e. the definitive optimization problem now becomes:"
"the alternative is to convert the optical packet into electrical one, which is not viable since electronic memories can not match the speed of optical network. however, there are no suitable way to buffer the optical signals."
"not all simulation environments can solve complex numbers. for example, the programming language r and the matlab plc compiler have out-of-the-box support for solving matrices which are both sparse and complex. in these cases, it is beneficial to simulate the imaginary parts of the load flow simulation in terms of only real numbers."
"cognitive diagnostic assessment (cda) [cit] in their evidence-centered assessment design. this framework is an effective structure for designing, producing and delivering assessments. these can be used to provide cognitive diagnostic assessment information. this model contains three logically connected models: student model, evidence model and task model."
"thus, a bayesian network and a cognitive diagnostic assessment are integrated into a unified model called bayesian cognitive diagnostic assessment model. this model basically consists of two kinds of variables: explanatory variable and evidence variable. an evidence variable is an observable variable and an explanatory variable is an unobservable variable. the evidence variables can be assembled into different cognitive diagnostic assessment models. figure 1 exhibits three explanatory variables, conceptual understanding, basic skills and symbol sense in the diagnostic assessment model; basic concepts, rule applications, term relations, procedural work, local focus, algebraic calculation, strategic work, global focus and algebraic reasoning are evidence variables. both explanatory variables and evidence variables consist of a bayesian cognitive diagnostic assessment model. students' achievement scores as evidence variable values update explanatory variables: conceptual understanding, basic skills and symbol sense. thus, a contingent table can be set up between each individual student achievement score and three explanatory variables. we will set up students' achievement scores in the horizontal level (row) and explanatory variables in the vertical level (column). the horizontal level provides a student with individual diagnostic information to inform student's progresses and obstacles; the vertical levels provide teachers and assessors with patterns of diagnostic information."
"given the model of the lv network and the framework of fig. 2, the next step is to formulate a battery controller which safeguards the voltage and current constraints of the network while being compatible with other control goals, such as day-trading. furthermore the algorithm has to be stable and operable in real-time."
"most dnos have design rules regarding lv network design, but do not have policies available regarding electricity storage as it is a relatively new phenomenon in mv/lv grids. this paper contributes to literature by both providing a battery controller and describing community battery network design guidelines, specifically aimed at dnos."
the basic working principle of the switch is as follows: the optical packet encoder consists of optical demultiplexers and tunable wavelength converter (twc). demultiplexers split incoming optical signal to the n different wavelengths. each wavelength is fed to a twc. twc converts wavelength of the packet suitable for the destination output buffer.
"however, matrix m is usually too large and too costly to invert. fortunately, it is not necessary to compute − m 1 . instead, it is more practical to solve:"
algebra is an important core subject area at the middle school level. algebra and other related mathematics courses provide students with knowledge and basic problem solving skills [cit] which directly influence success in advanced mathematics and sciences courses [cit] . deficiency of preparation of algebraic curriculum in secondary schools will impede mathematics learning and achievement.
"a straightforward way to do multicasting in an obs network is separate multicast in which multicast traffic and unicast traffic are assembled separately into different bursts. the tree-shared multicasting scheme was proposed [cit], that reduces the overhead due to guard bands and the control packet associated with each burst. this was also researched whether two multicast sessions could share a tree. if done in this manner, one of the benefits will be that a burst would be delivered to unintended destinations via a shared multicast tree. the other benefit would be the degree of overlapping among the multicast sessions through sharing strategy of the multicast tree."
"for monitoring overheating due to large currents and meeting voltage regulations, it is generally sufficient to model on a time scale of several minutes. the standard way to model such an electricity grid on this time scale is the load flow model [cit] . a load flow problem is generally nonlinear, due to its power constraints. this makes solving the necessary equations computationally expensive."
"learning tasks are usually defined as a series of objects which are implicitly contained in a learning environment [cit] . algebra i was taken as content domain. in algebra i learning, a learning task can be an algebraic concept, a topic, or a problem statement followed by several questions. some examples can be referred to in algebra i assessment [cit] ). an instructor emphasizes one or two aspects which reflect student progress in content and cognitive dimensions. in an assessment context, a learning task is elaborately tailored to a measureable object which reveals content, cognitive and psychometric characteristics. in this study, we develop learning tasks for assessment. the characteristics of these learning tasks in assessment will provide solid algebraic contents and cognitive evidences further develop similar learning tasks utilized in pedagogical environments. the tasks in assessment can be any formats such as essay questions, multiple choice questions, and other algebraic problems."
"packet header and payload in the scm are transmitted in the same time slot, with header is placed at an electrical sub-carrier above the baseband frequencies occupied by payload. packet header and payload are transmitted in the same time slot with separate wavelengths in the case of dual-wavelength coding."
"entire switch is slotted to facilitate the transmission of optical packet from the input to the output optical buffer. there are three parts in the switching fabric: optical packet encoder, space switch, and optical packet buffer."
"during the experiment, only active power was considered because very little reactive power could be expected to be present in this lv network. the customers in this network are regular households, which are known to consume little reactive power. also, the x/r ratio of the cables in the network is very low, making the phase angle nearly constant in the entire network. there is also a practical reason for neglecting reactive power as the installed sensors only logged real power."
"a step-by-step method was proposed for quickly modeling the impact of new and existing batteries on the lv grid. both the stabilizing and destabilizing potential regarding steady-state voltages and currents of the battery can be quickly estimated. by unlocking the potential of battery storage on a dno level, a fast and secure energy transition is one step closer."
"in our proposed distributed data collection algorithm, all the sus have to carrier-sense the spectrum to obtain a spectrum opportunity. therefore, we need to study how to properly set the carrier-sensing range with the objectives: 1) the secondary network does not cause unacceptable interference to the activities of the primary network; 2) all the sus transmitting data simultaneously are interferencefree; and meanwhile 3) the carrier-sensing range is as small as possible, which implies sus can obtain more spectrum opportunities. we have the following definitions."
"the difference between optical packet switching (ops) and optical burst witching (obs) was discussed in the article. the concepts were discussed comparing with existing optical switches. however, the article did not show the economical aspects for the obs. the paper tried to comprehend various researches those were done in the past. both the ops and obs are in the experimental stage. one of the draw back in developing commercially viable optical packet switches is that there are no effecting optical buffers. also ip packet transmission is required to be solved, regarding the size, memory capacity, nature of the opto-electric interfaces. though optical burst switches seem more appropriate for commercial use, no optical buffering should be included. this type of switches may be initially deployed at the metropolitan rings to connect ip routers. various models and algorithms were discussed. the article showed that there are many works left for the researchers to develop obs."
"it is observed that for a 1 millisecond delay, 200 kilometers of fiber is required. though recent advancements in semiconductor optical amplifiers (soa) are observed [cit], still it will take some more time before obs could be used in the transmission backbones. a comparison between optical packet switching (ops) and the optical burst switching (obs) is illustrated in the table 1 . according to the comparison shown in table 1, it is observed that the procedures for processing and synchronization of the overhead for obs are better than the ops. the main advantage, as we also discussed in the text, was that the obs do not require any optical buffer for storing data in the switching node."
"suppose a continuous data collection task consists of q ! 1 continuous snapshots. then, the delay and capacity of addc to finish this continuous data collection task are shown in theorem 3."
"as additional validation, the theoretical network design formulas proposed in this paper will be tested on more test beds. a test bed of 50 residential batteries is currently in development. researchers are welcome to contact the corresponding author of this paper to see if their own algorithms can be tested at our facility."
"here a is a directional connection matrix. every row corresponds to a network bus. every column of a corresponds to a network cable. each cable should have exactly one starting point denoted by a '1' and one end point denoted by '− 1'. it does not matter which bus of a contains the minus sign, as the resulting admittance matrix y will stay the same. z e is a square matrix and has the corresponding impedance of each cable and the equivalent resistance of the customers (z eq ) on its diagonal. since the matrix is diagonal, its inverse can be easily calculated by taking the inverse of every diagonal element."
this sections investigates how well the linear constant impedance load flow model applies to low voltage networks. the lv model constructed in the previous sections relies on a main assumption: the load model is assumed to behave as a constant impedance.
"lemma 4: [cit] . assume that d is a disk of radius r d and m is a set of points with mutual distance of at least 1. then,"
"using the conventional load flow software and modeling assumptions of liander dno, an analysis of the network of rijsenhout showed that the network was expected to have no voltage or capacity problems. however, during the experiments it became clear that the conventional modeling assumptions were incorrect and the network was subject to voltages which were too high according to regulations. sensor data proved that the voltage problems were caused by fluctuations of the voltage on the medium voltage grid, which directly influenced the voltage of the low voltage power grid and exceeded the modeling assumptions. however, this situation provided an excellent opportunity to prove that the battery could also mitigate the voltage problems. dutch inverters are required to automatically switch off in the event the voltage is above 250 v to mitigate voltages and this threshold was exceeded on a regular basis."
"for experimentation purposes liander, the largest dno of the netherlands serving over three million customers, placed a community battery in rijsenhout, a suburban village close to amsterdam, the netherlands. a schematic overview of the network of liander dno is displayed in fig. 1 . the battery is connected to the low voltage network and has a peak power of 55 kw and a capacity of 126 kw h. the main goal of placing the battery was the broad goal of obtaining practical knowledge how a community battery can benefit the dno. this paper reports on various aspects of dno community battery utilization. it contains control strategies for using a community battery for lv network congestion management. it is the first study to combine a battery control system with a real time grid model. it also analyses the battery's (de) stabilization potential and provides design guidelines for new community batteries."
"restriction is a word used to describe a category that includes solving equations and inequality problems, and calculating values of unknown variables that satisfies the required conditions [cit] . these kinds of problems not only exist in algebra, but are also found in economics and health sciences. function is an approach to dealing with relations between variables. two features are highlighted: covariance and dynamics. changes are observed between two variables. thus, algebra can be used in expressing relationships between two or more quantities."
"here z eq is the equivalent resistance of the customer, p user the real power consumption of the customer, n is a bus which represents a customer connection and u n,ref is the voltage at the customer location."
"as can be seen in fig. 7, the battery ramped up and down from 50 kw, its maximum rated power. it can also be concluded that the battery can control the voltage at the end of the lv network either 12 v up or down, covers the entire range of the allowed 4.5% voltage drop on lv networks. to determine the exact relation between battery power and voltage drop, the plot in fig. 8 was constructed. from this figure, it can be concluded that the relation between battery power and voltage drop can indeed be approached by a linear function within the operating range of the battery."
"all the packets from the input ports are combined on different wavelengths and broadcast to all the outputs. wavelength selectors are used to select the output packet. thus the switch itself leads to multicasting. there are two different architectures: keops switch with a broadcast-and-select, and switch with broadcast-and-select and recirculation buffer."
"several researchers explore algebraic pedagogical strategies in chunking patterns. [cit] argue algebraic knowledge is necessary to make generalizations, in problem solving, and modeling real-world situation using functions. the national council of teachers of mathematics [cit] suggests ies.ccsenet.org international education studies vol. 11, no. 2; that algebra instruction focuses on (a) understanding patterns, relations and functions; (b) representing and analyzing mathematical situations and structures using algebraic symbols; (c) using mathematical models to represent and understand quantitative relationships; and (d) analyzing change in various contexts. [cit] believe that school algebra should also focus on patterns and formulas, restrictions, functions and representing symbols. this brief and logical summary can be applied to both algebraic content and cognitive domains. patterns and structures of algebra have been a focal point of many studies [cit] . these are about \"investigating, identifying and formulating similarities relating to general patterns and underlying algebraic structure\" [cit] ."
"it can be concluded from (18) and its underlying equations, that the location of the battery is its most critical aspect. (20) shows that the location has great impact on the required amount of battery power regarding voltage problems. because of the linear property of (18), a battery placed twice as far away from the mv/lv connection generally needs only half the power rating. current problems cannot be influenced at all if the battery is not in the correct position, as (19) requires the overcurrent to be between the transformer and the battery itself. once the minimal required battery power has been determined, one can determine the required battery storage capacity. the storage capacity should meet two criteria; it should be sufficiently large to provide the requested power and solve the voltage or current problem. these criteria are displayed in (21) and (22) respectively."
"there are three classifications of assembly algorithms: timer-based, burt-slength-based, and mixed timer-burst-length. in the timerbased scheme, the timer starts at a new assembly cycle after a fixed time t. packets arriving at the egress are combined into a burst. the time-out value is set carefully: if the value is too large, the packets will arrive at an intolerable time; if the value is too short, many small bursts will be generated and make overhead complex. however, the burst length is not guaranteed. the size of a burst is equal to the sum of the size of all the packets arriving in a fixed time period. also the burst's interarrival time is constant. for a burst-length-base scheme, a predetermined minimum burst length is considered. with new incoming bursts, the length of the already stored packets exceeds the minimum length. in this scheme, there is not prediction of assembly delay time. this has a gaussian distribution characteristic. the variance of inter-arrival time of bursts that come from different edge nodes may become small with heavy traffic loads. this result in unwanted collisions of bursts and generates extra offset time losing synchronization. another way to achieve the both discussed in the above is through mixed-timer-burst-length scheme [cit] . this is implemented with a specific burst length or a fixed timeout threshold. adaptive assembly algorithms -explore either the time threshold, burst length threshold or both adjusted dynamically. this algorithm has greater operational complexity though shows better performances with strongly correlated input packet traffic. bursts generated in the adaptive assembly algorithm are buffered in the queue for an offset time while the control packet clears the path for the data signals in advance."
"proof. in a continuous data collection task consisting of q snapshots, each node in v s n ðd [ cþ has q data packets for transmission. therefore, based on theorem 1 and corollary 1, it is straightforward that the time consumption to collect all the data packets at v s n ðd [ cþ to d [ c is upper bounded by"
"hence, symbolic representation uses a set of signs, symbols and rules to express the variables and relationships among them. thus, algebra problems require translation from the problem context to algebraic language in a compact manner. representing symbols is also involved in developing a structure by which algebraic relationships such as variable relations can be represented [cit] ."
the output interface regenerates optical signals and put header back to the packet. the control unit controls whole affair. optical packet switches are typically designed for fixed-size packets.
"where dðá; áþ is the euclidean distance between two nodes, 9 2 is the path loss exponent, and p is the threshold sir value for the primary network. similarly, for 8s i 2 s t s, assume its intended receiver is s j . s j can successfully receive data from s i at time t only if the sir at s j associated with s i satisfies"
"considering that the upper bound of capacity for the data collection problem is oðw þ [cit], addc has already achieved the order-optimal continuous data collection capacity. ì from theorem 3, we can see that even continuous data collection introduces more traffic, more wireless interference, and the data accumulation problem, addc can still achieve order-optimal continuous data collection capacity, which is independent of the number of snapshots in a continuous data collection task. therefore, addc is scalable not only with respect to the number of sus in a secondary network, but also with respect to the number of snapshots in a continuous data collection task."
"since the voltage at the customer is usually not known, the reference voltage is be assumed to be the nominal voltage. from fig. 3 it can be observed that all nodes on the end of the network are now defined as swing buses, i.e., fixed voltage points. as the power constraints are replaced by resistances, the network now only consists of voltage sources, ground connections and resistors, resulting in a fully linear model."
"compared with snapshot data collection, continuous data collection introduces more communication traffic, therefore more wireless interference and longer transmission time. in addition, due to the fact that the base station can receive at most one data packet during a time slot (under the single-radio single-channel assumption), data packets are easier to accumulate at the nodes close to the base station."
"researchers (e.g., [cit] believe that algebra learning and progress can be examined in both conceptual understanding and procedural skills in a given algebraic sub-domain. current debates about this perspective are on the relations of conceptual understanding and procedural knowledge. several researchers claim that conceptual understanding is more important than procedural knowledge in algebraic learning. the conceptual understanding dominates cognitive processes while learning algebra and conceptual understanding is intertwined with the development of procedural skills [cit] . students with a greater conceptual understanding have better procedural knowledge in solving a math problem. conceptual understanding precedes procedural skills, further evidence indicates that conceptual understanding predicts student problem solving proficiency with their procedural knowledge [cit] ."
"the relation between procedural skills and conceptual understanding is a well-explored field. [cit] postulated the concept of mathematical proficiency based on five components: (a) conceptual understanding (b) procedural proficiency (c) strategic competence (d) adaptive reasoning, and (e) productive disposition."
"here t s is the sampling period, n the number of samples and p i min, the required battery power at time step i. the formula should be applied for the duration of the voltage/current largest problem. since community batteries are generally not meant for seasonal storage, it is sufficient to sample several days."
how to elaborately discern and effectively assess algebra learning progresses and difficulties has increasingly become a serious problem for algebra educators and algebra learning researchers. many algebra teachers face the situation that high school students do not comprehend algebraic and mathematical concepts and cannot proficiently establish connections among these algebraic concepts. students had difficulty understanding expressions and translations among algebraic relations [cit] . these same students had trouble solving problems involving equations. they also had difficulty with functions and non-linear relations and applying relevant algebraic knowledge specific in problem contexts.
"furthermore, not much literature is available on applying and validating proposed algorithms on real world batteries. the community battery subject of this study is only the second ever in the netherlands. the first one was placed by the dutch dno enexis and has been used for validating a charge path optimization algorithm, reducing network losses and reducing transformer peak load [cit] . however, since the battery was located next to the dt transformer, the ability to influence the lv network was very limited in contrary to the community battery subject of this paper."
"this literature review consists of three parts: algebra learning, cognitive task analysis and assessment. in the algebra learning part, we present conceptual understanding, procedural skills, strategic problem solving, reasoning skills and symbol sense. understanding these aspects well can foster understanding reasons for students' mistakes, difficulties and obstacles. cognitive tasks and analysis describe the rationale of how high quality cognitive tasks connect learning and assessment effectively."
"international education studies vol. 11, no. 2; the content dimension indicates what kinds of knowledge students learn; the cognitive dimension reveals what cognitive characteristics the task focuses on."
"crns introduce a novel promising communication paradigm, where sus can opportunistically access unused licensed spectrum without harming the communications among pus. in this paper, we study the distributed data collection problem for asynchronous crns. first, we study how to set a proper carrier-sensing range (pcr) for sus. subsequently, based on the derived pcr, we propose an asynchronous distributed data collection (addc) algorithm for crns with fairness consideration. through theoretical analysis, we show that addc successfully achieves order-optimal delay and capacity, which implies addc is scalable and practical. thirdly, we extend addc to deal with the continuous data collection issue, and analyze the delay and capacity performances of addc for continuous data collection, which are proven to be order-optimal. finally, the simulation results demonstrate that addc can effectively accomplish a data collection task and significantly reduce delay."
"it implies that the achievable continuous data collection capacity of addc is lower bounded by w npo ðnà1þð2 þ24 þ1 à1þ á w, which is order-optimal."
"here p u t,max, and p u t,min, are the maximum and minimum power the battery is allowed to inject into the network at time t without violating any voltage limits, u max and u min are the maximum and minimum allowed voltage at each customer by law. u n t, is the voltage at each customer which can be calculated by solving (2) . u δ p n w, is the voltage drop by applying 1 w of battery power to the grid in v/w. this variable is time-invariant and only depends on the network properties. the number can be obtained by solving by setting the battery power to 1 w, setting the customer power to a low but nonzero value and solving (2) . the customer load cannot be set to zero as most qr solvers cannot cope with infinite resistances. u δ p n w, is network-dependent and does not vary over time."
"there are two main motivations considered to place a community battery for network congestion reduction: to control the community voltage and to control the community currents. while it is theoretically possible to also control the network power factor to some extend, this is currently not a priority for distribution network operators, because of its rare occurrence."
"4. for the case that t i is before t j, it is clear that s i will transmit one data packet before s j according to algorithm 1. then, p is true."
"some researchers [cit] examine \"stumbling blocks\" related to (a) abstraction, (b) overgeneralization, and (c) procedural fluency and symbol sense. abstraction is the first difficulty. students could understand a concrete example such as a formula. however students may have difficulty transferring the formula to a general function, i.e. from abstract to concrete. the second difficulty is overgeneralization [cit] . in this situation, students struggle with identifying generalizations and their limits. students can work well in a simple situation. however, they make mistakes again when they solve different problems in a complex situation. the third difficulty is related to procedural fluency and symbol sense. although students have mastered the algebraic problem rules, and are able to apply them in a simple problem situation correctly, they still make mistakes in other similar situations. students lack structure knowledge of algebraic problems and further lack insight for certain algebraic problem steps."
"ì from theorem 1, we can see that algorithm 1 also takes fairness into consideration when it collects data. therefore, the waiting time for a node with data transmission is upper bounded. it follows that skewed data flows can be avoided in addc. furthermore, the following corollary can be obtained straightforwardly. from theorem 2, the proposed addc successfully achieves order-optimal data collection capacity even working in a distributed and asynchronous manner. compared with the existing order-optimal centralized algorithms, addc is scalable and more practical for unstable and frequently changed crns."
"various receding horizon controllers for battery charge path optimization have been developed [cit] . recently, battery controllers using model predictive control (mpc) have been proposed [cit] . these controllers can be deployed both centralized or decentralized. however, these controllers generally do not guarantee a stable grid operation as the currents and voltages in the grid are not taken into account. this is also due to the fact that the non-linear load flow equations cannot be directly applied in the quadratic mpc framework. furthermore, these mpc controllers have yet to be tested in a real world test bed. this paper contributes to the literature by proposing and validating a network model which is directly integrable into the mpc framework. this is the first study that enhances the load path optimization problem by adding a real time network model, making it more interesting for a real world application. most of the current algorithms work with predefined congestion points, i.e. network nodes which are expected to be most vulnerable to capacity or voltage problems. however, in practice the points are often hard to clearly define for large networks in which the loads are constantly changing. therefore adding a network model which is able to monitor all network nodes and lines simultaneously is a valuable addition to the current literature."
"the nodes in c initiate data transmissions simultaneously. then, to guarantee every su in c \\ v s can conduct data transmission successfully (i.e., every su in c \\ v s has a spectrum opportunity and all the sus transmit data without interference), it is sufficient that"
a simple but realistic situation is assumed. the network has a relatively simple radial structure and its cable locations and properties are known. it is also assumed that the location and size of the voltage problems are roughly known. these either have been determined using to determine the characteristics of the voltage drop the battery was given a 'saw tooth' shaped charging profile. it can be observed that there is an approximate linear relation between the battery power and the voltage drop between the transformer and the battery. smart meter data or direct (temporary) measurements.
"furthermore, relatively little is known about the exact nature of low voltage network load. it is the author's ambition to create an accurate load model by applying system identification methods, using the community battery as a means to 'excite' the network to find the voltage and current dependability of the power loads. this could result in a general method for dnos to identify the load types of their networks."
"the energy landscape is expected to change significantly in the netherlands over the next decades, as the share of renewable energy is increasing. this poses a significant challenge for distribution network operators (dnos), which are responsible for maintaining a reliable and affordable electricity distribution grid. especially the rise of residential solar power is challenging, as these installations can cause local voltage problems which can be cost intensive to solve."
"there are switching and scheduling sections in this switch [cit], as shown in fig 4. incoming and outgoing ports carriy single wavelengths. output wavelength varies with packets. scheduling section is used to solve contentions though twc at the inputs, then passes the packets through various arrayed waveguide gratings (awg) and odl in different time slots. the delay rule for this switch: for a single time slot, outputs can not receive two similar packets; buffer output can not transmit two packets at the same time. the switching section is made up of awg and twc. thus this section switches optical packets to their assigned outputs. however, one disadvantage of this switch is that it may suffer from headof-line blocking."
"the deviations around zero battery power, which can be observed in fig. 7, are caused by the imperfect inverter. the battery inverter cannot behave linearly at very low battery power levels. it can also be observed in fig. 8 that the linear fit has a slight additive bias. this is caused by the small residual load which also can be observed in fig. 7 ."
"this study developed a diagnostic assessment model for algebra i learning with bayesian network model. the bayesian network model represented the diagnostic assessment model in both conceptual and quantitative representation. the model had four exploratory variables which represent the learners' proficiency in solve algebra i learning tasks. the proficiency was represented in three aspects: conceptual understanding, basic skills and symbol sense. there were 9 evidential variables which generated the students' scores, which were updated in the bayesian network assessment model. it noticed that the hierarchical diagnostic assessment model was non-linear. from student b's score in algebra i learning we found that even though the student performed with relatively lower score in conceptual understanding, basic skills and symbol sense, the algebra i learning scored higher scores at 50.29 of 100. briefly this was related to that the assessment model statistically is nonlinear. the diagnostic assessment model was a dynamic model which recorded the all possible learning trajectories. thus the assessment model can effectively differentiate different cognitive aspects: conceptual understanding, basic skills and symbol sense. this diagnostic assessment model provided an example for how to effectively assess algebra i learning. the content domains and cognitive structures always vary. the diagnostic assessment model should be specified to stress on different contents and cognitive structures."
"the burst scheduler creates burst and their corresponding control packets, adjust offset time for each burst, schedule each burst on each output link and forward bursts and their control signals to the core network [cit] . recent studies show that different assembly schemes affect the assembled bursts traffic's characteristics [cit] ."
"here p t is the real power the battery supplies to the electricity network at timestep t. e is the energy stored in the battery, which cannot exceed e max . the first constraint corresponds to the network-related power limit. the second constraint ensures that the battery will not discharge when it is empty and not charge when it is full. if all the currents and voltages are within their boundaries, the battery does not need to act. however, if an undesired value is found, the battery will try to mitigate the problem. however, the formulated problem can not directly be put in a linear solver in its current form. to solve the problem using a linear solver, it is necessary to incorporate the absolute term of the objective function into the constraint function. this is achieved by adding an extra dummy variable e . also, the required voltage and current boundaries may be unattainable, because of the practical limitations of the battery. in such a situation, the linear solver will not find a feasible solution and the battery will be inactive. a more desirable behaviour is in a practical case to meet the required voltage and current constraints as much as possible."
"in algorithm 1, each su sets its carrier-sensing range to r in line 1. in lines 2-12, the data transmission process of one data packet is described. in line 3, the backoff timer of s i is set. subsequently, s i senses the spectrum (line 5). if the spectrum is busy, s i stops the countdown process (lines 6-7) . otherwise, the countdown process of s i 's backoff timer continues (lines 8-9) . if the backoff timer expires, s i carries out a data transmission when a spectrum opportunity appears (line 11). since we assume sus within each other's pcr will not have their timer expired at the same time, line 11 can be accomplished without interference. taking fairness into consideration, i.e., to avoid s i always occupying the spectrum, we still let s i wait for time c à t i after it transmits the current data packet (line 12). this can also be proven in theorem 1 (section 3.4)."
"since addc is a distributed and asynchronous algorithm, each su in addc makes actions only based on the status of the local wireless communication environment. therefore, addc still works for continuous data collection tasks because compared with snapshot data collection, continuous data collection just introduces more traffic loads without changing the wireless communication environment. in the following of this subsection, we analyze the delay and capacity performances of addc for continuous data collection."
"this corresponds to the c1channel in the fig 10. there are several classification of luac-vf: min-sv (starting void), min-ef (ending void), and best fit [cit] . min-sv is similar to luac-vf that employs techniques of computational geometry and is faster than luac-vf. min-ef minimizes newly created voids. the best fit minimizes the total length of starting and ending voids, generated after the reservation."
"mathematizing and thus, algebratizing a situation requires the use of symbol sense [cit] . symbol sense includes strategic work, global focus and algebraic reasoning. it is not enough that students can use symbols and complex symbolic representations, they also need to be able to judge which symbols are relevant to a given situation [cit] . combining symbol sense, conceptual and procedural knowledge and strategic problem solving will lead to a focus on deeper understanding of algebraic concepts."
"recent exploration and advancements in learning theories, cognitive diagnostic assessment and mathematical learning provide new knowledge and experience for our multidimensional diagnostic assessment research study."
"this problem can be directly solved by a linear optimization solver. since the problem is linear, the solution will be optimal if it is found. if the constraints do not conflict with each other, the solution always exists."
"in this formula e min is the minimal required storage capacity (kw h). c is the so-called c-value [cit], a factor which expresses the relation between battery power and capacity (kw h/kw). this factor is mostly technology dependent and can be as high as 3 for lithium-ion batteries [cit] . the second criterion is:"
"a wavelength-routed network which carries data from one access station to another without any intermediate o/e conversion is referred to as an all-optical wavelength-routed network. the wavelength routing in the said manner are done through optical packet switching (ops) and optical burst switching (obs). according the contemporary works discussed in various research articles, every researcher is of the opinion that wdm shall be the future solution to the explosive growth of the internet, hence compatible switching like optical packet or optical burst switches will be required for switching optical traffic. out of different developed techniques for the transfer of data over wdm, broadcast-andselect, wavelength routing, optical packet switching, and optical burst switching are mentionable. wavelength routing networks are already in use. prototypes of broadcastand-select networks have been developed. however, optical packet switching and optical burst switching are still in the research phase."
"exploiting wavelength domain -in the wdm, sometimes two packets are ready to be transmitted through the same output channel, creating blocking. one solution is that they are transmitted over two different wavelengths. this method requires that a single fiber combines number of wavelengths in it. the current coupling is about 200 wavelength/fiber. deflection routing -this method is generally implemented for the switches that have little buffer space. two conflicting output packets are routed as such that the correct one passes through the correct port. the other packet goes through available port and may result in longer delay or needed to be reordered at the destination. this process needs little or no buffering."
"there are many different variations of diagnostic cognitive assessment models. however the essence and crucial elements are on the examination of the learning process based on cognitive and learning theory. thus learning progresses, difficulties and obstacles can be identified."
"in the packet switches, sometimes more than one packet try to go out through a similar output at the same time, thus creates blocking at the output interface. the solution is made as such that the only the permitted one is allowed to pass and the rest of the signals are buffered. since this is a common problem for packet switches, three solutions are proposed: optical buffering, exploiting the wavelength domain, and deflection routing. optical buffering -optical delay lines (odls) or fiber delay line (fdl) are the only technique to implement optical buffering. the odl (or fdl) can make a packet wait for a specified amount of time. this delay time is related to the length of the delay line. however, delay lines are not commercially viable."
"a small downside of the constant impedance model is that it is prone to computational errors when a customer's power consumption is very close to zero. as can be seen in (1), if the power consumption is zero, the equivalent resistance is infinite. in practice, this problem can be easily solved by ensuring that the power consumption of each customer is always a few watts, which has a negligible influence on the outcome of the simulation."
"at its worst, the voltage problem was present for several hours. determining the required capacity by applying (22) yielded a required capacity of 35 kw h. however, given a c-value of 3 and using (21), the minimal required capacity turns out to be at least 45 kw h. this is also much less than the rated capacity of 125 kw h. it turns out the community battery of rijsenhout could have been approximately 50% smaller."
"in recent years, there have been increasing research studies focusing on mathematical learning and diagnostic assessment. [cit] summarized approaches to diagnostic assessment in three categories: cognitive diagnostic assessment, skill analysis and error analysis in mathematical learning. they also evaluated diagnostic assessment from pedagogical purposes in identifying persistent misconception, skills and errors. from content referent perspective, they also looked at diagnostic assessment as theory of cognitive processing in mathematics, broad skills across the curriculum, and procedural skills across the curriculum. from score estimation, cognitive diagnostic assessment represents knowledge state towards mastery of multidimensional cognitive attributes; skill analysis examines skill aggregation towards mastery of unidimensional subskills; and error analysis scrutinizes distractor element exploration in looking for error patterns."
task models provide a framework for establishing the contexts and tasks which will be used to observe individual performances. a task model is crucial to the assessment process because it determines what kinds of task variables can be extracted from data.
this paper provides a solid foundation for integrating residential and community-level storage in the existing lv network. a fast linear lv model was developed and applied to the lv feeder of the community-battery in rijsenhout. the model was proven to be sufficiently accurate for network stabilization purposes.
"a potential solution to this problem is congestion control using energy storage. by locally storing the energy generated by the solar power installations, the voltage and current in the low voltage network can be kept within the desired bounds. the most common version of this solution is a home battery system. however, it is more efficient to use a community battery since the home batteries are often not fully utilized [cit] . a community battery also requires less space and can be serviced more efficiently."
"for completeness, in this section, we study the performance of addc for continuous data collection in asynchronous crns. a continuous data collection task is a task to collect the data of qðq ! 1þ continuous snapshots to the base station without any data aggregation. similarly, the continuous data collection delay is the time consumption to finish a continuous data collection task. the continuous data collection capacity is defined as the average data receiving rate at the base station during a continuous data collection 3 . here, a stand-alone secondary network means the primary network does not exist or is not working, which implies all the sus always have spectrum opportunities."
the assessment model was also cognitive task structure specific. the researchers should have sufficient domain and learning science knowledge. thus the assessment model can provide effective diagnostic assessment information.
"burst assembly is the process of assembling incoming data from the higher layer into bursts at the ingress edge node of the obs network. as packets arrive from the higher layer, they are stored in electronic buffers according to their destination and class. the switching unit forwards incoming packets to burst assembly units."
"a second perspective is that conceptual and procedural knowledge may develop iteratively [cit], meaning that procedural knowledge could influence conceptual understanding. an iterative process model [cit] suggests that one type of knowledge gained first will improve another type of knowledge through problem representation. this perspective introduces a potential factor between the relations of conceptual and procedural knowledge development, and problem representation. in other words, cognitive tasks are a critical component to explore the relations of development of conceptual understanding and procedural knowledge."
"the controlled variable is the battery power at each time step p t . the final goal of the controller is to keep the battery at a certain given charge level e t,ref . this desired charge level is given by another entity, like a day-trader who is using the battery for energy trading. the optimization function is now posed as a discrete receding horizon problem. the objective function becomes:"
"a final advantage of a linear network model is its linear additive property, which means that each network load configuration can be simulated independently. in practice this means that all loads can be simulated separately and the resulting voltage drop and cable currents can be obtained by simply taking the sum of all solutions. this property will be exploited in next section to efficiently determine the maximum power the battery supply to or draw from the network."
"for solving (17), the optimizer depends on a prediction of the power consumption and solar power generation. this prediction is obtained by training a regression model using historical data and was provided by an external party. the model has a mean absolute percentage error (mape) of 5% for predicting household load and 10% for pv power 24 h ahead. during the experiment it was discovered that the accuracy of the energy consumption predictions is relatively unimportant. the controller anticipates on high demand or load by reserving capacity of the battery. using this available energy and the available real-time measurements, the controller then reacts to the voltage/current problems once they actually arise. it turns out that the most important prediction feature is the required amount of energy to mitigate voltage/ current problems, not the exact peak loads."
"bayesialab [cit], a bayesian network software, was used to build up the cognitive diagnostic assessment model. in this model, there are three explanatory variables: conceptual understanding, basic skills and symbol sense which consist of algebra i learning competence. in other words, cognitively we have to"
"in embedded-lr, cores never switch from speculative to nonspeculative execution. unlike \"best-effort\" htms, embedded-lr guarantees that every transaction eventually commits so embedded-lr is not subject to starvation. embedded-lr supports two abort policies: time stamp and priority abort. for the time stamp policy, the core with the earliest time stamp is allowed to proceed, whereas in the priority-abort policy, each core has a priority that is increased when it is rolled back and in case of conflict, the higher-priority transaction proceeds. table i summarizes all the possible configurations for embedded-le and embedded-lr. the two algorithms are discussed in more detail next."
"even though lock elision was proposed years ago, its main idea enters the mainstream via intel's haswell. the important benefit of hle is that it can be used in existing lock-based programs and it takes little programming effort to implement, making the solution suitable for legacy code. noting the adaptation of lock elision in future processor designs and knowing its potential, we propose in this article efficient alternative algorithms that extend the capabilities of these existing techniques by introducing an extra degree of flexibility and with energy efficiency as an additional primary criteria."
"moreover, figure 7, which reports the energy consumption for the same set of experiments, shows that for benchmarks that spend a considerable amount of time in critical sections, there is a significant reduction in energy consumption due to sleep mode (e.g., 18% for genome and reaches 48% for patricia). only redblack shows a slight increase (3%), while kmeans, skiplist, and labyrinth are not affected at all by sleep mode. since redblack has a very low abort rate, sleep mode only adds extra energy overhead."
"-unlike most prior works on speculative synchronization, we focus on energy efficiency as well as throughput since both are key constraints for embedded systems. specifically, we evaluate the energy-delay product (edp), a figure of merit that captures the trade-off between these two properties. -there are many possible hardware designs that could support lock elision. however, because we are considering embedded platforms, which are highly resource constrained, any proposed hardware mechanisms must be simple. we propose the addition of simple hardware structures that avoid changes to the underlying cache coherency protocol but leave us the flexibility to vary how synchronization conflicts are detected, how they are resolved (contention management), and which policy to use for switching between speculative and nonspeculative executions. -we provide a fully transparent solution for speculative execution of locks. this means programmers can take full advantage of the underlying speculative hardware support even when running code written using traditional locks. we evaluate our proposed architecture through a range of benchmarks written with standard locks."
"6 most benchmarks benefit in terms of performance from retrying the speculation several times, instead of not retrying at all (i.e., having number of retries set to zero). in particular, when the maximum number of retries is zero, performance generally tends to degrade as the number of cores increases. a limit of four is optimal for patricia and genome, but the rest of the benchmarks do not show significant change in performance based on which of the nonzero values we choose (vacation being the only exception, which shows a clearly worse performance if we restrict the number of retries to one instead of allowing more than one retry). because they both experience high contention, patricia and genome do not benefit from many retries. in benchmarks with high abort rate, switching to locking is preferable after a few retries, since speculation is likely to fail again. indeed, for patricia as the number of cores increases we have to make sure the abort rate does not increase to the point where it is counterproductive for performance. when we restrict the number of retries to zero, the abort rate is reduced to nearly zero, but very little thread parallelism is exploited for four or eight cores. if we allow one retry, the abort rate reaches 42% (for eight cores), but is still tolerable when it comes to improving performance. the same trend is experienced in genome as well. restricting the number of retries to zero gives a nearly zero abort rate, while allowing one retry yields abort rate 17%. for these two benchmarks that have highly contended critical sections, it is better to limit the number of retries, in order to prevent the abort rate from increasing to the point where it hurts performance. note that the exact same phenomenon is experienced in sleep modality."
"finally, in figure 24, we show the combined performance-energy consumption results and make the following observations. first, we see that embedded-lr and lock-sleep are the two best techniques when we care for both performance and energy consumption, with embedded-lr being better than lock-sleep for genome, redblack, and vacation (up to 19%) and very similar for skiplist, labyrinth, and kmeans. only for patricia, lock-sleep is better than embedded-lr (up to 12%), again because of its high abort rate. the next-best configuration for edp for most benchmarks is embedded-le-sleep with embedded-le being very close. the worst choice for edp is again locking. there are exceptions: first, kmeans does not show any significant difference for any of the applied techniques, which is expected as explained earlier. second, patricia is the only benchmark that shows a clear improvement in edp for embedded-le-sleep compared to embedded-le (73%). overall, with respect to edp, embedded-lr is the best choice, with lock-sleep following next."
"in this section, we evaluate the abort policies of the embedded-lr implementation. as described in section 3, this approach is distinct from embedded-le because the architecture does not use locks for mutual exclusion."
"we set the maximum number of allowed retries to different values, in order to see if the abort policy plays a different role in each case. we chose three different values: 0, 1, and 2 and an infinite number of maximum allowed retries, and for each of these values, we tested the two abort policies mentioned before. in all three experiments, the observed trends on performance, energy, and edp were the same, so here we will present only the results for edp that combines both metrics."
"the approach is implemented in fig. 1 which shows how to observe the dynamic system from measuring the input and output data of the real system. the pv system is constructed identification of hammerstein-weiner system for normal and shading operation of photovoltaic system and implemented for data collections purposes. the input-output data is recorded for the full dynamicity of the real-time system. the measured data are divided into several set of training and testing/validating of the identified model. nonlinear system identification is selected since the system operates in nonlinear ranges. furthermore, the model is estimated with experimental data, hence validated. once the proper model has been selected, estimated and validated, the model can then carried out for future intended applications."
"this article describes embedded-spec, an energy-efficient embedded architecture that supports lock elision, a synchronization mechanism that combines attractive properties of both locking and speculative synchronization. in lock elision, speculation is transparent: when conventional, lock-based software is executed, the underlying hardware attempts to execute certain critical sections as speculative transactions. when all goes well, threads that would have been serialized by a lock can execute that lock's critical section in parallel, provided there are no data conflicts at runtime. lock elision is appealing because it promises to increase concurrency without the need to retrofit code."
"3.4.1. embedded-le. figure 5 shows the flowchart of the embedded-le algorithm, which is implemented in middleware using application programming interface (api) function calls and hardware lock instructions (i.e., test(), testandset()). this algorithm ( figure 5 ) is called when a core tries to enter a critical section protected by a lock. for example, when core x tries to enter a critical section, it checks whether the maximum number of retries has been exceeded. if so, it calls testandset() to try to acquire the lock. if not, it calls test() to determine whether the lock is free. if the lock is not free, the core spins until the lock is free. when the lock is observed to be free, the core elides it, and executes the critical section speculatively."
"we draw the following conclusions: if reducing energy consumption is our primary goal, then we should use sleep-enabling techniques. moreover, we should not bother using speculation, but choose the lock-sleep technique instead. speculation is encouraged only in cases where we encounter increased parallelism. on the other hand, if performance is our primary goal, then embedded-lr is clearly the winner. finally, to improve the edp, we should generally pick embedded-lr or lock-sleep and avoid embedded-le or traditional locking. table iii summarizes the best and second-best configuration modes for each of the benchmarks we considered in our experiments. as we discovered, the best configurations may vary depending on whether we only care about performance, energy, or a combination of the two."
"hammerstein-weiner models were described as dynamic systems using one or two static nonlinear blocks in series with a linear block. the linear block is a discrete transfer function and represents the dynamic component of the model. in this paper, this structure is chosen as the best fitting model for nonlinear real-time ranges. fig. 2 shows the structure of nlhw which represents the dynamic system using input and output static nonlinear blocks in between dynamic linear blocks which is distorted by static nonlinearities [cit] . hammerstein-weiner structure is then used to capture the physical nonlinear effects in the system that will affect the input and output of the linear system. the applications of nlhw model depend on its inputs. if the output of a system depends nonlinearly on its inputs, it can be decompose the input-output relationship into two or more interconnected elements. this system is preferred because they have a convenient block representation, transparent relationship to linear systems, and easier to be implement than heavy-duty nonlinear models. in this paper, it is present an algorithm to identify single-input single-output (siso) hw systems."
"if eventually core x reaches the end of its critical section, it will check its own execution mode by calling check_in_transaction, to determine whether it has been running in locking mode or speculative mode in order to either release the lock or end the speculative execution."
"(1) embedded-le (embedded transparent lock elision): the critical section is executed speculatively by eliding the lock. the bloom module monitors memory accesses, and if there is a data conflict, it directs the conflicting cores to roll back their speculative executions and contend for the lock. one will succeed, and the rest will spin until the winner releases the lock. when the lock is released, the waiting cores retry their speculative executions. if the number of retries for a specific transaction (due to repeated conflicts) exceeds a threshold, the cores revert to nonspeculative execution for that instance of the transaction. when the end of the critical section is successfully reached, the number of retries is reset to zero. (2) embedded-lr (embedded transparent lock removal): as with embedded-le, the critical section is executed speculatively by eliding the lock, but in case of a data conflict, the bloom module directs all conflicting cores but one (the winning core), to roll back and suspend execution until the active core completes the critical section. when the winner completes, the suspended cores resume speculative execution, so a lock never needs to be explicitly acquired."
"we found that if we do not allow any retries on a failed speculation, then both abort policies yield the exact same results in performance, energy, and edp for all benchmarks, whether we enable sleep mode or not. this is expected since the abort policy does not really create much of a difference if we immediately switch back to locks on an event of a failed speculation. fig. 15 . edp for embedded-le using different abort policies and maximum number of allowed retries set to one. fig. 16 . edp for embedded-le using different abort policies and maximum number of allowed retries set to two. figure 15 shows edp results for the two abort policies in embedded-le mode when we allow at most one speculation retry. as we can see for all benchmarks, both abort policies show similar results, except for genome and patricia, which show considerable benefits for the requestor-abort policy compared to the abort-all policy, as the number of cores increases (18% and 10% improvement, respectively). if we allow two retries in embedded-le mode, as seen in figure 16, patricia is consistently better (21%) for requestor-abort, while genome does not show any difference in this case. vacation also shows a great benefit (19%) for eight cores. we note though, that as we increase the maximum allowed number of retries, the benefits of choosing the requestor-abort policy become more prominent. figure 17 shows the corresponding results for an infinite number of allowed retries in embedded-le mode. in this case, we observe a dramatic drop in the edp for specific benchmarks, like genome, patricia, and vacation (47%, 75%, and 63%, respectively) as we increase the number of cores. our conclusion from this set of experiments, is that for most benchmarks the abort policy does not affect the overall edp, but for genome, patricia, and vacation, the requestor-abort policy shows significant benefits that become more prominent as we increase the number of maximum allowed speculation retries. thus, we conclude that we could safely choose the requestor-abort policy every time we execute in embedded-le mode."
"in this section we evaluate the proposed embedded-spec design. we tested our architecture with several configurations. the first part of the evaluation is devoted to finding the optimal parameters in terms of energy consumption and execution time. our target metric is therefore edp, which is a standard, commonly used evaluation metric in computer architecture [cit] . as was done in previous work using the [cit] b ], the performance and power models are based mostly on data obtained from a 0.13 μm technology provided by stmicroelectronics for their nomadik platform [cit], and the energy model for the fully associative caches is based on efthymiou and garside [cit] . the second part of the evaluation is focused on evaluating the advantages of the optimal configuration over the baseline lock approach. the hardware parameters are reported in table ii ."
"note that for either abort policy, the aborted cores will have to explicitly acquire a lock once they have rolled back and restored their previous states. since multiple cores attempting to execute critical sections on the same lock id must be consistent (i.e., cores must all be executing either in speculative (le) or nonspeculative (lock) mode), in the case of the requestor-abort policy, the other cores will have to abort as well if the requestor core manages to acquire the lock before they commit. however, since the process of rollback can take several cycles, in many instances the nonaborted cores will commit before the lock is acquired, and therefore it would have been wasteful to abort all the cores immediately when the conflict was first detected."
"the aims of system identification are to obtain the best suitable mathematical model or transfer function of the real system between the exact data of inputs and outputs. the best fitting model can be helpful for gaining a better understanding on the real system and also useful to predict or simulate system behaviour, moreover to act as control technique for design and analysis of controllers which based on models of real systems [cit] ."
"before getting into the details of our implementation, it is important to describe the basic parameters of designing a speculative memory framework. here, we present a background discussion on the main components of speculative memory design and how existing transactional memory works used them. next, we will discuss our design choices and present the proposed algorithms."
"the workload is composed of a certain number of atomic operations (i.e., inserts, deletes, and lookups) to be performed on these two data structures. redblack trees and skiplists constitute the fundamental blocks of many memory management applications found in embedded applications."
"for energy consumption shown in figure 10, again zero becomes the worst choice as we increase the number of cores, but choosing between two, four, or an infinite number of retries does not make much difference for most benchmarks. retrying four times seems again to be slightly better for genome and patricia. for the edp, figure 11 shows that picking any nonzero number of retries will yield similar benefits for most benchmarks, except for patricia and genome, where restricting the maximum number of retries to four is clearly better (shows 10% edp improvement). if we had to choose a single maximum retry value to use for all benchmarks, we conclude that retrying up to four times would be overall the best choice."
"sleep mode. as described in section 3.4, the embedded-le implementation can be executed in conjunction with sleep mode, where if a thread is unable to acquire a lock immediately, it is switched to an idle state to reduce energy consumption. the energy savings, however, come at the expense of an increased execution time required to switch the cores back from sleep mode to normal operation. time is affected by including sleep mode execution along with lock elision (noted as embedded-le-sleep) as the number of cores is varied."
"the bloom module detects and resolves data conflicts during speculative executions. the affected cores will have their states restored, and the tx_abort register will be updated to indicate the abort occurred. the core calls the check_abort() function to determine whether it has been aborted. if not, the core proceeds speculatively. if so, the core terminates the speculative execution, and calls testandset() once to try to acquire the lock. if it succeeds, the core proceeds nonspeculatively. otherwise, the core returns to the retries count checkpoint to determine whether it should continue to speculate, or fall back to locking. either way, the core spins until the lock is freed."
"embedded-lr requires extensions to the bloom module, and small changes to the middleware, replacing each lock acquisition with a start_transaction instruction. once a core starts speculation, it will never try to acquire the lock, even in case of a conflict. when a conflict is detected, the losing cores are suspended."
"as seen in figure 6, all benchmarks except for patricia show an increase in execution time. 4 this increase is usually negligible and below 5%, but for vacation and redblack it reaches up to 6% and 10%, respectively. this increase is expected since switching to/from sleep mode imposes a small time overhead (0.2 μs, i.e., 40 cycles). only patricia shows a decrease in execution time of 4%. we believe that this happens because the small latency introduced by switching to sleep mode can shift timing in such a way that by the time sleeping cores wake up and retry speculation, the cores they previously conflicted with have completed their critical sections so they do not conflict again. for benchmarks such as patricia, which have relatively high abort rates, a timing shift can have a big impact on the resulting abort rate and hence on performance. indeed, in this experiment the abort rate for patricia decreased from 42% to 37% when using sleep mode."
"the bloom module [cit] ] is in charge of conflict detection and resolution. it monitors all transactional accesses, records them as per-core signatures, and notifies the cpus when data conflicts occur. as explained in more detail below, the bloom module used in this article departs from prior designs by also snooping on the semaphore memory."
"we described a lightweight implementation of lock elision on an embedded architecture. our implementation provides a transparent means of combining attractive properties of both locking and speculative synchronization. results show energy and performance benefits of our proposed scheme, especially for larger numbers of cores (e.g., four to eight cores). in comparing our two schemes (lock elision vs. lock removal), embedded-lr provides better performance and energy characteristics than embedded-le. however, we note that standard locks with sleep mode enabled may still be the best choice if minimizing energy consumption is more critical than improving performance."
"so far, we showed that for many benchmarks it is better to sleep instead of spin. however, to better understand the design space, in the following sections we will continue our parameter exploration testing both sleeping and spinning versions of each configuration."
conflict detection. when and how should a conflict be detected? conflict resolution. when and how should a conflict be resolved? data versioning. where and how should original values as well as speculative changes to shared data be stored?
"we find that embedded-spec can improve edp for most of the benchmarks under many of the configurations we consider. the benefits of speculation are sensitive to the critical section size, the degree of lock contention, the retry policy, and the underlying hardware transactional memory's contention management policy. we conclude that for platforms where energy efficiency matters, embedded-spec can provide real benefits, but that the underlying hardware architecture must be configured with care. the rest of this article is organized as follows. we discuss related work in section 2. next, section 3 provides a background discussion on the main speculative memory design components and then describes the embedded-spec architecture, along with its two principal configurations: embedded-le (for lock elision), which provides flexible policies for switching between speculative and nonspeculative executions, and embedded-lr (for lock removal), which relies on the progress guarantees provided by the underlying hardware transactional memory to resolve conflicts. section 4 evaluates these configurations, and their variants, by measuring the edp of a range of benchmarks. section 4.4 summarizes our conclusions and offers directions for future work."
"the conclusion we draw from this set of experiments is that if we care only about performance, we should use embedded-le-sleep modality instead of embedded-le for patricia, while we should avoid it for all other benchmarks. if we care only about energy consumption, then embedded-le-sleep modality is overall a better choice. similarly, if we care for both performance and energy consumption, then, overall, embedded-le-sleep is the better way to go."
"the transition of embedded systems toward multicore architectures promises an improvement in power-performance scalability.however, this promise can be realized only if applications are capable of a high enough level of concurrency at low energy cost. most embedded multiprocessor-system-on-chip (mpsoc) designs rely on shared memory for intercore communication, since it is easier to use than alternatives such as message passing."
"regardless of the implementation that is followed (hardware, software, or hybrid), when designing a speculative memory scheme, we have to decide on the following important aspects:"
"although the exact details of the lock elision implementation in haswell are not released, a best-effort speculation on its implementation have been discussed [cit] . hle uses two new instruction hint prefixes (xacquire and xrelease) to denote the region in the code where lock elision can be applied. when a lock acquisition is encountered in the code, the xacquire prefix is inserted to indicate the start of the lock elision region and the lock instruction is added to the read set of a transaction, but the lock is not acquired (i.e., the thread does not write new data in the lock address). this means that other threads can also enter the lock elision region simultaneously and transactionally access shared data. writing to the lock address during execution of the hle region will cause an abort. reads and writes to shared memory that happen within the lock elision region are added to the read and write sets of the corresponding transaction. when the xrelease prefix is encountered, it means that the end of the lock elision region has been reached, and the transaction attempts to commit. in a conflict event, the core restores the internal registers state that was saved prior to xacquire and ignores any writes to shared memory that happened within the hle region. the thread will retry the hle region again, but this time by normally acquiring the lock. this means that once aborted, no speculation retries are allowed right after. moreover, there is a limit on the number of simultaneous elisions. if this limit is surpassed, other regions will be executed through standard locking."
"another important aspect of conflict resolution is the abort policy in deciding which transactions should be aborted upon a conflict. the requestor-abort policy aborts the transaction that requested the data access that caused the conflict. the rationale behind this is that, since all transactions have made some progress before the requestor caused the conflict, the requestor should be the one to abort so that the other transactions can continue to make progress. another option is to let the requestor proceed and abort all other conflicting cores (i.e., the requestor-wins policy). this choice is more natural to the way cache coherency works. we could also abort all the transactions that conflicted and let them retry speculation again. regardless of the chosen abort policy, it is critical that, after a conflict we do not allow all transactions to retry at the same time since this would inevitably result in consecutive aborts. instead, they should delay retry (or \"backoff \") for different randomly chosen times. many works have used exponential-backoff strategies that increase the backoff time exponentially based on the number of consecutive aborts experienced by each transaction."
"for our experiments, we start with a design space exploration using the preceding set of benchmarks. from this design space exploration, we determine the best combination of abort and retry policies for the two embedded-spec algorithms. next, we compare our best configurations against standard locks."
"locks are typically used to guarantee memory consistency in shared memory programs. locks, however, can limit concurrency and therefore slow performance. they can also be costly in terms of energy. locks must be deployed conservatively whenever conflicting memory accesses are possible, even when they are unlikely. by contrast, speculative approaches, which detect conflicts dynamically, promise both to improve performance and to save energy. transactional memory, speculative lock elision (sle), and transactional lock removal (tlr) are hardware speculation techniques that allow critical sections to execute in parallel, without explicitly acquiring locks [cit], 2002] . if a data conflict does take place, it is detected, and one or more of the conflicting threads is rolled back and restarted, either speculatively or by acquiring a lock."
"we conclude that it is never disadvantageous to choose the requestor-abort policy over the abort-all policy. in fact, for some benchmarks like genome, patricia, and vacation, the requestor-abort policy is beneficial both in terms of performance and energy consumption, especially when we set a higher number of maximum allowed retries and we choose sleep modality."
"-the stamp benchmark suite [cit] . the selected workloads represent the following synchronization patterns and critical section sizes: (1) large nonconflicting critical sections (vacation); (2) barrier-based synchronization with small critical sections (kmeans); (3) large critical sections that may conflict (genome); and (4) a mix of large and small critical sections (labyrinth). -the mibench suite [cit] ] patricia: a patricia trie is a data structure used in place of full trees with very sparse leaf nodes. patricia is characterized by a high percentage of time spent in critical sections, and a high abort rate. -datastructures. redblack, skiplist: applications operating on special data structures."
"nonspeculative synchronization is supported by a fixed set of architectural hardware locks drawn from a preallocated section of memory, the semaphore memory, and accessible by standard synchronization calls such as test(), testandset(), and release()."
"to determine the overall best choice, we have to look at the edp, as shown in figure 14 . benchmarks such as redblack, skiplist, kmeans, and labyrinth show better results when choosing any nonzero number of retries, while vacation shows considerable improvement (23%) for an infinite number of allowed retries compared to just one. genome shows better edp when we restrict the number of retries to two. on the other hand, patricia seems to benefit greatly both in performance and energy when we do not allow any retries at all. this is expected, since benchmarks with high abort rates, such as patricia, benefit from switching to locks following a single misspeculation, while benchmarks with lower conflict levels benefit from retrying the speculation several times."
"there are quite number of nonlinear models can be used, such as autoregressive exogenous model (narxm), hammerstein model (hm), weiner model (wm) and hammerstein-wiener model (hwm). among them, the hammerstein-weiner model has been proved to be good descriptions as nonlinear dynamic systems [cit] . estimating hammerstein-wiener models requires uniformly sampled time-domain data. the data are consists of single-input and single-output (siso) channels."
"we note that the embedded-lr algorithm does not require much support in the middleware level, since the bloom module is already present in the hardware level. the only required feature at the middleware level is starting a new transaction instead of acquiring the lock. even in the event of a misspeculation, the lock will not be acquired. the idea behind this is to allow at least one core to complete the critical section. in case of a conflict, the core or cores that have been selected to stop (i.e., the \"losing\" cores), will be aborted and put in a hold state by the bloom module. when the \"winning\" core completes execution of its critical section, the core or cores kept in a hold state, will be released and will be allowed to retry the critical section speculatively. to track suspended cores, the bloom module is extended with a per-core hold_queue_list. when a core is aborted, its coreid is added to the winning core's hold_queue_list register. we also append to the winning core's hold_queue_list register the hold_queue_list lists of the aborted cores. when the winning core commits, every coreid in its hold_queue_list register is released and the list is cleared."
"next, we repeat the same set of experiments, but this time with sleep mode enabled. figures 18, 19 and 20 show the corresponding results. we generally observe similar trends as in the nonsleep modality, with the following differences. when we limit fig. 17 . edp for embedded-le using different abort policies and maximum number of allowed retries set to infinity. fig. 18 . edp for embedded-le-sleep using different abort policies and maximum number of allowed retries set to one. fig. 19 . edp for embedded-le-sleep using different abort policies and maximum number of allowed retries set to two. the maximum number of retries to one, as shown in figure 18, we see that the requestor-abort policy is slightly worse for vacation as we increase the number of cores, but still it is slightly better for genome and patricia. the differences observed in this case though are too small to draw a conclusion on which technique is better. as we move to higher numbers of allowed speculation retries, as shown in figures 19 and 20, the benefits of the requestor-abort policy become more visible in specific benchmarks. in particular, the edp reduction in the embedded-le-sleep experiment, when setting the number of allowed retries to infinity and using requestor-abort policy, is 43%, 80%, and 76% for genome, patricia, and vacation, respectively, while it was 47%, 75%, and 63% for the embedded-le experiment set."
"we conclude that if we want to increase performance and at the same time decrease energy consumption, then for most benchmarks (except for patricia and genome), we should allow retrying speculation for an unlimited number of times until it is successful, instead of switching back to locks. slight variations to the best nonzero value choice are observed, that lead us to pick noninfinite values, especially for genome (four in embedded-le mode and two in embedded-le-sleep mode) and patricia (four in embedded-le mode). the only exception to these observations is patricia in embedded-le-sleep mode. in this case, we see a significant improvement when we do not allow any retries and we immediately switch back to locks after an unsuccessful speculation attempt. again, this is due to the relatively high contention rate for this benchmark."
"to test our design, we chose several benchmarks that were adapted to our simulation platform, which does not include operating system support. the benchmarks belong to the following suites:"
"embedded-le supports two contention management policies. the requester-abort policy aborts only the core requesting the conflicting address, and the abort-all policy aborts all cores executing the same critical section. the second policy is motivated by the observation that once a core abandons speculation and tries to acquire the lock, it is highly likely it will force the other cores in the same critical section to abort eventually. we also examined a variation of embedded-le, in which cores suspend execution in a low-power idle mode instead of spinning when waiting for a lock. this approach (called embedded-le-sleep) saves power but increases latency (by 2ms). finally, we examined the effects of allowing aborted cores to attempt to elide the lock more than once before resorting to lock mode by setting a parameter max number of retries."
"having determined the optimal set of parameters for each benchmark, we are now ready to compare our implementation of embedded-spec with the standard lock approaches (lock and lock-sleep). using the best parameter configurations for each execution mode and each benchmark presented so far, we perform a set of experiments in which we compare the performance, energy consumption, and edp of each applied technique (locking, lock-sleep, embedded-le, embedded-le-sleep, and embedded-lr). figure 22 shows the execution time of each technique, normalized to the execution time of standard locks. as can be seen, for the one-core configuration locks provide better performance than any kind of speculation. this is expected, and is due to the additional hardware and software support necessary to enable the speculation. as we increase the number of cores though, the speculative approaches begin to show an advantage for all but the kmeans benchmark. as mentioned earlier, in kmeans the critical sections are rare and small (i.e., less than 5% of time is spent in critical sections), and the results show that embedded-spec does not provide benefits. at the same time, embedded-spec does not hurt performance when the benchmark does not include large speculative sections."
"if our primary goal is to improve performance, allowing an infinite number of retries is best for all benchmarks, except for genome and patricia, which show better performance for four maximum retries in embedded-le mode and two in embedded-le-sleep mode. if our primary goal is to decrease energy consumption, then not restricting the number of retries is again best for most benchmarks, except for patricia and genome, which yield better results if we restrict the number of retries to four. finally, if we want to decrease energy consumption, but we are in embedded-le-sleep mode, then we should not allow any retries for any of the benchmarks."
"max number of retries. note that in all our experiments for embedded-le so far, once a thread failed to elide a lock, it would then try to acquire it. we next extend embedded-le to allow a thread that had a conflict during lock elision to retry eliding the lock rather than immediately trying to acquire it. therefore, the next parameter we investigate for embedded-le is the max number of retries, which allows us to evaluate how many times it is worthwhile to retry a failed speculation on a high-conflict critical section before we switch back to lock mode. figure 9 shows the performance with a varying number of retries allowed before reverting to locks, in embedded-le mode. note that by setting this value to 0, embedded-le behaves as in prior experiments, acquiring the lock after a single abort."
"in our study, we were careful to preserve the locks and critical sections of the original benchmarks when we ported it to our embedded platform. for future work, we plan to investigate the degree to which simple refactoring of known hot spots in the code can enhance the benefits of lock elision. in addition, we plan to evaluate the behavior of the two speculative schemes over a wider set of benchmarks, which include more lockid values and broader synchronization patterns."
"in figure 8 we show the edp for the same set of experiments in order to measure the combined effect of sleep modality on both performance and energy consumption. even though execution time is increased for some benchmarks when sleep mode is used, the effect is largely compensated by the reduction in energy consumption, resulting in a significant edp improvement in most cases, reaching 14% for genome, 50% for patricia, and 20% for vacation. the overall effect of sleep mode in edp is insignificant for skiplist and nonexistent for kmeans and labyrinth. only redblack shows a clear decrease in edp from sleep modality for the same reasons mentioned before."
"the results for embedded-le-sleep appear in figures 12 and 13 . in this case, the results are similar, with a few notable differences. as in the nonsleep case, for most benchmarks, any nonzero number of retries yields similar results in terms of performance. especially for genome and patricia, retrying two times at most is better for performance than not restricting the number of retries. as in the embedded-le case, retrying speculation instead of switching back to locks immediately after an abort is always beneficial for performance. when looking at energy though, things change significantly, as now we are able to save considerable amounts of energy while waiting on the lock-in sleep mode instead of directly retrying speculative execution. in contrast, the more we allow retrying speculation, the more we risk wasting energy, as figure 13 shows. for all benchmarks (except kmeans, which does not spend enough time executing critical sections to matter), restricting the max number of retries to zero, yields considerable energy savings."
"abort policy. we continue our parameter exploration by experimenting with the abort policy, which is set within the bloom module abort manager. we compare the requestorabort policy, which aborts only the requesting core when a conflict occurs, to the all cores on the same lock id policy (or abort-all policy), which aborts all cores conflicting on the same lock-protected critical section."
"to support embedded-spec, we extended the bloom module's control logic as well as its individual bloom filters to make it aware of the architecture-supported hardware locks. figure 4 shows an overview of the bloom module and the internal details of a core bloom filter unit (bfu). the bloom module has the following functional blocks: -snooping shared memory address: snoops the shared memory address space to keep track of the addresses accessed during speculative execution. -bloom filters: per-core signatures corresponding to the read and write addresses accessed during the speculative execution. -control logic: implements the features needed to manage communication with the cores (i.e., the abort and hold signals). it also manages the abort policies and handles cache overflow. -snooping semaphore memory address: snoops traffic to and from the hardware locks. it detects test(), testandset(), and release() calls and their responses. -sle registers: per-core registers to keep track of the core status (i.e., which core is in speculative mode on which hardware lock and which core has ownership of a specific lock). these registers are kept updated by the snooping semaphore memory address block. -hold queue list registers: per-core registers to keep the list of aborted cores that need to be released at commit time. these registers are used only with embedded-lr."
"to explain the differences observed in the results of the aforementioned benchmarks, we need to bear in mind what is special about each one of them. the genome, patricia, and vacation benchmarks have large critical sections and they spend a significant portion of time executing critical sections. while patricia experiences high abort rates, vacation generally has nonconflicting transactions. the redblack and skiplist benchmarks are very similar in the sense that they both work on special data structures and have very low abort rates. the labyrinth benchmark includes a mix of large and small critical sections. finally, kmeans is a benchmark that spends a very small portion of its execution time in critical sections whose sizes are very small. that is why kmeans often does not show significant changes in behavior when fine-tuning some parameters."
"we also observe that embedded-lr yields the best performance for an increased number of cores, except for kmeans. embedded-lr yields performance improvement of at least 47% for patricia and up to 80% for genome, compared to standard locks. the next best configuration to embedded-lr is embedded-le and embedded-le-sleep, both yielding performance improvement of 10%, 31%, 45%, 50%, and 70% for vacation, redblack, skiplist, labyrinth, and genome, respectively. the only exceptions are kmeans, for the reasons mentioned, and patricia, which shows better performance for the locksleep and locking techniques than for embedded-le and embedded-le-sleep. this is expected for patricia, since it suffers from a relatively high abort rate, hence using locking instead of speculation is preferable. regarding performance, embedded-le and embedded-le-sleep show a very small difference, with embedded-le being slightly better, apart from patricia and redblack where the difference is more pronounced (23% and 6%, respectively). regarding locking compared to lock-sleep, the difference in performance is again insignificant, apart from patricia, where lock-sleep is clearly better (11%). figure 23 shows the energy consumption for the same set of experiments. here, locksleep is clearly preferable, showing energy benefits starting from 15% for kmeans and reaching up to 73% for genome and labyrinth. when we focus only on energy, locking with sleep mode enabled is clearly better than speculation since it does not encounter aborts. on the other hand, locking without sleep mode enabled becomes the worst choice for energy consumption as we can see in figure 23 . so, with best choice being the locksleep technique, the second best choice in terms of energy consumption is embedded-lr (except for patricia where embedded-le-sleep is 31% better than embedded-lr). embedded-le-sleep comes very close to embedded-lr in terms of energy consumption, with embedded-le following next for most cases. a common observation is that all sleep techniques yield better energy results, which is generally expected."
"nonlinear hammerstein-weiner (nlhw) model is able to provide an accurate description and prediction for a real system of pv system. under normal and shading conditions it may have a nonlinear behavior operation. there were three nonlinear estimators for inputs and outputs have been studied, i.e. piecewise-linear, sigmoid and wavelet-network. from the trial and error methods, nlhw model will contribute the highest percentage fitting with low fpe and loss function. meanwhile the scalar nonlinearity estimators for input and output nonlinearities are identify as piecewise linear and wavelet network severally. the step response results from the system identification tool for both conditions shows a good oscillatory reaction to the final steady value. the percentage of best fit can give better percentage if the multiple input data; in example temperatures are also considered. the algorithm method and linear order value selected from the project can be furthered investigated to be as a model based design to control a power electronics converter for a pv system. the system under normal and shading operation of photovoltaic system are estimated and analyzed using system identification approach."
"the embedded-spec architecture proposed here goes beyond sle and tlr and the haswell hle design in several ways. first, in haswell and the original sle proposal of rajwar and goodman [cit], a failed speculation immediately restarts nonspeculatively. there are no alternative failover mechanisms or policies. in contrast, our proposal offers flexible contention management (conflict resolution) alternatives, including alternatives to tlr's time stamps. moreover, the sle and tlr proposals, like most work in this area, were concerned with improving throughput, not energy efficiency. since we are concerned with embedded platforms, we take edp to be the principal figure of merit. pohlack and diestelhorst [cit] evaluated the results of applying lock elision to the memcached caching system. their lock elision implementation was based on amd's advanced synchronization facility (asf) [cit] ], a speculative synchronization architecture similar to transactional memory. our work differs from pohlack and diestelhorst [cit] in that we are concerned with embedded platforms and edp, instead of general-purpose platforms and throughput."
"the modular design concept popularly adopted in static systems applies also to run-time reconfigurable designs on fpgas. as we discussed in the previous section, the entire system is partitioned and different tasks are individually implemented as functional modules in dynamically reconfigurable designs. analogous to software processes running on top of oses and competing for the cpu time, each functional module can be regarded as a hardware process which is to be loaded into reconfigurable slots (i.e. prrs) on the fpga rather than general-purpose microprocessors (gpcpu). multiple hardware processes share the programmable resources and are scheduled to work according to certain types of disciplines on the awareness of computation requirements. context switching happens when the current hardware process in charge of one task is leaving the reconfigurable slot (being overwritten) and another new task is to be loaded to start working. all these key issues in the adaptive computing framework are classified into and addressed within certain layers in hardware or software. figure 7 demonstrates the layered hardware/software architecture and details in different aspects will be presented in the following subsections respectively. a dynamically reconfigurable platform may contain a general-purpose host computer system and application-specific functional modules. figure 8 shows a system on a xilinx virtex-4 fpga. existing commercial ip cores can be exploited to quickly construct the general computer design, consisting of the processor core, the main ddr memory controller, peripherals, and the interconnection infrastructure using the plb bus. in addition to the fundamental host computer system, run-time reconfigurable slots are reserved for being dynamically equipped with different functional modules. in the figure we show only one prr to explain the principle. when incorporated in the prr, pr modules communicate with the static base design, specifically the plb bus for receiving controls from the processor and i/o buffers to external devices. noting that the output signals of a pr module may unpredictably toggle during active reconfiguration, \"disconnect\" logic (illustrated in the callout frame in figure 8 ) is required to be inserted to disable prm outputs and isolate the unsteady signal state for the base design from being interfered. furthermore, a dedicated \"reset\" signal aims to solely reset the newly loaded module after each partial reconfiguration. both the \"disconnect\" in the previous xilinx partial reconfiguration early access design flow [cit], a special type of component called bus macro (bm) must be instantiated to straddle the pr region and the static design, in order to lock the implementation routing between them. this is the particular treatment on the communication channels between the static and the dynamically reconfigurable regions. the bm components have been removed in the new pr design flow [cit] . they are no longer needed and the partition i/os are automatically managed by the development software tool. one significant advantage of this hardware structure, is that it conforms to the modular design appraoch: different functional tasks are respectively implemented into ip cores. they are wrapped by the plb interface and integrated in the bus-based system design. normal static designs can be easily converted into a pr system by attentively treating the connection interface and mapping various functional modules in the same time-shared pr region. little special consideration is needed to construct a pr system on the basis of conventional static designs."
"cognizing driving preference. different drivers have different preferences about different types of roads, and they also have different impulse to reroute roads due to their different tolerance about the cost expectations of current congestion. for example, the drivers with low tolerance may choose a highway bypass which have a lower congestion cost expectations but have more traffic lights. tolerance of drivers changes dynamically with various spatial-temporal conditions such as travel distance, congestion time, and arrival time. therefore, a large deviation between the traffic optimization results and the actual expectation of drivers will lead to failure of traffic scheduling. quite a few drivers choose a looked like shortest road, only to find the route is congested by many vehicles whose drivers make a similar decision."
"the hypergeometric is more comprehensive and precise than the binomial and the poisson. even though the means are identical for all three distributions (np), the binomial variance varb(x) and the hypergeometric variance varh(x) have distinct values, but varb(x) is always greater than varh("
"the data of traverse time in different directions, the average vehicle speed and traffic volume of road segments can be generated by algorithm 3. when map-matching is done, more fine-grained data can also be obtained such as the average speed and traffic volume under different directions of road segments. the data after map-matching and statistics"
"3) due to the flexible and controllable characteristics of vsc, the ac/dc distribution network is generally more reliable than the traditional ac distribution network. the ac/dc distribution network can well adapt to the changes of the load distribution, better exploit the integration capability of renewable energy, and consequently improve the reliability. thus, this paper clarifies the improvement of reliability brought by ac/dc distribution network, and indicates the occasions that are more suitable for establishing ac/dc distribution network."
"a robot system which has an ability to reorganize its own mechanism would be singularly able to fulfill various tasks, by switching its locomotion mechanism according to the immediate terrain and by changing its end-effectors for manipulating objects. modular robots, which are composed of multiple autonomous modules, are able to self-reconfigure by replacing the geometric arrangement of the modules forming the robot system. this paper focuses on \"load adaptation\" by self-reconfiguration. the novel function would enable modular robots to autonomously change their configuration to the most appropriate one for enduring even unpredictable load condition, as if the robots were body tissue of natural life forms. a variety of mechanisms of modular robots have been developed [cit] . chain-type robots such as polybot [cit] and conro [cit] place an emphasis on mobility of the multidegree-of-freedom systems, and lattice-type robots such as crystalline [cit] and atron [cit] mainly focus on geometrical transformation of the modular structures. however, there are very few designs for supporting large external forces, and an algorithm for load-adaptive transformation has never discussed. it is because the almost developed modular robots are designed for the mobile or transformation function than the supporting function."
"where is the predicted value and is the observed value. the smaller the mae, the stronger the predictable ability of algorithms. as shown in table 2, whether it is the mae of vehicle speed, traffic count, or travel time in different direction (from west to east), (from east to west), (from south to north), and (from north to south), maplstm is smaller than gpr and convlstm. for a certain algorithm, the closer the value of \"train\" and \"test\" of each parameter is, the more robust it is. the results of convlstm are similar to maplstm but do not exceed maplstm. that is because convlstm with the ability to capture spatiotemporal correlations is good at predicting relatively single spatial pattern, but the spatial patterns of road traffic are complex. in the future, we will focus on complex spatial correlations in traffic environment. compared to convlstm, some parameters of convlstm+ are slightly better because convlstm+ increased the number of epoch."
"in this paper, we propose a fine-grained and lightweight approach for traffic predicting of road segments, named maplstm, a spatio-temporal long short-term memory network (lstm [cit] ) preluded by map-matching [cit] . maplstm only requires vehicles gps, and not need to deploy specialized traffic sensors in urban and not use the unobtainable data from ground loop. maplstm first obtains the historical and real-time traffic conditions of road segments via map-matching. then lstm is utilized to predict the traffic conditions of the corresponding road segments in the future. breaking the single-index forecasting, maplstm can predict multiple traffic conditions for road segments simultaneously. to summarize, the major contributions of this paper consist of the following aspects:"
"algorithm. before map-matching, it is necessary to have a information understanding about roads and vehicles. table 1 describes an example with a sample of the information. all the information about roads and vehicles can be correlated based the auxiliary information (id, longitude and latitude)."
"to generate synchronous slide movements of the chobie ii modules within a framework of the decentralized system, we have proposed \"temporary leader scheme\" where the modules elect their leader in each transformation turn. in this scheme, each module calculates motivation to become a leader based on the local shape around the module and pregiven 32 control parameters corresponding to undesirability of local shapes. the procedure is as follows. figure 6 ."
"traffic network possesses complicated spatio-temporal relationship. the prediction methods should have accuracy, robustness, adaptability and portability as the traffic flow is a high-dynamic, high-dimensional, non-linear and non-stationary random process. traffic conditions of road segments are influenced inevitably by the spatio-temporal information in the traffic network. deep learning can be used to model high-level abstractions by using multiple non-linear transformations, while the learning network has rarely taken the overall spatio-temporal dynamic pattern into account. it is not convincing to achieve accurate traffic prediction merely by spatial relations between regions or road segments. hence, the prediction results perform not well at certain times, which occur especially when there are insufficient gps trajectories through road segments. based on this, it is proper to consider more supplementary aspects such as mapmatching technology used to recognize traffic conditions for road segments accurately and finely."
"this section has been a first attempt at generalizing risk factors to both players. we have allowed consumers to recognize producer risk and producers may now acknowledge consumer risk. however, we have kept the two decision makers each as a self-determining unit. in later sections, we attempt to generalize acceptance sampling to the case of both risks applying to both producer and consumer simultaneously."
"in this paper, we introduced two new methods in order to enable chobie ii to construct a bridge-like structure with adapting to a load condition. the first method is for making and enlarging desirable local shapes. we confirmed that chobie ii can transform to characteristic configurations as a cantilever structure due to this method. the second method enables the modules to change their criterion for generating transformation so that the modules estimate more undesirably about shapes which cause overstressed states. adopting these methods, we demonstrated a motion of constructing a cantilever with reinforcing stress-concentrated portions. in addition, we discussed a transformation process in a load condition including an external force. from the result, it can be expected that the proposed method is also applicable to various load conditions within a framework of a decentralized control method. the future works are to input this algorithm into actual modules and verify the availability of the proposed load adaptation algorithm."
"two quadrants are labeled as correct, and the other two as errors. in general, we would like to maximize the probability of falling into the correct boxes and minimize the probability of error. following np, accepting as true the false null is a type ii error, whereas rejecting a true null is type i error. the exact definition of the null hypothesis is crucial, and as stated above should be defined as the condition that incurs the highest cost if chosen in error. the choice of which of the two hypotheses is to be the null depends, therefore, on the decision maker, and how he perceives the distinctive costs of the two errors."
"the most important feature of the motion-generating method is that a module which becomes a leader is not always a module which has the most undesirable local shape, but a module which has the largest value assessed about all shapes around the module. therefore, complicated transformation process can be generated by adjusting magnitude relations of the parameters in an appropriate manner. however, the above criteria only give a strategy to dissolve undesirable local shapes. we need a new strategy to make chobie ii construct a characteristic structure like a bridge. it is possible to solve the problem by extending the criteria expressed by 32 parameters."
"the paper has shown that sampling plans should be true to the data and the situation they represent. when lot sizes are finite, statistical approximations may lead to serious estimation errors. since the process of sampling is inherently error prone, the sampling process should employ only the most accurate data available, including lot size. the priority for researchers in any area should be the utilization of the most appropriate formulations, like the hypergeometric distribution in sampling plans, so that unnecessary additions to inherent errors do not occur."
"modern fpgas (e.g. xilinx virtex-4, 5, and 6, altera stratix 5 fpgas) offer the partial reconfiguration capability to dynamically change part of the design without disturbing the remaining system. this feature enables alternate utilization of on-fpga programmable resources, therefore resulting in large benefits such as more efficient resource utilization and less static power dissipation [cit] . figure 4 illustrates a reconfigurable design example on xilinx fpgas: in the design process, one partially reconfigurable region (prr) a is reserved in the overall design layout mapped on the fpga. on the early-stage dynamically reconfigurable fpgas (e.g. xilinx virtex-ii and virtex-ii pro), prr reservation must run through a complete slice column, because a slice column is the smallest load unit of a configuration bitstream frame [cit] latest fpga generations (e.g. xilinx virtex-4, 5, and 6), prrs can be the combination of slice squares. various functional partially reconfigurable modules (prm) are individually implemented within the pr region in the implementation process, and their respective partial bitstreams are generated and collectively initialized in a design database residing in a memory device in the system. during the system run-time, various bitstreams can be dynamically loaded into the fpga configuration memory by its controller named internal configuration access port (icap). with a new module bitstream overwriting the original one in the fpga configuration memory, the prr is loaded with the new module and the circuit functions according to its concrete design. in the dynamic reconfiguration process, the prr has to stop working for a short time (reconfiguration overhead) until the new module is completely loaded. the static portion of the system will not be interfered at all."
"after these steps, the recently equipped controller module becomes ready for memory accesses on the nor flash or the sram. in this design, ipc operations can be realized through the third-party shared memory such as ddr. for example when the system is just powered on, the sram lut initialization data are retrieved from the nonvolatile flash and buffered in the system ddr memory. after the flash controller is unloaded and the sram controller is activated in the prr by dynamic reconfiguration, the lut data are then migrated into the sram chip for application-specific computation. the ipc data flow is illustrated in figure 15 ."
"our algorithm is inspired by remodeling function of bone tissue, where resorption of old tissue and formation of new tissue occur as metabolism of the bone, and both processes are always kept at equilibrium state [cit] . when the environment around the bone changes and the equilibrium is disturbed, the shape or property of the bone also changes so as to transit at new equilibrium state. for example, when external force acts on a bone, formation of new tissue is excited and, as a result, the skeleton becomes thick and rigid, whose shape is more adapted for enduring the load condition than before loading. therefore, the remodeling function enables adaptive variations of living tissue according to its varying environment. we try to apply this mechanism to the reconfiguration algorithm of modular robots. in our new algorithm, each module detects the stress condition and varies its criterion for generating transformation of the structure according to the magnitude of stress. this method enables the modules to keep to an appropriate configuration against their immediate load condition, while each of the modules decides its own action in a distributed autonomous control manner."
"these circumstances lead the necessity in image processing to have a wide basis of orthogonal matrices including not only previously unknown matrices of large sizes, but also matrices of various orders including odd ones [cit] . thus, the requirements for the matrix basis can be briefly formulated as follows:"
"if there is line overload or voltage violation in ac/dc hybrid distribution networks after the fault occurs, the loads must be shed to maintain the distribution network reliable. unlike the traditional ac distribution network, in the calculation of the optimal load shedding for the ac/dc hybrid distribution network, it is necessary to consider the effect of the ac/dc converters and other control equipment. in this regard, this paper proposes an optimal load shedding model that considers the optimization of the vsc parameters. the minimum load shedding model can be expressed as follows:"
"in the design, a nor flash memory stores nonvolatile data necessary for system startup in the field, including both the bitstream file and the os kernel. suppose we are constructing a system aiming at memory bandwidth hungry computation for certain applications. hence a zero-bus turnaround (zbt) sram is integrated in the system in addition to the main ddr memory. the sram is utilized as a look-up conventional static design approach, both the flash and the sram controller are concurrently placed on the fpga in order to address the two types of memories."
"after sampling according to the above model and obtaining the state of the system, we must perform a series of analysis of the system to obtain the reliability index. first, according to the fault state, the control mode of the vsc must be adjusted. then, after performing the ac/ dc power flow calculation, we conduct the optimal load shedding calculation to obtain the reliability index when the operational constraints of the system is violated."
"the results are shown in table 6 . compared with scenario i, the changes of the reliability indices of the two systems are shown in fig. 10 . for the ac/dc network, the reliability indices are nearly identical in the two scenarios. however, for the ac network, the reliability indices in scenario ii are higher than those in scenario i."
"here, two values of gain are used depending on the situation; a gain more than 1 is multiplied if a shape is undesirable (the corresponding parameter is more than 0), and a gain less than 1 is multiplied if a shape is desirable (the corresponding parameter is less than 0). we call the former gain g +, and the latter g − . this rule makes the modules estimate more undesirably about shapes which cause overstressed states, and transformations to dissolve such shapes become to be generated with higher priority. that is, we can set that a shape is not so undesirable in normal situations but is very undesirable in critical situations. therefore, it is possible to shift the priorities of the transformation modes according to existence of overstressed states. the existence of the overstressed states can be detected by decentralized signal transfer from the overstressed modules."
"with the demand for clean energy in modern society, renewable energy is being widely integrated into the distribution network in the form of distributed power, which introduces new challenges for the reliable planning and operation of the distribution network [cit] . the uncertainty of the output of renewable energy affects the reliability of the distribution network. with the development of dc transmission technology and power electronics technology, the flexible dc power distribution technology based on voltage source converters (vscs) has begun to be applied in distribution networks. the traditional ac distribution network is gradually transformed into the ac/dc methods designed for ac distribution network cannot be directly utilized to the ac/dc hybrid distribution network. therefore, it is important to evaluate the reliability of the ac/dc hybrid distribution network."
"in normal operation, the upper-level power supply acts as a balanced node of the ac distribution network, while at least one vsc should be set in the u dc q control mode to provide voltage support in the dc distribution network. when a fault occurs, if the ac distribution network loses the balance node due to the fault, the control mode of the connected vsc should be set to u ac θ . if the dc distribution network loses the balance node due to the fault, the control mode of the connected vsc should be set to u dc q. all control modes of the remaining vscs should be set to pq. fig. 2 is an ac/dc distribution network, which shows three typical faults that may occur in the network. the changes of the control mode of vscs in different scenarios are shown in table 1 ."
"this paper discusses a method to realize load adaptive motion by adding new criteria. preparatory to the discussion, the mechanism and algorithms using previous criterion are described in sections 3 and 4. then, in section 5, we propose two additional criteria, and enable chobie ii to construct a bridge-like structure well balanced in stress distribution as shown in figure 2 . one is a strategy to induce desirable local shape, which enables chobie ii to transform to characteristic configurations like a bridge. the other is a rule that the modules convert the control parameters for motion generation depending on local load conditions. we demonstrate that, although each module only processes distributed information, the whole modules can transform to a bridge-like structure with reinforcing stressconcentrated portions. since the algorithm proposed here is based on a very simple methodology that overloaded states are undesirable for the structure, so it would be also available for modular robots other than chobie ii."
"where p ij /q ij is the active/reactive power flow from node i to node j; p i /q i is the sum of the active/reactive power injected into node i; r ij /x ij is the resist/reactance of branch ij; ac is the set of ac nodes; q shed,j is the shed reactive load of node j."
"2) the state of the components in the system is sampled using the nonsequential monte carlo method. simultaneously, the renewable energy output and load level are sampled."
"due to the higher fault rate of the vsc, the ac/dc network has a slightly higher saifi than the ac network. in contrast, the ac/dc network has a lower saidi than the ac network. the main reason is that it takes more time to operate the tie switches in the ac network, while vscs have faster adjustment speeds in the ac/dc network. with the improvement of the dc power electronics technology, the reliability of the ac/dc network will be even higher than the existing results."
"(, ) (6) end for; historical trajectories, gps sampling points in the trajectories are traversed to get the candidate point set which waiting to be corrected. for all candidate points, the spatial value can be reached by combining with the information of road network, longitude, latitude, and distance, and the temporal value can be reached by adding the time information. after spatial analysis and temporal analysis, matching results can be accomplished. after map-matching, roads information where the vehicles are located can be easily obtained, and the traffic data about the roads can also be clearly gained after statistics in turn."
"we have assumed in the discussion above that the null for the producer is that the lot is conforming, or analogously the production line is stable producing a good product. the engineer who tries to correct problems that do not exist (he rejects the null when it is true) is wasting precious time in worthless activities. this is a type i error, the basis of the p value 7 to the statistician and equivalent to producer risk for the industrial engineer. increasing the value of the cutoff c will decrease the probability of type i error by making rejection of the lot more difficult. however, increasing the value of c makes acceptance easier and therefore will increase the probability of type ii error, b p . considering that type ii error is relatively less important for the producer, the tradeoff tends to be attractive for the producer. from the consumer's side of the story and contrary to the producer, the null should be that the lot is nonconforming, in other words, that the consumer should naturally distrust the quality of the lot or process."
"the poisson variance varp(x) is simply np, equal to its mean and, consequently, when p is very small approaching zero, then varb(x) & varp(x).the advantage of the hypergeometric is that its variance is smaller, and furthermore, much smaller when n and n are proximate values. the vertical axis in the next figure is the ratio varh(x)/varb(x) which varies from 0.0 to 1.0. the x-and yaxes at the base are sample size n and population size n. the figure shows that when n is large and n small, there is practically no difference between the two variances. however, when n and n have similar values, the result is a value of varh(x)/varb(x) that approximates zero, the hypergeometric variance being much smaller than the binomial as seen in fig. 8 . even when population size n is 40,000, and when sample size n is relatively large, the difference between the two variances can be relevant. considering the fact that the hypergeometric distribution is more general and more accurate than both the binomial and the poisson, the question arises why do many applied areas of research continue to ignore the hypergeometric distribution?"
"producer error comes from the idea that the producer suffers more from the rejection of good lots than the acceptance of bad ones. to calculate producer risk, the producer table 1 traditional lot classification in acceptance sampling with sample results compared to the real state"
"extends the area of acceptance sampling in two directions. first, by suggesting the use of the hypergeometric distribution to calculate the parameters of sampling plans avoiding the unnecessary use of approximations such as the binomial or poisson distributions. we show that, under usual conditions, discrepancies can be large. the conclusion is that the hypergeometric distribution, ubiquitously available in commonly used software, is more appropriate than other distributions for acceptance sampling. second, and more importantly, we elaborate the theory of acceptance sampling in terms of hypothesis testing rigorously following the original concepts of np. by offering a common theoretical structure, hypothesis testing from np can produce a better understanding of applications even beyond the usual areas of industry and commerce such as public health and political polling. with the new procedures, both sample size and sample error can be reduced. what is unclear in traditional acceptance sampling is the necessity of linking the acceptable quality limit (aql) exclusively to the producer and the lot quality percent defective (ltpd) exclusively to the consumer. in reality, the consumer should also be preoccupied with a value of aql, as should the producer with ltpd. furthermore, we can also question why type i error is always uniquely associated with the producer as producer risk, and likewise, the same question arises with consumer risk which is necessarily associated with type ii error. the resolution of these questions is new to the literature. the article presents r code throughout."
it is important to note that mae is affected by the accuracy of the raw data and it will decline if the dataset is large enough.
"in these parameters, only α h32 has a value, which means, in a condition that \"upper end of the column is right end\" and \"right end of the row is upper end,\" then the shape on right end of the row is undesirable. consequently, a module satisfying the condition becomes a leader. then, in order to dissolve the undesirable shape, the leader commands modules on the upper row slide to rightward as shown in figure 8 . by repeating this process, chobie ii transforms to a configuration without undesirable local shapes. figure 9 shows a simulation result of this motion. in this figure, red color in inner parts of modules becomes bright in proportion to leader indexes of the modules, and a white \"l\" mark means a leader module. the number of leader in each transformation is basically one, however, when two or more modules are in the same condition, the modules may become leaders at the same time."
"in commerce, any negotiation puts buyer and supplier in direct conflict. 1 although the exchange of products and services can take place either with legal contracts or as informal agreements promoting the welfare of all participants, the main characteristic of negotiation is the attempt of one adversary to gain more. even in honest and open negotiations with a relatively free flow of well-defined objectives among all participants, there are still differences between the antagonisms of buyers and sellers. each adversary is an independent decision maker at least in theory, capable of assuming responsibility for her own decisions. in the commerce of large lots of standardized goods, statistical modeling and the concepts of probability can distinguish between different points of view, recognizing and revealing the conflicts inherent in negotiations. consequently, to ensure the quality of large lots, each party may require different contractual sampling plans which specify lot size (n), sample size (n) and the maximum number of defective parts (c) in the sample that still allows for lot acceptance, the formal symbols are pl (n, n, c) ."
"another merit of the algorithm for load adaptation is facility of combination with algorithms for structural construction. as the operation shown in figure 2, the primary purpose of the modular robots is to construct an objective configuration like a bridge, and load adaptive transformation is just a secondary purpose only required if large stress occurs in the structure. so, an important issue is that load adaptive transformation must not disturb the primary structural construction. to reflect the demand, the two algorithms should be combined with consistency. in this paper, we propose an algorithm in which motivation for load adaptation can be included in the transformation rule for structural construction. we show that the algorithm realizes adaptive transformations appropriate for the objective construction."
"the reliability analysis of the distribution system mainly includes two types of methods: analytical methods and simulation methods. due to the access of renewable energy, the analytical methods cannot easily solve the fluctuation problem. therefore, for the ac/dc hybrid distribution network with high integration of renewable energy, this paper uses the nonsequential monte carlo method to analyze its reliability."
"williamson's idea, limiting the use of this array, is that this array is skew-symmetric but with symmetric blocks, which allows to cover the hadamard matrices of orders below 140. as it was found out in the same computer center by dragomir djokovic [cit], the cell size equaled 35 is a limit. after this limit there are areas, where williamson arrays may or may not exist, so the existence of such matrices cannot be guaranteed and is difficult to verify."
where u min /u max are the lower and upper bounds of the node voltage. i max is the upper bound of the branch current.
"single sampling plans are ubiquitous in practically all industries and are the focus of this article. double and multiple stage sampling plans may be less risky in a theoretical sense showing a lower probability of sampling error, however, applications on the shop floor require intensive training and the learning curve is uncomfortably steep. single sampling plans, on the other hand, will reduce costs significantly and even bring a new dynamic to the factory. the elimination of costly 100% total inspection will enhance the adoption of modern management techniques in general, and help to make negotiations between buyers and sellers more transparent and organized. our intention is to show how to improve standard sampling plans like the iso standards (1999, 1985) by recalculating tables with the hypergeometric distribution, and more importantly taking into account the correct usage of hypothesis testing. all major and most emerging economies have similar standards. table 11 for producer sampling plans, the most utilized on a worldwide scale (but not always coherently), previews some important results from this paper. this table combines several tables of popular national and international sampling plans. table 11 step-by-step 1. choose the size of the lot n from the second column. 2. the third column gives the sample size n. 3. choose the worst level of quality permitted by the producer-acceptance quality level aql. the table contains only four levels: 0.04, 0.065, 0.65, 1%, reflecting the very strict industry standards of modern manufacturing. 4. column c presents the cutoff number of failed parts in the sample that still allow for the acceptance of the lot as conforming. consequently, if c ? 1 bad parts are found in the sample, this means the lot is rejected as non-conforming. to economize on resources and make the process even faster, sampling could be sequential and decisions taken even before all n items pass through inspection. c ? 1 bad parts may be found before the total sample n is taken and rejection may already occur. alternatively, nc parts could be sequentially sampled with no bad parts found, meaning that the lot should be accepted immediately. comments on table 11 following, are some comments about the revised numbers in the table."
"the premise of learning driving preferences is to obtain an understanding about the roads conditions. the more we aware of road properties, the more satisfied we cognise the personalized preferences. maplstm can have a fine-grained cognition of road traffic conditions, so we can learn the driving preferences easily. for drivers of vehicles, there are two preferences getting the most attention: time and distance. figure 5 shows the traffic information about the travel time, distance, vehicle density, and speed of each road segments in where the vehicle is driving from place a to place b. in order to compare the preference in driving, the full driving routes based different driving preferences including average speed, vehicle count, distance and travel time are shown in figure 6 ."
"a double-block structure is generalized by a four-block one, which is a combination of two bicyclic matrices with blocks a, b and c, d in the form of williamson array:"
"cone optimization is a kind of mathematical programming using a convex cone in a linear space, which has strict requirements on the mathematical model of the optimization problem. the objective function of the cone optimization must be a linear function of the decision variable. the constraints are composed of linear equations or inequalities and nonlinear second-order cones or rotating cones [cit] . therefore, to use the second-order cone optimization to solve the optimal loadshedding model, the above model must be transformed."
"the traditional route planning methods are more inclined to train drivers' basic selection tendency and do not have personalized features. the participants in these methods are considered the rational contenders perfectly. the planned result is the purely rational optimal solution and does not express the noncomplete rational decision-making preference for drivers in the actual routing decisions. although the questionnaire may be a handy pathway for cognizing driving preferences, it lacks efficiency and comprehensiveness."
"to solve this problem, this paper uses the time series sampling method to establish a multistate probability model of renewable energy and load. the specific method is as follows: 1) according to historical data, form the annual output power curve of renewable energy and annual load curve as shown in (3) (4) (5) ."
"after these steps, the optimal load shedding model is converted from a nonlinear programming model, which is difficult to solve, to a second-order cone programming model, which can be solved by an existing mature algorithm package such as cplex. hence, the requirements for fast convergence and optimal solution can be simultaneously satisfied."
"each decision maker should weigh the importance of two risks when constructing his own sampling plan: a primary risk based on his own null hypothesis and a secondary risk based on his own alternative hypothesis. for the consumer, the relation p aql c defines secondary risk. likewise, p [ ltpd p defines the secondary risk for the producer. consequently, eq. (1) and (2) can be rewritten as the following, featuring either the viewpoint of the producer emphasizing aql p and ltpd p, or the consumer emphasizing ltpd c and aql c, respectively. 9"
"a peripheral controller is the design component which interfaces to the peripheral device and interprets or responds to access instructions from the cpu or other master devices. so a flash memory controller is the design by which cpu addresses external flash chips. figure 2 shows the top-level block diagram of a flash memory controller for the processor local bus (plb) [cit] connection. it receives control commands from the plb to read from and write to external memory devices. the controller design provides basic read/write control signals, as well as the ability to configure the access time for read, write, and recovery time when switching between read and write operations. in addition, the memory data width and the bus data width are parameterizable. they can be automatically matched by performing multiple memory cycles when the memory data width is less than plb. this design structure is capable of realizing both synchronous and asynchronous device access. it may also support other parallel memory accesses with small modification effort, such as sram. [cit] figure 3 demonstrates a typical system-on-an-fpga design for embedded applications. as an example, we adopt the xilinx virtex-4 fx fpga for the implementation. we observe that all components are interconnected by the plb, including the microprocessor, memory controllers, the application-specific algorithm accelerator as well as peripheral devices. in case of system power-on, the fpga firmware bitstream is firstly downloaded to configure the fpga via a special configuration interface [cit] . afterwards an embedded linux os kernel is loaded by a bootloader program into the main memory of ddr for fast execution."
"journal of robotics (2) about each of the 8 shapes, combined states with other 4 shapes, that is, totally 32 kinds of state are considered as shown in figure 7 . we give undesirability of the states by the following 32 parameters:"
"from such a point of view, we have developed modular robots named \"chobie ii\" forming a mechanical structure which has adaptability to a load condition [cit] . chobie ii has the following properties."
"being inherent characteristic, gps-based taxies have proven to be an extremely useful data source for uncovering the underlying traffic behaviour. so far, the taxi gps data have been used for urban computing, detecting hot spots, map reconstruction, finding routes, and so on [cit] ."
"in the copert model [cit], hot emissions are one of the key essentials about traffic emissions. hot emissions occur when the engine of vehicle is at its normal mode. hot emission factor, the amount of pollutant a single vehicle emits per kilometer (g/km), is calculated as a function of travel speed v( /ℎ) [cit] ."
"as seen above, the definition of the null depends on point of view. the producer must decide on a limiting value for aql p above which the lot is unacceptable. in other words, if the fraction defective p is less than aql p (p b aql p ), then the lot is defined by the producer as conforming. on the other hand, for the consumer, (p b ltpd c ) defines the conforming lot. under no circumstances should we assume that the values of aql p and ltpd c are equal, nor should they be, given that they come from distinct decision makers on opposite sides of the negotiation."
"in addition, the predicted object is univocal in the existing methods, more is traffic volume or speed, which can merely infer the traffic state of the road segment is congestion, slow, normal, moderate, and unimpeded. it is necessary to explore fine-grained and accurate perception in a simple way."
"1. determine the values for the size of the sample n and the cutoff limit c. 2. draw the sample and count the number of defective items c in it. 3. a. if x b c then accept the lot as likely high quality, but this may be wrong for b percent of bad lots. b. if x [ c then reject the lot as likely low unacceptable quality, but this may be wrong for a percent of good lots."
"2) compared with the state enumeration method, the result of the proposed method is more accurate. it is more efficient and precise in the case while considering multiple failures which is closer to the actual operation case. in addition, the proposed method can better handle the fluctuation of renewable energy and load."
"when the sample includes a small number of defective items (x b c), this indicates to the buyer that the lot is high quality. equation (2) represents the conditional probability of the sample indicating high quality when, in fact, the lot is unacceptable (p [ ltpd). this condition represents a false negative (fn) ."
"the vsc has two controllable operating parameters. according to different control parameters, the vsc can operate under multiple control modes. the three most typical control modes are: 1) pq control mode. the two control parameters are the active and reactive powers on the ac side of the vsc."
"since the optimal load shedding model involves the optimization control of ac/dc converters and ac/dc power flow calculation, it cannot be directly solved by the solver. the artificial intelligence algorithm and heuristic algorithm mainly perform repeated corrections by continuously performing a power flow calculation, so the number of times of solving and time consumption are large. therefore, this paper adopts the second-order cone optimization algorithm and considers the optimization of the vsc parameter to solve the optimal load-shedding model."
"considering that the pioneering work of dr comes earlier than the formalization of hypothesis testing by np, the question is why acceptance sampling should integrate hypothesis testing at all. we will show that hypothesis testing can offer a common structure that generalizes and clarifies some issues in the application of acceptance sampling."
"(3) if there are some of the 32 states around a module, the module refers to the corresponding parameters. then, each module calculates summation of the parameters l as follows. the value l is called \"leader index\":"
"(b) data processing. the road segments experienced mapmatching also mean the information has been extended, where the road segments and vehicle information are paired off according to their id and location. therefore, we can have information statistics including vehicle speed, traverse time, and traffic volume taking one road segment as a unit (i.e., segment-based). the traverse time can be counted in different directions: from west to east, from east to west, from north to south, and from south to north. the processed data are sent to prediction model lstm as the training set and testing set."
"as for other pollutants like co 2 and pm 2.5, their emission factors are proportional to fc. table 4 compares the applications about traffic prediction in recent two years. it can be seen from table 4 that the traffic prediction methods is more inclined to use machine learning and deep learning algorithm to achieve more accurate and larger regional prediction; the advance cannot be separated from the rapid development of machine learning and deep learning in recent years."
"1) the proposed approach considers regulation ability of the vsc. it fully simulates the control characteristics and failure modes of vsc, establishes power flow and optimization algorithm which are suitable for ac/dc distribution network. therefore, this method can effectively evaluate the reliability of ac/dc hybrid distribution network."
"in fig. 1 the sampling plan is pl(3000, 200, 0), remembering that it is generally beneficial to the buyer to have a very small c. in this example, ltpd is 1%. with p equal to 1% or greater (quality worse), there is a probability of still accepting the lot equal to 0.125 or less. depending upon the necessities and market power of the buyer, consumer risk of less than 12.5% may be required. it is important to emphasize that the sampling plans analyzed in this section follow the cumulative hypergeometric distribution (appendix ''hypergeometric distribution'')."
"first, we explain expression of desirable states. as described in section 4, the 32 parameters for regulating a criterion are corresponding to the 32 kinds of states. when a parameter is 0, the corresponding state is not undesirable. as the parameter becomes larger, the state is considered to be more undesirable. that is, by inputting a parameter lower than 0, we can express that the corresponding state is desirable. in this regard, different patterns of transformations should be generated according to whether the corresponding state is undesirable or desirable. therefore, the procedure has a difference from that of section 4. after calculating leader index in the same way as (1), (2), and (3), then, at (4), a module which has the largest \"absolute value\" of leader index becomes a leader. if the leader index of the leader is lower than 0, the drive command from the leader at (5) changes to the direction for inducing the desirable shape, that is, opposite to the direction in the case of undesirable shape."
"when renewable energy penetration is high, the ac distribution network may not completely eliminate the electricity generated by renewable energy, which violates the upper limit of the node voltage and the system operational constraints, resulting in the abandonment of wind and light. the ac/dc distribution network can flexibly adjust the power imbalance in the system, reduce the violation of the upper limit of the node voltage, and improve the consumption capacity of renewable energy."
the current paper considers the structured hadamard matrices with the purpose of providing the developers of image processing systems with an easy way to select optimal matrices for mentioned above tasks.
"few researches have approached adaptive transformation to mechanical load condition of modular robots. one approach was presented by bojinov et. al [cit] . they proposed a control algorithm that modules forming a table-like structure generate a new leg or remove an extra leg according to the load distribution on the table top, and they showed by simulations that the arrangement of the table legs became appropriate for enduring the given load condition. however, since this algorithm enables only generation or removal of legs, it cannot work in shapes other than the table or in complicated load conditions such as including external force pointing in different directions. in addition, designed only for load adaptive transformation of all-ready constructed table, this algorithm cannot achieve both shape construction and load adaptation in the same control rule."
"as in conventional static designs, all hardware modules sharing a same reconfigurable slot can be managed by the host processor with or without os support. in a standalone mode without os, the processor addresses device components with low-level register accesses in application programs. while in oses, device drivers are expected to be customized. in a unix-like os, common file operations are programmed to access devices [cit], including \"open\", \"close\", \"read\", \"write\", \"ioctl\", etc. interrupt handlers should also be implemented if the hardware provides interrupt services. different device components multiplexed in a same pr region are allowed to share the same physical address space for system bus addressing, due to their operation exclusiveness on the time axis. in order to match software operations with the equipped hardware component, two approaches can be adopted: either a universal driver is customized for all the reconfigurable modules sharing a same pr region. respective device operations are regulated and collected in the code. the id number of pr modules is kept track of and passed to the driver, branching to the correct instructions according to the currently activated hardware module; or the drivers are separately compiled into software modules for different hardware components. the old driver is to be removed and the new one inserted, along with the presence of a newly loaded hardware device. among these two approaches, the former one can avoid the driver module removing/inserting time overhead in the os, while the latter one is more convenient for system upgrades when a new task is added to share a pr region. little special consideration or modification effort is required on the os and device drivers for run-time reconfigurable systems in comparison with static designs. the most important thing to note, is to keep track of the presently activated module in the prr and correctly match the driver software with the hardware. otherwise the device module may suffer from misoperations."
"the algorithm for structural construction described in section 4 adopts rules for generating transformations only to figure 7 : 32 patterns of states which are set \"undesirability\" parameters (undesirability of each of the 8 kinds of characteristic shapes is given by 4 parameters)."
"scenario ii: for the resident loads, the peaks of loads exist at night. in contrast, the peaks of industrial and commercial loads exist in the daytime, which leads to changes in the system operation mode. the annual load curve of the residential load is shown in fig. 8, and the annual load curve of the general industrial and commercial load is shown in fig. 9 . the abscissa in the figure represents a 24 hour day, and there are 8760 data points in total for 365 days. the red dotted line in the figure indicates the trend of the load level, which is represented by the standard value."
"3) determine whether there is a failure component. if there is, go to step 4; otherwise, return to step 2. 4) adjust the control mode of the vscs and set an initial value to the control parameters. 5) perform power flow calculation for the ac/dc hybrid distribution network to obtain the operation scenarios of distribution networks. 6) determine whether the operational constraints of the system is violated. if yes, go to step 7; otherwise, return to step 2. 7) solve the load shedding model with the control parameter optimization of vscs and obtain the minimum load curtailments."
"(2) the control rules for load adaptation can be additionally introduced in the shape construction algorithm, and so the modular robot can perform load adaptation only when necessary during working for the main purpose of shape construction."
"detailed analysis shows that those 15 fault states consist of three failure types that lead to load shedding only in the traditional ac distribution network, as shown in table 4 ."
"(2) as consumer risk since acquiring bad product would harm assembly lines or retail with lowquality inputs and merchandise. in traditional acceptance sampling, the probability of type 2 error (eq. (2)) takes the name of b. in the application of acceptance sampling, the producer and consumer predetermine the acceptable values for p(fn) and p(fp) along with ltpd and aql. the solution for n and c called a sampling plan (or sample design) pl(n, c) is mathematically determined from the binomial or poisson distribution. all of this information is summarized table 1 ."
(1) the algorithm does not depend on the shape or load condition of the structure because it is based on a simple methodology that reinforcement of the structure can be achieved by moving a module to the stress-concentrated portion.
"the symmetric hadamard matrices considered in this paper are built based on mersenne matrices and williamson's three-block array. they are significantly different in type from known sequences of hadamard matrices obtained by paley, sylvester, and scarpis methods and the williamson's classic array. mersenne matrices and hadamard matrices built on their basis are directly associated with numerical sequences. for a number of such sequences, there exist the cyclic and bicyclic matries and propus-form matrices, which provide their simple and efficient storage. mersenne matrices at orders equaled to prime numbers are guaranteed to be of simple structures."
"immediately after the fabrication of the lot, but before expediting the lot to the consumer, an inspection should occur to verify its quality level p. the interest of the producer is to expedite lots with acceptable quality to assure customer satisfaction and future transactions. when an inspection by the producer results in rejected lots, the common practice is to apply universal 100% inspection to the rejected lot still in the factory replacing all bad parts. from the producer point of view, the major worry is the probability of the rejection of good lots p(x [ c/ p b aql p ), which are false positives fp known as producer risk, a false alarm calling the producer to action where action is not necessary. the producer priority is to avoid the rejection of good lots, and, consequently, the sampling plan includes relatively large c. clearly, with c large, the producer judges some bad lots as conforming, but as emphasized above this error is less troublesome for the producer."
"scenario iii: the renewable energy penetration in this paper is defined as the ratio of the maximum power output of renewable energy to the maximum load level of the system. the eens and abandoned wind and light (awal) of the two systems under different renewable energy penetration levels are shown in table 7 . the trend is shown in fig. 11 . fig. 11 shows that when the renewable energy penetration is low, if the power generation level is low, the traditional ac distribution network may not be able to provide sufficient power, which violates the lower limit of the node voltage. in the ac/dc distribution network, since the dc link and vsc can be flexibly controlled to provide the required power, the violation of the lower limit of the node voltage is prevented, and the system reliability is improved."
"it is common in the literature [cit], but not in the original pioneering work of dr, to associate consumer and producer 2 risk with the concepts of the probability of type ii and type i error, respectively. in our approach, we shall go further and develop the generalization that both consumer and producer feel the cost of both errors. in other words, we shall explicitly allow for two type i errors and two type ii errors depending on the perspective of the consumer or the producer. we will show that the decision-making process may be compromised by the commonly used simplification that type i error is felt only by the producer and in like manner type ii error is inclusive only to the consumer. hypothesis testing from np, by offering a common theoretical structure, can produce a better understanding of the application of sampling procedures and their results. 3 in a series of examples, we show that measures of risk will be more reliable and risk itself lowered. [cit] was opposed to acceptance sampling. he argued that inspection by sampling leads to the erroneous acceptance of bad product as a natural and inevitable result of any commercial process, which in turn leads to the abandonment of continuous process improvement at the heart of the organization. deming's position that inspection should be either abandoned altogether or applied with 100% intensity has been debated in the literature [cit], for review and a bayesian approach to the question), and his position is supported by some. even though acceptance sampling is only a simple counting exercise with no analysis for uncovering the causes of nonconforming quality, 4 our position is that acceptance sampling should be an integral part of the commercial-industrial process and, even when perfect confidence reigns between buyer and seller, but sampling itself should never be abandoned. [cit], however, was very much in accord with statistical studies by random sampling that are restricted to inferring well-defined characteristics of large populations, just not as a procedure for continuous quality improvement."
"we also tried another load condition that external force is suddenly added after construction of a 15 module-length bridge (same shape as figure 16(h) ). the external force is equal to the weight of 5 modules. simulated transformation process is shown in figure 17 . there is no overloaded module before loading the external force (figure 17(a) ). after loading, 16 modules in the structure become overloaded state (figure 17(b) ), and structural transformations are generated so as to dissolve all of the states (figure 17(c) ). the finally reinforced shape is the same as figure 16(d), and then the modules begin to lengthen the bridge. this result suggests that, even if load condition suddenly changes during structural construction process, the algorithm enables the modules to switch the transformation mode to adapt to the load condition, and the whole shape converges in an appropriate one."
"later on, our considerations on hypothesis testing will require modifications in the above steps. appendix ''operating characteristic curve (occ)'' presents the operating characteristic curve (occ). the curve is a standard statistical tool for understanding and constructing sampling plans and appears several times in this article."
"where p wt is the annual output power curve of wt, p v is the annual output power curve of pv, p d is the annual load curve, and t is the selected time of each simulation."
"the flash memory is used to hold nonvolatile data for in-field system startup. it will be rarely addressed during the system operation unless external management commands require the bitstream or the os kernel to be updated. on the other hand, application-specific computation starts only after the fpga firmware is configured and the os is successfully booted. therefore on account of the occasionality of flash access as well as the operation exclusiveness between flash and sram, it generates resource utilization inefficiency if the flash controller is permanently mapped on the fpga design but does not function frequently. hence we consider to make the flash memory controller dynamically changeable and time-share the same on-chip resources with the sram controller."
"the high integration of renewable energy in distribution networks effectively improves the economics of the distribution networks, reduces the pollution caused by fossil energy, and introduces a strong uncertainty to the network. to consider these uncertainties, common processing methods include scene clustering, establishing a general outage table, etc. however, these methods do not consider the relative relationship among the output of pv, wt and the load level, which may affect the control characteristics of the ac and dc equipment and the accuracy of the calculation results."
"where p i wt and p i v are the output of wt and pv in simulation scenario i; p i d is the load level in simulation scenario i. the time series sampling method is used to characterize the uncertainties of the renewable energy and load, and it can reflect the relative relationship among the output of pv, wt and load level. when the wt, pv and load fluctuation are considered, the actual situation can be more closely characterized, and the application range is wider. this method can be conveniently combined with the nonsequential monte carlo method for a reliability analysis of ac/dc distribution networks with high integration of renewable energy."
"based on the result, several conclusions can be drawn: 1) as the number of failures considered by the state enumeration method increases, the result of the state enumeration method is gradually approaching the results of the proposed method, which illustrates the accuracy of the proposed method."
"through enabling either the flash controller or the sram controller with system self-awareness, multitasking has been accomplished within a single reconfigurable slot on the fpga. figure 16 demonstrates the rectangular shape of the reserved pr region on a virtex-4 fx20 fpga layout, as well as two controller implementations after place-and-route. the reconfigurable design results in a more efficient utilization of hardware resources, as listed in table 1 . we understand that both the flash memory controller and the sram controller must be concurrently placed in the static system design, implying a total resource consumption equivalent to the sum of both device modules. a pr region is reserved in the reconfigurable design, sufficiently large to accommodate all kinds of needed resources of both device modules. moreover, a little more resource margin is added for the place-and-route convenience of the software tool. in contrast to the conventional static approach, we observe that the reconfigurable system saves 43.7% luts, 33.8% slice registers and 47.9% i/o pads, with both flash and sram services realized. the reduced resource requirement not only enables to fit a large system design on small fpga chips for lower hardware cost, but also makes the i/o pads shared and simplifys the printed circuit board (pcb) routing."
"the communication devices between neighboring modules are composed of infrared leds and phototransistors. one led and one phototransistor are set on each of the four contact surfaces, and they are allocated so that an led of a module faces to a phototransistor of another module when the two modules are neighboring. by turning on/off the leds, the modules send signals to neighboring modules. furthermore, strain gauges are used as force sensors to obtain load conditions of the modules. the gauges are attached at the weakest position in the module structure, where the largest stress occurs, as shown in figure 5 . the specifications of the module are shown in table 1 ."
"in acceptance sampling, the statistical test of the validity of the null hypothesis is based on the relationship between x and c, given the value of n. when the null hypothesis suffers rejection, the researcher makes an inference as inference as to the population value of the characteristic in the hypothesis test. however, the value of the characteristic is not a point estimate but rather only a probabilistic generalization of a region of values inferred from x and c. in other words, the rejection of the null does not imply anything about the point value of p itself other than its role in determining the conformance of the lot. even the construction of confidence intervals do not supply isolated point estimates of the population parameters but rather an interval of probable variation around the point estimate."
"urban road traffic system is the lifeblood of a city, which ensures its operation. predicting traffic conditions for road segments is the prelude of working on intelligent transportation. in this paper, we proposed maplstm, a traffic predicting mechanism for road segments, to promote the development of its. maplstm can accelerate the landing of many applications in a lightweight and fine-grained way. in the future, autonomous humanlike driving based on road topography is worth concern, and we will focus on complex spatial correlations in traffic environment."
it is common practice in all standards for commerce and industry to work with consumer and producer risks at maximum values of either 5 or 10%. the common risk percentages produce four cases illustrated in table 4 .
"based on the fpga run-time reconfigurability, we present a dynamically reconfigurable nor flash controller for embedded designs. this technique is motivated by the operation occasionality of the flash memory and the resultant programmable resource waste on the fpga, when adopting the conventional static development approach. we discuss the"
"the context of hardware processes refers to the buffered incoming raw data, intermediate calculation results and control parameters in registers or on-chip memory blocks residing in the shared resources of pr regions or static interface blocks. in some applications, it becomes contextless when the buffered raw data are completely consumed and no intermediate state is needed to be recorded. thus the scheduler may simply swap out an active pr module. after some time when it resumes, a module reset will be adequate to restore its operation. otherwise, context saving and restoring must be accomplished. figure 9 and 10 respectively demonstrate these two circumstances: in the design in figure 9, two dynamically reconfigurable functional modules (adder and multiplier) do not share the interface registers and they both feature pure conbinational logic in using the prr. hence during each time when the prr is reconfigured with an arithmetic operator, the register values in the interface block are not needed to be saved or restored in order to obtain correct results of x and y.b y contrast in the design of figure 10, the operand registers in the static interface are shared and the reconfigurable region also contains the context of one operand for the addition operation. therefore in case of module switching, the operands of the former operation must be saved in the system memory, and the ones for the recently resumed operator are to be restored. generally speaking, two approaches can be employed to address the context saving and restoring issue: in case of small amounts of parameters or intermediate results, register accesses can efficiently read out the context into external memories and restore it when the corresponding hardware module resumes [cit] . when there are large quantities of data buffered in on-chip memory blocks, the icap interface can be utilized to read out the bitstream and extract the storage values for context saving [cit] . in order to avoid the design effort and large time overhead in the latter case, an alternative solution is to intentionally generate some periodic \"pause\" states without any context for the data processing module. context switching can be then delayed by the scheduler until meeting a pause state."
2) u dc q control mode. the two control parameters are the voltage amplitude on the dc side of the vsc and reactive power on the ac side of the vsc.
"symmetric constructions of haramard-walsh matrices formed by the way of arranging the columns by the frequency of the change in the signs of their entries are of particular interest in image processing tasks. however, ordered hadamard-walsh matrices can be obtained from mersenne-walsh matrices by inverting the signs of their entry values and adding borders. fig. 6 depicts a portrait of a mersenne-walsh matrix of the 31 order obtained by the way of ordering the м 31 matrix. the ways to compute hadamard matrices through chain matrix structures are detailed in papers [cit], where it was demonstrated that the class of orthogonal matrices with two element values is firmly connected to numbers and numerical sequences."
"where p vsc /q vsc is the active/reactive power through the vsc; s vsc is the capacity of the vsc; p vsc,ac /p vsc,dc is the active power from the ac/dc side of the vsc; α is the loss factor of the vsc."
"(c) lstm predicting. the traffic data of vehicle speed, the traverse time, and traffic volume based road segments are input to lstm concurrently for predicting task. the hidden layers of lstm can control the long-term or short-term impact on the current state. after output layer of lstm, it goes through a full connected network with three layers, in which the purpose is to better explore the implied relationships between states. maplstm enables cognition of road segment-based traffic conditions in a lightweight way. for the real-time cognition of global situations, maplstm is still valid by collaboration computing where a groups of cells work together to accomplish a relatively large task. edge computing after cloud computing is a typical collaborative computing environment and has been widely used [cit] ."
"in short, the ac/dc distribution network can greatly exploit the power supply ability of renewable energy due to its flexible and controllable characteristics. it can reduce the occurrence of the situation where the system operational constraints are violated when the renewable energy penetration is low or high, which improves the reliability of the system."
"subsequent studies using the orbits method cover one by one all difficult orders allowing to claim that propuses, despite the mentioned sacrifice of material, are a universal and always solvable symmetric structure. a matrix image of order 140, which is unsolvable for the williamson form, is depicted in fig. 3 . therefore, propuses seem to solve the well-known problem of hadamard concerning the solvability of orders 4k by orthogonal matrices with elements 1 and -1 in a symmetrical form. the reason for this is that orthogonal bicyclic and tetracyclic matrices are specific graphic illustrations of fermat-euler and lagrange theorems about solvability of integers by the sums of two and four squares. integers of the 4k form are not just even numbers, but they also can be divided by 4. therefore, the graphic illustration allows to simplify only in the form of symmetric three-cyclespropuses. four blocks, as it turns out, are simply not necessary to solve the decomposition problem. this somewhat unexpected result is little known. note that in addition to the cyclic and bicyclic forms of blocks of symmetric matrices, there is a negacyclic form, which is different from the cyclic form only in the inversion of signs of elements located below its entries diagonal."
"keywords acceptance sampling á lot quality assurance sampling (lqas) á hypergeometric á operating characteristic curve (occ) á receiver operating characteristic (roc) curve á hypothesis test á r cran when acceptance sampling is presented as a collection of tables in step-by-step recipes, practitioners can learn enough in a few hours to set up sampling plans, and filter out some of the non-conforming product in large lots. this, however, is not enough to negotiate agreements over the long run especially among international businesses. this review of acceptance sampling results from a consultant's need to go beyond the table-and-recipe stage of introductory training, and advance to a level of understanding compatible with the formation of international contracts. we dedicate this work to all those practitioners (especially elton thiesen and carlos reich) who successfully (and quickly) acquired sufficient knowledge of sampling standards to apply sampling plans on the shop floor but confronted a consultant/professor whose toolkit lacked an intuitive approach compatible with the shop floor environment. thanks to jan van lohuizen, armin koenig and osiris turnes who carefully read the first rough draft and offered insightful practical comments. special thanks to galit shmueli who kindly encouraged this project and made some initial corrections in personal correspondence. if merit exists, it belongs to everyone; errors are mine."
"for the industrial practitioner who needs to use a sampling plan from the producer point of view right now urgently, go directly to table 11 for producer sampling plans using the hypergeometric distribution. this table incorporates the most popular plans found in current standards for the producer, but risk factors come from the hypergeometric distribution. it is common in practice to use sampling plan tables without a careful understanding of the concepts that generate the numerical entries and may result in poor decision-making. moreover, this may be the reason for disregarding the hypergeometric distribution as the proper base of the calculations."
"the basis of quasi-orthogonal matrices used for image processing is significantly wider than the orthogonal basis of hadamard matrices and actually includes it. this provides for wider choice in selecting the best fit matrices for processing of specific images including those of non-standard sizes. the peculiarity of quasi-orthogonal mersenne matrices with two values of their entries is that -b is irrational. however, the computing power of today's cpus allow for efficient computations with such matrices, as well as hadamard matrices."
"to explain the lower eens of the ac/dc distribution network compared to the ac distribution network, the n-1 analysis is used to analyze the reliability improvement by the vscs, as shown in fig. 7 . the load sheddings of failure 16-71 of the two networks are identical. thus, the reliability improvement of the hybrid network is a result of fault 1-15."
"for the purpose, we add the following rules. when a module calculates its leader index, if there are overstressed modules in some directions on the row or column, parameters corresponding to the shapes in the directions are multiplied by a certain gain. exceptionally, the overstressed modules calculate their leader indexes without multiplying the gain. an example case is shown in figure 12 ."
"next, we introduce a method to realize load adaptive motions considering stressed states of the modules in order. for example, when the foregoing cantilever structure (figure 11 ) is actually formed in the gravity field, overload caused by the weight of the modules themselves may damage some modules. to avoid the problem, it is necessary that the modules can construct a cantilever with reinforcing overstressed parts which occur in the construction process. for such an adaptive motion, the criterion for generating transformation has to change according to whether there are overstressed modules in the structure or not. the \"r\" marked modules evaluate"
"in traditional acceptance sampling, consumer risk has been exclusively the subject of the consumer, and producer risk the subject of the producer. nonetheless, there is no conceptual reason to restrict each risk factor to only one adversary in the negotiation process. logically, there is no reason why the producer should not recognize and react to the probability of accepting the bad lot what has been called up to now consumer risk. accepting the bad lot is certainly a problem for the producer, however, as described earlier a problem of secondary intensity. likewise, rejecting the good lot and committing a false positive is also a problem for the consumer but of only moderate intensity. from the viewpoint of the consumer we have, ltpd c and p(ltpd c ) and furthermore aql c and p(aql c ). 8 here the consumer feels both risks, primarily the probability of accepting bad lots p(ltpd c ), and less intensely the probability of rejecting good lots [1 -p(aql c )]. the consumer can and should construct his sampling plan using both risks recognizing that less consumer risk should be his objective since its repercussions are more costly, while he tolerates more secondary risk. specifically, the consumer, for example, could use a p(ltpd c ) of 3% and a [1 -p(aql c )] of 10%. the producer could follow analogous procedures. the producer will apply not only the risk pair aql p and [1 -p(aql p )] as would be traditional, but also the risk pair ltpd p and p(ltpd p ) recognizing that the producer suffers from his own secondary risk even though by a lesser degree. for example, the producer could set [1 -p(aql p )] to 1% and p(ltpd p ) 10%."
"in summary, the lower eens of the ac/dc distribution network compared to the ac distribution network because of two reasons: 1) vscs can provide voltage support to the distribution network under extreme failure scenarios. when the failure occurs in the traditional ac network, the voltage drop may result in load shedding. however, in the ac/dc hybrid network, the voltage drop can be relieved due to the voltage support ability of vsc and improve the system reliability."
"another useful way of viewing the intricate relationships within the hypergeometric distribution in the context of acceptance sampling is in light of the receiver operating characteristics (roc) curve, a standard tool in the area of data mining and other computationally intense procedures [cit], chap. 8) . the construction of sampling plans with the help of roc curves will be very useful for the new procedures suggested later on."
"in the next section, we discuss traditional acceptance sampling emphasizing those concepts modified in the rest of the text. sections ''lot tolerance percent defective (ltpd) in consumer risk'' and ''acceptable quality limit (aql) in producer risk'' will present the traditional relationships between aql and producer risk, and ltpd and consumer risk. section ''a unique sampling plan for both parties-dr tradition'' closes the discussion of traditional acceptance sampling offering the possibilities of constructing sampling plans that are unique for both producer and consumer. in section ''acceptance sampling via hypothesis tests'', we will lay out our interpretation of np hypothesis testing and its connection to acceptance sampling. the next two sections will attempt a synthesis of basic concepts in np hypothesis testing and acceptance sampling. we then propose new procedures for the solution to unique sampling plans that simultaneously satisfy producer and consumer. finally, the last two sections present 1 the specific area of applied statistics elaborated here possesses at least three different names. in industrial settings and in this article, the name is acceptance sampling. when dr first introduced this statistical application, the name was inspection sampling. in medicine and public health, the name is lot quality assurance sampling (lqas). [cit] and nist/ [cit] . 2 consumer and producer are the names used in the original work of dr. sometimes we will use similar terms like buyer and seller without causing confusion. conclusions and ideas for future work in the area. a series of appendices offer review material for statistical concepts frequently used in acceptance sampling, including r snippets that give a brief description of the r code used in figures and tables."
"maplstm is fine-grained and lightweight way. it only requires sampled gps points of vehicles and not need to deploy expensive traffic sensors in urban and not use the unobtainable data from ground loop. in this section, we describe maplstm in detail. figure 3 shows the framework of maplstm, which consists of three processes: map-matching, data processing and lstm predicting."
"the classification rule in traditional acceptance sampling is relatively simple: a lot is acceptable if in a sample of size n the number of defective parts x is less than or equal to a predetermined cutoff value c. the inequality x b c identifying a high quality sample signifies that it is very likely that the lot also possesses an acceptable level of quality. on the other hand, if x is greater than c (x [ c), identifying a lowquality sample, then it is likely that the lot is also of low quality. the practice of acceptance sampling determines the values of c and n that reduce to an acceptable level the probability of error. in other words, the intention of acceptance sampling is to minimize the probability of wrongly classifying the lot. equation (1) represents the conditional probability of the sample indicating unacceptable low quality x [ c when, in fact, the lot is high quality p aql, a false positive (fp). 6 the cost of rejecting good lots falls heavily on the producer, more than the consumer."
"considering new features of the dc distribution technique, this paper proposes a reliability evaluation method for the ac/dc hybrid distribution network based on the nonsequential monte carlo simulation. compared to the reliability evaluation method for the traditional ac distribution network, the proposed method can analyze the effect of the dc network and vsc in detail. the remainder of the paper is organized as follows: section ii establishes the failure probability models of the ac and dc components and the probability models of renewable generations and loads. in section iii, the control modes of the vsc yielding to different fault scenarios are modeled based on its operating characteristics, and an optimal load shedding model considering the control modes of the vsc is developed, which can be solved by a second-order cone algorithm. case studies are performed in section iv, and the conclusions are drawn in section v."
"the remainder of this paper is organized as follows: section 2 reviews the literature on traffic prediction. section 3 describes the materials and gives details of our mechanism maplstm. the next, section 4 demonstrates the effectiveness and applications. this paper ends in section 5 with conclusion on our work."
"external force equal to 5 module weight here, x coordinate is directed toward right direction with the origin at the target module whose s should be estimated, and n x is the number of modules on the xth column (see figure 14(a) ). thus, formula (6) means s becomes larger as more modules exist far in the right direction from the target module. in formula (7), h is the number of modules connecting to the left-neighboring modules on the column including the target module, and i indicates what number the target module exist from the bottom in the column (see figure 14(b) ). thus, formula (7) means, following to the beam theory, s is inversely proportional to square of thickness of the beam and becomes smaller as the target module exists far from the top surface of the beam."
"the raw trajectory data cannot be used directly for our predicting task. it is necessary to match and statistics at first. if we want to get the traffic status prediction of road segments, we need to make a segment-based statistics about the traverse time in different directions, the vehicle speed and the traffic volume."
"dr worked in industry and commerce and, subsequently, the design of acceptance sampling they developed, because of the innate conflict between buyers and sellers, was strictly applicable to this environment. our review of hypothesis testing is at most a simple skeleton of the relevant area of scientific methodology, better elaborated in works like rice (1995, chapter 9) [cit] . nevertheless, our interpretation of acceptance sampling in light of hypothesis testing is new to the literature. first, we will concentrate on the nature and definition of the null hypothesis."
"what is unclear in traditional acceptance sampling is the necessity of linking aql exclusively to the producer and ltpd exclusively to the consumer. in reality, the consumer should also be preoccupied with a value of aql, as should the producer with ltpd. we also question why type i error is always associated with the producer as producer risk, and likewise, the same question arises with consumer risk which is necessarily associated with type ii error. the resolution of these questions is new to the literature and the remainder of this article will elaborate a response. in the next sections, we show that hypothesis test concepts from np are relevant to practical applications of acceptance sampling, but only if the specific nature of the decision maker is taken into account."
"transformation to remove the undesirable shape dissolve undesirable local shapes. to realize the motion that chobie ii constructs a bridge-like structure with adapting to its load condition as shown in figure 2, two new rules must be added to the above algorithm. first, we add a strategy to induce desirable local shapes and enable chobie ii to transform to characteristic configurations. next, we realize load adaptive motion of chobie ii by setting a rule that the modules give more attention to local shapes in the direction where large stress occurs."
"traffic condition prediction can not only be used as the design basis of signal control of its but also provide decision support for dynamic route guidance. whereas, there are still some bottlenecks in short or long term traffic prediction through a lot of real spatio-temporal data."
"at present, the reliability parameters of these five parts in the vsc in the distribution network do not have a mature conclusion, which can be derived from the reliability parameters of the high-voltage converter station in the transmission grid. for these five parts, the reliability parameter of the converter valve (v) can be expressed as (1) ."
"when the control mode of the vsc is u dc q, the dc side of the vsc can provide power support to the dc distribution network. when the control mode of the vsc is u ac θ, the ac side of the vsc can provide power support to the ac distribution network."
"the correct routine for hypothesis testing is that first, we elaborate the hypothesis by conceptualizing an important characteristic of the population, and only then, in a second step, are the relevant probabilities of the resulting sampled data calculated. more importantly, the state of the hypothesis in the population is usually unknown, and will remain that way forever. of course, one day in the future, end users will know the quality of the lot with certainty, depending upon the availability of all appropriate data. nevertheless, even after ample time has passed, lot quality will remain elusive."
"2) with n-1 failure considered, the calculation time of the state enumeration method is much less than the proposed method, but the error of the result is very large. with n-2 failure considered, the accuracy is better than the case while considering n-1 failure, but the calculation time of the state enumeration method exceeds the proposed method. this result indicates that the proposed method is more efficient and precise in the case while considering multiple failures which is closer to the actual operation case."
(1) gaussian process regression (gpr) [cit] . it is one of the most popular used prediction algorithms and often used to compare performance as a baseline.
"the main objective of this paper is to discuss the relationship between acceptance sampling and formal hypothesis tests as developed by np. considering that the pioneering work of dodge and romig (dr) (1929) in acceptance sampling, which has survived decades of academic debate and practice, arrived before the formalization of hypothesis testing by np, the question is why bring hypothesis testing into the discussion at all. throughout the rest of this paper, we will attempt to show that, if used appropriately, hypothesis testing offers a more logically complete structure to decision-making and therefore to better decisions."
"based on the nonsequential monte carlo simulation, the process of reliability evaluation of ac/dc hybrid distribution networks with high integration of renewable energy can be expressed as follows and as shown in fig. 3. 1) read in the data and set the maximum number of simulations. establish the annual output power curve of renewable energy and annual load curve based on historical data."
"in the simulation process, the state of the system is obtained by sampling the state of the component (normal operation/fault state) according to the failure rate of the components in the ac/dc distribution network. for each simulation, the renewable energy output and load level are obtained by the failure probability model of the ac and dc components and the probability model of renewable generation and load."
"scenario ii: the load curves in the ac distribution network 2 are resident; other load curves are general industrial and commercial. the renewable energy penetration is constant. scenario i: the monte carlo convergence standard is according to the coefficient of variation (cov) of the eens. when the value of cov is less than 0.05, the simulation process is considered to be convergent. the curve of log 10 (cov) throughout the calculation process is shown in fig. 6 ."
"[…] for the first requirement, there must be specified at the outset a value for the lot tolerance percent defective (ltpd) as well as a limit to the probability of accepting any submitted lot of unsatisfactory quality. the latter has, for convenience, been termed the consumer's risk…."
"the simulation shows that the modules gradually construct a cantilever structure. from this result, it is expected that chobie ii can construct various distinctive structures by using this method."
"the results are shown in table 3 . the eens and saidi of the ac/dc hybrid distribution network are lower than those of the ac distribution network, so it is more reliable than traditional ac distribution network."
"we show that the proposed algorithm is applicable to load conditions caused not only by weight of the modules but also by external force. here, assuming that chobie ii conveys a cabin baggage, we demonstrate the motion of lengthening a cantilever structure with external force acting downward at the tip of the cantilever. figures 16(a)-16(d) show a simulation result in case the external force is equal to the weight of 5 modules. it is confirmed that the modules adaptively construct a cantilever, and the shape of the cantilever is about 1.35 times as thick as that constructed without the external force as shown in figures 16(e)-16(h), compared in the same lengths (length of 5, 10, and 15 modules)."
"traffic prediction of road segments is a fundamental issue in the intelligent transportation systems (its), which can be hopefully used for planning optimal driving routes [cit], urban computing [cit], balancing traffic control [cit], and enhancing driving comfort [cit] . it is necessary to explore the traffic dynamics and analyze the evolution pattern of traffic flow. due to the generation of industrial iot big data, network infrastructures and computational models have been equipped and applied [cit] . if the global traffic information is not recognized accurately and timely, its will be not successfully deployed or the deployed system will be paralyzed sooner or later."
"(a) map-matching. a large number of sampled gps points stored in gps log need to be matched to road segments. in order to facilitate the operation, it is necessary to manually redivide road segments based on the road network before matching. generally, the division is based on the intersection, or no redivision, just based on the inherent segments structure in road network, if the calculation resources and road segments information are sufficient and detailed. we do our best to maintain the original topography relationship between the divided road segments. after map-matching, all gps points can be shifted to the corresponding road segments."
"the hypothesis test attempts to classify the lot by accepting or rejecting the null usually by examining a small random sample. in table 5, the decision maker indicates states of the null by examining a small sample of the population and consequently accepting or rejecting the null hypothesis. for the purpose of this article, we follow statistical methodology; a random sample from the relevant population indicates the state of the null. however, other methods are available outside the realm of statistics, like flipping a coin or throwing seashells in a basket. in the population itself, the null is, in reality, either true or false, even though this condition in the population is unknown to the decision maker, and continues to be unknown even after the sampling procedure. as shown in table 6, the result of the acceptance sampling procedure can have one of four possible results."
"we describe the mechanisms of the chobie ii module. figure 3 shows the overview and the proposed slide motion mechanism of chobie ii system. it consists of two lateral boards and a central board. the two lateral boards include symmetrical motion mechanisms that consist of two sets of gears as shown in figure 3 (b). they are allocated in vertical and horizontal directions, which enable the two directional movements of modules. on the other hand, the central board has grooves as sliding guides as shown in figure 4 (a). although the degree of freedom is less than that of crystalline which also applies sliding transformation [cit], the mechanism of chobie ii maintains high rigidity even during transformation. these mechanisms enable adjacent modules to keep joining each other strictly. in addition, since the block-like shape of the module has high space-filling property, chobie ii constructs a sturdy 2 dimensional lattice structure. transformations of the structure are carried out by simultaneous slide movements of modules in a straight line, that is, a specific \"row\" or \"column\" as shown in figure 4 (b)."
"the calculation of each element of lstm is shown in algorithm 1. at the current time, denotes forget gate, represents input gate obtained by the previous output ℎ −1 and the current input, denotes the cell state andd enotes the cell state at the previous time, denotes the output gate, and ℎ denotes the cell output. lstm can not only save information long ago under the control of but also avoid the current irrelevant content into memory based the gate ."
"for this reason, symmetric hadamard matrices in the form of block-symmetric structures -matrices with complex symmetries consisting of some sets of blocks (matrices of smaller order), not necessarily symmetric and orthogonal, are the most interesting. it is a wide class of orthogonal hadamard matrices 1, which can be computed using algorithms based on the sylvester method [cit] the aforementioned algorithms allow to compute hadamard matrices of orders of the common sequence 4k, but different in their appearance. this difference can be seen on their \"portraits\" [cit], on which two element values 1 and -1 are represented by cells of different colors. below we provide some of the most commonly used or new block structures that allowed to find new hadamard matrices."
"in general, the power of effectively predicting the future traffic conditions for road segments comes from the historical and real-time traffic information. according to the duration for the future, 3-10 days, 1-3 days, within 1 day, and no more than 15 minutes, traffic flow forecast usually is included longterm, recent-term, short-term and short-time [cit] . most of the existing methods present prediction trend either by using probability and statistics of the time-dependent evolution of current road, or only using the pure spatial relationships among various road segments. although available spatiotemporal information is combined to model the traffic network pattern, the information does not play out its full potential."
"in this paper, when analyzing the reliability of an ac/dc hybrid distribution network, the main components that may have fault states are the ac transformer, vsc, ac/dc line and ac/dc bus. apart from the vsc, the reliability models of the basic components can be expressed as determined reliability parameters, which can be obtained from historical data. as shown in fig. 1, the vsc consists of five parts: 1) ac equipment (ac-e); 2) converter valve (v); 3) control and protection system (c&p); 4) dc equipment (dc-e); 5) others (o). these five parts are in a series structure."
"both the flash and the sram controllers are picked up from the xilinx ip library. we do not concern their in-depth design details, but simply regard them as blackboxes instead with communication interfaces demonstrated in figure 12 . in the figure, the left side is the interface to external memory devices (flash or sram) and the right side is to the system bus. figure 13 shows the hardware structure: an off-chip asynchronous nor flash memory and a synchronous sram share the same data, address and control bus i/o pads of the fpga. these two chips are exclusively selected by the \"ce\" signal. the flash and the sram controllers are both slave devices on the system bus. they are selectively activated in the reserved prr by run-time partial reconfiguration. in order to isolate the unsteady output signals from the prr during active reconfiguration, \"disconnect\" logic is inserted in both interfaces between the controllers and the plb bus or external devices. moreover, a dedicated \"reset\" signal takes charge of solely reseting the newly loaded module after each run-time reconfiguration. both the \"disconnect\" and the separate \"reset\" signals are driven by a gpio core under the control of the host processor. an open-source linux kernel runs on the host powerpc 405 processor. to manage run-time operations in linux, device drivers for hardware ip cores have been brought up to provide programming interfaces to application programs. we configure the open-source memory technology device (mtd) driver [cit] to support nor flash accesses in linux. other drivers are customized specifically for the lut block in sram, plb_gpio and mst_hwicap. with drivers loaded, device nodes will show up in the \"/dev\" directory of the linux file system, and can be accessed by predefined file operations. the drivers are compiled into modules. they will be inserted into the os kernel when the corresponding device hardware is configured, or removed when not needed any longer. the hardware process scheduler is implemented in a c program. it detects the memory access requirements on flash or sram from either the system interior or external user commands, and meanwhile manages the work sequence of both types of memories. figure 14 shows a flow chart, in which the scheduler alternately loads the flash and the sram controller with context awareness. during the device module reconfiguration, the linux os as well as the remaining hardware system keeps running without breaks. in this figure, steps labeled with \"a -g\" are used to dynamically configure the sram controller, and the ones labeled with \"a -g\" are to load the flash controller. events marked by the symbol \"≪\"a r ed e t e c t e d by the scheduler to trigger hardware context switching. main switching steps before device operations include:"
"case 10-5 in fig. 5, represents a sampling plan that pleases the buyer and demonstrates his market power by putting the seller at a disadvantage. this case is actually quite frequent when the seller is a small or medium sized establishment and the buyer is a large retailing or manufacturing firm; producer risk [1 -p(aql p )] has been placed at 10% while consumer risk p(ltpd c ) remains at 5%. aql continues to be 0.005. the resulting unique sampling plan is pl (3000, 1400, 9) . the buyer should be very pleased with this plan represented by a risk factor of 4.7%, while on the other side, the supplier finds his position weakened, as he is obligated to produce at a relatively high-quality rate aql of 0.005 and must confront a risk factor of 9.7%. once again, the difference is large between the outcomes of the hypergeometric and the binomial probability functions."
"simply stated, a hypothesis is a clear statement of a characteristic of a population and usually its numerical value, or of a relationship among characteristics (something happens associated with something else), that may or may not be true. it carries with itself a doubt that calls for evaluation. hypotheses are not unique but come in pairs (or multiples not reviewed here) of exclusive statements in the sense that if one statement is true then the other statement is false. when the decision maker judges one of the hypotheses as true, he necessarily judges the other as false. the lot is conforming or non-conforming. vaccination drives reached the target population or not. your candidate is winning the election campaign or is not winning. the accused is either innocent or guilty. from the viewpoint of the decision maker, the consequences of incorrectly rejecting one of the hypotheses are usually more severe than those of incorrectly rejecting the other. as we have seen above, lots are either conforming or non-conforming, and for the consumer for instance, incorrectly accepting the non-conforming lot committing the false negative can be disastrous. in such a case, the null hypothesis is the statement that costs the most when wrongly judged [cit] . this nomenclature serves to organize relevant social or industrial questions or laboratory experiments. the null carries the symbol ho, the alternative hypothesis ha. from the consumer's point of view, therefore, the null hypothesis is that the lot is nonconforming. rejecting this null when it is true incurs extremely high costs for the consumer. in similar fashion but from the producer point of view, the null hypothesis is that the lot is conforming, because as mentioned already, rejecting this null has extremely high costs for the producer. we illustrate these differences in table 5 ."
"recognizing that the points of view of the producer or consumer come from different perspectives, the value chosen for c is of pivotal importance. the producer will want a c that is relatively large, admitting the possibility of accepting bad lots so that there is no rejection of good lots. the consumer, on the other hand, will desire a low value for c, consequently rejecting some good lots but better guaranteeing the acceptance of only good lots. therefore, it will be difficult if not impossible to find values for c that will serve the desires of both adversaries."
"case 5-10, illustrated in fig. 4, is the most encountered in practice: consumer risk at 10% and producer risk 5%. buyers (who are disinterested or ignorant to the disadvantages) apply sampling plans that follow these risk levels even though they are prejudicial to the buyer himself. for aql and ltpd at 0.005 and 0.01, respectively, the plan pl(3000, 1400, 10) satisfies the risk conditions specified. buyer risk is 0.098 and producer risk is 0.034. this plan is slightly easier to apply than case 5-5, given the smaller sample n and acceptance number c."
"(2) based on a large scale of taxi gps trajectories, we propose maplstm to extract features from the highdynamic, high-dimensional, non-linear and non-stationary traffic flow. and we confirm that maplstm have a higher predicting accuracy than gpr [cit] and convlstm [cit] ."
"as mentioned above, the existing methods are still laborious for lightweight, fine-grained, and accurate prediction. so we propose maplstm to predict traffic conditions effectively. we analyze and compare the use about the predicted traffic conditions in navigation planning, as in table 3, the lower the computing complexity, the lighter the planning algorithm; the higher the navigation accuracy, the better the navigation performance; perdurability represents the sustainability of a transportation system; the higher the perdurability, the more sustainable the transportation system."
"it should be noted that even if a cyclic matrix is not symmetric, it is still symmetric relative to its second diagonal in the way it is built, and it can be brought to symmetry by mirroring the elements relative to the vertical or horizontal center line (such matrices are known as reverse cyclic matrices). however, the matrices' orthogonal and cyclic properties are contradictory, which was first noticed by ryser [cit], who formulated his well-known hypothesis that there are no cyclic hadamard matrices of orders above 4. evidently, the area of application of such matrices is limited, while their storage and calculation for such orders is nonessential."
"8) determine whether the number of simulations has reached the preset value. if yes, the simulation process ends, and the reliability index is calculated and output; otherwise, return to step 2."
"from the above results, two others characteristics of the ac-dc distribution network can be drawn: flexibility and controllability. when an ac distribution network cannot provide sufficient power due to excessive load, the dc link and vsc can flexibly perform power transmission to provide the required power; otherwise, when the load is relatively low, a reverse power transmission can be performed to provide power support for other distribution networks. however, in the traditional ac distribution network, such power transmission cannot occur, so the aforementioned fault scenario may occur and cause a load shedding process. therefore, when there are different types of loads in several areas of the system, the ac/dc distribution network can greatly adapt to the changes of the load distribution due to the flexibility and controllability of its operating mode, which improves the reliability of the system."
"the tasks related to digital image processing using orthogonal matrices include the filtering, compression, antinoise coding, and masking of images and video sequences [cit] . the practice of application of orthogonal matrices requires them to be of simple structures. this defines, in many ways, the size of memory required to store predefined matrices or the time required to generate matrices by the image processing system. the symmetric structures of hadamard matrices involve the cyclic, negacyclic, block-symmetric, of \"core with bordering\" shape, and others."
"the first requirement for the method will, therefore, be in the form of a definite assurance against passing any unsatisfactory lot that is submitted for inspection."
"in scenario ii, the eens and saidi of the ac distribution network are even greater than the ac/dc hybrid distribution network compared to scenario i. in addition to the three failure types in scenario i, one more type of failure state that may cause power loss only in the ac distribution network: when the sampling time is at night, the load level of the ac distribution network 2 is relatively high. at the same time, if the output of renewable generation is low, there may be a voltage drop at the end node of the line, which results in load shedding. in this situation, for the ac/dc distribution network, due to the flexible and controllable capability of vsc and dc link, when ac distribution network 2 is overloaded, ac distribution networks 1 and 3 can transfer power to ac distribution network 2 to avoid voltage or current violations."
"the high penetration of renewable energy and application of the ac/dc hybrid distribution technique make the traditional reliability assessment method no longer applicable. in this paper, the probability models of renewable energy and load are established. then, multiple control modes of vsc are modeled considering different fault scenarios. thereafter, an optimal load shedding model for the ac/dc hybrid distribution network is proposed. based on this model, a reliability assessment approach is developed for ac/dc hybrid distribution networks. case studies are conducted, and the following conclusions are drawn:"
"5 also called limiting quality and consumer's risk quality in iso (1999 iso (, 1985 . 6 the use of the concepts of positive and negative are common in the medical and data science literature, for instance, provost and fawcett (2013, chap. 7) . sensitivity of a test to recognize true positives (tp/ (all positives)). specificity is the capacity to recognize true negatives (tn/(all negatives))."
"2) assume that the total duration of the simulation is n years and 8760 hours a year. then, extract a random integer t between 0-8760 * n as the selected time of simulation scenario i."
"when there is a fault state of the system, a part of distribution network may lose power support. at this time, the control mode of the vsc must be changed to provide power support to the system."
"as noted before, the existence of hadamard matrices is limited by 4k orders, which significantly limits their application for processing of images of arbitrary resolution. a number of limitations exists in application of sylvester and scarpis methods, especially for computing matrices of large orders. relatively recent concepts [cit], which provided some exotic orders in a simple form, only confirm this general rule. it should be noted that floatingpoint calculations take virtually the same time as integer calculations on modern computers. that is why, stepping away from the requirement for entries to have integer values, which is typical for hadamard matrices, allows the significant expansion of the basis of orthogonal matrices with two values of elements."
"to simulate the load adaptive transformations on a computer, it is required to estimate the output voltage s [mv] of the strain gauge sensor circuit of the modules. in this paper, we use estimation formulas (6) and (7) derived from the result of load experiments using actual machines and linear beam theory:"
"in this method, although each module deals with only local information on its row and column in a decentralized manner, the whole structure can generate transformation for dissolving undesirable shapes. the criterion is regulated by 32 parameters a v and a h in (1) which mean undesirability of local contour shapes in figure 7 . by setting these parameters appropriately, we can enable chobie ii to perform various motions."
"applying this method, we perform a simulation of constructing a cantilever structure from a rectangle structure. there are three transformation modes required for this motion, as shown in figures 10(a), 10(b), and 10(c) ."
"map-matching is the process of aligning a sequence of observed gps positions with the road network on a digital map [cit] . as a preprocessing step of maplstm, map-matching can effectively improve the existing huge amount of low-sampling-rate gps trajectories in data set. as shown in figure 2, map-matching can be performed with the same or different time interval as the gps points. the gps points without map-matching can only be mapped to the road network. not all gps points can be mapped to their corresponding segments due to the gps positioning error. but after map-matching, all gps points can be corrected to the corresponding road segments."
"the requirements for quality of video information and its resolution are constantly increasing. this is a trend promoted by manufacturers of image sensors and displays. recently, the processed images were of pal, secam and ntsc formats, whereas today the uhd (ultra high definition) digital format implies the processing of 38402160 (4k) and 76804320 (8k) video resolutions. the quality box technology of selection of image areas (on an image in a video sequence) requires the processing of images of arbitrary size."
"2) in the ac/dc network, each ac network may have multiple backup sources because of the connection of the dc network. due to the flexibility of the operation mode of the vsc, the network can have continuous power control capability, which effectively reduces the load shedding. in contrast, it is difficult for the ac system to transfer all loads through the switching operation, and it takes a certain amount of time. therefore, the ac/dc network has a stronger transfer capability and higher reliability."
"where b t is the transposed b matrix and -a t is the transposed a matrix with inversed signs of its elements. only blocks a and b are required to obtain symmetric matrices in the bicyclic form, which amounts to half of the number of its elements 2(n/2) 2 . the bicyclic form, without taking symmetry into account, requires only n elements. such matrices, being of block-symmetric structure, can be symmetric at the same time, as shown in fig. 1 . paradoxically, the resolution threshold of a bicyclic structure by symmetric matrices was not verified by anyone for a long time, although it is logical to assume that the raiser constraint applies to this structure. even, existence of a solution in such a simple form would be satisfactory. according to our evaluations [cit], which were pre-verified in a large-scale computer experiment held in a specialized computer center in canada [cit], the bound of symmetric bicyclic hadamard matrices of orders does not exceed the 32 nd order (lower bound 8 follows from the ryser hypothesis and the sylvester method)."
"first, by examining message flows, we find that replies arrive quickly and a significant portion of messages propagate far away from the originator, i.e., the discussions are not restricted to his/her followers. further, we show how messages related to michael jackson's death spread through the network, which demonstrates the power of twitter as a social medium."
"two messages posted by different users may share the same message id. -text: the content of the message, up to 140 characters. -created at: the creation time for this message."
"the presentation is not without its limitations. the biggest limitation of the present research is that deviance itself is hard to quantify. [cit] stated, it is difficult to arrive at an accurate estimate of a player's off-field misbehavior. the measure used here in the present study is comprised of only incidents and the media attention related to those incidents. thus, there may be incidents that do not receive any attention that would fall into the search terms outlined above. furthermore, the measure does not separate types of incidents. from the search terms used to generate the newspaper counts, there are many possible behaviors that players could have undertaken as well as many different types of punishments both from a team standpoint (e.g., suspension) and a legal standpoint (e.g., jail, plea agreement) that are not quantified within this analysis. the nature and types of negative off-field behavior might take more precedent for team executives and evaluators."
"hence we conclude that for retweet influence, either the number of messages (rt m ) or users (rt u ) can be used whereas for reply influence, it would be prudent to evaluate the choice of r m or r u with the specific application scenario."
"table 1 presents the summary statistics for the sample period. recall there are 4,061 [cit] . we find defensive skill players (db and lb) make up the highest percentage of the observations at 31.5 percent of the overall sample. they are followed by offensive skill players (fb, rb, te, and wr) at 28 percent and defensive lineman at 17.5 percent. the maximum number of major awards won was six (manti te'o, university of notre dame), and the maximum number of all-american honors was three (barrett jones, university of alabama). the average age for a drafted player is 22 with 11 percent of the drafted players coming from non-division 1-fbs programs and 27.5 percent coming from non bcsaq schools. the majority of the observations (71 percent) participated in at least one event at the combine."
"examining the combine performance variables, the results from table 3 show an increase in the 40 yard dash time leads to an increase in a player's overall draft selection. in other words, the slower a player is at the combine, he is selected in the higher rounds of the amateur draft."
"furthermore, we are interested in the top k most influential users. it is problematic to compare users with small influentials. on osns many users are likely to have small influence, i.e., the long tail effect. small variance may change their rankings a lot. for example, lots of users have only a couple of replies, getting one more reply may improve a user's rank by several thousands or more. in this paper we evaluate the ranking distance with top 1, 000, 5, 000, and 10, 000 most influential users."
"thus, the purpose of the present research is to examine the impact that deviant behavior in an amateur football player's last season of college impacts his overall draft position in the national football league draft (nfl) and if this impact differs with a change in policy. [cit] and is the main mechanism used by the nfl to allocate amateur talent [cit] . also, prior studies show that draft position is a significant indicator regarding playing time and career length (e.g., [cit] ."
"in some cases, especially in the event of breaking news, it is possible to identify a set of closely related messages by keywords. these messages are all about the same event although they may belong to different message flows. section 4 presents our analysis of 500k messages related to michael jackson's death."
"another limitation is we only examine the players that were selected in the nfl draft. [cit] noted a player's criminal record can close doors in many employment situations because the establishment may not always probe deeper in the context or possible complexities of a situation. thus, there could be players with on-field talent to be drafted, however, character and/or legal issues may have prevented nfl teams from drafting the players. the present research cannot account for that scenario. thus future research should further examine these incidents to look at the impact the personal conduct policy has on an amateur player's draft status."
"to evaluate how stable these metrics are, we split the dataset into two sets according to when the messages were posted and compare their ranking lists. if with a certain social influence metric, the ranking lists of these two sets are close to each other, this metric is relatively stable."
"-we discuss in detail how to evaluate social influence metrics, such as the matching process to compare the top k lists. - [cit] only considered ρ and overlap."
lerman and ghosh [cit] measured how popular news spread on digg (a social news aggregator) and twitter. voting on digg and retweeting on twitter are used to identify related messages. the number of votes/retweets is shown to increase quickly within a short period of time and saturates after about a day. the distribution of story sizes (number of votes/retweets a story has) is approximately a normal distribution instead of the widely observed power-law distribution. their observations are based on the aggregated patterns of many stories thus does not focus on breaking news. a key difference is that breaking news such as michael jackson's death lasts much longer time and covers many more users. lerman and hogg [cit] further developed a model to estimate the popularity of news. it is not clear if their model is able to predict the spread pattern for breaking news as such stories tend to be outliers compared to average popular stories.
"to show the change of social influence over time, we sort the messages according to their timestamps and split them into two by selecting the first 50% messages as dataset i and the second 50% messages as dataset ii. for each dataset we compute ρ, τ, and overlap for their top k lists, shown as table 3 . the results on f is not available because we only have one snapshot of twitter.com thus do not know the number of friends of each user before and after a certain time. the maximum of each column is highlighted. for ρ, τ, and overlap, we always have the following order:"
"1. twitter.com (38.0%); 2. tweetdeck (11.8%), a popular twitter client; 3. twitterfeed (5.7%), a service which enables users to send messages by posting to their blogs."
the web ecology project [cit] examined 1.8m tweets about michale jackson's death and show how users express emotion on twitter. this work focuses on content and semantic analysis and provides us with insights for how users encode emotional content with a small number of words. our analysis in section 4 complements their work.
"shown as fig. 2, the number of covered users increases quickly in the first 15 days, with a large jump around the memorial service. within 70 days about 9m users were covered, roughly 12% of the entire twitter graph. the number of covered users keeps increasing after the memorial service although with a slower rate. this suggests that these messages are sent by new originators. examining the differences between initial shocks and afterward thoughts will be an interesting direction for future work. to further analyze the message propagation process, we define a poster as a twitter user who posted at least one mj related message. fig. 3 shows the cumulative distributions for the number of of covered users and the number of posters. the first 5% posters covered about 20% of the users, which demonstrates the amplification power of osns as a medium, i.e., a small number of users may push a message to a large number of users. as more and more users post messages, few new users are covered since most users have already been covered by earlier posters. this is a strong signal of the small-world effect."
"for the mj data set, 58.5% messages were posted through the twitter website, and the five top mobile clients contributed only 11.0%. further investigation in the data suggests that users are more likely to use mobile devices for sharing events related to themselves, for example, where they are or what they are doing."
"according to wikipedia, social influence \"occurs when an individual's thoughts or actions are affected by other people.\" when it comes to specific fields or application scenarios, this vague definition needs to be clarified, i.e., what is the action and how to determine whether an action is affected by other people."
"estimating a regression model with the dependent variable being the log of overall draft position, we find a player's deviance one year prior to the nfl draft improves his overall draft position, meaning he is drafted higher in the nfl draft under the original personal conduct policy. however, examining the player's deviance one year prior to the draft under the goodell era does not impact a player's draft position. furthermore, the present research examines the relationship across different types of player subgroups-defensive skill players, offensive skill players, offensive linemen, and defensive lineman. the results show some variation regarding the impact of deviance on a player's overall amateur draft status under both personal conduct policies."
"the second variable of interest is the interaction between the number of mentions in the newspaper and the modified personal conduct policy under roger goodell (deviance*goodell). examining table 2, we find this variable to be insignificant. taken this variable along with the deviance variable, we can say that under the previous personal conduct policy, players were rewarded in terms of a better draft position with an increase in deviance. however, this advantage has since gone away with the introduction of the modified personal conduct policy under goodell, which provides quicker and harsher penalties for players whose off-field behavior does not coincide with the values of the league. table 3 specifically examines nfl combine participants across four different player groups. from table 3, notice the deviance variable is negative and significant for both the offensive and defensive skill position players at the 90 and 99 percent confidence interval respectively. however, the number of mentions in the newspaper did not affect the draft position of offensive and defensive lineman. examining the interaction variable, this variable is insignificant for all positions except for defensive skill position players. for the defensive skill position players, there is a positive and significant increase in draft position meaning each additional mention in the newspaper leads to a lower draft position. one possible explanation revolves around risky workers [cit] as this position grouping may not be the area in which team executives want players taking risks on the field due to potentially giving up 7 points. thus, this risky action of off-field misconduct may signal behaviors teams do not want to see with defensive skill position players."
"twitter.com is a \"real-time information network\" for people to share news and communicate with others. a user (twitter) may send messages (tweets) via any of the following channels:"
"secondly, we also compare the ranking lists given by different metrics to examine their correlations. if two ranking lists are close to each other, these two metrics are similar. we want to identify the metrics which are similar and those which are unique."
"research across many disciplines examined criminal and other defiant behaviors among athletes across various sport levels. research examining professional sport and criminal/deviant behaviors focused on the policy implications for both the criminal justice system [cit] and professional leagues and its member clubs [cit] . more recently, researchers begun to examine how off-thefield issues affect different stakeholders. [cit], for example, found that amateur players who were charged with a crime the year prior to being drafted were selected lower in the nfl draft. if players were not charged with a crime but were accused of a crime, they were selected higher in the draft."
"both nondiv1fbs and nonbcsaq focus on the school the player attended the year before being selected. nondiv1fbs is a dummy variable equal to 1 if the observed player was drafted from a school that was not a division i-fbs school while nonbc-saq is a dummy variable equal to 1 if the observed player did not play at a school that was part of a bowl championship series automatic qualifying (bcsaq) conference. it is expected both of these variables will have positive and significant coefficients as this result would be consistent with previous research incorporating these variables (e.g., [cit] the variable combinepart is a dummy variable equal to 1 if the player participated in one of the combine events besides being interviewed by teams. data regarding combine participation was collected from the nfl combine results website."
"25% message flows lasted less than two minutes and 75% lasted less than an hour. in other words, most conversations ended quickly. there is also a huge gap between its mean (6 hours) and median (8.9 minutes), indicating some outliers with long life time. therefore we conclude that on twitter, messages get replied quickly and propagated relatively far away, although most conversations last for a short period of time."
"online social networks (osns) have become a major internet service for people to communicate with each other. to measure the information propagation process and the driving force behind it, social influence, this paper presents a detailed analysis of 58m messages over twitter.com. more specifically, the following problems are investigated in this paper."
"on twitter.com, a user may reply to a message sent by another user. such reply message has the fields of \"in reply to status id\" and \"in reply to user id\" thus allows us to match it with the message it is replying to and further identify the message flow. 28.1% messages in our dataset are replies. [cit] reported a similar portion of replies in their dataset (25.4%). the mj dataset has a much smaller portion of replies, 9.4%, which suggests that it is more common for people to express their own feelings or opinions about mj's death instead of discussing it with their friends."
"the awards variable looks at the cumulative number of individual major college awards 2 (besides all american honors) won during a player's college career. allamerican is the cumulative the number of 1st, 2nd, or 3rd team all american awards won during the player's college career. the data for both of these variables were collected and verified from multiple websites. the age variable is the age of the player at the time of the draft. [cit] stated, the age variable is a proxy for competency, maturity, and ability. the age variable was collected from pro football reference."
"we find that an increase in the amount of repetitions on the bench press increase lowers the player's overall selection in the draft for offensive players. for defensive players, the number of repetitions on the bench press does not impact draft position. the results from the vertical jump shows different results for defensive skill players and offensive linemen compared to offensive skill and defensive linemen. for the defensive skill players and offensive lineman, the higher the vertical jump leads to a lower overall selection in the amateur draft. for the other two player groupings, the performance in the vertical jump does not impact overall draft selection. finally, the shuttle run does not impact overall draft position for all four player groupings."
"-internet slang or short phrases, e.g., the top 10 most popular tweets in our data set shown as table 1 . -automatically generated messages, most of which are sent by virus/worms or online services for advertisement or spam purposes. for example, there is a website which allows users to customize the background of their twitter pages. after a user enables this service, it automatically posts an advertisement on behalf of the user to his/her followers. there are also many messages advertising for websites which claim to boost the number of followers for users. some of them are in fact performing phishing or \"bait and switch\" attacks. extensive worms/epidemiology studies have been devoted to the propagation of such messages therefore we do not look further into it in this paper. \"\"(empty) 11,917 \"lol!\" 10,295"
"both the number of messages and the number of users can be used to assess the influence. to see the difference, we compare the top k lists generated by the same influence metric with different assessments. shown as table 4, the difference between rt m and rt u is small, and more importantly, it does not get much larger as k increases. r m and r u, on the other hand, are relatively far away from each other. we find that the gap between them gets larger as k increases, shown as table 5 . this suggests that the distribution of replies is highly skewed, i.e., some users make many more replies than others. checking the pair of users involved in a reply, we find that 47% replies are between the top 10% most frequently communicated user pairs."
"among all the messages we crawled, we select the tweets containing \"michael jackson\" or \"mj\" as mj related messages. to filter the false positives introduced by the query \"mj,\" which mostly are generated by url shortening services such as tinyurl.com and bit.ly, we require that for each message there is no leading character before \"mj.\" after removing the noise, we found 549, 667 mj related messages (about 1% of the entire data set we crawled) posted by 305, 035 users. 548, 102 messages were posted after jun 25, 2009 12:21:04 p.m. (pst), when the 911 call was made to save michael jackson's life. we assume that these messages are related to the breaking event."
"in summary, the results showed that an increase in the number of deviance incidents in the year prior to being selected in the nfl amateur draft improves a player's overall draft spot. when partitioning the sample into four player groupings, we find this relationship to be consistent with both defensive and offensive skill players. however, the opposite relationship for offensive lineman occurs in regards to deviance. of particular interest in the present research is to see how the relationship between deviance and draft status changes with the modification of the personal conduct policy under roger goodell. we find that deviance under goodell is generally insignificant in regards to a player's overall draft status except for defensive skill players who sees their overall draft selection diminish."
"given the attempt by league executives to deter improper off-field behavior that may impact the reputation and financial viability of the league, one may expect that team executives take an amateur player's off-field behavior into account when evaluating his potential selection in the nfl amateur draft. the impact that off-field misconduct has on an amateur player's draft position over these two policies is examined below."
"the final group of variables specifically examines a player's participation in the combine and some of the player's performance results from the combine. regarding table 2, there is a dummy variable for if the observed player participated in at least one event of the nfl combine. the results from this variable in table 2 shows that participating in the nfl combine led to a significant decrease in overall draft selection, meaning that the player was draft lower in the draft. while this result is not surprising given the nfl would invite some of the top draftees to the combine, players have the ability to voluntarily not participate in the combine due to personal choice or injury. thus, participating in the combine does provide benefit to those players. table 3 presents the results that include six combine specific variables, the listed height and weight of the player and the performance of the player in the 40-yard dash, bench press, vertical jump, and shuttle run. in regards to the height variable, we find that height is a significant predictor of draft selection for offensive skill players, defensive lineman and offensive lineman. more precisely, height improves the overall draft positions for players of these three positions. these results support previous research examining height and draft selection in the national basketball association (nba) [cit] . across all four position groups, an increase in the player's weight improves his overall selection in the draft."
"except rt u, both ρ and τ decrease as the list gets longer (i.e., k gets larger). this validates the motivation to use the top k lists instead of the entire list, i.e., the top k users are more stable than the users with lower ranks."
--1 if a ranking list is the reverse of the other; -0 if the ranking lists are completely independent; -1 if the ranking lists are the same.
"in addition, to get the social graph for computing propagation distance, we crawled 61, 907, 902 nodes with 1, 520, 959, 208 links, which covers 88% of the entire twitter network according to the estimation given by moore [cit] ."
"from the present research, we can say that the modification of the personal conduct policy did begin to change the behavior of teams in evaluating amateur players."
"as we have stated before, the mj dataset has fewer replies (9.4%) than the entire dataset (28.1%). more importantly, all the messages in the mj dataset are considered to be related to michael jackson's death. therefore we can examine the aggregated propagation patterns of all these messages, which are more interesting than those of individual message flows."
"there are many possible interpretations for why a positive and significant coefficient may occur. the first is the failure of the policy to actually change a team's drafting decision. if one of the purposes of the personal conduct policy is to deter negative off-field behavior of players and, by extension, make teams aware of offfield issues more closely prior to acquiring a player, the results from table 2 would suggest the initial policy did not influence a team's drafting decision. furthermore, the original personal conduct policy was written in such a manner that the commissioner could not punish the player until he was convicted or reached a plea deal (mahone jr., 2008) . the lack of speed in punishing players and the overall ineffectiveness of the original policy would seem to have an effect on the negative and significant coefficient in table 2 . the second reason for the positive coefficient is it could be the deviance variable is detecting aggressive behavior that is a desired quality for nfl teams according to previous research (e.g., [cit] ) . the third reason is misconduct can be a form of risky behavior. [cit] stated in their analysis of mlb player salaries, workers whose performance is risky or more uncertain will earn a higher wage. certainly any deviant behavior undertaken by individuals carries an element of risk both for the person as well as other stakeholder such as employers. however, team executives may want to take that risk of a player because of the potential for higher future rewards. [cit] when he stated \"[t]eams wanted to win, so they overlooked all of the negative consequences that came with this talent. talent wins games; it also buys second chances\" (p. 109). in other words, teams are willing to overlook off-field behavior due to the talent possessed by these amateur players and the projection of how these amateur players will impact a team's future on-field performance."
"the second stream examines the rule breakers themselves. [cit] looked at the reasons provided as to why athletes are violent towards women. she outlined several theories used in the literature to explain a player's off-field behavior. [cit] examined 100 nfl players who had committed various violent crimes against females. he found certain football positions, such as running backs, increased the chances that the player would commit some form of violent crime against a woman. [cit] examined the sentencing of elite athletes who committed violent crimes. in his investigation, he found that sentencing laws differed between states as well as discussed the challenges of sentencing elite athletes compared to non-athletes."
"the final three variables are the variables of interest in the present study. deviance is the count of deviance issues a player accumulated 365 days prior to the nfl draft. deviance issues were analyzed on an individual basis for each player based on a predetermined set of search terms. for the purpose of the present study, deviance was classified with the following predetermined search terms: arrest, suspension, drugs, violence, off-field, misconduct, cheating, alcohol, scandal, fight, assault, and battery. these search terms revealed the newspaper articles containing one or more of the search terms as well as the player under examination. the newspaper articles were examined to make sure that the article was about the player committing one of these forms of deviance. deviance issues were logged based on media exposure as disclosed by factiva (academic license). the second variable is a dummy variable for the draft years after roger goodell took over as commissioner. thus, the goodell variable takes the value of 1 [cit] drafts, 0 for all other draft years. the final variable is an interaction term between the previous two variables (deviance*goodell). this variable examines the influence of deviance in years while goodell is commissioner and its effect on a player's overall draft selection."
"we also report the overlap between the top k lists, i.e., the number of items which rank top k in both lists. the overlap is normalized by k for comparison between different ks."
"a social influence metric gives a score for each user being measured. for example, f gives the number of followers of a user. the score itself does not tell us how influential a user is, while when we compare the scores of two users, the one with larger score is likely to be more influential. thus what really matters is the relative order of the users according to the metric, which can be represented by a list of users ranked by the metric. with ranking lists, it is possible to compare social influence given by two metrics which capture different actions, for example, the number of followers (f ) and the number of replies (r m )."
"shown as fig. 1, these messages cover about two months after the tragedy. initially there were many speculations and shocks, which corresponds to the first spike. the initial spike would be larger if we started our crawling at day 0 instead of day 2. there is a spike in day 3, probably because it is the first day we queried twitter for entire 24 hours. the largest spike occurred during the day of michael jackson's memorial service, july 7th, 2009, i.e., 12 days after his death. comparing to an average day within our data collection window, there were 10 times more mj related messages posted at that day. after that spike, the number of messages kept steady between day 16 and day 60, although we stopped querying twitter to find new users (not tweets) around day 50. this \"ripple\" effect suggests that breaking news does not always disappear quickly, a counterexample of the \"15 minutes of fame\" theory. propagation distance can not be used here because this message (mj's death) was widely covered in the real world. there are many originators for this message and it is hard to tell if a user gets it from a followee or not. thus we evaluate the coverage of the message within the twitter network. more specifically, if a user posts a message related to mj's death, all of his/her followers will receive this message, i.e., they are covered by this message. as a user may receive mj related messages multiple times, we only consider the number of unique users who are covered."
"to quantify the difference between two ranking lists, the following two measures are widely used. given n different items and their two permutations (ranking lists) x and y, -spearman's rank correlation coefficient (ρ) [cit] :"
the paper is structured as followed. the first section looks at the literature examining off-court issues in professional sports. the second section provides a brief history of the nfl's personal conduct policy. the third section presents the data and estimation strategy for the present research. the fourth section presents the results followed by a discussion and conclusion section.
the discussions in this paper reveal the complications we have to deal with to characterize message propagation and evaluate social influence. we believe that the analysis and measurements presented here pave the way for systematic measurements and investigations of osns.
"the commissioner of a professional sports league acts as the chief executive officer of the league [cit] . according to the nfl by-laws, 1 the commissioner has many responsibilities. one of these responsibilities is to \"adopt legal action or such other steps or procedures\" when players, coaches, referees, or anybody else associated with the league is \"guilty of any conduct detrimental either to the league, its member clubs or employees, or to professional football.\" as stated earlier, the nfl and its member clubs dealt with issues of player deviance off-the-field as well as on-field violence, where some has expressed concerns regarding the sport being too violent and aggressive. [cit] s, \"'murder' was the only criminal offense said to bar an athlete from playing in the nfl.\" (p. 1071). [cit] also stated that league executives were afraid that off-field misconduct was getting out of control that these actions would hurt the long-term survivability of the league. as a result, league executives attempted to improve the off-field conduct of its players when in contact with the general public."
"paul tagliabue [cit] . as the policy was written, many issues presented themselves which made it difficult for tagliabue to attempt to deter player behavior. the policy was written so \"punishments were not imposed until after a player had received either a conviction or its equivalent, such as a plea of no contest or a plea to a lesser charge. however, this policy precluded swift action and was thus apparently viewed as insufficient in quelling the rise in incidences of player misconduct and protecting the public image of the nfl\" (mahone jr., 2008, p. 185-186) . [cit] wrote, \" [t] his personal conduct policy was not invoked often and had little effect on the negative publicity the nfl received for the indiscretions of its players\" (p. 1581)."
playing at a non-division i-fbs school as well as a non-bcsaq is insignificant for all groupings except for defensive skill positions. the height of the player at the nfl combine is negative and significant for all groups except for defensive skill players. the weight of the player at the nfl combine is negative and significant for three of the four positional groupings. the forty yard dash is positive and significant for all groupings while the bench press is significant for the offensive player groupings but not for the defensive player groupings. the vertical jump is significant for defensive skill and offensive linemen only and the shuttle run is insignificant in all the positional groupings. the deviance variable has a negative and significant influence for all the positional groupings except for defensive linemen while the interaction term (deviance*goodell) is only significant with the defensive skill position grouping.
"finally, the maximum number of deviance hits from the factiva database search for any one player was 19. we find most variables are significant at the 90 percent confidence interval or higher. all of the position variables are negative and significant compared to the reference group, which is special teams. the number of awards (both by position and all american), participating in the combine, and deviance variables are negative and significant. the age, nond1fbs, nonbcsaq, and goodell variables are positive and significant. finally, the interaction term between deviance and goodell is insignificant. table 3 presents the regression results for the four positional player groupings and the nfl combine data as described in equation 2. all four models explain between 26.5 and 40 percent of the observed variation of the dependent variable. from table 3, the number of non-all american awards is negative and significant for the defensive skill and defensive line players. the all american award is negative and significant for all positional groupings. the age variable is positive and significant for each grouping except for the offensive linemen."
"overall, scholars attempted to examine off-field criminal misconduct by professional athletes. this literature can be divided into three areas. of particular interest in the present research is the final stream which focuses on the impact that off-field behavior has on different stakeholder outcomes. the outcome the present research looks to examine is a player's amateur draft position and the role that misconduct may influence in regards to the player's overall draft position. [cit] found evidence supporting the belief that off-field behavior influences overall draft position in the nfl. due to the limited sample period, however, they did not look at the impact that changes to the league's personal conduct policy has on this outcome. the present research seeks to examine this impact."
"besides, five major mobile twitter clients, including tweetie, ubertwitter, and twitterfon, contribute to 21.1% together. in our data, there are 112 sources via which at least 10, 000 messages were sent."
where x i and y i are the ranks of item i in list x and y respectively. -kendall tau rank correlation coefficient (τ ) [cit] :
"on flickr.com, a popular photo sharing website, users may choose to become a \"fan\" of a photo. [cit] observed that even for popular photos, only 19% of fans are more than 2 hops away from the uploaders. on twitter, however, we find that 37.1% message flows spread more than 3 hops away from the originators, shown as table 2 . the large propagation distance indicates that twitter is a better medium for propagating information. meanwhile, text messages are probably easier to propagate than photos. the longest message flow in our data set consists of 1,555 replies made by 1,512 users. fraction of messages 45.1% 9.4% 8.4% 37.1%"
"there is a large body of literature on information propagation and social influence, most of which looks at traditional social networks. here we limit our discussion to osn related models and measurements."
"this order also holds for all ks we have tested. the first four metrics are close to each other with f being far away, which shows that f is a poor estimator for other social influences. meanwhile, it suggests future social influence studies that analysis on any of the first four metrics (r m, r u, rt u, and rt m ) might apply to the other three but f is likely to be an outlier thus needs to be examined carefully."
"the overlap of two lists ranked by r u is the largest among all 4 metrics being evaluated here, i.e., the set of users with most repliers (not replies) is most stable. its corresponding ρ and τ are the second largest (only smaller than rt u ), implying that r u is a reliable metric. [cit] found that r u is also a good estimator for the number of messages a user posts. hence r u seems to be a promising candidate for measuring social influence."
-twitter website or its api -cellphone short message services (sms) -instant messenger (im) -e-mail many twitter clients have been developed on top of these channels. messages are broadcast by twitter to the sender's followers through the channels they choose. the way twitter distributes messages provides a lot of flexibilities for users to send and receive messages. [cit] and is estimated to have 70m users as of jan. 2010 [cit] .
"examining the university characteristics, the results in table 2 indicate playing at a non-division 1-fbs school and at non-bcsaq increase the player's overall selection, meaning a player is selected in later rounds of the draft. this result intuitively makes sense and is consistent with previous research (e.g., [cit] ) as players who play at the top schools generally face stronger competition over the course of their careers allowing nfl scouts to have a better idea of how the player may project as an nfl player. examining these two variables in table 3, we find insignificant results in all four models except for the defensive skill position. for this variable, we find that playing at a nondivision i-fbs school led to a selection in the higher rounds of the nfl draft."
"1. sort all replies according to their timestamps with the earliest message on the top. 2. walk through the sorted message list from the top to the bottom. for each message i, assuming j is the message which i replies to, i.e., j is i's parent. if there exists a tree which has j, make i a child of j. otherwise, create a new tree with j as its root, and attach i to j. 3. when we reach the end of the message list, output all message flows we have discovered."
"different from the forwarding/broadcasting models in computer networks or peer-to-peer (p2p) networks, the content of m is often modified by users during the propagation over osns, which makes it hard to trace m . in this paper, we model the propagation of a message as a message flow, which is a tree of messages. each node in the tree is a message, which is a reply to its parent node. when a message has multiple originators who receive this message outside of the social network of interest, we may model it as a forest by making each originating message the root of a tree. section 3 presents our analysis of message flows on twitter.com."
"-how to evaluate social influence metrics (section 5. the rest of this paper is organized as follows. section 2 describes how we collect the data set for analysis. section 3 presents our measurement results for message propagation and section 4 shows how the breaking news of michael jackson's death spread through the social network. then section 5 evaluates five social influence metrics. after reviewing prior work in section 6, the paper concludes with section 7."
"-follower influence (f ): the action here is receiving messages (following). the more followers a user has, the more influential this user is. it is also known as degree influence, which corresponds to the size of the audience a user has. -reply influence (r): the action here is replying. the more replies a user receives, the more influential he/she is. this influence can be quantified by the number of replies (r m ) the user receives or the number of users who make these replies (r u ). r u is less biased towards the users who make lots of replies. we evaluate both of them in this paper. -retweet influence (rt ): the action here is retweeting. similarly, the more frequently the user's messages are retweeted by others, the more influential this user is. this can also be quantified by the number of retweets (rt m ) or the number of users who retweet (rt u )."
-truncated: messages having more than 140 chars will be truncated by twitter and have this field set. none of the 58.5m messages are truncated. -in reply to status id: the message id which this message replies to.
"over the past decade, the nfl attempted to deter negative off-field behavior of its players by implementing and modifying a personal conduct policy [cit] . with the adoption and modification of this policy, one might anticipate this policy would have an effect on where individual players are selected in the amateur draft. previous research, examining over 1,200 amateur draft selection under the current personal conduct policy, found that certain incidences led to a player's overall amateur draft selection to be diminished [cit] ."
"the present research also estimates a separate empirical specification looking specifically at players' nfl combine performance. equation 2 presents the model for the nfl combine participants. in equation 2, all previous variables outlined above have the same descriptions. the new combine variables are taken from the nfl combine results website. previous research shown performance at the combine does influence overall draft position at certain positions [cit] . the first variable is the height of the player expressed in inches (height). the second variable is the weight of the player expressed in pounds (weight). 40yd is the player's forty yard dash time, bench is the number of bench press repetitions a, there are several estimation issues that must be examined. the first is in regards to the dependent variable. since the dependent variable is the order of selection in the draft, it could present some estimation issues in its current form. thus, we take the logarithmic transformation of the overall pick variable to use in equations 1 and 2. the second issue is in regards to heteroscedasticity of the equation error term. as a result, we use white's robust standard error correction in both equations consistent with previous research [cit] . the third issue is multicollinearity. to address any potential issue with multicollinearity, variance inflation factors (vifs) are calculated. results from these calculations for both equations do not indicate any potential issues with multicollinearity. the final issue has to do with the sample size for one of the position groupings in equation 2. the qb group did not contain enough observations to estimate a regression model. thus, we only estimate regression models for the other four groups."
"osns are built on top of social relationships or friendships while the definition of friendship varies from one osn to another. the relationship between two users on twitter.com is following. more specifically, if alice is following bob, alice is able to receive all bob's (public) messages. in this case, alice is a follower of bob and bob is a followee of alice."
"other variables within the model also provide interesting results. in table 2, all of the five position groups have a higher draft position in comparison to the reference group (special teams). these results are not surprising given that special teams players, in particular kickers and punters, are generally selected towards the end of the nfl draft. the number of awards the player won during his college career as well as the number of all american honors leads to an improved draft position. these results would intuitively make sense given that these awards are given to the top player of his respective position or position group (e.g., defense). examining the results in table 3, awards and all amer-ican honors affect different position groups differently. the results in table 3 show an increase in the awards variable leads to a defensive player being selected lower in the draft. for offensive players, awards do not influence draft position. there may be a couple of explanations for this result. the first explanation could be the skill set needed to succeed in the nfl is more transferable of the player receiving the awards on defense compared to players who receive the awards on offense. the second explanation could be that the awards is indicative of a high quality player on defense and is used by nfl team scouting departments to help decipher quality compared to offense awards. examining the results for the all american variable in both tables 2 and 3, it is negative and statistically significant across all models indicating the more all american awards the player receives leads to an improved overall draft position. we also find the older the player is, his overall selection is diminished. this positive and significant effect occurs in four of the five models in tables 2 and 3 with the offensive linemen model in table 3 being the only insignificant result."
"thus we collected two datasets. one is the entire data set for analyzing the overall (or average) message propagation patterns and the other, referred as the \"mj\" dataset, is used for breaking news propagation analysis."
"we found 1, 538, 698 message flows in the entire data set and each flow has 10.7 messages on average. now we can analyze how these messages propagate by answering the following questions."
"25% replies were made within 67 seconds and 75% were made within 16.5 minutes. this indicates that the communications on twitter.com are mostly real time. the mean time to reply a message is 2.9 hours while the median is only 3.5 minutes, which indicates that there exists large delays for some replies. for example, we observed that a message got replied 20 months after it was sent."
"furthermore, as previous studies have shown that during information propagation some users are more influential than others [cit], we evaluate several social influences by developing a set of metrics to compare different influences. more specifically, we address the following problems."
"for viscous flows the authors have found it very advantageous to use quadrilateral elements in the highly stretched regions of the mesh. 13 as a result the refinement process becomes more complicated than in the simpler case of purely conforming triangular meshes. 19, 37 for meshes containing quadrilateral elements it is convenient to allow for nonconforming interfaces (i.e. hanging nodes) in the mesh. therefore the triangles are now also refined such that they can have non-conforming interfaces. refinement of both element types is done on a four to one basis with no more than a two to one discrepancy between the size of neighboring elements. furthermore, while other techniques smooth their refined meshes after they are generated, we choose not to do this because it can corrupt the structure of the anisotropic boundary layer mesh."
"the following formulation is based on the approach described in reference. 16 consider the functional of interest l (u) evaluated with the discrete flow-field variables. furthermore consider a coarse mesh t h on which a flow solution u h has been obtained and used to evaluate the functional l h (u h ) on the coarse (i.e. current) mesh. given this flow solution and functional we seek an approach by which we can estimate the functional on a globally refined mesh t h, without computing the flow solution on the globally refined mesh. therefore we expand the fine grid functional in a taylor series about a solution projected from the coarse mesh to the fine mesh denoted by u h h ."
"discontinuous galerkin (dg) methods are capable of generating high-order accurate solutions to the euler and navier-stokes equations. however, this is only attained if the solution is smooth. unfortunately for aerodynamic applications solutions are rarely smooth. non-smooth solutions can result from the expected discontinuities such as shock waves and contact discontinuities as well as from additional sources, which are not covered as thoroughly in the literature. for example turbulence models used with the rans equations often have non-smooth features associated with them. in our recent work we have noticed that the turbulence model of spalart and allmaras 22, 23 has a discontinuity in its working variable at the interface between turbulent/non-turbulent regions of the flow. this discontinuity results in oscillations in the high-order solution, which can easily cause solver failure. this discontinuity is an example of a somewhat unexpected mode of solver failure, where a flow that one would initially assume is smooth in fact turns out to be non-smooth."
"the lift vs. ndof is depicted in figure 16 does not converge to a fixed value as uniform p-enrichment is applied. figure 16 (b) depicts the iterative converge for this case using the artificial diffusion. one can immediately see that computing shocks with this method can become quite expensive especially when compared with the adjoint hp-adaptation convergence history in figure 15 (b). figure 17 (a) depicts the lift vs. the computational time, which clearly shows that hp-adaptation generates more accurate lift values at a reduced cost compared to higher-order shock capturing with artificial diffusion. additionally for each order of accuracy the artificial diffusion parameters had to be adjusted sometimes by a factor of 2 or more to get the solution to converge. while the hp-adaptive approach is not the most elegant shock capturing method for dg discretizations it has some significant advantages; it is easy to use (i.e. requires only 1 parameter to tune, which is the value of smoothness indicator that selects h-refinement vs. p-enrichment), gives robust and fast iterative convergence, and with each refinement the functional improves and eventually achieves a grid converged value."
to eliminate the term involving the solution on the fine mesh we appeal to the constraint equation. for steady state solutions the temporal derivative is zero and the residual defined by eq. (7) can also be expanded about the projected solution
"in this work we propose to use these techniques to adaptively enrich the discretization order and refine the mesh for dg discretizations of the navier-stokes equations on mixed-element meshes, i.e. meshes containing triangles and quadrilaterals. a discrete adjoint formulation is used to obtain the error estimates in the functional of interest. the formulation is based on a discrete adjoint approach using a fully dual (adjoint) consistent discretization. references 18, 21 have shown numerically that using dual-inconsistent discretizations can lead to sub-optimal convergence of the primal solution, while using dual consistent or asymptotically dual consistent discretizations leads to optimal convergence of the primal problem. herein we use a modified symmetric interior penalty method (sip) for the navier-stokes equations."
"while for non-adaptive techniques the coupling of discretization order and number of degrees of freedom poses a rather serious problem for limiting the solution as shown numerically in reference, 13 it is actually an advantage in the context of an adaptive method. this property of dg discretizations allows for a flexible procedure by which resolution can be added to a problem. in this work we examine the use of hp-adaptation for two purposes: the first of which is to place degrees of freedom within the domain as optimally as possible, the second of which is to improve solver robustness by not attempting to use high-order polynomial approximations in regions of the mesh where it is not appropriate (e.g. near discontinuities). this results in a minimum amount of adhoc parameters to tune, in fact there is only one which is required by the proposed hp-adaptive method. further the constant presence of discontinuous solutions in practical problems of interest motivates one to examine adaptation techniques that take this into account as robustly as possible."
"in contrast to h-refinement, p-enrichment refines the element in question by maintaining the current element size and connectivity. the p-enrichment procedure is much simpler than the h-refinement procedure and consists of simply increasing the discretization order from p to p + 1 on the element flagged for refinement. it is implemented without regard to element type as depicted in figures 3(a) and 3(b), and a jump of no more than one order is permitted between elements."
"the presence of hanging nodes complicates the inter-element surface integral compared to a conforming mesh. in our approach an edge connected by any two nodes is defined as a unique edge in the mesh. thus triangles with a hanging node actually have four edges (similarity quadrilaterals can have up to six edges). the surface integral between non-conforming elements where one has a hanging node is accomplished by computing each edge integral separately and then pushing their contributions back into the elements on each side of the edges. while for the element with the hanging node the two edges that surround the hanging node have unique identification numbers in physical space, they have the same local edge number on the non-conforming element in the transformed space. this is to say that the edges surrounding a hanging node make up equal portions of a single edge in the transformed space. furthermore, while for conforming edges the edge integral is the done over the full edge in transformed space, the edge integral for the non-conforming element is done over a fraction (usually 1/2) of the edge in transformed space. this results in a deformation and translation of the quadrature point locations on each side of an edge. the volume integrals are unaffected by the presence of hanging nodes."
the refinement pattern for triangles is depicted in figure 1 . the triangle is refined using mid-point subdivision where a node is inserted at the mid-point of each edge on the triangle. this results in four children (4:
the adjoint solution should be expected to cost as much as the flow solution and thus we do not want to compute the fine grid adjoint directly. therefore the coarse adjoint solution is obtained via
"where h c (·, ·, n) is the convective numerical flux and h v (·, ·, ·, ·, ·, ·, n) is the viscous numerical flux on the interior faces γ i . the numerical fluxes h b c (·, n) and h b v (·, ·, ·, n) denote boundary numerical fluxes (which are different from the interior numerical fluxes) on a boundary edge γ b . reference 25 has shown that using the riemann solver on the boundary results in a dual inconsistent discretization. for a dually consistent discretization, the boundary numerical flux is taken as the native flux normal to the boundary evaluated at the boundary condition state u b h (u + h ). the interior convective numerical fluxes are chosen to be riemann solvers, and current implementations include the flux difference splitting schemes of rusanov, 26 roe, 27 hll 28 and hllc. [cit] the numerical flux for the viscous term is obtained via a modified version of the symmetric interior penalty method (sip) presented in references, [cit] 32 which seeks to penalize the solution for being discontinuous at the element interfaces. in fact the form of the sip method used in this work is a hybridization of the one discussed in reference 12 and reference. 32 it is now convenient to introduce the following average and jump operators for both vector and scalar quantities. it is understood from the notation that for the jump of a vector we mean a vector in the physical space and not in the number of equations. the average operator is defined by"
"future work will focus on investigating techniques for enhancing robustness that do not rely on hp-adaptation but could and probably should be used in conjunction with hp-adaptation. the techniques under investigation include filtering, 40 a new artificial viscosity method first developed in the context of the spectral difference method by references, 41, 42 and a modification of the artificial viscosity method presented in. 39 in particular future work will focus on combining the technique of reference 39 with the hp-adaptation as well as removing some of the tunable parameters. robustness enhancement along with hp-adaptation will be required for efficient and reliable computations of turbulent flows and future work will also focus on computing turbulent flows using hp-adaptation."
in this work the adaptation procedure is driven by estimating the error in an output functional of interest. this is known as goal-oriented or adjoint-based adaptation. the objective of this procedure is to adapt the mesh based on an element's contribution to the error in the output functional. the predicted error may also be used to give a correction to the functional value.
where the flow residual on the fine mesh r h (u h h ) are non-zero since the coarse mesh flow solution projected to the fine mesh does not satisfy the discrete equations on the fine mesh. next we define the fine adjoint variable λ h as the variable satisfying
in an attempt to enforce a no more than 4:1 refinement rule any element with all but one of its edges flagged for refinement will have its final edge refined. if an an element has an edge marked for refinement and that edge is connected to a hanging node then the element is flagged for a full refinement.
"high-order discontinuous galerkin (dg) methods are receiving a great deal of attention for the computation of convection-dominated problems. [cit] recently there has been work done to extend these methods to convectiondiffusion problems such as the navier-stokes equations. [cit] in addition a posteriori error estimation for functional outputs is becoming a mature technique for estimating the contribution of discretization error to simulation outputs. [cit] these a posteriori error estimates are based on solutions to the so called adjoint (dual) problem. these error estimates provide a method to guide adaptive refinement techniques to optimally place degrees of freedom within a problem. recently these techniques have been used to perform adaptive mesh refinement (i.e. h-refinement only) of purely triangular meshes in the context of dg discretizations of the rans equations. 17, 18 reference 19 has used these techniques for the hp-adaptation of high speed shocked flows using a dg discretization of the compressible euler equations."
"the above expressions simply state that we compute the jump of density, velocities, and pressure on each cell. we then take the maximum value of these jumps to use as the smoothness indicator for the cell. the choice between whether to adapt an element with h-refinement or p-enrichment is made by"
where l h (u h h ) is the fine mesh functional evaluated with the coarse mesh solution projected to the fine mesh. the vector
"respectively. note that the jump in a scalar quantity is a vector and the jump in a vector quantity is a scalar. also note that a vector in fields is treated as a \"scalar\" by the jump operator and thus the jump of u h is a matrix with the number of rows equal to the number of equations and the number of columns equal to the number of spatial dimensions in the domain. using this notation the sip numerical fluxes are given as"
"a unique property of dg discretizations is that the order of accuracy and the number of degrees of freedom (dofs) are coupled. this is in contrast to the traditional finite-volume or finite-difference techniques which instead rely on extended stencils to increase the discretization order. this property is what makes developing limiters for dg more difficult than for traditional cfd methods, and poses a rather serious drawback for computing discontinuous solutions with dg. while the addition of artificial viscosity can be used to treat non-smooth solutions, no theoretical minimum bound on the required amount of artificial viscosity has been given, thus complicating the use of this approach. furthermore initial investigations into using an artificial diffusion method for shock capturing indicate that the functional accuracy is not necessarily improved when the discretization order is increased. however, artificial diffusion may have a place as a robustness enhancement method in combination with some type of adaptation."
subdivided element. the quadrilaterals are refined in an analogous manner as depicted in figure 2 with the exception that an additional node is placed at the center of the refined quadrilateral. the present method allows for the presence of hanging nodes for both triangular and quadrilateral elements as shown in figures 1 and 2 .
"while a large amount of work has been conducted to create efficient solvers for discontinuous galerkin(dg) discretizations of the euler and navier-stokes equations, 3, 12, 13, 24 much less attention has been paid to solver robustness. we view the proposed approach as both a method by which to place degrees of freedom optimally within the domain and as a method to improve the robustness of the dg solver via careful attention to how the degrees of freedom are added to the mesh. in regions where the solution is smooth p-enrichment is utilized while in regions where the solution is not smooth h-refinement is employed."
"discontinuous galerkin methods couple the order of accuracy with the number of degrees of freedom. this gives dg methods additional flexibility with regard to the placement of the degrees of freedom by an adaptation algorithm. in particular dg methods have two paths by which to increase resolution in a given problem: h-refinement and penrichment. reference 17, 18 has developed an unsteady mesh adaptation procedure for turbulent flows in the context of high-order dg discretizations. however the mesh adaptations are performed at a fixed order of accuracy and thus only exploit one method of adding resolution to the problem. references 19, 37 have developed an hp-adaptive approach for dg discretizations of the compressible euler equations on purely triangular meshes and demonstrated its effectiveness for computing both purely smooth flows and flows with discontinuities. herein we propose to extend the work of reference 19, 37 to viscous flows on mixed-element meshes, using a similar combined h-refinement and p-enrichment approach. in what follows we describe each method of adaptation in isolation and then discuss how they are combined to give the overall hp-adaptive approach."
in this work the curved boundaries are mapped supper parametrically to order p max +1 where p max is the maximum order in the grid. the set of discrete equations is solved in modal space and the integrals are evaluated using gaussian quadrature rules . [cit] these quadrature rules require projection of the solution from the modal space to the quadrature points. to preserve the p + 1 accuracy of the dg scheme the volume integrals are computed using a rule that integrates a polynomial of degree 2p exactly. to the same end surface integrations are carried out with a rule that integrates a polynomial of degree 2p + 1 exactly. 36
it is well known that newton's method will diverge if the initial guess is too far from the final solution. thus the flux jacobian matrix is augmented with a damping term to increase robustness. the damped newton iteration is given as:
"while no explicit limiter has been used to generate these results one can view the hp-adaptive approach as a form of limitation. tradition slope limiters effectively reduce the order of accuracy locally. the idea behind slope limitation is to assume that high-order is appropriate everywhere in the grid and then to remedy those areas where high-order is not appropriate, which is a top down approach. the hp-adaptation can be viewed as a bottom up approach to limitation because it starts with low-order and moves towards high-order where appropriate. in the context of dg discretizations the bottom up approach has an advantage because it takes the coupling between order of accuracy and resolution into account naturally."
the proposed hp-adaptive method has been evaluated using three test cases. the first two test cases are the laminar viscous flow over a naca0012 and a two element airfoil. the first test case is presented to compare hp-adaptation with h-refinement as well as with both uniform h-refinement and uniform p-enrichment. the two element airfoil case is a practical application of the hp-adaptive method to a more complicated geometry. the third and final test case is the transonic flow over a naca0012 airfoil. this case is presented to demonstrate the robust shock capturing ability of the hp-adaptive method. the results of the hp-adaptation are compared with an artificial viscosity method.
"all results have been computed using the riemann solver of roe 27 on the cell interfaces. the equations are integrated implicitly using newton's method with an exact linearization. the linear solver employed within each newton iteration is the multgrid preconditioned gmres (mgpc-gmres) solver of references. 12, 13 all test cases are steady state solutions and the flow and adjoint equations have been converged such that their residuals have been reduced by 13 orders of magnitude at each stage of the adaptive process. in some sense this represents the worst case scenario for timing adaptive methods because one would probably only partially converge the intermediates steps before moving on to the next adaptive cycle."
"though hp-adaptation is designed to place degrees of freedom optimally for a given objective functional, it can also be viewed as a technique to enhance the robustness of the dg solver. in essence it seeks to design the mesh based on the solution, which for cases of under-resolved phenomena such as those encountered in reference 13 should result in a mesh of sufficient resolution such that high-order polynomials can be used throughout. for flow features that will most likely remain under-resolved for the entire simulation (e.g. shocks and contact discontinuities) the hpadaptive scheme is capable of adding degrees of freedom while maintaining low p-order in such a way as to avoid gibbs phenomena, providing a natural way for the present dg solver to begin handing non-smooth solutions robustly."
the fine level residual is constrained to be zero which allows one to re-arrange eq. (15) to solve for the quantity involving the unknown fine solution as
"while p-enrichment induces no additional geometrical complexity, one does need to address how many quadrature points must be used to integrate the fluxes along the edges. in previous work we have integrated edge fluxes to 2p + 1 accuracy. when using a grid with variable discretization order it is necessary to use an integration rule that integrates to 2(max(p +, p − )) + 1 accuracy where p + and p − denote the element order on each side of an edge. the solutions on either side of the edge are evaluated using the number of modes from each element sharing the edge. the volume integrals remain unaffected by the variable discretization order throughout the mesh."
"where the cfl is the courant-friedrichs-lewy number and a is the sound speed. due to the block-sparse nature and size of the matrix an iterative method is used to solve the linear system arising from the newton's method. the iterative solver takes the form of either a linear hp-multigrid method or a multgrid preconditioned gmres(mgpc-gmres) method, 12, 13 both of which use an under relaxed line-implicit jacobi smoother. see reference 12, 13 for details of the solver and a review of its performance."
"where the r h (w h, u h, ∇u h ) is the discrete spatial residual. the discrete spatial residual is just the collection of the spatial derivative and source terms in eq. (5) . the spatial residual is integrated by parts resulting in the following weak form"
"the choice of penalty parameter can be rather adhoc as it is only required to be \"large enough\" to stabilize the scheme. however, shahbazi in reference 10 derived an explicit expression for the penalty parameter for poisson's equation. a modified version of this expression given in reference 12 has been successfully implemented for this work. the value of the penalty parameter on an interface is"
if aa believes that the connection from feeling bi to preparation of bi has strength ω2i and aa believes that human has feeling bi with level v1i and aa believes that the human's preparation of bi has level v2i and the learning rate from feeling bi to preparation of bi is η and the extinction rate from feeling bi to preparation of bi is ζ then after ∆t aa will believe that the connection from feeling bi to preparation of bi will have
"without loss of generality, assume that the audit monitoring process converged to a misbehaving link (n ω − 1, n ω ), where n ω is the misbehaving node. the source divides p sd, into two paths such that packets are routed through either n ω or n ω +1, but not both, and attempts to re-identify the misbehaving link. instead of performing the entire audit monitoring process in each of the paths, the source concentrates on the nodes around nω−1, n ω ."
"the desire considered in the example scenario is assumed to be generated by sensing an unbalance in a body state b, according to the principle that organisms aim at maintaining homeostasis of their internal milieu. the first dynamic property addresses how body states are sensed. the following properties specify how the ambient agent observes and generates beliefs about the human's sensing and sensory representation process."
"in absence of infrastructure, mobile ad hoc networks need more cooperation and reputation for secured and reliable communications. in this paper, the intermediate relay nodes are responsible to relay the packets from the source node to the destination node and willing to carry traffic other than their own. when ad hoc networks are deployed in hostile environments, the protocol-compliant behavior cannot be assumed. unattended devices could not cooperate with message transmission and may drop more packets during communication in order to degrade the performance of network."
"in all traces γ and all time points t the level of a desire of the human if the desire of a human exceeds the threshold, then there exists a later time point at which this is not the case. for the simulation traces generated using the ambient agent model, this property is satisfied for all traces (with a threshold value of 0.7). the overall behavior as expressed in p1 can be accomplished by intervention by giving one or more suggestions at the right moment (expressed in p2) in combination with the human responding to these suggestions (expressed in p3)."
"trust and reputation are two important factors that make the network secure and safe in mobile ad hoc network. here the selfish nodes are encouraged to forward packets and as means of forwarding packets selfish nodes are validated for its misbehaving activities. a misbehaving node poses a malicious behavior cannot be allowed to forward the packets as it expose the system's vulnerability and leads different types of attacks in manet. to address this issue, a new model has been proposed so as to establish a reputed cooperative route as well as considering the security factors of the nodes participating in this communication. the malicious nodes are removed from the given path based on the inputs of an efficient audit monitoring process. this audit monitoring process invokes the trust and the reputed metrics for verifying individual node reputation and successful evaluation. in this audit monitoring process, reputed route discovery is established by removing all malicious nodes for secured and reputed cooperative communication"
"the enhanced additive increase/multiplicative decrease (eaimd) principle is adapted in order to rapidly isolate a misbehaving node from routing paths. due to the multiplicative factor α, the reputation of a misbehaving node speedily declines with repeated misbehavior. the difference between the reputation and decrease and increase mechanism is the eamid prevents the selective misbehaving node from oscillating between periods of misbehavior and good behavior for the purpose of dropping the packets while remaining in active paths."
"in this proposed work, the trust based model that effectively isolates both continuous and selective packet droppers. the proposed model integrates reputation management, identification of misbehaving nodes, and trustworthy route discovery based on behavioral audits. when this proposed model is compared with previous methods, this system evaluates the node behavior on the basis of per-packet. moreover, the proposed system detects continuous and selective dropping attacks, even if end-to-end traffic is encrypted. simulation results prove that this system successfully avoids misbehaving nodes, even when a large portion of the network refuses to forward the packets."
"finally, neighbour monitoring typically records coarse metrics of misbehaviours such as packet counts and that are not sufficient to detect highly sophisticated selective dropping attacks. motivated by the inadequacy and inefficiency of transmission overhearing, the amdmm is developed that earns overhead on a per-flow basis with less energy expenses. moreover, it allows the full customization of the misbehavior criteria for detecting selective dropping strategies."
"in the formula ϑ is a rate of developing unbalance over time (for example, getting hungry), and ρ a rate of compensating for this unbalance. note that the specific formula used here to adapt the level of b is meant as just an example. as no assumptions on body state b are made, this formula is meant as a stand-in for more realistic formulae that could be used for specific body states b."
"in this paper, first in section 2 the domain model for the dynamics of desires, preparations and feelings is described. section 3 presents the ambient agent model which integrates the domain model. section 4 presents some simulation results of the integrated ambient agent model. in section 5 verification of the integrated model is addressed. section 6 is a discussion."
"the proposed system provides identifies the misbehaviour nodes and eliminating the misbehaviors from the network. this system consists the three major factors such reputation, route discovery, and an audit monitoring process. these factors interact with functions of misbehavior, discovery of trustworthy routes as well as the evaluation of the reputation among the mobile nodes. this integrated system is responsible for managing reputation among the nodes and it is purely based on the recommendations of the audit monitoring process. in this proposed approach, each node has its own view of the other nodes. the first and second hand information are taken into the consideration for the reputation metrics. the first-hand information represents the direct observations of the nodes and the second hand information denotes the opinion of the other nodes. these two observations are used to identify the reputation and misbehaving nodes in the entire network."
"we have adopted the eamid principle in order to rapidly isolate a misbehaving node from routing paths. due to the multiplicative factor α, the reputation of a misbehaving node rapidly declines with repeated misbehavior. the difference between the reputation process prevents a selectively misbehaving node from oscillating between periods of misbehavior and good behavior for the purpose of dropping packets while remaining in active paths."
"the proposed amdmm will provide the paths consist of highly reputed nodes, subject to a desired path. when paths contain misbehaving nodes, the proposed monitoring system effectively audits and enhances the nodes reputation for the proper communication. the identification strategy obtains through knowledge of nodes' reputation. also, this monitoring method performs the reputation using storage-efficient membership structures. related work"
"the desire assessment is used to generate an intervention intention, whenever needed. this intention persists until the point in time at which the intervention has to be performed. table 2 shows the criteria used in the ambient agent's decision process, where the human is assumed to consider an option if the level of the associated preparation state is predicted above a certain threshold, which in the example scenario is set to 0.1, whereas the different options that are available are characterized as good or bad based on the values of the effectivity rates of those options higher or lower than 0.5. the shaded cases in table 1 indicate the cases for which intervention is intended: a bad option is considered by the human, or a good option is not considered."
"based on the model for the human and the agent model described in the previous sections, a number of simulations have been performed within the leadsto simulation environment [cit] . the model was tested in a small scenario, involving an ambient agent and a human, indicated by aa and human, respectively. the example scenario taken here considers a person who is getting hungry which generates a desire to eat for which a number of options is available at that time. as the level of desire increases this makes the person more tempted to eat, and in particular to choose the option that is associated to the best feeling. as the domain model is integrated within the ambient agent, it can predict the human's desire level well in advance, and assesses the extent to which the human will consider the different options that are available to fulfill this desire."
"based on the criteria given in table 1 above, if the ambient agent predicts that the human will consider those options that are not effective for fulfilling the desire, then it will suggest not to choose them. similarly, if the assessment process of the ambient agent determines any options that are quite effective for the human to choose, but the human will not consider those, then it will suggest the human to choose them. the scenario starts with some initial values of the human's desire and feeling levels, and then keeps on updating this, using the integrated model explained in section 3. an example simulation trace (under fixed parameter settings) is illustrated in figure 2 and 3 (here the time delays within the temporal leadsto relations were taken 1 time unit). in the appendix an additional case is shown. in these figures, where time is on the horizontal axis, the upper part shows the time periods, in which the binary logical state properties hold (indicated by the dark lines); for example, has_state(aa, assessment(has_state(human, high_desire(b), 204))). below this part, quantitative information is provided about the human's actual desire, preparation states, connection strength levels, levels of different body states and the ambient agent aa's prediction of these. values for these levels for the different time periods are shown by the dark lines. note that the scale on the vertical axis differs over the different graphs, and only a selection of the relevant state properties is shown."
"moreover, selfish users do not configure their devices properly and to refuse the packets to forwarding order to save energy. existing solutions for identifying selfish or misbehaving nodes either use some form of evaluation of node behaviour or provided the incentives to the nodes to stimulate of selfish nodes to cooperate. the per-packet behaviour evaluation technique addresses the issue of transmission overhearing and per-packet acknowledgements."
"if aa believes that the human has a sensory representation for body state bi with level v, then it will believe that the human has feeling bi with level v."
"in order to investigate whether the ambient agent indeed acts according to what is expected, some logical properties (requirements) have been identified, formalised, and verified against the simulation traces of the model. in this section, first the language used to express such properties is briefly introduced, followed by the specification of the actual properties, and the result of their verification. using a formal specification for desired properties of the ambient agent enables automatic verification of them against simulation traces. this automated verification is performed using the logical language ttl and its software environment (cf. [cit] based on these statements, dynamic properties are formulated in a sorted first-order predicate logic, using quantifiers over time and traces and the usual first-order logical connectives such as ¬, ∧, ∨, ⇒, ∀, ∃. for more details, see [cit] ). an overall property to be satisfied by the agent is that if the level of a desire of the human exceeds a particular threshold, it should eventually become below the threshold."
"temporal relationships alp9, alp10 and alp11 below describe the ambient agent's reasoning about how preparations of body states b i and affect body states b and b i . the idea is that the actions performed by body states b i are different means to satisfy the desire related to b, by having an impact on the body state that decreases the activation level v (indicating the extent of unbalance) of body state b. in addition, when performed, each of them involves an effect on a specific body state b i which can be interpreted as a basis for a form of satisfaction felt for the specific way in which b was satisfied. so, on the one hand a specific action performance involving b i has an effect on body state b, by decreasing the level of unbalance entailed by b, and on the other hand it has an effect on the body state b i by increasing the level of satisfaction entailed by b i . this level of satisfaction may be proportional to the extent to which the unbalance is reduced, but may also be disproportional."
variants of this property have been used to incorporate interventions which affect the preparations of some b i : they are assumed to become 0 (suggestion not to do) or 1 (suggestion to do); for example:
"in all traces γ, if the ambient agent at time point t1 predicts that at time point t2 the human will have a desire exceeding the threshold th, then the ambient agent will give a suggestion to the human. this property holds for all traces (when a threshold of 0.7 is chosen)."
"in this proposed system, we have successfully simulated novel and a practical scheme amdmm, a comprehensive misbehavior detection and mitigation system which integrates three critical functions: reputation management, route discovery, and identification of misbehaving nodes via this audit monitoring system. we showed that amdmm recovers the network operation even if a large fraction of nodes is misbehaving at a significantly lower communication cost. moreover amdmm can detect selective dropping attacks over end-toend encrypted traffic streams. the attack resistant reputation monitoring and efficient trust based auditing method is planning to use in this future research work."
"this reputation system identifies and isolates the misbehaving nodes from the network to ensuring the reputed packet transmission via trustworthy nodes in manet. nodes identify the misbehaving nodes with low reputation values are excluded from routing path. also, this system is responsible for computing and managing the reputation among the nodes. the approach is adapted in this system which ensures that the each node maintains its own view of the reputation of other nodes. such implementation relieves the communication overhead from the transmission of information to a centralized location as well as translates to the distributed ad hoc networks. moreover, it allows nodes to hold the individual reputation metrics for their peer nodes depending on their direct and indirect interactions."
"next it is shown how the ambient agent estimates the preparations that are triggered. it is assumed that within the person activation of a desire, together with a feeling, induces preparations for a number of action options: those actions the person considers relevant options to satisfy the desire, for example based on earlier experiences. dynamic property alp6 describes such responses to an activated desire in the form of the preparation for specific actions. it combines the activation levels v and v i of two states (desire and feeling) through connection strengths ω 1i and ω 2i respectively. this specifies part of the recursive as-if loop between feeling and body state. this dynamic property uses a combination model based on a"
"this proposed model is simulated using ns2 simulation tool. in this evaluation the following 10, 20, 30, and 40 set of mobile nodes are randomly distributed in an environment. each node has three locations in the physical environment, and randomly travels among these locations with a uniform speed. we used two way ground propagation model. the antenna type is omni antenna model and the transmission range is 250m.also, the used traffic type is cbr and the aodv protocol is used for packet forwarding. in each session, the source routed 10,000 packets to the destination via the established path. to isolate the performance degradation due to malicious dropping, lower, there may be the contention and retransmissions due to collisions were abstracted. the performance evaluation for reputed packet delivery between existing dynamic source routing (dsr) protocol and proposed amdmm system is presented in figure 1."
"in this proposed work, the amdmm effectively identifies misbehaving nodes and it is accelerated based on the input received from the reputation module. the detailed descriptions of these systems are given below."
"for the example scenario this dynamic property is used for b to estimate the person's sensing of the body state b from which the desire originates (e.g., a state of being hungry), and the body states b i involved in feeling satisfaction with specific ways in which the desire is fulfilled. how sensory representations are generated is addressed in dynamic property alp4."
"aa believes that the human senses body state, then it will generate a belief that after ∆t the human will have a sensory representation for this."
"second-hand information is used only if lack of prior interaction in first-hand information. the nonavailability of the first-hand information is identified by the monitoring through t0 epochs (time or duration). the implementation of an simple aging mechanism that prevents sleeper attacks wherein a node behaves for an initial period of time to attain the high reputation, before reveal misbehaviors. a node n i averages all the second-hand information reported by other nodes within the last t0 epochs, in the absence of first-hand information. let ii(t) denote the set of nodes that provided second-hand information to n i within the last t0 epochs. the reputation value is computed as"
"the source will converge to a set v j (set of misbehaving nodes) of neighboring misbehaving nodes with set v j containing at most one honest node. to identify the misbehaving nodes, all nodes in v j must be excluded in turn from p sd according to the path division/expansion process. if v cannot be further reduced, the audit module reports all nodes in v as misbehaving and their reputation is updated accordingly by the reputation process."
"if aa believes that the human's preparation state for body state bi with level v occurred, then it will believe that the human's body state bi is modified with level v."
"in all traces γ, if the ambient agent gives a suggestion to the human at time point t to either avoid a body state b (don't eat for this case) or accomplish a body state b (i.e., eat), then the human will follow this suggestion, indicated by a preparation state for b being 0 for the case of an avoidance suggestion, or a 1 in case of an accomplish suggestion for the body state b. this last property is satisfied for all traces as well."
"this system proposes the trust and reputation based communication in mobile ad hoc networks. for this, trust and reputation are introduced in this paper to transmit the message from the source node to the destination node in manet. if the node forwards the maximum number of packets that has received, the nodes' reputation is increased, whereas when a node drops maximum number of packets, the nodes' reputation is decreased. this kind of non-cooperation and trustless based uncertainties with mobile nodes will leads to network degradation and decreases the entire network performance. so, the trust and reputation system stimulates the nodes to cooperate with each other and also to mitigate the misbehaving nodes without incurring any performance cost or creating any additional delay's during this reputed communication."
"in this paper, we present and experimentally evaluate a set of ingredients that, combined, allow applying robust fully automated importance splitting on general models, including non-markovian ones, for res in smc. the ingredients are (i) a compositional method to automatically construct an importance function from the formal model and a temporal logic property query (section 3), (ii) three existing splitting techniques that determine the details of how to manage the partial runs and calculate a correct estimate (section 4), and (iii) two algorithms-one existing, one new-to derive thresholds and factors (section 5). we consider both transient and steady-state properties. we use the splitting methods restart (section 4.1), fixed effort [cit] (section 4.2), and fixed success [cit] (section 4.3). while restart was proposed for steady-state analysis and later extended to transient properties [cit], the latter two are geared for estimating probabilities of transient events 1 . the two algorithms for threshold selection are a sequential monte carlo (seq) approach [cit] with a single fixed splitting factor specified by the user for all thresholds (section 5.1) and a new \"expected success\" (exp) technique (section 5.2). exp selects thresholds and an individual splitting factor for each threshold, removing the need for the user to manually select a global splitting factor. we implemented all techniques in the fig tool [cit] and the modes simulator [cit] of the modest toolset [cit] . the techniques can be freely combined, and work for all the formalisms supported by the two tools-including ctmc, input-output stochastic automata (iosa [cit] ), and stochastic timed automata (sta [cit] ). we finally perform an extensive experimental evaluation (section 7) of the various combinations on several case studies, including three new and challenging examples for steady-state measures."
"where t a and t b are the entrance times into a and b, respectively. the steady-state probability of the rare event a (also denoted long run probability) is"
"we also experimented with expected numbers of runs that move up of 2 and 4, which implies that the number of partial runs grows-by those factors on average-as simulations reach higher values of f l . in practice this always led to dismal performance or timeouts, most often due to too many splits in our experiments. this indicates a simulation overhead, an \"unbalanced growth\" to put it in garvel's terms, and thus we only consider the original single (1) expected successful run for es in the sequel. we further note that the property is not an input to algorithm 4; the algorithm can thus be employed for both transient and steady-state analysis as-is."
"we implemented the compositional importance function generation of section 3, the splitting methods described in section 4, and the threshold calculation methods of section 5 in the modes simulator [cit] of the modest toolset [cit] and the fig tool [cit] for input-output stochastic automata."
"in any case the most relevant characteristic of the compositional method described is that, aside from the choice of operator (for which + as default has worked well for most models studied), the procedure requires no user input to compute the global importance function f i . moreover, it takes into account both the structure of the target formula and the structure of the state space of each model component. memory usage is determined by the number of discrete local states (required to be finite) over all components. typically, component state spaces are small even when the composed state space explodes."
"we consider several queueing systems since these are frequently used benchmarks for res [24, 26-28, 45, 50, 62] . the ctmc could easily be modified to use general distributions and our techniques and tools would still work the same."
"for transient properties, we use modes to evaluate the performance of all relevant combinations of the implemented res methods on ctmc queueing models, network protocols modelled as pta, and a more complex file server setting modelled as sta."
"our experiments show that steady-state properties on the considered models are truly challenging for automated importance splitting. in most cases, one of the method combinations is still noticeably better than plain smc. however, unlike for transient properties, there is no combination that performs consistently best. notwithstanding, the combination of restart with expected success was always competitive, and it remains reasonable to keep it as a safe-if not always our results show that we have found a fully automated rare event simulation approach based on importance splitting that performs very well for transient properties: automatic compositional importance functions together with restart and the expected success method. it pushes automated importance splitting for general models into the realm of very rare events with probabilities down to the order of 10 −23 . for steady-state properties, however, the picture is not so clear: different methods and method parameterisations work best for different model instances. still, the fully automated combination of restart with expected success shows competitive performance and thus appears as a reasonable default. further research will be necessary to find out what the key differences are in the behaviour of transient and steady-state analysis to cause such distinct results."
"in general, the quality of a choice of parameters depends heavily on the structure of the model at hand; making good choices requires an expert in the system domain, who should be experienced with the modelling formalism as well as the selected res method [cit] . in this work we study ways to alleviate such a requirement, proposing combinations of techniques that enable an effective application of importance splitting on a general set of systems. we highlight that to align res with the spirit of (statistical) model checking as a \"push-button\" approach, it is necessary to devise an automatic selection of parameters that perform well in most situations. furthermore, the methods automating such selection must not negate the memory usage advantages of smc with respect to traditional model checking. these constitute the main challenges we address here."
"all importance splitting methods provide unbiased estimators for the (transient or steadystate) property under study. the quality of the importance function, i.e. how well it resembles the proximity of the states to the rare event, determines the variance of the estimator. the goal is to obtain an estimator with lower variance than with the use of standard monte carlo simulation. this means that the performance, but not the correctness, of importance splitting hinges on the quality of the importance function f i ."
"we provide an overview of the performance results for all model instances in table 2 . for the database and pipeline models, we report the averages of three runs of each experiment again. columnp lists the average of all (up to 18) individual estimates for each instance, which were again consistent. column t/o recalls the timeout used for the respective models. the remaining columns report the half-width of the confidence interval, in percent ofp, obtained at the timeout. dashes mark cases where the rare event was not encountered even once."
.n−1 . this is due to the underlying negative binomial distribution; see [cit] for details. the method thus requires f e (l) 2 for all levels l.
"the compositional importance function generation was first developed for and implemented in the fig simulator [cit] . for input/output stochastic automata (iosa [cit] ), originally designed in an extension of the prism language [cit] to consider arbitrary (continuous) probability distributions as in stochastic automata [cit] ."
"a breadth-first search (bfs) that starts from a and uses the reverted edges of the graph can compute these importance values. the idea can be refined e.g. to consider the (potentially) probabilistic nature of the transitions: add weights to the edges of the graph, to reflect quantitatively 8 the likelihood of taking the transitions they represent, and employ dijkstra or a* instead of bfs. however, even in its most simple form, the method can perform well on general models because it is embedded in a framework for importance splitting implementation-the quality of the importance function is a cornerstone, but other mechanisms like an adaptive threshold/effort selection can alleviate some poor importance approximations. furthermore, the reason why this method is particularly amenable to the analysis of general systems models is its compositionality, on which we elaborate next."
"openclosed: our second ctmc has two parallel queues [cit], both initially empty: an open queue q o, receiving packets at rate 1 from an external source, and a closed queue q c that receives internal packets. one server processes packets from both queues: packets from q o are processed at rate 4 while q c is empty; otherwise, packets from q c are served at rate 2. the latter packets are put back into another internal queue, which are independently moved back to q c at rate 1"
"we provide an overview of the performance results for all model instances in table 1 . we report the averages of three runs of each experiment to account for fluctuations due to the inherent randomisation in the simulation and especially in the threshold selection algorithms. column p lists the average of all (up to 45) individual estimates for each instance. all estimates were consistent, including smc in the few cases where it did not time out. to verify that the compositional importance function construction does not lead to high memory usage, we list the total number of states that it needs to store in column n i . these numbers are consistently low; even on the two pta cases, they are far below the total number of states of the composed state spaces. the remaining columns report the total time, in seconds, that each approach took to compute the importance function, perform threshold selection, and use the respective splitting method to estimate the probability of the transient rare event. dashes mark timeouts."
"for the code presented in algorithm 3 for transient analysis, seq may get stuck in the same way as fixed success. we encountered this with our wlan case study of section 7.1. our tool thus restarts seq after a 30 s timeout; on the wlan model, it then always succeeded with at most two retries. an alternative is that, in lines 6 and 14, a predefined max simulation run length is used (instead of the transient property φ) to determine the end of a simulation. this is also precisely what is done to perform steady-state analysis using seq, where the property query does not specify an end-of-simulation condition."
"nuclear reactors, smart power grids, automated storm surge barriers, networked industrial automation systems: we increasingly rely on critical technical systems and infrastructures whose failure or extended unavailability would have drastic consequences. it is imperative to perform a quantitative evaluation in the design phase based on a formal stochastic model, e.g. on extensions of continuous-time markov chains (ctmc), stochastic petri nets (spn), or fault trees. only after the probability of failure and the expected unavailability are shown to be sufficiently low can the system design be implemented. calculating such values-which may be on the order of 10 −15 or lower-is challenging. for finite-state markov chains or probabilistic timed automata (pta [cit] ), probabilistic model checking can numerically approximate the desired values, but the ance splitting for res in a specific model can vary drastically with the choices made for these parameters [cit] ."
"traditionally, f i is specified ad hoc for each model domain by a res expert [cit] . methods to automatically compute an importance function are usually specialised to a specific formalism or a particular model structure, potentially providing guaranteed efficiency improvements [cit] . we use an automatic method that is applicable to any stochastic compositional model with a partly discrete state space. as a heuristic, it does not provide mathematical guarantees of performance improvements, but is aimed at generality and providing \"usually good\" results with minimal user input. we describe this method in detail in section 3."
"a disadvantage of fixed success is that it is not guaranteed to terminate: if the model, importance function, and thresholds are such that, with positive probability, it may happen that all initial states found for some level lie in a bottom strongly connected component without target states, then the (modified) loop of line 3 of the algorithm diverges. we have not encountered this situation in our experiments, though."
"runs that continue after splitting. each run is tagged with the level on which it is created. when a run crosses a threshold from above into a level below its creation level, it ends (the run is killed). a run also ends when it reaches an avoid or target state. we state restart formally to perform importance splitting for transient analysis as algorithm 1. figure 2 illustrates its behaviour. the horizontal axis is the model's time steps while the vertical direction shows the current state's importance. target states are marked and avoid states are marked #. we have three levels with thresholds at importance values 3 to 4 and 9"
"given a model and importance function f i, importance splitting increases the simulation effort of sample paths that visit states with growing importance. this can be carried out in different ways, as we detail in section 4. all techniques save and restore states from sample paths. for instance, in a typical restart implementation, when a simulation run (i.e. a path currently being sampled in the model) visits a state with higher importance than those observed before, the state is saved and new (independent) simulation runs are initiated from that state."
"restart, as presented in algorithm 1 for transient analysis, is carefully designed such that the mean of the results of many restart runs is an unbiased estimator for the true probability of the transient property [cit] . in particular, over many restart runs, underestimation caused by runs that die when going down is compensated by overestimation from the one that survives and is later split again."
"the application of restart to steady-state analysis is a special case of the batch-means method [cit] . for a single restart run performed from m.initial() for t (simulation) time units, say m runs visited the rare event. let t * j be the total time that the j-th such run spent on a target state. thenγ"
"the result of a restart run-consisting of a main and several child runs-is the weighted number of runs that reach target. each run's weight is 1 divided by the product of the splitting factors of all levels. the result is thus a positive rational number. note that this is in contrast to standard monte carlo simulation, where each run is a bernoulli trial with outcome 0 or 1. this affects the statistical analysis on which the confidence interval over multiple runs is built."
"the goal of our work was to find a res approach that provides consistently good performance at a maximal degree of automation. aside from the compositional importance function generation, we have three splitting methods and two approaches to threshold selection, all implemented in two different tools, at our disposal now. to find out whether there is a combination of all of these that consistently works well, and that could thus be used as a fully-automated default setting in modes and fig, we perform an experimental evaluation of all method combinations on a number of benchmarks and case studies from the literature. for transient properties, we use modes in order to cover a wide variety of modelling formalisms from ctmc to sta and exploit its more efficient specialised simulation engines. for steady-state properties, we use fig on both ctmc encoded as iosa and true iosa models that make use of non-markovian continuous probability distributions."
"in contrast to restart, each run of the fixed effort method [cit] performs a fixed number f e (l) of partial runs on each level l. each of these ends when it either crosses a threshold from below into level l + 1, encounters a target state, or encounters an avoid state. we count the first two cases as n l up . in the first case, the new state is stored in a set of initial states for level l + 1. when the initial state of each partial run can be chosen randomly, or in a round-robin fashion among the available initial states [cit] . when a fixed effort run ends, the fraction of partial runs started in level l that moved up is an approximation of the conditional probability of reaching level l + 1 given that level l was reached. since target states exist only on the highest level, the overall result is thus simply the product of the fraction n l up / f e (l) for all levels l, i.e. a rational number in the interval [cit] . the average of the result of many fixed effort runs is again an unbiased estimator for the probability of the transient property [cit] ."
"for steady-state properties, we use fig to evaluate the performance of the presented methods. of the splitting methods, only restart is designed for steady-state properties. we thus only consider the combinations of restart with the automatic compositional importance function generation and the two threshold and factor selection methods from section 5."
"for models with a notion of time that is different from the process' index (e.g. for ctmc as explained above), we take the above definition of the steady-state probability on the corresponding induced (continuous-time) process with that notion of time instead. this works since our events are defined over states only."
"modes is the statistical model checker of the modest toolset. it implements all of the methods described in sections 3 to 5, however for transient properties only. it uses the toolset's infrastructure to transform various input languages into an internal metamodel corresponding to a network of stochastic hybrid automata (sha [cit] ) with discrete variables. the following input languages are currently supported: -modest [cit], a process algebra-based modelling language for stochastic timed systems featuring high-level constructs such as recursive process calls, loops, and exception handling; -xsadf [cit], an extension of scenario-aware dataflow with continuous probability distributions and nondeterminism, a formalism particularly suited to the study of embedded streaming applications; and -jani [cit], a model exchange format designed to improve the interoperation of quantitative verification tools. other tools provide converters to jani from various petri net formats or the prism language [cit] . jani closely corresponds to the modest toolset's internal metamodel. due to the mapping to a single internal representation, modes naturally provides res capabilities for all of these input languages. the complexity of generating simulation traces-for res or monte carlo simulation-however inherently depends on the underlying mathematical modelling formalism. an input language may support multiple such formalisms. modes contains simulation algorithms specifically optimised for the following cases [cit] : -for dtmc (discrete-time markov chains), simulation is simple and efficient: obtain the current state's probability distribution over successors, randomly select one of them (using the distribution's probabilities), and continue from that state. -for ctmc, the situation is similar: obtain the set of enabled outgoing transitions, randomly select a delay from the exponential distribution parameterised by the sum of their rates, then make a random selection of one transition weighted by the transitions' rates. -pta extend markov decision processes with clocks, transition guards, and location invariants as in timed automata. pta explicitly keep a memory of elapsed times in the clocks. they admit finite-state abstractions that preserve reachability probabilities and allow them to essentially be simulated as dtmc. modes implements region graph-and zone-based simulation of pta as dtmc [cit] . with fewer restrictions, they can also be treated as sta. -sta extend pta with general continuous probability distributions. the sta simulator needs to keep track of the values of all clocks. for each transition, it has to compute the set of time points at which the transition is enabled. these sets can be unions of several disjoint intervals. overall, sta simulation thus requires relatively higher computational effort."
"f e (l) partial runs, each of which will end with probability 1. the method is specifically designed for transient properties; it does not map naturally to steady-state analysis where there is no end-ofsimulation condition. like restart needs splitting levels via function f s, fixed effort needs the effort function f e that determines the number of partial runs for each level."
"both training and test sets do not include false positive face tracks. we manually removed false positive face tracks, although most can be eliminated automatically using various simple post-processing methods. figure 2 . equal error rate (eer) as a function of the number of training examples when using metrics learned from only supervised tracks (s, green) and using semi-supervised learning that also exploits unlabeled tracks to learn the metric (s+u, magenta). the performance of the l2 distance (red) and a metric learned on the lfw set (blue) are also shown for reference."
"face identification is the problem of determining whether two faces are of the same person or not, i.e. it is a binary classification task over pairs of examples, where the positive class corresponds to face pairs of the same person. this contrasts with face recognition, where a face should be recognized as one of a set of known individuals, or potentially rejected as being none of those, which is a multi-class classification problem over single examples. generally, the identification confidence score can be interpreted as a similarity measure between faces: faces are more similar as they are more likely to be classified as a positive pair. face identification is extremely challenging since the appearance variability of a single person may be very large compared to inter-person variations. subtle inter-person appearance variations are easily obscured by big intra-person appearance variations due to photometric factors such as lighting, scale, and viewpoint, or due to changes in expression, hair style, or occlusions. in this work we address face identification in videos where, instead of a single image per face, we have a sequence of face images collected using a tracker."
"in the following section we discuss the related work in more detail. in section 3 we present our face identification approach, as well as the extraction of face tracks and facial features. in section 4 we present our experimental results based on three episodes of the tv series \"buffy the vampire slayer\". finally, we present our conclusions in section 5."
"in the experiments involving supervised and semisupervised learning, we provide tracks only from the eight main characters as the supervised examples. in contrast, for the unsupervised and semi-supervised scenarios, unsupervised learning is performed on the tracks both from the main characters and the ones labeled as \"other\". this provides a realistic setting where the unsupervised learning includes faces of many other people, e.g. in the background, that are not the main characters in the video. considering that the \"other\" category constitutes approximately 25% of the test tracks, its presence significantly increases the difficulty of unsupervised learning."
"other recent work studies face recognition without using labeled examples. instead, incomplete or ambiguous forms of supervision are used. for example, [cit] consider recognition of people in captioned images taken from yahoo!news by automatically linking faces in the image with names in the caption. they do so based on correlations between name occurrence and face appearance that can be detected in large data collections. others have worked on uncontrolled video material such as tv series [cit] or movies [cit], where scripts and subtitles can be used to obtain cues as to which characters are present when. these weak cues for character presence are then combined with facial similarities to perform character recognition."
"in order to build face tracks in videos, we first use a face detector on individual video frames and then link the obtained detections. such a detection-based approach for object tracking has been shown effective in uncontrolled videos [cit] ."
"in figure 8 we illustrate several clusters, selected from the clustering with eight clusters, which equals the number of characters in the test set. we use the two best solutions selected from table 2 : the clustering obtained with the unsupervised cast-specific metric (top, cost 95), and the one obtained by supervised learning on all 227 training tracks (bottom, cost 41). for each cluster we show one face per track, with a maximum of eight. the clusters are sorted by size from top to bottom, and we do not display clusters which contain only a one or two tracks."
"the motivation for the min-min distance is that it will be robust against pose and expression changes, since it only compares the most similar appearances. when we use metrics specifically learned to suppress intra-person appearance variations, ideally, all faces of the same person should be close and not only the ones with the same expression or pose. therefore, we could also use the average face-to-face distance"
"similarly, using all pairs of tracks that appear together in a video frame, we generate a set of negative training pairs n u by collecting all between-track face pairs:"
"our goal is to exploit unlabeled face tracks to learn metrics that are robust to intra-person appearance variations. by using unlabeled tracks, we can learn a metric from the same faces that need to be recognized at a later stage. closely related to our work, [cit] learns metrics from captioned news images in a multiple-instance learning setting where bags of examples (faces in an image) come with bags of labels (names in the caption). an alternating optimization procedure learns a metric based on names-faces associations, and then updates the name-face associations given the metric. in our work we go one step further by not requiring any labels at all; instead we rely on the structure of the face tracks."
"in this section we describe our processing pipeline to extract face-tracks, and facial-features in section 3.1, see figure 1 for an overview. we continue in section 3.2 to present how we learn metrics for face identification from the extracted face tracks, and how we used them for track identification in section 3.3."
"in figure 2 we used the average face-to-face distance d a (·, ·) to define the track-to-track distance. in table 1 we compare these eer rates to the ones obtained using the min-min distance with our cast-specific metrics. we see that the average distance consistently outperforms the min-min distance. to understand this, we plot in figure 4 histograms of the face-to-face distances found among positive and negative track pairs using the fully supervised metric. while generally positive pairs have smaller distances, some negative face pairs also have small distances. therefore, it is more robust to measure the track-to-track distances by av- eraging, so as to reduce the influence of a single face pair with a small distance. for the l2 and lfw metrics there is very little performance difference, they achieve 41.9% and 35.5% respectively using the min-min distance, compared to 42.5% and 36.2% using average distance."
"a potential advantage of the average distance is that it is based on more face comparisons and might therefore be less sensitive to outliers: a pair of faces of different people that have, erroneously, a small distance. we will compare these two track distances for identification in our experiments."
"face track identification. in our first set of experiments we evaluate track identification performance using different metrics. figure 2 shows the identification equal error rate (eer) as a function of the total number of supervised training tracks. the eer is computed by sorting all pairs of test tracks by their distance, then computing for all distance thresholds the false positive and false negative rate, and then reporting the point where both errors are equal. we compare the results obtained using only the supervised tracks to learn the metric, and when including the unlabeled tracks for metric learning. the left-most point on the semisupervised curve (s+u) corresponds to only using unsupervised examples. when using supervised tracks, we choose an equal number of tracks of each character from the training set when possible, when all tracks of one character are exhausted we add more examples of other characters."
"recently, there has been considerable interest in face recognition without using labeled examples [cit] . instead, ambiguous and incomplete supervision from image captions, or subtitles and scripts for video, are used in combination with facial similarity to perform recognition. in contrast to our work, default or non-optimized metrics are used to define face similarities. we show that this is suboptimal, as these similarities can be sensitive to intraperson appearance changes due to nuisance factors such as lighting, scale, and viewpoint changes, or due to changes in expression, hair style, or occlusions."
"we split the data set into 312 training and 327 test tracks, with the number of training and test tracks being approximately equal for each character. there are 85 training and 71 testing tracks assigned to the \"other\" category. when separating the data into training and test set, we use temporally continuous parts, the length of which vary depending on the distribution of occurrence of a character. the tracks in the training set are used for supervised learning, and the ones in the test set to evaluate performance. the tracks in the test set are also used to gather unsupervised examples for metric learning. however, we never use the category labels of the test tracks for training."
"in figure 3 we visualize the metric learning results by projecting the faces in the test set on the 2d principal subspace of the matrix l that has been learned. we can see that the different characters are completely mixed when using the lfw metric, while the cast-specific metrics yield much better separation. note that using the completely unsupervised metric (figure 3(c) ), each person is represented in different clusters, while this is not the case using all 227 training tracks as supervision. this is explained by the training bias in the unsupervised case: groups of tracks of a single person might remain separated, if there are no positive training pairs that link different tracks."
"the clustering produced using the unsupervised metric is fair, but unbalanced. although the first cluster is only 55% pure, the second cluster is 93% pure, and contains the same person under a wide range of poses, expressions, and lighting conditions. the last two clusters are pure, but contain only a few tracks. the fully supervised metric yields clusters that are much more balanced in size, and with a high degree of purity. we find this an encouraging result, since the clustering itself is completely unsupervised. it essentially shows that using cast-specific metrics we can group face tracks from uncontrolled video by identity with a high degree of accuracy."
"we have shown that learning a cast-specific metric is useful to improve results for identification, recognition, and clustering of face tracks automatically extracted from uncontrolled tv video. we have also shown that to some degree, such metrics can be learned in an unsupervised manner, by exploiting the temporal structure of the face tracks to sample training pairs for metric learning. a third conclusion is that face identification metrics learned on the labeled faces in the wild data set do not offer a great advantage over using a simple l2 metric over the face descriptors. this can be explained by the differences between news images and tv video, e.g. lighting is generally good in news photographs, and very poor in tv video. another difference is the amount of pose variation: while in news photography people tend to face the camera, in video a wide range of poses is observed as characters engage in conversation or other actions. finally, in video one also has to cope with poor image quality due to motion blur. in future work we want to extend the our work to also exploit the profile faces contained in the tracks, and to learn metrics that are not only able to compare faces that are either both profile or frontal, but also to compare pairs of faces where one is frontal and the other profile. another goal of future work is to evaluate our cast-specific metrics in the full subtitle and script based character recognition setting. furthermore, we are interested in the applications of our approach to other instance verification problems in video where tracking can be exploited to drive unsupervised metric learning."
"face-track recognition. in our next set of experiments we evaluate face recognition using the different metrics. in figure 5 we use a nearest neighbor (nn) classifier to assign the test tracks to one of the eight characters, while in figure 6 we use a kernelized multi-class logistic discriminant classifier. for both classifiers we use distances learned from (i) unsupervised examples, (ii) only supervised examples, and (iii) the semi-supervised combination of these. we use the same tracks to learn the (semi-) supervised metrics and the classifiers. for comparison, we also include results obtained using (iv) the l2 metric, and (v) a metric learned on the lfw data set. we see that also for recognition, the cast-specific metrics yield much better performance than using the l2 or lfw metric. using all 227 training tracks for recognition and the logistic discriminant classifier, the lfw metric yields a recognition rate of 68%, where the semi-supervised metric attains 86%. for small numbers of training examples, the unsupervised metrics perform comparable to the supervised ones, while for larger numbers of labeled samples it is advantageous to include the unsupervised examples. perhaps surprisingly, we find both classifiers to give similar results."
"our first contribution is to show that such cast-specific metrics lead to significantly better performance than generic metrics trained on faces of many other people. our second contribution is to show that cast-specific metrics can be learned without any supervision. given face tracks, we exploit the fact that all faces in a given track are of one person, and that two tracks that appear in the same video frame contain faces of different people. in this manner we automatically collect positive and negative face pairs to train a cast-specific metric. we refer to this approach as \"unsupervised\" metric learning throughout the paper. note that it can also be considered as a \"self-supervised\" learning approach."
"our data set consists of tracks from episodes 9, 21 and 45 of the tv series \"buffy the vampire slayer\", where each episode belongs to a different season of the series. we manually annotated 639 of the automatically extracted face tracks, which in total encompass around 45.000 face detections. in our annotations, we use nine categories, where eight of them represent the main characters and the remaining one is used for other characters."
"in the left plot of figure 7 we give the labeling costs for unsupervised metrics: l2, learned from the lfw data set, and using unsupervised learning from the face tracks in the test set. we see that for up to 10 clusters, the l2 and lfw metric yield costs that are near the worst possible cost. by inspection, we find that this is because they generate one big cluster that contains almost all faces, and others with very few faces. in the right plot we compare (semi-) supervised metrics learned from 10 and 227 labeled training tracks. using only 10 labeled tracks supervised-only learning performs about as badly as the l2 and lfw metrics, and in this case adding the unsupervised learning significantly improves the performance. using all 227 training tracks to learn the metric allows to obtain much better results, and in this case including the unsupervised training examples from the test set has little effect on performance. in table 2 we give the labeling cost obtained in the case of eight clusters, corresponding to the number of characters in the test set."
"many of the false positives of the face detector do not have temporal support. therefore, such false detections are easily eliminated by forming face tracks only from detections with a sufficiently large number of shared klt pointtracks, and then discarding very short tracks. similarly, there are sometimes temporal gaps in the true face tracks. such missed detections are recovered by filling in these gaps using a least-squares estimation technique [cit] . using the bounding-box coordinates of the detections in a track, the coordinates of the missing detections are estimated by minimizing the distances to the coordinates of neighboring detections. the same estimation method is also used for temporal smoothing of the already existing detection bounding boxes."
"in practice a large number of training pairs can be generated without using any supervision: the 327 tracks in our test set generate roughly 1.4 million positive pairs, and the 79 pairs of distinct tracks that occur at the same time yield approximately 600.000 negative training pairs. this large number of training pairs obtained in this manner, however, have some biases. the positive within-track pairs occur nearby in time, which means that they show less appearance variations, e.g. lighting and pose will vary less within a track than across different tracks. the negative tracks can be biased: if there are some characters that co-occur much more often than others, the metric learning will focus on distinguishing these characters."
"we experimentally compare our unsupervised castspecific metric to a cast-specific metric learned from labeled face tracks as well as to generic ones. as generic metrics we use the l2 distance over the face descriptors and a metric learned on the lfw data set. experimental results show that our completely unsupervised cast-specific metric significantly outperforms generic metrics. furthermore, using a small number of labeled face tracks in addition to the automatically generated training pairs further reduces the error rates to around half the error of the generic metrics."
